<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7032 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7032</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7032</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-265659184</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.02783v3.pdf" target="_blank">Large Language Models on Graphs: A Comprehensive Survey</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7032.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7032.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plain Verbalization (edge/adjacency lists)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plain verbalization of graphs as edge lists or adjacency lists (natural language / text tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple graph-to-text approach that writes graph edges or adjacency lists directly as natural language or symbol sequences (e.g., "[(0,1),(1,2),(2,0)]" or "Node 0 is connected to 1 and 2...") to feed into language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Edge list / adjacency list natural-language serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each graph is converted to an explicit textual listing of edges or adjacency lists, either using compact symbolic tuples (e.g., '(u,v)') or full natural-language sentences describing neighbors for each node. No special tokens beyond normal text/symbol tokens are required.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential / token‑based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>edge-list ordering or per-node adjacency listing (no specific traversal mandated)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>NLGraph, LLMtoGraph, GUC, GraphQA (datasets referenced for pure-graph reasoning evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph reasoning / graph question answering (connectivity, shortest path, cycle detection, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LLMs (e.g., GPT-family, LLaMA; used in zero-shot / few-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained decoder-only or encoder-decoder LLMs used in prompting setups (zero/few-shot or fine-tuned).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (on graph QA tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: works on simpler tasks (connectivity, neighbor identification); performance degrades on complex tasks and larger graphs (e.g., cycle detection, Hamiltonian paths).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Allows direct use of existing LLMs without architectural change; simple baseline for probing graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Can be verbose and long for even moderately sized graphs, hitting LLM context limits; unstructured for large graphs; poor scaling and limited reliability on complex algorithmic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Less effective than feature-encoding/fine-tuning approaches and GNN-assisted encodings for complex reasoning; performs similar or worse than paraphrased or grounded verbalizations on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7032.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7032.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paraphrased / Grounded Verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paraphrased or grounded natural-language verbalization of graph structure (Format-Explanation, Role Prompting, grounding in scenarios)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transforms raw edge/adjacency descriptions into more natural, concise, or domain-grounded sentences (e.g., framing nodes as people in a social graph) or prompts the LLM to reformat the graph input for itself.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Paraphrased / format-explained graph descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph structure is embedded in more natural narrative or role-based text (e.g., 'Alice knows Bob and Charlie; Bob knows ...') or produced by self-formatting prompts (Format-Explanation) intended to be easier for LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential / token‑based / narrative</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>paraphrasing of adjacency/edge lists into concise natural-language templates or role-play scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>GraphQA and other pure-graph reasoning benchmarks (as used in evaluations cited)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph reasoning (QA), probing LLM reasoning improvement from naturalized inputs</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LLMs under prompting (zero-shot CoT, role prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained LLMs used with tailored prompts to encourage reformatting or role-based grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy on graph reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: can improve performance on some tasks relative to raw verbalization, but improvements are inconsistent and not systematic across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>No retraining required; can sometimes improve few-shot or CoT reasoning by making graph structure more natural for LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Improvements are task-dependent and not reliable for complex algorithmic graph problems; still subject to length/context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Often better than raw symbolic lists for some tasks, but inferior to specialized encodings or explicit graph-feature encodings when available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7032.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7032.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Implicit Feature Sequence (GraphEnc → LM tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implicit feature sequence: graph encoder (GNN) produces token sequences projected into LM token space</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graphs are encoded by a learned graph encoder (e.g., GNN) into a sequence of latent tokens which are projected into the language model token embedding space and concatenated with text tokens as LM input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GraphEnc-produced special tokens projected to text token space</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A GNN or graph encoder produces a set or sequence of vector tokens representing graph structure; learnable projection matrices map these vectors into the LM's token embedding space so they can be consumed alongside normal text tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token‑based / sequential (learned latent tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>learned graph encoding (GNN/Graph Transformer) followed by linear projection into LM embedding space; ordering may follow encoder output ordering</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Tasks reported in [41] (substructure counting, maximum triplet sum, shortest path, bipartite matching) and text-attributed graph datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph reasoning and graph-related prediction tasks (both synthetic algorithmic tasks and real graph tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Hybrid GNN + LLM pipelines (examples: GNP, GraphGPT, DGTL, METERN as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GNN or Graph Transformer used as graph encoder; large pretrained LM (encoder-only/decoder-only) consumes projected tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task-specific accuracy/counting error; qualitative improvement reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported as substantial improvement in [41] for several algorithmic problems after fine-tuning LLMs on graph-encoder produced sequences (qualitative; numeric details not provided in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables fine-tuning LLMs with compact learned graph representations, yielding large improvements on specialized distributions; requires training the graph encoder and projection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires extra trainable graph encoder and projection layers; modality gap between graph embeddings and text embeddings must be bridged; introduces additional training complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms raw verbalization and heuristic paraphrasing in the tasks cited when fine-tuned; more costly than pure rule-based sequence linearizations but more expressive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7032.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7032.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rule-based Graph2Seq (ego-graph templates)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rule-based Graph2Seq: linearization of ego-graphs into natural-language templates (Graph as Sequence, rule-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Serialize each node's ego-graph into a short natural-language template (often limited to a few hops) and feed the sequence to a language model for node-level tasks such as classification or link prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Rule-based ego-graph linearization (natural-language templates)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Hand-crafted templates describe the center node and its neighboring nodes/relationships (e.g., 'The central paper is v_i. Its author neighbors are v_j and v_k...'), converting local graph context to text.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential / hierarchical (local ego-graph serialized into text)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>ego-graph extraction (1-3 hops) followed by template filling into human-readable sentences</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Academic citation and other text-attributed graphs (datasets summarized in Table II of the survey)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Node classification, link prediction, edge classification</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGLM, GraphText-style LLM prompting, encoder-only LMs (BERT/SciBERT) used as encoders</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained LMs used as feature extractors or fine-tuned predictors; templates augment input with structural context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy, Micro-F1, Macro-F1 for node/edge classification; link prediction metrics as appropriate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Simple to implement without adding parameters; can improve node-level performance especially when node text is scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Natural-language templates are brittle and may not capture rich structural patterns; limited to local neighborhoods due to sequence length constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Easier and parameter-free compared to GNN-based Graph2Seq but less expressive; benefits especially when textual node signals are weak.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7032.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7032.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNN-based Graph2Seq (projected graph tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GNN-based Graph2Seq: encoding ego-graphs with GNNs into special tokens projected into LM input</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use a graph encoder (GNN) to produce dense representations for ego-graphs or subgraphs, then map these representations to tokens compatible with LLM input space (learned projection) and concatenate with node text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GNN-produced graph tokens projected into LM token space</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph encoder aggregates neighbor structure to produce vectors which are linearly projected (with learnable matrices) into the LM token embedding space, effectively inserting graph-aware tokens into the LM sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token‑based / sequential (learned tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>graph encoder (e.g., GNN/Graph Transformer) over ego-graph; projection into LM embedding dimension; concatenation with text tokens</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Text-attributed graph benchmarks (academic, e-commerce, social networks referenced in Table II)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Node classification, link prediction, recommendation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GNP, GraphGPT, GraphGPT-style hybrids, Graph-grounded LMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cascaded architectures where a trained GNN provides additional token embeddings for an LM; LLM can be encoder-only or decoder variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Node/edge classification accuracy, F1; task-specific recommendation metrics (MRR, NDCG)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides stronger structural signals to LMs than rule-based text alone and can improve downstream accuracy when trained end-to-end or in two-stage pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Extra compute and training complexity; potential modality alignment issues between graph vectors and text embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Generally more powerful than rule-based linearizations when structure matters; more complex to train than pure LLM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7032.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7032.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph as Code Sequence (proposal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-as-code sequence (JSON/XML-like linearization for code LLMs) — proposed alternative</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proposal to linearize graph structure into code-like formats (JSON, XML) to leverage code-focused LLMs' strengths and provide a more structured, semantically explicit textual encoding of graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Code-like graph serialization (JSON / XML / structured code tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode nodes, edges, and attributes into a structured code format (e.g., JSON objects/lists) rather than free-form natural language, allowing deterministic structure and easier parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical / token‑based (structured code tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>serialize graph into structured code notation (node list, adjacency lists, attribute dicts); ordering may follow node ID or explicit canonicalization</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Proposed for zero-shot inference and better structural encoding for LMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Code-focused LLMs (CodeT5, Codex-like models suggested)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained code LLMs with strong inductive bias for structured tokens and deterministic parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Proposed to provide deterministic, more structure-aware input that could improve zero-shot performance without training new graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Proposed idea; not empirically validated in surveyed works; canonicalization and sequence length remain challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Hypothesized to be better than free-form natural language templates for structure fidelity, and simpler than training a full graph encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7032.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7032.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES (Simplified Molecular-Input Line-Entry System)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A line notation that records node (atom) symbols encountered during a depth-first traversal of a molecular graph, widely used to linearize molecules into text suitable for LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SMILES linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Depth-first traversal string notation representing atoms and bonds with special symbols; rings and branches encoded with closure symbols and parentheses.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential / token‑based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>depth-first traversal (with ring-closure and branching notation); canonicalization algorithms can produce canonical SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PubChem, ChEMBL, PI1M and other molecular corpora (used by downstream models cited)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Molecular property prediction, molecular generation, molecule-text translation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SMILES-BERT, MFBERT, MolGPT, MolXPT, Chemformer, MolT5, Galactica (adapted on SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only (SMILES-BERT), decoder-only (MolGPT), encoder-decoder (Chemformer, MolT5) LMs pretrained or fine-tuned on SMILES corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Validity/novelty/uniqueness for generation; classification/regression metrics (AUC, MAE, RMSE) for property prediction; BLEU for molecule→text tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Makes molecular graphs consumable by standard LMs and enables use of LM pretraining and generation capabilities; effective for many molecular tasks when combined with domain data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Multiple SMILES strings can represent the same molecule (permutation non-uniqueness) and naive LM generation can yield syntactically invalid SMILES; sequence order injects inductive bias and may hurt permutation invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Less robust than SELFIES (which guarantees valid molecules) and more syntactically fragile than graph-native GNN encodings; canonical SMILES reduces ambiguity but randomized SMILES can be used for augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7032.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7032.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Canonical SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Canonical SMILES (unique SMILES via canonicalization algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deterministic variant of SMILES produced by canonicalization algorithms that yields a unique SMILES representation per molecule (used to remove multiplicity of representations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Canonical SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Apply a canonicalization algorithm to SMILES generation to produce a unique, deterministic linearization for each molecular graph.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential / token‑based (deterministic)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>depth-first traversal combined with canonical ordering rules (canonicalization algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Used across molecular datasets when unique representation desired (PubChem, ChEMBL)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Property prediction, model training where deterministic representation is desired</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Any LM consuming SMILES (SMILES-BERT, MolGPT, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LMs trained on canonical SMILES to reduce representational multiplicity during learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task-specific metrics (AUC, MAE, generation validity)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reduces representation variance (one string per molecule) which can simplify training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Removes data augmentation benefits provided by multiple SMILES for the same molecule; canonicalization does not prevent syntactic errors during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Canonical SMILES trades off augmentation/diversity for determinism; randomized SMILES augmentations can improve LM generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7032.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7032.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSMILES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSMILES</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SMILES variant adapted to reduce syntactic errors during generation by changing ring-closure and branching notation to be more ML-friendly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DeepSMILES: An adaptation of SMILES for use in machine-learning of chemical structures.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>DeepSMILES linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Modifies SMILES syntax (especially ring closure and branch encoding) to simplify the grammar for machine learning models, reducing common invalid-output modes.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential / token‑based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>depth-first traversal with modified grammar for closures and branches</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Used in ML molecular generation literature</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Molecule generation and prediction</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LMs trained on DeepSMILES sequences</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LMs consume DeepSMILES as text sequences; the representation aims to reduce syntactic error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Generation validity (syntactic validity) and downstream property metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reduces some syntactic invalids in LM-generated molecules compared to vanilla SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not guarantee valid molecules in all cases; not as robust as SELFIES.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Improves over SMILES in syntactic robustness but less reliable than SELFIES for guaranteed validity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7032.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7032.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELFIES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELFIES (Self-Referencing Embedded Strings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A molecular string representation guaranteed to produce syntactically valid molecules for any SELFIES string, eliminating invalid-structure generation errors common with SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selfreferencing embedded strings (SELFIES): A 100% robust molecular string representation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SELFIES linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A formally defined string grammar that maps bijectively (or robustly) to valid molecular graphs, ensuring that any string under the SELFIES alphabet decodes to a chemically valid graph.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential / token‑based (robust)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>specialized token alphabet and grammar replacing SMILES closure/branching conventions with error-correcting constructs</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Used in molecular generation literature and LM pretraining experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Molecule generation, property prediction, LM-based molecular design</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Models trained on SELFIES (LM variants adapted to SELFIES tokenization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LMs ingest SELFIES token sequences; SELFIES guarantees decoding to valid molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Validity of generated molecules (syntactic/chemical validity), novelty, uniqueness</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: SELFIES yields near-100% syntactic validity for generated molecules (survey states 'consistently yields valid molecular graphs').</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Greatly reduces invalid-output issues in generation and simplifies model evaluation of generative outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>SELFIES strings may be longer or less human-readable than SMILES; LM pretraining corpora historically contain fewer SELFIES examples, possibly limiting zero-shot LM ability to use SELFIES without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Superior to SMILES and DeepSMILES in guaranteeing validity; may require adaptation for best LM performance due to different token distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7032.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7032.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES Enumeration / Randomized SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES enumeration / randomized SMILES (data augmentation by multiple string views)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate multiple distinct SMILES strings for the same molecular graph (by varying traversal order) to augment training data and help sequence models learn permutation invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SMILES enumeration as data augmentation for neural network modeling of molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Randomized / enumerated SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Produce alternative SMILES strings for the same molecule by randomizing the starting atom and traversal order, creating multiple text views of the same graph.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential / augmentation-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>randomized depth-first traversal and non-canonical ordering to produce multiple SMILES per molecule</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Used on molecular datasets for augmentation in generative and predictive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Model training data augmentation for property prediction and molecular generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LMs trained with SMILES data augmentation (SMILES-BERT variants, generative models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard LMs benefit from multiple SMILES views to reduce overfitting to a particular traversal ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Downstream predictive performance (classification/regression), generative quality</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: improves sequential-model performance (cited as improving generative model quality and predictive accuracy in prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Acts as an effective data augmentation technique that helps LMs learn permutation-invariant molecular features and improves generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Increases dataset size and training cost; does not inherently solve syntactic validity issues of SMILES generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Provides augmentation benefits that mimic permutation invariance of GNNs; alternative is to use permutation-invariant GNN encodings directly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7032.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7032.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Molecular Tokenization (char / BPE / SentencePiece)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecular string tokenization approaches (character-level, substring-level with BPE/SentencePiece, RT method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different tokenization strategies applied to molecular line notations: character-level tokens, subword tokenization (SentencePiece/BPE), or specialized tokenizers for regression tasks to better fit LM architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Character-level and subword (BPE/SentencePiece) tokenization for molecular strings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Tokenization of SMILES/SELFIES may be done at single-character granularity or via learned subword units (BPE/SentencePiece) to create vocabulary units better suited to model capacity and tasks; RT proposes special tokenization for regression.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token‑based / preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>apply character-split or apply subword segmentation (SentencePiece/BPE) on linearized molecule strings</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SMILES corpora (e.g., PubChem subsets) used in molecular LM pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Molecule classification, regression, generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SMILES-BERT, MolXPT, MolGPT, MolT5 and other LM variants with different tokenizers</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LMs adapted to particular tokenization schemes; substring tokenization can reduce sequence length and improve modeling of frequent motifs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task-specific metrics (AUC, MAE, BLEU, validity)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Choice of tokenizer affects sequence length, vocabulary size, and model learning dynamics; substring-level tokenization (BPE) often beneficial for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Character-level tokenization leads to longer sequences; subword vocabularies require careful construction and domain-appropriate corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Subword tokenization reduces sequence length and can capture common substructures vs. character-level which is simpler but longer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models on Graphs: A Comprehensive Survey', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. <em>(Rating: 2)</em></li>
                <li>Selfreferencing embedded strings (SELFIES): A 100% robust molecular string representation. <em>(Rating: 2)</em></li>
                <li>DeepSMILES: An adaptation of SMILES for use in machine-learning of chemical structures. <em>(Rating: 2)</em></li>
                <li>SMILES enumeration as data augmentation for neural network modeling of molecules. <em>(Rating: 2)</em></li>
                <li>Randomized SMILES strings improve the quality of molecular generative models. <em>(Rating: 2)</em></li>
                <li>Can language models solve graph problems in natural language?. <em>(Rating: 2)</em></li>
                <li>Graph-ToolFormer: To empower LLMs with graph reasoning ability via prompt augmented by ChatGPT. <em>(Rating: 1)</em></li>
                <li>GraphText: Graph reasoning in text space. <em>(Rating: 2)</em></li>
                <li>Talk like a graph: Encoding graphs for large language models. <em>(Rating: 2)</em></li>
                <li>GraphGPT: Graph instruction tuning for large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7032",
    "paper_id": "paper-265659184",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "Plain Verbalization (edge/adjacency lists)",
            "name_full": "Plain verbalization of graphs as edge lists or adjacency lists (natural language / text tokens)",
            "brief_description": "A simple graph-to-text approach that writes graph edges or adjacency lists directly as natural language or symbol sequences (e.g., \"[(0,1),(1,2),(2,0)]\" or \"Node 0 is connected to 1 and 2...\") to feed into language models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Edge list / adjacency list natural-language serialization",
            "representation_description": "Each graph is converted to an explicit textual listing of edges or adjacency lists, either using compact symbolic tuples (e.g., '(u,v)') or full natural-language sentences describing neighbors for each node. No special tokens beyond normal text/symbol tokens are required.",
            "representation_type": "sequential / token‑based",
            "encoding_method": "edge-list ordering or per-node adjacency listing (no specific traversal mandated)",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "NLGraph, LLMtoGraph, GUC, GraphQA (datasets referenced for pure-graph reasoning evaluations)",
            "task_name": "Graph reasoning / graph question answering (connectivity, shortest path, cycle detection, etc.)",
            "model_name": "General LLMs (e.g., GPT-family, LLaMA; used in zero-shot / few-shot prompting)",
            "model_description": "Pretrained decoder-only or encoder-decoder LLMs used in prompting setups (zero/few-shot or fine-tuned).",
            "performance_metric": "Accuracy (on graph QA tasks)",
            "performance_value": "Qualitative: works on simpler tasks (connectivity, neighbor identification); performance degrades on complex tasks and larger graphs (e.g., cycle detection, Hamiltonian paths).",
            "impact_on_training": "Allows direct use of existing LLMs without architectural change; simple baseline for probing graph reasoning.",
            "limitations": "Can be verbose and long for even moderately sized graphs, hitting LLM context limits; unstructured for large graphs; poor scaling and limited reliability on complex algorithmic tasks.",
            "comparison_with_other": "Less effective than feature-encoding/fine-tuning approaches and GNN-assisted encodings for complex reasoning; performs similar or worse than paraphrased or grounded verbalizations on many tasks.",
            "uuid": "e7032.0",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Paraphrased / Grounded Verbalization",
            "name_full": "Paraphrased or grounded natural-language verbalization of graph structure (Format-Explanation, Role Prompting, grounding in scenarios)",
            "brief_description": "Transforms raw edge/adjacency descriptions into more natural, concise, or domain-grounded sentences (e.g., framing nodes as people in a social graph) or prompts the LLM to reformat the graph input for itself.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Paraphrased / format-explained graph descriptions",
            "representation_description": "Graph structure is embedded in more natural narrative or role-based text (e.g., 'Alice knows Bob and Charlie; Bob knows ...') or produced by self-formatting prompts (Format-Explanation) intended to be easier for LLM reasoning.",
            "representation_type": "sequential / token‑based / narrative",
            "encoding_method": "paraphrasing of adjacency/edge lists into concise natural-language templates or role-play scenarios",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "GraphQA and other pure-graph reasoning benchmarks (as used in evaluations cited)",
            "task_name": "Graph reasoning (QA), probing LLM reasoning improvement from naturalized inputs",
            "model_name": "General LLMs under prompting (zero-shot CoT, role prompting)",
            "model_description": "Pretrained LLMs used with tailored prompts to encourage reformatting or role-based grounding.",
            "performance_metric": "Accuracy on graph reasoning tasks",
            "performance_value": "Qualitative: can improve performance on some tasks relative to raw verbalization, but improvements are inconsistent and not systematic across tasks.",
            "impact_on_training": "No retraining required; can sometimes improve few-shot or CoT reasoning by making graph structure more natural for LMs.",
            "limitations": "Improvements are task-dependent and not reliable for complex algorithmic graph problems; still subject to length/context limits.",
            "comparison_with_other": "Often better than raw symbolic lists for some tasks, but inferior to specialized encodings or explicit graph-feature encodings when available.",
            "uuid": "e7032.1",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Implicit Feature Sequence (GraphEnc → LM tokens)",
            "name_full": "Implicit feature sequence: graph encoder (GNN) produces token sequences projected into LM token space",
            "brief_description": "Graphs are encoded by a learned graph encoder (e.g., GNN) into a sequence of latent tokens which are projected into the language model token embedding space and concatenated with text tokens as LM input.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "GraphEnc-produced special tokens projected to text token space",
            "representation_description": "A GNN or graph encoder produces a set or sequence of vector tokens representing graph structure; learnable projection matrices map these vectors into the LM's token embedding space so they can be consumed alongside normal text tokens.",
            "representation_type": "token‑based / sequential (learned latent tokens)",
            "encoding_method": "learned graph encoding (GNN/Graph Transformer) followed by linear projection into LM embedding space; ordering may follow encoder output ordering",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Tasks reported in [41] (substructure counting, maximum triplet sum, shortest path, bipartite matching) and text-attributed graph datasets",
            "task_name": "Graph reasoning and graph-related prediction tasks (both synthetic algorithmic tasks and real graph tasks)",
            "model_name": "Hybrid GNN + LLM pipelines (examples: GNP, GraphGPT, DGTL, METERN as cited)",
            "model_description": "GNN or Graph Transformer used as graph encoder; large pretrained LM (encoder-only/decoder-only) consumes projected tokens.",
            "performance_metric": "Task-specific accuracy/counting error; qualitative improvement reported",
            "performance_value": "Reported as substantial improvement in [41] for several algorithmic problems after fine-tuning LLMs on graph-encoder produced sequences (qualitative; numeric details not provided in survey).",
            "impact_on_training": "Enables fine-tuning LLMs with compact learned graph representations, yielding large improvements on specialized distributions; requires training the graph encoder and projection.",
            "limitations": "Requires extra trainable graph encoder and projection layers; modality gap between graph embeddings and text embeddings must be bridged; introduces additional training complexity.",
            "comparison_with_other": "Outperforms raw verbalization and heuristic paraphrasing in the tasks cited when fine-tuned; more costly than pure rule-based sequence linearizations but more expressive.",
            "uuid": "e7032.2",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Rule-based Graph2Seq (ego-graph templates)",
            "name_full": "Rule-based Graph2Seq: linearization of ego-graphs into natural-language templates (Graph as Sequence, rule-based)",
            "brief_description": "Serialize each node's ego-graph into a short natural-language template (often limited to a few hops) and feed the sequence to a language model for node-level tasks such as classification or link prediction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Rule-based ego-graph linearization (natural-language templates)",
            "representation_description": "Hand-crafted templates describe the center node and its neighboring nodes/relationships (e.g., 'The central paper is v_i. Its author neighbors are v_j and v_k...'), converting local graph context to text.",
            "representation_type": "sequential / hierarchical (local ego-graph serialized into text)",
            "encoding_method": "ego-graph extraction (1-3 hops) followed by template filling into human-readable sentences",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "Academic citation and other text-attributed graphs (datasets summarized in Table II of the survey)",
            "task_name": "Node classification, link prediction, edge classification",
            "model_name": "InstructGLM, GraphText-style LLM prompting, encoder-only LMs (BERT/SciBERT) used as encoders",
            "model_description": "Pretrained LMs used as feature extractors or fine-tuned predictors; templates augment input with structural context.",
            "performance_metric": "Accuracy, Micro-F1, Macro-F1 for node/edge classification; link prediction metrics as appropriate",
            "performance_value": null,
            "impact_on_training": "Simple to implement without adding parameters; can improve node-level performance especially when node text is scarce.",
            "limitations": "Natural-language templates are brittle and may not capture rich structural patterns; limited to local neighborhoods due to sequence length constraints.",
            "comparison_with_other": "Easier and parameter-free compared to GNN-based Graph2Seq but less expressive; benefits especially when textual node signals are weak.",
            "uuid": "e7032.3",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GNN-based Graph2Seq (projected graph tokens)",
            "name_full": "GNN-based Graph2Seq: encoding ego-graphs with GNNs into special tokens projected into LM input",
            "brief_description": "Use a graph encoder (GNN) to produce dense representations for ego-graphs or subgraphs, then map these representations to tokens compatible with LLM input space (learned projection) and concatenate with node text.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "GNN-produced graph tokens projected into LM token space",
            "representation_description": "Graph encoder aggregates neighbor structure to produce vectors which are linearly projected (with learnable matrices) into the LM token embedding space, effectively inserting graph-aware tokens into the LM sequence.",
            "representation_type": "token‑based / sequential (learned tokens)",
            "encoding_method": "graph encoder (e.g., GNN/Graph Transformer) over ego-graph; projection into LM embedding dimension; concatenation with text tokens",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Text-attributed graph benchmarks (academic, e-commerce, social networks referenced in Table II)",
            "task_name": "Node classification, link prediction, recommendation",
            "model_name": "GNP, GraphGPT, GraphGPT-style hybrids, Graph-grounded LMs",
            "model_description": "Cascaded architectures where a trained GNN provides additional token embeddings for an LM; LLM can be encoder-only or decoder variants.",
            "performance_metric": "Node/edge classification accuracy, F1; task-specific recommendation metrics (MRR, NDCG)",
            "performance_value": null,
            "impact_on_training": "Provides stronger structural signals to LMs than rule-based text alone and can improve downstream accuracy when trained end-to-end or in two-stage pipelines.",
            "limitations": "Extra compute and training complexity; potential modality alignment issues between graph vectors and text embedding space.",
            "comparison_with_other": "Generally more powerful than rule-based linearizations when structure matters; more complex to train than pure LLM fine-tuning.",
            "uuid": "e7032.4",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Graph as Code Sequence (proposal)",
            "name_full": "Graph-as-code sequence (JSON/XML-like linearization for code LLMs) — proposed alternative",
            "brief_description": "Proposal to linearize graph structure into code-like formats (JSON, XML) to leverage code-focused LLMs' strengths and provide a more structured, semantically explicit textual encoding of graphs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Code-like graph serialization (JSON / XML / structured code tokens)",
            "representation_description": "Encode nodes, edges, and attributes into a structured code format (e.g., JSON objects/lists) rather than free-form natural language, allowing deterministic structure and easier parsing.",
            "representation_type": "hierarchical / token‑based (structured code tokens)",
            "encoding_method": "serialize graph into structured code notation (node list, adjacency lists, attribute dicts); ordering may follow node ID or explicit canonicalization",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Proposed for zero-shot inference and better structural encoding for LMs",
            "model_name": "Code-focused LLMs (CodeT5, Codex-like models suggested)",
            "model_description": "Pretrained code LLMs with strong inductive bias for structured tokens and deterministic parsing.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Proposed to provide deterministic, more structure-aware input that could improve zero-shot performance without training new graph encoders.",
            "limitations": "Proposed idea; not empirically validated in surveyed works; canonicalization and sequence length remain challenges.",
            "comparison_with_other": "Hypothesized to be better than free-form natural language templates for structure fidelity, and simpler than training a full graph encoder.",
            "uuid": "e7032.5",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "SMILES",
            "name_full": "SMILES (Simplified Molecular-Input Line-Entry System)",
            "brief_description": "A line notation that records node (atom) symbols encountered during a depth-first traversal of a molecular graph, widely used to linearize molecules into text suitable for LMs.",
            "citation_title": "SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules.",
            "mention_or_use": "mention",
            "representation_name": "SMILES linearization",
            "representation_description": "Depth-first traversal string notation representing atoms and bonds with special symbols; rings and branches encoded with closure symbols and parentheses.",
            "representation_type": "sequential / token‑based",
            "encoding_method": "depth-first traversal (with ring-closure and branching notation); canonicalization algorithms can produce canonical SMILES",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "PubChem, ChEMBL, PI1M and other molecular corpora (used by downstream models cited)",
            "task_name": "Molecular property prediction, molecular generation, molecule-text translation",
            "model_name": "SMILES-BERT, MFBERT, MolGPT, MolXPT, Chemformer, MolT5, Galactica (adapted on SMILES)",
            "model_description": "Encoder-only (SMILES-BERT), decoder-only (MolGPT), encoder-decoder (Chemformer, MolT5) LMs pretrained or fine-tuned on SMILES corpora.",
            "performance_metric": "Validity/novelty/uniqueness for generation; classification/regression metrics (AUC, MAE, RMSE) for property prediction; BLEU for molecule→text tasks",
            "performance_value": null,
            "impact_on_training": "Makes molecular graphs consumable by standard LMs and enables use of LM pretraining and generation capabilities; effective for many molecular tasks when combined with domain data.",
            "limitations": "Multiple SMILES strings can represent the same molecule (permutation non-uniqueness) and naive LM generation can yield syntactically invalid SMILES; sequence order injects inductive bias and may hurt permutation invariance.",
            "comparison_with_other": "Less robust than SELFIES (which guarantees valid molecules) and more syntactically fragile than graph-native GNN encodings; canonical SMILES reduces ambiguity but randomized SMILES can be used for augmentation.",
            "uuid": "e7032.6",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Canonical SMILES",
            "name_full": "Canonical SMILES (unique SMILES via canonicalization algorithm)",
            "brief_description": "A deterministic variant of SMILES produced by canonicalization algorithms that yields a unique SMILES representation per molecule (used to remove multiplicity of representations).",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Canonical SMILES",
            "representation_description": "Apply a canonicalization algorithm to SMILES generation to produce a unique, deterministic linearization for each molecular graph.",
            "representation_type": "sequential / token‑based (deterministic)",
            "encoding_method": "depth-first traversal combined with canonical ordering rules (canonicalization algorithm)",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "Used across molecular datasets when unique representation desired (PubChem, ChEMBL)",
            "task_name": "Property prediction, model training where deterministic representation is desired",
            "model_name": "Any LM consuming SMILES (SMILES-BERT, MolGPT, etc.)",
            "model_description": "LMs trained on canonical SMILES to reduce representational multiplicity during learning.",
            "performance_metric": "Task-specific metrics (AUC, MAE, generation validity)",
            "performance_value": null,
            "impact_on_training": "Reduces representation variance (one string per molecule) which can simplify training and evaluation.",
            "limitations": "Removes data augmentation benefits provided by multiple SMILES for the same molecule; canonicalization does not prevent syntactic errors during generation.",
            "comparison_with_other": "Canonical SMILES trades off augmentation/diversity for determinism; randomized SMILES augmentations can improve LM generalization.",
            "uuid": "e7032.7",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "DeepSMILES",
            "name_full": "DeepSMILES",
            "brief_description": "A SMILES variant adapted to reduce syntactic errors during generation by changing ring-closure and branching notation to be more ML-friendly.",
            "citation_title": "DeepSMILES: An adaptation of SMILES for use in machine-learning of chemical structures.",
            "mention_or_use": "mention",
            "representation_name": "DeepSMILES linearization",
            "representation_description": "Modifies SMILES syntax (especially ring closure and branch encoding) to simplify the grammar for machine learning models, reducing common invalid-output modes.",
            "representation_type": "sequential / token‑based",
            "encoding_method": "depth-first traversal with modified grammar for closures and branches",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Used in ML molecular generation literature",
            "task_name": "Molecule generation and prediction",
            "model_name": "LMs trained on DeepSMILES sequences",
            "model_description": "LMs consume DeepSMILES as text sequences; the representation aims to reduce syntactic error rates.",
            "performance_metric": "Generation validity (syntactic validity) and downstream property metrics",
            "performance_value": null,
            "impact_on_training": "Reduces some syntactic invalids in LM-generated molecules compared to vanilla SMILES.",
            "limitations": "Does not guarantee valid molecules in all cases; not as robust as SELFIES.",
            "comparison_with_other": "Improves over SMILES in syntactic robustness but less reliable than SELFIES for guaranteed validity.",
            "uuid": "e7032.8",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "SELFIES",
            "name_full": "SELFIES (Self-Referencing Embedded Strings)",
            "brief_description": "A molecular string representation guaranteed to produce syntactically valid molecules for any SELFIES string, eliminating invalid-structure generation errors common with SMILES.",
            "citation_title": "Selfreferencing embedded strings (SELFIES): A 100% robust molecular string representation.",
            "mention_or_use": "mention",
            "representation_name": "SELFIES linearization",
            "representation_description": "A formally defined string grammar that maps bijectively (or robustly) to valid molecular graphs, ensuring that any string under the SELFIES alphabet decodes to a chemically valid graph.",
            "representation_type": "sequential / token‑based (robust)",
            "encoding_method": "specialized token alphabet and grammar replacing SMILES closure/branching conventions with error-correcting constructs",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Used in molecular generation literature and LM pretraining experiments",
            "task_name": "Molecule generation, property prediction, LM-based molecular design",
            "model_name": "Models trained on SELFIES (LM variants adapted to SELFIES tokenization)",
            "model_description": "LMs ingest SELFIES token sequences; SELFIES guarantees decoding to valid molecules.",
            "performance_metric": "Validity of generated molecules (syntactic/chemical validity), novelty, uniqueness",
            "performance_value": "Qualitative: SELFIES yields near-100% syntactic validity for generated molecules (survey states 'consistently yields valid molecular graphs').",
            "impact_on_training": "Greatly reduces invalid-output issues in generation and simplifies model evaluation of generative outputs.",
            "limitations": "SELFIES strings may be longer or less human-readable than SMILES; LM pretraining corpora historically contain fewer SELFIES examples, possibly limiting zero-shot LM ability to use SELFIES without fine-tuning.",
            "comparison_with_other": "Superior to SMILES and DeepSMILES in guaranteeing validity; may require adaptation for best LM performance due to different token distributions.",
            "uuid": "e7032.9",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "SMILES Enumeration / Randomized SMILES",
            "name_full": "SMILES enumeration / randomized SMILES (data augmentation by multiple string views)",
            "brief_description": "Generate multiple distinct SMILES strings for the same molecular graph (by varying traversal order) to augment training data and help sequence models learn permutation invariance.",
            "citation_title": "SMILES enumeration as data augmentation for neural network modeling of molecules.",
            "mention_or_use": "mention",
            "representation_name": "Randomized / enumerated SMILES",
            "representation_description": "Produce alternative SMILES strings for the same molecule by randomizing the starting atom and traversal order, creating multiple text views of the same graph.",
            "representation_type": "sequential / augmentation-based",
            "encoding_method": "randomized depth-first traversal and non-canonical ordering to produce multiple SMILES per molecule",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "Used on molecular datasets for augmentation in generative and predictive tasks",
            "task_name": "Model training data augmentation for property prediction and molecular generation",
            "model_name": "LMs trained with SMILES data augmentation (SMILES-BERT variants, generative models)",
            "model_description": "Standard LMs benefit from multiple SMILES views to reduce overfitting to a particular traversal ordering.",
            "performance_metric": "Downstream predictive performance (classification/regression), generative quality",
            "performance_value": "Qualitative: improves sequential-model performance (cited as improving generative model quality and predictive accuracy in prior work).",
            "impact_on_training": "Acts as an effective data augmentation technique that helps LMs learn permutation-invariant molecular features and improves generalization.",
            "limitations": "Increases dataset size and training cost; does not inherently solve syntactic validity issues of SMILES generation.",
            "comparison_with_other": "Provides augmentation benefits that mimic permutation invariance of GNNs; alternative is to use permutation-invariant GNN encodings directly.",
            "uuid": "e7032.10",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Molecular Tokenization (char / BPE / SentencePiece)",
            "name_full": "Molecular string tokenization approaches (character-level, substring-level with BPE/SentencePiece, RT method)",
            "brief_description": "Different tokenization strategies applied to molecular line notations: character-level tokens, subword tokenization (SentencePiece/BPE), or specialized tokenizers for regression tasks to better fit LM architectures.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Character-level and subword (BPE/SentencePiece) tokenization for molecular strings",
            "representation_description": "Tokenization of SMILES/SELFIES may be done at single-character granularity or via learned subword units (BPE/SentencePiece) to create vocabulary units better suited to model capacity and tasks; RT proposes special tokenization for regression.",
            "representation_type": "token‑based / preprocessing",
            "encoding_method": "apply character-split or apply subword segmentation (SentencePiece/BPE) on linearized molecule strings",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "SMILES corpora (e.g., PubChem subsets) used in molecular LM pretraining",
            "task_name": "Molecule classification, regression, generation",
            "model_name": "SMILES-BERT, MolXPT, MolGPT, MolT5 and other LM variants with different tokenizers",
            "model_description": "LMs adapted to particular tokenization schemes; substring tokenization can reduce sequence length and improve modeling of frequent motifs.",
            "performance_metric": "Task-specific metrics (AUC, MAE, BLEU, validity)",
            "performance_value": null,
            "impact_on_training": "Choice of tokenizer affects sequence length, vocabulary size, and model learning dynamics; substring-level tokenization (BPE) often beneficial for efficiency.",
            "limitations": "Character-level tokenization leads to longer sequences; subword vocabularies require careful construction and domain-appropriate corpora.",
            "comparison_with_other": "Subword tokenization reduces sequence length and can capture common substructures vs. character-level which is simpler but longer.",
            "uuid": "e7032.11",
            "source_info": {
                "paper_title": "Large Language Models on Graphs: A Comprehensive Survey",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules.",
            "rating": 2,
            "sanitized_title": "smiles_a_chemical_language_and_information_system_1_introduction_to_methodology_and_encoding_rules"
        },
        {
            "paper_title": "Selfreferencing embedded strings (SELFIES): A 100% robust molecular string representation.",
            "rating": 2,
            "sanitized_title": "selfreferencing_embedded_strings_selfies_a_100_robust_molecular_string_representation"
        },
        {
            "paper_title": "DeepSMILES: An adaptation of SMILES for use in machine-learning of chemical structures.",
            "rating": 2,
            "sanitized_title": "deepsmiles_an_adaptation_of_smiles_for_use_in_machinelearning_of_chemical_structures"
        },
        {
            "paper_title": "SMILES enumeration as data augmentation for neural network modeling of molecules.",
            "rating": 2,
            "sanitized_title": "smiles_enumeration_as_data_augmentation_for_neural_network_modeling_of_molecules"
        },
        {
            "paper_title": "Randomized SMILES strings improve the quality of molecular generative models.",
            "rating": 2,
            "sanitized_title": "randomized_smiles_strings_improve_the_quality_of_molecular_generative_models"
        },
        {
            "paper_title": "Can language models solve graph problems in natural language?.",
            "rating": 2,
            "sanitized_title": "can_language_models_solve_graph_problems_in_natural_language"
        },
        {
            "paper_title": "Graph-ToolFormer: To empower LLMs with graph reasoning ability via prompt augmented by ChatGPT.",
            "rating": 1,
            "sanitized_title": "graphtoolformer_to_empower_llms_with_graph_reasoning_ability_via_prompt_augmented_by_chatgpt"
        },
        {
            "paper_title": "GraphText: Graph reasoning in text space.",
            "rating": 2,
            "sanitized_title": "graphtext_graph_reasoning_in_text_space"
        },
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models.",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "GraphGPT: Graph instruction tuning for large language models.",
            "rating": 1,
            "sanitized_title": "graphgpt_graph_instruction_tuning_for_large_language_models"
        }
    ],
    "cost": 0.024461249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>27 September 2024</p>
<p>Bowen Jin bowenj4@illinois.edu 
University of Illinois at Urbana-Champaign
61820ChampaignILUSA</p>
<p>Gang Liu 
University of Illinois at Urbana-Champaign
61820ChampaignILUSA</p>
<p>Jin ) Bowen 
University of Illinois at Urbana-Champaign
61820ChampaignILUSA</p>
<p>University of Notre Dame
Notre Dame
46556INUSA</p>
<p>Chi Han chihan3@illinois.edu 
University of Notre Dame
Notre Dame
46556INUSA</p>
<p>Heng Ji hengji@illinois.edu 
University of Notre Dame
Notre Dame
46556INUSA</p>
<p>Jiawei Han hanj@illinois.edu 
University of Notre Dame
Notre Dame
46556INUSA
27 September 2024C15CD5B6A334EB385E0BB8D58A98792010.1109/TKDE.2024.3469578Received 1 February 2024; revised 16 June 2024; accepted 10 September 2024.</p>
<p>Abstract-Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning).While LLMs are mainly designed to process pure texts, there are many realworld scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions).Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning).In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs.We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs.We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models.Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets.Finally, we conclude with potential future research directions in this fast-growing field.</p>
<p>Index Terms-Graph neural networks, graph representation learning, large language models (LLMs), natural language processing.</p>
<p>I. INTRODUCTION</p>
<p>L ARGE language models (LLMs) (e.g., BERT [23], T5 [29],</p>
<p>LLaMA [118]) which represents a direction of everincreasing models' sizes pre-trained on larger corpora, have demonstrated powerful capabilities in solving natural language processing (NLP) tasks, including question answering [1], text generation [2] and document understanding [3].There are no clear and static thresholds regarding the model sizes.Early LLMs (e.g., BERT [23], RoBERTa [24]) adopt an encoderonly architecture and show capabilities in text representation learning [4] and natural language understanding [3].In recent years, more focus has been given to larger decoder-only architectures [118] or encoder-decoder architectures [29].As the model size scales up, such LLMs have also shown reasoning ability and even more advanced emergent ability [5], exposing a strong potential for Artificial General Intelligence (AGI).</p>
<p>While LLMs are extensively applied to process pure texts, there is an increasing number of applications where the text data are associated with structure information which are represented in the form of graphs.As presented in Fig. 1, in academic networks, papers (with title and description) and authors (with profile text), are interconnected with authorship relationships.Understanding both the author/paper's text information and author-paper structure information on such graphs can contribute to advanced author/paper modeling and accurate recommendations for collaboration; In the scientific domain, molecules are represented as graphs and are often paired with text that describes their basic properties (e.g., mass and weight).Joint modeling of both the molecule structure (graph) and the associated rich knowledge (text) is important for deeper molecule understanding.Since LLMs are mainly proposed for modeling texts that lie in a sequential fashion, those scenarios mentioned above pose new challenges on how to enable LLMs to encode the structure information on graphs.In addition, since LLMs have demonstrated their superb text-based reasoning ability, it is promising to explore whether they have the potential to address fundamental graph reasoning problems on pure graphs.These graph reasoning tasks include inferring connectivity [6], shortest path [7], subgraph matching [8], and logical rule induction [18].</p>
<p>Recently, there has been an increasing interest [9] in extending LLMs for graph-based applications (summarized in Fig. 1).According to the relationship between graph and text presented Fig. 1.According to the relationship between graph and text, we categorize three LLM on graph scenarios.Depending on the role of LLM, we summarize three LLM-on-graph techniques."LLM as Predictor" is where LLMs are responsible for predicting the final answer."LLM as Aligner" will align the inputs-output pairs with those of GNNs."LLM as Encoder" refers to using LLMs to encode and obtain feature vectors. in Fig. 1, the application scenarios can be categorized into pure graphs, text-attributed graphs (nodes/edges are associated with texts), and text-paired graphs.Depending on the role of LLMs and their interaction with graph neural networks (GNNs), the LLM on graphs techniques can be classified into treating LLMs as the final component for prediction (LLM as Predictor), treating LLMs as the feature extractor for GNNs (LLM as Encoder), and align the latent space of LLMs with GNNs (LLM as Aligner).</p>
<p>There are a limited number of existing surveys exploring the intersection between LLMs and graphs.Related to deep learning on graphs, Liu et al. [20] discuss pretrained foundation models on graphs, including their backbone architectures, pretraining methods, and adaptation techniques.Pan et al. [21] review the connection between LLMs and knowledge graphs (KGs) especially on how KGs can enhance LLMs training and inference, and how LLMs can facilitate KG construction and reasoning.Mao et al. [203] and Li et al. [204] review LLM on graphs focusing on techniques rather than applications.In summary, existing surveys either focus more on GNNs rather than LLMs or fail to provide a systematic perspective on their applications in various graph scenarios as in Fig. 1.Our paper provides a comprehensive review of the LLMs on graphs for broader researchers from diverse backgrounds besides the computer science and machine learning community who want to enter this rapidly developing field (Fig. 2).</p>
<p>Our Contributions: The notable contributions of our paper are summarized as follows:</p>
<p>r Categorization of Graph Scenarios: We systematically summarize the graph scenarios where language models can be adopted into: pure graphs, text-attributed graphs, and text-paired graphs.</p>
<p>r Systematic Review of Techniques: We provide the most comprehensive overview of language models on graph techniques.For different graph scenarios, we summarize the representative models, provide detailed illustrations of each of them, and make necessary comparisons.</p>
<p>r Abundant Resources: We collect abundant resources on language models on graphs, including benchmark datasets, open-source codebases, and practical applications.</p>
<p>r Future Directions: We delve into the foundational prin- ciples of language models on graphs and propose six prospective avenues for future exploration.Organization of Survey: The rest of this survey is organized as follows.Section II-B introduces the background of LLMs and GNNs, lists commonly used notations, and defines related concepts.Section III categorizes graph scenarios where LLMs can be adopted and summarizes LLMs on graph techniques.Sections IV, V, and VI provides a detailed illustration of LLM methodologies for different graph scenarios.Section VII delivers available datasets, open-source codebases, and a collection of applications across various domains.Section VIII introduces some potential future directions.Section IX summarizes the paper.</p>
<p>II. DEFINITIONS &amp; BACKGROUND</p>
<p>A. Definitions</p>
<p>We provide definitions of various types of graphs and introduce the notations (as shown in Table I) in this section.</p>
<p>Definition 1 (Graph): A graph can be defined as G = (V, E).Here V signifies the set of nodes, while E denotes the set of edges.A specific node can be represented by v i ∈ V, and an edge directed from node v j to v i can be expressed as e ij = (v i , v j ) ∈ E. The set of nodes adjacent to a particular node v is articulated as
N (v) = {u ∈ V|(v, u) ∈ E}.
Definition 2 (Graph with node-level textual information): This type of graph can be denoted as G = (V, E, D), where V, E and D are node set, edge set, and text set, respectively.Each v i ∈ V is associated with some textual information d v i ∈ D. For instance, in an academic citation network, one can interpret v ∈ V as the scholarly articles, e ∈ E as the citation links between them, and d ∈ D as the textual content of these articles.A graph with node-level textual information is also called a text-attributed graph [31], a text-rich graph [61], or a textual graph [71].</p>
<p>Definition 3 (Graph with edge-level textual information): This type of graph can be denoted as G = (V, E, D).Each e ij ∈ E is associated with some textual information d e ij ∈ D. For example, in a social network, one can interpret v ∈ V as the users, e ∈ E as the interaction between the users, and d ∈ D as the textual content of the messages sent between the users.Such a graph is also called a textual-edge network [73].</p>
<p>Definition 4 (Graph with graph-level textual information): This type of graph can be denoted as the pair (G, d G ), where G = (V, E).V and E are node set and edge set.d G is the text set paired to the graph G.For instance, in a molecular graph G, v ∈ V denotes an atom, e ∈ E represents the strong attractive forces or chemical bonds that hold molecules together, and d G represents the textual description of the molecule.We note that texts may also be associated with subgraph-level concepts and then paired with the entire graph.Such a graph is also called a text-paired graph.</p>
<p>B. Background</p>
<p>(Large) Language Models: Language Models (LMs), or language modeling, is an area in the field of natural language processing (NLP) on understanding and generation from text distributions.In recent years, large language models (LLMs) have demonstrated impressive capabilities in tasks such as machine translation, text summarization, reasoning, and question answering [26], [42], [111], [112], [113], [114], [194], [208].</p>
<p>Language models have evolved significantly over time.BERT [23] marks significant progress in language modeling and representation.BERT models the conditional probability of a word given its bidirectional context, also named masked language modeling (MLM) objective :
E S∼D s i ∈S log p(s i |s 1 , . . . , s i−1 , s i+1 , . . . , s N S ) , (1)
where S is a sentence sampled from the corpus D, s i is the i-th word in the sentence, and N S is the length of the sentence.On the other hand, the objective of causal language modeling or text generation is defined as:
E S∼D s i ∈S log p(s i |s 1 , . . . , s i−1 ) . (2)
Following BERT, other masked language models are proposed, such as RoBERTa [24], ALBERT [115], and ELECTRA [116], with similar architectures and objectives of text representation.</p>
<p>Efforts have been made to combine language models with other modalities such as vision [95], [120] and biochemical structures [46], [121], [122].In this paper, we will discuss its combination with graphs.The lifecycle of an LLM usually involves some or all the following steps: pretraining, finetuning, and prompting.In pretraining, LLMs are usually trained on a larger corpus with multiple language modeling objectives [23], [26], [28], which aims to endow LLMs with strong language understanding and completion capability.If domain-specific abilities are expected, LLMs are then finetuned with a smaller amount of domainspecific data [36], [37], [38], [39], [42], [43].Human preference optimization methods are sometimes applied after this stage to align outputs better with users' intentions or social values [205], [206], [207].Finally, various prompting or prompt engineering techniques can be deployed to boost downstream task performance [47], [48], [49].A more comprehensive description can be found in Appendix A, available online</p>
<p>We would like to point out that the word "large" in LLM is not associated with a clear and static threshold to divide language models."Large" actually refers to a direction in which language models are inevitably evolving, and larger foundational models tend to possess significantly more representation and generalization power.Hence, we define LLMs to encompass both medium-scale PLMs, such as BERT, and large-scale LMs, like GPT-4, as suggested by [21].</p>
<p>Graph Neural Networks &amp; Graph Transformers:</p>
<p>In real-world scenarios, not all the data are sequential like text, many data lies in a more complex non-euclidean structure, i.e., graphs.GNN is proposed as a deep-learning architecture for graph data.Primary GNNs including GCN [83], GraphSAGE [84] and, GAT [85] are designed for solving node-level tasks.They mainly adopt a propagation-aggregation paradigm to obtain node representations:
h (l) v = AGG (l) h (l−1) v , PROP (l) {h (l−1) u | u ∈ N (v)} .
When propagation is global (u ∈ V), the Graph Transformer [140], [141] with attention-weighted node importance during sum aggregation can be defined.Let W Q , W K , W V be the query, key, and value matrices, respectively, and k exp denote the similarity between two nodes.Then, we have:
Attn(h (l−1) v ) = u∈V k exp (h (l−1) v , h (l−1) u ) w∈V k exp (h (l−1) v , h (l−1) w ) h (l−1) u W V ,
where k exp (h
(l−1) v , h (l) w ) = exp( h (l−1) v W Q h (l−1) w W K √ d K
).To solve graph-level tasks, GNN models like GIN [188] or Graph Transformers obtain graph representations using a READOUT function:
h G = READOUT({h v i | v i ∈ G}).
The READOUT functions include mean pooling, max pooling, and so on.Subsequent work on GNN tackles the issues of oversmoothing [138], over-squashing [139], interpretability [144], and bias [142].While message-passing-based GNNs excel in structure encoding, researchers aim to enhance their expressiveness with Graph Transformers.These models leverage global multi-head attention mechanisms and integrate graph inductive biases through positional encoding, structural encoding, combining message-passing with attention layers, or improving attention efficiency on large graphs.Graph Transformers have been proven to be a state-of-the-art solution for many pure graph problems.</p>
<p>Language Models Versus Graph Transformers: Modern language models and graph Transformers both use Transformers [92] as the base model architecture.This makes the two concepts hard to distinguish, especially when the language models are adopted on graph applications.In this paper, "Transformers" typically refers to Transformer language models for simplicity.Here, we provide three points to help distinguish them: 1) Tokens (word token versus node token): Transformers take a token sequence as inputs.For language models, the tokens are word tokens; while for graph Transformers, the tokens are node tokens.In those cases where tokens include both word tokens and node tokens if the backbone Transformers is pretrained on text corpus (e.g., BERT [23] and LLaMA [118]), we will call it a "language model".2) Positional Encoding (sequence versus graph): language models typically adopt the absolute or relative positional encoding considering the position of the word token in the sequence, while graph Transformers adopt shortest path distance [140], random walk distance, the eigenvalues of the graph Laplacian [141] to consider the distance of nodes in the graph.3) Goal (text versus graph): The language models are originally proposed for text encoding and generation; while graph Transformers are proposed for node encoding or graph encoding.In those cases where texts are served as nodes/edges on the graph if the backbone Transformers is pretrained on text corpus, we will call it a "language model".</p>
<p>III. CATEGORIZATION AND FRAMEWORK</p>
<p>In this section, we first introduce our categorization of graph scenarios where language models can be adopted.Then we discuss the categorization of LLM on graph techniques.Finally, Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>we summarize the training &amp; inference framework for language models on graphs.</p>
<p>A. Categorization of Graph Scenarios With LLMs</p>
<p>Pure Graphs without Textual Information are graphs with no text information or no semantically rich text information.Examples include traffic graphs and power transmission graphs.Those graphs often serve as context to test the graph reasoning ability of large language models (solve graph theory problems) or serve as knowledge sources to enhance the large language models (alleviate hallucination).</p>
<p>Text-Attributed Graphs refer to graphs where nodes or edges are associated with semantically rich text information.They are also called text-rich networks [31], textual graphs [71] or textual-edge networks [73].Examples include academic networks, e-commerce networks, social networks, and legal case networks.On these graphs, researchers are interested in learning representations for nodes or edges with both textual and structure information [71] [73].</p>
<p>Text-Paired Graphs have textual descriptions defined for the entire graph structure.For example, graphs like molecules may be paired with captions or textual features.While the graph structure significantly contributes to molecular properties, text descriptions can complement our understanding of molecules.The graph scenarios can be found in Fig. 1.</p>
<p>B. Categorization of LLMs on Graph Techniques</p>
<p>According to the roles of LLMs and what are the final components for solving graph-related problems, we classify LLM on graph techniques into three main categories:</p>
<p>LLM as Predictor: This category of methods serves LLM as the final component to output representations or predictions.It can be enhanced with GNNs and can be classified depending on how the graph information is injected into LLM: 1) Graph as Sequence: This type of method makes no changes to the LLM architecture, but makes it be aware of graph structure by taking a "graph token sequence" as input.The "graph token sequence" can be natural language descriptions for a graph or hidden representations outputted by graph encoders.2) Graph-Empowered LLM: This type of method modifies the architecture of the LLM base model (i.e., Transformers) and enables it to conduct joint text and graph encoding inside their architecture.</p>
<p>3) Graph-Aware LLM Finetuning: This type of method makes no changes to the input of the LLMs or LLM architectures, but only fine-tunes the LLMs with supervision from the graph.</p>
<p>LLM as Encoder: This method is mostly utilized for graphs where nodes or edges are associated with text information (solving node-level or edge-level tasks).GNNs are the final components and we adopt LLM as the initial text encoder.To be specific, LLMs are first utilized to encode the text associated with the nodes/edges.The outputted feature vectors by LLMs then serve as input embeddings for GNNs for graph structure encoding.The output embeddings from the GNNs are adopted as final node/edge representations for downstream tasks.However, these methods suffer from convergence issues, sparse data issues, and inefficient issues, where we summarize solutions from optimization, data augmentation, and knowledge distillation perspectives.</p>
<p>LLM as Aligner: This category of methods adopts LLMs as text-encoding components and aligns them with GNNs which serve as graph structure encoding components.LLMs and GNNs are adopted together as the final components for task solving.To be specific, the alignment between LLMs and GNNs can be categorized into 1) Prediction Alignment where the generated pseudo labels from one modality are utilized for training on the other modality in an iterative learning fashion and 2) Latent Space Alignment where contrastive learning is adopted to align text embeddings generated by LLMs and graph embeddings generated by GNNs.</p>
<p>In the following sections, we will follow our categorization in Section III and discuss detailed methodologies for each graph scenario.</p>
<p>IV. PURE GRAPHS</p>
<p>The study of pure graphs in graph theory is essential for understanding the introduction of LLMs into graph-related reasoning problems.Pure graphs are a universal representation format used to address a wide range of algorithmic problems in computer science.Many graph-based concepts, such as shortest paths, specific sub-graphs, and flow networks, are strongly connected to real-world applications [132], [133], [134], [192].Therefore, reasoning based on pure graphs is crucial for providing theoretical solutions and insights for real-world applications.</p>
<p>Nevertheless, many reasoning tasks require a computation capacity beyond traditional GNNs.GNNs are typically designed to carry out a bounded number of operations given a graph size.In contrast, graph reasoning problems can require up to indefinite complexity depending on the task's nature.On the other hand, LLMs demonstrate excellent emergent reasoning ability [47], [111], [112] recently.This is partially due to their autoregressive mechanism, which enables computing indefinite sequences of intermediate steps with careful prompting or training [47], [48].</p>
<p>The following subsections discuss the attempts to incorporate LLMs into pure graph reasoning problems.We will also discuss the corresponding challenges, limitations, and findings.Table 4 in the Appendix, available online lists a categorization of these efforts.Usually, input graphs are serialized as part of the input sequence, either by verbalizing the graph structure [123], [124], [125], [127], [128], [129], [130], [131] or by encoding the graph structure into implicit feature sequences [41].The studied reasoning problems range from simpler ones like connectivity, shortest paths, and cycle detection to harder ones like maximum flow and Hamiltonian pathfinding (an NP-complete problem).A comprehensive list of the studied problems is listed in Appendix Table 5, available online.Note that we only list representative problems here.This table does not include more domain-specific problems, such as the spatial-temporal reasoning problems in [127].We first briefly describe the approaches to formatting the graph inputs to be fed to LLMs.</p>
<p>Plainly Verbalizing Graphs: Verbalizing the graph structure in natural language is the most straightforward way of representing graphs.Representative approaches include describing the edge Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.and adjacency lists, widely studied in [123], [124], [127], [130].For example, for a triangle graph with three nodes, the edge list can be written as "[(0, 1), (1,2), (2,0)]", which means node 0 is connected to node 1, node 1 is connected to node 2, node 2 is connected to node 0. It can also be written in natural language such as "There is an edge between node 0 and node 1, an edge between node 1 and node 2, and an edge between node 2 and node 0." On the other hand, we can describe the adjacency list from the nodes' perspective.For example, for the same triangle graph, the adjacency list can be written as "Node 0 is connected to node 1 and node 2. Node 1 is connected to node 0 and node 2. Node 2 is connected to node 0 and node 1."</p>
<p>Paraphrasing Graphs: The verbalized graphs can be lengthy, unstructured, and complicated to read, even for humans, so they might not be the best input format for LLMs to infer the answers.To this end, researchers also attempt to paraphrase the graph structure into more natural or concise sentences.[125] find that by prompting LLMs to generate a format explanation of the raw graph inputs for itself (Format-Explanation) or to pretend to play a role in a natural task (Role Prompting), the performance on some problems can be improved but not systematically.[130] explores the effect of grounding the pure graph in a real-world scenario, such as social networks, friendship graphs, or co-authorship graphs.In such graphs, nodes are described as people, and edges are relationships between people.</p>
<p>Encoding Graphs Into Implicit Feature Sequences: Finally, researchers also attempt to encode the graph structure into implicit feature sequences as part of the input sequence [41].Unlike the previous verbalizing approaches, this usually involves training a graph encoder to encode the graph structure into a sequence of features and fine-tuning the LLMs to adapt to the new input format.</p>
<p>A. Direct Answering</p>
<p>Although graph-based reasoning problems usually involve complex computation, researchers still attempt to let language models directly generate answers from the serialized input graphs as a starting point, partially because of the simplicity of the approach and partially in awe of other emergent abilities of LLMs.Although various attempts have been made to optimize how graphs are presented in the input sequence discussed in the sections above, bounded by the finite sequence length and computational operations, this approach has a fundamental limitation to solving complex reasoning problems such as NP-complete ones.Unsurprisingly, most studies find that LLMs possess preliminary graph understanding ability, but the performance is less satisfactory on more complex problems or larger graphs [41], [123], [124], [125], [127], [130] where reasoning is necessary.</p>
<p>On plainly verbalized graphs, one can prompt LLMs to answer questions either in zero-shot or few-shot (in-context learning) settings.The former asks questions directly given the graph structure, while the latter asks questions about the graph structure after providing a few examples of questions and answers.[123], [124], [125] do confirm that LLMs can answer easier questions such as connectivity, neighbor identification, and graph size counting but fail to answer more complex questions such as cycle detection and Hamiltonian pathfinding.Their results also reveal that providing more examples in the few-shot setting increases the performance, especially on easier problems, although it is still not satisfactory.Results on paraphrased graphs indicate that encoding in real-world scenarios can improve performance on some problems, but it still cannot be done consistently.By encoding graphs into features, [41] demonstrates drastic performance improvement on problems including substructure counting, maximum triplet sum, shortest path, and bipartite matching.This indicates that fine-tuning LLMs has great fitting power on a specific task distribution.</p>
<p>B. Heuristic Reasoning</p>
<p>Direct mapping to the output leverages the LLMs' powerful representation power to "guess" the answers.Still, it does not fully utilize the LLMs' impressive emergent reasoning ability, which is essential for solving complex reasoning problems.To this end, attempts have been made to let LLMs perform heuristic reasoning on graphs.This approach encourages LLMs to perform a series of intermediate reasoning steps that might heuristically lead to the correct answer, which resembles a path-finding reasoning schema [202].</p>
<p>Reasoning Step by</p>
<p>Step: Encouraged by the success of chainof-thought (CoT) reasoning [47], [112], researchers also attempt to let LLMs perform reasoning step by step on graphs.Chainof-thought encourages LLMs to roll out a sequence of reasoning steps to solve a problem, similar to how humans solve problems.Zero-shot CoT is a similar approach that does not require any examples.These techniques are studied in [41], [123], [124], [125], [127], [130], [131].Results indicate that CoT-style reasoning can improve the performance on simpler problems, such as cycle detection and shortest path detection.Still, the improvement is inconsistent or diminishes on more complex problems, such as Hamiltonian path finding and topological sorting.</p>
<p>Retrieving Subgraphs as Evidence: Many graph reasoning problems, such as node degree counting and neighborhood detection, only involve reasoning on a subgraph of the whole graph.Such properties allow researchers to let LLMs retrieve the subgraphs as evidence and perform reasoning on the subgraphs.Build-a-Graph prompting [123] encourages LLMs to reconstruct the relevant graph structures and then perform reasoning on them.This method demonstrates promising results on problems except for Hamiltonian pathfinding, a notoriously tricky problem requiring reasoning on the whole graph.Another approach, Context-Summarization [125], encourages LLMs to summarize the key nodes, edges, or sub-graphs and perform reasoning.</p>
<p>Searching on Graphs: This kind of reasoning is related to the search algorithms on graphs, such as breadth-first search (BFS) and depth-first search (DFS) Although not universally applicable, BFS and DFS are the most intuitive and effective ways to solve some graph reasoning problems.Numerous explorations have been made to simulate searching-based reasoning, especially on knowledge-graph question answering.This approach enjoys the advantage of providing interpretable evidence besides the answer.Reasoning-on-Graphs (RoG) [128] is a representative approach that prompts LLMs to generate several relation paths as plans, which are then retrieved from the knowledge graph (KG) and used as evidence to answer the questions.Another approach is to iteratively retrieve and reason on the subgraphs from KG [129], [131], simulating a dynamic searching process.At each step, the LLMs retrieve neighbors of the current nodes and then decide to answer the question or continue the next search step.These methods address the scalability challenge when knowledge from multiple graphs is available.</p>
<p>C. Algorithmic Reasoning</p>
<p>The previous two approaches are heuristic, which means that the reasoning process accords with human intuition but is not guaranteed to lead to the correct answer.In contrast, these problems are usually solved by algorithms in computer science.Therefore, researchers also attempt to let LLMs perform algorithmic reasoning on graphs.[123] proposed "Algorithmic Prompting", which prompts the LLMs to recall the algorithms that are relevant to the questions and then perform reasoning step by step according to the algorithms.Their results, however, do not show consistent improvement over the heuristic reasoning approach.A more direct approach, Graph-ToolFormer [126], lets LLMs generate API calls as explicit reasoning steps.These API calls are then executed externally to acquire answers on an external graph.This approach is suitable for converting real-world tasks into pure graph reasoning problems, and it has demonstrated efficacy in various applications such as knowledge graphs, social networks, and recommendation systems.</p>
<p>D. Discussion</p>
<p>Despite the extensive research, there has not been a consensus about the best practice in graph representation in LLMs.The eventual solution to this problem should reach a perfect balance between computation efficiency and information completeness, probably drawing inspiration from long-context LLM researches [209], [210].The above reasoning methods are not mutually exclusive, and future efforts can be made to combine them to achieve better performance.For example, efficiency in algorithmic searching can be improved by prompting language models for better heuristics.</p>
<p>V. TEXT-ATTRIBUTED GRAPHS</p>
<p>Text-attributed graphs exist ubiquitously in the real world, e.g., academic networks, and legal case networks.Learning on such networks requires the model to encode both the textual information associated with the nodes/edges and the structure information lying inside the input graph.Depending on the role of LLM, existing works can be categorized into three types: LLM as Predictor, LLM as Encoder, and LLM as Aligner.We summarize all surveyed methods in Appendix Table 6, available online.</p>
<p>A. LLM as Predictor</p>
<p>These methods serve the language model as the main model architecture to capture both the text information and graph structure information.They can be categorized into three types: Graph as Sequence methods, Graph-Empowered LLMs, and Graph-Aware LLM finetuning methods, depending on how structure information in graphs is injected into language models (input versus architecture versus loss).In the Graph as Sequence methods, graphs are converted into sequences that can be understood by language models together with texts from the inputs.In the Graph-Empowered LLMs methods, people modify the architecture of Transformers (which is the base architecture for LLMs) to enable it to encode text and graph structure simultaneously.In the Graph-Aware LLM finetuning methods, LLM is fine-tuned with graph structure supervision and can generate graph-contextualized representations.</p>
<p>1) Graph as Sequence: In these methods, the graph information is mainly encoded into the LLM from the "input" side.The ego-graphs associated with nodes/edges are serialized into a sequence H G v which can be fed into the LLM together with the texts d v :
H G v = Graph2Seq(G v ),(3)h v = LLM([H G v , d v ]).(4)
Depending on the choice of Graph2Seq(•) function, the methods can be further categorized into rule-based methods and GNNbased methods.The illustration of the categories can be found in Fig. 3. Rule-Based: Linearizing Graphs into Text Sequence with Rules: These methods design rules to describe the structure with natural language and adopt a text prompt template as Graph2Seq(•).For example, given an ego-graph G v i of the paper node v i connecting to author nodes v j and v k and venue nodes v t and v s ,
H G v i = Graph2Seq(G v i ) = "
The centor paper node is v i .Its author neighbor nodes are v j and v k and its venue neighbor nodes are v t and v s ".This is the most straightforward and easiest way (without introducing extra model parameters) to encode graph structures into language models.Along this line, InstructGLM [45] designs templates to describe local egograph structure (maximum 3-hop connection) for each node and conduct instruction tuning for node classification and link prediction.GraphText [64] further proposes a syntax tree-based method to transfer structure into text sequence.Researchers [81] also study when and why the linearized structure information on graphs can improve the performance of LLM on node classification and find that the structure information is beneficial when the textual information associated with the node is scarce (in this case, the structure information can provide auxiliary information gain).</p>
<p>GNN-Based: Encoding Graphs into Special Tokens with GNNs: Different from rule-based methods which use natural language prompts to linearize graphs into sequences, GNN-based methods adopt graph encoder models (i.e., GNN) to encode the ego-graph associated with nodes into special token representations which are concatenated with the pure text information into Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.the language model:
H G v = Graph2Seq(G v ) = GraphEnc(G v ).(5)
The strength of these methods is they can capture hidden representations of useful structure information with a strong graph encoder, while the challenge is how to fill the gap between graph modality and text modality.GNP [40] adopts a similar philosophy from LLaVA [90], where they utilize GNN to generate graph tokens and then project the graph tokens into the text token space with learnable projection matrices.The projected graph tokens are concatenated with text tokens and fed into the language model.GraphGPT [44] further proposes to train a text-grounded GNN for the projection with a text encoder and contrastive learning.DGTL [75] introduces disentangled graph learning, serves graph representations as positional encoding, and adds them to the text sequence.METERN [74] adds learnable relation embeddings to node textual sequences for text-based multiplex representation learning on graphs [91].</p>
<p>2) Graph-Empowered LLMs: In these methods, researchers design advanced LLM architecture (i.e., Graph-Empowered LLMs) which can conduct joint text and graph encoding inside their model architecture.Transformers [92] serve as the base model for nowadays pretrained LMs [23] and LLMs [35].However, they are designed for natural language (sequence) encoding and do not take non-sequential structure information into consideration.To this end, Graph-Empowered LLMs are proposed.They have a shared philosophy of introducing virtual structure tokens H G v inside each Transformer layer:
H (l) d v = H (l) G v , H (l) d v (6)
where H G v can be learnable embeddings or output from graph encoders.Then the original multi-head attention (MHA) in Transformers is modified into an asymmetric MHA to take the structure tokens into consideration:
MHA asy H (l) d v , H (l) d v = U u=1 head u H (l) d v , H (l) d v ,
where head u H (l)
d v , H (l) d v = softmax ⎛ ⎝ Q (l) u K (l) u d/U ⎞ ⎠ • V (l) u , Q (l) u = H (l) d v W (l) Q,u , K (l) u = H (l) d v W (l) K,u , V (l) u = H (l) d v W (l) V,u . (7)
With the asymmetric MHA mechanism, the node encoding process of the (l + 1)-th layer will be:
H (l) d v = Normalize H (l) d v + MHA asy H (l) d v , H (l) d v , H (l+1) d v = Normalize H (l) d v + MLP H (l) d v . (8)
Along this line of work, GreaseLM [66] proposes to have a language encoding component and a graph encoding component in each layer.These two components interact through a modality-fusion layer (MInt layer), where a special structure token is added to the text Transformer input, and a special node is added to the graph encoding layer.DRAGON [80] further proposes strategies to pretrain GreaseLM with unsupervised signals.GraphFormers [71] are designed for node representation learning on homogeneous text-attributed networks where the current layer [CLS] token hidden states of neighboring documents are aggregated and added as a new token on the current layer center node text encoding.Patton [31] proposes to pretrain GraphFormers with two novel strategies: networkcontextualized masked language modeling and masked node prediction.Heterformer [72] introduces virtual neighbor tokens for text-rich neighbors and textless neighbors which are concatenated with the original text tokens and fed into each Transformer layer.Edgeformers [73] are proposed for representation learning on textual-edge networks where edges are associated with rich textual information.When conducting edge encoding, virtual node tokens will be concatenated onto the original edge text tokens for joint encoding.</p>
<p>3) Graph-Aware LLM Finetuning: In these methods, the graph information is mainly injected into the LLM by "finetuning on graphs".Researchers assume that the structure of graphs can provide hints on what documents are "semantically similar" to what other documents.For example, papers citing each other in an academic graph can be of similar topics.These methods adopt vanilla language models that take text as input (e.g., BERT [23] and SciBERT [25]) as the base model and finetune them with structure signals on the graph [50].After that, the LLMs will learn node/edge representations that capture the graph homophily from the text perspective.This is the simplest way to utilize LLMs on graphs.However, during encoding, the model itself can only consider text.</p>
<p>Most methods adopt the two-tower encoding and training pipeline, where the representation of each node is obtained separately and the model is optimized as follows:
h v i = LLM θ (d v i ), min θ f (h v i , {h v + i }, {h v − i }). (9)
Here v + i represents the positive nodes to v i , v − i represents the negative nodes to v i and f (•) denotes the pairwise training objective.Different methods have different strategies for v + i and v − i with different training objectives f (•).SPECTER [50] constructs the positive text/node pairs with the citation relation, explores random negatives and structure hard negatives, and fine-tunes SciBERT [25] with the triplet loss.SciNCL [51] extends SPECTER by introducing more advanced positive and negative sampling methods based on embeddings trained on graphs.Touchup-G [53] proposes the measurement of feature homophily on graphs and brings up a binary cross-entropy fine-tuning objective.TwHIN-BERT [55] mines positive node pairs with off-the-shelf heterogeneous information network embeddings and trains the model with a contrastive social loss.MI-CoL [58] discovers semantically positive node pairs with metapath [89] and adopts the InfoNCE objective.E2EG [59] utilizes a similar philosophy from GIANT [57] and adds a neighbor prediction objective apart from the downstream task objective.WalkLM [60] conducts random walks for structure linearization before fine-tuning the language model.A summarization of the two-tower graph-centric LLM fine-tuning objectives can be found in Appendix Table 7, available online.</p>
<p>There are other methods using the one-tower pipeline, where node pairs are concatenated and encoded together:
h v i ,v j = LLM θ (d v i , d v j ), min θ f (h v i ,v j ).(10)
LinkBERT [30] proposes a document relation prediction objective (an extension of next sentence prediction in BERT [23]) which aims to classify the relation of two node text pairs from contiguous, random, and linked.MICoL [58] explores predicting the node pairs' binary meta-path or meta-graph indicated relation with the one-tower language model.4) Discussion: Although the community is making good progress, there are still some open questions to be solved.</p>
<p>Graph as Code Sequence: Existing graphs as sequence methods are mainly rule-based or GNN-based.The former relies on natural language to describe the graphs which is not natural for structure data, while the latter has a GNN component that needs to be trained.A more promising way is to obtain a structure-aware sequence for graphs that can support zero-shot inference.A potential solution is to adopt codes (that can capture structures, e.g., graph XML or JSON) to describe the graphs and utilize code LLMs [22].</p>
<p>Advanced Graph-Empowered LLM Techniques: Graphempowered LLM is a promising direction to achieve foundational models for graphs.However, existing works are far from enough: 1) Task.Existing methods are mainly designed for representation learning (with encoder-only LLMs) which are hard to adopt for generation tasks.A potential solution is to design Graph-Empowered LLMs with decoder-only or encoderdecoder LLMs as the base architecture.2) Pretraining.Pretraining is important to enable LLMs with contextualized data understanding capability, which can be generalized to other tasks.However, existing works mainly focus on pretraining LLMs on homogeneous text-attributed networks.Future studies are needed to explore LLM pretraining in more diverse real-world scenarios including heterogeneous text-attributed networks [72], dynamic text-attributed networks [127], and textual-edge networks [73].</p>
<p>B. LLM as Encoder</p>
<p>LLMs extract textual features to serve as initial node feature vectors for GNNs, which then generate node/edge representations and make predictions.These methods typically adopt an LLM-GNN cascaded architecture to obtain the final representation h v i for node v i :
x v i = LLM(d v i ) h v i = GNN(X v , G). (11)
Here x v i is the feature vector that captures the textual information d v i associated with v i .The final representation h v i will contain both textual information and structure information of v i and can be used for downstream tasks.In the following sections, we will discuss the optimization, augmentation, and distillation of such models.The figures for these techniques can be found in Fig. 4.</p>
<p>1) Optimization: One-</p>
<p>Step Training refers to training the LLM and GNN together in the cascaded architecture for the downstream tasks.TextGNN [76] explores GCN [83], Graph-SAGE [84], GAT [85] as the base GNN architecture, adds skip connection between LLM output and GNN output, and optimizes the whole architecture for sponsored search task.Ads-GNN [77] further extends TextGNN by proposing edge-level information aggregation.GNN-LM [65] adds GNN layers to enable the vanilla language model to reference similar contexts in the corpus for language modeling.Joint training LLMs and GNNs in a cascaded pipeline is convenient but may suffer from efficiency [67] (only support sampling a few one-hop neighbors regarding memory complexity) and local minimal [34] (LLM underfits the data) issues.</p>
<p>Two-</p>
<p>Step Training means first adapting LLMs to the graph, and then finetuning the whole LLM-GNN cascaded pipeline.GIANT [57] proposes to conduct neighborhood prediction with the use of XR-Transformers [78] and results in an LLM that can output better feature vectors than bag-of-words and vanilla BERT [23] embedding for node classification.LM-GNN [67] introduces graph-aware pre-fine-tuning to warm up the LLM on the given graph before fine-tuning the whole LLM-GNN pipeline and demonstrating significant performance gain.SimTeG [34] finds that the simple framework of first training the LLMs on the downstream task and then fixing the LLMs and training the GNNs can result in outstanding performance.They further find that using the efficient fine-tuning method, e.g., LoRA [39] to tune the LLM can alleviate overfitting issues.GaLM [79] explores ways to pretrain the LLM-GNN cascaded Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.architecture.The two-step strategy can effectively alleviate the insufficient training of the LLM which contributes to higher text representation quality but is more computationally expensive and time-consuming than the one-step training strategy.</p>
<p>2) Data Augmentation: With its demonstrated zero-shot capability [42], LLMs can be used for data augmentation to generate additional text data for the LLM-GNN cascaded architecture.The philosophy of using LLM to generate pseudo data is widely explored in NLP [82], [88].LLM-GNN [63] proposes to conduct zero-shot node classification on text-attributed networks by labeling a few nodes and using the pseudo labels to fine-tune GNNs.TAPE [69] presents a method that uses LLM to generate prediction text and explanation text, which serve as augmented text data compared with the original text data.A following medium-scale language model is adopted to encode the texts and output features for augmented texts and original text respectively before feeding into GNNs.ENG [70] brings forward the idea of generating labeled nodes for each category, adding edges between labeled nodes and other nodes, and conducting semi-supervised GNN learning for node classification.</p>
<p>3) Knowledge Distillation: LLM-GNN cascaded pipeline is capable of capturing both text information and structure information.However, the pipeline suffers from time complexity issues during inference, since GNNs need to conduct neighbor sampling and LLMs need to encode the text associated with both the center node and its neighbors.A straightforward solution is to serve the LLM-GNN cascade pipeline as the teacher model and distill it into an LLM as the student model.In this case, during inference, the model (which is a pure LLM) only needs to encode the text on the center node and avoid time-consuming neighbor sampling.AdsGNN [77] proposes an L2-loss to force the outputs of the student model to preserve topology after the teacher model is trained.GraD [68] introduces three strategies including the distillation objective and task objective to optimize the teacher model and distill its capability to the student model.</p>
<p>4) Discussion: Given that GNNs are demonstrated as powerful models in encoding graphs, "LLMs as encoders" seems to be the most straightforward way to utilize LLMs on graphs.However, there are still open questions.</p>
<p>Limited Task: Go Beyond Representation Learning: Current "LLMs as encoders" methods or LLM-GNN cascaded architectures are mainly focusing on representation learning, given the single embedding propagation-aggregation mechanism of GNNs, which prevents it from being adopted to generation tasks (e.g., node/text generation).A potential solution to this challenge can be to conduct GNN encoding for LLM-generated token-level representations and to design proper decoders that can perform generation based on the LLM-GNN cascaded model outputs.</p>
<p>Low Efficiency: Advanced Knowledge Distillation: The LLM-GNN cascaded pipeline suffers from time complexity issues since the model needs to conduct neighbor sampling and then embedding encoding for each neighboring node.Although there are methods that explore distilling the learned LLM-GNN model into an LLM model for fast inference, they are far from enough given that the inference of LLM itself is time-consuming.A potential solution is to distill the model into a much smaller LM or even an MLP.Similar methods [86] have been proven effective in GNN to MLP distillation and are worth exploring for the LLM-GNN cascaded pipeline as well.</p>
<p>C. LLM as Aligner</p>
<p>These methods contain an LLM component for text encoding and a GNN component for structure encoding.These two components are served equally and trained iteratively or parallelly.LLMs and GNNs can mutually enhance each other since the LLMs can provide textual signals to GNNs, while the GNNs can deliver structure information to LLMs.According to how the LLM and the GNN interact, these methods can be further categorized into: LLM-GNN Prediction Alignment and LLM-GNN Latent Space Alignment.The illustration of these two categories of methods can be found in Fig. 5. with the structure data on a graph iteratively.LLM will generate labels for nodes from the text perspective and serve them as pseudo-labels for GNN training, while GNN will generate labels for nodes from the structure perspective and serve them as pseudo-labels for LLM training.By this design, these two modality encoders can learn from each other and contribute to a final joint text and graph encoding.In this direction, LTRN [56] proposes a novel GNN architecture with personalized PageRank [93] and attention mechanism for structure encoding while adopting BERT [23] as the language model.The pseudo labels generated by LLM and GNN are merged for the next iteration of training.GLEM [61] formulates the iterative training process into a pseudo-likelihood variational framework, where the E-step is to optimize LLM and the M-step is to train the GNN.</p>
<p>1) LLM-GNN Prediction</p>
<p>2) LLM-GNN Latent Space Alignment: It denotes connecting text encoding (LLM) and structure encoding (GNN) with cross-modality contrastive learning:
h d v i = LLM(d v i ), h v i = GNN(G v ),(12)l(h d v i , h v i ) = Sim(h d v i , h v i ) j =i Sim(h d v i , h v j ) , (13)L = v i ∈G 1 2|G| (l(h d v i , h v i ) + l(h v i , h d v i ))(14)
A similar philosophy is widely used in vision-language joint modality learning [95].Along this line of approaches, Con-Grat [52] adopts GAT [85] as the graph encoder and tries MPNet [33] as the language model encoder.They have expanded the original InfoNCE loss by incorporating graph-specific elements.These elements pertain to the most likely second, third, and subsequent choices regarding the nodes from which a text originates and the texts that a node generates.In addition to the node-level multi-modality contrastive objective, GRENADE [54] proposes KL-divergence-based neighbor-level knowledge alignment: minimize the neighborhood similarity distribution calculated between LLM and GNN.G2P2 [62] further extends node-text contrastive learning by adding textsummary interaction and node-summary interaction.Then, they introduce using label texts in the text modality for zero-shot classification, and using soft prompts for few-show classification.THLM [32] proposes to pretrain the language model by contrastive learning with a heterogeneous GNN on heterogeneous text-attributed networks.The pretrained LLM can be fine-tuned on downstream tasks.</p>
<p>3) Discussion: Most existing methods adopt homogeneous text-graph alignment, assuming that the semantic relation between the two modalities, namely text and graph, is singular.However, this is not usually the case in the real world, given: 1) The existence of multimodal attributes: Other modalities, e.g., images can appear together with text and graph.In this case, it is worth researching how to align the multimodal attributes in a graph scenario.2) Heterogeneous semantic relations: the semantic relationships between data units (text/image/graph) can be multiplex.Different relations have different distributions and a single semantic alignment will fail to capture the comprehensively [74].</p>
<p>VI. TEXT-PAIRED GRAPHS</p>
<p>Graphs are prevalent data objects in scientific disciplines such as cheminformatics [182], [193], [199], material informatics [180], bioinformatics [200], and computer vision [146].Within these diverse fields, graphs frequently come paired with critical graph-level text information.For instance, molecular graphs in cheminformatics are annotated with text properties such as toxicity, water solubility, and permeability properties [180], [182].Research on such graphs (scientific discovery) could be accelerated by the text information and the adoption of LLMs.In this section, we review the application of LLMs on graph-captioned graphs with a focus on molecular graphs.According to the technique categorization in Section III-B, we begin by investigating methods that utilize LLMs as Predictor.Then, we discuss methods that align GNNs with LLMs.We summarize all surveyed methods in Appendix Table 8 and Figure 6, available online.</p>
<p>A. LLM as Predictor</p>
<p>In this subsection, we review how to conduct "LLM as Predictor" for graph-level tasks.Existing methods can be categorized into Graph as Sequence (treat graph data as sequence input) and Graph-Empowered LLMs (design model architecture to encode graphs).</p>
<p>1) Graph as Sequence: For text-paired graphs, we have three steps to utilize existing LLM for graph inputs.Step 1: Linearize graphs into sequence with rule-based methods.Step 2: Tokenize the linearized sequence.Step 3: Train/Finetune different LLMs (e.g., Encoder-only, Encoder-Decoder, Decoder-only) for specific tasks.We will discuss each step as follows.</p>
<p>Step 1: Rule-based Graph Linearization.Rule-based linearization converts molecular graphs into text sequences that can be processed by LLMs.To achieve this, researchers develop specifications based on human expertise in the form of line notations [147].For example, the Simplified Molecular-Input Line-Entry System (SMILES) [147] records the symbols of nodes encountered during a depth-first traversal of a molecular graph.The International Chemical Identifier (InChI) [148] encodes molecular structures into unique string texts with more hierarchical information.Canonicalization algorithms produce unique SMILES for each molecule, often referred to as canonical SMILES.However, there are more than one SMILES corresponding to a single molecule and SMILES sometimes represent invalid molecules; LLMs learned from these linearized sequences can easily generate invalid molecules (e.g., incorrect ring closure symbols and unmatched parentheses) due to syntactical errors.To this end, DeepSMILES [149] is proposed.It can alleviate this issue in most cases but does not guarantee 100% robustness.The linearized string could still violate basic physical constraints.To fully address this problem, SELFIES [150] is introduced which consistently yields valid molecular graphs.</p>
<p>Step 2: Tokenization.These approaches for linearized sequences are typically language-independent.They operate at both character level [166], [177] and substring level [161], [168], [172], [173], [174], [175], based on SentencePiece or BPE [154].</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>Additionally, RT [163] proposes a tokenization approach that facilitates handling regression tasks within LM Transformers.</p>
<p>Step 3: Encoding the Linearized Graph with LLMs.Encoder-only LLMs: Earlier LLMs like SciBERT [25] and BioBERT [179] are trained on scientific literature to understand natural language descriptions related to molecules but are not capable of comprehending molecular graph structures.To this end, SMILES-BERT [178] and MFBERT [175] are proposed for molecular graph classification with linearized SMILES strings.Since scientific natural language descriptions contain human expertise which can serve as a supplement for molecular graph structures, recent advances emphasize joint understanding of them [158], [174]: The linearized graph sequence is concatenated with the raw natural language data and then input into the LLMs.Specifically, KV-PLM [174] is built based on BERT [23] to understand the molecular structure in a biomedical context.CatBERTa [158], as developed from RoBERTa [24], specializes in the prediction of catalyst properties for molecular graphs.</p>
<p>Encoder-Decoder LLMs: Encoder-only LLMs may lack the capability for generation tasks.In this section, we discuss LLMs with encoder-decoder architectures.For example, Chemformer [155] uses a similar architecture as BART [28].The representation from the encoder can be used for property prediction tasks, and the whole encoder-decoder architecture can be optimized for molecule generation.Others focus on molecule captioning (which involves generating textual descriptions from a molecule) and text-based molecular generation (where a molecular graph structure is generated from a natural description).Specifically, MolT5 [122] is developed based on the T5 [29], suitable for these two tasks.It formulates molecule-text translation as a multilingual problem and initializes the model using the T5 checkpoint.The model was pre-trained on two monolingual corpora: the Colossal Clean Crawled Corpus (C4) [29] for the natural language modality and one million SMILES [155] for the molecule modality.Text+Chem T5 [170] extends the input and output domains to include both SMILES and texts, unlocking LLMs for more generation functions such as text or reaction generation.ChatMol [165] exploits the interactive capabilities of LLMs and proposes designing molecule structures through multi-turn dialogs with T5.</p>
<p>Decoder-only LLMs: Decoder-only architectures have been adopted for recent LLMs due to their advanced generation ability.MolGPT [176] and MolXPT [168] are GPT-style models used for molecule classification and generation.Specifically, MolGPT [176] focuses on conditional molecule generation tasks using scaffolds, while MolXPT [168] formulates the classification task as a question-answering problem with yes or no responses.RT [163] adopts XLNet [27] and focuses on molecular regression tasks.It frames the regression as a conditional sequence modeling problem.Galactica [177] is a set of LLMs with a maximum of 120 billion parameters, which is pretrained on two million compounds from PubChem [182].Therefore, Galactica could understand molecular graph structures through SMILES.With instruction tuning data and domain knowledge, researchers also adapt general-domain LLMs such as LLaMA to recognize molecular graph structures and solve molecule tasks [159].Recent studies also explore the in-context learning capabilities of LLMs on graphs.LLM-ICL [167] assesses the performance of LLMs across eight tasks in the molecular domain, ranging from property classification to molecule-text translation.MolReGPT [164] proposes a method to retrieve molecules with similar structures and descriptions to improve in-context learning.LLM4Mol [162] utilizes the summarization capability of LLMs as a feature extractor and combines it with a smaller, tunable LLM for specific prediction tasks.</p>
<p>2) Graph-Empowered LLMs: Different from the methods that adopt the original LLM architecture (i.e., Transformers) and input the graphs as sequences to LLMs, graph-empowered LLMs attempt to design LLM architectures that can conduct joint encoding of text and graph structures.Some works modify the positional encoding of Transformers.For instance, GIM-LET [46] treats nodes in a graph as tokens.It uses one Transformer to manage both the graph structure and text sequence
[v 1 , v 2 , . . . , v |V| , s |V|+1 , . . . , s |V|+|d G | ],
where v ∈ V is a node and s ∈ d G is a token in the text associated with G.This sequence cannot reflect graph structure.Therefore, a new position encoding (PE) is used to jointly encode graph structures and text sequences.It defines the relative distance between tokens i and j as follows:
PE(i, j) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ i − j if i, j ∈ d G , GSD(i, j) + Mean e k ∈SP(i,j) x e k if i, j ∈ V, −∞ if i ∈ V, j ∈ d G , 0 if i ∈ d G , j ∈ V. (15)
GSD is the graph shortest distance between two nodes, and Mean k∈SP(i,j) represents the mean pooling of the edge features x e k along the shortest path SP(i, j) between nodes i and j.</p>
<p>GIMLET [46] adapts bi-directional attention for node tokens and enables texts to selectively attend to nodes.These designs render the Transformer's submodule, which handles the graph part, equivalent to a Graph Transformer [140].Cross-attention is also used to interact representations between graphs and texts.Given the graph hidden state h G , its node-level hidden state H v and text hidden state H d G , Text2Mol [121] implemented interaction between representations in the hidden layers of encoders, while Prot2Text [160] implemented this interaction within the layers of between encoder and decoder
H d G = softmax( W Q H d G •(W K H v ) T √ d k ) • W V H v , where W Q , W K , W V
are trainable parameters that transform the query modality (e.g., sequences) and the key/value modality (e.g., graphs) into the attention space.Furthermore, Prot2Text [160] utilizes two trainable parameter matrices W 1 and W 2 to integrate the graph representation into the sequence representation
H d G = (H d G + 1 |d G | h G W 1 )W 2 .
3) Discussion: LLM Inputs with Sequence Prior: The first challenge is that the progress in advanced linearization methods has not progressed in tandem with the development of LLMs.Emerging around 2020, linearization methods for molecular graphs like SELFIES offer significant grammatical advantages, yet advanced LMs and LLMs from graph machine learning and language model communities might not fully utilize these, as these encoded results are not part of pretraining corpora prior to their proposal.Consequently, recent studies [167] indicate that LLMs, such as GPT-3.5/4,may be less adept at using SELFIES compared to SMILES.Therefore, the performance of LM-only and LLM-only methods may be limited by the expressiveness of older linearization methods, as there is no way to optimize these hard-coded rules during the learning pipeline of LLMs.However, the second challenge remains as the inductive bias of graphs may be broken by linearization.Rule-based linearization methods introduce inductive biases for sequence modeling, thereby breaking the permutation invariance assumption inherent in molecular graphs.It may reduce task difficulty by introducing sequence order to reduce the search space.However, it does not mean model generalization.Specifically, there could be multiple string-based representations for a single graph from single or different approaches.Numerous studies [151], [152], [153] have shown that training on different string-based views of the same molecule can improve the sequential model's performance, as these data augmentation approaches manage to retain the permutation-invariance nature of graphs.These advantages are also achievable with a permutation-invariant GNN, potentially simplifying the model by reducing the need for complex, string-based data augmentation design.</p>
<p>LLM Inputs With Graph Prior: Rule-based linearization may be considered less expressive and generalizable compared to the direct graph representation with rich node features, edge features, and the adjacency matrix [186].Various atomic features include atomic number, chirality, degree, formal charge, number of hydrogen atoms, number of radical electrons, hybridization state, aromaticity, and presence in a ring.Bond features encompass the bond's type (e.g., single, double, or triple), the bond's stereochemistry (e.g., E/Z or cis/trans), and whether the bond is conjugated [187].Each feature provides specific information about atomic properties and structure, crucial for molecular modeling and cheminformatics.One may directly vectorize the molecular graph structure into binary vectors [185] and then apply parameterized Multilayer Perceptrons (MLPs) on the top of these vectors to get the graph representation.These vectorization approaches are based on human-defined rules and vary, such as MACCS, ECFP, and CDK fingerprints [185].These rules take inputs of a molecule and output a vector consisting of 0/1 bits.Each bit denotes a specific type of substructure related to functional groups that could be used for various property predictions.Fingerprints consider atoms and structures, but they cannot automatically learn from the graph structure.GNNs could serve as automatic feature extractors to replace or enhance fingerprints.Some specific methods are explored in Section VI-A2, while the other graph prior such as the eigenvectors of a graph Laplacian and the random walk prior could also be used [141].</p>
<p>LLM Outputs for Prediction: LMs like KV-PLM [174], SMILES-BERT [178], MFBERT [175], and Chemformer [155] use a prediction head on the output vector of the last layer.These models are finetuned with standard classification and regression losses but may not fully utilize all the parameters and advantages of the complete architecture.In contrast, models like RT [163], MolXPT [168], and Text+Chem T5 [170] frame prediction as a text generation task.These models are trained with either masked language modeling or autoregressive targets, which requires a meticulous design of the context words in the text [163].Specifically, domain knowledge instructions may be necessary to activate the in-context learning ability of LLMs, thereby making them domain experts [167].For example, a possible template could be divided into four parts: {General Description}{Task-Specific Description}{Question-Answer Examples}{Test Question}.</p>
<p>LLM Outputs for Reasoning: Since string representations of molecular graphs usually carry new and in-depth domain knowledge, which is beyond the knowledge of LLMs, recent work [145], [156], [164] also attempts to utilize the reasoning ability of LLMs, instead of using them as a knowledge source for predicting the property of molecular graphs.ReLM [156] utilizes GNNs to suggest top-k candidates, which were then used to construct multiple-choice answers for in-context learning.ChemCrow [145] designs the LLMs as the chemical agent to implement various chemical tools.It avoided direct inference in an expertise-intensive domain.</p>
<p>B. LLM as Aligner</p>
<p>1) Latent Space Alignment: One may directly align the latent spaces of the GNN and LLM through contrastive learning and predictive regularization.Typically, a graph representation from a GNN can be read out by summarizing all node-level representations, and a sequence representation can be obtained from the [CLS] token.We first use two projection heads, which are usually MLPs, to map the separate representation vectors from the GNN and LLM into a unified space as h G and h d G , and then align them within this space.Specifically, MoMu [173] and MoMu-v2 [172] retrieve two sentences from the corpus for each molecular graph.During training, graph data augmentation was applied to molecular graphs, creating two augmented views.Consequently, there are four pairs of G and d G .For each pair, the contrastive loss for space alignment is as MoMu = − log
exp(cos(h G ,h d G )/τ ) dG =d G exp(cos(h G ,h dG )/τ )
where τ is the temperature hyper-parameter and dG denotes the sequence not paired to the graph G. MoleculeSTM [171] also applies contrastive learning to minimize the representation distance between a molecular graph G and its corresponding texts d G , while maximizing the distance between the molecule and unrelated descriptions.MoleculeSTM [171] randomly samples negative graphs or texts to construct negative pairs of (G, d) and ( G, d).Similarly, MolFM [161] and GIT-Mol [157] implement contrastive loss with mutual information and negative sampling.These two methods also use cross-entropy to regularize the unified space with the assumption that randomly permuted graph and text inputs are predictable if they originate from the same molecule.However, the aforementioned methods cannot leverage task labels.Given a classification label y, CLAMP [169] learns to map active molecules (y = 1) so that they align with the corresponding assay description for each molecular graph G:
CLAMP = y log(σ(τ −1 h T G h d G )) + (1 − y) log(1 −
Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>TABLE II DATA COLLECTION
IN SECTION V FOR TEXT-ATTRIBUTED GRAPHS σ(τ −1 h T G h d G ))
. CLAMP [169] requires labels to encourage that active molecules and their corresponding text descriptions are clustered together in the latent space.To advance the alignment between two modalities, MolCA [166] trains the Query Transformer (Q-Former) [189] for moleculetext projecting and contrastive alignment.Q-former initializes N q learnable query tokens {q k } N q k=1 .These query tokens are updated with self-attention and interact with the output of GNNs through cross-attention to obtain the k-th queried molecular representation vector (h G ) k := Q-Former(q k ).The query tokens share the same self-attention modules with the texts, but use different MLPs, allowing the Q-Former to be used for obtaining the representation of text sequence h d G := Q-Former([CLS]).Then we have MolCA = − g2t − t2g , where g2t = log
exp(max k cos((h G ) k ,h d G )/τ ) dG =d G exp(max k cos((h G ) k ,h dG )/τ ) , and t2g = log exp(max k cos(h d G ,(h G ) k )/τ ) G =G exp(max k cos(h d G ,(hG ) k )/τ ) .
2) Discussion: Larger-Scale GNNs: GNNs integrate atomic and graph structural features for molecular representation learning [144].Specifically, Text2Mol [121] utilizes the GCN [83] as its graph encoder and extracts unique identifiers for node features based on Morgan fingerprints [185].MoMu [173], MoMu-v2 [172], MolFM [161], GIT-Mol [157], and MolCA [166] prefer GIN [188] as the backbone, as GIN has been proven to be as expressive and powerful as the Weisfeiler-Lehman graph isomorphism test.As described in Section II-B, there has been notable progress in making GNNs deeper, more generalizable, and more powerful since the proposal of the GCN [83] in 2016 and the GIN [188] in 2018.However, most reviewed works [157], [161], [166], [172], [173] are developed using the GIN [188] as a proof of concept for their approaches.These pretrained GINs feature five layers and 300 hidden dimensions.The scale of GNNs may be a bottleneck in learning semantic meaningful representation and there is a risk of over-reliance on one modality, neglecting the other.Therefore, for future large-scale GNN designs comparable to LLMs, scaling up the dimension size and adding deeper layers, may be considered.</p>
<p>Besides, Transformer encoders [141] may also improve the expressive power of deep GNNs.</p>
<p>Generation Decoder with GNNs: GNNs are often not used as decoders for graph generation.The prevalent decoders are mostly text-based, generating linearized graph structures such as SMILES.These methods may be sensitive to the sequence order in the linearized graph.Generative diffusion models [201] on graphs could be utilized in future work to design generators with GNNs.</p>
<p>VII. RESOURCES AND APPLICATIONS</p>
<p>A. Datasets, Splitting and Evaluation</p>
<p>We summarize the datasets for three scenarios (namely pure graphs, text-attributed graphs, and text-paired graphs) and show them in Tables V, II, and III respectively.</p>
<p>1) Pure Graphs: In Table 5, we summarize the pure graph reasoning problems discussed in Section IV.Many problems are shared or revisited in different datasets due to their commonality.NLGraph [123], LLMtoGraph [124] and GUC [125] study a set of standard graph reasoning problems, including connectivity, shortest path, and graph diameter.GraphQA [130] benchmarks a similar set of problems but additionally describes the graphs in real-world scenarios to study the effect of graph grounding.LLM4DyG [127] focuses on reasoning tasks on temporally evolving graphs.Accuracy is the most common evaluation metric as they are primarily formulated as graph question-answering tasks.</p>
<p>2) Text-Attributed Graphs: We summarize the famous datasets for evaluating models on text-attributed graphs in Table II.The datasets are mostly from the academic, e-commerce, book, social media, and Wikipedia domains.The popular tasks to evaluate models on those datasets include node classification, link prediction, edge classification, regression, and recommendation.The evaluation metrics for node/edge classification include Accuracy, Macro-F1, and Micro-F1.For link prediction and recommendation evaluation, Mean 3) Text-Paired Graphs: Table III shows text-paired graph datasets (including text-available and graph-only datasets).For Data Splitting, options include random splitting, source-based splitting, activity cliffs and scaffolds [195], and data balancing [142].Graph classification usually adopts AUC [187] as the metrics, while regression uses MAE, RMSE, and R2 [144].For text generation evaluation, people tend to use the Bilingual Evaluation Understudy (BLEU) score; while for molecule generation evaluation, heuristic evaluation methods (based on factors including validity, novelty, and uniqueness) are adopted.However, it is worth noted that BLEU score is efficient but less accurate, while heuristic evaluation methods are problematic subject to unintended modes, such as the superfluous addition of carbon atoms in [196].</p>
<p>B. Open-Source Implementations</p>
<p>HuggingFace: HF Transformers1 is the most popular Python library for Transformers-based language models.Besides, it also provides two additional packages: Datasets 2 for easily accessing and sharing datasets and Evaluate3 for easily evaluating machine learning models and datasets.</p>
<p>Fairseq: Fairseq 4 is another open-source Python library for Transformers-based language models.PyTorch Geometric: PyG5 is an open-source Python library for graph machine learning.It packages more than 60 types of GNN, aggregation, and pooling layers.</p>
<p>Deep Graph Library: DGL 6 is another open-source Python library for graph machine learning.</p>
<p>RDKit: RDKit7 is one of the most popular open-source cheminformatics software programs that facilitates various operations and visualizations for molecular graphs.It offers many useful APIs, such as the linearization implementation for molecular graphs, to convert them into easily stored SMILES and to convert these SMILES back into graphs.</p>
<p>C. Practical Applications</p>
<p>1) Scientific Discovery: Virtual Screening: It aims to search a library of unlabeled molecules to identify useful structures for a given task.Machine learning models could automatically screen out trivial candidates to accelerate this process.However, training accurate models is not easy since labeled molecules are limited in size and imbalanced in distribution [142].There are many efforts to improve GNNs against data sparsity [142], [144], [191].However, it is difficult for a model to generalize and understand in-depth domain knowledge that it has never been trained on.Texts could be complementary knowledge sources.Discovering task-related content from massive scientific papers and using them as instructions has great potential to design accurate GNNs in virtual screening [46].</p>
<p>Molecular Generation: Molecular generation and optimization is one fundamental goal for drug and material discovery.Scientific hypotheses of molecules [198], can be represented in the joint space of GNNs and LLMs.Then, one may search in the latent space for a better hypothesis that aligns with the text description (human requirements) and adheres to structural constraints like chemical validity.Chemical space has been found to contain more than 10 60 molecules [197], which is beyond the capacity of exploration in wet lab experiments.Generating constrained candidates within relevant subspaces is a challenge [201] and promising, especially when incorporating textual conditions.</p>
<p>Synthesis Planning: Synthesis designs start from available molecules and involve planning a sequence of steps that can finally produce a desired chemical compound through a series of reactions [198].This procedure includes a sequence of reactant molecules and reaction conditions.Both graphs and texts play important roles in this process.For example, graphs may represent the fundamental structure of molecules, while texts may describe the reaction conditions, additives, and solvents.LLMs can assist in the planning by suggesting possible synthesis paths directly or by serving as agents to operate on existing planning tools [145].</p>
<p>2) Computational Social Science: In computational social science, researchers are interested in modeling the behavior of people/users and discovering new knowledge that can be utilized to forecast the future.The behaviors of users and interactions between users can be modeled as graphs, where the nodes are associated with rich text information (e.g., user profile, messages, emails).We will show two example scenarios below.</p>
<p>E-commerce: In E-commerce platforms, there are many interactions (e.g., purchase, view) between users and products.For example, users can view or purchase products.In addition, the users, products, and their interactions are associated with rich text information.For instance, products have titles/descriptions and users can leave a review of products.In this case, we can construct a graph [101] where nodes are users and products, while edges are their interactions.Both nodes and edges are associated with text.It is important to utilize both the text information and the graph structure information (user behavior) to model users and items and solve complex downstream tasks (e.g., item recommendation [105], bundle recommendation [106], and product understanding [107]).</p>
<p>Social Media: In social media platforms, there are many users and they interact with each other through messages, emails, and so on.In this case, we can build a graph where nodes are users and edges are the interaction between users.There will be text associated with nodes (e.g., user profile) and edges (e.g., messages).Interesting research questions will be how to do joint text and graph structure modeling to deeply understand the users for friend recommendation [108], user analysis [109], community detection [110], and personalized response generation [96], [97].</p>
<p>3) Specific Domains: In many specific domains, text data are interconnected and lie in the format of graphs.The structure information on the graphs can be utilized to better understand the text unit and contribute to advanced problem-solving.</p>
<p>Academic Domain: In the academic domain, graphs [12] are constructed with papers as nodes and their relations (e.g., citation, authorship, etc) as edges.The representation learned for papers on such graphs can be utilized for paper recommendation [102], paper classification [103], and author identification [104].</p>
<p>Legal Domain: In the legal domain, opinions given by the judges always contain references to opinions given for previous cases.In such scenarios, people can construct a graph [98] based on the citation relations between opinions.The representations learned on such a graph with both text and structure information can be utilized for clause classification [99] and opinion recommendation [100].</p>
<p>Education Domain: In the education domain, we can construct a graph with coursework as nodes and their relations as edges.The model learned on such a graph can be utilized for knowledge tracing [135] and student performance prediction [136].</p>
<p>VIII. FUTURE DIRECTIONS</p>
<p>Better Benchmark Datasets: Most pure graph benchmarks evaluate LLMs' reasoning ability on homogeneous graphs but do not include evaluations on heterogeneous or spatial-temporal graphs.For text-attributed graphs, as summarized in Table II, most benchmark datasets are from academic domains and ecommerce domains.However, in the real world, text-attributed graphs are ubiquitous across multiple domains (e.g., legal and health).More diverse datasets are needed to comprehensively evaluate LLMs on real-world scenarios.For text-paired graphs, as summarized in Table III, there is a lack of comprehensive datasets covering various machine learning tasks in chemistry.Although a massive number of scientific papers are available, preprocessing them into a ready-to-use format and pairing them with specific molecular graph data points of interest remains a cumbersome and challenging task.Besides, we could investigate graph-text pairs in 3D space, where each molecule may be associated with atomic coordinates [137].</p>
<p>Broader Task Space with LLMs: More comprehensive studies on the performance of LLMs for graph tasks hold promise for the future.While LLMs as encoder approaches have been explored for text-attributed graphs, their application to text-captioned molecular graphs remains underexplored.Promising directions include using LLMs for data augmentation and knowledge distillation to design domain-specific GNNs for various text-paired graph tasks.Furthermore, although graph generation has been approached in text-paired graphs, it remains an open problem for text-attributed graphs (i.e., how to conduct joint text and graph structure generation)</p>
<p>Efficienct LLMs on Graphs: While LLMs have shown a strong capability to learn on graphs, they suffer from inefficiency in graph linearization and model optimization.On one hand, as discussed in Sections V-A1 and VI-A1, many methods rely on transferring graphs into sequences that can be inputted into LLMs.However, the length of the transferred sequence will increase significantly as the size of the graph increases.This poses challenges since LLMs always have a maximum sequence input length and a long input sequence will lead to higher time and memory complexity.On the other hand, optimizing LLMs itself is computationally expensive.Although some general efficient tuning methods such as LoRA are proposed, there is a lack of discussion on graph-aware LLM efficient tuning methods.</p>
<p>Generalizable and Robust LLMs on Graphs: Another interesting direction is to explore the generalizability and robustness of LLMs on graphs.Generalizability refers to having the ability to transfer the knowledge learned from one domain graph to another; while robustness denotes having consistent prediction regarding obfuscations and attacks.Although LLMs have demonstrated their strong generalizability in processing text, they still suffer from robustness and hallucination issues, which are to be solved for graph data modeling as well.</p>
<p>Multi-Modal Foundation Models: One open question is, "Should we use one foundation model to unify different modalities, and how?"The modalities can include texts, graphs, and even images.For instance, molecules can be represented as graphs, described as texts, and photographed as images; products can be treated as nodes in a graph, associated with a title/description, and combined with an image.Designing a model that can conduct joint encoding for all modalities will be useful but challenging.Furthermore, there has always been tension between building a unified foundational model and customizing model architectures for different domains.It is thus intriguing to ask whether a unified architecture will suit different data types, or if tailoring model designs according to domains will be necessary.Correctly answering this question can save economic and intellectual resources from unnecessary attempts and also shed light on deeper understanding of graph-related tasks.</p>
<p>LLMs as Dynamic Agents on Graphs: Although LLMs have shown their advanced capability in generating text, one-pass generation of LLMs suffers from hallucination and misinformation issues due to the lack of accurate parametric knowledge.Simply augmenting retrieved knowledge in context is also bottlenecked by the capacity of the retriever.In many real-world scenarios, graphs such as academic networks, and Wikipedia are dynamically looked up by humans for knowledge-guided reasoning.Simulating such a role of dynamic agents can help LLMs more accurately retrieve relevant information via multihop reasoning, thereby correcting their answers and alleviating hallucinations.</p>
<p>IX. CONCLUSION</p>
<p>In this paper, we provide a comprehensive review of large language models on graphs.We first categorize graph scenarios where LMs can be adopted and summarize the large language models on graph techniques.We then provide a thorough review, analysis, and comparison of methods within each scenario.Furthermore, we summarize available datasets, open-source codebases, and multiple applications.Finally, we suggest future directions for large language models on graphs.</p>
<p>Fig. 2 .
2
Fig. 2. A taxonomy of LLM on graph scenarios and techniques with representative examples.</p>
<p>Fig. 3 .
3
Fig. 3.The illustration of various LLM as Predictor methods, including (a) Rule-based Graph As Sequence, (b) GNN-based Graph As Sequence, (c) Graph-Empowered LLMs.</p>
<p>Fig. 4 .
4
Fig. 4. The illustration of various techniques related to LLM as Encoder, including (a) One-step Training, (b) Two-step Training, (c) Data Augmentation, and (d) Knowledge Distillation.</p>
<p>Fig. 5 .
5
Fig. 5.The illustration of LLM as Aligner methods, including (a) LLM-GNN Prediction Alignment and (b) LLM-GNN Latent Space Alignment.</p>
<p>Alignment: This refers to training the LLM with the text data on a graph and training the GNN Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>Large Language Models on Graphs: A Comprehensive Survey Bowen Jin , Gang Liu , Graduate Student Member, IEEE, Chi Han , Meng Jiang , Heng Ji , Member, IEEE, and Jiawei Han , Fellow, IEEE</p>
<p>(Survey Paper)</p>
<p>TABLE III DATA
III
COLLECTION IN SECTION VI FOR TEXT-CAPTIONED GRAPHSReciprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG), and Hit Ratio (Hit) usually serve as metrics.While evaluating model performance on regression tasks, people tend to adopt mean absolute errors (MAE) or root mean square error (RMSE).</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
https://huggingface.co/docs/transformers/index
https://huggingface.co/docs/datasets/index
https://huggingface.co/docs/evaluate/index
https://github.com/facebookresearch/fairseq
https://pytorch-geometric.readthedocs.io/en/latest/index.html
https://www.dgl.ai/
https://www.rdkit.org/docs/ Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
ACKNOWLEDGMENTAny opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily represent the views, either expressed or implied, of DARPA or the U.S. Government.; date of current version 13 November 2024.This work was supported in part by US DARPA INCAS under Program HR0011-21-C0165, in part by BRIES under Program HR0011-24-3-0325, in part by National Science Foundation under Grant IIS-19-56151, in part by the Molecule Maker LabInstitute: An AI Research Institutes program supported by NSF under Award 2019897, in part by the Institute for Geospatial Understanding through an Integrative Discovery Environment (I-GUIDE) by NSF under Award 2118329, in part by U.S. DARPA ITM under Program FA8650-23-C-7316, in part by Agriculture and Food Research Initiative (AFRI) under Grant 2020-67021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture, in part by NSF under Award 2142827, Award 2146761, and Award 2234058, and in part by ONR under Grant N00014-22-1-2507.Recommended for acceptance by K. Zheng.Bowen Jin received the BS degree from Tsinghua University in 2021.He is currently working toward the PhD degree in computer science with the University of Illinois at Urbana-Champaign, advised by Prof. Jiawei Han.His research focuses on large language models, information networks, and data/text mining, with their applications in information retrieval and knowledge discovery.He has published first-authored papers in SIGIR, ICLR, ACL, and KDD.He receives the Apple PhD Fellowship in 2024.Gang Liu (Graduate
End-to-end open-domain question answering with bertserini. W Yang, Proc. Conf. North Amer. Conf. North Amer2019</p>
<p>Text summarization with pretrained encoders. Y Liu, M Lapata, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2019</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2018</p>
<p>Sentence-BERT: Sentence embeddings using Siamese BERT-networks. N Reimers, I Gurevych, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2019</p>
<p>Emergent abilities of large language models. J Wei, Trans. Mach. Learn. Res. 2022</p>
<p>Algorithmic Aspects of Graph Connectivity. H Nagamochi, T Ibaraki, 2018Cambridge Univ. PressCambridge, U.K.</p>
<p>Computing the shortest path: A search meets graph theory. A V Goldberg, C Harrelson, Proc. 16th Annu. ACM-SIAM Symp. Discrete Algorithms. 16th Annu. ACM-SIAM Symp. Discrete Algorithms2005</p>
<p>Efficient subgraph matching on billion node graphs. Z Sun, H Wang, H Wang, B Shao, J Li, arXiv:1205.66912012</p>
<p>Exploring the potential of large language models (LLMS) in learning on graphs. Z Chen, arXiv:2307.033932023</p>
<p>Automating the construction of internet portals with machine learning. A K Mccallum, K Nigam, J Rennie, K Seymore, Inf. Retrieval. 32000</p>
<p>CiteSeer: An automatic citation indexing system. C L Giles, K D Bollacker, S Lawrence, Proc. 3rd ACM Conf. Digit. Libraries. 3rd ACM Conf. Digit. Libraries1998</p>
<p>Microsoft academic graph: When experts are not enough. K Wang, Z Shen, C Huang, C H Wu, Y Dong, A Kanakia, Quantitative Sci. Stud. 112020</p>
<p>The effect of metadata on scientific literature tagging: A cross-field cross-model study. Y Zhang, B Jin, Q Zhu, Y Meng, J Han, Proc. Int. Conf. World Wide Web. Int. Conf. World Wide Web2023</p>
<p>Item recommendation on monotonic behavior chains. M Wan, J Mcauley, RecSys. 2018</p>
<p>Justifying recommendations using distantlylabeled reviews and fine-grained aspects. J Ni, J Li, J Mcauley, Proc. Conf. Empir. Methods Natural Lang. Process. 9th Int. Joint Conf. Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess. 9th Int. Joint Conf. Natural Lang. ess2019</p>
<p>Collective classification in network data. P Sen, G Namata, M Bilgic, L Getoor, B Galligher, AI Mag. 2932008</p>
<p>KEPLER: A unified model for knowledge embedding and pre-trained language representation. X Wang, Trans. Assoc. Comput. Linguistics. 92021</p>
<p>Neural-answering logical queries on knowledge graphs. L Liu, B Du, H Ji, C Zhai, H Tong, Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining2021</p>
<p>A comprehensive survey on graph neural networks. Z Wu, S Pan, F Chen, G Long, C Zhang, S P Yu, IEEE Trans. Neural Netw. Learn. Syst. 321Jan. 2021</p>
<p>Towards graph foundation models: A survey and beyond. J Liu, arXiv:2310.118292023</p>
<p>Unifying large language models and knowledge graphs: A roadmap. S Pan, L Luo, Y Wang, C Chen, J Wang, X Wu, arXiv:2306.083022023</p>
<p>Codet5 : Open code large language models for code understanding and generation. Y Wang, H Le, A D Gotmare, N D Bui, J Li, S C Hoi, arXiv:2305.079222023</p>
<p>BERT: Pre-Training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, Proc. Conf. North Amer. Conf. North Amer2019</p>
<p>RoBERTa: A robustly optimized bert pretraining approach. Y Liu, arXiv:1907.116922019</p>
<p>SciBERT: A pretrained language model for scientific text. I Beltagy, K Lo, A Cohan, arXiv:1903.106762019</p>
<p>Language models are few-shot learners. T Brown, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2020</p>
<p>XLNet: Generalized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J Carbonell, R R Salakhutdinov, Q V Le, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2019</p>
<p>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Proc. Conf. Assoc. Comput. Linguistics. Conf. Assoc. Comput. Linguistics2020</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, 140:1-140:67J. Mach. Learn. Res. 212020</p>
<p>LinkBERT: Pretraining language models with document links. M Yasunaga, J Leskovec, P Liang, Proc. Conf. Assoc. Comput. Linguistics. Conf. Assoc. Comput. Linguistics2022</p>
<p>Patton: Language model pretraining on text-rich networks. B Jin, Proc. Conf. Assoc. Comput. Linguistics, 2023. Conf. Assoc. Comput. Linguistics, 2023</p>
<p>Pretraining language models with text-attributed heterogeneous graphs. T Zou, L Yu, Y Huang, L Sun, B Du, arXiv:2310.125802023</p>
<p>MPNet: Masked and permuted pre-training for language understanding. K Song, X Tan, T Qin, J Lu, Proc. Int. Conf. Neural Inf. Int. Conf. Neural Inf2020</p>
<p>SimTeG: A frustratingly simple approach improves textual graph learning. K Duan, arXiv:2308.025652023</p>
<p>ChatGPT for good? On opportunities and challenges of large language models for education. E Kasneci, Learn. Individual Differences. 1032023. 102274</p>
<p>The power of scale for parameter-efficient prompt tuning. B Lester, R Al-Rfou, N Constant, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2021</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. X L Li, P Liang, Proc. Conf. Assoc. Comput. Linguistics, 2021. Conf. Assoc. Comput. Linguistics, 2021</p>
<p>Parameter-efficient transfer learning for NLP. N Houlsby, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. Learn2019</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2022</p>
<p>Graph neural prompting with large language models. Y Tian, arXiv:2309.154272023</p>
<p>GraphLLM: Boosting graph reasoning ability of large language model. Z Chai, arXiv:2310.058452023</p>
<p>Finetuned language models are zero-shot learners. J Wei, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2022</p>
<p>Multitask prompted training enables zero-shot task generalization. V Sanh, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2022</p>
<p>GraphGPT: Graph instruction tuning for large language models. J Tang, arXiv:2310.130232023</p>
<p>Natural language is all a graph needs. R Ye, C Zhang, R Wang, S Xu, Y Zhang, arXiv:2308.071342023</p>
<p>GIMLET: A unified graph-text model for instructionbased molecule zero-shot learning. H Zhao, arXiv:2306.130892023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, arXiv:2305.106012023</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, A Kubicek, R Gerstenberger, L Gianinazzi, J Gajda, arXiv:2308.096872023</p>
<p>Specter: Document-level representation learning using citation-informed transformers. A Cohan, S Feldman, I Beltagy, D Downey, D S Weld, Proc. Conf. Assoc. Comput. Linguistics. Conf. Assoc. Comput. Linguistics2020</p>
<p>Neighborhood contrastive learning for scientific document representations with citation embeddings. M Ostendorff, N Rethmeier, I Augenstein, B Gipp, G Rehm, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2022</p>
<p>ConGraT: Self-supervised contrastive pretraining for joint graph and text embeddings. W Brannon, arXiv:2305.143212023</p>
<p>TouchUp-G: Improving feature representation through graph-centric finetuning. J Zhu, X Song, V N Ioannidis, D Koutra, C Faloutsos, arXiv:2309.138852023</p>
<p>GRENADE: Graph-centric language model for self-supervised representation learning on text-attributed graphs. Y Li, K Ding, K Lee, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2023</p>
<p>TwHIN-BERT: A socially-enriched pre-trained language model for multilingual tweet representations at twitter. X Zhang, Y Malkov, O Florez, S Park, B Mcwilliams, J Han, Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining2023</p>
<p>Minimallysupervised structure-rich text categorization via learning on text-rich networks. X Zhang, C Zhang, X L Dong, J Shang, J Han, Proc. Int. Conf. World Wide Web. Int. Conf. World Wide Web2021</p>
<p>Node feature extraction by self-supervised multi-scale neighborhood prediction. E Chien, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2022</p>
<p>Metadata-induced contrastive learning for zero-shot multi-label text classification. Y Zhang, Proc. Int. Conf. World Wide Web. Int. Conf. World Wide Web2022</p>
<p>E2EG: End-to-end node classification using graph topology and text-based node attributes. T A Dinh, J D Boef, J Cornelisse, P Groth, arXiv:2208.046092022</p>
<p>WalkLM: A uniform language model fine-tuning framework for attributed graph embedding. Y Tan, Z Zhou, H Lv, W Liu, C Yang, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2023</p>
<p>Learning on large-scale text-attributed graphs via variational inference. J Zhao, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2023</p>
<p>Augmenting low-resource text classification with graph-grounded pre-training and prompting. Z Wen, Y Fang, Proc. Int. ACM SIGIR Conf. Res. Develop. Inf. Retrieval. Int. ACM SIGIR Conf. Res. Develop. Inf. Retrieval2023</p>
<p>Label-free node classification on graphs with large language models (LLMS). Z Chen, arXiv:2310.046682023</p>
<p>GraphText: Graph reasoning in text space. J Zhao, arXiv:2310.010892023</p>
<p>GNN-LM: Language modeling based on global contexts via GNN. Y Meng, S Zong, X Li, X Sun, T Zhang, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2022</p>
<p>GreaseLM: Graph reasoning enhanced language models for question answering. X Zhang, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2022</p>
<p>Efficient and effective training of language and graph neural network models. V N Ioannidis, Proc. AAAI Conf. AAAI Conf2023</p>
<p>Train your own GNN teacher: Graph-aware distillation on textual graphs. C Mavromatis, Proc. null2023</p>
<p>Explanations as features: LLM-based features for text-attributed graphs. X He, X Bresson, T Laurent, B Hooi, arXiv:2305.195232023</p>
<p>Empower text-attributed graphs learning with large language models (LLMs). J Yu, Y Ren, C Gong, J Tan, X Li, X Zhang, arXiv:2310.098722023</p>
<p>GraphFormers: GNN-nested transformers for representation learning on textual graph. J Yang, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2021</p>
<p>Heterformer: Transformer-based deep node representation learning on heterogeneous text-rich networks. B Jin, Y Zhang, Q Zhu, J Han, Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining2023</p>
<p>Edgeformers: Graph-empowered transformers for representation learning on textual-edge networks. B Jin, Y Zhang, Y Meng, J Han, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2023</p>
<p>Learning multiplex embeddings on text-rich networks with one text encoder. B Jin, W Zhang, Y Zhang, Y Meng, H Zhao, J Han, arXiv:2310.066842023</p>
<p>Disentangled representation learning with large language models for text-attributed graphs. Y Qin, X Wang, Z Zhang, W Zhu, arXiv:2310.181522023</p>
<p>TextGNN: Improving text encoder via graph neural network in sponsored search. J Zhu, Proc. Int. Conf. World Wide Web. Int. Conf. World Wide Web2021</p>
<p>Adsgnn: Behavior-graph augmented relevance modeling in sponsored search. C Li, Proc. Int. ACM SIGIR Conf. Res. Develop. Inf. Retrieval, 2021. Int. ACM SIGIR Conf. Res. Develop. Inf. Retrieval, 2021</p>
<p>Fast multiresolution transformer fine-tuning for extreme multi-label text classification. J Zhang, W C Chang, H F Yu, I Dhillon, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2021</p>
<p>Graph-aware language model pre-training on a large graph corpus can help multiple graph applications. H Xie, Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining2023</p>
<p>Deep bidirectional language-knowledge graph pretraining. M Yasunaga, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2022</p>
<p>Can LLMs effectively leverage graph structural information: When and why. J Huang, X Zhang, Q Mei, J Ma, arXiv:2309.165952023</p>
<p>Adversarial robustness for large language NER models using disentanglement and word attributions. X Jin, B Vinzamuri, S Venkatapathy, H Ji, P Natarajan, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2023</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2017</p>
<p>Inductive representation learning on large graphs. W Hamilton, Z Ying, J Leskovec, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2017</p>
<p>Graph attention networks. P Veličković, G Cucurull, A Casanova, A Romero, P Lio, Y Bengio, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2018</p>
<p>Graph-less neural networks: Teaching old MLPs new tricks via distillation. S Zhang, Y Liu, Y Sun, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2022</p>
<p>Towards deeper graph neural networks. M Liu, H Gao, S Ji, Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining2020</p>
<p>Generating training data with language models: Towards zero-shot language understanding. Y Meng, J Huang, Y Zhang, J Han, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2022</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. </p>
<p>PathSim: Meta path-based top-k similarity search in heterogeneous information networks. Y Sun, J Han, X Yan, P S Yu, T Wu, Proc. VLDB Endowment. VLDB Endowment20114</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2023</p>
<p>Unsupervised attributed multiplex network embedding. C Park, D Kim, J Han, H Yu, Proc. AAAI Conf. AAAI Conf2020</p>
<p>Attention is all you need. A Vaswani, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2017</p>
<p>Topic-sensitive pagerank. T H Haveliwala, Proc. Int. Conf. World Wide Web. Int. Conf. World Wide Web2002</p>
<p>Representation learning with contrastive predictive coding. A V D Oord, Y Li, O Vinyals, arXiv:1807.037482018</p>
<p>Learning transferable visual models from natural language supervision. A Radford, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. Learn2021</p>
<p>Decoding the silent majority: Inducing belief augmented social graph with large language model for response forecasting. C Sun, arXiv:2310.132972023</p>
<p>Measuring the effect of influential messages on varying personas. C Sun, J Li, H P Chan, C Zhai, H Ji, Proc. Conf. Assoc. Comput. Linguistics. Conf. Assoc. Comput. Linguistics2023</p>
<p>Legal networks: The promises and challenges of legal network analysis. R Whalen, 2016ArtMichigan State Law Rev</p>
<p>Situation entity types: Automatic classification of clause-level aspect. A Friedrich, A Palmer, M Pinkal, Proc. Conf. Assoc. Comput. Linguistics. Conf. Assoc. Comput. Linguistics2016</p>
<p>LegalBench: A collaboratively built benchmark for measuring legal reasoning in large language models. N Guha, arXiv:2308.114622023</p>
<p>Personalized entity resolution with dynamic heterogeneous knowledge graph representations. Y Lin, arXiv:2104.026672021</p>
<p>Scientific paper recommendation: A survey. X Bai, M Wang, I Lee, Z Yang, X Kong, F Xia, IEEE Access. 72019</p>
<p>Research paper classification using supervised machine learning techniques. S Chowdhury, M P Schoen, Proc. Int. Eng. Technol. Comput. 2020</p>
<p>Author identification on the large scale. D Madigan, A Genkin, D D Lewis, S Argamon, D Fradkin, L Ye, Proc. Meeting. MeetingClassification Soc. North Amer2005</p>
<p>LightGCN: Simplifying and powering graph convolution network for recommendation. X He, K Deng, X Wang, Y Li, Y Zhang, M Wang, Proc. 43rd Int. 43rd Int2020</p>
<p>Bundle recommendation with graph convolutional networks. J Chang, C Gao, X He, D Jin, Y Li, Proc. Int. Int2020</p>
<p>Open-world learning and application to product classification. H Xu, B Liu, L Shu, P Yu, Proc. Proc. Int. Conf. World Wide Web. Int. Conf. World Wide Web2019</p>
<p>Friend recommendation based on multi-social graph convolutional network. L Chen, Y Xie, Z Zheng, H Zheng, J Xie, IEEE Access. 82020</p>
<p>Unsupervised clickstream clustering for user behavior analysis. G Wang, X Zhang, S Tang, H Zheng, Proc. CHI Conf. CHI Conf2016</p>
<p>Overlapping community detection with graph neural networks. O Shchur, S Günnemann, arXiv:1909.122012019</p>
<p>Large language models are zero-shotreasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Adv. Neural Inf. Process. Syst. 352022</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Proc. null202235</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2022</p>
<p>Language models are unsupervised multitask learners. A Radford, OpenAI Blog. 182019</p>
<p>ALBERT: A lite bert for self-supervised learning of language representations. Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2019</p>
<p>ELECTRA: Pretraining text encoders as discriminators rather than generators. K Clark, M T Luong, Q V Le, C D Manning, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2019</p>
<p>Sparks of artificial general intelligence: Early experiments with GPT-4. S Bubeck, arXiv:2303.127122023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, arXiv:2307.092882023</p>
<p>A Q Jiang, arXiv:2310.06825Mistral 7B. 2023</p>
<p>Flamingo: A visual language model for fewshot learning. J B Alayrac, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2022</p>
<p>Text2Mol: Cross-modal molecule retrieval with natural language queries. C Edwards, C Zhai, H Ji, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2022</p>
<p>Translation between molecules and natural language. C Edwards, T Lai, K Ros, G Honke, H Ji, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2022</p>
<p>Can language models solve graph problems in natural language?. H Wang, S Feng, T He, Z Tan, X Han, Y Tsvetkov, arXiv:2305.100372023</p>
<p>Evaluating large language models on graphs: Performance insights and comparative analysis. C Liu, B Wu, arXiv:2308.112242023. 2023</p>
<p>GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. J Guo, L Du, H Liu, arXiv:2305.150662023</p>
<p>Graph-ToolFormer: To empower LLMs with graph reasoning ability via prompt augmented by ChatGPT. J Zhang, arXiv:2304.111162023</p>
<p>LLM4DyG: Can large language models solve problems on dynamic graphs?. Z Zhang, arXiv:2310.171102023</p>
<p>Reasoning on graphs: Faithful and interpretable large language model reasoning. L Luo, Y F Li, G Haffari, S Pan, arXiv:2310.010612023</p>
<p>StructGPT: A general framework for large language model to reason over structured data. J Jiang, K Zhou, Z Dong, K Ye, W X Zhao, J R Wen, arXiv:2305.096452023</p>
<p>Talk like a graph: Encoding graphs for large language models. B Fatemi, J Halcrow, B Perozzi, arXiv:2310.045602023</p>
<p>Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. J Sun, arXiv:2307.076972023</p>
<p>Developing algorithms and software for geometric path planning problems. D Z Chen, 10.1145/242224.242246ACM Comput. Surv. 28181996</p>
<p>Airline scheduling with max flow algorithm. A Iqbal, M Hossain, A Ebna, Proc. Int. Joint Conf. Int. Joint Conf2018</p>
<p>Scheduling the covering delivery problem in last mile delivery. L Jiang, X Zang, I I Alghoul, X Fang, J Dong, C Liang, Expert Syst. Appl. 1872022Art. no. 115894</p>
<p>Graph-based knowledge tracing: Modeling student proficiency using graph neural network. H Nakagawa, Y Iwasawa, Y Matsuo, Proc. IEEE/WIC/ACM Int. Conf. Web Intell. IEEE/WIC/ACM Int. Conf. Web Intell2019</p>
<p>Peer-inspired student performance prediction in interactive online question pools with graph neural network. H Li, H Wei, Y Wang, Y Song, H Qu, Proc. 29th ACM Int. Conf. Inf. Knowl. Manage. 29th ACM Int. Conf. Inf. Knowl. Manage2020</p>
<p>Artificial intelligence for science in quantum, atomistic, and continuum systems. X Zhang, arXiv:2307.084232023</p>
<p>A survey on oversmoothing in graph neural networks. T K Rusch, M M Bronstein, S Mishra, arXiv:2303.109932023</p>
<p>Understanding over-squashing and bottlenecks on graphs via curvature. J Topping, F D Giovanni, B P Chamberlain, X Dong, M M Bronstein, arXiv:2111.145222021</p>
<p>Do transformers really perform badly for graph representation?. C Ying, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2021</p>
<p>Recipe for a general, powerful, scalable graph transformer. L Rampášek, M Galkin, V P Dwivedi, A T Luu, G Wolf, D Beaini, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2022</p>
<p>Semi-supervised graph imbalanced regression. G Liu, T Zhao, E Inae, T Luo, M Jiang, arXiv:2305.120872023</p>
<p>Nodeformer: A scalable graph structure learning transformer for node classification. Q Wu, W Zhao, Z Li, D P Wipf, J Yan, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2022</p>
<p>Graph rationalization with environment-based augmentations. G Liu, T Zhao, J Xu, T Luo, M Jiang, ACM Trans. Knowl. Discov. Data. 1842022</p>
<p>Chem-Crow: Augmenting large-language models with chemistry tools. A M Bran, S Cox, A D White, P Schwaller, arXiv:2304.053762023</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. </p>
<p>IAM graph database repository for graph based pattern recognition and machine learning. K Riesen, H Bunke, Proc. Struct., Syntactic, Statist. Pattern Recognit. Joint IAPR Workshop. Struct., Syntactic, Statist. Pattern Recognit. Joint IAPR Workshop2008</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, J. Chem. Inf. Comput. Sci. 2811988</p>
<p>InChI-the worldwide chemical structure identifier standard. S Heller, A Mcnaught, S Stein, D Tchekhovskoi, I Pletnev, J. Cheminformatics. 512013</p>
<p>DeepSMILES: An adaptation of SMILES for use in machine-learning of chemical structures. N O'boyle, A Dalke, 2018</p>
<p>Selfreferencing embedded strings (SELFIES): A 100% robust molecular string representation. M Krenn, F Häse, A Nigam, P Friederich, A Aspuru-Guzik, Mach. Learn.: Sci. Technol. 142020</p>
<p>SMILES enumeration as data augmentation for neural network modeling of molecules. E J Bjerrum, arXiv:1703.070762017</p>
<p>Randomized SMILES strings improve the quality of molecular generative models. J Arús-Pous, J. Cheminformatics. 1112019</p>
<p>Augmentation is what you need!. I V Tetko, P Karpov, E Bruno, T B Kimber, G Godin, Proc. Int. Conf. Artif. Neural Netw. Int. Conf. Artif. Neural NetwSpringer International Publishing2019</p>
<p>SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. T Kudo, J Richardson, Proc. Conf. Empir. Methods Natural Lang. Conf. Empir. Methods Natural Lang2018</p>
<p>Chemformer: A pre-trained transformer for computational chemistry. R Irwin, S Dimitriadis, J He, E J Bjerrum, Mach. Learn. Sci. Technol. 312022Art. no. 015022</p>
<p>ReLM: Leveraging language models for enhanced chemical reaction prediction. Y Shi, A Zhang, E Zhang, Z Liu, X Wang, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2023</p>
<p>GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text. P Liu, Y Ren, Z Ren, arXiv:2308.069112023</p>
<p>Catalyst property prediction with CatBERTa: Unveiling feature exploration strategies through large language models. J Ock, C Guntuboina, A B Farimani, arXiv:2309.005632023</p>
<p>Mol-instructions: A large-scale biomolecular instruction dataset for large language models. Y Fang, arXiv:2306.080182023</p>
<p>Prot2Text: Multimodal protein's function generation with GNNs and transformers. H Abdine, M Chatzianastasis, C Bouyioukos, M Vazirgiannis, arXiv:2307.143672023</p>
<p>MolFM: A multimodal molecular foundation model. Y Luo, K Yang, M Hong, X Liu, Z Nie, arXiv:2307.094842023</p>
<p>Can large language models empower molecular property prediction?. C Qian, H Tang, Z Yang, H Liang, Y Liu, arXiv:2307.074432023</p>
<p>Regression transformer enables concurrent sequence regression and generation for molecular language modelling. J Born, M Manica, Nature Mach. Intell. 542023</p>
<p>Empowering molecule discovery for molecule-caption translation with large language models: A ChatGPT perspective. J Li, arXiv:2306.066152023</p>
<p>Interactive molecular discovery with natural language. Z Zeng, B Yin, S Wang, J Liu, C Yang, H Yao, arXiv:2306.119762023</p>
<p>MolCA: Molecular graph-language modeling with crossmodal projector and uni-modal adapter. Z Liu, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. ess2023</p>
<p>What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. T Guo, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2023</p>
<p>MolXPT: Wrapping molecules with text for generative pre-training. Z Liu, W Zhang, Y Xia, L Wu, S Xie, T Qin, Proc. Conf. Assoc. Comput. Linguistics. Conf. Assoc. Comput. Linguistics2023</p>
<p>Enhancing activity prediction models in drug discovery with the ability to understand human language. P Seidl, A Vall, S Hochreiter, G Klambauer, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. Learn2023</p>
<p>Unifying molecular and textual representations via multitask language modelling. D Christofidellis, G Giannone, J Born, O Winther, T Laino, M Manica, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. Learn2023</p>
<p>A multi-modal molecule structure-text model for textbased retrieval and editing. S Liu, Nature Mach. Intell. 52023</p>
<p>Extracting molecular properties from natural language with multimodal contrastive learning. R Lacombe, A Gaut, J He, D Lüdeke, K Pistunova, Proc. Int. Conf. Mach. Learn. Workshop Comput. Biol. 2023</p>
<p>A molecular multimodal foundation model associating molecule graphs with natural language. B Su, arXiv:2209.054812022</p>
<p>A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Z Zeng, Y Yao, Z Liu, M Sun, Nature Commun. 138622022</p>
<p>Functional output regression for machine learning in materials science. M Iwayama, S Wu, C Liu, R Yoshida, J. Chem. Inf. Model. 62202022</p>
<p>MolGPT: Molecular generation using a transformer-decoder model. V Bagal, R Aggarwal, P K Vinod, U D Priyakumar, J. Chem. Inf. Model. 6292021</p>
<p>Galactica: A large language model for science. R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, arXiv:2211.090852022</p>
<p>Smiles-BERT: Large scale unsupervised pre-training for molecular property prediction. S Wang, Y Guo, Y Wang, H Sun, J Huang, Proc. ACM Int. Conf. Bioinf. Comput. Biol. Health Inform. 2019</p>
<p>BioBERT: A pre-trained biomedical language representation model for biomedical text mining. J Lee, Bioinformatics. 3642020</p>
<p>PI1M: A benchmark database for polymer informatics. R Ma, T Luo, J. Chem. Inf. Model. 602020</p>
<p>ChEBI in 2016: Improved services and an expanding collection of metabolites. J Hastings, Nucleic acids Res. 442016</p>
<p>PubChem 2019 update: Improved access to chemical data. S Kim, Nucleic Acids Res. 47D12019</p>
<p>ChEMBL: A large-scale bioactivity database for drug discovery. A Gaulton, Nucleic Acids Res. 402012</p>
<p>The ChEMBL database in 2023: A drug discovery platform spanning multiple bioactivity data types and time periods. B Zdrazil, Nucleic Acids Res. 532024</p>
<p>Molecular fingerprint-derived similarity measures for toxicological read-across: Recommendations for optimal use. C L Mellor, Regulatory Toxicol. Pharmacol. 2019</p>
<p>SELFIES and the future of molecular string representations. M Krenn, Patterns. 3102022. 100588</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. W Hu, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2020</p>
<p>How powerful are graph neural networks?. K Xu, W Hu, J Leskovec, S Jegelka, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2019</p>
<p>BLIP-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, arXiv:2301.12597</p>
<p>Moflow: An invertible flow model for generating molecular graphs. C Zang, F Wang, Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining2020</p>
<p>Datacentric learning from unlabeled graphs with diffusion model. G Liu, E Inae, T Zhao, J Xu, T Luo, M Jiang, arXiv:2303.101082023</p>
<p>Knowledge graph prompting for multi-document question answering. Y Wang, N Lipka, R A Rossi, A Siu, R Zhang, T Derr, Proc. Conf. Assoc. Advance. Conf. Assoc. Advance2024</p>
<p>GraSeq: Graph and sequence fusion learning for molecular property prediction. Z Guo, W Yu, C Zhang, M Jiang, N V Chawla, Proc. Int. Conf. Inf. Knowl. Manage. Int. Conf. Inf. Knowl. Manage2020</p>
<p>Diversifying content generation for commonsense reasoning with mixture of knowledge graph experts. W Yu, C Zhu, L Qin, Z Zhang, T Zhao, M Jiang, Proc. Conf. Assoc. Comput. Linguistics Findings. Conf. Assoc. Comput. Linguistics Findings2022</p>
<p>A systematic study of key elements underlying molecular property prediction. J Deng, Z Yang, H Wang, I Ojima, D Samaras, F Wang, Nature Commun. 1412023</p>
<p>On failure modes in molecule generation and optimization. P Renz, D Van Rompaey, J K Wegner, S Hochreiter, G Klambauer, Drug Discov. Today, Technol. 322019</p>
<p>The chemical space project. J L Reymond, Accounts Chem. Res. 4832015</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, Nature. 62079722023</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. </p>
<p>Chemical-reaction-aware molecule representation learning. H Wang, arXiv:2109.098882021</p>
<p>KEBLM: Knowledge-enhanced biomedical language models. T M Lai, C Zhai, H Ji, J. Biomed. Informat. 1432023. 104392</p>
<p>Inverse molecular design with multi-conditional diffusion guidance. G Liu, J Xu, T Luo, M Jiang, arXiv:2401.138582024</p>
<p>The future is not one-dimensional: Complex event schema induction by graph modeling for event prediction. M Li, arXiv:2104.063442021</p>
<p>Advancing graph representation learning with large language models: A comprehensive survey of techniques. Q Mao, Z Liu, C Liu, Z Li, J Sun, arXiv:2402.059522024</p>
<p>A survey of graph meets large language model: Progress and future directions. Y Li, arXiv:2311.123992023</p>
<p>Policy shaping: Integrating human feedback with reinforcement learning. S Griffith, K Subramanian, J Scholz, C L Isbell, A L Thomaz, Proc. Int. Conf. Neural Inf. Int. Conf. Neural Inf2013</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.063472017</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, E Mitchell, C D Manning, S Ermon, C Finn, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2024</p>
<p>A survey of large language models. W X Zhao, arXiv:2303.182232023</p>
<p>LM-infinite: Simple on-the-fly length generalization for large language models. C Han, Q Wang, W Xiong, Y Chen, H Ji, S Wang, arXiv:2308.161372023</p>
<p>Roformer: Enhanced transformer with rotary position embedding. J Su, M Ahmed, Y Lu, S Pan, W Bo, Y Liu, Neurocomputing. 5682024. 127063</p>            </div>
        </div>

    </div>
</body>
</html>
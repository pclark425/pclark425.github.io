<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5811 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5811</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5811</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-12a763cb52f650710900790ca0bc43e5d5b88be6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/12a763cb52f650710900790ca0bc43e5d5b88be6" target="_blank">Generated Knowledge Prompting for Commonsense Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Generated knowledge prompting develops generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question, and improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks.</p>
                <p><strong>Paper Abstract:</strong> It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning.Our code is available at github.com/liujch1998/GKP</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5811.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5811.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GKP (few-shot generated-knowledge prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generated Knowledge Prompting (few-shot demonstration elicitation + knowledge-augmented prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-step method that (1) elicits question-related natural-language knowledge statements from a large LM via few-shot demonstrations, and (2) integrates those statements by concatenating each with the question and selecting the answer supported with highest confidence by an inference model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (generator, few-shot) + T5-11b / UnifiedQA / Unicorn (inference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (GPT-3) + 11B (T5-11b / UnifiedQA) / Unicorn-ft (size not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NumerSense, CommonsenseQA (CSQA), CommonsenseQA 2.0 (CSQA2), QASC</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice commonsense reasoning benchmarks: numerical commonsense (NumerSense), general commonsense (CSQA, CSQA2), and grade-school scientific reasoning (QASC).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Knowledge generation prompt: instruction + 5 human-written question→knowledge demonstrations (task-specific few-shot), then sample M continuations k_m from GPT-3 (nucleus sampling p=0.5, cap 64 tokens except CSQA2: 128 tokens). Knowledge integration: form augmented inputs q_m = [k_m || q] (concatenate knowledge and question); compute p_I(a|q_m) for each candidate answer a using the inference model; aggregate by taking max_m p_I(a|q_m) and choose argmax over answers. Typical M = 20 (saturation point).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot T5-11b: NumerSense accuracy improved from 64.05% → 72.47%; CSQA from 39.89% → 47.26%; QASC from 44.89% → 55.00%. Finetuned: CSQA2 (Unicorn-ft) 70.2% → 73.03%; QASC (UnifiedQA-ft) 76.74% → 80.33%. (Metrics: accuracy as reported in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+8.42 pp (NumerSense), +7.37 pp (CSQA), +10.11 pp (QASC) on zero-shot T5-11b; +2.83 pp (CSQA2, Unicorn-ft) ; +3.59 pp (QASC, UnifiedQA-ft).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors attribute gains to three factors: (1) quality of knowledge statements (task-focused, turning questions into explicit reasoning steps); (2) quantity (more diverse helpful knowledge statements up to a saturation M improves performance); (3) the integration strategy (selecting the single most-supportive knowledge statement via max). The method leverages existing LMs as flexible external knowledge sources without requiring a structured KB or joint finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generated Knowledge Prompting for Commonsense Reasoning', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5811.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5811.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template-based self-talk</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-talk (template-prompted clarifications)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A template-based method that elicits clarifications/knowledge via hand-designed templates from a LM and appends them to the inference input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised commonsense question answering with self-talk</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (template-prompted generator) + T5-11b (inference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (GPT-3) + 11B (T5-11b)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CommonsenseQA (CSQA) - dev set comparison</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>5-way multiple-choice commonsense QA (CSQA).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Generate knowledge by filling task-specific templates (self-talk) and concatenate generated clarifications to the question for inference (0-shot inference).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against demonstration-prompted generated knowledge (GKP).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>CSQA (T5-11b zero-shot dev): template-based self-talk improvement reported as 45.37% (table entry) versus GKP 47.26% (T5-11b with demonstration-prompted knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GKP (demonstration-prompted) 47.26% vs self-talk 45.37% on CSQA dev (absolute difference reported: +1.89 pp in favor of GKP).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-1.89 pp (self-talk vs demonstration-prompted GKP on CSQA dev)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (self-talk was less effective than few-shot demonstration prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors argue template-based prompts constrain the variety and utility of elicited knowledge and require careful hand-crafting, making them less flexible and slightly less effective than few-shot demonstration prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generated Knowledge Prompting for Commonsense Reasoning', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5811.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5811.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random & Context sentence baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random sentence generation and context continuation baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines that either sample random sentences from a LM (unconditioned) or sample continuations of the question (context sentences) and append them to the question for inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 generator (random or context sampling) + T5-11b inference</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (GPT-3) + 11B (T5-11b)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NumerSense and other evaluated tasks (baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multiple-choice commonsense tasks as main experiments; these baselines evaluate non-question-conditioned or weakly-conditioned generated text as added context.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Random: sample unconditioned sentences from LM and concatenate with question. Context: sample continuations of the question itself (LM continuation) and concatenate.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to targeted, question-conditioned few-shot knowledge generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative and tabulated results show random sentences 'barely help and even hurt' inference performance; context sentences provide modest gains vs vanilla baseline (paper reports context > random > vanilla in some columns). Exact tabulated examples show modest absolute gains for context sentences while random sentences may reduce or not change performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>context gave small improvements; random often hurt or negligible</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Unconditioned/random generations tend to be irrelevant or noisy; continuations of the question are more relevant and can help somewhat, but neither approach matches the benefit of targeted question-conditioned knowledge elicited via few-shot demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generated Knowledge Prompting for Commonsense Reasoning', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5811.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5811.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-based knowledge (IR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-based knowledge augmentation (IR from Wikipedia/KB/Google snippets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augmenting questions by retrieving potentially relevant knowledge from external sources (Wikipedia, GenericsKB, web snippets, or gold supporting facts) and concatenating for inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>IR source (Wikipedia/GenericsKB/Google snippets) + T5-11b / Unicorn / UnifiedQA inference</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NumerSense, CSQA2, QASC (dataset-dependent retrieval setups)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multiple-choice commonsense tasks; retrieval supplies external sentences/facts instead of LM-generated statements.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Retrieve supporting sentences from a corpus or knowledge base and concatenate them with the question for inference (single retrieved facts or multiple).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with generated knowledge prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Retrieval-based knowledge is competitive: on NumerSense retrieved knowledge gave small improvements (~0.18% to 1.02% in test partitions) while GKP provided substantially larger gains; on CSQA2 web-retrieved knowledge sometimes outperforms GKP, and on QASC gold retrieved facts (dataset-provided) outperform generated knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>NumerSense: IR gave marginal gains (~0.18–1.02 pp) vs GKP much larger (+8.83 and +7.37 pp over IR on different splits). QASC: gold retrieved facts > GKP.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>variable (sometimes comparable or better if high-quality, in-domain retrieval exists; GKP better when no in-domain KB is available)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>High-quality, in-domain retrieved facts can be most useful; where such KBs/snippets don’t exist or are mismatched to the domain, LM-generated relevant knowledge can match or exceed retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generated Knowledge Prompting for Commonsense Reasoning', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5811.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5811.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Answers-as-knowledge baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using model-generated answers (few-shot) as augmentations instead of knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt GPT-3 to generate direct answers (rather than intermediate knowledge), then (a) evaluate few-shot GPT-3 inference directly, or (b) use generated answers as augmentations to SOTA inference models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 few-shot answers + SOTA inference models (T5-11b, UnifiedQA, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (GPT-3) + inference model sizes vary</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NumerSense, CSQA, QASC, CSQA2 (evaluated as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Direct answer generation baseline to compare against knowledge-mediated prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Replace knowledge statement in prompts with ground-truth answer or generate answers from GPT-3 (one or M=20) and (optionally) supply those as context to inference models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to generated intermediate knowledge statements used as context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Few-shot GPT-3 direct answers were poorly calibrated and underperformed GKP by ~14–20 percentage points across tasks; using GPT-3-generated answers to prompt SOTA inference models still performed worse than GKP on most tasks (one exception: CSQA with T5 inference). Example numbers: few-shot GPT-3 inference reported at 60.5% on NumerSense (table entry), lower than GKP-augmented inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Approx. -14 to -20 pp worse than GKP for direct few-shot GPT-3 answers across tasks (per paper's summary).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Answers lack the intermediate reasoning steps; knowledge statements can turn problems into explicit reasoning procedures that inference models can use more effectively than raw candidate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>One exception: on CSQA with T5 inference, generated answers used as prompts performed comparably in that single case.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generated Knowledge Prompting for Commonsense Reasoning', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5811.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5811.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Integration strategy: Max-selection vs MoE vs PoE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge integration methods (max aggregation, Mixture-of-Experts, Product-of-Experts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three aggregation strategies for combining scores from multiple knowledge-augmented inputs: choose max support (authors' method), average-sum (Mixture-of-Experts), or multiplicative combination (Product-of-Experts).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-11b (inference) with generated knowledge variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B (T5-11b)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QASC (dev)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>8-way multiple-choice grade-school science QA where two background facts compose the answer.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Given K_q = {k_m}, form q_m = [k_m || q]; compute p_I(a|q_m) for each m; aggregate via (a) max_m p_I(a|q_m) (authors' method), (b) sum_m p_I(a|q_m) (MoE), (c) product_m p_I(a|q_m) (PoE).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Max-selection (authors) vs MoE vs PoE</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>QASC-dev (T5-11b): Max-selection = 58.32% ; MoE = 56.26% ; PoE = 55.94% (absolute accuracies reported in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Max-selection outperforms MoE by 2.06 pp and PoE by 2.38 pp on QASC-dev with T5-11b.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+2.06 pp (vs MoE), +2.38 pp (vs PoE) in QASC-dev</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>max-selection improved over MoE and PoE</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Max-selection favors the single knowledge statement that most strongly supports one choice, avoiding dilution from noisy or conflicting knowledge present in other statements; MoE (sum) or PoE (product) can be harmed by many noisy knowledge statements.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generated Knowledge Prompting for Commonsense Reasoning', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5811.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5811.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quantity of knowledge (M)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of number of generated knowledge statements per question (M)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation on how many generated knowledge statements are produced and used (M), showing performance increases with M up to a saturation point and then declines when too many noisy statements are added.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (generator) + T5-11b (inference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (GPT-3) + 11B (T5-11b)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QASC (analysis) and general statement across tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice commonsense/science QA; ablation studies examine M.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Generate M knowledge statements per question (sampling continuations), then integrate via max-selection across q_m. M varied in experiments (plotted in Figure 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Performance generally increases with M and saturates at M = 20; beyond M=20 performance begins to decline (authors report M=20 as chosen value). Specific plotted example: QASC dev with T5-11b shows performance increasing to saturation at 20 then decreasing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>increased up to M=20, then decreased with larger M</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>More candidate knowledge increases coverage and the chance of producing a highly-supportive helpful statement, but overly large M introduces more noisy, irrelevant or harmful statements that can lower net performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generated Knowledge Prompting for Commonsense Reasoning', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5811.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5811.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generator size effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of knowledge-generation model size (GPT-3 family ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments varying the size of the GPT-3 knowledge generator (0.4B, 1.3B, 6.7B, 175B) show improvements scale with generator size: very small models do not help, mid-sized help moderately, largest helps most.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (various sizes as knowledge-generation model) + T5-11b inference</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.4B, 1.3B, 6.7B, 175B (GPT-3 variants) + 11B (T5-11b)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NumerSense (dev) analysis</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Numerical commonsense masked number prediction; ablation of generator size shown in Figure 4.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Use different-sized GPT-3 variants to generate K_q (same prompts/demos); integrate via T5-11b inference as in main method.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On NumerSense dev with T5-11b inference: 6.7B generator gave a +5.0 pp improvement; 175B generator gave a +10.5 pp improvement; 1.3B and 0.4B generators did not give significant improvement (authors' reported summary).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+5.0 pp (6.7B) ; +10.5 pp (175B) ; 1.3B/0.4B: not significant</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>larger generator size → larger improvement (up to 175B tested)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Larger generative models produce higher-quality and more factual, relevant knowledge statements; relatively large generator is needed to elicit reliable knowledge, though the absolute largest is not strictly necessary (mid-large models also help).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generated Knowledge Prompting for Commonsense Reasoning', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5811.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5811.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inference model size effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of inference model size (amplification and lightweight models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Examines how the size of the inference model modulates the magnitude of improvement from added generated knowledge: smaller inference models benefit proportionally more, and knowledge can 'amplify' capabilities even for large models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 family (small → large → 11B) and GPT-3 (inference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5-small/large/3B/11B (various); GPT-3 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NumerSense (dev) analysis</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Numerical commonsense test: measure absolute improvement when applying generated-knowledge prompting to inference models of different sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Keep same generated knowledge; vary inference model size; evaluate improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report that gains increase as inference model size decreases: smallest T5 with GKP matches T5-3b baseline; T5-large with GKP outperforms GPT-3 baseline; GPT-3 inference benefits by ~9.0% from knowledge elicited from itself. (Exact per-size numbers plotted in Figure 3.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Example: GPT-3 inference saw a +9.0 pp improvement from knowledge elicited from itself; smaller T5 models saw larger relative gains making them comparable to larger baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved more for smaller inference models (amplification effect)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Smaller inference models have more to gain from externally supplied, articulated knowledge; generated knowledge can 'amplify' knowledge already present in a model, benefiting even large inference models.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generated Knowledge Prompting for Commonsense Reasoning', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unsupervised commonsense question answering with self-talk <em>(Rating: 2)</em></li>
                <li>Prompting contrastive explanations for commonsense reasoning tasks <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 2)</em></li>
                <li>Explain yourself! leveraging language models for commonsense reasoning <em>(Rating: 1)</em></li>
                <li>DynaGen <em>(Rating: 1)</em></li>
                <li>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5811",
    "paper_id": "paper-12a763cb52f650710900790ca0bc43e5d5b88be6",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "GKP (few-shot generated-knowledge prompting)",
            "name_full": "Generated Knowledge Prompting (few-shot demonstration elicitation + knowledge-augmented prompting)",
            "brief_description": "A two-step method that (1) elicits question-related natural-language knowledge statements from a large LM via few-shot demonstrations, and (2) integrates those statements by concatenating each with the question and selecting the answer supported with highest confidence by an inference model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (generator, few-shot) + T5-11b / UnifiedQA / Unicorn (inference)",
            "model_size": "175B (GPT-3) + 11B (T5-11b / UnifiedQA) / Unicorn-ft (size not specified)",
            "task_name": "NumerSense, CommonsenseQA (CSQA), CommonsenseQA 2.0 (CSQA2), QASC",
            "task_description": "Multiple-choice commonsense reasoning benchmarks: numerical commonsense (NumerSense), general commonsense (CSQA, CSQA2), and grade-school scientific reasoning (QASC).",
            "problem_format": "Knowledge generation prompt: instruction + 5 human-written question→knowledge demonstrations (task-specific few-shot), then sample M continuations k_m from GPT-3 (nucleus sampling p=0.5, cap 64 tokens except CSQA2: 128 tokens). Knowledge integration: form augmented inputs q_m = [k_m || q] (concatenate knowledge and question); compute p_I(a|q_m) for each candidate answer a using the inference model; aggregate by taking max_m p_I(a|q_m) and choose argmax over answers. Typical M = 20 (saturation point).",
            "comparison_format": null,
            "performance": "Zero-shot T5-11b: NumerSense accuracy improved from 64.05% → 72.47%; CSQA from 39.89% → 47.26%; QASC from 44.89% → 55.00%. Finetuned: CSQA2 (Unicorn-ft) 70.2% → 73.03%; QASC (UnifiedQA-ft) 76.74% → 80.33%. (Metrics: accuracy as reported in paper.)",
            "performance_comparison": null,
            "format_effect_size": "+8.42 pp (NumerSense), +7.37 pp (CSQA), +10.11 pp (QASC) on zero-shot T5-11b; +2.83 pp (CSQA2, Unicorn-ft) ; +3.59 pp (QASC, UnifiedQA-ft).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Authors attribute gains to three factors: (1) quality of knowledge statements (task-focused, turning questions into explicit reasoning steps); (2) quantity (more diverse helpful knowledge statements up to a saturation M improves performance); (3) the integration strategy (selecting the single most-supportive knowledge statement via max). The method leverages existing LMs as flexible external knowledge sources without requiring a structured KB or joint finetuning.",
            "counterexample_or_null_result": null,
            "uuid": "e5811.0",
            "source_info": {
                "paper_title": "Generated Knowledge Prompting for Commonsense Reasoning",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Template-based self-talk",
            "name_full": "Self-talk (template-prompted clarifications)",
            "brief_description": "A template-based method that elicits clarifications/knowledge via hand-designed templates from a LM and appends them to the inference input.",
            "citation_title": "Unsupervised commonsense question answering with self-talk",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (template-prompted generator) + T5-11b (inference)",
            "model_size": "175B (GPT-3) + 11B (T5-11b)",
            "task_name": "CommonsenseQA (CSQA) - dev set comparison",
            "task_description": "5-way multiple-choice commonsense QA (CSQA).",
            "problem_format": "Generate knowledge by filling task-specific templates (self-talk) and concatenate generated clarifications to the question for inference (0-shot inference).",
            "comparison_format": "Compared against demonstration-prompted generated knowledge (GKP).",
            "performance": "CSQA (T5-11b zero-shot dev): template-based self-talk improvement reported as 45.37% (table entry) versus GKP 47.26% (T5-11b with demonstration-prompted knowledge).",
            "performance_comparison": "GKP (demonstration-prompted) 47.26% vs self-talk 45.37% on CSQA dev (absolute difference reported: +1.89 pp in favor of GKP).",
            "format_effect_size": "-1.89 pp (self-talk vs demonstration-prompted GKP on CSQA dev)",
            "format_effect_direction": "reduced (self-talk was less effective than few-shot demonstration prompting)",
            "explanation_or_hypothesis": "Authors argue template-based prompts constrain the variety and utility of elicited knowledge and require careful hand-crafting, making them less flexible and slightly less effective than few-shot demonstration prompting.",
            "counterexample_or_null_result": null,
            "uuid": "e5811.1",
            "source_info": {
                "paper_title": "Generated Knowledge Prompting for Commonsense Reasoning",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Random & Context sentence baselines",
            "name_full": "Random sentence generation and context continuation baselines",
            "brief_description": "Baselines that either sample random sentences from a LM (unconditioned) or sample continuations of the question (context sentences) and append them to the question for inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 generator (random or context sampling) + T5-11b inference",
            "model_size": "175B (GPT-3) + 11B (T5-11b)",
            "task_name": "NumerSense and other evaluated tasks (baseline comparisons)",
            "task_description": "Same multiple-choice commonsense tasks as main experiments; these baselines evaluate non-question-conditioned or weakly-conditioned generated text as added context.",
            "problem_format": "Random: sample unconditioned sentences from LM and concatenate with question. Context: sample continuations of the question itself (LM continuation) and concatenate.",
            "comparison_format": "Compared to targeted, question-conditioned few-shot knowledge generation.",
            "performance": "Qualitative and tabulated results show random sentences 'barely help and even hurt' inference performance; context sentences provide modest gains vs vanilla baseline (paper reports context &gt; random &gt; vanilla in some columns). Exact tabulated examples show modest absolute gains for context sentences while random sentences may reduce or not change performance.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "context gave small improvements; random often hurt or negligible",
            "explanation_or_hypothesis": "Unconditioned/random generations tend to be irrelevant or noisy; continuations of the question are more relevant and can help somewhat, but neither approach matches the benefit of targeted question-conditioned knowledge elicited via few-shot demonstrations.",
            "counterexample_or_null_result": null,
            "uuid": "e5811.2",
            "source_info": {
                "paper_title": "Generated Knowledge Prompting for Commonsense Reasoning",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Retrieval-based knowledge (IR)",
            "name_full": "Retrieval-based knowledge augmentation (IR from Wikipedia/KB/Google snippets)",
            "brief_description": "Augmenting questions by retrieving potentially relevant knowledge from external sources (Wikipedia, GenericsKB, web snippets, or gold supporting facts) and concatenating for inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "IR source (Wikipedia/GenericsKB/Google snippets) + T5-11b / Unicorn / UnifiedQA inference",
            "model_size": null,
            "task_name": "NumerSense, CSQA2, QASC (dataset-dependent retrieval setups)",
            "task_description": "Same multiple-choice commonsense tasks; retrieval supplies external sentences/facts instead of LM-generated statements.",
            "problem_format": "Retrieve supporting sentences from a corpus or knowledge base and concatenate them with the question for inference (single retrieved facts or multiple).",
            "comparison_format": "Compared with generated knowledge prompting.",
            "performance": "Retrieval-based knowledge is competitive: on NumerSense retrieved knowledge gave small improvements (~0.18% to 1.02% in test partitions) while GKP provided substantially larger gains; on CSQA2 web-retrieved knowledge sometimes outperforms GKP, and on QASC gold retrieved facts (dataset-provided) outperform generated knowledge.",
            "performance_comparison": "NumerSense: IR gave marginal gains (~0.18–1.02 pp) vs GKP much larger (+8.83 and +7.37 pp over IR on different splits). QASC: gold retrieved facts &gt; GKP.",
            "format_effect_size": null,
            "format_effect_direction": "variable (sometimes comparable or better if high-quality, in-domain retrieval exists; GKP better when no in-domain KB is available)",
            "explanation_or_hypothesis": "High-quality, in-domain retrieved facts can be most useful; where such KBs/snippets don’t exist or are mismatched to the domain, LM-generated relevant knowledge can match or exceed retrieval.",
            "counterexample_or_null_result": null,
            "uuid": "e5811.3",
            "source_info": {
                "paper_title": "Generated Knowledge Prompting for Commonsense Reasoning",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Answers-as-knowledge baseline",
            "name_full": "Using model-generated answers (few-shot) as augmentations instead of knowledge",
            "brief_description": "Prompt GPT-3 to generate direct answers (rather than intermediate knowledge), then (a) evaluate few-shot GPT-3 inference directly, or (b) use generated answers as augmentations to SOTA inference models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 few-shot answers + SOTA inference models (T5-11b, UnifiedQA, etc.)",
            "model_size": "175B (GPT-3) + inference model sizes vary",
            "task_name": "NumerSense, CSQA, QASC, CSQA2 (evaluated as baseline)",
            "task_description": "Direct answer generation baseline to compare against knowledge-mediated prompting.",
            "problem_format": "Replace knowledge statement in prompts with ground-truth answer or generate answers from GPT-3 (one or M=20) and (optionally) supply those as context to inference models.",
            "comparison_format": "Compared to generated intermediate knowledge statements used as context.",
            "performance": "Few-shot GPT-3 direct answers were poorly calibrated and underperformed GKP by ~14–20 percentage points across tasks; using GPT-3-generated answers to prompt SOTA inference models still performed worse than GKP on most tasks (one exception: CSQA with T5 inference). Example numbers: few-shot GPT-3 inference reported at 60.5% on NumerSense (table entry), lower than GKP-augmented inference.",
            "performance_comparison": null,
            "format_effect_size": "Approx. -14 to -20 pp worse than GKP for direct few-shot GPT-3 answers across tasks (per paper's summary).",
            "format_effect_direction": "reduced",
            "explanation_or_hypothesis": "Answers lack the intermediate reasoning steps; knowledge statements can turn problems into explicit reasoning procedures that inference models can use more effectively than raw candidate answers.",
            "counterexample_or_null_result": "One exception: on CSQA with T5 inference, generated answers used as prompts performed comparably in that single case.",
            "uuid": "e5811.4",
            "source_info": {
                "paper_title": "Generated Knowledge Prompting for Commonsense Reasoning",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Integration strategy: Max-selection vs MoE vs PoE",
            "name_full": "Knowledge integration methods (max aggregation, Mixture-of-Experts, Product-of-Experts)",
            "brief_description": "Three aggregation strategies for combining scores from multiple knowledge-augmented inputs: choose max support (authors' method), average-sum (Mixture-of-Experts), or multiplicative combination (Product-of-Experts).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-11b (inference) with generated knowledge variants",
            "model_size": "11B (T5-11b)",
            "task_name": "QASC (dev)",
            "task_description": "8-way multiple-choice grade-school science QA where two background facts compose the answer.",
            "problem_format": "Given K_q = {k_m}, form q_m = [k_m || q]; compute p_I(a|q_m) for each m; aggregate via (a) max_m p_I(a|q_m) (authors' method), (b) sum_m p_I(a|q_m) (MoE), (c) product_m p_I(a|q_m) (PoE).",
            "comparison_format": "Max-selection (authors) vs MoE vs PoE",
            "performance": "QASC-dev (T5-11b): Max-selection = 58.32% ; MoE = 56.26% ; PoE = 55.94% (absolute accuracies reported in Table 4).",
            "performance_comparison": "Max-selection outperforms MoE by 2.06 pp and PoE by 2.38 pp on QASC-dev with T5-11b.",
            "format_effect_size": "+2.06 pp (vs MoE), +2.38 pp (vs PoE) in QASC-dev",
            "format_effect_direction": "max-selection improved over MoE and PoE",
            "explanation_or_hypothesis": "Max-selection favors the single knowledge statement that most strongly supports one choice, avoiding dilution from noisy or conflicting knowledge present in other statements; MoE (sum) or PoE (product) can be harmed by many noisy knowledge statements.",
            "counterexample_or_null_result": null,
            "uuid": "e5811.5",
            "source_info": {
                "paper_title": "Generated Knowledge Prompting for Commonsense Reasoning",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Quantity of knowledge (M)",
            "name_full": "Effect of number of generated knowledge statements per question (M)",
            "brief_description": "Ablation on how many generated knowledge statements are produced and used (M), showing performance increases with M up to a saturation point and then declines when too many noisy statements are added.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (generator) + T5-11b (inference)",
            "model_size": "175B (GPT-3) + 11B (T5-11b)",
            "task_name": "QASC (analysis) and general statement across tasks",
            "task_description": "Multiple-choice commonsense/science QA; ablation studies examine M.",
            "problem_format": "Generate M knowledge statements per question (sampling continuations), then integrate via max-selection across q_m. M varied in experiments (plotted in Figure 2).",
            "comparison_format": null,
            "performance": "Performance generally increases with M and saturates at M = 20; beyond M=20 performance begins to decline (authors report M=20 as chosen value). Specific plotted example: QASC dev with T5-11b shows performance increasing to saturation at 20 then decreasing.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "increased up to M=20, then decreased with larger M",
            "explanation_or_hypothesis": "More candidate knowledge increases coverage and the chance of producing a highly-supportive helpful statement, but overly large M introduces more noisy, irrelevant or harmful statements that can lower net performance.",
            "counterexample_or_null_result": null,
            "uuid": "e5811.6",
            "source_info": {
                "paper_title": "Generated Knowledge Prompting for Commonsense Reasoning",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Generator size effect",
            "name_full": "Effect of knowledge-generation model size (GPT-3 family ablation)",
            "brief_description": "Experiments varying the size of the GPT-3 knowledge generator (0.4B, 1.3B, 6.7B, 175B) show improvements scale with generator size: very small models do not help, mid-sized help moderately, largest helps most.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (various sizes as knowledge-generation model) + T5-11b inference",
            "model_size": "0.4B, 1.3B, 6.7B, 175B (GPT-3 variants) + 11B (T5-11b)",
            "task_name": "NumerSense (dev) analysis",
            "task_description": "Numerical commonsense masked number prediction; ablation of generator size shown in Figure 4.",
            "problem_format": "Use different-sized GPT-3 variants to generate K_q (same prompts/demos); integrate via T5-11b inference as in main method.",
            "comparison_format": null,
            "performance": "On NumerSense dev with T5-11b inference: 6.7B generator gave a +5.0 pp improvement; 175B generator gave a +10.5 pp improvement; 1.3B and 0.4B generators did not give significant improvement (authors' reported summary).",
            "performance_comparison": null,
            "format_effect_size": "+5.0 pp (6.7B) ; +10.5 pp (175B) ; 1.3B/0.4B: not significant",
            "format_effect_direction": "larger generator size → larger improvement (up to 175B tested)",
            "explanation_or_hypothesis": "Larger generative models produce higher-quality and more factual, relevant knowledge statements; relatively large generator is needed to elicit reliable knowledge, though the absolute largest is not strictly necessary (mid-large models also help).",
            "counterexample_or_null_result": null,
            "uuid": "e5811.7",
            "source_info": {
                "paper_title": "Generated Knowledge Prompting for Commonsense Reasoning",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Inference model size effect",
            "name_full": "Effect of inference model size (amplification and lightweight models)",
            "brief_description": "Examines how the size of the inference model modulates the magnitude of improvement from added generated knowledge: smaller inference models benefit proportionally more, and knowledge can 'amplify' capabilities even for large models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 family (small → large → 11B) and GPT-3 (inference)",
            "model_size": "T5-small/large/3B/11B (various); GPT-3 175B",
            "task_name": "NumerSense (dev) analysis",
            "task_description": "Numerical commonsense test: measure absolute improvement when applying generated-knowledge prompting to inference models of different sizes.",
            "problem_format": "Keep same generated knowledge; vary inference model size; evaluate improvement.",
            "comparison_format": null,
            "performance": "Authors report that gains increase as inference model size decreases: smallest T5 with GKP matches T5-3b baseline; T5-large with GKP outperforms GPT-3 baseline; GPT-3 inference benefits by ~9.0% from knowledge elicited from itself. (Exact per-size numbers plotted in Figure 3.)",
            "performance_comparison": null,
            "format_effect_size": "Example: GPT-3 inference saw a +9.0 pp improvement from knowledge elicited from itself; smaller T5 models saw larger relative gains making them comparable to larger baselines.",
            "format_effect_direction": "improved more for smaller inference models (amplification effect)",
            "explanation_or_hypothesis": "Smaller inference models have more to gain from externally supplied, articulated knowledge; generated knowledge can 'amplify' knowledge already present in a model, benefiting even large inference models.",
            "counterexample_or_null_result": null,
            "uuid": "e5811.8",
            "source_info": {
                "paper_title": "Generated Knowledge Prompting for Commonsense Reasoning",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unsupervised commonsense question answering with self-talk",
            "rating": 2
        },
        {
            "paper_title": "Prompting contrastive explanations for commonsense reasoning tasks",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 2
        },
        {
            "paper_title": "Explain yourself! leveraging language models for commonsense reasoning",
            "rating": 1
        },
        {
            "paper_title": "DynaGen",
            "rating": 1
        },
        {
            "paper_title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "rating": 1
        }
    ],
    "cost": 0.017857249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Generated Knowledge Prompting for Commonsense Reasoning</h1>
<p>Jiacheng Liu ${ }^{\circ}$ Alisa Liu ${ }^{\circ}$ Ximing Lu ${ }^{\circ}$ Sean Welleck ${ }^{\circ}$ Peter West ${ }^{\circ}$ Ronan Le Bras ${ }^{\text {® }}$ Yejin Choi ${ }^{\circ}$ Hannaneh Hajishirzi ${ }^{\circ}$<br>${ }^{\circ}$ Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington<br>${ }^{\text {® }}$ Allen Institute for Artificial Intelligence<br>liujc@cs.washington.edu</p>
<h4>Abstract</h4>
<p>It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Generated knowledge prompting highlights largescale language models as flexible sources of external knowledge for improving commonsense reasoning. Our code is available at github.com/liujch1998/GKP</p>
<h2>1 Introduction</h2>
<p>It remains an open research question whether external knowledge is needed for commonsense reasoning. On one hand, a substantial body of prior work has reported that integrating external knowledge can help improve task performance (Mitra et al., 2019; Bian et al., 2021, inter alia), especially if the knowledge is high quality (e.g. hand-crafted by experts). On the other hand, recent leaderboards are often dominated by large-scale pretrained models that are fine-tuned on a target benchmark (Khashabi et al., 2020; Lourie et al., 2021), suggesting that the benefits of external knowledge may wash away as the underlying models increase in size and are pretrained on ever larger amounts of raw text.</p>
<p>Even if external knowledge is found to be effective on a particular task, flexibility remains a fundamental hurdle to integrating external knowl-
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Generated knowledge prompting involves (i) using few-shot demonstrations to generate questionrelated knowledge statements from a language model; (ii) using a second language model to make predictions with each knowledge statement, then selecting the highest-confidence prediction.
edge, as many benchmarks currently lack appropriate knowledge bases with sufficient coverage. Furthermore, prior methods often require task-specific, custom supervision for knowledge integration (Mitra et al., 2019; Chang et al., 2020), introducing a burden for rapidly adapting new pretrained models to a wide variety of tasks.</p>
<p>In this paper, we investigate whether external knowledge can be helpful for commonsense reasoning, even on top of the largest state-of-the-art pretrained models (e.g. T5-11b (Raffel et al., 2019) and its variants), with a focus on four recent commonsense benchmarks. To facilitate easier adaptation with any zero-shot or finetuned models, we propose an approach that does not require access to a structured knowledge base or joint finetuning for knowledge integration.</p>
<p>The key insight behind our method, Generated Knowledge Prompting (sketched in Figure 1), is that we can generate useful knowledge from a language model, then provide the knowledge as an input prompt that is concatenated with a question. To</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Question / Knowledge</th>
<th style="text-align: center;">Prediction</th>
<th style="text-align: center;">Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NumerSense</td>
<td style="text-align: center;">the word children means [M] or more kids. <br> The word child means one kid.</td>
<td style="text-align: center;">one <br> two</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.3710 .35 \ &amp; 0.91 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">CSQA</td>
<td style="text-align: center;">She was always helping at the senior center, it brought her what? People who help others are usually happier.</td>
<td style="text-align: center;">feel better <br> happiness</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9710 .02 \ &amp; 0.98 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">CSQA2</td>
<td style="text-align: center;">Part of golf is trying to get a higher point total than others. The player with the lowest score wins.</td>
<td style="text-align: center;">yes <br> no</td>
<td style="text-align: center;">$\begin{aligned} &amp; 1.0010 .00 \ &amp; 1.00 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">QASC</td>
<td style="text-align: center;">Sponges eat primarily <br> Sponges eat bacteria and other tiny organisms.</td>
<td style="text-align: center;">cartilage <br> krill and plankton</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9510 .00 \ &amp; 0.99 \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples where prompting with generated knowledge rectifies model prediction. Each section shows the correct answer in green, the incorrect answer in red, and the prediction scores from the inference model that only sees the question (top) and the same model that sees the question prompted with the given knowledge (bottom).
support a variety of settings without finetuning, the quality and flexibility of knowledge is crucial. We propose a simple, yet effective, method that elicits knowledge statements (i.e. knowledge expressed as natural language statements) from generic language models in a few-shot setting. Compared to prior work that elicits knowledge via clarification questions (Shwartz et al., 2020) or contrastive explanations (Paranjape et al., 2021), our approach can generate knowledge flexibly, beyond the scope of pre-defined templates (Table 1).</p>
<p>Experiments show that our method improves both zero-shot and finetuned models on numerical commonsense (NumerSense (Lin et al., 2020)), general commonsense (CommonsenseQA (Talmor et al., 2019), CommonsenseQA 2.0 (Talmor et al., 2021)), and scientific commonsense (QASC (Khot et al., 2020)) benchmarks, setting a new state-of-the-art on three of these datasets. It outperforms the template-based knowledge generation method self-talk (Shwartz et al., 2020), while performing comparably to retrieval-based systems.</p>
<p>We find three factors contribute to the performance of generated knowledge prompting: (i) the quality of knowledge, (ii) the quantity of knowledge where the performance improves with more knowledge statements, and (iii) the strategy for integrating knowledge during inference. Our qualitative analysis suggests that the generated knowledge statements cover a variety of types, and can transform commonsense question answering to explicit reasoning procedures, e.g. deduction, that are supported by off-the-shelf and finetuned language models.</p>
<h2>2 Generated Knowledge Prompting</h2>
<p>A multiple-choice commonsense reasoning task involves predicting an answer $a \in A_{q}$ given a ques-
tion $q \in Q$, where the set of choices $A_{q}$ is finite and can vary by question, and both questions and answers are variable-length text sequences. Our method answers commonsense questions in two steps.</p>
<p>The first step is knowledge generation, where we use a language model $p_{G}(k \mid q)$ to generate knowledge statements conditioned on the question:</p>
<p>$$
K_{q}=\left{k_{m}: k_{m} \sim p_{G}(k \mid q), m=1 \ldots M\right}
$$</p>
<p>where each knowledge statement $k_{m}$ is a variablelength text sequence. Intuitively, each statement contains information that is helpful for answering the question (e.g. Table 1).</p>
<p>The second step is knowledge integration, where generated knowledge is integrated into the decision process of a language model used for inference,</p>
<p>$$
\hat{a}=\underset{a \in A_{q}}{\arg \max } p_{I}\left(a \mid q, K_{q}\right)
$$</p>
<p>In contrast, the vanilla setting of using the inference model without knowledge is represented by $\hat{a}=\arg \max <em q="q">{a \in A</em>(a \mid q)$.}} p_{I</p>
<p>Next, we describe the knowledge generation and integration steps in detail.</p>
<h3>2.1 Knowledge Generation</h3>
<p>We generate question-related knowledge statements by prompting a language model. The prompt consists of an instruction, a few demonstrations that are fixed for each task, and a new-question placeholder. The demonstrations are human-written, and each consists of a question in the style of the task and a knowledge statement that is helpful for answering this question. For a given task, we write five demonstrations using the format in Table 2.</p>
<p>We write questions (or select them from the training set, when available) that are representative of</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">NumerSense</th>
<th style="text-align: center;">QASC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prompt</td>
<td style="text-align: center;">Generate some numerical facts about objects. Examples:</td>
<td style="text-align: center;">Generate some knowledge about the input. Examples:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Input: penguins have <mask> wings.</td>
<td style="text-align: center;">Input: What type of water formation is formed by clouds?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Knowledge: Birds have two wings. Penguin is a kind of bird.</td>
<td style="text-align: center;">Knowledge: Clouds are made of water vapor.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">...</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Input: a typical human being has <mask> limbs.</td>
<td style="text-align: center;">Input: The process by which genes are passed is</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Knowledge: Human has two arms and two legs.</td>
<td style="text-align: center;">Knowledge: Genes are passed from parent to offspring.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Input: {question}</td>
<td style="text-align: center;">Input: {question}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Knowledge:</td>
<td style="text-align: center;">Knowledge:</td>
</tr>
</tbody>
</table>
<p>Table 2: Prompts for knowledge generation for two of our tasks, NumerSense and QASC. The prompt consists of an instruction, five demonstrations of question-knowledge pairs, and a new question placeholder. For full prompts on all the tasks we evaluate on, see Appendix A.2.
challenges posed by the task (e.g. numerical commonsense, scientific commonsense). We pair each question with a knowledge statement that turns the commonsense problem posed by the question into an explicit reasoning procedure, without directly answering the question. For example, the knowledge statement Birds have two wings. Penguin is a kind of bird. is helpful for the question Penguins have <mask> wings, because it turns the problem into deductive reasoning. Meanwhile, Penguins have two wings. would be a poor knowledge statement to demonstrate according to our guideline.</p>
<p>When generating knowledge for a new question $q$, we plug the question into the placeholder, and repeatedly sample generated continuations of this prompt to obtain a set of knowledge statements $K_{q}=\left{k_{1}, k_{2}, \ldots, k_{M}\right}$. For full prompts on all the tasks we evaluate on, see Appendix A.2.</p>
<h3>2.2 Knowledge Integration via Prompting</h3>
<p>In the knowledge integration step, we use a language model - called the inference model - to make predictions with each generated knowledge statement, then select the highest-confidence prediction. Specifically, we use each knowledge statement to prompt the model, forming $M$ knowledgeaugmented questions:</p>
<p>$$
q_{0}=q, q_{1}=\left[k_{1} | q\right], \ldots, q_{M}=\left[k_{M} | q\right]
$$</p>
<p>where $|\cdot||\cdot|$ denotes text concatenation.
We compute an aggregated score for each answer choice $a$ using the augmented question that best supports it under the inference model:</p>
<p>$$
p_{I}\left(a \mid q, K_{q}\right) \propto \max <em I="I">{0 \leq m \leq M} p</em>\right)
$$}\left(a \mid q_{m</p>
<p>Intuitively, this favors knowledge statements that strongly support one of the choices.</p>
<p>The predicted answer is then,</p>
<p>$$
\hat{a}=\underset{a \in A_{q}}{\arg \max } \max <em I="I">{0 \leq m \leq M} p</em>\right)
$$}\left(a \mid q_{m</p>
<p>which is the choice that gets most support from one of the knowledge statements. This prediction uses a single knowledge statement, which we refer to as the selected knowledge:</p>
<p>$$
\hat{k}=k_{\hat{m}} \text { where } \hat{m}=\underset{0 \leq m \leq M}{\arg \max } \max <em q="q">{a \in A</em>\right)
$$}} p_{I}\left(a \mid q_{m</p>
<p>The inference model may be any existing language model taken off-the-shelf (i.e. zero-shot) or finetuned on the task. We do not do any further finetuning with knowledge prompting.</p>
<h2>3 Experimental Setup</h2>
<p>Here, we describe the implementation details of our method and how they are adapted to each task.</p>
<p>For knowledge generation, we use GPT-3 (Brown et al., 2020) as the underlying language model, where our few-shot prompting method is most effective. We generate $M=20$ knowledge statements for each question with nucleus sampling $p=0.5$ (Holtzman et al., 2019), and discard repetitions and empty strings. Generation is terminated when it exceeds 64 tokens or hits the $\backslash \mathrm{n}$ token. ${ }^{1}$</p>
<p>For inference, we use off-the-shelf T5 (Raffel et al., 2019) and GPT-3, as well as finetuned models that are state-of-the-art on each dataset, including UnifiedQA (UQA) (Khashabi et al., 2020) and Unicorn (Lourie et al., 2021). See details in the task setup below.</p>
<h3>3.1 Datasets and Task Setup</h3>
<p>We evaluate our method on four commonsense reasoning datasets which cover a variety of challenges and problem formats.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>NumerSense (Lin et al., 2020) consists of numerical statements about common objects and concepts where for each sentence we need to recover a masked number word. The choices are integers ranging from zero to ten, plus the word no, so the task can be framed as a multiple-choice problem. Since NumerSense is a diagnostic dataset, we only use zero-shot inference models, which is the current SOTA. We follow Zhang (2021) who uses the state-of-the-art zero-shot T5 with text-infilling setup and select the choice with highest likelihood on its token(s). We also implement zero-shot GPT3 inference, where we plug in each choice to the question and compute the choice probability as the generative probability of the entire sentence, normalized over all the choices.
CommonsenseQA (CSQA) (Talmor et al., 2019) is a 5-way multiple-choice QA dataset about common world scenarios. We do inference with the zero-shot and finetuned T5 models. For zero-shot T5, we format the question as text-infilling, and predict the choice with highest sequence-to-sequence language modeling probability. For finetuned T5 (including UnifiedQA which is SOTA), we use the same setup as Khashabi et al. (2020).
CommonsenseQA 2.0 (CSQA2) (Talmor et al., 2021) is a binary classification dataset where we need to judge whether commonsense statements are true or false. We only do inference with the finetuned model, due to poor calibration of zero-shot models on this dataset. We use finetuned Unicorn (Lourie et al., 2021), which is the current SOTA, following the setup in Talmor et al. (2021).
QASC (Khot et al., 2020) is an 8-way multiplechoice QA dataset about grade school science. This dataset also includes two pieces of background knowledge per question, whose composition fully answers the question. We do inference with zeroshot T5 and finetuned T5 (including UnifiedQA which is SOTA), using the same setups as CSQA.</p>
<h3>3.2 Knowledge Generation Baselines</h3>
<p>We study the impact of our knowledge generation method (shorthanded as $K$ ) by comparing with the following baselines:
No knowledge ( $\varnothing$ ) We refer to inference without any knowledge statements as the vanilla baseline.
Random sentences $(R)$ Sampling random sentences from the language model without conditioning on the question. We use the same implementation setup as our knowledge generation method (i.e.
also using GPT-3, with the same hyperparameters). Context sentences (C) Sampling sentences from the context of the question. This is implemented by sampling text continuations of the question from the language model. We use the same implementation setup as our knowledge generation method.
Template-generated knowledge ( $T$ ) Self-talk (Shwartz et al., 2020) uses manually-designed templates to elicit knowledge statements from language models. For fair comparison, we use GPT-3 as the knowledge generator in self-talk, and bound the number of generations to $M=20$ per question. Templates and other hyperparameters are kept the same as their original paper.
Retrieval-based knowledge (IR) Instead of being generated, knowledge can be retrieved from appropriate sources. We consider the following retrieval-based methods. For NumerSense, knowledge is retrieved from sentences in Wikipedia and GenericsKB. For CSQA2, we use snippets returned by Google when querying the question. For QASC, we use the associated fact sentences that are used to create each question.
Answers $(A)$ Instead of generating knowledge, GPT-3 can be prompted to generate direct answers to questions. In the prompts, we use the same input questions as those in knowledge generation, while replacing the knowledge statement with the ground truth answer. We consider two baselines: (1) Generate one answer per question and use this to measure the performance of the few-shot GPT-3 inference model; (2) Generate $M=20$ answers per question, and use these answers to prompt the SOTA inference models.</p>
<h2>4 Experimental Results</h2>
<p>As we will show, our generated knowledge prompting method sets new state-of-the-art results on most datasets we evaluate on, and works well under both zero-shot and finetuned settings. In particular, our knowledge generation outperforms naive baselines as well as template-based knowledge generation, and is on-par with retrieval-based systems.</p>
<h3>4.1 Overall Performance</h3>
<p>Table 3 shows the results on zero-shot and finetuned models following our task setups.
New state-of-the-art. We apply our method on top of the same inference model used in the previous state-of-the-art. On NumerSense, we achieve a</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$A$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$B_{1}$</th>
<th style="text-align: center;">$B_{2}$</th>
<th style="text-align: center;">$C$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$D_{1}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$D_{2}$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dataset <br> Inference Model</td>
<td style="text-align: center;">NumerSense</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CSQA</td>
<td style="text-align: center;">CSQA</td>
<td style="text-align: center;">CSQA2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">QASC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">QASC</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">dev test ${ }<em _all="{all" _text="\text">{\text {core }}$ test ${ }</em>$}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-11b <br> dev</td>
<td style="text-align: center;">UQA-11b-ft <br> dev</td>
<td style="text-align: center;">Unicorn-ft <br> dev</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-11b <br> dev test</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">UQA-11b-ft <br> dev test</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">( $\varnothing$ ) Vanilla baseline</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">70.23</td>
<td style="text-align: center;">64.05</td>
<td style="text-align: center;">39.89</td>
<td style="text-align: center;">85.18</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">$70.2^{\dagger}$</td>
<td style="text-align: center;">48.16</td>
<td style="text-align: center;">44.89</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(R) Random sentences</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">21.79</td>
<td style="text-align: center;">85.42</td>
<td style="text-align: center;">70.37</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">49.35</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(C) Context sentences</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">42.51</td>
<td style="text-align: center;">85.34</td>
<td style="text-align: center;">70.92</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">55.83</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(T) Template-based</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">45.37</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(IR) Retrieval-based</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">70.41</td>
<td style="text-align: center;">$65.10^{<em> </em>}$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">$73.3^{\dagger \dagger}$</td>
<td style="text-align: center;">76.89</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(A) Answers</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">51.84</td>
<td style="text-align: center;">84.93</td>
<td style="text-align: center;">69.22</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">52.48</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(K) Ours</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">79.24</td>
<td style="text-align: center;">72.47</td>
<td style="text-align: center;">47.26</td>
<td style="text-align: center;">85.34</td>
<td style="text-align: center;">72.37</td>
<td style="text-align: center;">73.03</td>
<td style="text-align: center;">58.32</td>
<td style="text-align: center;">55.00</td>
</tr>
<tr>
<td style="text-align: center;">prev. SOTA (no IR)</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">72.61</td>
<td style="text-align: center;">$66.18^{*}$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">79.1 (test) ${ }^{\text {® }}$</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">$70.2^{\dagger}$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">81.75</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot GPT-3 Infer.</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">71.58</td>
<td style="text-align: center;">53.80</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">66.09</td>
</tr>
</tbody>
</table>
<p>Table 3: Experimental results of applying different knowledge generation methods on various tasks and inference models. T5-11b is the zero-shot inference model, whereas other inference models are finetuned based on T5-11b. We bold the best and underline the second best numbers. Previous SOTA and retrieval-based methods are also based on the inference model in their corresponding column: * T5-11b 1.1 +digits (Submission by ISI Waltham); ${ }^{<em> </em>}$ T5-11b + IR (Yan, 2021); # UQA-11b-ft (Khashabi et al., 2020) (SOTA of single-model methods without referencing ConceptNet); $\dagger$ Unicorn-ft (Talmor et al., 2021); $\dagger \dagger$ Unicorn-ft + Google snippets (Talmor et al., 2021); $\ddagger$ UQA-11b-ft (Khashabi et al., 2020).
$6 \%(66.18 \rightarrow 72.47)$ improvement over the previous best method based on the zero-shot T5 model. The previous state-of-the-art among non-retrieval methods on CSQA2 is based on the finetuned Unicorn model, upon which we improve by $2 \%$ ( 70.2 $\rightarrow$ 73.03). For QASC, the previous best is based on the finetuned UnifiedQA model, upon which we improve by $3 \%(76.74 \rightarrow 80.33)$.
Zero-shot settings. Columns $A, B_{1}$, and $D_{1}$ in Table 3 show that our method substantially improves zero-shot inference models, by $7 \%$ to $10 \%$ across NumerSense ( $64.05 \rightarrow 72.47$ ), CSQA $(39.89 \rightarrow 47.26)$, and QASC $(44.89 \rightarrow 55.00)$.
Finetuned settings. Columns $B_{2}, C$, and $D_{2}$ in Table 3 indicate that our method consistently improves upon the vanilla baseline set by finetuned inference models (though by smaller margins than in the zero-shot settings).</p>
<h3>4.2 Knowledge Generation Methods</h3>
<p>Table 3 reports the performance with different knowledge generation baselines. Generally, random sentences barely help and even hurt the inference model, whereas context sentences of the question provide some gain. In contrast, knowledge generated by our method consistently leads to substantial performance improvements, which implies that our knowledge is of high quality.
Knowledge is an essential factor. The few-shot GPT-3 model is poorly calibrated to directly answer
commonsense questions, underperforming our best models by $14 \%$ to $20 \%$ across all tasks. Even when we use answers generated by few-shot GPT-3 to prompt the SOTA inference models, this still significantly falls behind our method on almost all the tasks and models we consider (with one exception - CSQA with T5 inference). Through the medium of knowledge, our method can effectively leverage useful information possessed by GPT-3 to help improve even the SOTA models on various commonsense reasoning tasks.
Our knowledge outperform template generated knowledge. We compare our knowledge generation method with the template-based self-talk on the CSQA dev set. (CSQA is the only task we experiment with that has self-talk templates available.) Our method leads to a larger improvement over the T5-11b baseline than self-talk (by $1.89 \%$ ), showing that it is better at eliciting helpful knowledge from models.
Our knowledge is comparable with retrievalbased knowledge. On NumerSense, the retrieved knowledge only improves inference performance by $0.18 \%$ on test-core and $1.02 \%$ on test-all, while our method further outperforms it by $8.83 \%$ and $7.37 \%$, respectively. This shows that knowledge retrieved from a loosely-related knowledge base can be far less useful than our generated knowledge. On CSQA2, although we are not able to beat the web-retrieved knowledge,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance with different number of generated knowledge statements per question (QASC dev set, T5-11b inference model).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Integration method</th>
<th style="text-align: center;">QASC-dev</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ours</td>
<td style="text-align: center;">58.32</td>
</tr>
<tr>
<td style="text-align: center;">Mixture-of-Experts</td>
<td style="text-align: center;">56.26</td>
</tr>
<tr>
<td style="text-align: center;">Product-of-Experts</td>
<td style="text-align: center;">55.94</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance with different knowledge integration methods (QASC dev set, T5-11b inference model).
our method still bridges the performance gap without referring to Google search. For QASC, the "retrieved" knowledge is actually gold knowledge from a knowledge base that was used to construct the dataset. As a result, our generated knowledge falls significantly short of the retrieved knowledge. In summary, our generated knowledge is roughly comparable with retrieved knowledge in terms of downstream performance, and is most valuable when there is no appropriate in-domain knowledge base to retrieve from.</p>
<h3>4.3 Analysis</h3>
<p>Better performance with more knowledge. We analyze the impact of the number of generated knowledge statements, $M$, and show the results in Figure 2. Generally, the performance increases with the quantity of knowledge statements. It saturates at $M=20$ and begins to decline when more knowledge statements are introduced, which may be because more noisy knowledge is generated.
The knowledge integration method. In addition to the knowledge integration method described in $\S 2.2$, we experiment with two alternatives: Mixture-of-Experts (MoE) and Product-of-Experts (PoE) (Hinton, 2002). These make the following modifications to Equation 1, respectively:</p>
<p>$$
\begin{aligned}
\text { MoE: } p_{I}\left(a \mid q, K_{q}\right) &amp; \propto \sum_{0 \leq m \leq M} p_{I}\left(a \mid q_{m}\right) \
\text { PoE: } p_{I}\left(a \mid q, K_{q}\right) &amp; \propto \prod_{0 \leq m \leq M} p_{I}\left(a \mid q_{m}\right)
\end{aligned}
$$</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Improvement on top of different sizes of inference model (Numersense dev set).
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Improvement by different sizes of knowledge generation model (Numersense dev set, T5-11b inference model).</p>
<p>The results in Table 4 indicate that our knowledge integration method - i.e. adaptively choosing the best knowledge to rely on - is best among the three.
Lightweight inference models and amplification. We found that the size of inference model affects the magnitude of improvement. Figure 3 shows the NumerSense performance gain on top of different sizes of inference model. As we use smaller inference models, the performance gain increases drastically. In particular, with our method the smallest T5 model is as powerful as the T5-3b baseline, and T5-large outperforms the GPT-3 baseline. This indicates that model-generated knowledge can enable high performing, yet lightweight, inference models. Furthermore, the improvement does not diminish as the inference model becomes as big as the knowledge generation model, as the inference by GPT-3 can benefit by $9.0 \%$ from the knowledge elicited from itself. This indicates that our method can somewhat amplify the useful knowledge already possessed by the model, leading to better predictions.
The size of knowledge generation model. Figure 4 shows the NumerSense performance gain when using different sizes of GPT-3 as the knowledge generation model. On top of the T5-11b inference model, The 6.7B knowledge model gives</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Human evaluation of generated knowledge. Left: Percentage of good knowledge statements along each axis. Right: Agreement between human and machine on helpfulness of selected knowledge.
a $5.0 \%$ improvement, narrower than the $10.5 \%$ improvement given by the 175B knowledge model. The 1.3B and 0.4B knowledge models do not give a significant improvement. Therefore, we do not necessarily need the largest version of GPT-3 as the knowledge source, though we do need the model to be relatively large in order to generate useful and reliable knowledge.</p>
<h3>4.4 Human Evaluation</h3>
<p>We conduct a human evaluation on NumerSense and QASC to study the quality of generated knowledge and the interpretability of its impact on task performance.</p>
<p>Evaluation. We report the quality of knowledge statements along four axes: (1) Grammaticality: whether it is grammatical; (2) Relevance: whether it is relevant to the topic or concepts mentioned on the question; (3) Factuality: whether it is (mostly) factually correct; and (4) Helpfulness: whether it helps answering the question in an either direct or indirect way, and may fall into one of the three categories: helpful (i.e. supports the correct answer), harmful (i.e. negates the correct answer or supports an incorrect answer), or neutral (neither helpful nor harmful). These metrics are adapted from Shwartz et al. (2020) and are defined in Appendix A.3.</p>
<p>From each dataset, we sample up to 50 selected knowledge (§2.2) that change the correctness of T5-11b's prediction (i.e. rectifies model prediction from wrong to right, or misleads model prediction from right to wrong). The knowledge are labeled by two NLP experts and a moderate level of agreement was reached (Fleiss Kappa $\kappa=0.57$ (Landis and Koch, 1977)). To ensure objectivity, it is not revealed to the annotators whether the knowledge rectifies or misleads the model prediction.</p>
<p>Results. Figure 5 summarizes the results. The vast majority of selected knowledge are grammatical and relevant to the question, and $83 \%$ of them are factually correct. $72 \%$ are seen as being helpful for answering the question according the human evaluators, whereas $13 \%$ are harmful. Out of the knowledge statements that rectify the model predictions, $93 \%$ are labeled as helpful by the human evaluators; in contrast, when the knowledge statement misleads the model, only $21 \%$ are labeled as helpful, and $39 \%$ harmful. Of the knowledge deemed helpful by human and rectifies model prediction, $95 \%$ are factual, while of those deemed harmful by human and misleads model prediction, $86 \%$ are non-factual, suggesting that improving knowledge factuality is a promising path towards more helpful knowledge. We also analyzed the nonselected knowledge and found that these statements have slightly lower factuality and helpfulness than the selected knowledge.</p>
<h3>4.5 Qualitative Examples</h3>
<p>Table 5 shows a few examples where the generated knowledge rectifies model prediction. Due to space constraints we only show the selected knowledge (§2.2) for each question. In all examples, the model without prompted knowledge assigns a higher score to an incorrect answer than the correct answer, while with knowledge prompting, the correct answer is assigned a much higher score. Prompting with generated knowledge can transform commonsense reasoning into explicit reasoning procedures such as paraphrasing, induction, deduction, analogy, abductive reasoning, logical elimination, negation, and numerical reasoning.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Question / Knowledge</th>
<th style="text-align: center;">Prediction</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Reasoning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NumerSense</td>
<td style="text-align: center;">clams have evolved to have [M] shells. <br> Clams have a bivalve shell.</td>
<td style="text-align: center;">no <br> two</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.3710 .18 \ &amp; 0.89 \end{aligned}$</td>
<td style="text-align: center;">Commonsense <br> Paraphrasing</td>
</tr>
<tr>
<td style="text-align: center;">NumerSense</td>
<td style="text-align: center;">an easel can have [M] or four legs. <br> A tripod is a kind of easel.</td>
<td style="text-align: center;">two <br> three</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.4510 .45 \ &amp; 0.46 \end{aligned}$</td>
<td style="text-align: center;">Commonsense <br> Induction</td>
</tr>
<tr>
<td style="text-align: center;">CSQA</td>
<td style="text-align: center;">Where does a heifer's master live? <br> The master of a heifer is a farmer.</td>
<td style="text-align: center;">slaughter house <br> farm house</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.8910 .01 \ &amp; 0.92 \end{aligned}$</td>
<td style="text-align: center;">Commonsense <br> Deduction</td>
</tr>
<tr>
<td style="text-align: center;">CSQA</td>
<td style="text-align: center;">Aside from water and nourishment what does your dog need? <br> Dogs need attention and affection.</td>
<td style="text-align: center;">walked <br> lots of attention</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.5510 .04 \ &amp; 0.91 \end{aligned}$</td>
<td style="text-align: center;">Commonsense <br> Elimination</td>
</tr>
<tr>
<td style="text-align: center;">CSQA</td>
<td style="text-align: center;">I did not need a servant. I was not a what? <br> People who have servants are rich.</td>
<td style="text-align: center;">in charge rich person</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.4710 .32 \ &amp; 0.99 \end{aligned}$</td>
<td style="text-align: center;">Commonsense <br> Abduction</td>
</tr>
<tr>
<td style="text-align: center;">CSQA2</td>
<td style="text-align: center;">Part of golf is trying to get a higher point total than others. <br> The player with the lowest score wins.</td>
<td style="text-align: center;">yes <br> no</td>
<td style="text-align: center;">$\begin{aligned} &amp; 1.0010 .00 \ &amp; 1.00 \end{aligned}$</td>
<td style="text-align: center;">Commonsense <br> Negation</td>
</tr>
<tr>
<td style="text-align: center;">CSQA2</td>
<td style="text-align: center;">Eighth plus eight is smaller than fifteen. <br> Eighth plus eight is sixteen, which is larger than fifteen.</td>
<td style="text-align: center;">yes <br> no</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9710 .03 \ &amp; 1.00 \end{aligned}$</td>
<td style="text-align: center;">Commonsense <br> Numerical</td>
</tr>
<tr>
<td style="text-align: center;">QASC</td>
<td style="text-align: center;">[M] is used for transportation. <br> Bicycles are used for transportation.</td>
<td style="text-align: center;">plastic <br> boats</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.4110 .12 \ &amp; 0.74 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Commonsense } \ &amp; \text { Analogy } \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Table 5: More examples where prompting with generated knowledge reduces the reasoning type and rectifies the prediction. The first row of each section is the original question and the inference results associated with it; the second row is a model-generated knowledge statement that prompts the inference model. We show correct answers in green, incorrect answers in red, and their corresponding scores assigned by the inference model.</p>
<h2>5 Related Work</h2>
<p>Knowledge can be elicited from pretrained language models. Numerous works have shown that pretrained language models implicitly contain a large amount of knowledge that can be queried via conditional generation (Davison et al., 2019; Petroni et al., 2019; Jiang et al., 2020). Consequently, these models can directly perform inference on tasks like commonsense reasoning (Trinh and Le, 2018; Yang et al., 2020), text classification (Shin et al., 2020; Puri and Catanzaro, 2019), and natural language inference (Shin et al., 2020; Schick and Schütze, 2021). Inspired by these observations, we elicit question-related knowledge in an explicit form from language models and use them to guide the inference.
Leveraging external knowledge for commonsense reasoning. Some work uses external commonsense knowledge bases to make improvements on various NLP tasks, including commonsense reasoning. One approach is to inject commonsense knowledge into language models, either by pretraining on knowledge bases (Ma et al., 2021; Chang et al., 2020; Mitra et al., 2019; Zhong et al., 2019) or finetuning the model so that it can reason with additional retrieved knowledge (Chang et al., 2020; Mitra et al., 2019; Bian et al., 2021). Another di-
rection is to ground the question into a knowledge graph and do inference with graph-based reasoning (Lin et al., 2019; Lv et al., 2020; Yasunaga et al., 2021).</p>
<p>A common prerequisite of these methods is a high-quality, high-coverage, in-domain commonsense knowledge base (Ma et al., 2019). Some commonsense reasoning datasets are derived from existing knowledge bases; for example, CommonsenseQA (Talmor et al., 2019) is derived from ConceptNet (Speer et al., 2017), and Social IQA (Sap et al., 2019b) is derived from ATOMIC (Sap et al., 2019a). For such datasets, it is natural to elicit related knowledge from the underlying knowledge base that derived them, and typically this would demonstrate considerable gains (Mitra et al., 2019; Chang et al., 2020). However, if there is a domain mismatch between the dataset and the knowledge base, such gains tend to diminish (Mitra et al., 2019; Ma et al., 2019). This becomes a bottleneck when encountering datasets that have no suitable knowledge base (e.g. NumerSense (Lin et al., 2020) and CommonsenseQA 2.0 (Talmor et al., 2021)), or when the system needs to handle commonsense queries that do not fit in any of the commonsense domains represented by an existing knowledge base. Our work overcomes this diffi-</p>
<p>culty by leveraging pretrained language models as the source of commonsense knowledge.</p>
<p>Adding generated text during inference. Recently, several works show that model performance on commonsense reasoning can be boosted by augmenting the question with model-generated text, such as clarifications, explanations, and implications. Self-talk (Shwartz et al., 2020) elicits clarifications to concepts in the question and appends them to the inference model input. Contrastive explanations (Paranjape et al., 2021) prompts inference models with generated explanations that contrast between two answer choices. The aforementioned methods depend on task-specific templates to inquire the generator, which means they are only capable of eliciting a limited variety of knowledge and require careful hand-crafting to transfer to new tasks. Other explanation-based methods (Latcinnik and Berant, 2020; Rajani et al., 2019) finetune the generator model so that it produces explanations that are used for question augmentation. DynaGen (Bosselut et al., 2021) uses pretrained commonsense models to generate implications of a question and expands the inference input with these generations. However, its usage of COMeT (Bosselut et al., 2019) as the generator confines its applicability to the social commonsense domain. Our work contributes to this general line of research, yet different from these previous methods that elicit knowledge with task-specific templates or from finetuned knowledge generators, our method requires only a few human-written demonstrations in the style of the task, making it much more flexible, easy-to-transfer, and engineering-efficient.</p>
<h2>6 Conclusion</h2>
<p>We introduce generated knowledge prompting, a simple method to elicit and integrate knowledge from language models so as to improve performance on commonsense reasoning tasks. In particular, we generate knowledge statements by prompting a language model with task-specific, humanwritten, few-shot demonstrations of questionknowledge pairs. We show that knowledge can be integrated by simply plugging it in at inference time, with no need to finetune the model for knowledge integration. Our method shows effectiveness across multiple datasets, sets the new state-of-theart on three commonsense reasoning tasks, and works under a variety of settings. The method's success highlights language models as sources of
flexible, high-quality knowledge for commonsense reasoning.</p>
<h2>Acknowledgements</h2>
<p>This work was funded in part by the Natural Sciences and Engineering Research Council of Canada (NSERC) (funding reference number 401233309), DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and the Allen Institute for AI. We also thank Google Cloud Compute, as well as OpenAI.</p>
<p>We thank Daniel Khashabi, Vered Shwartz, Bhargavi Paranjape, Bill Yuchen Lin, Jonathan Herzig for their help with the experiments and evaluation.</p>
<h2>References</h2>
<p>Ning Bian, Xianpei Han, Bo Chen, and Le Sun. 2021. Benchmarking knowledge-enhanced commonsense question answering via knowledge-to-text transformation. arXiv preprint arXiv:2101.00760.</p>
<p>Antoine Bosselut, Ronan Le Bras, and Yejin Choi. 2021. Dynamic neuro-symbolic knowledge graph construction for zero-shot commonsense question answering. In Proceedings of the 35th AAAI Conference on Artificial Intelligence (AAAI).</p>
<p>Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4762-4779, Florence, Italy. Association for Computational Linguistics.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</p>
<p>Ting-Yun Chang, Yang Liu, Karthik Gopalakrishnan, Behnam Hedayatnia, Pei Zhou, and Dilek Hakkani-Tur. 2020. Incorporating commonsense knowledge graph in pretrained models for social commonsense tasks. In Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 74-79, Online. Association for Computational Linguistics.</p>
<p>Joe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1173-1178, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Geoffrey E Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771-1800.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751.</p>
<p>Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1896-1907, Online. Association for Computational Linguistics.</p>
<p>Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. 2020. Qasc: A dataset for question answering via sentence composition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8082-8090.</p>
<p>J Richard Landis and Gary G Koch. 1977. The measurement of observer agreement for categorical data. biometrics, pages 159-174.</p>
<p>Veronica Latcinnik and Jonathan Berant. 2020. Explaining question answering models through text generation. arXiv preprint arXiv:2004.05569.</p>
<p>Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019. KagNet: Knowledge-aware graph networks for commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2829-2839, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. 2020. Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6862-6868, Online. Association for Computational Linguistics.</p>
<p>Nicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark. arXiv preprint arXiv:2103.13009.</p>
<p>Shangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, and Songlin Hu. 2020. Graphbased reasoning over heterogeneous external knowledge for commonsense question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8449-8456.</p>
<p>Kaixin Ma, Jonathan Francis, Quanyang Lu, Eric Nyberg, and Alessandro Oltramari. 2019. Towards generalizable neuro-symbolic systems for commonsense question answering. In Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 22-32, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Kaixin Ma, Filip Ilievski, Jonathan Francis, Yonatan Bisk, Eric Nyberg, and Alessandro Oltramari. 2021. Knowledge-driven data construction for zero-shot evaluation in commonsense question answering. In 35th AAAI Conference on Artificial Intelligence.</p>
<p>Arindam Mitra, Pratyay Banerjee, Kuntal Kumar Pal, Swaroop Mishra, and Chitta Baral. 2019. How additional knowledge can improve natural language commonsense question answering? arXiv preprint arXiv:1909.08855.</p>
<p>Bhargavi Paranjape, Julian Michael, Marjan Ghazvininejad, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2021. Prompting contrastive explanations for commonsense reasoning tasks. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4179-4192, Online. Association for Computational Linguistics.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Raul Puri and Bryan Catanzaro. 2019. Zero-shot text classification with generative language models. arXiv preprint arXiv:1912.10165.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4932-4942, Florence, Italy. Association for Computational Linguistics.</p>
<p>Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. 2019a. Atomic: An atlas of machine commonsense for if-then reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3027-3035.</p>
<p>Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019b. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463-4473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Timo Schick and Hinrich Schütze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255-269, Online. Association for Computational Linguistics.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics.</p>
<p>Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4615-4629, Online. Association for Computational Linguistics.</p>
<p>Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Thirty-first AAAI conference on artificial intelligence.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and Jonathan Berant. 2021. Commonsenseqa 2.0: Exposing the limits of ai through gamification.</p>
<p>Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847.</p>
<p>Jun Yan. 2021. Usc ink submission on numersense.
Jheng-Hong Yang, Sheng-Chieh Lin, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin. 2020. Designing templates for eliciting commonsense knowledge from pretrained
sequence-to-sequence models. In Proceedings of the 28th International Conference on Computational Linguistics, pages 3449-3453, Barcelona, Spain (Online). International Committee on Computational Linguistics.</p>
<p>Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. 2021. QA-GNN: Reasoning with language models and knowledge graphs for question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 535-546, Online. Association for Computational Linguistics.</p>
<p>Yuhui Zhang. 2021. Stanford submission on numersense.</p>
<p>Wanjun Zhong, Duyu Tang, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. 2019. Improving question answering by commonsense-based pre-training. In CCF International Conference on Natural Language Processing and Chinese Computing, pages 16-28. Springer.</p>
<h2>A Appendix</h2>
<h2>A. 1 Comparison with Prior Methods</h2>
<p>Table 6 summarizes the comparison between our generated knowledge prompting method and prior methods that add generated text to an inference model for commonsense reasoning tasks. Our method is unique because it uses few-shot demonstrations to prompt for knowledge generation, and can apply to finetuned inference models without joint finetuning with knowledge.</p>
<h2>A. 2 Prompts for Knowledge Generation</h2>
<p>Table 7 through 10 shows the full prompts for knowledge generation that we use for each evaluated task: NumerSense, CSQA, CSQA2, and QASC.</p>
<h2>A. 3 Human Evaluation Guidelines</h2>
<p>Table 11 and 12 shows the detailed guidelines we use for human evaluation of generated knowledge.</p>
<h2>B Checklist</h2>
<h2>B. 1 Limitations and Risks</h2>
<p>Limitations. Our method is tested on a representative selection of commonsense reasoning tasks and datasets. Applying this method to other tasks may require people with moderate expertise to craft a task-specific prompt to feed into the method.</p>
<p>Risks. It is possible that our proposed method may lower the performance of commonsense reasoning systems, if not implemented properly or using badly-designed prompts. Such risk can be mitigated by following the prompt design guidelines in this paper $(\S 2.1)$.</p>
<h2>B. 2 Computation</h2>
<p>We do not train any new model in this paper. Inference is conducted on Quadro RTX 8000 GPUs and costs about 200 GPU hours in total. Knowledge generation is done with the OpenAI GPT-3 API, with an approximate cost of $\$ 500$.</p>
<p>Our method is implemented with PyTorch and the Huggingface Transformers library.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Knowledge Generator</th>
<th style="text-align: center;">Inference Model</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CAGE (Rajani et al., 2019)</td>
<td style="text-align: center;">task-finetuned</td>
<td style="text-align: center;">joint-finetuned</td>
</tr>
<tr>
<td style="text-align: center;">Latcinnik and Berant (2020)</td>
<td style="text-align: center;">task-finetuned</td>
<td style="text-align: center;">joint-finetuned</td>
</tr>
<tr>
<td style="text-align: center;">DynaGen (Bosselut et al., 2021)</td>
<td style="text-align: center;">task-finetuned</td>
<td style="text-align: center;">joint-finetuned</td>
</tr>
<tr>
<td style="text-align: center;">Self-talk (Shwartz et al., 2020)</td>
<td style="text-align: center;">template-prompted</td>
<td style="text-align: center;">0 -shot</td>
</tr>
<tr>
<td style="text-align: center;">Contrastive expl. (Paranjape et al., 2021)</td>
<td style="text-align: center;">template-prompted</td>
<td style="text-align: center;">0 -shot \&amp; joint-finetuned</td>
</tr>
<tr>
<td style="text-align: center;">Generated knowledge prompting (ours)</td>
<td style="text-align: center;">demonstrations-prompted</td>
<td style="text-align: center;">0 -shot \&amp; task-finetuned</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparison of methods that add generated text to an inference model. Knowledge Generator: taskfinetuned - a model finetuned to generate task-specific knowledge; template-prompted - an off-the-shelf LM from which knowledge statements are elicited via templates; demonstration-prompted - an off-the-shelf LM from which knowledge statements are elicited via few-shot demonstrations (\$2.1). Inference Model: 0 -shot - an off-the-shelf LM that is set up to make predictions; task-finetuned - a model finetuned with task training data (and without seeing extra knowledge); joint-finetuned - a model finetuned with task training data and generated knowledge.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NumerSense</td>
<td style="text-align: center;">Generate some numerical facts about objects. Examples:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Input: penguins have <mask> wings. <br> Knowledge: Birds have two wings. Penguin is a kind of bird.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Input: a parallelogram has <mask> sides. <br> Knowledge: A rectangular is a parallelogram. A square is a parallelogram.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Input: there are <mask> feet in a yard. <br> Knowledge: A yard is three feet.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Input: water can exist in <mask> states. <br> Knowledge: There states for matter are solid, liquid, and gas.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Input: a typical human being has <mask> limbs. <br> Knowledge: Human has two arms and two legs.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Input: {question} <br> Knowledge:</td>
</tr>
</tbody>
</table>
<p>Table 7: Prompt for knowledge generation on NumerSense. Demonstration examples are manually written and the knowledge enables explicit reasoning procedures to answer the input question.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CSQA</td>
<td style="text-align: left;">Generate some knowledge about the concepts in the input. Examples:</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: Google Maps and other highway and street GPS services have replaced what?</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge: Electronic maps are the modern version of paper atlas.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: The fox walked from the city into the forest, what was it looking for?</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge: Natural habitats are usually away from cities.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: You can share files with someone if you have a connection to a what?</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge: Files can be shared over the Internet.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: Too many people want exotic snakes. The demand is driving what to carry them?</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge: Some people raise snakes as pets.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: The body guard was good at his duties, he made the person who hired him what?</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge: The job of body guards is to ensure the safety and security of the employer.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: {question}</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge:</td>
</tr>
</tbody>
</table>
<p>Table 8: Prompt for knowledge generation on CSQA. Demonstration examples are selected from the CSQA training set; we manually write relevant knowledge for each input question.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CSQA2</td>
<td style="text-align: left;">Generate some knowledge about the input. Examples:</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: Greece is larger than mexico.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">sq km, making Mexico 1,389\% larger than Greece.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: Glasses always fog up.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">forming a film that you see as fog. Your lenses will be relatively cool compared to your breath,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">especially when the outside air is cold.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: A fish is capable of thinking.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">cognitive powers match or exceed those of 'higher' vertebrates including non-human primates.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Fish's long-term memories help them keep track of complex social relationships.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: A common effect of smoking lots of cigarettes in one's lifetime is a higher than</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">normal chance of getting lung cancer.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">had nine times the risk of dying from lung cancer than never smokers. Among people who smoked</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">higher than that of never smokers.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: A rock is the same size as a pebble.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">(2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: ${$ question $}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge:</td>
</tr>
</tbody>
</table>
<p>Table 9: Prompt for knowledge generation on CSQA2. Demonstration examples are selected from the CSQA2 training set; we use the annotated Google featured snippet as the knowledge.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">QASC</td>
<td style="text-align: left;">Generate some knowledge about the input. Examples:</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: What type of water formation is formed by clouds?</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge: Clouds are made of water vapor.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: What can prevent food spoilage?</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge: Dehydrating food is used for preserving food.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: The process by which genes are passed is</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge: Genes are passed from parent to offspring.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: The stomach does what in the body?</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge: The stomach is part of the digestive system.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: What can cause rocks to break down?</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge: Mechanical weathering is when rocks are broken down by mechanical means.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Input: {question}</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Knowledge:</td>
</tr>
</tbody>
</table>
<p>Table 10: Prompt for knowledge generation on QASC. Demonstration examples are selected from the QASC training set; we use one of the gold separate facts as the knowledge.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Attribute</th>
<th style="text-align: center;">Options</th>
<th style="text-align: center;">Description and Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Grammaticality</td>
<td style="text-align: center;">grammatical; ungrammatical but understandable; completely gibberish</td>
<td style="text-align: center;">Whether the knowledge statement is grammatical. We are aware that some of the statements are not fully grammatical. If you can still understand what the statement says, even if it's an incomplete sentence or slightly ungrammatical, please select the "ungrammatical but understandable" option.</td>
</tr>
<tr>
<td style="text-align: center;">Relevance</td>
<td style="text-align: center;">relevant; not relevant</td>
<td style="text-align: center;">Whether a knowledge statement is relevant to the given question. A statement is relevant if it covers one the same topics as the question, or contains a salient concept that is same or similar to one in the question. Examples: <br> [Question] you may take the subway back and forth to work <mask> days a week. <br> [Knowledge] You take the subway back and forth to work five days a week. <br> [Judgment] Relevant, because the question and knowledge are both about the topic of business days. <br> [Question] a bradypus torquatus is native to brazil and has <mask> toes on each limb. <br> [Knowledge] A bradypus torquatus is a kind of mammal. A mammal has four limbs. <br> [Judgment] Relevant, because the question and knowledge share a common salient concept "bradypus torquatus".</td>
</tr>
<tr>
<td style="text-align: center;">Factuality</td>
<td style="text-align: center;">factual; not factual</td>
<td style="text-align: center;">Whether a knowledge statement is (mostly) factually correct or not. If there are exceptions or corner cases, it can still be considered factual if they are rare or unlikely. Examples: <br> [Knowledge] A limousine has four doors. <br> [Judgment] Factual. <br> [Knowledge] A human hand has four fingers and a thumb. <br> [Judgment] Factual, despite that there are exceptions - people with disabilities may have less or more fingers. <br> [Knowledge] A rectangle is a shape with two equal sides. <br> [Judgment] Not factual, because a rectangle has four sides.</td>
</tr>
</tbody>
</table>
<p>Table 11: Human evaluation guidelines. Continued in Table 12.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Attribute</th>
<th style="text-align: left;">Options</th>
<th style="text-align: left;">Description and Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Helpfulness</td>
<td style="text-align: left;">helpful; neutral;</td>
<td style="text-align: left;">Whether a knowledge statement provides useful information in support OR <br> contradiction of the answer. A statement is helpful if it supports the answer <br> either directly or indirectly. More on indirect support - The statement might not <br> directly answer the question directly, yet it may support an indirect reasoning <br> path that reaches the answer. A statement is harmful if it negates the answer or <br> supports an alternative potential answer either directly or indirectly. A statement <br> is neutral if it is neither helpful nor harmful. Examples:</td>
</tr>
<tr>
<td style="text-align: left;">Helpfulness</td>
<td style="text-align: left;">helpful; neutral;</td>
<td style="text-align: left;">[Question] you may take the subway back and forth to work <mask> days a <br> week. <br> [Answer] five</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">harmful</td>
<td style="text-align: left;">[Knowledge] You take the subway back and forth to work five days a week. <br> [Judgment] Helpful. Because the statement directly supports the answer.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">[Question] spiders have <mask> legs. <br> [Answer] eight</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">[Knowledge] Arachnids have eight legs. <br> [Judgment] Helpful. Although the statement does not directly refer to spiders, <br> together with the fact that "spiders are a kind of arachnids" it completes a <br> reasoning chain in deriving the answer.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">[Question] a game of chess may have <mask> outcomes. <br> [Answer] three</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">[Knowledge] A game of chess has two outcomes. <br> [Judgment] Harmful. Since the statement supports answering "two" instead of <br> "three".</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">[Question] a bradypus torquatus is native to brazil and has <mask> toes on each <br> limb.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">[Answer] three</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">[Knowledge] A bradypus torquatus is a kind of mammal. A mammal has four <br> limbs.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">[Judgment] Neutral. The statement does not provide information in favor or <br> contrast of the answer.</td>
</tr>
</tbody>
</table>
<p>Table 12: (continued) Human evaluation guidelines.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ An exception is with the CSQA2 dataset, where for the best results we choose $M=5$ and allow for up to 128 tokens in each generation.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
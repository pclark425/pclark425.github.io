<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7638 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7638</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7638</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-269004480</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2024.lrec-main.1030.pdf" target="_blank">Multilingual Brain Surgeon: Large Language Models Can Be Compressed Leaving No Language behind</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have ushered in a new era in Natural Language Processing, but their massive size demands effective compression techniques for practicality. Although numerous model compression techniques have been investigated, they typically rely on a calibration set that overlooks the multilingual context and results in significant accuracy degradation for low-resource languages. This paper introduces Multilingual Brain Surgeon (MBS), a novel calibration data sampling method for multilingual LLMs compression. MBS overcomes the English-centric limitations of existing methods by sampling calibration data from various languages proportionally to the language distribution of the model training datasets. Our experiments, conducted on the BLOOM multilingual LLM, demonstrate that MBS improves the performance of existing English-centric compression methods, especially for low-resource languages. We also uncover the dynamics of language interaction during compression, revealing that the larger the proportion of a language in the training set and the more similar the language is to the calibration language, the better performance the language retains after compression. In conclusion, MBS presents an innovative approach to compressing multilingual LLMs, addressing the performance disparities and improving the language inclusivity of existing compression techniques. Keywords: Large Language Model, Multilingual Model Compression</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7638",
    "paper_id": "paper-269004480",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.005054749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language Behind</p>
<p>Hongchuan Zeng 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Hongshen Xu xuhongshen@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Lu Chen chenlusz@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Suzhou Laboratory
SuzhouChina</p>
<p>Kai Yu kai.yu@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Suzhou Laboratory
SuzhouChina</p>
<p>Qingyuan Li 
Yifan Zhang 
Liang Li 
Peng Yao 
Bo Zhang 
Xiangxiang Chu 
Yerui Sun 
Li Du 
Yuchen Xie 
Fptq 
Ji Lin 
Liyang Liu 
Josh Achiam 
Steven Adler 
Sandhini Agarwal 
Lama Ahmad 
Ilge Akkaya 
Floren- Cia Leoni Aleman 
Diogo Almeida 
Janko Al- Tenschmidt 
Sam Altman 
Shyamal Anadkat 
Red Avila 
Igor Babuschkin 
Suchir Balaji 
Valerie Balcom 
Paul Baltescu 
Haiming Bao 
Moham- Mad Bavarian 
Jeff Belgum 
Irwan Bello 
Jake Berdine 
Gabriel Bernadett-Shapiro 
Christopher Berner 
Lenny Bogdonoff 
Oleg Boiko 
Made- Laine Boyd 
Anna-Luisa Brakman 
Greg Brock- Man 
Tim Brooks 
Miles Brundage 
Kevin Button 
Trevor Cai 
Rosie Campbell 
Andrew Cann 
Brit- Tany Carey 
Chelsea Carlson 
Rory Carmichael 
Brooke Chan 
Che Chang 
Fotis Chantzis 
Derek Chen 
Sully Chen 
Ruby Chen 
Jason Chen 
Mark Chen 
Ben Chess 
Chester Cho 
HyungCasey Chu 
Won Chung 
Dave Cummings 
Jeremiah Currier 
Yunxing Dai 
Cory Decareaux 
Thomas Degry 
Noah Deutsch 
Damien Dev- Ille 
Arka Dhar 
David Dohan 
Steve Dowling 
Sheila Dunning 
Adrien Ecoffet 
Atty Eleti 
Tyna Eloundou 
David Farhi 
Liam Fedus 
Niko Fe- Lix 
Simón Posada Fishman 
Juston Forte 
Is- Abella Fulford 
Leo Gao 
Elie Georges 
Chris- Tian Gibson 
Vik Goel 
Tarun Gogineni 
Gabriel Goh 
Rapha Gontijo-Lopes 
Jonathan Gordon 
Morgan Grafstein 
Scott Gray 
Ryan Greene 
ShixiangJoshua Gross 
Shane Gu 
Yufei Guo 
Chris Hallacy 
Jesse Han 
Jeff Harris 
Yuchen He 
Mike Heaton 
Johannes Heidecke 
Chris Hesse 
Alan Hickey 
Wade Hickey 
Peter Hoeschele 
Brandon Houghton 
Kenny Hsu 
Shengli Hu 
Xin Hu 
Joost Huizinga 
Shantanu Jain 
Shawn Jain 
Joanne Jang 
Angela Jiang 
Roger Jiang 
Haozhun Jin 
Denny Jin 
Shino Jomoto 
Bil- Lie Jonn 
Heewoo Jun 
Tomer Kaftan 
Łukasz Kaiser 
Ali Kamali 
Ingmar Kanitscheider 
Ni- Tish Shirish Keskar 
Tabarak Khan 
Logan Kil- Patrick 
Jong Wook Kim 
Christina Kim 
Yongjik Kim 
Jan Hendrik Kirchner 
Jamie Kiros 
Matt Knight 
Daniel Kokotajlo 
Łukasz Kondraciuk 
An- Drew Kondrich 
Aris Konstantinidis 
Kyle Kosic 
Gretchen Krueger 
Vishal Kuo 
Michael Lampe 
Ikai Lan 
Teddy Lee 
Jan Leike 
Jade Leung 
ChakDaniel Levy 
Ming Li 
Rachel Lim 
Molly Lin 
Stephanie Lin 
Mateusz Litwin 
Theresa Lopez 
Ryan Lowe 
Patricia Lue 
Anna Makanju 
Kim Malfacini 
Sam Manning 
Todor Markov 
Yaniv Markovski 
Bianca Martin 
Katie Mayer 
An- Drew Mayne 
Bob Mcgrew 
Scott Mayer Mckin- Ney 
Christine Mcleavey 
Paul Mcmillan 
Jake Mcneil 
David Medina 
Aalok Mehta 
Jacob Menick 
Luke Metz 
Andrey Mishchenko 
Pamela Mishkin 
Vinnie Monaco 
Evan Morikawa 
Daniel Mossing 
Tong Mu 
Mira Murati 
Oleg Murk 
David Mély 
Ashvin Nair 
Reiichiro Nakano 
Ra- Jeev Nayak 
Arvind Neelakantan 
Richard Ngo 
Hyeonwoo Noh 
Long Ouyang 
Cullen O'keefe 
Jakub Pachocki 
Alex Paino 
Joe Palermo 
Ash- Ley Pantuliano 
Giambattista Parascandolo 
Joel Parish 
Emy Parparita 
Alex Passos 
Mikhail Pavlov 
Andrew Peng 
Adam Perelman 
Mario Ryder 
Ted Saltarelli 
Shibani Sanders 
Girish Santurkar 
Heather Sas- Try 
David Schmidt 
John Schnurr 
Daniel Schulman 
Kyla Selsam 
Toki Sheppard 
Jessica Sherbakov 
Sarah Shieh 
Pranav Shoker 
Szymon Shyam 
Eric Sidor 
Maddie Sigler 
Jordan Simens 
Katarina Sitkin 
Ian Slama 
Benjamin Sohl 
Yang Sokolowsky 
Na- Talie Song 
Felipe Petroski Staudacher 
Such </p>
<p>Jiaming Tang
Haotian Tang
Shang Yang, Chuang Gan, and Song Han2023Xingyu Dang</p>
<p>Zhanghui Kuang, Ao-jun Zhou, Jing-Hao Xue, Xinjiang Wang, Wenming YangShilong Zhang, Yimin Chen, Qingmin Liao, and Wayne</p>
<p>de Avila Belbute Peres
Michael Petrov
Poko-rnyHen-rique Ponde de Oliveira Pinto, Michael</p>
<p>Michelle Pokrass
Vitchyr H. Pong, Alethea Power, Boris Power, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron RaymondTolly Powell, Elizabeth Proehl</p>
<p>Francis Real
Kendra Rimbach
Carl Ross, Bob Rot-stedHenri Roussez, Nick</p>
<p>Na-talie Summers
Ilya Sutskever
Jie Tang</p>
<p>Niko-las Tezak
Phil Tillet, Jerry TworekMadeleine B. Thompson, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley</p>
<p>Juan Fe-lipe Cerón Uribe
Andrea Vallone, Arun Vi-jayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay WangAlvin Wang, Ben Wang</p>
<p>Multilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language Behind
3FB57D85A0CE3A573936EC7551E37FD8Large Language Model, Multilingual Model Compression
Large Language Models (LLMs) have ushered in a new era in Natural Language Processing, but their massive size demands effective compression techniques for practicality.Although numerous model compression techniques have been investigated, they typically rely on a calibration set that overlooks the multilingual context and results in significant accuracy degradation for low-resource languages.This paper introduces Multilingual Brain Surgeon (MBS), a novel calibration data sampling method for multilingual LLMs compression.MBS overcomes the English-centric limitations of existing methods by sampling calibration data from various languages proportionally to the language distribution of the model training datasets.Our experiments, conducted on the BLOOM multilingual LLM, demonstrate that MBS improves the performance of existing English-centric compression methods, especially for low-resource languages.We also uncover the dynamics of language interaction during compression, revealing that the larger the proportion of a language in the training set and the more similar the language is to the calibration language, the better performance the language retains after compression.In conclusion, MBS presents an innovative approach to compressing multilingual LLMs, addressing the performance disparities and improving the language inclusivity of existing compression techniques.The codes are available at: https://github.com/X-LANCE/MBS.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) with their remarkable performance.However, their colossal size and computational demands necessitate effective Model Compression (MC) techniques for practical use.In the case of multilingual LLMs, the vast size is crucial for retaining information from various languages and mitigating the curse of multilinguality (Conneau et al., 2020;Goyal et al., 2021).Moreover, wide language coverage and interference among languages pose a harder challenge for compressing multilingual LLMs.</p>
<p>Existing approaches for MC have predominantly focused on model quantization (Frantar et al., 2023;Dettmers et al., 2022;Xiao et al., 2023;Yao et al., 2022), where model parameters are mapped to lower bit-level representations, and network pruning, which reduces the size of neural networks by eliminating unnecessary connections.Inspired by the classic Optimal Brain Damage (OBD) and Optimal Brain Surgeon (OBS) pruning framework (Hassibi et al., 1993;Le Cun et al., 1989), various approaches, namely GPTQ (Frantar et al., 2023) for model quantization, SparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2023) for network pruning, have been proposed to compress †Lu Chen and Kai Yu are the corresponding authors.</p>
<p>LLMs.These compression methods utilize a calibration dataset to determine the priority of parameters and thus are retraining-free, avoiding expensive fine-tuning cost especially for LLMs.</p>
<p>However, neither of these methods has considered the multilingual scenario: all of them use a single-language (e.g., English) calibration dataset to determine the priority of parameters for multilingual models.A significant performance drop on multilingual tasks is observed due to this Englishcentric approach, especially in the case of lowresource languages.</p>
<p>In this paper, we propose Multilingual Brain Surgeon (MBS), which has successfully achieved significant sparsity levels when compressing multilingual LLMs while simultaneously minimizing the performance drop across different languages in the models, leaving no language behind after compression.Specifically, as shown in Figure 1, MBS samples the calibration data of different languages proportionally to the language distribution of the model training dataset.This approach effectively addresses the multilingual compression problem compared to previous monolingual sampling methods.Furthermore, we observed the dynamics of language interaction during compression and drew two main conclusions: 1) The larger the proportion of a language in the model training dataset, the more resistant it is to compression.2) The more similar the downstream language is to the calibration language, the less performance drop it obtained after compression.We further propose a measure of similarity among languages to explain and predict the performance drop.</p>
<p>The experiments were conducted on BLOOM (BigScience Workshop, 2022), one of the most effective open-source multilingual LLM models.We sample the calibration data from CC-100 (Wenzek et al., 2020), a widely used dataset of web-crawled data containing 100+ languages.The perplexity of languages is tested on XL-Sum (Hasan et al., 2021), a dataset that contains high-quality articles from BBC covering 45 languages.Experimental results demonstrate that MBS enhances the performance of GPTQ, SparseGPT, and Wanda compared to using only English calibration data.We want to further highlight that MBS is applicable to all compression methods that involve the use of calibration data, especially those following the OBS/OBD framework (Hassibi et al., 1993;Le Cun et al., 1989), which necessitates approximations of second-derivative information.</p>
<p>Background</p>
<p>Optimal Brain Surgeon (OBS)</p>
<p>Optimal Brain Surgeon (Hassibi et al., 1993) is a classic network pruning algorithm.It assumes that a network's error converges to a local minimum and calculates the second-order derivatives (Hessian matrix H) of the error (E) with respect to each parameter (w) to determine which connections can be safely pruned without significantly affecting performance.The increase in error (L j ) when a parameter (w j ) is set to zero, and the optimal adjustment (δw) of the remaining weights to compensate for the removal are given by:
L j = 1 2 w j 2 [H −1 ] jj (1) δw = − w j [H −1 ] jj H −1 :,j .(2)</p>
<p>Error Measurement</p>
<p>The network's error can be expressed in terms of the l 2 -error between the outputs before and after compression (Hubara et al., 2021).Given inputs X (the training dataset), the original weights W, the updated weights Ŵ, and a sparsity mask M of the same size as W, the error is defined as:
E = ||WX − (M ⊙ Ŵ)X|| 2 2 .(3)
In the case of quantization, the mask is a matrix filled with ones.The second-order derivatives (H) of the error with respect to the parameters are therefore represented as H = 2XX T , which forms the basis of our approximation objective.</p>
<p>SparseGPT, Wanda and GPTQ</p>
<p>To assess the importance of parameters, SparseGPT and Wanda employ different pruning metrics.Taking inspiration from OBS, SparseGPT defines its metric as S i,j = [|W| 2 /diag((X T X + λI) −1 )] i,j , with λ being the Hessian dampening factor to prevent inverse computation collapse.On the other hand, Wanda uses S i,j = |W i,j | • ||X j || 2 as its pruning metric.</p>
<p>Remarkably, these two metrics are essentially equivalent when λ is set to 0, and only the diagonal elements of the Hessian matrix X T X + λI are retained:
diag(((X T X + λI) ⊙ I) −1 ) = (||X j || 2 2 ) −1 . (4)
This assumption aligns with the practice of Optimal Brain Damage (Le Cun et al., 1989), which retains only the diagonal elements of the second-order derivatives matrix.Consequently, we can conclude that:
S SparseGP T = S 2 W anda
(5)</p>
<p>if we disregard the non-diagonal elements of H.The primary distinctions between SparseGPT and Wanda are as follows:</p>
<p>• SparseGPT retains the non-diagonal elements of the Hessian metrics, whereas Wanda takes the opposite approach.• SparseGPT performs adjustments (δw) on non-pruned parameters to compensate for removal, while Wanda does not.</p>
<p>Equally inspired by OBD, the quantization formulas provided by GPTQ are as follows:
w j = argmin wj (quant (w j ) − w j ) 2 [H −1 ] jj (6) δ F = − w j − quant (w j ) [H −1 ] jj • H −1 :,j(7)
Here, w j represents the greedy-optimal weight to quantize next, δ F denotes the corresponding optimal update of weights, and quant(w) rounds the value of w to the nearest point on the quantization grid.It's evident that these formulas follow a similar pattern to the OBD/OBS approach, and the information of the Hessian matrix H is crucial in all these methods.</p>
<p>Is Monolingual Calibrating Applicable to Multilingual MC?</p>
<p>Previous model compression methods only use English corpus as the sole calibration data, neglecting other languages.This raises the question: how does monolingual calibration impact the performance of other languages during multilingual model compression?In this section, we aim to explore this issue theoretically, focusing on two main aspects: the proportion of languages in the training data, and the similarity between languages.</p>
<p>Further experimental analysis will be provided in Section 5.3.We denote the total error of the model as E, and the error on language m as E m .We know that model training convergence applies to the whole training dataset.Thus, E resides in a local minimum.However, for languages m and n, E m and E n may not necessarily be in their own local minima.</p>
<p>This also explains the presence of the multilingual curse (Conneau et al., 2020), where the performance of a multilingual model in all languages is lower than that of a monolingual model with the same configuration.This occurs because the model is in a global local minimum, rather than individual local minima for each language.Due to their differing distributions, the local minima for each language do not overlap.The reason why using larger language models can alleviate this problem might be that, with a huge amount of parameters, they can simulate a distribution sophisticated enough where different languages' local minima are close.</p>
<p>Proportion in training data</p>
<p>Due to the fact that the size of English corpus is much larger than low-resource languages, we may suppose a language pair m and n with a significantly different training corpus size (p n &gt;&gt; p m ).</p>
<p>Intuitively, we can assume that languages with larger corpora in the training set tend to have their minimum error closer to the minimum of E because they contribute more weight to the total error.This characteristic makes them more robust against compression.Conversely, languages with smaller corpora inherently have their minimum error farther from the minimum of E, and compression can potentially push them even further away.This phenomenon is manifested in the following way illustrated in Figure 2: when compressing models with only the calibration data of the wellrepresented 1 language n, it has a significant impact on the performance of the underrepresented language m.However, compressing models with only the calibration data of the underrepresented language m has a comparatively minor impact on the performance of the well-represented language n.</p>
<p>Similarity between languages</p>
<p>In the second scenario, we may suppose that the two languages are as well-represented as each other (p m ≈ p n ).According to Equation 1, the priority of compression is fully determined by H, so it is sufficient to compare H m and H n .We may suppose the non-diagonal elements are trivial (Le Cun et al., 1989) to calculate the inverse of H.The metric is thus simplified to S = |W| • ||X|| 2 , so we can directly compare ||X|| 2 , which is a vector of length q (number of parameters), and each of the elements is the sum of the square of the inputs at the corresponding position.</p>
<p>A classic method to compare the similarity of two vectors is cosine similarity.The choice of cosine similarity over Euclidean distance is motivated by the need to compare two vectors based on the likelihood that their largest components remain consistent after undergoing the same element-wise multiplication with unknown vectors (model parameters).This can be modeled as the comparison of two vectors after they have experienced the same coordinate axis transformation, assessing whether their largest components remain identical.Clearly, when two vectors have a smaller angle between them, the likelihood that their largest components remain the same after undergoing the same coordinate axis transformation is relatively higher (demonstrated in Figure 3).</p>
<p>Figure 3: The angle between language 2 and language 3 is smaller than that between language 1 and language 2. After element-wise multiplication, language 2 and 3 are more likely to prioritize the same parameter w 1 because their angle before multiplication is smaller.</p>
<p>However, it's important to acknowledge that cosine similarity does not fulfill the properties of a distance metric, particularly the triangle inequality.Consequently, we cannot directly deduce the similarity between languages 1 and 3 from the similarities between 1 and 2, and 2 and 3.However, the property of a distance metric is less critical in the context of our work, since our goal is only to compare the similarity between the calibration language and the non-calibration languages, rather than among non-calibration languages.</p>
<p>We can compute the cosine similarity between ||X m || 2 and ||X n || 2 .When they are similar, using only data of language m as calibration data will introduce little performance drop in language n, and vice versa.That is to say, when two languages are very different, employing data from just one of the two languages as calibration data will lead to a significant performance decrease in the other.</p>
<p>Multilingual Brain Surgeon (MBS)</p>
<p>To mitigate interference among languages in multilingual model compression, we introduce Multilingual Brain Surgeon (MBS), a method that proportionally samples calibration data from different languages based on their distribution in the model training dataset.We provide additional theoretical details as follows.</p>
<p>In the OBD/OBS framework, we treat the error (E) as a whole.This makes sense for monolingual models since they contain only one language.However, for multilingual models, the error can be regarded as the sum of errors (E n ) associated with different languages.For a model trained on multiple languages, we can express the total error as follows:
E = E 1 + E 2 + E 3 + . . . + E n . (8)
Consequently, the Hessian matrix can be represented as the sum of Hessian matrices for each language:
H = H 1 + H 2 + H 3 + . . . + H n ,(9)
where
H n = X n T X n .
Here, X n represents the inputs (training data) for language n, with a shape of q × p n , where q is the total number of network parameters, and p n is the total number of training samples for language n.</p>
<p>Let's denote a subset of training data as
X [k]
n .Then, we have:
H n = X n T X n = pn k=1 X [k] n T X [k] n ,(10)
which leads to:
H = p1 k=1 X [k] 1 T X [k] 1 + p2 k=1 X [k] 2 T X [k] 2 +. . .+ pn k=1 X [k] n T X [k] n .
(11) It's evident that each language's contribution to H depends on its representation in the model's training data.Therefore, when selecting calibration data, it's essential to choose samples from each language in proportion to its presence in the training set.Specifically, for language n, the percentage of its representation in the training set is p n /p, where p is the total number of training samples.Thus, we should allocate a proportionate amount of data from language n (i.e., p n /p percent) in the calibration data used for compression.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Models.The experiments were conducted using the BLOOM (BigScience Workshop, 2022) model family, which is recognized as one of the most effective open-source multilingual LLMs.Our primary tests were performed on both the BLOOM-560m and BLOOM-7b1 models to provide insights into the performance of smaller and larger models.For the network pruning experiments, a pruning sparsity of 50% was applied.In the quantization experiments, the models were quantized to 3 bits precision with groupings of size 1024.</p>
<p>Datasets &amp; Language Selection.For calibration data, we selected CC-100 (Wenzek et al., 2020), a dataset comprising web-crawled content in over 100 languages, similar to the setup used by previous studies like Frantar and Alistarh (2023), Sun et al. (2023), andFrantar et al. (2023) which used a monolingual English dataset called C4 (Raffel et al., 2019).</p>
<p>To evaluate multilingual perplexity, we employed XL-Sum (Hasan et al., 2021), a dataset containing high-quality articles from BBC covering 45 languages, as our benchmark.Additionally, we assessed perplexity on the test sets of raw-WikiText2 (Merity et al., 2016), a widely used English perplexity benchmark.Due to resource limitations for certain languages in the BLOOM model, we conducted experiments on a subset of 20 languages, which were those available in CC-100, XL-Sum, and BLOOM.These languages include Arabic (ar), Bengali (bn), Chinese simplified (zh-Hans), Chinese traditional (zh-Hant), French (fr), Gujarati (gu), Hindi (hi), Igbo (ig), Indonesian (id), Marathi (mr), Nepali (ne), Portuguese (pt), Spanish (es), Swahili (sw), Tamil (ta), Telugu (te), Urdu (ur), Vietnamese (vi), and Yoruba (yo).</p>
<p>Evaluation.We evaluated the perplexity of the compressed model separately for each language using XL-Sum.We also conducted zero-shot evaluations, employing the widely recognized EleutherAIeval-harness (Gao et al., 2021), with a focus on multilingual tasks to assess the performance of less-represented languages.The zero-shot tasks that we have chosen to evaluate the compressed model are specified in Table 1.</p>
<p>Calibration data &amp; Baselines.Our calibration data consisted of 256 segments, each containing 2048 tokens, sampled from CC-100.We used our MBS sampling method and sampled 87, 47, 37, 31, 14, 13, 7, 4, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 segments respectively for en, zh-Hans, fr, es, pt, ar, vi, hi, id, bn, ta, te, ur, ne, mr, gu, zh-Hant, sw, yo and ig.Additionally, we conducted tests in the Equal MBS setting, in which an equal number of segments were sampled from each language.We also implemented the monolingual compression setting, using 256 segments from the same language.</p>
<p>Language Similarity Study.To study language similarity, we conducted monolingual pruning on English and Igbo, representing the best-represented and worst-represented languages in our dataset, respectively.We also performed similar experiments on Urdu and Tamil, which respectively represent the least and most similar languages to the others (further explanation is provided in the results section).To compare language similarity, we utilized the representations after the embedding layer of the BLOOM model, as the compression algorithms do not affect the embedding layer.</p>
<p>Main results</p>
<p>We conducted our MBS sampling technique to compress both BLOOM-7b1 and BLOOM-560m models, using GPTQ, SparseGPT, and Wanda.The trends observed in the results for these two models are similar.For the sake of better formatting, we will present the results for the 7b1 model in the main text and provide the results for the 560M model in the appendices.</p>
<p>Perplexity. Figure 4 presents the evaluation of perplexity for each language after compression on the BLOOM-7b1 model.The baselines consist of monolingual compression using English-only calibration data.</p>
<ol>
<li>Across various compression methods, the MBS sampling technique consistently leads to minimal increases in perplexity.This holds true whether we utilize Wanda or SparseGPT for pruning or GPTQ for quantization.2. For underrepresented languages (located on the right side of the axis), MBS can notably reduce the increase in perplexity after compression, thus preserving the model's capacity for lower-resourced languages, leaving no languages behind.3.Even for the most well-represented language, specifically English (on both datasets "en" and "wikitext2"), using MBS sampling introduces a lower perplexity than its monolingual English-centric sampling counterpart.</li>
</ol>
<p>Zero shot tasks.</p>
<p>Table 1 provides an overview of the performance of zero-shot tasks after the compression process.The results demonstrate that, in the majority of tasks, utilizing MBS sampling yields superior performance compared to other sampling techniques.Furthermore, the performance after compression closely approximates that of the dense model, highlighting the effectiveness of our approach.</p>
<p>Equal MBS.Table 2 provides the results for Equal MBS, where an equal number of samples are taken from each language.While Equal MBS is not the optimal setting, it generally improves the performance of the compressed model.This demonstrates that even without access to the distribution of languages in the training set, Equal MBS can still enhance compression results for the chosen languages, showcasing the versatility of our method.</p>
<p>Monolingual Compression Study</p>
<p>Factor 1: Proportion in training data</p>
<p>To investigate how the proportion of a language in the training data affects compression results, we selected English (en) and Igbo (ig), which have the largest and smallest proportions in the training data among the languages in our experiments, respectively.The results are presented in Figure 5.</p>
<p>It is evident that if we use only English as our calibration data, it significantly impacts less wellrepresented languages, causing substantial increases in perplexity, particularly for Marathi (mr) and Gujarati (gu).However, for better-represented languages, English (en) has a relatively smaller influence, as observed with Chinese simplified (zh-Hans), French (fr), and Spanish (es).Conversely, when we use only Igbo as our calibration data, the increase in perplexity for the other languages is relatively small.Clearly, languages with a lower representation in the training set tend to experience a more substantial increase in perplexity.</p>
<p>Factor 2: Similarity between languages</p>
<p>We calculated the cosine similarity of ||X n || 2 2 for different languages using BLOOM-7b1, and then converted this similarity into degrees.This allowed us to create a distance map between languages.To visualize the relative positions of different languages, we employed Multidimensional scaling (Mead, 1992) and generated a 2-dimensional figure (Figure 6).The original distance map is included in the appendices.</p>
<p>Upon observing this graph, we can identify some interesting clusters.Typically, languages from different language families tend to form distinct clusters.For instance, there is a cluster comprising Indo-European languages such as English, Spanish, French, and Portuguese, a cluster for Chinese simplified and Chinese traditional, both of which are Chinese languages, and another cluster consisting of Niger-Congo languages like Yoruba, Igbo, and Swahili.This clustering may be attributed to the following factors:</p>
<p>• Shared Grammar Structure: Languages within the same language family often share similar grammar structures.• Shared Tokens: During the tokenization process, these languages frequently share tokens, including prefixes, suffixes, and other wordbuilding elements.</p>
<p>To investigate how language similarity impacts compression outcomes, we chose to examine two extreme cases: Tamil (ta), which is the language that is "closest" to all other languages (with an average distance of 7.25), and Urdu (ur), which is the language that is "farthest" from all other languages (with an average distance of 15.45).The results for Wanda are displayed in Figure 7, while the results for SparseGPT, which exhibit similar patterns to those of Wanda, are provided in the appendices.It is evident that when we use Tamil (ta) as our sole calibration data, the increase in perplexity for other languages is relatively small, especially for languages that are "closer" to Tamil, such as Bengali (bn) and Hindi (hi).Conversely, when Urdu (ur) serves as our sole calibration data, the increase in perplexity for other languages is relatively significant on average.The consistent pattern across all four graphs reveals that languages more distant from the language being compressed tend to exhibit a more significant increase in perplexity.</p>
<p>An intriguing case study can be conducted on Chinese simplified and Chinese traditional.Despite their close proximity on the language map, they sig-nificantly differ in corpus size.Chinese simplified enjoys a much larger proportion, resulting in a more pronounced impact on Chinese traditional after the compression process, while Chinese simplified remains relatively unaffected.These experiments demonstrate the validity and accuracy of our theory.</p>
<p>Related Work</p>
<p>Large Language Model.</p>
<p>Large Language Models (Zhao et al., 2023) like GPT-4(OpenAI et al., 2024), LLaMA(Touvron et al., 2023) and OPT (Zhang et al., 2022), which have revolutionized Natural Language Processing through their ability   to understand and generate nuanced text.Alongside, multilingual language models (Doddapaneni et al., 2021) such as BLOOM(BigScience Workshop, 2022) and XLM-R (Conneau et al., 2020) are breaking language barriers by learning universal representations from texts across numerous languages.These developments underscore a significant shift towards creating more versatile and inclusive NLP systems, with research focusing on architectural innovations, training efficiencies, and cross-lingual capabilities to enhance global digital interaction.</p>
<p>We would like to emphasize that MBS can be applied to any model compression method that utilizes calibration data, particularly methods based on the OBS/OBD framework, where the approximation of Figure 6: Distance map of different languages associated with their corresponding language families.We can see that languages with the same family cluster together from this map.second-derivative information is required.Thanks to a survey on model compression for large language models by (Zhu et al., 2023), we examined the state-of-the-art model compression methods for large language models, and we found that our MBS is useful for almost all of them.</p>
<p>Pruning and quantization are two major model compression methods for LLMs.</p>
<p>Pruning.Pruning reduces model size and complexity by eliminating unnecessary or redundant components.It can be categorized into structured pruning, where higher-granularity structures like rows or columns of weight matrices are removed, and unstructured pruning, which eliminates in- dividual weights, leading to irregular sparse structures.In the domain of unstructured pruning, MBS can be applied to Wanda (Sun et al., 2023) and SparseGPT (Frantar and Alistarh, 2023) that we presented in the background section, and also LoRAPrune (Zhang et al., 2023).In the structured pruning domain, MBS can empower LLM-Pruner (Ma et al., 2023).</p>
<p>Quantization.Quantization involves converting floating-point numbers into lower bit-level representations, integers, or other discrete forms and can be categorized into Quantization-Aware Training and Post-Training Quantization.MBS finds numerous applications in quantization, particularly in post-training quantization.In post-training quantization, certain approaches focus on quantizing only the weights of LLMs.Among these methods, MBS can be applied to AWQ (Lin et al., 2023), GPTQ (Frantar et al., 2023), OWQ (Lee et al., 2023), SpQR (Dettmers et al., 2023), SqueezeLLM (Kim et al., 2023), QuIP (Chee et al., 2023), and SignRound (Cheng et al., 2023).Some other methods try to quantize both weights and activations of LLMs.Among them, MBS can be applied to SmoothQuant (Xiao et al., 2023), RPTQ (Yuan et al., 2023), OliVe (Guo et al., 2023), ZeroQuant-V2 (Yao et al., 2023), Outlier Suppression+ (Wei et al., 2023), FPTQ (Li et al., 2023), QuantEase (Behdin et al., 2023), and OmniQuant (Shao et al., 2023).</p>
<p>Conclusions</p>
<p>In summary, the Multilingual Brain Surgeon (MBS) is a groundbreaking approach for improving multilingual LLMs.It tackles the English-centric bias in existing techniques and enhances LLM performance after compressing.Our experiments on the BLOOM model highlight the effectiveness of MBS, benefiting pruning and quantization methods like SparseGPT, Wanda, and GPTQ.</p>
<p>We also studied language interaction during compression, finding that language proportion in the training dataset and language similarity are crucial factors.Languages with larger proportion are less affected by compression, while similar languages perform better when only one language is used in calibration data.Our proposed similarity measure accurately predicts performance drops in such scenarios.</p>
<p>This research not only enhances the practicality of multilingual LLMs compression methods but also maintains language coverage, making multilingual NLP applications more inclusive and powerful.</p>
<p>Guzmán, Armand Joulin, and Edouard Grave.2020.CCNet: Extracting high quality monolingual datasets from web crawl data.</p>
<p>In Proceedings of the 12th Language Resources and Evaluation Conference, pages 4003-4012, Marseille, France.European Language Resources Association.Table 3: The number of segments taken from each language by each sampling method.</p>
<p>A. Details of Calibration Data</p>
<p>The number of segments taken from each language is detailed in Table 3.We rounded up the segment counts for languages with fewer than one segment to ensure their representation in the calibration data.In the equal sampling scenario, to maintain comparability, some languages have one segment less than others to achieve a total of 256 segments.</p>
<p>B. MBS results tables</p>
<p>Perplexity.Table 6 showcases the perplexity evaluation for each language after pruning on the BLOOM-560m model.The observed trends align closely with those observed on the BLOOM-7b1 model.</p>
<p>Zero shot tasks.Table 7 illustrates the zero-shot task results for the pruned BLOOM-560m model.It is noticeable that the average accuracy using the MBS sampling method continues to outperform the baselines, although the results appear to exhibit more variability.This variability can be attributed to the reduced capacity of smaller models to maintain their multilingual capabilities.</p>
<p>The role of parameter compensation.We have observed a notable distinction in the effects of SparseGPT and Wanda.In monolingual pruning, SparseGPT, which involves parameter updates and employs a more precise pruning metric, appears to have a more detrimental impact on less wellrepresented languages.However, when we apply MBS, SparseGPT continues to outperform Wanda.This phenomenon may be attributed to the fact that smaller models are more sensitive to parameter updates.A biased Hessian matrix can exacerbate the model's divergence from the correct direction through these updates.Conversely, a correctly approximated Hessian matrix can effectively guide the pruning in the correct direction.</p>
<p>C. Monolingual pruning results</p>
<p>Distance map of BLOOM-560m model.The distance map depicting the relationships between languages in the BLOOM-560m model is depicted in Figure 8.We can discern a similar clustering pattern to that observed in the BLOOM-7b1 model.The original distance matrices are provided in the following tables (12,13) 6: Perplexity for each language and their respective increases when compared to the dense BLOOM-560m model after pruning.Evaluation performed on XL-Sum and WikiText2 datasets.From top to bottom, languages are ranked in order from the most well-represented to the least represented.</p>
<p>Figure 1 :
1
Figure 1: MBS samples calibration data from different languages proportionally to the language distribution of training datasets.This approach (right part) effectively addresses the multilingual compression problem compared to previous monolingual sampling methods (left part).</p>
<p>Figure 2 :
2
Figure 2: Languages with larger corpora have their minimum error closer to the minimum of E. Monolingual compression effectively "pushed" the model's state towards the minimum error of that particular language.</p>
<p>Figure 4 :
4
Figure 4: Perplexity for each language and their respective increases when compared to the dense BLOOM-7b1 model after pruning (left) or quantization (right).From left to right, languages are ranked in order from the most well-represented to the least represented.</p>
<p>Figure 5 :
5
Figure 5: Monolingual pruning results using Wanda with calibration data in English or Igbo.The size of each bubble corresponds to the magnitude of the increase in perplexity for the model in that particular language, while the vertical axis represents the size of training data in log(bytes) from the language in the training set of BLOOM.The languages with a smaller proportion in the training set experience a greater increase in perplexity.</p>
<p>Figure 7 :
7
Figure 7: Similarly to Figure 5, but focusing on Urdu or Tamil.The languages less similar to the calibration language experience a greater increase in perplexity.</p>
<p>Figure 8 :
8
Figure 8: The graph illustrates the relative positions of different languages.Different dot shapes represent different language families.The closer they are on the graph, the more similar they are to each other.</p>
<p>Table 1 :
1Accuracy of 0-shot taskDenseWandaWanda +MBSSparseGPTSparseGPT +MBSGPTQGPTQ +MBSxcopa↑id69.80% 67.20% 67.40%65.60%66.40%67.20% 67.40%sw51.60% 54.80% 53.80%55.20%51.20%54.60% 55.00%ta59.20% 61.20% 57.80%60.60%58.60%58.40% 57.80%vi70.80% 69.80% 67.20%66.80%66.40%67.00% 68.20%zh65.20% 62.00% 63.60%62.20%63.80%61.00% 62.60%Average63.32% 63.00% 61.96%62.08%61.28%61.64% 62.20%xstory_cloze↑ar58.57% 53.94% 54.93%54.93%56.32%56.45% 57.18%en70.75% 68.23% 67.70%69.23%68.96%68.70% 68.96%es66.12% 64.39% 63.20%62.87%64.39%64.53% 64.79%hi60.56% 56.92% 57.18%57.64%58.44%58.04% 58.17%id64.46% 59.96% 60.29%59.23%61.81%60.89% 62.54%sw53.94% 50.89% 51.69%50.69%52.02%52.28% 52.95%te57.45% 56.52% 56.72%56.78%57.97%57.18% 57.71%zh61.88% 58.37% 59.56%57.91%60.89%60.03% 60.03%Average61.71% 58.65% 58.91%58.66%60.10%59.76% 60.29%xwinograd↑en82.15% 79.40% 78.88%80.09%79.74%79.35% 79.57%fr71.08% 71.08% 67.47%72.29%73.49%65.06% 67.47%pt76.81% 74.14% 75.29%71.48%74.14%69.20% 72.24%zh74.40% 74.40% 75.79%74.40%75.20%71.23% 73.81%Average76.11% 74.76% 74.36%74.57%75.64%71.21% 73.27%pawsx↑en61.30% 53.60% 54.75%57.50%58.25%56.75% 58.60%es59.35% 51.75% 54.05%54.10%56.60%57.95% 56.10%fr50.90% 47.45% 46.45%50.85%47.10%52.30% 48.60%zh47.35% 45.05% 45.45%45.70%47.45%49.10% 50.00%Average54.73% 49.46% 50.18%52.04%52.35%54.03% 53.33%xnli↑ar33.83% 33.67% 33.91%34.89%34.51%33.67% 34.75%en53.91% 52.20% 52.59%53.49%53.49%52.73% 52.93%es48.70% 48.14% 47.47%45.13%46.81%46.63% 47.54%fr49.68% 43.57% 48.38%46.29%49.00%48.58% 48.62%hi46.51% 42.63% 44.51%40.60%45.97%44.19% 46.63%sw37.92% 38.36% 37.80%37.35%36.29%36.63% 37.33%ur42.10% 39.82% 40.54%40.42%39.58%38.42% 41.98%vi47.05% 45.99% 46.35%42.46%44.89%44.29% 46.09%zh35.43% 35.31% 33.99%34.57%34.21%35.27% 34.71%Average43.90% 42.19% 42.84%41.69%42.75%42.27% 41.35%Average↑57.63% 55.36% 55.49%55.38%56.13%55.59% 57.08%
0-shot task performance of BLOOM-7b1 with different model compression methods.</p>
<p>Compression Methods Average 0-shot Task Accuracy↑ Average ppl↓
Wanda55.36%64.70Wanda+ Equal MBS55.20%24.97Wanda+ MBS55.49%26.28SparseGPT55.38%59.84SparseGPT+ Equal MBS55.86%22.62SparseGPT+ MBS56.13%23.82GPTQ55.59%31.17GPTQ+ Equal MBS56.52%23.15GPTQ+ MBS57.08%24.26</p>
<p>Table 2 :
2
Performance of Equal MBS, where an equal number of segments are sampled from each language.</p>
<p>Table 4 :
4
Perplexity for each language and their respective increases when compared to the dense BLOOM-7b1 model after pruning.Evaluation performed on XL-Sum and WikiText2 datasets.From top to bottom, languages are ranked in order from the most well-represented to the least represented.
Dataset Dense Wanda ↑SparseGPT ↑MBS + Wanda↑MBS + SparseGPT↑en13.6815.6715%14.929%15.5514% 15.0110%zh-Hans 23.7034.7547%35.5650%26.5912% 25.879%fr9.5913.1637%13.4140%10.6811% 10.398%es10.7113.8229%13.7528%11.9111% 11.598%pt10.9714.5833%17.3758%12.2512% 11.969%ar14.4029.19103% 25.3376%16.4514% 15.8810%vi10.1614.7645%15.0048%11.5914% 11.2411%hi10.9618.2667%19.0974%12.5214% 12.1911%id20.4829.3743%37.2782%23.7616% 22.9712%bn17.2733.3793%40.50134% 20.2917% 19.5113%ta16.5542.23155% 44.10167% 20.1021% 19.3417%te18.1064.97259% 69.20282% 24.5936% 22.0522%ur13.2627.03104% 30.56130% 15.8319% 15.1014%ne27.22152.67 461% 148.00444% 34.9028% 32.9121%mr23.07176.78 666% 144.65527% 32.2540% 28.9125%gu21.52184.62 758% 118.48450% 30.8443% 26.9725%zh-Hant21.84113.34 419% 102.26368% 24.9614% 24.3011%sw34.35145.54 324% 135.84295% 54.3258% 44.2329%yo53.29128.12 140% 126.54137% 79.6249% 67.5227%ig39.1690.41131% 90.89132% 59.0051% 49.1025%wikitext2 11.3716.1542%13.9122%13.8222% 13.2617%Average 20.0864.70222% 59.84198% 26.2831% 23.8219%DatasetDense GPTQ GPTQ+MBSen13.6815.315.37zh-Hans23.728.6926.28fr9.5910.9410.46es10.7112.4911.9pt10.9713.0512.36ar14.417.3516.12vi10.1611.9711.15hi10.9613.7212.27id20.4825.3623.45bn17.2722.8819.83ta16.5524.519.53te18.132.8322.67ur13.2618.2915.4ne27.2245.533.97mr23.0743.3229.57gu21.5240.427.83zh-Hant21.8426.8524.75sw34.3568.4246.19yo53.2998.4667.82ig39.1671.5649.96wikitext2 11.3712.612.56Average20.0831.1724.26</p>
<p>Table 5 :
5
Perplexity for each language of BLOOM-7b1 model before and after quantization.</p>
<p>.
DatasetDense WandaSparseGPTMBS + WandaMBS + SparseGPTen13.6834.3032.0235.0832.87zh-Hans 23.7072.92101.7962.3959.28fr9.5922.8924.9921.6620.37es10.7125.4127.9423.9822.56pt10.9727.4734.9925.8224.27ar14.4080.091.13E+1441.3647.57vi10.1643.26127.5930.7529.64hi10.9644.101.20E+1529.1743.84id20.48109.1376038.8364.4079.85bn17.27102.784.42E+2356.65121.11ta16.55176.251.71E+0764.40209.90te18.10355.986.96E+05116.20 355.54ur13.2697.653.62E+2044.6061.35ne27.22555.211.27E+17135.67 200.16mr23.07503.514.25E+12142.76 619.28gu21.52327.112.17E+13136.47 197.21zh-Hant 21.84137.00266.4961.3058.56sw34.35919.146011.36357.63 336.80yo53.291.02E+03 3557.88542.40 450.00ig39.16728.011306.30370.35 307.76wikitext2 11.3730.5829.7531.0929.90Average 20.08257.992.11E+22114.01 157.51Table</p>
<p>Table 8 :
8
Monolingual pruning results using Wanda on BLOOM-7b1 with calibration data in en, ig, ta, and ur.Perplexity evaluated on XL-sum and wikitext2.Languages are ranked from the most well-represented to the least represented, from top to bottom.
DatasetDense en↑ig↑ta↑ur↑en13.6815.6715%17.61 29%17.2026%18.9338%zh-Hans23.7034.7547%33.97 43%31.9935%31.7034%fr9.5913.1637%12.70 32%13.9445%14.8455%es10.7113.8229%14.16 32%15.8348%17.1760%pt10.9714.5833%14.60 33%15.7844%18.2066%ar14.4029.19103%26.27 82%22.8258%17.6523%vi10.1614.7645%12.74 25%13.4833%14.6244%hi10.9618.2667%15.82 44%13.1820%13.9728%id20.4829.3743%27.30 33%27.4834%33.9666%bn17.2733.3793%26.71 55%21.9427%30.1174%ta16.5542.23155%27.14 64%19.3117%24.4448%te18.1064.97259%39.77 120% 25.5841%37.68108%ur13.2627.03104%25.07 89%19.7049%15.3816%ne27.22152.67 461%64.74 138% 46.5971%46.6071%mr23.07176.78 666%66.86 190% 45.7198%68.97199%gu21.52184.62 758%48.30 124% 35.1763%37.9676%zh-Hant21.84113.34 419%49.41 126% 56.20157% 36.8169%sw34.35145.54 324%58.74 71%91.94168% 125.09 264%yo53.29128.12 140%72.86 37%127.05 138% 180.01 238%ig39.1690.41131%51.30 31%91.28133% 149.15 281%wikitext2 11.3716.1542%16.25 43%14.1424%15.6838%Average 20.0864.70189.1% 34.40 68.7% 36.4963.4% 45.1990.2%DatasetDense enigtauren13.6814.929%16.28 19%16.5321%16.8023%zh-Hans23.7035.5650%30.31 28%31.4533%33.0539%fr9.5913.4140%11.42 19%12.7132%12.7533%es10.7113.7528%12.82 20%14.1232%14.5836%pt10.9717.3758%13.49 23%14.8635%15.3840%ar14.4025.3376%18.54 29%19.3134%16.8717%vi10.1615.0048%12.07 19%13.2631%13.0729%hi10.9619.0974%14.44 32%12.7316%12.4614%id20.4837.2782%25.56 25%27.3433%26.8431%bn17.2740.50134%24.21 40%21.0722%21.8526%ta16.5544.10167%26.36 59%17.848%22.5836%te18.1069.20282%31.56 74%24.1233%28.6658%ur13.2630.56130%19.20 45%17.3231%14.127%ne27.22148.00 444%47.77 76%42.6257%40.1247%mr23.07144.65 527%43.74 90%37.8164%49.41114%gu21.52118.48 450%39.39 83%34.8762%32.8753%zh-Hant21.84102.26 368%30.51 40%50.00129% 43.85101%sw34.35135.84 295%46.74 36%72.81112% 72.94112%yo53.29126.54 137%61.08 15%105.97 99%113.09 112%ig39.1690.89132%41.37 6%74.4490%86.14120%wikitext2 11.3713.9122%13.56 19%13.6620%13.9523%Average 20.0859.84169.3% 27.64 37.8% 32.1447.3% 33.4051.1%</p>
<p>Table 9 :
9
Monolingual pruning results using SparseGPT on BLOOM-7b1 with calibration data in en, ig, ta, and ur.Perplexity evaluated on XL-sum and wikitext2.Languages are ranked from the most well-represented to the least represented, from top to bottom.
DatasetDense enigteiden13.6834.30151%49.17259%53.12288%38.50181%zh-Hans23.7072.92208%87.98271%126.56 434%79.06234%fr9.5922.89139%32.46238%41.75335%24.91160%es10.7125.41137%38.29258%47.38343%28.18163%pt10.9727.47150%42.04283%57.68426%30.47178%ar14.4080.09456%79.26451%66.66363%55.03282%vi10.1643.26326%39.63290%150.26 1379% 38.05275%hi10.9644.10303%50.54361%32.37195%36.25231%id20.48109.13433%108.26 429%952.66 4551% 60.77197%bn17.27102.78495%141.25 718%68.01294%83.81385%ta16.55176.25965%147.99 794%60.52266%163.29 887%te18.10355.981866% 260.31 1338% 95.77429%445.67 2362%ur13.2697.65636%143.11 979%49.14271%63.39378%ne27.22555.211940% 320.61 1078% 143.19 426%237.41 772%mr23.07503.512083% 290.52 1159% 140.02 507%220.62 856%gu21.52327.111420% 215.13 900%124.55 479%286.66 1232%zh-Hant21.84137.00527%156.15 615%227.56 942%167.85 668%sw34.35919.142576% 419.52 1121% 897.07 2511% 465.66 1256%yo53.291024.94 1823% 529.35 893%711.20 1235% 573.17 976%ig39.16728.011759% 285.28 629%773.60 1876% 409.11 945%wikitext2 11.3730.58169%40.70258%46.07305%33.40194%Average 20.08257.99883.9% 165.60 634.4% 231.67 850.2% 168.63 610.0%</p>
<p>Table 10 :
10
Monolingual pruning results of Wanda on BLOOM-560m.
DatasetDense enigteiden1432434840zh-Hans24102128108110zh-Hant22266162227279fr1025283928es1128344832pt1135375435ar141.13E+14 8.10E+07 2473.52E+07vi101287094878hi111.20E+15 3.93E+10 1282.98E+09id2076039704911397 62bn174.42E+23 2.16E+10 17886.12E+15ta1717114940 361068851.80E+08te18696319180476752.40E+07ur133.62E+20 552838071.55E+07ne271.27E+17 1.70E+09 6366.43E+10mr234.25E+12 2.13E+08 12503.76E+11gu222.17E+13 30512524.96E+06sw34601146713631502yo5335583428821463ig391306205990753wikitext2 1130364134Average 202.11E+22 2.99E+09 10202.92E+14</p>
<p>Table 11 :
11
Monolingual pruning results of SparseGPT on BLOOM-560m.
enzh-Hanszh-Hantfresptarvihiidbntateurnemrguswyoigen010115679810698122014912101010zh-Hans100113151681231266511856161718zh-Hant111015171881441366510766171919fr5131502311512410101422161113667es6151720112715613121725181315756pt7161831012715513131725181316545ar988111212086865814848111212vi812145778010288121912810466hi103412151561001033310534141616id612134658210087121913810466bn966101313683802412633121414ta866101213583720412624111313te12551417178123124408452161818ur2011102225251419101912128071210232525ne14871618188125136647055161818mr956111313483832512504121313gu126613151681041034210540141617sw10161767511414412111623161214033yo10171965412616614131825181316301ig10181976512616614131825181317310Average9.39.410.19.2510.610.658.58.38.17.957.67.259.215.4510.27.458.7510.211.111.35
In the rest of the paper, we call a language "wellrepresented" when its proportion is relatively big in the model training set, and "underrepresented" when its proportion is relatively small.
AcknowledgmentsThis work is funded by the China NSFC Projects (92370206, U23B2057, 62106142 and 62120106006) and Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102).
Quantease: Optimization-based quantization for language models -an efficient and intuitive algorithm. Bibliographical References, Kayhan Behdin, Ayan Acharya, Aman Gupta, Sathiya Keerthi, Rahul Mazumder, 2023</p>
<p>BLOOM (revision 4ab0472). Bigscience Workshop, 10.57967/hf/00032022</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordIlya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</p>
<p>Quip: 2-bit quantization of large language models with guarantees. Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De Sa, 2023</p>
<p>Optimize weight rounding via signed gradient descent for the quantization of llms. Wenhua Cheng, Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He, Kaokao Lv, 2023</p>
<p>. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, 2020Unsupervised cross-lingual representation learning at scale</p>
<p>-bit matrix multiplication for transformers at scale. Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, Llm. 82022int8(</p>
<p>Spqr: A sparse-quantized representation for near-lossless llm weight compression. Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Alexander Saleh Ashkboos, Torsten Borzunov, Dan Hoefler, Alistarh, 2023</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2019</p>
<p>. Sumanth Doddapaneni, Gowtham Ramesh, M Mitesh, Anoop Khapra, Pratyush Kunchukuttan, Kumar, 2021A primer on pretrained multilingual language models</p>
<p>Rigging the lottery: Making all tickets winners. Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, Erich Elsen, CoRR, abs/1911.111342019</p>
<p>Sparsegpt: Massive language models can be accurately pruned in one-shot. Elias Frantar, Dan Alistarh, 2023</p>
<p>Gptq: Accurate posttraining quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, 2023</p>
<p>Giri Anantharaman, and Alexis Conneau. 2021. Larger-scale transformers for multilingual masked language modeling. Trevor Gale, Erich Elsen, Sara Hooker ; Naman, Jingfei Goyal, Myle Du, Ott, arXiv:2105.005722019arXiv preprintThe state of sparsity in deep neural networks</p>
<p>OliVe: Accelerating large language models via hardware-friendly outlier-victim pair quantization. Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan Yang, Yunxin Liu, Minyi Guo, Yuhao Zhu, 10.1145/3579371.3589038Proceedings of the 50th Annual International Symposium on Computer Architecture. the 50th Annual International Symposium on Computer ArchitectureACM2023</p>
<p>Learning both weights and connections for efficient neural network. Song Han, Jeff Pool, John Tran, William Dally, Advances in Neural Information Processing Systems. Curran Associates, Inc201528</p>
<p>XL-sum: Large-scale multilingual abstractive summarization for 44 languages. Tahmid Hasan, Abhik Bhattacharjee, Md Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M Sohel Rahman, Rifat Shahriyar, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Optimal brain surgeon and general network pruning. B Hassibi, D G Stork, G J Wolff, 10.1109/ICNN.1993.298572IEEE International Conference on Neural Networks. 19931</p>
<p>Accelerated sparse neural training: A provable and efficient method to find n:m transposable masks. Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Seffi Naor, Daniel Soudry, 2021</p>
<p>How well do sparse imagenet models transfer?. Eugenia Iofinova, Alexandra Peste, Mark Kurtz, Dan Alistarh, CoRR, abs/2111.134452021</p>
<p>Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, Kurt Keutzer, Squeezellm: Dense-and-sparse quantization. 2023</p>
<p>Inducing and exploiting activation sparsity for fast inference on deep neural networks. Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander Matveev, John Carr, Michael Goin, William Leiserson, Sage Moore, Bill Nell, Nir Shavit, Dan Alistarh, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningVirtual. PMLR2020119</p>
<p>Cross-lingual language model pretraining. Guillaume Lample, Alexis Conneau, 2019</p>
<p>Optimal brain damage. Yann Le Cun, John S Denker, Sara A Solla, Jason Wei, Akila Cj Weinmann, Peter Welihinda, Jiayi Welinder, Lilian Weng, Matt Weng, Dave Wiethoff, Clemens Willner, Samuel Winter, Hannah Wolrich, Lauren Wong, Sherwin Workman, Jeff Wu, Michael Wu, Kai Wu, Tao Xiao, Sarah Xu, Kevin Yoo, Qiming Yu, Wojciech Yuan, Rowan Zaremba, Chong Zellers, Marvin Zhang, Shengjia Zhang, Tianhao Zhao, Juntang Zheng, William Zhuang, Zhuk, Proceedings Jonathan Ward. Jonathan Ward1989ret Zoph. 2024. Gpt-4 technical report</p>
<p>Ac/dc: Alternating compressed/decompressed training of deep neural networks. Alexandra Peste, Eugenia Iofinova, Adrian Vladu, Dan Alistarh, 2021</p>
<p>Exploring the limits of transfer learning with a unified text-to. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, 2019text transformer. arXiv e-prints</p>
<p>Omniquant: Omnidirectionally calibrated quantization for large language models. Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, Ping Luo, 2023</p>
<p>A simple and effective pruning approach for large language models. Mingjie Sun, Zhuang Liu, Anna Bair, J Zico Kolter, 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. </p>
<p>Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong Liu, 2023</p>
<p>Smoothquant: Accurate and efficient posttraining quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song Han, 2023</p>
<p>Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He, 2022</p>
<p>Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation. Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong He ; Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, Bingzhe Wu, 2023. 2023Rptq: Reorder-based post-training quantization for large language models</p>
<p>Loraprune: Pruning meets low-rank parameter-efficient fine-tuning. Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang, 2023</p>
<p>. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, 2022Opt: Open pre-trained transformer language models</p>
<p>. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, 2023A survey of large language models</p>
<p>To prune, or not to prune: exploring the efficacy of pruning for model compression. Michael Zhu, Suyog Gupta, 2017</p>
<p>Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang, arXiv:2308.07633A survey on model compression for large language models. 2023arXiv preprint</p>
<p>. Language Resource, References Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony Dipofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle Mcdonell, 10.5281/zenodo.5371628Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wangand Andy Zou. 2021. A framework for few-shot language model evaluation</p>
<p>Pointer sentinel mixture models. Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, 2016</p>
<p>Exploring the limits of transfer learning with a unified text-to. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, 2019text transformer. arXiv e-prints</p>
<p>. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco</p>            </div>
        </div>

    </div>
</body>
</html>
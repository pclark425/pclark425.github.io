<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8696 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8696</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8696</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-156.html">extraction-schema-156</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <p><strong>Paper ID:</strong> paper-257984324</p>
                <p><strong>Paper Title:</strong> Representational formats of human memory traces</p>
                <p><strong>Paper Abstract:</strong> Neural representations are internal brain states that constitute the brain’s model of the external world or some of its features. In the presence of sensory input, a representation may reflect various properties of this input. When perceptual information is no longer available, the brain can still activate representations of previously experienced episodes due to the formation of memory traces. In this review, we aim at characterizing the nature of neural memory representations and how they can be assessed with cognitive neuroscience methods, mainly focusing on neuroimaging. We discuss how multivariate analysis techniques such as representational similarity analysis (RSA) and deep neural networks (DNNs) can be leveraged to gain insights into the structure of neural representations and their different representational formats. We provide several examples of recent studies which demonstrate that we are able to not only measure memory representations using RSA but are also able to investigate their multiple formats using DNNs. We demonstrate that in addition to slow generalization during consolidation, memory representations are subject to semantization already during short-term memory, by revealing a shift from visual to semantic format. In addition to perceptual and conceptual formats, we describe the impact of affective evaluations as an additional dimension of episodic memories. Overall, these studies illustrate how the analysis of neural representations may help us gain a deeper understanding of the nature of human memory.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8696.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8696.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perceptual / Visual format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perceptual (visual) representational format</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional-level format that encodes sensory features of experiences (colors, shapes, textures, low-level spatial/retinotopic information and higher-order visual object parts); typically supports image-like, detail-rich representations used in perception and early memory stages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Perceptual / Visual representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Represents concepts via sensory/visual features (e.g., color, texture, edges, shapes, object parts) at multiple levels of visual complexity; functionally this format preserves fine-grained, stimulus-specific information that can be reinstated for recognition or imagery.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>feature-based / distributed (sensory-feature coding)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Object recognition, encoding–retrieval similarity (encoding-encoding and encoding-retrieval), visual short-term memory (VSTM) maintenance, perceptual retrieval tasks (picture-based recognition), measures of self-similarity and between-item similarity in fMRI/iEEG.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Early (250–770 ms) encoding periods and early VVS activity reflect visual/perceptual formats; visual-format matching (to cDNN early layers) correlates with neural patterns in early visual cortex and predicts perceptual memory performance in picture-based tests (cited findings: encoding/retrieval matching to visual DNN RDMs predicts memory—Davis et al. 2021; iEEG studies show visual formats present during early encoding and late maintenance windows). Higher visual detail predicts better perceptual discrimination but may be lost faster over time than conceptual features (Lifanov et al. 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Contrasted with conceptual/semantic formats: visual formats dominate early perceptual stages and sensory cortex, while conceptual formats dominate later processing and anterior areas during retrieval. Evidence shows both can coexist; sometimes bottom-up visual information can aid conceptual retrieval and vice versa (cross-format benefits reported by Davis et al. 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Perceptual detail is not always preserved across consolidation; some studies report faster forgetting of visual detail and emergence of coarser conceptual representations. However, exemplar-specific visual information can persist in some cortical regions and during replay (Schapiro et al. 2017; Ferreira et al. 2019). Visual-format evidence is modality-limited and cannot by itself explain semantic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Perceptual formats provide the rich, high-dimensional input that can be transformed into compressed or generalized conceptual representations; they support fine-grained discrimination and episodic specificity, and their fidelity (self-similarity) predicts recognition memory accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representational formats of human memory traces', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8696.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8696.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conceptual / Semantic format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conceptual (semantic) representational format</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional-level format that encodes meaning-level features (categories, prototypes, semantic relations) rather than low-level sensory detail; supports gist-like, generalized representations and semantic retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Conceptual / Semantic representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Represents concepts via semantic features, category membership, prototypes, and higher-order relations (i.e., non-perceptual properties). Functionally this yields compressed, lower-dimensional representations abstracted from sensory specifics and supports generalization across exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed / feature-based (semantic embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Conceptual retrieval (word/label-based tests), reaction-time differences for conceptual vs perceptual probes, long-term memory retrieval, false-alarm rates to semantically related lures, encoding tasks that favor semantic processing, RSA comparisons to dNLP semantic model RDMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Late processing stages and anterior VVS / association cortex show semantic-format signatures; matching encoding patterns to semantic (dNLP) RDMs predicted subsequent LTM success (Davis et al. 2021; Liu et al. 2021). Retrieval often shows activation of conceptual formats prior to perceptual detail when no sensory input is present (Linde-Domingo et al. 2019). Greater emphasis on semantic features supports long-term consolidation and generalization but can increase false alarms for semantically similar lures (Naspi et al. 2021a,b).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Compared to perceptual formats, conceptual formats favor generalization and category-level similarity (increase between-item similarity for category members). Evidence favors conceptual formats for durable LTM and rapid generalization, whereas perceptual formats support exemplar-specific discrimination; both formats are engaged and their relative contribution depends on task, time, and prior knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Over-reliance on semantic format can produce false recognitions for semantically related foils; some exemplar-specific information is preserved despite semantization, and conceptual dominance is not ubiquitously observed across all brain regions or tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Semantic formats implement gist-abstraction/semantization that reduces representational dimensionality to enable integration with prior knowledge and support generalization; they are functionally beneficial for durable memory and rapid retrieval when sensory cues are absent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representational formats of human memory traces', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8696.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8696.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantization / Gist-abstraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantization (gist-abstraction) transformation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional-level transformation process by which sensory-detailed episodic representations become compressed into gist-like semantic representations over time or during early post-encoding stages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Semantization (gist-abstraction)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Not a static format but a transformation that selectively strengthens conceptual/shared features and reduces stimulus-unique sensory details, yielding more similar representations for items sharing category-level features (a shift from high-dimensional perceptual codes to lower-dimensional semantic codes).</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>transformative / hybrid (process producing semantic-dominant representations)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Comparisons of perceptual vs conceptual reaction times over delays, false alarm rates to semantically related lures, VSTM maintenance analyses, sleep and replay studies (consolidation), encoding–retrieval similarity (ERS) measures predicting LTM.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Evidence indicates semantization can begin rapidly during encoding and VSTM (iEEG: late encoding/maintenance shows semantic-format signatures that predict LTM; Liu et al. 2021). Behavioral data show faster recall for conceptual features over time and increased conceptual false alarms (Lifanov et al. 2021; Naspi et al. 2021a). Sleep replay supports generalization of shared features but can also preserve unique features (Himmer et al. 2019; Schapiro et al. 2017), indicating semantization is partial and context-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Semantization describes a shift from perceptual to conceptual formats but the evidence supports coexistence rather than complete replacement: perceptual details can persist in some regions and under some conditions, and both perceptual and conceptual matches can aid retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Not all studies show wholesale loss of perceptual detail; semantization interacts with prior knowledge and encoding context (schema congruency can accelerate semantization for congruent items but not for incongruent ones). Replay and consolidation can preserve exemplar-specific information, challenging an exclusively lossy semantization view.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Semantization is proposed as a mechanism to integrate new experiences into neocortical semantic knowledge and to enable efficient generalization; rapid semantization during VSTM may functionally support LTM formation by reducing dimensionality and emphasizing diagnostic conceptual features.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representational formats of human memory traces', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8696.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8696.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>cDNN/dNLP model formats</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DNN-derived representational formats (convolutional DNN visual layers and deep NLP semantic embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Computational representational formats derived from deep neural networks: early convolutional layers model low-level visual formats, deeper convolutional/fully-connected layers model higher-order visual/category formats, and deep NLP embeddings model semantic/conceptual format; these are used functionally as model RDMs for RSA mapping to brain data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>DNN-derived visual and semantic representational formats</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Functionally, cDNN early layers represent sensory feature vectors (edges, colors, textures), mid- to late layers represent object-parts and category prototypes (higher-order visual), while dNLP models provide distributed semantic embedding vectors capturing co-occurrence-derived conceptual similarity; these computational formats serve as proxies for functional representational formats in brain–model comparisons via RSA.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed / model-based (computational proxy for brain feature spaces)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Object recognition tasks, RSA comparisons between model RDMs and neural RDMs in perception, VSTM maintenance, encoding and retrieval, and predicting subsequent memory performance (encoding–model matching predicting LTM).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multiple studies report hierarchical correspondence: early cDNN layers map to early visual cortex, deeper layers to ventral stream and IT; dNLP embeddings map to semantic/association cortex. In memory studies, matching encoding representations to either visual cDNN or semantic dNLP RDMs predicted memory performance in perceptual and conceptual retrieval (Davis et al. 2021). iEEG RSA combining cDNN and dNLP showed a temporal shift from visual to semantic formats during encoding/maintenance predicting later memory (Liu et al. 2020, 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>DNN-derived formats provide operationalized instantiations of perceptual vs conceptual formats and enable direct comparison; evidence favors a gradient from cDNN-visual to dNLP-semantic matches over time. Recurrent and more brain-like DNNs can improve match to neural dynamics, suggesting architectural differences matter.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>DNNs are approximations and lack many aspects of brain function (recurrence, attention, long-term knowledge integration); cDNNs are limited to visual features and do not capture affective/contextual formats unless augmented. Thus model–brain correspondences are suggestive but provisional.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Using DNN layers and dNLP embeddings as functional proxies allows quantification of representational format content and dynamics, supporting the claim that memory traces can be decomposed into coexisting formats whose relative prominence changes with time and task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representational formats of human memory traces', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8696.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8696.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Affective / Emotional format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Affective (emotional) representational format / emotional binding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional-level dimension encoding affective evaluation (valence, arousal) and binding emotion to episodic elements, which can selectively enhance memory for emotional or central aspects of an episode.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Affective / emotional representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Represents episodic content by associating elements with affective evaluations (valence, arousal) and linking elements via an emotion-based binding mechanism (e.g., amygdala-centered), functionally enhancing salience, generalization, or privileged consolidation for emotionally tagged features.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>dimension-based / hybrid (affective dimension overlaid on sensory/semantic codes)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Emotional memory enhancement paradigms; Trier Social Stress Test (TSST) incidental encoding of central vs peripheral objects; fear conditioning and generalization; recognition performance and representational similarity analyses in amygdala and related networks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Stress during encoding increased memory for central objects; fMRI RSA showed central objects' representations became more similar to each other and to the stressor (committee faces) in the left amygdala in stressed vs control participants (Bierbrauer et al. 2021), and this similarity explained superior memory. Fear-learning studies show pattern similarity changes predictive of long-term fear memory (Visser et al. 2011, 2013). Negative emotion can enhance visual specificity for certain memories (Kensinger et al. 2006, 2007).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Affective format can modulate both perceptual and conceptual formats by biasing which features are bound and consolidated; emotional binding may outweigh spatio-temporal hippocampal binding, producing different generalization patterns (Yonelinas & Ritchey emotional binding vs hippocampal binding).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Emotional enhancement effects depend strongly on timing (stress before/during retrieval impairs recall); effects can be heterogeneous across features (sometimes enhancing perceptual detail, other times conceptual gist). The mechanisms (amygdalar binding vs hippocampal binding) are debated and evidence is region- and task-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Affective representations act as an additional (orthogonal) representational dimension that selectively prioritizes and binds episode elements, shaping representational geometry (increasing within-episode similarity and stressor-linked generalization) and thereby altering retrieval probability and cues that trigger memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representational formats of human memory traces', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8696.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8696.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contextual / Schema / Event-specific format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contextual / schema / event-specific representational formats (including binding and cognitive maps)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Functional formats that encode contextual embeddings, spatio-temporal context, schemas/scripts or event-specific relations that organize elements into coherent episodes and support episodic specificity and reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Contextual / schema / event-specific representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Represents episodes by embedding items within spatio-temporal or schematic contexts (e.g., scene, script, cognitive map) and by binding multiple elements into integrated episodic units; functionally supports discrimination between episodes and reconstruction of temporal/spatial relations.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>hybrid / relational (binding + structured schemas)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Maze navigation / cognitive map behavior, episodic memory tests distinguishing central vs peripheral elements, binding paradigms (object–context associations), consolidation and systems-level transformation (schema-congruent encoding), encoding–retrieval similarity, hippocampal pattern separation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Tolman's cognitive map is cited as behavioral evidence for internal contextual representations. Hippocampal pattern separation supports distinct event-specific encoding and better memory (LaRocque et al. 2013). NPRC (Gilboa & Moscovitch 2021) describes coexistence of event-specific visual formats and schemas/semantic knowledge. Schema congruency modulates generalization during consolidation and integration in mPFC (van Kesteren et al. 2013; Audrain & McAndrews 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Contextual/schema formats interact with perceptual and conceptual formats: schemas facilitate transformation toward semanticized traces but can also preserve event-unique details; hippocampal pattern separation contrasts with cortical integration (perirhinal/parahippocampal cortices) where between-item similarity can aid encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Not a single, unitary format—context and schema effects are heterogeneous and depend on congruency, prior knowledge, and task; some schema-congruent information is integrated at the cost of specificity while other evidence shows schema-related integration can preserve unique features under certain reactivation/rehearsal conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Contextual and schema-based representations provide scaffolds for neocortical integration, guide selective consolidation, and determine whether memories are stored as distinct event-specific traces or integrated generalized knowledge; they functionally mediate when and how semantization vs preservation of detail occurs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representational formats of human memory traces', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Visual and semantic representations predict subsequent memory in perceptual and conceptual memory tests <em>(Rating: 2)</em></li>
                <li>Stable maintenance of multiple representational formats in human visual short-term memory <em>(Rating: 2)</em></li>
                <li>Transformative neural representations support long-term episodic memory <em>(Rating: 2)</em></li>
                <li>The neural representations underlying human episodic memory <em>(Rating: 2)</em></li>
                <li>Representational similarity analysis - connecting the branches of systems neuroscience <em>(Rating: 2)</em></li>
                <li>Deep convolutional neural networks outperform feature-based but not categorical models in explaining object similarity judgments <em>(Rating: 1)</em></li>
                <li>Semantic and perceptual representations at encoding contribute to true and false recognition of objects <em>(Rating: 2)</em></li>
                <li>Memory for specific visual details can be enhanced by negative arousing content <em>(Rating: 1)</em></li>
                <li>Neural pattern similarity predicts long-term fear memory <em>(Rating: 2)</em></li>
                <li>Schemas provide a scaffold for neocortical integration of new memories over time <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8696",
    "paper_id": "paper-257984324",
    "extraction_schema_id": "extraction-schema-156",
    "extracted_data": [
        {
            "name_short": "Perceptual / Visual format",
            "name_full": "Perceptual (visual) representational format",
            "brief_description": "A functional-level format that encodes sensory features of experiences (colors, shapes, textures, low-level spatial/retinotopic information and higher-order visual object parts); typically supports image-like, detail-rich representations used in perception and early memory stages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representational_format_name": "Perceptual / Visual representation",
            "representational_format_description": "Represents concepts via sensory/visual features (e.g., color, texture, edges, shapes, object parts) at multiple levels of visual complexity; functionally this format preserves fine-grained, stimulus-specific information that can be reinstated for recognition or imagery.",
            "format_type": "feature-based / distributed (sensory-feature coding)",
            "cognitive_task_or_phenomenon": "Object recognition, encoding–retrieval similarity (encoding-encoding and encoding-retrieval), visual short-term memory (VSTM) maintenance, perceptual retrieval tasks (picture-based recognition), measures of self-similarity and between-item similarity in fMRI/iEEG.",
            "key_findings": "Early (250–770 ms) encoding periods and early VVS activity reflect visual/perceptual formats; visual-format matching (to cDNN early layers) correlates with neural patterns in early visual cortex and predicts perceptual memory performance in picture-based tests (cited findings: encoding/retrieval matching to visual DNN RDMs predicts memory—Davis et al. 2021; iEEG studies show visual formats present during early encoding and late maintenance windows). Higher visual detail predicts better perceptual discrimination but may be lost faster over time than conceptual features (Lifanov et al. 2021).",
            "comparison_with_other_formats": "Contrasted with conceptual/semantic formats: visual formats dominate early perceptual stages and sensory cortex, while conceptual formats dominate later processing and anterior areas during retrieval. Evidence shows both can coexist; sometimes bottom-up visual information can aid conceptual retrieval and vice versa (cross-format benefits reported by Davis et al. 2021).",
            "limitations_or_counter_evidence": "Perceptual detail is not always preserved across consolidation; some studies report faster forgetting of visual detail and emergence of coarser conceptual representations. However, exemplar-specific visual information can persist in some cortical regions and during replay (Schapiro et al. 2017; Ferreira et al. 2019). Visual-format evidence is modality-limited and cannot by itself explain semantic generalization.",
            "theoretical_claims_or_implications": "Perceptual formats provide the rich, high-dimensional input that can be transformed into compressed or generalized conceptual representations; they support fine-grained discrimination and episodic specificity, and their fidelity (self-similarity) predicts recognition memory accuracy.",
            "uuid": "e8696.0",
            "source_info": {
                "paper_title": "Representational formats of human memory traces",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Conceptual / Semantic format",
            "name_full": "Conceptual (semantic) representational format",
            "brief_description": "A functional-level format that encodes meaning-level features (categories, prototypes, semantic relations) rather than low-level sensory detail; supports gist-like, generalized representations and semantic retrieval.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representational_format_name": "Conceptual / Semantic representation",
            "representational_format_description": "Represents concepts via semantic features, category membership, prototypes, and higher-order relations (i.e., non-perceptual properties). Functionally this yields compressed, lower-dimensional representations abstracted from sensory specifics and supports generalization across exemplars.",
            "format_type": "distributed / feature-based (semantic embedding)",
            "cognitive_task_or_phenomenon": "Conceptual retrieval (word/label-based tests), reaction-time differences for conceptual vs perceptual probes, long-term memory retrieval, false-alarm rates to semantically related lures, encoding tasks that favor semantic processing, RSA comparisons to dNLP semantic model RDMs.",
            "key_findings": "Late processing stages and anterior VVS / association cortex show semantic-format signatures; matching encoding patterns to semantic (dNLP) RDMs predicted subsequent LTM success (Davis et al. 2021; Liu et al. 2021). Retrieval often shows activation of conceptual formats prior to perceptual detail when no sensory input is present (Linde-Domingo et al. 2019). Greater emphasis on semantic features supports long-term consolidation and generalization but can increase false alarms for semantically similar lures (Naspi et al. 2021a,b).",
            "comparison_with_other_formats": "Compared to perceptual formats, conceptual formats favor generalization and category-level similarity (increase between-item similarity for category members). Evidence favors conceptual formats for durable LTM and rapid generalization, whereas perceptual formats support exemplar-specific discrimination; both formats are engaged and their relative contribution depends on task, time, and prior knowledge.",
            "limitations_or_counter_evidence": "Over-reliance on semantic format can produce false recognitions for semantically related foils; some exemplar-specific information is preserved despite semantization, and conceptual dominance is not ubiquitously observed across all brain regions or tasks.",
            "theoretical_claims_or_implications": "Semantic formats implement gist-abstraction/semantization that reduces representational dimensionality to enable integration with prior knowledge and support generalization; they are functionally beneficial for durable memory and rapid retrieval when sensory cues are absent.",
            "uuid": "e8696.1",
            "source_info": {
                "paper_title": "Representational formats of human memory traces",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Semantization / Gist-abstraction",
            "name_full": "Semantization (gist-abstraction) transformation",
            "brief_description": "A functional-level transformation process by which sensory-detailed episodic representations become compressed into gist-like semantic representations over time or during early post-encoding stages.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "Semantization (gist-abstraction)",
            "representational_format_description": "Not a static format but a transformation that selectively strengthens conceptual/shared features and reduces stimulus-unique sensory details, yielding more similar representations for items sharing category-level features (a shift from high-dimensional perceptual codes to lower-dimensional semantic codes).",
            "format_type": "transformative / hybrid (process producing semantic-dominant representations)",
            "cognitive_task_or_phenomenon": "Comparisons of perceptual vs conceptual reaction times over delays, false alarm rates to semantically related lures, VSTM maintenance analyses, sleep and replay studies (consolidation), encoding–retrieval similarity (ERS) measures predicting LTM.",
            "key_findings": "Evidence indicates semantization can begin rapidly during encoding and VSTM (iEEG: late encoding/maintenance shows semantic-format signatures that predict LTM; Liu et al. 2021). Behavioral data show faster recall for conceptual features over time and increased conceptual false alarms (Lifanov et al. 2021; Naspi et al. 2021a). Sleep replay supports generalization of shared features but can also preserve unique features (Himmer et al. 2019; Schapiro et al. 2017), indicating semantization is partial and context-dependent.",
            "comparison_with_other_formats": "Semantization describes a shift from perceptual to conceptual formats but the evidence supports coexistence rather than complete replacement: perceptual details can persist in some regions and under some conditions, and both perceptual and conceptual matches can aid retrieval.",
            "limitations_or_counter_evidence": "Not all studies show wholesale loss of perceptual detail; semantization interacts with prior knowledge and encoding context (schema congruency can accelerate semantization for congruent items but not for incongruent ones). Replay and consolidation can preserve exemplar-specific information, challenging an exclusively lossy semantization view.",
            "theoretical_claims_or_implications": "Semantization is proposed as a mechanism to integrate new experiences into neocortical semantic knowledge and to enable efficient generalization; rapid semantization during VSTM may functionally support LTM formation by reducing dimensionality and emphasizing diagnostic conceptual features.",
            "uuid": "e8696.2",
            "source_info": {
                "paper_title": "Representational formats of human memory traces",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "cDNN/dNLP model formats",
            "name_full": "DNN-derived representational formats (convolutional DNN visual layers and deep NLP semantic embeddings)",
            "brief_description": "Computational representational formats derived from deep neural networks: early convolutional layers model low-level visual formats, deeper convolutional/fully-connected layers model higher-order visual/category formats, and deep NLP embeddings model semantic/conceptual format; these are used functionally as model RDMs for RSA mapping to brain data.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "DNN-derived visual and semantic representational formats",
            "representational_format_description": "Functionally, cDNN early layers represent sensory feature vectors (edges, colors, textures), mid- to late layers represent object-parts and category prototypes (higher-order visual), while dNLP models provide distributed semantic embedding vectors capturing co-occurrence-derived conceptual similarity; these computational formats serve as proxies for functional representational formats in brain–model comparisons via RSA.",
            "format_type": "distributed / model-based (computational proxy for brain feature spaces)",
            "cognitive_task_or_phenomenon": "Object recognition tasks, RSA comparisons between model RDMs and neural RDMs in perception, VSTM maintenance, encoding and retrieval, and predicting subsequent memory performance (encoding–model matching predicting LTM).",
            "key_findings": "Multiple studies report hierarchical correspondence: early cDNN layers map to early visual cortex, deeper layers to ventral stream and IT; dNLP embeddings map to semantic/association cortex. In memory studies, matching encoding representations to either visual cDNN or semantic dNLP RDMs predicted memory performance in perceptual and conceptual retrieval (Davis et al. 2021). iEEG RSA combining cDNN and dNLP showed a temporal shift from visual to semantic formats during encoding/maintenance predicting later memory (Liu et al. 2020, 2021).",
            "comparison_with_other_formats": "DNN-derived formats provide operationalized instantiations of perceptual vs conceptual formats and enable direct comparison; evidence favors a gradient from cDNN-visual to dNLP-semantic matches over time. Recurrent and more brain-like DNNs can improve match to neural dynamics, suggesting architectural differences matter.",
            "limitations_or_counter_evidence": "DNNs are approximations and lack many aspects of brain function (recurrence, attention, long-term knowledge integration); cDNNs are limited to visual features and do not capture affective/contextual formats unless augmented. Thus model–brain correspondences are suggestive but provisional.",
            "theoretical_claims_or_implications": "Using DNN layers and dNLP embeddings as functional proxies allows quantification of representational format content and dynamics, supporting the claim that memory traces can be decomposed into coexisting formats whose relative prominence changes with time and task.",
            "uuid": "e8696.3",
            "source_info": {
                "paper_title": "Representational formats of human memory traces",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Affective / Emotional format",
            "name_full": "Affective (emotional) representational format / emotional binding",
            "brief_description": "A functional-level dimension encoding affective evaluation (valence, arousal) and binding emotion to episodic elements, which can selectively enhance memory for emotional or central aspects of an episode.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "Affective / emotional representation",
            "representational_format_description": "Represents episodic content by associating elements with affective evaluations (valence, arousal) and linking elements via an emotion-based binding mechanism (e.g., amygdala-centered), functionally enhancing salience, generalization, or privileged consolidation for emotionally tagged features.",
            "format_type": "dimension-based / hybrid (affective dimension overlaid on sensory/semantic codes)",
            "cognitive_task_or_phenomenon": "Emotional memory enhancement paradigms; Trier Social Stress Test (TSST) incidental encoding of central vs peripheral objects; fear conditioning and generalization; recognition performance and representational similarity analyses in amygdala and related networks.",
            "key_findings": "Stress during encoding increased memory for central objects; fMRI RSA showed central objects' representations became more similar to each other and to the stressor (committee faces) in the left amygdala in stressed vs control participants (Bierbrauer et al. 2021), and this similarity explained superior memory. Fear-learning studies show pattern similarity changes predictive of long-term fear memory (Visser et al. 2011, 2013). Negative emotion can enhance visual specificity for certain memories (Kensinger et al. 2006, 2007).",
            "comparison_with_other_formats": "Affective format can modulate both perceptual and conceptual formats by biasing which features are bound and consolidated; emotional binding may outweigh spatio-temporal hippocampal binding, producing different generalization patterns (Yonelinas & Ritchey emotional binding vs hippocampal binding).",
            "limitations_or_counter_evidence": "Emotional enhancement effects depend strongly on timing (stress before/during retrieval impairs recall); effects can be heterogeneous across features (sometimes enhancing perceptual detail, other times conceptual gist). The mechanisms (amygdalar binding vs hippocampal binding) are debated and evidence is region- and task-dependent.",
            "theoretical_claims_or_implications": "Affective representations act as an additional (orthogonal) representational dimension that selectively prioritizes and binds episode elements, shaping representational geometry (increasing within-episode similarity and stressor-linked generalization) and thereby altering retrieval probability and cues that trigger memory.",
            "uuid": "e8696.4",
            "source_info": {
                "paper_title": "Representational formats of human memory traces",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Contextual / Schema / Event-specific format",
            "name_full": "Contextual / schema / event-specific representational formats (including binding and cognitive maps)",
            "brief_description": "Functional formats that encode contextual embeddings, spatio-temporal context, schemas/scripts or event-specific relations that organize elements into coherent episodes and support episodic specificity and reconstruction.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "Contextual / schema / event-specific representation",
            "representational_format_description": "Represents episodes by embedding items within spatio-temporal or schematic contexts (e.g., scene, script, cognitive map) and by binding multiple elements into integrated episodic units; functionally supports discrimination between episodes and reconstruction of temporal/spatial relations.",
            "format_type": "hybrid / relational (binding + structured schemas)",
            "cognitive_task_or_phenomenon": "Maze navigation / cognitive map behavior, episodic memory tests distinguishing central vs peripheral elements, binding paradigms (object–context associations), consolidation and systems-level transformation (schema-congruent encoding), encoding–retrieval similarity, hippocampal pattern separation.",
            "key_findings": "Tolman's cognitive map is cited as behavioral evidence for internal contextual representations. Hippocampal pattern separation supports distinct event-specific encoding and better memory (LaRocque et al. 2013). NPRC (Gilboa & Moscovitch 2021) describes coexistence of event-specific visual formats and schemas/semantic knowledge. Schema congruency modulates generalization during consolidation and integration in mPFC (van Kesteren et al. 2013; Audrain & McAndrews 2022).",
            "comparison_with_other_formats": "Contextual/schema formats interact with perceptual and conceptual formats: schemas facilitate transformation toward semanticized traces but can also preserve event-unique details; hippocampal pattern separation contrasts with cortical integration (perirhinal/parahippocampal cortices) where between-item similarity can aid encoding.",
            "limitations_or_counter_evidence": "Not a single, unitary format—context and schema effects are heterogeneous and depend on congruency, prior knowledge, and task; some schema-congruent information is integrated at the cost of specificity while other evidence shows schema-related integration can preserve unique features under certain reactivation/rehearsal conditions.",
            "theoretical_claims_or_implications": "Contextual and schema-based representations provide scaffolds for neocortical integration, guide selective consolidation, and determine whether memories are stored as distinct event-specific traces or integrated generalized knowledge; they functionally mediate when and how semantization vs preservation of detail occurs.",
            "uuid": "e8696.5",
            "source_info": {
                "paper_title": "Representational formats of human memory traces",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Visual and semantic representations predict subsequent memory in perceptual and conceptual memory tests",
            "rating": 2,
            "sanitized_title": "visual_and_semantic_representations_predict_subsequent_memory_in_perceptual_and_conceptual_memory_tests"
        },
        {
            "paper_title": "Stable maintenance of multiple representational formats in human visual short-term memory",
            "rating": 2,
            "sanitized_title": "stable_maintenance_of_multiple_representational_formats_in_human_visual_shortterm_memory"
        },
        {
            "paper_title": "Transformative neural representations support long-term episodic memory",
            "rating": 2,
            "sanitized_title": "transformative_neural_representations_support_longterm_episodic_memory"
        },
        {
            "paper_title": "The neural representations underlying human episodic memory",
            "rating": 2,
            "sanitized_title": "the_neural_representations_underlying_human_episodic_memory"
        },
        {
            "paper_title": "Representational similarity analysis - connecting the branches of systems neuroscience",
            "rating": 2,
            "sanitized_title": "representational_similarity_analysis_connecting_the_branches_of_systems_neuroscience"
        },
        {
            "paper_title": "Deep convolutional neural networks outperform feature-based but not categorical models in explaining object similarity judgments",
            "rating": 1,
            "sanitized_title": "deep_convolutional_neural_networks_outperform_featurebased_but_not_categorical_models_in_explaining_object_similarity_judgments"
        },
        {
            "paper_title": "Semantic and perceptual representations at encoding contribute to true and false recognition of objects",
            "rating": 2,
            "sanitized_title": "semantic_and_perceptual_representations_at_encoding_contribute_to_true_and_false_recognition_of_objects"
        },
        {
            "paper_title": "Memory for specific visual details can be enhanced by negative arousing content",
            "rating": 1,
            "sanitized_title": "memory_for_specific_visual_details_can_be_enhanced_by_negative_arousing_content"
        },
        {
            "paper_title": "Neural pattern similarity predicts long-term fear memory",
            "rating": 2,
            "sanitized_title": "neural_pattern_similarity_predicts_longterm_fear_memory"
        },
        {
            "paper_title": "Schemas provide a scaffold for neocortical integration of new memories over time",
            "rating": 2,
            "sanitized_title": "schemas_provide_a_scaffold_for_neocortical_integration_of_new_memories_over_time"
        }
    ],
    "cost": 0.017405999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Representational formats of human memory traces
6 April 2023</p>
<p>Rebekka Heinen rebekka.heinen@rub.de 0000-0002-6888-8101
Anne Bierbrauer 0000-0002-3803-7015
Oliver T Wolf 0000-0002-9320-2124
Nikolai Axmacher 0000-0002-0475-6492</p>
<p>1 Department of Neuropsychology, Institute of Cognitive Neuroscience, Faculty of Psychology, Ruhr University
Bochum, Universitätsstraße 150, 44801 Bochum, Germany Martinistraße 52, 20251 Hamburg, Germany Bochum, Universitätsstraße 150, 44801 Bochum, Germany</p>
<p>2 Institute for Systems Neuroscience, Medical Center Hamburg-Eppendorf,</p>
<p>3 Department of Cognitive Psychology, Institute of Cognitive Neuroscience, Faculty of Psychology, Ruhr University</p>
<p>Representational formats of human memory traces
6 April 2023FDDA8E264AF5B978254FF595BAB51AAE10.1007/s00429-023-02636-9Received: 6 December 2022 / Accepted: 28 March 2023 /MemoryNeural representationsRepresentational similarity analysisRepresentational formatsDeep neural networks
Neural representations are internal brain states that constitute the brain's model of the external world or some of its features.In the presence of sensory input, a representation may reflect various properties of this input.When perceptual information is no longer available, the brain can still activate representations of previously experienced episodes due to the formation of memory traces.In this review, we aim at characterizing the nature of neural memory representations and how they can be assessed with cognitive neuroscience methods, mainly focusing on neuroimaging.We discuss how multivariate analysis techniques such as representational similarity analysis (RSA) and deep neural networks (DNNs) can be leveraged to gain insights into the structure of neural representations and their different representational formats.We provide several examples of recent studies which demonstrate that we are able to not only measure memory representations using RSA but are also able to investigate their multiple formats using DNNs.We demonstrate that in addition to slow generalization during consolidation, memory representations are subject to semantization already during short-term memory, by revealing a shift from visual to semantic format.In addition to perceptual and conceptual formats, we describe the impact of affective evaluations as an additional dimension of episodic memories.Overall, these studies illustrate how the analysis of neural representations may help us gain a deeper understanding of the nature of human memory.</p>
<p>Introduction: why should we assume representations?</p>
<p>When we think back to what we did yesterday, we are usually able to literally picture how a specific episode looked like, and perhaps also how it sounded, smelled, and felt.This ability to form a mental image or internal representation plays a crucial role for both re-experiencing the past and making plans for the future (Schacter and Addis 2007;Bonnici et al. 2012;Cheng et al. 2016;Brown et al. 2016).How is the sensory information about this episode transformed into a long-lasting neural memory trace?Will different aspects such as visual and abstract information be stored differently in memory?How can we measure the representational format of memories?</p>
<p>First of all: What is a representation?Described as early as 1904 by Richard Semon (e.g., Schacter 2001), most cognitive neuroscientists nowadays believe that mental representations of past and future episodes rely on a neural substrate that we can localize in the brain-on the "neural representation" of the represented episode (deCharms and Zador 2000;Shea 2018)-this notion has not always been accepted.Beginning with the "cognitive revolution" in the 1960s, cognitivism replaced behaviorism, a scientific movement trying to explain behavior not only without introspection, but also without assuming mental representations (Watson 1913;Skinner 1953;Egan 2014;Shea 2018;Newen and Vosgerau 2020).In contrast to behaviorism, cognitivists emphasized the importance of intentional states and mental Rebekka Heinen and Anne Bierbrauer have contributed equally.contents for understanding cognitive functioning.According to this representational view, a mental representation consists of (1) a vehicle, i.e., a physical entity such as a population of neurons that is able to represent information, and (2) a content, i.e., the information about the outside world or about internal states that is carried by the vehicle (Fodor 2008;Roskies 2021).In addition-and critical for our review-this content can have (3) different representational formats: A given experience can be either represented conceptually or non-conceptually (Boghossian 1995).While conceptual representational formats are composed of semantic thoughts, non-conceptual formats rely on sensory aspects of an experience.Arguably, most "real-life" representations consist of both representational formats.For example, the representation of a visit to the ocean (content) comprises the fact that one was at a certain beach at a certain time (conceptual representational formats) and the feeling of sand beneath one's feet, the color of the water, and the heat of the sun (non-conceptual representational formats).The brain states carrying both types of information constitute the vehicles of mental representations.In this review, we will focus on these two types of formats-perceptual and conceptual.</p>
<p>The representational theory of the mind assumes that cognitive functioning consists of the formation and the transformation of mental representations.It will thus be important to develop methods to measure these representations and assessing their vehicle in the brain has become a core aim of contemporary cognitive neuroscience.</p>
<p>A case for internal representations</p>
<p>"A neural representation is a pattern of neural activity that stands for some environmental feature in the internal workings of the brain" (Vilarroya 2017, p. 4) and focuses on particular features in the world-i.e., neural representations have a representational content and involve a particular representational format (deCharms and Zador 2000).At early steps of sensory processing, neural representations involve representational formats that are more strongly correlated with external input than at later processing stages.For example, Hubel and Wiesel (1959) studied how the early visual cortex responds to bars at different angular directions.The striate cortex and other cortices at the beginning of the sensory processing hierarchy exhibit pronounced topographic organization, such that the patterns of activity are isomorphic with the external world (Poldrack 2021).At later processing steps, neural representations are less strongly driven by sensory inputs and more strongly shaped by cognitive operations.A famous example of such a representation occurs in an experiment that Tolman described in his book "Cognitive maps in rats and men" (1948): A rodent explores a maze and may find rewards when choosing the correct path.After some time, the reward path is blocked, and the rodent is offered several different alternative paths.Tolman could demonstrate that rodents took the shortest alternative path.This is indicative of an internal representation-in this case of relative spatial locations-that is referred to as "cognitive map", as the behavior of the rodent cannot be solely explained by stimulus-response learning based on stimulusoutcome associations.</p>
<p>How can we measure and analyze neural representations?</p>
<p>Out of many ideas and possibilities how stimulus information is represented in neural structures, three prominent theories evolved which differ regarding the neural features containing representations.On the level of single neurons, the 'rate coding' hypothesis claims that the mean firing rate of each neuron carries information about stimuli (Adrian 1928;DeCharms and Zador 2000).The 'temporal coding' hypothesis posits that in addition to the mean firing rate the precise timing of spikes is crucial (DeCharms and Zador 2000;Gerstner and Kistler 2002;Gollisch and Meister 2008).We consider these coding schemes on the single unit level as "sparse" since they focus on coding by one or a few neurons (Axmacher et al. 2008;Reddy and Kanwisher 2006).In addition, the activity of large populations of neurons also carries information (Deadwyler and Hampson 1997;DeCharms and Zador 2000;Georgopoulos et al. 1986;Hebb 1949).This scheme of 'population coding' would be consistent with a large number of broadly tuned neurons that code for a given stimulus (Reddy and Kanwisher 2006).</p>
<p>At the population level, neural representations can be measured by decoding approaches which can be applied to various kinds of non-invasive data in human participants (most importantly, functional magnetic resonance imaging, fMRI, or electroencephalography, EEG).In contrast to univariate analysis techniques which reflect overall activity changes a commonly used way to assess neural representations in cognitive neuroscience is multivariate pattern analysis (MVPA).With the advent of MVPA it has become possible to extract representational contents and formats from distributed patterns of neural activity, e.g., voxel activity values in fMRI data or power values at various frequency bands, time points, and channels in EEG data (Naselaris et al. 2011;Hebart and Baker 2018;Kunz et al. 2018;Roskies 2021).</p>
<p>When two stimuli elicit similar overall activity levels and their informational content is reflected by the pattern of voxel activations instead, it may be impossible to find univariate activation differences.Therefore, MVPA aims at decoding the information that the patterns of activity carry about external stimuli (Haynes and Rees 2006;Kriegeskorte et al. 2008a;Mur et al. 2009;Haxby et al. 2014;Kragel et al. 2018).Even when brain regions are relevant for processing a large number of different stimuli, it thus becomes possible to differentiate neural representations of two stimuli based on their activation pattern (Mur et al. 2009;Raizada et al. 2010), which may reflect a neural population code (Kamitani and Tong 2005;Watrous et al. 2015;Kriegeskorte and Diedrichsen 2019).</p>
<p>The underlying assumption of MVPA is that neural representations can be characterized via high-dimensional statespaces whose dimensions correspond to stimulus attributes, and that each individual representation corresponds to one point in this space (Haxby et al. 2014).The two most commonly used MVPA methods are pattern classification (Pereira et al. 2009) and representational similarity analysis (RSA; Kriegeskorte et al. 2008a;Kriegeskorte and Diedrichsen 2019).</p>
<p>RSA allows researchers to characterize the geometry of a representational space that can be based on various stimulus features (Kriegeskorte and Kievit 2013;Haxby et al. 2014;Kriegeskorte and Wei 2021;Roskies 2021).Importantly, RSA abstracts from the specific type of data that is investigated (e.g., fMRI or EEG) and consists of a matrix of similarities which quantifies the (dis)similarity between neural representations (Haxby et al. 2014).Hence, it becomes possible to analyze second-order similarities-i.e., the correspondence between two separate similarity matrices (RDMs)-of (1) neural representations measured in different brain regions, species, or modalities, (2) neural activity and behavioral outcomes, or (3) neural activity and computational models (Kriegeskorte et al. 2008a;Kriegeskorte and Kievit 2013;Haxby et al. 2014;Roskies 2021).In other words, RSA allows for an analysis of any kind of data pattern irrespective of the data format.</p>
<p>Implementing RSA requires the coding of neural activity as vectors, separately for the experimental conditions (e.g., for stimuli in an experiment).Afterwards, the representational distances between these vectors are calculated.The similarity or distance measures that are most often used are Pearson or Spearman correlations, or Euclidean or Mahalanobis distance.Higher similarity corresponds to lower representational distance and vice versa.The result is a representational dissimilarity matrix (RDM; Kriegeskorte et al. 2008a), i.e., a matrix that reflects the similarities or distances between every stimulus (or more generally, condition) with every other stimulus, resulting in a nxn matrix (e.g., stimulus x stimulus, condition x condition).Approaches like multidimensional scaling allow for a mapping of this highdimensional representational space in lower-dimensional spaces (often 2D or 3D) in order to facilitate interpretation.</p>
<p>We now can extract information from the RDMs to characterize the underlying neural representations.First, selfsimilarity, sometimes also called representational fidelity or reliability (see Xue 2018 for review), refers to the similarity of brain patterns when the same stimulus is presented twice.Although self-similarity most commonly refers to the similarity of a stimulus compared to others (i.e., to nonself similarity), some studies use this term to denote the similarity between repetitions of the same stimulus (i.e., Xue et al. 2010).Here, we use this term to refer to the first case (within vs. between similarity).This tells us how faithful a neural representation reflects a given stimulus.Second, RDMs allow us to investigate the relationships between different stimuli, i.e., between-item similarity.This betweenitem similarity may reflect the features of a stimulus that are represented by a given brain region-i.e., two stimuli with similar low-level visual features, such as spatial frequencies or gratings, have similar representations in early visual cortices, while conceptual similarities lead to similar representations in association cortices (Kriegeskorte et al. 2008a, b).Based on these differences, RSA allows unraveling the representational format of neural representations.This method is highly flexible since many different features or conditions can be investigated in one experiment.One possible application is the investigation of human episodic memory, which we will describe next.</p>
<p>How do neural representations relate to memory?</p>
<p>An episodic memory can be conceived of as an internal representation of a previous experience (Goldman-Rakic 1995;Brewer et al. 1998;Cheng et al. 2016;Vilarroya 2017).At the neural level, it is widely assumed that memory representations are stored in memory traces or engrams-a term coined by Richard Semon in order to refer to learninginduced alterations of brain (micro-)structure (Semon 1904(Semon , 1909)).</p>
<p>According to Semon, engrams are biological states that are objectively observable, which means that in principle, we can locate and manipulate them (Semon 1904(Semon , 1909;;Josselyn et al. 2015;Kunz et al. 2018).Second, they represent specific memory contents and thus, when activated, lead to expression of this memory content (i.e., behaviorally measurable memory retrieval) (Liu et al. 2014a, b;Kunz et al. 2018).Moreover, they may be distributed within and across brain areas, an aspect that had been suggested by Lashley (1950) and was empirically supported for encoding by Haxby et al. (2001) and for retrieval by Brodt et al. (2018) more than 50 years later.Each engram corresponds specifically and uniquely to one particular memory, and this relationship is stable such that a given engram, when activated, should always elicit the same memory (Han et al. 2009;Liu et al. 2012Liu et al. , 2014a, b;, b;Kunz et al. 2018).However, engrams or memory traces may also be transformed by various factors, such as time, memory consolidation, and novel learning (Dudai et al. 2015).</p>
<p>The formation of episodic memories requires the representation of the episode in a lasting memory trace (Xue 2018).In humans, various characteristics of memory representations have been associated with episodic memory performance: First, high amounts of self-similarity-i.e., of the memory representation of a particular content-predict subsequent memory (Xue et al. 2010;Visser et al. 2013).This result was found across various brain regions involving frontoparietal areas, the posterior cingulate cortex and sensory regions that are involved in processing the respective stimuli (Xue 2018).Self-similarity of memory representations may either refer to situations when a particular stimulus is encoded multiple times (encoding-encoding-similarity) or when encoding and retrieval of the same stimulus are compared (encoding-retrieval-similarity), and both measures predict memory accuracy (Xue et al. 2010;Xue 2018;Ten Oever et al. 2021).Thus, higher similarity between memory representations of the same stimulus seems to support (recognition) memory.</p>
<p>Interestingly, between-item similarity has been associated with memory performance as well, although not necessarily in a positive manner: Indeed, different theoretical frameworks and empirical results predict a memory advantage either for more distinct or for more similar memory representations of different items.Some studies found that stronger discrimination between different items (i.e., higher distinctiveness) supports memory (LaRocque et al. 2013;Xue 2018).The distinctiveness hypothesis is based on the idea that distinctiveness reduces possible interference with other, similar stimuli and thereby supports memory (Kılıç et al. 2017).The idea of distinctiveness is closely related to pattern separation in the hippocampus, a process by which similar memories are stored as distinct, non-overlapping representations (Bakker et al. 2008;Yassa and Stark 2011).An fMRI study confirmed that higher pattern distinctiveness in the hippocampus is indeed associated with better memory performance (LaRocque et al. 2013).In contrast, in perirhinal and parahippocampal cortex as well as in the amygdala, higher between-item similarity of neural representations benefits memory encoding, possibly because they are integrated into one unique episode that is distinct from other episodes (Visser et al. 2011(Visser et al. , 2013;;LaRocque et al. 2013;Bierbrauer et al. 2021).While these studies point to better memory performance with higher distinctiveness, there is also evidence that global pattern similarity-i.e., the similarity between different exemplars of the same conceptmay support memory (Davis et al. 2014), even causing false alarms for new exemplars of the same concept (Wing et al. 2020).</p>
<p>These results support the idea that memory representations have multiple representational formats, whose representational 'geometries' (generalized or distinct) may exert different influences on memory encoding.In this review, we define visual/perceptual representational formats as reflecting visual stimulus features (e.g., their colors, textures, or shapes).Conversely, we define conceptual/semantic formats as reflecting semantic stimulus features including category information.How can we quantify the representational formats and measure the degree to which a stimulus might be represented in a visual or a conceptual format?</p>
<p>Using deep neural networks as models of representational formats</p>
<p>In recent years, the field of artificial intelligence has revolutionized our lives, with artificial neural network models (ANN) achieving near human-like performance in areas such as language translation (Popel et al. 2020) and car driving (Gupta et al. 2021) and even out-performing humans in various complex games such as chess (McGrath et al. 2022), Go (Silver et al. 2017), Starcraft (Vinyals et al. 2019) or Stratego (Perolat et al. 2022).If ANN models are able to perform on a human level, can we also utilize them to better understand our own brain processes?</p>
<p>To gain insight into the transformation of visual features into conceptual representations, convolutional Deep Neural Networks (cDNNs) from object recognition (Fig. 1A) have become models of choice.These models process image input through several convolutional layers, which are connected sparsely, up to fully-connected layers that assign a label to contents of the image.Strikingly, recent multivariate studies have found the same visual hierarchy and gradient in feature complexity in cDNNs trained on object recognition as observed in the brain (Leeds et al. 2013;Khaligh-Razavi and Kriegeskorte 2014;Güçlü and van Gerven 2015;Yamins and DiCarlo 2016;Cichy et al. 2016;Wen et al. 2018).These results were obtained across various data modalities, ranging from fMRI (Güçlü and van Gerven 2015;Allen et al. 2022) via magnetoencephalography (MEG; Clarke et al. 2018) and scalp EEG (Graumann et al. 2022) to oscillations in intracranial EEG (Kuzovkin et al. 2018) and monkey single-unit data (Cadieu et al 2014) and even behavioral outcomes, such as similarity judgements (Mur et al. 2013;Davis et al. 2021).</p>
<p>These findings demonstrate that the internal representations on multiple levels of complexity formed by cDNNs are closely linked to the features that are processed along the ventral visual stream (VVS) (Fig. 1A).Processing of visual information along the VVS reveals a hierarchy from basic visual features to higher-order visual and semantic category features (Cowell et al. 2010;DiCarlo et al. 2012;Kravitz et al. 2013).The visual cortex processes low-level visual properties, such as colors, shapes, and textures, and neurons in these areas have small receptive field sizes that lead to pronounced retinotopic specialization (Hubel and Wiesel 1962).As the signal progresses through the VVS to more anterior regions, such as the inferior temporal cortex (IT cortex; Kriegeskorte et al. 2008b), the fusiform gyrus (Clarke et al. 2011) and the lateral occipital cortex (LOC; Tyler et al. 2013), feature complexity and receptive field size increase, leading to higher-order representational formats involving object parts and domain-level semantic features (Clarke et al. 2013;Clarke 2015).</p>
<p>In cDNNs from object recognition, starting with lowlevel features such as edges and colors in the first convolutional layer, complexity increases to textures, object parts and finally, object categories in the last network layer.While early cDNN layers show similar activation patterns for images with shared visual features such as similar colors and textures (e.g., orange color of a pumpkin and of a basketball), independent of the conceptual similarity of stimuli, category-specific features explain similarities in later layers (e.g., wings, feathers and a beak for birds).Similar representational transformations have been found along the VVS (Mur et al. 2013;Hebart et al. 2020), revealing that cDNN models from computer vision can accurately reflect neural representations during object recognition.Surprisingly, in contrast to this functional overlap, the most prominently used network "AlexNet" (Krizhevsky et al. 2017) contains one of the simplest architectures.Yet, AlexNet and other, shallower cDNN models such as CorNet (Kubilius et al. 2018(Kubilius et al. , 2019) ) are models that match neural representations relatively well (Nonaka et al. 2021) and show high classification performance in object recognition.On the other hand, it has been demonstrated that recurrency in DNN architectures may further improve the match to neural representations during object recognition (Kubilius et al. 2019;Kietzmann et al. 2019b; van Bergen and Kriegeskorte 2020), suggesting that recurrent DNNs should be increasingly used in the future to study neural representations.</p>
<p>Neural representations can be mapped onto DNN feature spaces via RSA, using the fact that RSA reflects representational geometry independent of data modality (Fig. 1B).</p>
<p>Treating DNN feature activations as patterns allows one to compute similarities between all pairs of stimuli in each layer of the DNN or model, resulting in one RDM per DNN layer/model.Subsequently, DNN RDMs and neural RDMs can be correlated and compared for their similarity structures Fig. 1 Linking representational formats in DNNs and in semantic models to representations in the human brain.A Different representational formats in convolutional deep neural networks (cDNNs) and deep natural language processing models (dNLPs).In cDNNs, images are processed with a gradient in complexity, comparable to the human ventral visual pathway.As in the brain, basic visual information is processed in the first layer (whose receptive field properties roughly match those of V1) and is then passed through the convolutional layers of the network, which process increasingly complex information.After the last convolutional layer, the connections change, as now each neuron is connected to each neuron in subsequent layers (fullyconnected).The network then chooses the most active (i.e., most likely) label in the highest layer.Early layers of the cDNN process edges, colors, and textures, such that e.g., animals of different categories (species) are sorted together e.g., based on their color.DNN neu-rons in middle layers have more complex receptive fields, processing object features such as the beak of the flamingo or their long necks and legs.Late layers respond to a visual prototype of the flamingo and show distinct representational similarity patterns (e.g., all bears are sorted together, even if they differ in their low-level visual features).While convolutional layers process lower-level sensory information, fully-connected layers process higher-order visual features including object classes.In addition to the cDNN, one can use dNLP models to quantify the representation of semantic information, based on embedding vectors of words or sentences.B The neural activations corresponding to all pairs of stimuli can be used to generate RDMs for all layers of the cDNN and the semantic dNLP, which can then be correlated to RDMs from brain or behavioral data generated using the same stimuli (Mur et al. 2013;McClure and Kriegeskorte 2016).Several visualization techniques such as multi-dimensional scaling (MDS; Lin et al. 2019; Fig. 1A), class activation maps (Zhou et al. 2015) or similarities from RDMs (Kriegeskorte and Golan 2019) provide information on the representational formats that are processed in individual DNN layers or models.Linking DNN representations to neural representations thus allows one to examine properties of neural representations (e.g., in terms of brain regions, oscillation frequencies, or latencies) that reflect different representational formats and are for example responsible for the shift from perceptual to conceptual formats.</p>
<p>Many studies mentioned above employed cDNN models from image classification challenges to assess representational formats during neural processing.However, these cDNN models are limited to visual and higher-order visual representations, while category abstraction and many memory functions rely on conceptual representations (Clarke 2019).More specifically, even though perceptual features may allow for the derivation of conceptual representations in a feed-forward way (Clarke et al. 2018), this process can be facilitated by top-down semantic knowledge (Taylor et al. 2012;van Kesteren et al. 2013), emphasizing the need for models that involve semantic processing.In fact, research on language processing even provided evidence for multiple levels of semantic features, as indicated by faster performance for general domain features compared to exemplarspecific features (Randall et al. 2004;Macé et al. 2009;Devereux et al. 2018).Thus, instead of focusing on one single cDNN model, Clarke (2019) proposed the additional use of deep learning models from natural language processing (deep Natural Language Processing models; dNLP).Previous research showed that these models can accurately reflect conceptual representations during object recognition in the VVS (Devereux et al. 2018) and even during more abstract tasks such as those involving narrative content (Lee and Chen 2022).DNLP models are trained on text input (e.g., wiki pages, books, user reviews) rather than images.These corpus-based models, such as BERT (Devlin et al. 2018), the Google Sentence encoder (Cer et al. 2018), Infersent (Conneau et al. 2017) or GPT-3 (Brown et al. 2020) assign a word embedding vector to each word or sentence based on co-occurrences of these concepts, and these vectors can then be used to study semantic similarities.</p>
<p>Taken together, although cDNN models are only very rough approximations to the neural processes and connectivity within the VVS, the findings reviewed above demonstrate that representations in DNN layers are relatively accurate models of the neural representations at different levels of abstraction, which makes them specifically interesting to study neural properties of and changes in representational format (Marblestone et al. 2016;Kietzmann et al. 2019a;Richards et al. 2019;Storrs and Kriegeskorte 2019;Saxe et al. 2021).Surprisingly, only few studies thus far employed DNNs to study representational formats of memory representations.Davis et al. (2021) were among the first to apply visual and semantic DNN model features to investigate the effects of representational formats during encoding on subsequent memory.Participants first viewed images of natural objects that they had to name and were then tested in two retrieval tasks.During retrieval, the authors separately made either perceptual or conceptual formats task-relevant by either displaying old and new images (perceptual) or the label of the concepts (conceptual).Using fMRI, the authors could show that matching of encoding representations to RDMs from either a visual cDNN or semantic models (taxonomy/encyclopedic) predicted memory performance in both retrieval conditions.Conceptual and perceptual formats recruited different brain areas though, namely the anterior VVS and the early visual cortex, respectively.Interestingly, although the two representational formats were linked to different brain areas depending on the retrieval task (perceptual/conceptual), the performance in both tasks benefited from matching with the respective other format during retrieval as well, suggesting that perceptual memory benefits from top-down information, while bottom-up visual information facilitates conceptual memory.</p>
<p>The role of representational formats for understanding the dynamics of memory representations</p>
<p>Several findings of item-specific memory representations concern frontoparietal and midline regions (e.g., Baldassano et al. 2017;Fernandino et al. 2022;Huth et al. 2012;Lee and Kuhl 2016).Since these areas do not reflect sensory processing, it is currently not clear why they exhibit pronounced stimulus specificity.In the future, DNNs trained on more complex objective functions than stimulus categorization may account for the formats in these areas.However, studies using RSA identified an important role of the VVS in transforming visual into conceptual representations (DiCarlo et al. 2012;Kravitz et al. 2013;Martin et al. 2018).This transformation from perceptual to conceptual representations during perception (Kriegeskorte and Kievit 2013) may also give rise to different representational formats of memory traces, which may rely predominantly on either perceptual or semantic representational formats as well.Since both visual and semantic formats play a role during object recognition, the question arises whether memory traces during the different stages of memory processing-encoding, shortterm memory maintenance, consolidation, and retrievalwould reveal such formats as well.Already at early visual processing steps, top-down knowledge plays an important role.Typically, we do not encounter objects without any prior information on their use and behavioral importance but using conceptual representations that are stored in longterm memory (Tulving and Watkins 1975;Xue 2018).At the same time, neural representations are not stable but subject to transformation processes (Xue 2022).According to the neural-psychological-representation-correspondence (NPRC) by Gilboa and Moscovitch (2021) memory traces can occur in different forms, a given episode can be represented in an event-specific visual format, while at the same time containing information about schemas and semantic information from prior knowledge.In addition, these representational formats may dynamically change due to various factors such as time after encoding, task context, goals, or prior knowledge, resulting in transformations between formats.How are these representations formed and especially, how are they transformed?</p>
<p>One candidate framework on the transformation of visual information into long-term memory is based on the concept of semantization or gist-abstraction (Konkle et al. 2010;Winocur and Moscovitch 2011;Linde-Domingo et al. 2019;Lifanov et al. 2021).During semantization, sensory information is integrated into long-term semantic knowledge through representational transformations (Paller and Wagner 2002;Xue 2018;Favila et al. 2020).According to this framework, conceptual features of a sensory input are selectively strengthened, while detailed sensory information is reduced, facilitating the integration of novel experiences with prior semantic knowledge.In line with this theory, studies found better memory performance for conceptual features as compared to low-level/perceptual features (Bainbridge et al. 2017;Bainbridge 2019;Linde-Domingo et al. 2019).Memory was also improved for stimuli that could easily be linked with pre-existing schemas as compared to those that did not match a schema (van Kesteren et al. 2013), and reaction times were faster for conceptual compared to perceptual features during recall (Lifanov et al. 2021).Thus, semantization can be defined as a transformation from detail-rich to compressed gist-like representations, suggesting a change in representational format.In addition, one would expect a transformation of memory traces such that they become more similar for stimuli that share the same prototypical conceptual features (e.g., beak and wings of birds) and less similar for stimuli with similar visual details (e.g., red parrot and red tomato).</p>
<p>In this case, semantization may actually lead to an increase of false alarms to semantically similar lures or to novel exemplars of previously presented concepts.Indeed, Naspi et al. (2021a) found more errors for lures consisting of prototypical exemplars of a given category, indicating that enhanced gist-abstraction during encoding or consolidation can lead to increased false alarms at recognition.At the same time, false alarms also increased for lures with high visual similarity to originally encoded images, suggesting that not all unique visual information is lost after encoding.Delhaye and Bastin (2021), who focused on the impact of visual or semantic processing during encoding, found semantization to be independent of encoding type format.Interestingly, Naspi et al. (2021b) could show that both visual and semantic formats in VVS contributed to successful memory encoding, but categorical information in regions anterior to the VVS predicted later forgetting.These studies demonstrate that there is no rigid transformation from visual to semantic formats during encoding but an interplay between different formats at different steps of memory processing.</p>
<p>A substantial body of evidence has shown off-line replay of memory representations during sleep (Frankland and Bontempi 2005;Deuker et al. 2013;Dudai et al. 2015).Integration of novel experiences into prior knowledge is assumed to be caused by strengthening of those features that are shared across encoded contents (Káli and Dayan 2004;Lewis and Durrant 2011;Himmer et al. 2019).These results are in line with the idea that replay facilitates generalization processes (Liu et al. 2019).While sleep contributes to integration by enhancing memory for shared features of newly encoded content, there is also evidence for sleep to prevent loss of unique feature representations (Schapiro et al. 2017).This might indicate that not only conceptual but multiple representational formats, including perceptual details, are strengthened due to off-line replay during sleep, which in turn might slow down the supposed loss of visual detail over time.</p>
<p>Perceptual details might be subject to faster forgetting in order to promote an integration of conceptual or superordinate categorical features into memory, considering that different representational formats of a memory trace may be forgotten independently (Brady et al. 2013).In line with supposed gist abstraction and loss of visual detail due to semantization, Lifanov et al. (2021) found that a perceptualconceptual gap (e.g., a shift from faster reaction times for perceptual features to faster reaction times for conceptual features) increased over time, suggesting faster forgetting of visual details while conceptual features were integrated into long-term memory (LTM).During retrieval, conceptual features were activated prior to visual detail when no visual input was present (Linde-Domingo et al. 2019;Davis et al. 2021) and were found to be involved during memory retrieval of both perceptual and conceptual representational formats (Davis et al. 2021; see above).While these results could lead to the wrong conclusion of a complete loss of visual detail over time, Ferreira et al. (2019) found higher neural similarities between category-related but also between episode-unique information, demonstrating that conceptualization during semantization does not necessarily come at the cost of visual detail.</p>
<p>Overall, these findings deliver further evidence for a dynamic transformation of representational formats (Paller and Wagner 2002;Xue 2018Xue , 2022;;Favila et al. 2020;Liu et al. 2020Liu et al. , 2021).Yet, the question remains whether consolidation induces transformations of memory traces from one format to another (i.e., from perceptual to conceptual, losing all perceptual detail) or whether memory traces consist of multiple formats with only their accessibility changing across time, and depending on encoding tasks and/or retrieval cues.While this can be investigated by analyses of encoding-retrieval similarity (i.e., Ten Oever et al. 2021), DNNs could be used to further investigate the underlying representational formats and to address the question whether these formats are subject to transformation or continue to coexist.In the next section of this review, we will take a closer look at how DNNs may be used to investigate such changes in the representational format of memory traces during the earliest possible stage when semantization might occur, i.e., directly after the offset of a stimulus, and during subsequent processing stages.</p>
<p>Beyond recognition-using DNNs to investigate early stages of semantization</p>
<p>As described above, previous research has demonstrated that cDNN features can be used to study representational formats during object recognition (Güçlü and van Gerven 2015;Cichy et al. 2016;Kuzovkin et al. 2018;Clarke et al. 2018).However, these studies did not assess the question of how these visual inputs were transformed during the consecutive stages of long-term memory encoding, memory consolidation, and long-term memory retrieval.Investigating representational formats during initial stages of memory formation, we could test whether the supposedly slow process of gist-abstraction (O'Reilly et al. 2014) unfolding during systems consolidation might happen more rapidly and already in earlier post-encoding stages.</p>
<p>A very recent study thus set out to investigate if DNN similarities would reflect neural similarities during a visual short-term memory (VSTM) task (Fig. 2A), with VSTM being the earliest offline processing stage following perception (Liu et al. 2020).A follow-up study (Fig. 2B) then tested the effects of short-term maintenance and consecutive transformation stages on LTM retrieval (Liu et al.  2 Representational formats during visual short-term memory maintenance and long-term memory encoding and retrieval.A Participants saw cue-object pairs and maintained the objects during a long maintenance period, which was followed either by a picture of the same item or of a similar lure.B During a subsequent longterm memory test, the cue word was presented, and participants were asked to vividly imagine the associated image.Afterwards they conducted a forced choice test on the category of the image.C Analysis methods: We combined RDMs from eight different layers of a cDNN and from a dNLP to model lower-order visual, higher-order visual and semantic representational formats, respectively.Model RDMs were then compared to corresponding RDMs from intracranial EEG data during the different task periods.D During encoding, higher-order visual formats were gradually transformed into semantic representational formats.E More pronounced semantic formats during encoding predicted subsequent long-term memory success.F Semantic but not visual formats were found during successful memory retrieval (s=seconds) 1 3 2021) to test whether semantization already occurs during VSTM and whether early semantization may improve LTM performance.</p>
<p>VSTM is defined as the active maintenance of visual information for a short period of time in a limited capacity store (Baddeley and Hitch 1974;Luck and Vogel 1997).Current research suggests an important role of VSTM for integrating information, bridging the gap between perception and long-term memory (Chota and Van der Stigchel 2021), specifically involving regions along the VVS (Meyers et al. 2008;Cichy et al. 2014).Studies indicate "dynamic coding" with neurons carrying different information across the maintenance period (Stokes 2015), reflected by distinct representational formats.Along the VVS, these distinct formats have already been observed during object recognition (e.g., Devereux et al. 2018).Is there evidence for different representational formats present already during VSTM and for a shift from perceptual to semantic formats prior to LTM retrieval?</p>
<p>To address this question, we first analyzed similarities of neural patterns during an encoding and a maintenance period in a delayed matching to sample task (Fig. 2A) while participants (presurgical epilepsy patients) underwent intracranial EEG (iEEG) recordings (Liu et al. 2020).We then examined whether neural patterns during encoding reappeared during maintenance and long-term memory retrieval (Liu et al. 2021).During the maintenance period of the VSTM task, we found item-specific reinstatement of information from two distinct time windows during encoding, an early (250-770 ms post stimulus onset) and a later period (1000-1980 ms post stimulus onset), suggesting that both periods may contain different representational formats.Further analyses revealed higher item-specificity for the late encoding time window, indicating that specifically late representational formats are maintained faithfully during VSTM.Thus, neural similarity analysis revealed reinstatement of two distinct formats, but how exactly can these formats be characterized?</p>
<p>The integration of visual input with long-term knowledge suggests an involvement of semantic information (Cichy et al. 2014;Stokes 2015), while cDNNs from object recognition capture visual and higher-order visual features only.Thus, we decided to combine a cDNN with a dNLP model to investigate matching of neural representational formats to either visual formats from the cDNN or semantic formats from the dNLP model (Fig. 2C).</p>
<p>Current theories suggest an involvement of VSTM in the transformation of visual stimuli into abstract long-term memory representations (Meyers et al. 2008;Cichy et al. 2014;Stokes 2015).Accordingly, we found evidence for visual features during stimulus encoding periods followed by abstract semantic representations during later processing periods, indicating a transformation of representational formats from sensory to abstract (i.e., non-perceptual) formats (Fig. 2D).Specifically, the absence of sensory information during the later period suggests an interplay between bottom-up visual processing during the early and semantic top-down processing during later processing steps-possibly reflecting the integration of novel sensory stimuli into long-term memory stores (Clarke 2015;Jozwik et al. 2017;O'Donnell et al. 2018).In addition, the presence of a semantic format may be beneficial in order to transform stimuli into a lower-dimension representation with reduced information content, which may provide a functional benefit: Conci et al. (2021) found VSTM capacity to be linked to participants' prior knowledge, with higher capacity if the stimulus meaning was known.</p>
<p>Recent findings from studies using fMRI provide additional evidence of shared representational formats between VSTM and long-term memory retrieval (Vo et al. 2022).Bainbridge et al. (2021) found different levels of abstraction when comparing encoding and retrieval representations.Whereas both fine-grained (e.g., penguin, lion) and coarse (e.g., bird, feline) features were observed during encoding, primarily coarse features were present during recall.Specifically, their results demonstrate a shift from the VVS showing peak activity during encoding to anterior areas during retrieval.Yet, this study does not show a complete loss of perceptual (e.g., fine-grained) information that would reflect a transformation of the same memory trace since this perceptual information was still observable in some areas.Whereas Audrain and McAndrews (2022) also found that memory representations became coarser over time, interestingly they found this generalization was linked to prior knowledge with only congruent semantic stimuli associations integrated in the medial prefrontal cortex (mPFC).This suggests that rapid semantization, i.e., due to congruency to prior knowledge, can facilitate memory transformation.</p>
<p>A follow-up analysis on the results during VSTM described above supports these findings even further (Liu et al. 2021): In line with our results from the maintenance period, we found that transformation into semantic formats was linked to subsequent LTM performance.Specifically, remembered images showed more pronounced semantic formats during encoding compared to forgotten images (Fig. 2E) and were linked to the occurrence of semantic but not sensory formats during retrieval (Fig. 2F).Interestingly, item-specific memory representations during retrieval were more similar to the visual short-term maintenance period compared to encoding.Together with better memory performance when conceptual formats were abstracted during encoding, the findings of these studies suggest semantization already happening at early stages of memory (i.e., encoding and VSTM) which in turn leads to better long-term memory formation.Overall, there seem to be parallel generalization processes during both encoding and consolidation, modulated by factors such as prior knowledge, which are fundamental to memory formation in both humans and neural networks (Kumaran et al. 2016) and supposedly are not limited to post-encoding consolidation periods.</p>
<p>Beyond sensory formats-affective and contextual dimensions</p>
<p>In previous sections we focused on perceptual and conceptual formats, yet we hardly believe that these two cover the entirety of representational dimensions in neurocognitive processing (Gilboa and Moscovitch 2021).There might be additional, more abstract formats (e.g., involving scripts and schemata) or additional dimensions, such as the affective evaluation or the contextual embedding of an episode.When we think back to the example described above on a day at the beach, we may not only remember its multisensory aspects (e.g., the feeling of the sand, the sound of waves) but also the emotions we felt in that moment.</p>
<p>Indeed, there is evidence for neural representations of affective dimensions and categories across large-scale brain networks (Kragel and LaBar 2016), even spanning to areas along the VVS (Kragel et al. 2019).Concerning memory representations of emotional contents and their potential contextual embedding, emotions have been shown to modulate memory formation via processes of emotional binding (Mather 2007;Yonelinas and Ritchey 2015).Typically, emotions enhance memory (Talmi 2013;LaBar and Cabeza 2006), and this is particularly the case for negative emotions (Kensinger 2007).Interestingly, negative emotions seem to specifically enhance certain representational formats, with some studies indicating better accessibility of perceptual formats (Kensinger et al. 2006(Kensinger et al. , 2007)).Similar effects may occur for negative emotions induced by psychosocial stress, a particularly ecologically relevant condition (Freund et al. 2023).How will affective evaluation and contextual embedding affect neural representations of different stimuli of a stressful episode?</p>
<p>It is well established that the effects of stress on memory depend on the phase (Roozendaal 2002;Het et al. 2005;Joëls et al. 2006;Wolf 2009;Shields et al. 2017) of memory processing.While stress before or during encoding may have mixed effects, it is usually beneficial when experienced after encoding or during consolidation.By contrast, experiencing stress shortly before or during retrieval is consistently detrimental to performance (de Quervain et al. 1998;Wolf 2009;Shields et al. 2017).</p>
<p>In order to investigate memories of a stressful episode, we applied an ecologically valid experimental design in which stimuli are incidentally encoded during a psychosocial stress intervention (Wolf 2019).In the Trier Social Stress Test (TSST; Kirschbaum et al. 1993), participants conduct a mock job interview in front of a neutrally acting committee.In previous studies, Wiemers et al. adapted the TSST to contain a number of different everyday objects (Wiemers et al. 2013;Wolf 2019).In this version of the TSST (Fig. 3A), the interview room and especially the table in front of the committee are equipped with a number of different objects, which are incidentally encoded during the TSST.Half of these objects are manipulated by the committee members in a standardized way to render them more salient for the participant ("central objects"), while other objects are not manipulated ("peripheral objects").Several studies showed that central objects are better remembered than peripheral objects, and that this effect is increased by psychosocial stress (Wiemers et al. 2013(Wiemers et al. , 2014;;Herten et al. 2017a, b).A Participants conducted a psychosocial stress intervention (Trier Social Stress Test, TSST) in which some objects were manipulated by stress-inducing committee members (central objects) while others were not (peripheral objects).A second group of participants took part in a non-stressful control version of the task.B We found that central objects were better remembered than peripheral objects, and that this effect was enhanced in stressed participants.C On the next day, pictures of all objects were presented in the MRI scanner, and we measured their representational similarity.In the left amygdala, we found that central objects were more similar to other objects of the same episode and dissimilar to distractor objects when comparing stressed vs. control participants (upper row).Better memory performance for these objects was explained by the similarity of their representations to the representation of the stressor, i.e., the committee members' faces (lower row)</p>
<p>These results are in line with the hypothesis that stress particularly enhances the encoding of central cues (Easterbrook 1959;Wolf 2019).</p>
<p>We speculated that stress may enhance later memory for central objects by supporting generalization or binding processes of their neural representations.The term "binding" typically refers to the formation of integrated representations of multiple aspects of an episode, i.e., of different elements within one spatiotemporal context, and has been proposed to rely critically on the hippocampus (Ranganath 2010;Eichenbaum 2017).Yonelinas and Ritchey (2015) have suggested an "emotional binding" account according to which an emotion instead of the spatio-temporal context binds the features of an episode.They proposed that emotional binding occurs in the amygdala, that it may outweigh spatio-temporal binding processes in the hippocampus, and that this is the reason why emotional memories are less likely to be forgotten.Another binding approach proposed by Mather (2007) may provide an explanation for the superior memory of the central aspects in an emotional episode.In her "object-based framework", she suggests that emotionally arousing objects attract attention and that this is the reason why the constituent features of the object are bound and well-remembered.</p>
<p>On the neural level, these binding approaches would predict higher similarity (lower representational distances) between neural representations of central objects.Generalization effects in humans have been previously found in a fear learning paradigm and were predictive of long-term fear memories (Visser et al. 2011(Visser et al. , 2013)).Specifically, pattern similarity changes in ventromedial PFC at the time of learning could predict the behavioral expression of long-term fear learning, i.e., changes in pupil dilation (Visser et al. 2013).In addition, fear learning led to generalization in other brain regions such as anterior cingulate cortex, amygdala, and superior frontal gyrus (Visser et al. 2011).These results suggest that increased pattern similarity between conditioned and unconditioned stimuli supports fear conditioning in a variety of brain regions including the amygdala-i.e., that higher pattern similarity in these regions reflects generalization and binding processes.</p>
<p>We investigated the effects of stress on memory representations and their impact on subsequent recognition memory (Bierbrauer et al. 2021).We conducted the TSST (Fig. 3A) and a non-stressful control version and tested memory performance for central and peripheral objects and the faces of the committee members.In line with previous studies (Wiemers et al. 2013), central objects were generally better remembered than peripheral objects (Fig. 3B).This effect was significantly more pronounced for stressed participants.Using fMRI, we measured the neural representations of central and peripheral objects and of the faces (Fig. 3C).Interestingly, we found that neural representations of central objects in the stressful episode became more similar to other objects from the same episode and dissimilar to distractor objects (i.e., objects that belonged to other potential episodes).In addition, we could explain higher memory performance for these objects by the similarity of their representations to the representation of the stressor, i.e., the committee members' faces.This suggests that the beneficial effects of stress on memory formation rely on a generalization of neural representations within the stressful episode, which is driven by higher similarity with the representation of the stressor.This representational change may also explain why memories of stressful experiences can be triggered by neutral cues with low representational distance to the stressor.</p>
<p>Our study demonstrates that investigating the representational structure or "geometry" of affective and contextual dimensions of memory traces may provide mechanistic insights into representational formats beyond perceptual and conceptual dimensions.In other words, understanding how neural representations are transformed by factors such as stress will help us understand how these factors change our memories.In the future, it would be interesting to link perceptual and conceptual format from DNNs to data from affective episodes to further broaden the understanding of how affective evaluation and contextual embedding modulates these formats, and how they may act as additional formats.</p>
<p>Conclusions</p>
<p>We started out describing several aspects of the memory for a recently experienced episode.The mental "image" of this episode as well as its non-sensory aspects relate to neural representations in various brain regions, across several levels of brain organization and in different representational formats.We described that RSA is well suited to assess the structure of memory representations (i.e., their representational geometry) and that we can employ DNNs to differentiate multiple representational formats.Not only can we quantify the formats themselves, but we also gain insights into how one format is transformed into another format and how this process may benefit memory consolidation and long-term memory retrieval.Importantly, we can demonstrate that generalization is not limited to consolidation but may also happen more rapidly, i.e., during encoding and maintenance.In this review we highlighted visual, higher-order visual and semantic formats that can be easily modeled by current cDNN and dNLP architectures.These models only provide a first approximation to the large variety of representational formats that are processed in the brain, including formats along dimensions such as affective evaluation or contextual embedding.In addition, they indicate the importance of combining computational and neuroscientific methods to understand memory.We propose that elucidating the neural representations underlying episodic memories should be a major goal in memory research.</p>
<p>Fig.</p>
<p>Fig.2Representational formats during visual short-term memory maintenance and long-term memory encoding and retrieval.A Participants saw cue-object pairs and maintained the objects during a long maintenance period, which was followed either by a picture of the same item or of a similar lure.B During a subsequent longterm memory test, the cue word was presented, and participants were asked to vividly imagine the associated image.Afterwards they conducted a forced choice test on the category of the image.C Analysis methods: We combined RDMs from eight different layers of a</p>
<p>Fig. 3
3
Fig. 3 Effects of psychosocial stress on memory representations.A Participants conducted a psychosocial stress intervention (Trier Social Stress Test, TSST) in which some objects were manipulated by stress-inducing committee members (central objects) while others were not (peripheral objects).A second group of participants took part in a non-stressful control version of the task.B We found that central objects were better remembered than peripheral objects, and that this effect was enhanced in stressed participants.C On the next</p>
<p>Data availability Enquiries about data availability of the experiments presented in this review should be directed to the corresponding author.Funding Open Access funding enabled and organized by Projekt DEAL.R.H. and A.B. received funding from the Deutsche Forschungsgemeinschaft (DFG; German Research Foundation)-project no.122679504-SFB 874.O.T.W. was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)-project no.122679504-SFB 874, project no.419039274-FOR 2812 and project no.396877307 (ORA framework).N.A. received funding from the Deutsche Forschungsgemeinschaft (DFG; German Research Foundation)-project Nos.316803389-SFB 1280, via project no.122679504-SFB 874, and via ERC CoG 864164.Author contributions RH: Conceptualization, writing-original draft, writing-review and editing, visualization, project administration.AB: Conceptualization, writing-original draft, writing-review and editing.OTW: Resources, writing-review and editing.NA: Conceptualization, resources, writing-review and editing, supervision, project administration.DeclarationsConflict of interest The authors declare that they have no competing interests.Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made.The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material.If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.To view a copy of this licence, visit http:// creat iveco mmons.org/ licen ses/ by/4.0/.Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Eda ; Adrian, Ej, G St-Yves, Y Wu, 10.1038/s41593-021-00962-xNat Neurosci. 251928. 2022The Basis of Sensation, the Action of the Sense Organs</p>
<p>Schemas provide a scaffold for neocortical integration of new memories over time. S Audrain, M P Mcandrews, 10.1038/s41467-022-33517-0Nat Commun. 1357952022</p>
<p>Memory formation by refinement of neural representations: the inhibition hypothesis. N Axmacher, C E Elger, J Fell, 10.1016/j.bbr.2007.12.018Behav Brain Res. 1892008</p>
<p>Working memory. Psychology of learning and motivation. A D Baddeley, G Hitch, 1974Elsevier</p>
<p>Memorability: how what we see influences what we remember. Psychology of learning and motivation. W A Bainbridge, 2019Academic Press</p>
<p>Memorability: a stimulusdriven perceptual neural signature distinctive from memory. W A Bainbridge, D D Dilks, A Oliva, 10.1016/j.neuroimage.2017.01.063Neuroimage. 1492017</p>
<p>Distinct representational structure and localization for visual encoding and recall during visual imagery. W A Bainbridge, E H Hall, C I Baker, 10.1093/cercor/bhaa329Cereb Cortex. 312021</p>
<p>Pattern separation in the human hippocampal CA3 and dentate gyrus. A Bakker, C B Kirwan, M Miller, Cel Stark, 10.1126/science.1152882Science. 3192008</p>
<p>Discovering event structure in continuous narrative perception and memory. C Baldassano, J Chen, A Zadbood, 10.1016/j.neuron.2017.06.041Neuron. 952017</p>
<p>The memory trace of a stressful episode. A Bierbrauer, M-C Fellner, R Heinen, 10.1016/j.cub.2021.09.044Curr Biol. 312021</p>
<p>P Boghossian, Companion to metaphysics. J Kim, E Sosa, R Rs, Oxford1995</p>
<p>Detecting representations of recent and remote autobiographical memories in vmPFC and hippocampus. H M Bonnici, M J Chadwick, A Lutti, 10.1523/JNEUROSCI.2475-12.2012J Neurosci. 322012</p>
<p>Real-world objects are not represented as bound units: independent forgetting of different object details from visual memory. T F Brady, T Konkle, G A Alvarez, A Oliva, 10.1037/a0029649J Exp Psychol Gen. 1422013</p>
<p>Making memories: brain activity that predicts how well visual experience will be remembered. J B Brewer, Z Zhao, J E Desmond, 10.1126/science.281.5380.1185Science. 2811998</p>
<p>Fast track to the neocortex: a memory engram in the posterior parietal cortex. S Brodt, S Gais, J Beck, M Erb, K Scheffler, M Schönauer, 10.1126/science.aau2528Science. 36264182018</p>
<p>Prospective representation of navigational goals in the human hippocampus. T I Brown, V A Carr, K F Larocque, 10.1126/science.aaf0784Science. 3522016</p>
<p>Language models are fewshot learners. T B Brown, B Mann, N Ryder, arXiv:2005.14165Adv Neurol Inform Process Syst. 2020</p>
<p>Deep neural networks rival the representation of primate IT cortex for core visual object recognition. C F Cadieu, H Hong, Dlk Yamins, 10.1371/journal.pcbi.1003963PLoS Comput Biol. 10632014</p>
<p>D Cer, Y Yang, S-Y Kong, arXiv:1803.11175Universal sentence encoder. 2018</p>
<p>Dissociating memory traces and scenario construction in mental time travel. S Cheng, M Werning, T Suddendorf, 10.1016/j.neubiorev.2015.11.011Neurosci Biobehav Rev. 602016</p>
<p>Dynamic and flexible transformation and reallocation of visual working memory representations. S Chota, S Van Der Stigchel, 10.1080/13506285.2021.1891168Vis Cogn. 29682021</p>
<p>Resolving human object recognition in space and time. R M Cichy, D Pantazis, A Oliva, 10.1038/nn.3635Nat Neurosci. 172014</p>
<p>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence. R M Cichy, A Khosla, D Pantazis, 10.1038/srep27755Sci Rep. 6277552016</p>
<p>Dynamic information processing states revealed through neurocognitive models of object semantics. A Clarke, 10.1080/23273798.2014.970652Lang Cogn Neurosci. 302015</p>
<p>Neural dynamics of visual and semantic object processing. A Clarke, 10.1016/bs.plm.2019.03.002Psychol Learn Motiv. 702019</p>
<p>The evolution of meaning: spatio-temporal dynamics of visual object recognition. A Clarke, K I Taylor, L K Tyler, 10.1162/jocn.2010.21544J Cogn Neurosci. 232011</p>
<p>From perception to conception: how meaningful objects are processed over time. A Clarke, K I Taylor, B Devereux, 10.1093/cercor/bhs002Cereb Cortex. 232013</p>
<p>Oscillatory dynamics of perceptual to conceptual transformations in the ventral visual pathway. A Clarke, B J Devereux, L K Tyler, 10.1162/jocn_a_01325J Cogn Neurosci. 302018</p>
<p>The nationality benefit: long-term memory associations enhance visual working memory for color-shape conjunctions. M Conci, P Kreyenmeier, L Kröll, 10.3758/s13423-021-01957-2Psychon Bull Rev. 282021. 1982-1990</p>
<p>Supervised learning of universal sentence representations from natural language inference data. A Conneau, D Kiela, H Schwenk, arXiv:1705.023642017</p>
<p>Components of recognition memory: dissociable cognitive processes or just differences in representational complexity?. R A Cowell, T J Bussey, L M Saksida, 10.1002/hipo.20865Hippocampus. 202010</p>
<p>Global neural pattern similarity as a common basis for categorization and recognition memory. T Davis, G Xue, B C Love, J Neurosci. 342014</p>
<p>Visual and semantic representations predict subsequent memory in perceptual and conceptual memory tests. S W Davis, B R Geib, E A Wing, 10.1093/cercor/bhaa269Cereb Cortex. 312021</p>
<p>Stress and glucocorticoids impair retrieval of long-term spatial memory. D J De Quervain, B Roozendaal, J L Mcgaugh, 10.1038/29542Nature. 3941998</p>
<p>The significance of neural ensemble codes during behavior and cognition. S A Deadwyler, R E Hampson, Annu Rev Neurosci. 201997</p>
<p>Neural representation and the cortical code. R C Decharms, A Zador, 10.1146/annurev.neuro.23.1.613Annu Rev Neurosci. 232000</p>
<p>Semantic and perceptual encoding lead to decreased fine mnemonic discrimination following multiple presentations. E Delhaye, C Bastin, 10.1080/09658211.2020.1849309Memory. 292021</p>
<p>Memory consolidation by replay of stimulus-specific neural activity. L Deuker, J Olligs, J Fell, 10.1523/JNEUROSCI.0414-13.2013J Neurosci. 332013</p>
<p>Integrated deep visual and semantic attractor neural networks predict fMRI pattern-information along the ventral object processing pathway. B J Devereux, A Clarke, L K Tyler, 10.1038/s41598-018-28865-1Sci Rep. 8106362018</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M-W Chang, K Lee, K Toutanova, arXiv:1810.048052018</p>
<p>How does the brain solve visual object recognition?. J J Dicarlo, D Zoccolan, N C Rust, 10.1016/j.neuron.2012.01.010Neuron. 732012</p>
<p>The consolidation and transformation of memory. Y Dudai, A Karni, J Born, 10.1016/j.neuron.2015.09.004Neuron. 882015</p>
<p>The effect of emotion on cue utilization and the organization of behavior. J A Easterbrook, 10.1037/h0047707Psychol Rev. 661959</p>
<p>How to think about mental content. F Egan, 10.1007/s11098-013-0172-0Philos Stud. 1702014</p>
<p>On the integration of space, time, and memory. H Eichenbaum, 10.1016/j.neuron.2017.06.036Neuron. 952017</p>
<p>Transforming the concept of memory reactivation. S E Favila, H Lee, B A Kuhl, 10.1016/j.tins.2020.09.006Trends Neurosci. 432020</p>
<p>Decoding the information structure underlying the neural representation of concepts. L Fernandino, J-Q Tong, L L Conant, 10.1073/pnas.2108091119Proc Natl Acad Sci. 2022</p>
<p>Retrieval aids the creation of a generalised memory trace and strengthens episode-unique information. C S Ferreira, I Charest, M Wimber, 10.1016/j.neuroimage.2019.07.009Neuroimage. 2011159962019</p>
<p>The organization of recent and remote memories. J A Fodor, 10.1038/nrn1607Nat Rev Neurosci. 62008. 2005Oxford University PressLOT2: the language of thought revisited</p>
<p>Emotional memory in the lab: using the Trier Social Stress Test to induce a sensory-rich and personally meaningful episodic experience. I M Freund, J Peters, M Kindt, R M Visser, Psychoneuroendocrinology. 1481059712023</p>
<p>Neuronal population coding of movement direction. A P Georgopoulos, A B Schwartz, R E Kettner, 10.1126/science.3749885Science. 2334771851986</p>
<p>Spiking neuron models: single neurons, populations, plasticity. W Gerstner, W M Kistler, 2002Cambridge University PressCambridge, UK</p>
<p>No consolidation without representation: correspondence between neural and psychological representations in recent and remote memory. A Gilboa, M Moscovitch, 10.1016/j.neuron.2021.04.025Neuron. 1092021</p>
<p>Cellular basis of working memory. P S Goldman-Rakic, 10.1016/0896-6273(95)90304-6Neuron. 141995</p>
<p>Rapid neural coding in the retina with relative spike latencies. T Gollisch, M Meister, 10.1126/science.1149639Science. 3192008</p>
<p>The spatiotemporal neural dynamics of object location representations in the human brain. M Graumann, C Ciuffi, K Dwivedi, 10.1038/s41562-022-01302-0Nat Hum Behav. 62022</p>
<p>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream. U Güçlü, Maj Van Gerven, 10.1523/JNEUROSCI.5023-14.2015J Neurosci. 352015</p>
<p>Deep learning for object detection and scene perception in self-driving cars: survey, challenges, and open issues. A Gupta, A Anpalagan, L Guan, A S Khwaja, 10.1016/j.array.2021.100057Array. 101000572021</p>
<p>Selective erasure of a fear memory. J-H Han, S A Kushner, A P Yiu, Science. 3232009</p>
<p>Distributed and overlapping representations of faces and objects in ventral temporal cortex. J V Haxby, M I Gobbini, M L Furey, 10.1126/science.1063736Science. 2932001</p>
<p>Decoding neural representational spaces using multivariate pattern analysis. J V Haxby, A C Connolly, J S Guntupalli, 10.1146/annurev-neuro-062012-170325Annu Rev Neurosci. 372014</p>
<p>Decoding mental states from brain activity in humans. J-D Haynes, G Rees, 10.1038/nrn1931Nat Rev Neurosci. 72006</p>
<p>Deconstructing multivariate decoding for the study of brain function. M N Hebart, C I Baker, 10.1016/j.neuroimage.2017.08.005Neuroimage. 1802018</p>
<p>Revealing the multidimensional mental representations of natural objects underlying human similarity judgements. M N Hebart, C Y Zheng, F Pereira, C I Baker, 10.1038/s41562-020-00951-3Nat Hum Behav. 42020</p>
<p>The Organization of Behavior. D O Hebb, 10.1016/s0361-9230(99)00182-3Brain research bulletin. 505-61949. 1949Wiley</p>
<p>The role of eye fixation in memory enhancement under stress -an eye tracking study. N Herten, T Otto, O T Wolf, 10.1016/j.nlm.2017.02.016Neurobiol Learn Mem. 1402017a</p>
<p>Memory for objects and startle responsivity in the immediate aftermath of exposure to the Trier Social Stress Test. N Herten, D Pomrehn, O T Wolf, 10.1016/j.bbr.2017.03.002Behav Brain Res. 3262017b</p>
<p>A meta-analytic review of the effects of acute cortisol administration on human memory. S Het, G Ramlow, O T Wolf, 10.1016/j.psyneuen.2005.03.005Psychoneuroendocrinology. 302005</p>
<p>Rehearsal initiates systems memory consolidation, sleep makes it last. L Himmer, M Schönauer, Dpj Heib, 10.1126/sciadv.aav1695Sci Adv. 516952019</p>
<p>Receptive fields of single neurones in the cat's striate cortex. D H Hubel, T N Wiesel, 10.1113/jphysiol.1959.sp006308J Physiol. 1481959</p>
<p>Receptive fields, binocular interaction and functional architecture in the cat's visual cortex. D H Hubel, T N Wiesel, 10.1113/jphysiol.1962.sp006837J Physiol. 1601962</p>
<p>A continuous semantic space describes the representation of thousands of object and action categories across the human brain. A G Huth, S Nishimoto, A T Vu, J L Gallant, Neuron. 762012</p>
<p>Learning under stress: how does it work?. M Joëls, Z Pu, O Wiegert, 10.1016/j.tics.2006.02.002Trends Cogn Sci. 102006</p>
<p>Finding the engram. S A Josselyn, S Köhler, P W Frankland, 10.1038/nrn4000Nat Rev Neurosci. 162015</p>
<p>Deep convolutional neural networks outperform feature-based but not categorical models in explaining object similarity judgments. K M Jozwik, N Kriegeskorte, K R Storrs, M Mur, 10.3389/fpsyg.2017.01726Front Psychol. 817262017</p>
<p>Off-line replay maintains declarative memories in a model of hippocampal-neocortical interactions. S Káli, P Dayan, 10.1038/nn1202Nat Neurosci. 72004</p>
<p>Decoding the visual and subjective contents of the human brain. Y Kamitani, F Tong, 10.1038/nn1444Nat Neurosci. 82005</p>
<p>Negative emotion enhances memory accuracy. E A Kensinger, 10.1111/j.1467-8721.2007.00506.xCurr Dir Psychol Sci. 1642007</p>
<p>Memory for specific visual details can be enhanced by negative arousing content. E A Kensinger, R J Garoff-Eaton, D L Schacter, 10.1016/j.jml.2005.05.005J Mem Lang. 542006</p>
<p>How negative emotion enhances the visual specificity of a memory. E A Kensinger, R J Garoff-Eaton, D L Schacter, 10.1162/jocn.2007.19.11.1872J Cogn Neurosci. 19112007</p>
<p>Deep supervised, but not unsupervised, models may explain IT cortical representation. S-M Khaligh-Razavi, N Kriegeskorte, 10.1371/journal.pcbi.1003915PLoS Comput Biol. 10152014</p>
<p>Deep neural networks in computational neuroscience. T C Kietzmann, P Mcclure, N Kriegeskorte, 10.1093/acrefore/9780190264086.013.462019aOxford University PressOxford research encyclopedia of neuroscience</p>
<p>Recurrence is required to capture the representational dynamics of the human visual system. T C Kietzmann, C J Spoerer, Lka Sörensen, 10.1073/pnas.1905544116Proc Natl Acad Sci. Natl Acad SciUSA2019b. 1905511644116</p>
<p>Models that allow us to perceive the world more accurately also allow us to remember past events more accurately via differentiation. A Kılıç, A H Criss, K J Malmberg, R M Shiffrin, 10.1016/j.cogpsych.2016.11.005Cogn Psychol. 922017</p>
<p>The "Trier Social Stress Test"-a tool for investigating psychobiological stress responses in a laboratory setting. C Kirschbaum, K M Pirke, D H Hellhammer, 10.1159/000119004Neuropsychobiology. 2890041993</p>
<p>Conceptual distinctiveness supports detailed visual long-term memory for realworld objects. T Konkle, T F Brady, G A Alvarez, A Oliva, 10.1037/a0019165J Exp Psychol Gen. 1392010</p>
<p>Decoding the nature of emotion in the brain. P A Kragel, K S Labar, 10.1016/j.tics.2016.03.011Trends Cogn Sci. 202016</p>
<p>Representation, pattern information, and brain signatures: from neurons to neuroimaging. P A Kragel, L Koban, L F Barrett, T D Wager, 10.1016/j.neuron.2018.06.009Neuron. 992018</p>
<p>Emotion schemas are embedded in the human visual system. P A Kragel, M C Reddan, K S Labar, T D Wager, 10.1126/sciadv.aaw4358Sci Adv. 5743582019</p>
<p>The ventral visual pathway: an expanded neural framework for the processing of object quality. D J Kravitz, K S Saleem, C I Baker, 10.1016/j.tics.2012.10.011Trends Cogn Sci. 172013</p>
<p>Peeling the onion of brain representations. N Kriegeskorte, J Diedrichsen, 10.1146/annurev-neuro-080317-061906Annu Rev Neurosci. 422019</p>
<p>Neural network models and deep learning. N Kriegeskorte, T Golan, 10.1016/j.cub.2019.02.034Curr Biol. 292019</p>
<p>Representational geometry: integrating cognition, computation, and the brain. N Kriegeskorte, R A Kievit, 10.1016/j.tics.2013.06.007Trends Cogn Sci. 172013</p>
<p>Neural tuning and representational geometry. N Kriegeskorte, X-X Wei, 10.1038/s41583-021-00502-3Nat Rev Neurosci. 222021</p>
<p>Representational similarity analysis -connecting the branches of systems neuroscience. N Kriegeskorte, M Mur, P Bandettini, 10.3389/neuro.06.004.2008Front Syst Neurosci. 242008a</p>
<p>Matching categorical object representations in inferior temporal cortex of man and monkey. N Kriegeskorte, M Mur, D A Ruff, 10.1016/j.neuron.2008.10.043Neuron. 602008b</p>
<p>ImageNet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, 10.1145/3065386Commun ACM. 60862017</p>
<p>CORnet: modeling the neural mechanisms of core object recognition. J Kubilius, M Schrimpf, A Nayebi, 10.1101/4083852018</p>
<p>Brain-like object recognition with high-performing shallow recurrent ANNs. J Kubilius, M Schrimpf, K Kar, arXiv:1909.06161Adv Neural Inform Process Syst. 2019</p>
<p>What learning systems do intelligent agents need? Complementary learning systems theory updated. D Kumaran, D Hassabis, J L Mcclelland, Trends Cogn Sci. 202016</p>
<p>Tracking human engrams using multivariate analysis techniques. Handbook of behavioral neuroscience. L Kunz, L Deuker, H Zhang, N Axmacher, 10.1016/B978-0-12-812028-6.00026-42018Elsevier</p>
<p>Activations of deep convolutional neural networks are aligned with gamma band activity of human visual cortex. I Kuzovkin, R Vicente, M Petton, 10.1038/s42003-018-0110-yCommun Biol. 11072018</p>
<p>Cognitive neuroscience of emotional memory. K S Labar, R Cabeza, 10.1038/nrn1825Nat Rev Neurosci. 72006</p>
<p>Global similarity and pattern separation in the human medial temporal lobe predict subsequent memory. K F Larocque, M E Smith, V A Carr, 10.1523/JNEUROSCI.4293-12.2013J Neurosci. 332013</p>
<p>In search of the engram. K Lashley, Symp Soc Exp Biol. 41950</p>
<p>Predicting memory from the network structure of naturalistic events. H Lee, J Chen, 10.1038/s41467-022-31965-2Nat Commun. 1342352022</p>
<p>Reconstructing perceived and retrieved faces from activity patterns in lateral parietal cortex. H Lee, B A Kuhl, 10.1523/JNEUROSCI.4286-15.2016J Neurosci. 362016. 2016</p>
<p>Comparing visual representations across human fMRI and computational vision. D D Leeds, D A Seibert, J A Pyles, M J Tarr, 10.1167/13.13.25J vis. 13252013</p>
<p>Overlapping memory replay during sleep builds cognitive schemata. P A Lewis, S J Durrant, 10.1016/j.tics.2011.06.004Trends Cogn Sci. 152011</p>
<p>Feature-specific reaction times reveal a semanticisation of memories over time and with repeated remembering. J Lifanov, J Linde-Domingo, M Wimber, 10.1038/s41467-021-23288-5Nat Commun. 1231772021</p>
<p>Visualizing representational dynamics with multidimensional scaling alignment. B Lin, M Mur, T Kietzmann, N Kriegeskorte, arXiv:1906.092642019</p>
<p>Evidence that neural information flow is reversed between object perception and object reconstruction from memory. J Linde-Domingo, M S Treder, C Kerrén, M Wimber, 10.1038/s41467-018-08080-2Nat Commun. 101792019</p>
<p>Optogenetic stimulation of a hippocampal engram activates fear memory recall. X Liu, S Ramirez, P T Pang, 10.1038/nature11028Nature. 4842012</p>
<p>Inception of a false memory by optogenetic manipulation of a hippocampal memory engram. X Liu, S Ramirez, S Tonegawa, 10.1098/rstb.2013.0142Philos Trans R Soc Lond B Biol Sci. 3692014a. 20130142</p>
<p>Identification and manipulation of memory engram cells. X Liu, S Ramirez, R L Redondo, S Tonegawa, 10.1101/sqb.2014.79.024901Cold Spring Harb Symp Quant Biol. 792014b</p>
<p>Human replay spontaneously reorganizes experience. Y Liu, R J Dolan, Z Kurth-Nelson, Tej Behrens, 10.1016/j.cell.2019.06.012Cell. 1782019</p>
<p>Stable maintenance of multiple representational formats in human visual short-term memory. J Liu, H Zhang, T Yu, 10.1073/pnas.2006752117Proc Natl Acad Sci. 1172020. 20067 52117</p>
<p>Transformative neural representations support long-term episodic memory. J Liu, H Zhang, T Yu, 10.1126/sciadv.abg9715Sci Adv. 797152021</p>
<p>The capacity of visual working memory for features and conjunctions. S J Luck, E K Vogel, 10.1038/36846Nature. 3901997</p>
<p>The time-course of visual categorizations: you spot the animal faster than the bird. Mj-M Macé, O R Joubert, J-L Nespoulous, M Fabre-Thorpe, 10.1371/journal.pone.0005927PLoS One. 4272009</p>
<p>Toward an integration of deep learning and neuroscience. A H Marblestone, G Wayne, K P Kording, 10.3389/fncom.2016.00094Front Comput Neurosci. 10942016</p>
<p>Integrative and distinctive coding of visual and conceptual object features in the ventral visual stream. C B Martin, D Douglas, R N Newsome, 10.7554/eLife.318732018</p>
<p>Emotional arousal and memory binding: an objectbased framework. M Mather, 10.1111/j.1745-6916.2007.00028.xPerspect Psychol Sci. 22007</p>
<p>Representational distance learning for deep neural networks. P Mcclure, N Kriegeskorte, 10.3389/fncom.2016.00131Front Comput Neurosci. 101312016</p>
<p>Acquisition of chess knowledge in AlphaZero. T Mcgrath, A Kapishnikov, N Tomašev, 10.1073/pnas.2206625119Proc Natl Acad Sci. 119e22066251192022</p>
<p>Dynamic population coding of category information in inferior temporal and prefrontal cortex. E M Meyers, D J Freedman, G Kreiman, 10.1152/jn.90248.2008J Neurophysiol. 1002008</p>
<p>Revealing representational content with pattern-information fMRI-an introductory guide. M Mur, P A Bandettini, N Kriegeskorte, 10.1093/scan/nsn044Soc Cogn Affect Neurosci. 42009</p>
<p>Human object-similarity judgments reflect and transcend the primate-IT object representation. M Mur, M Meys, J Bodurka, 10.3389/fpsyg.2013.00128Front Psychol. 41282013</p>
<p>Encoding and decoding in fMRI. T Naselaris, K N Kay, S Nishimoto, J L Gallant, 10.1016/j.neuroimage.2010.07.073Neuroimage. 562011</p>
<p>Multiple dimensions of semantic and perceptual similarity contribute to mnemonic discrimination for pictures. L Naspi, P Hoffman, B Devereux, 10.1037/xlm0001032J Exp Psychol Learn Mem Cogn. 472021a</p>
<p>Perceptual and semantic representations at encoding contribute to true and false recognition of objects. L Naspi, P Hoffman, B Devereux, A M Morcom, 10.1523/JNEUROSCI.0677-21.2021J Neurosci. 412021b</p>
<p>A Newen, G Vosgerau, Situated mental representations. What are mental representations. Oxford University Press2020</p>
<p>Brain hierarchy score: Which deep neural networks are hierarchically brainlike? iScience. S Nonaka, K Majima, S C Aoki, Y Kamitani, 10.1016/j.isci.2021.103013202124103013</p>
<p>Semantic and functional relationships among objects increase the capacity of visual working memory. R E O'donnell, A Clement, J R Brockmole, 10.1037/xlm0000508J Exp Psychol Learn Mem Cogn. 442018</p>
<p>Complementary learning systems. R C O'reilly, R Bhattacharyya, M D Howard, N Ketz, 10.1111/j.1551-6709.2011.01214.xCogn Sci. 382014</p>
<p>Observing the transformation of experience into memory. K A Paller, A D Wagner, 10.1016/s1364-6613(00)01845-3Trends Cogn Sci. 6002002</p>
<p>Machine learning classifiers and fMRI: a tutorial overview. F Pereira, T Mitchell, M Botvinick, 10.1016/j.neuroimage.2008.11.007Neuroimage. 452009</p>
<p>Mastering the game of Stratego with model-free multiagent reinforcement learning. J Perolat, De Vylder, B Hennes, D , 10.1126/science.add4679Science. 3782022</p>
<p>The physics of representation. R A Poldrack, 10.1007/s11229-020-02793-ySynthese. 1992021</p>
<p>Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals. M Popel, M Tomkova, J Tomek, L Kaiser, J Uszkoreit, O Bojar, Z Žabokrtský, 10.1038/s41467-020-18073-9Nat Commun. 11143812020</p>
<p>Quantifying the adequacy of neural representations for a cross-language phonetic discrimination task: prediction of individual differences. Rds Raizada, F-M Tsao, H-M Liu, P K Kuhl, 10.1093/cercor/bhp076Cereb Cortex. 202010</p>
<p>Distinctiveness and correlation in conceptual structure: behavioral and computational studies. B Randall, H E Moss, J M Rodd, 10.1037/0278-7393.30.2.393J Exp Psychol Learn Mem Cogn. 302004</p>
<p>Binding items and contexts. C Ranganath, 10.1177/0963721410368805Curr Dir Psychol Sci. 193688052010. 21410</p>
<p>Coding of visual objects in the ventral stream. L Reddy, N Kanwisher, 10.1016/j.conb.2006.06.004Curr Opin Neurobiol. 1642006</p>
<p>A deep learning framework for neuroscience. B A Richards, T P Lillicrap, P Beaudoin, 10.1038/s41593-019-0520-2Nat Neurosci. 222019</p>
<p>Stress and memory: opposing effects of glucocorticoids on memory consolidation and memory retrieval. B Roozendaal, 10.1006/nlme.2002.4080Neurobiol Learn Mem. 782002</p>
<p>Representational similarity analysis in neuroimaging: proxy vehicles and provisional representations. A L Roskies, 10.1007/s11229-021-03052-4Synthese. 1992021</p>
<p>If deep learning is the answer, what is the question?. A Saxe, S Nelli, C Summerfield, 10.1038/s41583-020-00395-8Nat Rev Neurosci. 222021</p>
<p>Forgotten ideas, neglected pioneers: richard semon and the story of memory. D L Schacter, 10.4324/97802037201342001Psychology Press</p>
<p>The cognitive neuroscience of constructive memory: remembering the past and imagining the future. D L Schacter, D R Addis, 10.1098/rstb.2007.2087Philos Trans R Soc Lond B Biol Sci. 3622007</p>
<p>Sleep benefits memory for semantic category structure while preserving exemplarspecific information. A C Schapiro, E A Mcdevitt, L Chen, 10.1038/s41598-017-12884-5Sci Rep. 7148692017</p>
<p>Die Mneme als erhaltendes Prinzip im Wechsel des organischen Geschehens. R Semon, Wilhelm Engelmann. 1904Wilhelm EngelmannLeipzig Semon R (1909) Die nmemischen Empfindungen</p>
<p>The effects of acute stress on episodic memory: a meta-analysis and integrative review. N ; Shea, G S Shields, M A Sazma, A M Mccullough, A P Yonelinas, 10.1037/bul0000100Psychol Bull. 1432018. 2017Oxford University PressRepresentation in cognitive science</p>
<p>Mastering the game of go without human knowledge. D Silver, J Schrittwieser, K Simonyan, 10.1038/nature24270Nature. 5502017</p>
<p>Activity-silent" working memory in prefrontal cortex: a dynamic coding framework. B F Skinner, 10.1016/j.tics.2015.05.004Trends Cogn Sci. 191953. 2015Science and human behavior</p>
<p>K R Storrs, N Kriegeskorte, arXiv:1903.01458Deep learning for cognitive neuroscience. 2019</p>
<p>Enhanced emotional memory. D Talmi, 10.1177/0963721413498893Curr Dir Psychol Sci. 224988932013. 21413</p>
<p>Contrasting effects of feature-based statistics on the categorisation and basic-level identification of visual objects. K I Taylor, B J Devereux, K Acres, 10.1016/j.cognition.2011.11.001Cognition. 1222012</p>
<p>An engram of intentionally forgotten information. Ten Oever, S Sack, A T Oehrn, C R Axmacher, N , 10.1038/s41467-021-26713-xNat Commun. 1264432021</p>
<p>Cognitive maps in rats and men. E C Tolman, 10.4324/9780203789155-11Psychol Rev. 5541948</p>
<p>Structure of memory traces. E Tulving, M J Watkins, Psychol Rev. 821975</p>
<p>Objects and categories: feature statistics and object processing in the ventral stream. L K Tyler, S Chiu, J Zhuang, 10.1162/jocn_a_00419J Cogn Neurosci. 252013</p>
<p>Going in circles is the way forward: the role of recurrence in visual inference. R S Van Bergen, N Kriegeskorte, 10.1016/j.conb.2020.11.009Curr Opin Neurobiol. 652020</p>
<p>Differential roles for medial prefrontal and medial temporal cortices in schema-dependent encoding: from congruent to incongruent. Mtr Van Kesteren, S F Beul, A Takashima, 10.1016/j.neuropsychologia.2013.05.027Neuropsychologia. 512013</p>
<p>Neural representation. A survey-based analysis of the notion. O Vilarroya, 10.3389/fpsyg.2017.01458Front Psychol. 814582017</p>
<p>Grandmaster level in StarCraft II using multi-agent reinforcement learning. O Vinyals, I Babuschkin, W M Czarnecki, 10.1038/s41586-019-1724-zNature. 5752019</p>
<p>Associative learning increases trial-by-trial similarity of BOLD-MRI patterns. R M Visser, H S Scholte, M Kindt, 10.1523/JNEUROSCI.2178-11.2011J Neurosci. 312011. 2011</p>
<p>Neural pattern similarity predicts long-term fear memory. R M Visser, H S Scholte, T Beemsterboer, M Kindt, 10.1038/nn.3345Nat Neurosci. 162013</p>
<p>Shared representational formats for information maintained in working memory and information retrieved from long-term memory. V A Vo, D W Sutterer, J J Foster, 10.1093/cercor/bhab267Cereb Cortex. 322022</p>
<p>More than spikes: common oscillatory mechanisms for content specific neural representations during perception and memory. A J Watrous, J Fell, A D Ekstrom, N Axmacher, 10.1016/j.conb.2014.07.024Curr Opin Neurobiol. 2015</p>
<p>Psychology as the behaviorist views it. J B Watson, 10.1037/h0074428Psychol Rev. 201913</p>
<p>Neural encoding and decoding with deep learning for dynamic natural vision. H Wen, J Shi, Y Zhang, 10.1093/cercor/bhx268Cereb Cortex. 282018</p>
<p>A friendly version of the trier social stress test does not activate the HPA axis in healthy men and women. U S Wiemers, D Schoofs, O T Wolf, 10.3109/10253890.2012.714427Stress. 2013</p>
<p>Odors as effective retrieval cues for stressful episodes. U S Wiemers, M M Sauvage, O T Wolf, 10.1016/j.nlm.2013.10.004Neurobiol Learn Mem. 1122014</p>
<p>Cortical overlap and cortical-hippocampal interactions predict subsequent true and false memory. E A Wing, B R Geib, W-C Wang, 10.1523/JNEUROSCI.1766-19.2020J Neurosci. 402020</p>
<p>Memory transformation and systems consolidation. G Winocur, M Moscovitch, 10.1017/S1355617711000683J Int Neuropsychol Soc. 172011</p>
<p>Stress and memory in humans: twelve years of progress?. O T Wolf, 10.1016/j.brainres.2009.04.013Brain Res. 12932009</p>
<p>Memories of and influenced by the Trier Social Stress Test. O T Wolf, 10.1016/j.psyneuen.2018.10.031Psychoneuroendocrinology. 105312019. 2018</p>
<p>The neural representations underlying human episodic memory. G Xue, 10.1016/j.tics.2018.03.00403. 004Trends Cogn Sci. 222018. 2018</p>
<p>From remembering to reconstruction: the transformative neural representation of episodic memory. G Xue, 10.1016/j.pneurobio.2022.102351Prog Neurobiol. 2191023512022</p>
<p>Greater neural pattern similarity across repetitions is associated with better memory. G Xue, Q Dong, C Chen, 10.1126/science.1193125Science. 3302010</p>
<p>Using goal-driven deep learning models to understand sensory cortex. Dlk Yamins, J J Dicarlo, 10.1038/nn.4244Nat Neurosci. 192016</p>
<p>Pattern separation in the hippocampus. M A Yassa, Cel Stark, 10.1016/j.tins.2011.06.006Trends Neurosci. 342011</p>
<p>The slow forgetting of emotional episodic memories: an emotional binding account. A P Yonelinas, M Ritchey, 10.1016/j.tics.2015.02.009Trends Cogn Sci. 192015</p>
<p>Learning deep features for discriminative localization. B Zhou, A Khosla, A Lapedriza, arXiv:1512.04150Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>            </div>
        </div>

    </div>
</body>
</html>
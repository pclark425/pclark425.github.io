<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7849 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7849</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7849</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-273404356</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.13191v3.pdf" target="_blank">MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback</a></p>
                <p><strong>Paper Abstract:</strong> Automatic question generation (QG) is essential for AI and NLP, particularly in intelligent tutoring, dialogue systems, and fact verification. Generating multiple-choice questions (MCQG) for professional exams, like the United States Medical Licensing Examination (USMLE), is particularly challenging, requiring domain expertise and complex multi-hop reasoning for high-quality questions. However, current large language models (LLMs) like GPT-4 struggle with professional MCQG due to outdated knowledge, hallucination issues, and prompt sensitivity, resulting in unsatisfactory quality and difficulty. To address these challenges, we propose MCQG-SRefine, an LLM self-refine-based ( C ritique and C orrection) framework for converting medical cases into high-quality USMLE-style questions. By integrating expert-driven prompt engineering with iterative self-critique and self-correction feedback, MCQG-SRefine significantly enhances human expert satisfaction regarding both the quality and difficulty of the questions. Furthermore, we introduce an LLM-as-Judge -based automatic metric to replace the complex and costly expert evaluation process, ensuring reliable and expert-aligned assessments. 1</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7849.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7849.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-as-Judge vs Expert X</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-0125-preview) used as an automatic judge compared to adjudicated human expert (Expert X)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper directly compares GPT-4 used in two LLM-as-Judge modes (rating and pairwise comparison) against human expert judgments (two medical expert annotators with adjudication by a third expert, Expert X) on quality and difficulty of generated USMLE-style MCQs derived from PMC-Patients clinical notes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Reference-free evaluation of USMLE-style multiple-choice question quality and preference (rating and pairwise comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>USMLE-style MCQs generated from PMC-Patients clinical notes (373 questions with human-provided topics/test points; 385 with machine-provided topics/test points)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4 (gpt-4-0125-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4 chat-completions API (gpt-4-0125-preview) used with default judge prompts; evaluation performed in two modes: (1) rating across selected aspects and (2) pairwise comparison preference. Temperature/top-p defaults; context window 8192 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Two medical experts (medical students / clinicians with 2+ years hospital experience) performed blinded pairwise evaluation; disagreements resolved by a third licensed physician (Expert X) to produce final human preference labels.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa (primary), Percentage Agreement (also reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.538904899</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>initial slight reliability before filtering; low reliability on chain-of-thought/reasoning aspects; position bias (order-sensitivity); length bias; instability/poor sensitivity on nuanced medical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LLM-as-Judge (GPT-4) correlates moderately with Expert X after careful aspect selection/ filtering; rating-mode (detailed aspect ratings) outperformed pairwise comparison mode in agreement with Expert X; GPT-4 tended to be more consistent on surface/format aspects (context concision, distractor format) and less reliable on deep reasoning and nuanced justifications; direct use of the full critique rubric produced only slight agreement, requiring selection of a smaller subset of aspects to improve correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Scalable, fast, low-cost alternative to time-consuming expert annotation; provides multi-aspect, reference-free ratings and pairwise preferences; enables automated filtering/selection of best iteration rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Two LLM-as-Judge modes tested: (A) Rating: GPT-4 produced multi-aspect scores across 30 initial aspects, later filtered to 10 key aspects; (B) Pairwise comparison: GPT-4 asked to prefer between two system outputs; to mitigate position bias they collected preferences with both orderings and discarded inconsistent cases. Human evaluation used blinded, randomized order. Correlations to Expert X measured by Percentage Agreement and Cohen's kappa. Aspect-filtering algorithms (Greedy and exhaustive All-Combo across top n aspects) were applied to maximize correlation with Expert X.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7849.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7849.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aspect-filtering (All-Combo) improvement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aspect-filtering of LLM-as-Judge ratings using Greedy and All-Combo selection (improves correlation with human expert)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study found that selecting a subset of evaluation aspects (from an original 30 to a screened 10) and using an All-Combo selection procedure substantially raised agreement between LLM-as-Judge ratings and human expert judgments, moving reliability from slight to moderate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Improving correlation of LLM-as-Judge rating scores with expert human preference for USMLE-style MCQs</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Same USMLE-style MCQs from PMC-Patients (human and machine topic/test-point sets)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4 (gpt-4-0125-preview) used to produce per-aspect ratings</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>LLM provided per-aspect numerical ratings for up to 30 aspects (context, question, correct answer, distractors, reasoning). The paper tested Greedy and exhaustive All-Combo selection across top-n aspects (1<=n<=11) to pick the subset maximizing agreement with Expert X. Final selected combination comprised 10 key aspects (context: concision/relevance/misdirection; question: concluding/clarity; correct answer: occurrence/depth; distractor: common mistakes; reasoning: logical flow/evidence-based).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Expert X (adjudicated human preference labels), derived from two medical expert annotators with third expert adjudication</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Percentage Agreement and Cohen's kappa</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.505409582</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Using the full set of critique aspects produced only slight reliability (low kappa ~0.226); many individual aspects had poor or negative kappas, especially reasoning-related aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Filtering to the most correlated aspects dramatically improved alignment with human judgment: Percentage Agreement rose from ~67.5% (no filtering) to 80.0% (All-Combo best subset), and Cohen's kappa rose from ~0.226 to ~0.539 (moderate reliability). Demonstrates that careful aspect selection is critical when using LLMs as evaluators in specialized domains.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>After aspect filtering, LLM ratings can reliably approximate expert preferences enabling automated, scalable evaluation; provides interpretable per-aspect scores useful for iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Computed correlation of each aspect's rating with Expert X; applied two heuristics: Greedy (add aspects in descending correlation) and All-Combo (exhaustive search across combinations of top-n aspects) to find subset maximizing correlation; stopped search after diminishing returns observed for top 9-11 aspects; selected final 10-aspect combination and reported improved PA and kappa.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7849.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7849.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Judge model comparison (position & agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison across multiple LLM judge models (GPT-4, GPT-4o, GPT-3.5-turbo, Claude-3 variants) for position bias and agreement with Expert X</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper compared several LLMs as pairwise-comparison judges and found differences in order-sensitivity (position bias) and agreement with Expert X: GPT-4 showed the least position bias but only moderate agreement on valid (order-consistent) cases; other models (e.g., Claude-3-opus) showed higher agreement on valid cases but larger position bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Pairwise preference (comparison) evaluation of two system-generated USMLE questions</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Subset of human evaluation data (orderings reversed to test position bias) drawn from PMC-Patients derived MCQs</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Multiple: GPT-4, GPT-4o, GPT-3.5-turbo, Claude-3-haiku, Claude-3-opus, Claude-3.5-sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Each model was prompted under the LLM-as-Judge (comparison) setting (same human evaluation guideline). To detect position bias, preferences were collected under two orderings (A then B, and B then A); data points with inconsistent preferences across orderings were filtered out and only valid data used for agreement statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Expert X (adjudicated expert preference), same human evaluation protocol as above</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Percentage Agreement and Cohen's kappa on valid (order-consistent) data</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.2121</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Position bias (order sensitivity) leading to discarded cases; several models exhibited >50% inconsistent orderings; inconsistency reduces usable comparison data and inflates apparent agreement if not controlled for.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4 had the least position bias (35% inconsistent cases) and achieved ~61.53% agreement and Cohen's kappa ~0.2121 on valid data; GPT-3.5 and some Claude variants had higher inconsistency but some (Claude-3-opus) achieved higher agreement on valid data (PA ~68.42%, kappa ~0.3152) despite larger position bias. Overall, model rankings depend on whether one prioritizes low position bias or higher agreement on filtered valid cases.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Comparing multiple LLMs as judges identifies trade-offs (bias vs agreement) and allows selecting a judge model appropriate to the evaluation constraints (e.g., GPT-4 for lower position bias).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Pairwise comparison prompts identical to human blind evaluation; for each sample, two preference collections were made with reversed order to detect position bias; inconsistent samples (different model preference depending on order) were discarded; agreement computed on remaining valid samples; reported per-model position-bias rates, Percentage Agreement and Cohen's kappa on valid data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Large language models are state-of-the-art evaluators of translation quality <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Criticbench: Evaluating large language models as critic <em>(Rating: 2)</em></li>
                <li>Storyer: Automatic story evaluation via ranking, rating and reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7849",
    "paper_id": "paper-273404356",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "GPT-4-as-Judge vs Expert X",
            "name_full": "GPT-4 (gpt-4-0125-preview) used as an automatic judge compared to adjudicated human expert (Expert X)",
            "brief_description": "This paper directly compares GPT-4 used in two LLM-as-Judge modes (rating and pairwise comparison) against human expert judgments (two medical expert annotators with adjudication by a third expert, Expert X) on quality and difficulty of generated USMLE-style MCQs derived from PMC-Patients clinical notes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback",
            "evaluation_task": "Reference-free evaluation of USMLE-style multiple-choice question quality and preference (rating and pairwise comparison)",
            "dataset_name": "USMLE-style MCQs generated from PMC-Patients clinical notes (373 questions with human-provided topics/test points; 385 with machine-provided topics/test points)",
            "judge_model_name": "GPT-4 (gpt-4-0125-preview)",
            "judge_model_details": "OpenAI GPT-4 chat-completions API (gpt-4-0125-preview) used with default judge prompts; evaluation performed in two modes: (1) rating across selected aspects and (2) pairwise comparison preference. Temperature/top-p defaults; context window 8192 tokens.",
            "human_evaluator_type": "Two medical experts (medical students / clinicians with 2+ years hospital experience) performed blinded pairwise evaluation; disagreements resolved by a third licensed physician (Expert X) to produce final human preference labels.",
            "agreement_metric": "Cohen's kappa (primary), Percentage Agreement (also reported)",
            "agreement_score": 0.538904899,
            "reported_loss_aspects": "initial slight reliability before filtering; low reliability on chain-of-thought/reasoning aspects; position bias (order-sensitivity); length bias; instability/poor sensitivity on nuanced medical reasoning",
            "qualitative_findings": "LLM-as-Judge (GPT-4) correlates moderately with Expert X after careful aspect selection/ filtering; rating-mode (detailed aspect ratings) outperformed pairwise comparison mode in agreement with Expert X; GPT-4 tended to be more consistent on surface/format aspects (context concision, distractor format) and less reliable on deep reasoning and nuanced justifications; direct use of the full critique rubric produced only slight agreement, requiring selection of a smaller subset of aspects to improve correlation.",
            "advantages_of_llm_judge": "Scalable, fast, low-cost alternative to time-consuming expert annotation; provides multi-aspect, reference-free ratings and pairwise preferences; enables automated filtering/selection of best iteration rounds.",
            "experimental_setting": "Two LLM-as-Judge modes tested: (A) Rating: GPT-4 produced multi-aspect scores across 30 initial aspects, later filtered to 10 key aspects; (B) Pairwise comparison: GPT-4 asked to prefer between two system outputs; to mitigate position bias they collected preferences with both orderings and discarded inconsistent cases. Human evaluation used blinded, randomized order. Correlations to Expert X measured by Percentage Agreement and Cohen's kappa. Aspect-filtering algorithms (Greedy and exhaustive All-Combo across top n aspects) were applied to maximize correlation with Expert X.",
            "uuid": "e7849.0",
            "source_info": {
                "paper_title": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Aspect-filtering (All-Combo) improvement",
            "name_full": "Aspect-filtering of LLM-as-Judge ratings using Greedy and All-Combo selection (improves correlation with human expert)",
            "brief_description": "The study found that selecting a subset of evaluation aspects (from an original 30 to a screened 10) and using an All-Combo selection procedure substantially raised agreement between LLM-as-Judge ratings and human expert judgments, moving reliability from slight to moderate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback",
            "evaluation_task": "Improving correlation of LLM-as-Judge rating scores with expert human preference for USMLE-style MCQs",
            "dataset_name": "Same USMLE-style MCQs from PMC-Patients (human and machine topic/test-point sets)",
            "judge_model_name": "GPT-4 (gpt-4-0125-preview) used to produce per-aspect ratings",
            "judge_model_details": "LLM provided per-aspect numerical ratings for up to 30 aspects (context, question, correct answer, distractors, reasoning). The paper tested Greedy and exhaustive All-Combo selection across top-n aspects (1&lt;=n&lt;=11) to pick the subset maximizing agreement with Expert X. Final selected combination comprised 10 key aspects (context: concision/relevance/misdirection; question: concluding/clarity; correct answer: occurrence/depth; distractor: common mistakes; reasoning: logical flow/evidence-based).",
            "human_evaluator_type": "Expert X (adjudicated human preference labels), derived from two medical expert annotators with third expert adjudication",
            "agreement_metric": "Percentage Agreement and Cohen's kappa",
            "agreement_score": 0.505409582,
            "reported_loss_aspects": "Using the full set of critique aspects produced only slight reliability (low kappa ~0.226); many individual aspects had poor or negative kappas, especially reasoning-related aspects.",
            "qualitative_findings": "Filtering to the most correlated aspects dramatically improved alignment with human judgment: Percentage Agreement rose from ~67.5% (no filtering) to 80.0% (All-Combo best subset), and Cohen's kappa rose from ~0.226 to ~0.539 (moderate reliability). Demonstrates that careful aspect selection is critical when using LLMs as evaluators in specialized domains.",
            "advantages_of_llm_judge": "After aspect filtering, LLM ratings can reliably approximate expert preferences enabling automated, scalable evaluation; provides interpretable per-aspect scores useful for iterative refinement.",
            "experimental_setting": "Computed correlation of each aspect's rating with Expert X; applied two heuristics: Greedy (add aspects in descending correlation) and All-Combo (exhaustive search across combinations of top-n aspects) to find subset maximizing correlation; stopped search after diminishing returns observed for top 9-11 aspects; selected final 10-aspect combination and reported improved PA and kappa.",
            "uuid": "e7849.1",
            "source_info": {
                "paper_title": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Judge model comparison (position & agreement)",
            "name_full": "Comparison across multiple LLM judge models (GPT-4, GPT-4o, GPT-3.5-turbo, Claude-3 variants) for position bias and agreement with Expert X",
            "brief_description": "The paper compared several LLMs as pairwise-comparison judges and found differences in order-sensitivity (position bias) and agreement with Expert X: GPT-4 showed the least position bias but only moderate agreement on valid (order-consistent) cases; other models (e.g., Claude-3-opus) showed higher agreement on valid cases but larger position bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback",
            "evaluation_task": "Pairwise preference (comparison) evaluation of two system-generated USMLE questions",
            "dataset_name": "Subset of human evaluation data (orderings reversed to test position bias) drawn from PMC-Patients derived MCQs",
            "judge_model_name": "Multiple: GPT-4, GPT-4o, GPT-3.5-turbo, Claude-3-haiku, Claude-3-opus, Claude-3.5-sonnet",
            "judge_model_details": "Each model was prompted under the LLM-as-Judge (comparison) setting (same human evaluation guideline). To detect position bias, preferences were collected under two orderings (A then B, and B then A); data points with inconsistent preferences across orderings were filtered out and only valid data used for agreement statistics.",
            "human_evaluator_type": "Expert X (adjudicated expert preference), same human evaluation protocol as above",
            "agreement_metric": "Percentage Agreement and Cohen's kappa on valid (order-consistent) data",
            "agreement_score": 0.2121,
            "reported_loss_aspects": "Position bias (order sensitivity) leading to discarded cases; several models exhibited &gt;50% inconsistent orderings; inconsistency reduces usable comparison data and inflates apparent agreement if not controlled for.",
            "qualitative_findings": "GPT-4 had the least position bias (35% inconsistent cases) and achieved ~61.53% agreement and Cohen's kappa ~0.2121 on valid data; GPT-3.5 and some Claude variants had higher inconsistency but some (Claude-3-opus) achieved higher agreement on valid data (PA ~68.42%, kappa ~0.3152) despite larger position bias. Overall, model rankings depend on whether one prioritizes low position bias or higher agreement on filtered valid cases.",
            "advantages_of_llm_judge": "Comparing multiple LLMs as judges identifies trade-offs (bias vs agreement) and allows selecting a judge model appropriate to the evaluation constraints (e.g., GPT-4 for lower position bias).",
            "experimental_setting": "Pairwise comparison prompts identical to human blind evaluation; for each sample, two preference collections were made with reversed order to detect position bias; inconsistent samples (different model preference depending on order) were discarded; agreement computed on remaining valid samples; reported per-model position-bias rates, Percentage Agreement and Cohen's kappa on valid data.",
            "uuid": "e7849.2",
            "source_info": {
                "paper_title": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality",
            "rating": 2,
            "sanitized_title": "large_language_models_are_stateoftheart_evaluators_of_translation_quality"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Criticbench: Evaluating large language models as critic",
            "rating": 2,
            "sanitized_title": "criticbench_evaluating_large_language_models_as_critic"
        },
        {
            "paper_title": "Storyer: Automatic story evaluation via ranking, rating and reasoning",
            "rating": 1,
            "sanitized_title": "storyer_automatic_story_evaluation_via_ranking_rating_and_reasoning"
        }
    ],
    "cost": 0.01727575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback
18 Oct 2024</p>
<p>Zonghai Yao zonghaiyao@umass.edu 
Aditya Parashar aparashar@umass.edu 
Huixue Zhou 
University of Massachusetts
Lowell 3</p>
<p>UMass Chan Medical School</p>
<p>Won Seok Jang wonseok_jang@uml.edu 
Feiyun Ouyang feiyun_ouyang@uml.edu 
Zhichao Yang zhichaoyang@umass.edu 
Hong Yu hong_yu@uml.edu </p>
<p>University of Massachusetts
Amherst 1</p>
<p>University of Minnesota</p>
<p>MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback
18 Oct 2024F19A1D68F4F3D4806B15ECE5A1100E6DarXiv:2410.13191v2[cs.CL]GPT4 generated question MCQG-SRefine generated question
Automatic question generation (QG) is essential for AI and NLP, particularly in intelligent tutoring, dialogue systems, and fact verification.Generating multiple-choice questions (MCQG) for professional exams, like the United States Medical Licensing Examination (USMLE), is particularly challenging, requiring domain expertise and complex multi-hop reasoning for high-quality questions.However, current large language models (LLMs) like GPT-4 struggle with professional MCQG due to outdated knowledge, hallucination issues, and prompt sensitivity, resulting in unsatisfactory quality and difficulty.To address these challenges, we propose MCQG-SRefine, an LLM self-refinebased (Critique and Correction) framework for converting medical cases into high-quality USMLE-style questions.By integrating expertdriven prompt engineering with iterative selfcritique and self-correction feedback, MCQG-SRefine significantly enhances human expert satisfaction regarding both the quality and difficulty of the questions.Furthermore, we introduce an LLM-as-Judge-based automatic metric to replace the complex and costly expert evaluation process, ensuring reliable and expertaligned assessments. 1 * indicates equal contribution</p>
<p>Introduction</p>
<p>In Artificial Intelligence (AI) and Natural Language Processing (NLP), automatic question generation (QG) from knowledge bases, texts, and images (Guo et al., 2024) plays a crucial role in enhancing question-answering (QA) models (Chen et al., 2023c;Guo et al., 2022), supporting intelligent tutoring systems (Zhao et al., 2022;Cai et al., 2023), improving dialogue systems, and aiding fact verification (Pan et al., 2021;Zhang and Gao, 2023).Multiple-choice question generation (MCQG), a specialized type of QG, is extensively used in exams to assess students' knowledge efficiently (Zhang et al., 2021).However, creating MCQs is labor-intensive, requiring the design of effective stems, prediction of common errors as distractors, and provision of corrective feedback (Ch and Saha, 2018).In professional fields, MCQs often require field experts because they need to reflect real-world scenarios and involve complex multi-hop reasoning.These are unique challenges not typically encountered in general QG tasks.</p>
<p>The United States Medical Licensing Examination (USMLE) exemplifies the need for highquality MCQG (Scoles, 2008).Preparing for the USMLE costs medical students over $5000 on average (Philip A Bucur, 2019).For exam boards and instructors, creating MCQs is both time-consuming and expensive (Gierl et al., 2012).Any application that can automate this process is highly valuable to medical educators (Homolak, 2023;Gilardi et al., 2023).Due to the high difficulty with the need for domain knowledge and complex reasoning, USMLE questions are becoming important large language models (LLMs) benchmarks (Jin et al., 2021).Top LLMs like GPT-4 have shown over 90% accuracy on sample USMLE questions (Achiam et al., 2023).Recent research explores leveraging GPT-4's potential in USMLE-MCQG to improve question generation efficiency for educators and assist students in exam preparation (Klang et al., 2023;Agarwal et al., 2023;Biswas, 2023).</p>
<p>However, relying solely on LLMs like GPT-4 to generate USMLE questions presents several challenges.Firstly, their performance is constrained by their training data, leading to two major issues: outdated knowledge (Mousavi et al., 2024) and hallucination (Zhang et al., 2023) (Limit1).Outdated knowledge means that LLMs can only repeat or integrate old USMLE questions or medical documents in their training data during the generation.Consequently, they struggle to create new ques-Figure 1: USMLE MCQ generated by GPT-4 and MCQG-SRefine.The GPT-4 question contains several errors and inconsistencies, such as extraneous information, a distractor option format mismatch, mentioning symptoms instead of conditions, and a context that contains the answer.The MCQG-SRefine addresses these issues, resulting in a higher quality question that aligns the context, question, and answer options more coherently.Irrelevant details are removed, the question focuses on the key clinical condition of sepsis, distractor options are presented in a consistent format, and the context no longer gives away the answer.</p>
<p>tions based on the latest medical cases or guidelines like medical experts.Hallucination refers to LLMs potentially providing misinformation in questions, which could harm students' learning outcomes.Secondly, generating questions on specific concepts requires precise prompts (Limit2), which students might not know how to formulate (Sahoo et al., 2024).For example, given one USMLE question about cardiovascular physiology, if GPT-4 is asked to "Generate a more difficult USMLE question.", it only provides a longer stem.It will focus on the specific topics and test points only when specifically asked to generate a question "testing knowledge of the hemodynamic profile of aortic stenosis."Lastly, the quality and difficulty of the questions often do not meet expectations (Artsi et al., 2024;Benítez et al., 2024) (Limit3).As shown in Figure 1, the output from GPT-4 contains several errors and inconsistencies, such as extraneous information, mismatched distractor option formats, direct mention of the condition instead of symptoms, and context that includes the answer.The generated questions often lack the depth required for students' critical thinking.</p>
<p>To address these challenges, we propose a new task: converting medical cases that appear in case reports or clinical notes into USMLE-style MCQs.</p>
<p>Our approach involves several key design elements to alleviate the above limitations: For Limit1, previous research has attempted to prompt LLMs to generate USMLE-MCQs by following expert-crafted instructions in zero-shot setting or further adding existing questions as fewshot examples (Artsi et al., 2024).To our knowledge, we are the first to qualitatively and quantitatively study how to convert medical cases into USMLE-MCQs.These medical cases provide valuable real-world information on disease progression, accurate assessments, diagnoses, and potential treatment plans.Using the latest medical cases as input, LLMs can generate up-to-date questions, thereby minimizing the limitations of outdated knowledge.Additionally, grounding questions' key elements in the original medical cases can help LLMs reduce hallucinations, enhancing the reliability of the generated content.</p>
<p>For Limit2, we follow the National Board of Medical Examiners (NBME) guidelines2 to establish a checklist of 41 target topics covering all potential exam areas.We then deployed a ColBERT retriever (Santhanam et al., 2021) using USMLE Content Outline3 as a collection for test points retrieval.Each input medical case in our experiments is evaluated by experts with exam experience to determine if it contains sufficient information to generate questions related to the specified target topics and test points.We also compared the topics and test points identified by experts with those generated by LLMs, assessing their impact on the quality and difficulty of the resulting questions.</p>
<p>For Limit3, as illustrated in Figure 1, we used the triplets (medical case, topic, and test point) as our question generation pipeline input.We work with experts for prompt engineering based on USMLE guidelines.We then created our MCQG-SRefine (e.g., self-refine with iterative Critique and Correction feedback) following three steps.S1 -Initial MCQ Generation: Generate an initial USMLE-MCQ based on the triplets.S2 -Critique Feedback: Prompt the LLM itself to provide feedback on the S1 USMLE-MCQ.S3 -Correction Feedback: Correct the USMLE-MCQ based on the S2-generated critique feedback.Through iterative critique and correction, MCQG-SRefine significantly enhances the quality and difficulty of generated USMLE-style MCQs.Human evaluations confirm its effectiveness, showing a strong preference for MCQG-SRefine generated questions, with a preference ratio of 72.5% in win, 10% tie, and 17.5% loss when compared to GPT-4 generated questions.In terms of difficulty, MCQG-SRefine generates more challenging questions.Specifically, when provided with expert-identified topics and test points, there is an 80% reduction in easy questions, a 2.25-fold increase in medium questions, and a 4-fold increase in hard questions.</p>
<p>Finally, designing a reliable reference-free metric to automatically evaluate the quality of systemgenerated USMLE-MCQs is challenging.Recent research indicates that LLM-as-Judge correlates more closely with human evaluations than traditional metrics (Chen et al., 2023a;Chiang and Lee, 2023;Kocmi and Federmann, 2023;Zheng et al., 2024;Zhang et al., 2024a;Kim et al., 2023Kim et al., , 2024)), though these methods remain underexplored in medical NLP tasks.Our goal is to replace the costly expert evaluation process in USMLE-MCQG with LLM-as-Judge.We used 30 criteria designed by experts in their human evaluation, covering different aspects of USMLE questions, to guide LLM-as-Judge in providing rating or comparison feedback on different systems' questions.By further exploring the filtering methods for the 30 criteria, we finally screened out a combination of 10 key criteria.This improved the correlation between LLM-as-Judge and expert evaluations, as measured by Cohen's kappa, from 0.226 (slight reliability) to 0.539 (moderate reliability).Using the results from this automated evaluation system, it was shown that the preference rate for questions generated by MCQG-SRefine over those generated by GPT-4 was 79.97% in favor and 20.03% against.Moreover, MCQG-SRefine demonstrated overall improvements across 10 criteria within the 5 components, not just in a specific area.</p>
<p>Method</p>
<p>Problem statement: Given a medical case n detailing a patient's history, diagnosis, treatment, and outcome, we aim to generate a USMLE question u.Here, u = <c, q, a, d> consists of a context (c), which is a modified excerpt from n tailored to align with the target style and obscure evidence information that can easily lead directly to the correct answer; a question (q) based on the generated context, which may be one or several sentences; the correct answer (a) to this question; and several distractor options (d).</p>
<p>Topic and test point identification</p>
<p>As discussed in Limit2, generating questions using LLMs without specific guidance, such as defined topics t and test points k, often results in questions that lack relevance, quality, and appropriate difficulty.These questions may fall outside the scope of the USMLE exam, being either too simple or overly complex.Therefore, the quality and difficulty of the generated USMLE questions are significantly influenced by the selection of topics t and test points k.Topics t refer to a list of target topics selected from 41 potential topics outlined in the NBME official guidelines, categorized into 10 sections 4 .Both the LLM and human experts are provided with this list to generate a maximum of five topics (t) that are highly relevant to the medical case.Test points k refer to the core concepts closely related to the correct answer.We employ the Col-BERT retriever 5 , denoted as π rtr , to retrieve suit-Figure 2: The framework for generating USMLE-style questions involves four main steps, as illustrated in the figure.First, the initialization generates the context, question, answer, and distractor options using retrieval and generation models.The generation model then answers the generated question along with a reasoning.Next, the feedback step evaluates the generated components on various rubrics and generates textual feedback and scores, stopping if feedback scores exceed a threshold.Finally, the refine step iterates by using the feedback to improve the components before cycling back to the answer step.</p>
<p>able test points from 18 sections 6 of the USMLE content outline.This test points retrieval process involves querying the model with the medical case (n) and the identified topics (t).The result is a list of highly specific USMLE concepts, such as 'keloid formation'.π llm then generates specific test point concepts using these filtered concepts as a foundation, ensuring they are directly relevant to the context (c) and topics (t).These test points can either originate from the filtered list of USMLE concepts or be derived from the content of the medical case itself.Additionally, we ask human experts to identify specific test points they believe are related to their selected topics, referring to the key medical concepts mentioned in the USMLE content outline.More details can be found in Appendix Table 12.In Section 4, we compare the topics and test points generated by humans and LLMs for their impact on the quality of the resulting questions. 6Details can be found in Appendix A.</p>
<p>Initialization</p>
<p>As illustrated in Figure 2, the MCQG-SRefine pipeline begins with the INIT step, which comprises 4 generation steps, each targeting a component of the goal u = <c, q, a, d>.To assist the model in referencing similar examples for better generation of each component of u, we deploy a ColBERT retriever model R to retrieve a small set of USMLE examples from the MedQA (Jin et al., 2021) question bank.As shown in Figure 2, given the input &lt; n, t, k &gt; from Topic and Test point identification ste, R first uses &lt; n, t, k &gt; as a query to retrieve few-shot examples, and then LLM follows the INIT-c prompts in Appendix Table 16 to generate the context c.Subsequently, after obtaining c, we continue to use &lt; n, t, k, c &gt; as a query to retrieve few-shot examples and follow the INIT-q prompts in Appendix Table 16 to generate the question q.The exact process is applied for t and k.It is important to note that we trim the re-trieved examples for each component; for instance, in INIT-c prompts, we only retain the context component of each example, and similarly, for the other three components q, a, d, only the relevant parts are kept.As demonstrated in Appendix Table 15, this INIT step already results in a USMLE MCQ for a given input &lt; n, t, k &gt;.However, as discussed in Figure 1, despite incorporating several advanced prompting engineering methods in the INIT step-including prompts designed by medical experts according to USMLE guidelines, tailored topics&amp;test points for each input medical case, as well as step-by-step retrieval and generation-the LLM-generated USMLE MCQs in the INIT step still fall short of the required quality and difficulty.</p>
<p>Question Answering Feedback Collection</p>
<p>Inspired by recent work that augments the standard QG model with an additional QA module to further constrain the generated questions (Su et al., 2022;Xie et al., 2020;Sun et al., 2019), we add a Question Answering Feedback Collection module.This provides additional feedback from the questionanswering perspective to further challenge the quality and difficulty of the questions.Our motivation stems from the fact that LLMs like GPT-4 have proven to perform exceptionally well on USMLE QA tasks, achieving human-expert levels in both accuracy and reasoning processes.By analyzing the rationale and correctness of the final answers produced by the language models during the QA process, we can gather valuable insights into the quality and difficulty of the questions.Specifically, given the context c, question q, and options composed of a∪d (with their order shuffled), we collect the LLM's generated attempt a a along with the reasoning r that supports a a in this step.An example output is shown in Appendix Table 15.</p>
<p>Critique</p>
<p>After generating all components in INIT and QA step, the LLM is asked to critique each component.The set to be critiqued is u critique = ⟨c, q, a, d, r⟩.The LLM receives a scoring guide G, which includes all aspects that need to be evaluated for each component.The prompt includes this guide G as well as several manually written example critiques of scored components E fs critique = ⟨e fs c , e fs q , e fs a , e fs r ⟩ and ⟨G, n, t, k, c, q, a, d, a a ⟩.The final output of this step is LLM critique feedback on all components, f = ⟨f c , f q , f a , f d , f r ⟩, which includes short text critiques and scores for each aspect.</p>
<p>Correction</p>
<p>The Correction step aims to correct each of the generated components of u in the INIT step.</p>
<p>LLM is prompted with <E f s correction , n, t, k, f, c, q, a, d, a a , r> and asked to generate u correction , which can perform better on all the component's critique aspects.Here E f s correction is a set of manually written few shot examples which are incrementally improving using the previous output's feedback.u correction is again given to the Critique step to check if the feedback scores are greater than a fixed threshold, which, if true, stops the iterative Critique and Correction and, if not, continues7 .</p>
<p>Experimental Design and Setup</p>
<p>Experimental Setup For all our experiments with MCQG-SRefine, we use the chat completions API from OpenAI and the gpt-4-0125-preview, which has a context window of 8192 tokens, and the values of the hyperparameters temperature and top-p are set to 1. Similarly, for all other models used for the LLM-as-Judge comparison feedback generation, we used their default hyperparameters.</p>
<p>Dataset For the medical cases utilized in the generation of USMLE questions, we used unidentified patient summaries from the PMC-Patients dataset (Zhao et al., 2023).The average length of these patient summaries was ~419 words.The frequency of topics used is listed in Appendix Table 13 14.Experimental Design Our experimental design is motivated to answer the following research questions: Evaluate whether MCQG-SRefine improves both the quality and difficulty of the generated questions.Specifically, we employed the MCQG-SRefine pipeline to generate USMLE questions and compared them with baseline questions generated by GPT-4 under identical inputs and settings.Our inputs consisted of medical cases n from the PMC-Patients dataset, with topics t and test points k that were either human-annotated or generated by the LLM.We generated 373 questions from the humanannotated ⟨n, t, k⟩ set and 385 questions using the LLM-generated set.</p>
<p>To assess the quality of the questions (RQ1), we first engaged two medical experts8 to express their preferences between the two sets (GPT-4 and MCQG-SRefine) of system-generated questions based on an annotation guideline (Appendix Table 17).The evaluators were blinded to the source of the questions, and the order of the questions was randomized for each data point.We calculated the Percentage Agreement (87.5%) and Cohen's kappa (0.66722) for the two evaluators' preferences, indicating substantial reliability of our human evaluation settings.Subsequently, a third human expert9 facilitated discussions with the initial evaluators to make the final decision for each data point, representing the final human expert preference (referred to as Expert X).</p>
<p>For evaluating the difficulty level of the questions (RQ2), the human evaluators were also asked to classify the difficulty of both questions into one of three categories: Easy, Medium, and Difficult.Specifically, we randomly selected 50 real-world USMLE-style questions from the AMBOSS10 dataset (10 for each difficulty level) as examples for the experts to reference.AMBOSS categorizes question difficulty from 1 to 5, where 1 is the easiest and 5 is the most difficult.We grouped levels 1 and 2 as Easy examples, levels 3 and 4 as Medium examples, and level 5 as Hard examples.</p>
<p>LLM-as-Judge for evaluation metrics In addition to human evaluation, recent work has demonstrated that LLM-as-Judge (particularly GPT-4based) has a high correlation with human assessment in reference-free settings (Chen et al., 2023a;Chiang and Lee, 2023;Kocmi and Federmann, 2023;Zheng et al., 2024;Zhang et al., 2024a;Kim et al., 2023Kim et al., , 2024;;Lan et al., 2024).Recent work in the medical domain has also shown the potential of LLM-as-Judge to replace traditional evaluation metrics (Yao et al., 2024;Schmidgall et al., 2024;Mehandru et al., 2024).In this work, finding reliable automatic evaluation metrics in a referencefree setting is crucial, as it can reduce the burden of expert evaluation and help improve LLMs in future work (e.g., as a reward model).To achieve this, we explored two common LLM-as-Judge modes: rating and comparison.Regarding the evaluation criteria for each part of the MCQ, we found that directly using the criteria from the critique section of MCQG-SRefine did not correlate well with the human evaluation results of Expert X.Therefore, we conducted a detailed correlation analysis between each criterion's score and human evaluation, ranking them accordingly.Based on this analysis, we applied different filtering methods to identify the most relevant combination of criteria 11 .Finally, we selected the following ten aspects: Context (concision, relevance, misdirection), Question (concluding, clarity), Correct Answer (occurrence, depth of understanding), Distractor (common mistakes), and Reasoning (logical flow, evidence-based reasoning).Specifically, GPT4-as-judge provides two evaluation indicators: 1. Detailed ratings and reasons for the above five sections and ten aspects; 2. Preference between MCQA-SRefine and GPT-4 generated questions.</p>
<p>Results</p>
<p>Main results Figure 3 demonstrates the overwhelming advantage of MCQG-SRefine over GPT-4 with a 70-80% win rate in human preference about question quality (RQ1).In Figure 4, we also observe a decrease of about 80% in easy questions, a 2.25 times increase in medium questions, and a 4 times increase in hard questions with the input of these medical cases with expert-provided topics and key points.For machine-provided topics and key point cases, the proportion of easy questions decreased by 33.3%, a 2 times increase in medium questions, but there was no increase in the proportion of hard questions (RQ2).This demonstrates the effectiveness of MCQG-SRefine in increasing the difficulty of questions.This also indicates that the quality of topics and key points provided by experts is higher, so LLM can generate more diffi- and key points k were generated.We only put the final Expert X preferences here, but we provide more results in the Appendix Table 10.The percentage agreement between experts is 87.5% (Human<t, k>: 90%, Machine<t, k>: 85%).The Cohen's kappa between experts is 0.66722 (Human<t, k>: 0.75, Machine<t, k>: 0.57), indicating substantial reliability.cult questions by thinking more deeply during the critique and correction steps.Further improving the quality of the machine-provided topics and key points can be a future direction for improvement.</p>
<p>Figure 5 and Table 1 present the results of the LLM-as-judge evaluation.For human-provided topics and key point cases, MCQG-SRefine achieved a win rate of 79.8% compared to 20.2% for GPT-4.Similarly, for machine-provided topics and key point cases, MCQG-SRefine achieved a win rate of 80.1%, outperforming GPT-4's 19.9%.Notably, MCQG-SRefine consistently outperformed GPT-4 across all five evaluated components with LLM-as-judge (Rating) results rather than demonstrating an advantage in only a single aspect 12 .This indicates a balanced and comprehensive improvement of MCQG-SRefine. 12The aspect-level score can be found in Appendix Table 11.</p>
<p>Qualitative analysis One of the main issues with the questions generated by GPT-4 is that they often directly include the correct answer or too obvious relevant keywords within the context component (Appendix Table 19 Case Studies 1 and 3).So the questions directly generated by GPT-4 often make the answers obvious, but MCQG-SRefine can modify this information into hints for the correct answer through its critical and corrective steps, which experts consider a better way to construct questions for candidates (Case Study 4).</p>
<p>Another finding is that the questions generated by MCQG-SRefine are more concise compared to those by .Experts pointed out that this conciseness makes them more similar to real USMLE questions.Our experiments show that GPT-4 adopted a very conservative strategy when generating context due to our emphasis on hallucination issues in the prompts.This strategy involves copying and pasting much information from raw medical case inputs to avoid generating potentially incorrect new facts.Although this does reduce the occurrence of hallucinations -our human assessment shows that 7.5% of GPT-4 problems contain factual errors -it inevitably sacrifices the typical simplicity and highly refined information presentation of USMLE problems, as well as the logical coherence of multi-hop reasoning between information in context, question, correct answer, and distractors.This is a significant stylistic difference between USMLE questions and the original medical cases.We also found that prompt engineering was ineffective in resolving this issue.We interpret this as a shortcut behavior learned by GPT-4 during aligning with human preferences stage training (e.g., RLHF (Ouyang et al., 2022)) to reduce output diversity to mitigate hallucinations (Kirk et al., 2023).In contrast, MCQG-SRefine maintains a high level of factual accuracy (with 5% factual errors) and further improves the quality of USMLE questions by iteratively criticizing and correcting each component of the question, as well as shifting the perspective from QG to QA to make it closer to actual exam questions.This is another major reason why MCQG-SRefine outperforms GPT-4 in human evaluation.However, when MCQG-SRefine reduced the amount of useless information provided in the questions to increase quality and difficulty, omitting too much information sometimes made inferring the correct answer more challenging (Case Study 2), although this was a rare occurrence in our human evaluation (7.5%).Table 1: The win rate of MCQG-SRefine and GPT-4 questions using LLM-as-Judge (Comparision).</p>
<p>Discussion</p>
<p>Round-wise analysis Since MCQG-SRefine operates as an iterative system, an interesting question is whether GPT-4 can consistently provide meaningful critiques and corrections for itself in such a specialized and complex setting.To explore this, we conducted round-wise analyses, with key findings presented in Figures 6, while additional analyses are discussed in the appendix.Figure 6 shows the best-scoring rounds for human-and machinegenerated topic + key points.For human-generated topics and key points, 30% of the best scores came from the first round of output, while the remaining 70% were evenly distributed across rounds 2, 3, and 4.This suggests that the LLM cannot consistently ensure that its critique and corrections will continuously improve the quality of the generated questions.However, based on the main results, selecting the best round after multiple iterations generally leads to a much higher quality of final questions compared to the initial output.We observed similar results for machine-generated topics and key points, but the proportion of best scores from the first round was lower (24.9%).This is consistent with the main results, where topics and key points provided by humans were clearer, making it more likely for the LLM to generate high-quality questions in the first round, while those provided  by machines required more improvement.</p>
<p>Improving LLM-as-judge reliability We found that directly using the critique criteria from Section 2.4 for LLM-as-Judge only resulted in slight reliability when correlated with Expert X.We explored two heuristic algorithms to improve the effectiveness of LLM-as-Judge through aspect filtering 13 .Specifically, we calculated the correlation of each aspect's score from the collected rating feedback with Expert X, then sorted these aspects in descending order based on percentage agreement or Cohen's kappa.In the Greedy14 , we added aspects sequentially to the final rating score calculation based on their correlation, from highest to lowest, and recalculated the correlation between LLM-as-Judge (rating) and Expert X.Similarly, in the All-Combo, we calculated the final rating score for all possible combinations of the top n aspects (1 &lt;= n &lt;= 11)15 , selecting the combination with the highest correlation to Expert X as the output of the All-Combo algorithm.As shown in Appendix Table 24 and Table 25, the All-Combo method identified the optimal aspect combination.</p>
<p>In Table 2, we observed that the percentage agreement and Cohen's kappa of LLM-as-Judge significantly improved16 .</p>
<p>6 Related Work</p>
<p>LLMs for Generating Medical MCQs</p>
<p>Cheung et al. ( 2023) conducted the first comprehensive study comparing LLMs with human experts for generating medical exam MCQs.Using the ChatGPT-3.5interface, they generated MCQs from two standard medical textbooks (Kasper et al., 2015;Williams and O'Connell, 2008).Each question included four answer options, without any postgeneration modifications.ChatGPT took only 21 minutes to generate 50 MCQs, roughly 10% of the time required by human experts (211 minutes).Despite the efficiency, human-authored questions were found to be superior in quality, outperforming AI in 60% of the acceptable MCQs.Klang et al. (2023) performed a blind evaluation of MCQs generated by ChatGPT-4.Initially, ChatGPT-4 generated questions with four answer options using examples from previous exams (fewshot learning).These questions were brief and lacked clinical context, necessitating an additional prompt to include clinical background.After refinement, most of the MCQs were considered effective.</p>
<p>Agarwal et al. ( 2023) compared ChatGPT-3.5,Bard, and Bing in generating MCQs for an 11module physiology course by the National Medical Commission (NMC) of India.The results indicated that ChatGPT generated more effective MCQs, although they were less challenging compared to those generated by Bard and Bing.Ayub et al. (2023) focused on dermatology board exams by uploading Continuing Medical Education (CME) articles from the Journal of the American Academy of Dermatology (JAAD) into ChatGPT-3.5.The generated MCQs were evaluated based on accuracy, complexity, and clarity, with only 40% meeting the criteria for applicability, while the remainder were found to be unclear, inaccurate, or overly simplistic.</p>
<p>Sevgi et al. ( 2023) tasked ChatGPT-3.5 with generating three neurosurgery board exam questions, including answers and explanations.Similarly, Han et al. (2024) used ChatGPT-3.5 to generate three MCQs incorporating clinical context and lab values, experimenting with rephrasing to adjust answers and increase difficulty.Totlis et al. (2023) employed ChatGPT-4 to generate MCQs on anatomy, focusing on increasing difficulty levels and creating appropriate answer pairings.Biswas (2023) used ChatGPT to prepare USMLE Step 1 MCQs, though no independent evaluations were conducted for these studies.</p>
<p>To our knowledge, this study is the first to comprehensively explore both the qualitative and quantitative aspects of converting real-world medical cases into USMLE-style MCQs.We leverage authentic medical case information to generate highquality, up-to-date questions that align with current medical standards and best practices.</p>
<p>Self-Refine using LLM Feedback</p>
<p>Learning from feedback helps align LLMs with desired outcomes, enhancing their ability to follow instructions through different forms of feedback, such as human preference feedback (Ouyang et al., 2022), AI-generated preference feedback (Lee et al., 2023;Dubois et al., 2024), or fine-grained feedback (Wu et al., 2024;Lightman et al., 2023).</p>
<p>Unlike preference and fine-grained feedback, which provide scalar values as training signals, natural language or correction feedback offers richer information (Scheurer et al., 2022;Ma et al., 2023;Yao et al., 2023;Mishra et al., 2024), making it particularly effective for self-correcting language models (Welleck et al., 2022;Pan et al., 2023).</p>
<p>Recent research has demonstrated that LLMs (Achiam et al., 2023;Heng et al., 2024) can self-correct their responses to meet various user requirements, such as reducing harmful content, incorporating specific keywords, diversity requirement generation, or debugging code (Madaan et al., 2024;Chen et al., 2023b).This self-correction process generally involves generating a critique that identifies shortcomings in the initial response, followed by revising it based on the self-critique-an iterative process that has shown promise in enhancing LLM output quality (Pan et al., 2023).</p>
<p>Inspired by the success of these iterative selfrefinement methods, we are the first to explore using this approach for generating USMLE-style MCQs.By leveraging self-feedback, we aim to create high-quality, clinically relevant questions that adhere to the rigorous standards of medical education.</p>
<p>LLM-as-Judge using LLM feedback</p>
<p>The critique capabilities of LLMs have been extensively used for the automatic evaluation of response quality, often employing models like GPT-4 (Achiam et al., 2023;Liu et al., 2023;Fu et al., 2023) or critique-adjusted LLMs (Ke et al., 2023;Li et al., 2023).Despite their success, these meth-ods have demonstrated instability in certain complex task scenarios (Wang et al., 2023b;Zhang et al., 2024b).</p>
<p>LLMs have shown a high correlation with human evaluations in tasks such as summarization and story generation, effectively scoring candidate texts or comparing them based on specific evaluation aspects (Chen et al., 2023a).For example, studies by Chiang and Lee (2023) and Kocmi and Federmann (2023) have shown that LLM evaluations produce results comparable to those of expert human evaluators in story generation and translation tasks.Similarly, research by Zheng et al. (2024) and Zhang et al. (2024a) indicates that powerful LLM reviewers, such as GPT-4, achieve over 80% consistency with human preferences in multi-turn dialogue scenarios, both in controlled and crowdsourced settings, reaching agreement levels similar to human evaluators.</p>
<p>Further evidence of GPT-4's effectiveness as an evaluator is demonstrated by its performance in the PROMETHEUS (Kim et al., 2023(Kim et al., , 2024) ) and CRITICBENCH (Lan et al., 2024) benchmarks.However, most LLM-as-judge research has primarily focused on general NLP fields, with limited exploration in specialized domains like clinical NLP.This gap is largely due to challenges such as the need for domain-specific knowledge, difficulties in designing evaluation prompts that meet domain standards, and the inherent challenges of generalizing evaluation tools to specialized fields (Yao et al., 2024;Singhal et al., 2023;Li et al., 2024).</p>
<p>To our knowledge, we are the first to explore the application of LLM-as-judge in the context of USMLE-MCQ generation and evaluation.Our work addresses these challenges to create reliable, domain-specific evaluations that meet the unique requirements of medical education.</p>
<p>Conclusion</p>
<p>In conclusion, MCQG-SRefine improves LLM ability to generate high-quality USMLE-style MCQs by integrating expert-driven prompts with iterative self-refinement.The framework significantly enhances question quality and difficulty while aligning closely with expert evaluations.Additionally, our LLM-as-Judge metric offers a scalable alternative to costly expert assessments.</p>
<p>A Topics and Test points</p>
<p>Topics t refer to a list of target topics selected from 41 potential topics outlined in the NBME official guidelines, categorized into the following 10 sections:   As shown in Table 5, the round-wise metrics for machine data demonstrate a stable overall performance, with the total score showing minimal fluctuation across rounds, peaking slightly in Round 2 (0.907) and ending at 0.903 in Round 4. This suggests that while the feedback process maintains performance, it does not lead to significant improvements.The context score exhibits a notable increase from 0.9688 in Round 1 to 0.974 in Round 4, coupled with a decrease in standard deviation, indicating enhanced and more consistent context understanding.The C. Answer score also shows a gradual upward trend, improving from 0.795 in Round 1 to 0.810 in Round 4, reflecting slow but steady progress in answer generation.The distractor option score remains relatively high and stable across rounds, while the reasoning score experiences a decline from 0.967 in Round 1 to 0.948 in Round 3, with a slight recovery to 0.951 in Round 4. Variability in performance is evident, with some metrics, such as the context score, showing decreased standard deviation, indicating more con-sistency, while others, like the reasoning score, exhibit increased variability, highlighting differential effects on the consistency of various components.</p>
<p>Based on the analysis of the round-wise metrics from both human and machine data, the selffeedback process demonstrates the most significant improvements in context understanding and answer generation.These enhancements are evidenced by the consistent upward trends in context and correct answer scores across rounds.However, the data also suggests a potential trade-off between reasoning ability and other metrics, as seen in the decline of reasoning scores, particularly after the initial rounds.The plateau or slight decline in overall performance after 2-3 rounds indicates that the benefits of the feedback process diminish with excessive iterations, implying that a limited number of rounds may be optimal for maximizing improvements without compromising other aspects of question generation.These findings highlight the importance of balancing the feedback process to achieve comprehensive improvements across all key metrics.</p>
<p>In addition, the analysis of the round-wise metrics in Table 6 and 7 reveals several key trends in the performance of the question generation pipeline.As the rounds progress, the context length consistently decreases, indicating that the pipeline effectively refines the context by excluding extraneous information, leading to more precise and focused questions.The accuracy of the QA component improves over the rounds, suggesting that the iterative process enhances the overall quality of the questions, making them more answerable by the LLM.However, this improvement reaches a point of diminishing returns in the later rounds, implying that a limited number of iterations may be optimal.On the other hand, the equality between the correct answer and the keypoint deteriorates over the rounds, indicating that the pipeline makes the correct answer more subtly related to the keypoint rather than directly copying it.This shift suggests that while the pipeline reduces redundancy, it may also introduce complexity that could impact the clarity and directness of the correct answer-keypoint relationship.</p>
<p>B.2 Other Basic statistics</p>
<p>We also calculate Refinement rounds for humanannotated and model-generated topics &amp; key points in Figure 7; Question length in no. of words (human annotated topic+keypoint) in Figure 8; Ques-   tion length in no. of words (machine annotated topic+keypoint) in Figure 9;</p>
<p>B.3 Correlation analysis</p>
<p>Our analysis in Figure 10 11 and Table 8 reveals a strong inverse relationship between the length of clinical notes/questions and the number of rounds required to achieve the best score in both humangenerated and machine-generated data.The perfect negative Spearman and Kendall correlations (-1.0) indicate that as the length of inputs increases, fewer rounds are consistently needed to reach optimal performance.Human-generated content shows slightly stronger linear correlations compared to machine-generated data.These findings suggest that longer, more detailed inputs provide richer information, allowing the model to converge on optimal performance more quickly.Practically, this implies that shorter inputs may benefit from more iterative rounds, while longer inputs may require fewer rounds to achieve the best results.The consistency of this pattern across both clinical note length and question length underscores the importance of input complexity in determining the efficiency of the iterative improvement process.C Improving reliability of LLM-as-judge</p>
<p>C.1 Greedy Algorithm</p>
<p>The Greedy algorithm iteratively constructs an optimal set of aspects by sequentially adding the most correlated aspects.Let A = {a 1 , a 2 , ..., a n } be the set of all aspects, and h be the human preference data.The algorithm proceeds as follows:</p>
<p>Algorithm 1 Greedy Aspect Selection
S ′ ← S ∪ {a i } 6: r ← CalculateRating(S ′ ) 7: c ← Correlation(r, h) 8: if c &gt; c best then 9: S ← S ′ 10: c best ← c 11:
end if 12: end for 13: return S, c best where CalculateRating(S) computes the final rating score using the aspects in set S, and Correlation(r, h) calculates either the percentage agreement or Cohen's kappa between the rating r and human preferences h.</p>
<p>C.2 All-Combo Algorithm</p>
<p>The All-Combo algorithm exhaustively evaluates all possible combinations of the top n aspects to find the optimal subset.Let k be the number of aspects in a combination, where 1 ≤ k ≤ n.This algorithm returns the best aspect combinations and their corresponding correlation scores for both percentage agreement and Cohen's kappa, as these may differ depending on the evaluation metric used.</p>
<p>D More Experiments for LLM-as-Judge</p>
<p>Determine if LLM-as-Judge is an effective metric.RQ3: Rating or comparison; RQ4: Which LLM is the better judge?</p>
<p>D.1 Settings</p>
<p>To explore RQ3, we utilized ratings generated during the MCQG-SRefine critique step as LLM-as-Judge (rating) scores.</p>
<p>For LLM-as-Judge (comparison), we employed the same guidelines and settings used in the human evaluation from RQ1 in main results, prompting the model to choose a preferred output between the two system-generated questions (e.g., GPT-4 and MCQG-SRefine).To ensure the results were not biased by the inherent positional preferences of the LLM (e.g., position bias) (Wang et al., 2023a;Zheng et al., 2023;Zeng et al., 2023), we collected preferences from two sequence settings: one where the GPT-4 generated output was shown first followed by the MCQG-SRefine output, and the reverse order.We filtered out preferences that did not match across the two sequence settings, using the remaining data for further analysis.To ensure the results were not influenced by length bias (i.e., LLMs like GPT-4 prefer longer generations during their automatic evaluation) (Wang et al., 2023c;Zeng et al., 2023), we deliberately included those data points in the evaluation data which had the least context length ratio for the initial and the MCQG-SRefine questions.These examples constituted half the human evaluation data and the rest were chosen randomly from the dataset.Notably, the guidelines used in human evaluation, LLM-as-Judge (rating), and LLM-as-Judge (comparison) were consistent.Thus, by standardizing the evaluation data, we could fairly compare the judgments of Expert X and LLM-as-Judge (rating/comparison) under identical settings to assess the reliability of LLM-as-Judge.</p>
<p>To investigate RQ4, we prompted various LLMs 18 using the LLM-as-Judge (comparison) settings and compare the preference with Expert X.</p>
<p>D.2 Results</p>
<p>For RQ3, we found that the rating method performs slightly better than the comparison method.Percentage Agreement between GPT-4-Comparison and Expert X: Total agreement: 53.8% (Human: 58.33%, Machine: 50%).Percentage Agreement between GPT-4-Rating and Expert X: Total agreement: 61.538% (Human: 58.33%, Machine: 64.285%).For RQ4, as shown in Appendix 18 GPT-3.5-turbo,GPT-4, GPT-4o, Claude-3-haiku/opus, and Claude- Table 9 and 18, we observed that GPT-4 as the LLM-as-judge has the least position bias (35% of the data showed inconsistent results under two different orders, while other models showed more than 50% inconsistency).Moreover, GPT-4 has a relatively higher correlation with human evaluations in the valid data (i.e., data without position bias) with a percentage agreement of 61.53% and a Cohen's kappa of 0.2121.On the other hand, Claude-3-opus showed 52.5% position bias but had the highest correlation with human evaluation in the valid data, with a percentage agreement of 68.42% and a Cohen's kappa of 0.3152.</p>
<p>Patient History</p>
<p>An 84-year-old female with a past medical history of hypertension presented with weakness, dry cough, and shortness of breath for four days.The patient had received two doses of the COVID vaccine, with the second dose in March 2021.In the ER, her vital signs were BP 133/93, HR 103 bpm, RR 22 breaths/min, oxygen saturation of 96% on 40 L per minute of supplemental oxygen via high-flow nasal cannula, and afebrile.Laboratory assessment is in Table.• ⊠ immediate management or priority in management (septic shock)</p>
<p>• □ follow-up or monitoring approach regarding the management plan</p>
<p>• ⊠ current/short-term management (sepsis)</p>
<p>• □ severity of patient condition in terms of need for referral for surgical treatments/procedures    Here, u :&lt; c, q, a, d &gt; consists of: -context (c) which is a modified excerpt from n tailored to align with the target style and obscure evidence information that can easily lead directly to the correct answer, based on a topic t and a keypoint k; -question (q) based on the generated context based on the generated context c, which may be one or several sentences; -correct answer (a) to this question and, -distractor options (d) to challenge the test-taker.</p>
<p>Our framework consists of a MCQG-SRefine pipeline for generating the target u.</p>
<p>Input: Medical Cases</p>
<p>A 67-year-old Caucasian female presented to our hospital with a chief complaint of persistent bright red blood per rectum.Her medical history was significant for hypertension, hyperlipidemia, diabetes mellitus type 2, coronary artery disease with three prior myocardial infarctions, recurrent cerebrovascular accidents requiring anticoagulation with warfarin, gastroesophageal reflux disease, asthma, and endometrial cancer status post radiation therapy.Fifteen months prior to the current presentation, the patient was noted to have a grade 1 endometrial adenoma but was not considered a good surgical candidate due to multiple comorbidities.Vaginal hysterectomy was considered but due to her long and narrow vagina, this option was deferred initially.Her only treatment option was radiation therapy and brachytherapy.She eventually underwent total abdominal hysterectomy with bilateral salpingo-oopherectomy due to continued pelvic pain.The patient denied any prior history of gastrointestinal (GI) bleeding.Her bleeding was described as one large episode of bright red blood per rectum associated with blood clots.She denied any abdominal pain, nausea, vomiting, diarrhea, constipation, or melena.The most recent colonoscopy was performed four months ago and revealed three diminutive polyps in the transverse colon with pathology confirming tubular adenoma.Her physical examination was significant for mild left-sided abdominal tenderness but was otherwise unremarkable.Rectal examination was notable for nonbleeding hemorrhoids and no visible blood.Blood work revealed white blood cells of 14.3k/uL (normal range 4.3-10.0k/uL) and hemoglobin of 9.6 g/dL (normal range 11.8-14.8g/dL), which is similar to the patient's baseline.Creatinine was slightly elevated to 1.2 and blood urea nitrogen was elevated to 39.International normalized ratio was 2.0.Due the large volume of hematochezia and presence of anemia, the patient was admitted to the hospital and underwent a colonoscopy, which revealed a large, fungating, friable, and ulcerated nonobstructing mass in the sigmoid colon.The mass was noncircumferential, measured 4 cm in length, and was located 15-19 cm from the anal verge.Biopsies were obtained with cold forceps for histology and the proximal and distal margins of the mass were tattooed.Histology showed invasive and moderately differentiated carcinoma without visible goblet cells.Given the patient's history of endometrial cancer, immunohistochemistry was performed and was consistent with an endometrial (endometrioid subtype) primary.Pan-computed tomography was negative for any obvious distant disease.The tumor marker carcinoembryonic antigen level was normal as well.The patient subsequently underwent surgical resection of the affected colon followed by radiation therapy.At a follow-up visit one year later, the patient was doing well without active GI complaints.</p>
<p>Input: Topics</p>
<p>The topics come from an official list of 16 target topics from USMLE.We use both human annotated and model identified topics for generating USMLE questions.-Four sub-steps generate different components of the target output u.</p>
<p>-Examples for few-shot prompting are retrieved from the COLBERT retriever model R.</p>
<p>-These examples consist of USMLE questions from a question bank.</p>
<p>Context-Question Pair Retrieval:</p>
<p>-Use a query to retrieve the most relevant context-question pairs from R.</p>
<p>-Prompt model M with these pairs, along with other relevant information, to generate the context c.</p>
<p>Question Retrieval:</p>
<p>-Use the context to retrieve the next most relevant question from R.</p>
<p>-Prompt model M with this question, along with other information, to generate the question q.Iteration:</p>
<p>-Repeat the process until generating the complete target output u, including context, question, correct answer, and distractor options.</p>
<p>MCQG-SRefine INIT Output</p>
<p>Context: A 67-year-old Caucasian female with a significant medical history including hypertension, hyperlipidemia, diabetes mellitus type 2, coronary artery disease with three prior myocardial infarctions, recurrent cerebrovascular accidents, gastroesophageal reflux disease, asthma, and endometrial cancer presented with persistent bright red blood per rectum.She underwent radiation therapy and brachytherapy for her endometrial cancer but eventually required total abdominal hysterectomy with bilateral salpingo-oopherectomy due to persistent pelvic pain.She had no prior history of gastrointestinal bleeding.Her recent colonoscopy revealed three diminutive polyps in the transverse colon confirmed as tubular adenoma.Upon admission for an episode of hematochezia and anemia, she underwent a colonoscopy revealing a large, fungating, nonobstructing mass in the sigmoid colon.The biopsy was consistent with invasive and moderately differentiated carcinoma, and immunohistochemistry indicated an endometrial primary.The patient underwent surgical resection of the colon mass and subsequent radiation therapy.A follow-up visit a year later revealed no active GI complaints.</p>
<p>Question: What is the most appropriate long-term treatment for a 67-year-old female patient with a history of endometrial cancer who presented with a large, nonobstructing mass in the sigmoid colon due to invasive and moderately differentiated carcinoma?Correct answer: Radiotherapy or chemotherapy</p>
<p>Distractor options:</p>
<p>-A: Oral corticosteroids -B: Aspirin therapy -C: Bisphosphonates -D: Hormone replacement therapy -E: Anticoagulation therapy MCQG-SRefine QUESTION ANSWER Step -In this step u is presented as a USMLE question for M , i.e. u without the knowledge of the correct answer a, the options presented are d ∪ a.</p>
<p>-M generates an attempted answer aa and a rationale which supports aa in the form of a reasoning r behind it.MCQG-SRefine QUESTION ANSWER Output Attempted answer:Radiotherapy or chemotherapy Reasoning: The patient's history of endometrial cancer and the finding of a large, nonobstructing mass in the sigmoid colon are significant.The biopsy results revealing invasive and moderately differentiated carcinoma with immunohistochemistry indicating an endometrial primary suggest that this mass is likely a metastasis from her endometrial cancer.In such a case, systemic therapy with chemotherapy or targeted radiation therapy is often the most appropriate treatment to control the disease and prevent further metastases.</p>
<p>-Option A (Oral corticosteroids) is not typically used as a primary treatment for cancer.It may be used in certain situations to manage side effects of cancer or cancer treatment, but it wouldn't be the mainstay of treatment in this case.</p>
<p>-Option B (Aspirin therapy) is sometimes used as a preventive measure against colorectal cancer, but it is not a treatment for existing cancer.</p>
<p>-Option C (Bisphosphonates) are used to prevent bone loss and fractures in patients with certain types of cancer, such as breast and prostate cancer, that have spread to the bones.They are not typically used as a primary treatment for colorectal or endometrial cancer.</p>
<p>-Option D (Hormone replacement therapy) is typically used to manage symptoms of menopause, but it's not typically used to treat endometrial cancer, particularly in a patient with a history of this disease.</p>
<p>-Option E (Anticoagulation therapy) is used to treat or prevent blood clots, but it does not treat cancer itself.Hence, based on the above discussion, the most appropriate long-term treatment for this patient would be radiotherapy or chemotherapy</p>
<p>MCQG-SRefine CRITIQUE Step</p>
<p>-After generating all the components in the initialization and answer steps, the model M is tasked with critiquing each component in this feedback step.</p>
<p>-The set of components to be critiqued, u critique , which includes the context (c), question (q), correct answer (a), distractor options (d), and rationale (r).</p>
<p>-Model M receives a scoring guide (G) containing all the aspects on which each component needs to be scored.</p>
<p>-The prompt also includes few-shot examples of already scored components and relevant information.</p>
<p>-The final output of this step is the feedback for all components, denoted as f .It includes a short textual critique and a score for each aspect of the respective components.Rubric for the Feedback Output Rubric for the Context:</p>
<p>-Relevant: The context should be relevant to the topic provided.</p>
<p>-Concision: The context should be concise and shouldn't include extraneous information or just be a copy of the clinical note.</p>
<p>-Coherent: The context should be coherent enough and should organically be built for the question in the end.</p>
<p>-Consistent: The context is consistent with the information in the clinical note and the topic.</p>
<p>-Specific: The context should be specific and address the topic.</p>
<p>-Fluent: The context is fluent in terms of grammar and flow of words and ideas.</p>
<p>-Clueing: Instead of directly mentioning the diagnosis of a medical condition, it should be clued in through the mention of symptoms.</p>
<p>-Completeness: The context should be complete and free from gaps or missing information that could lead to ambiguity in answering the question accurately.</p>
<p>-Misdirection: Does the context avoid misleading the test taker intentionally or unintentionally?Rubric for the Question:</p>
<p>-Relevant: The question should be answerable from the information provided in the context and should not be abrupt.</p>
<p>-Clear: The question should not be vague or ambiguous.</p>
<p>-Concluding: The flow of ideas from the context should organically result into the question.</p>
<p>-Difficulty: The question should not be too easy.</p>
<p>-Clarity: Is the question unambiguous to avoid incorrect interpretations caused by ambiguity or poor wording?Rubric for the Correct Answer:</p>
<p>-Relevant: The correct answer should be either the keypoint or very related to it.</p>
<p>-Occurrence: The correct answer or any of its semantic or syntactic forms and directly related medical concepts should not occur in the context.</p>
<p>-Justification: Is the correct answer logically supported by the context and aligned with the provided information?</p>
<p>-Depth of Understanding: Does the correct answer demand nuanced understanding of the context or concepts, ensuring the test taker genuinely grasps the material?</p>
<p>-Prevention of Guesswork: Does the correct answer deter guessing and align with the context, avoiding common misconceptions?Rubric for the Distractors:</p>
<p>-Format: The distractors should be in similar formats as the correct answer, i.e. if it is an abbreviation or an explanation.</p>
<p>-Length: The distractors should have similar length as the correct answer.</p>
<p>-Relation: The distractors should be related to the correct answer through some medical concepts or they should be the same kind of medical entities.</p>
<p>-Variation: Distractors should be distinct from each other and from the correct answer.</p>
<p>-Plausibility: Do the options align with the context and challenge critical thinking?-Differentiation: Are the options distinct and does the correct answer clearly outshine the distractors based on context and available information?</p>
<p>-Common Mistakes: Distractors should align with common misconceptions to test genuine understanding?Rubric for the Chain of Thought/Reasoning:</p>
<p>-Logical Flow: Does the chain of thought exhibit a coherent sequence of steps or considerations that are easy to follow?-Evidence-Based Reasoning: Is the answer supported by evidence or information from the context, justifying the chosen response?</p>
<p>-Consideration of Options: Does the chain of thought demonstrate critical evaluation of each option, employing a systematic process to eliminate distractors with supporting evidence?Example of Distractors Feedback Output -Format: The format of the distractor options matches that of the correct answer.All options are treatment modalities, which is suitable for the question.Score: 5/5.</p>
<p>-Length: The length of the distractor options is similar to the length of the correct answer.Score: 5/5.</p>
<p>-Relation: The distractor options are related to the correct answer as they are all medical treatments, but not all are related to cancer treatment which is the specific context.Score: 3/5.</p>
<p>-Variation: The distractor options are distinct from each other and from the correct answer, which helps to test the test-taker's understanding of the context.Score: 5/5.</p>
<p>-Plausibility: The distractor options are plausible treatments for certain conditions, but not necessarily the condition described in the context, which is advanced cancer.Score: 3/5.</p>
<p>-Differentiation: The correct answer (radiotherapy or chemotherapy) clearly outshines the distractors based on the context and available information.Score: 5/5.</p>
<p>-Common Mistakes: The distractors do not align with common misconceptions about cancer treatment.They represent treatments for other conditions, not common mistakes in cancer management.Score: 3/5.Distractor Score: Total Score: 29/35 MCQG-SRefine Correction Step -The feedback generated in the previous step is utilized to refine each of the generated components of the target output u from the initialization step.</p>
<p>-M is prompted with inputs including examples for refinement, the feedback (f ), and the current components (c, q, a, d, aa, r).</p>
<p>-M generates u ref ined , aiming to improve its performance on all aspects critiqued in the feedback.</p>
<p>u ref ined is then subjected to the answer step and then to the feedback step.</p>
<p>-If the feedback scores surpass a threshold, the iterative refinement process halts.Otherwise, the process continues for up to four steps of feedback and refinement.</p>
<p>MCQG-SRefine Correction Output</p>
<p>Context: A 67-year-old female with a history of endometrial cancer presented with symptoms of rectal bleeding.She had previously undergone radiation therapy and brachytherapy for her cancer.On admission, a colonoscopy was performed due to her history and the presenting symptoms, which revealed a large mass in the sigmoid colon.Biopsy of the mass showed invasive and moderately differentiated carcinoma, with immunohistochemistry suggesting an endometrial primary.The patient underwent surgical resection of the mass followed by radiation therapy.Question: Following surgical resection of a colon mass in a patient with a history of endometrial cancer, what would be the most appropriate long-term management approach?</p>
<p>Correct answer: radiotherapy or chemotherapy Distractor options:</p>
<p>-a) oral corticosteroids -b) aspirin therapy -c) bisphosphonates -d) hormone replacement therapy -e) anticoagulation therapy</p>
<p>Evaluation Guide</p>
<p>Your task is to evaluate two system outputs, each consisting of a machine-generated USMLE question.Each question has four components: context, a question based on the context, the correct answer, and distractor options.</p>
<p>The questions are generated using a clinical (EHR) note, a topic from the official USMLE topic list, and a keypoint.The keypoint is a specific aspect on which the student is tested, either related to or being the correct answer itself.</p>
<p>You have to annotate the following: Metrics for the context:</p>
<p>-Relevant: The context should be relevant to the topic provided.</p>
<p>-Concision: The context should be concise and shouldn't include extraneous information or just be a copy of the clinical note.</p>
<p>-Coherent: The context should be coherent enough and should organically be built for the question in the end.</p>
<p>-Consistent: The context is consistent with the information in the clinical note and the topic.</p>
<p>-Specific: The context should be specific and address the topic.</p>
<p>-Fluent: The context is fluent in terms of grammar and flow of words and ideas.</p>
<p>-Clueing: Instead of directly mentioning the diagnosis of a medical condition, it should be clued in through the mention of symptoms.</p>
<p>-Completeness: The context should be complete and free from gaps or missing information that could lead to ambiguity in answering the question accurately.</p>
<p>-Misdirection: Does the context avoid misleading the test taker intentionally or unintentionally?</p>
<p>Metrics for the question:</p>
<p>-Relevant: The question should be answerable from the information provided in the context and should not be abrupt.</p>
<p>-Clear: The question should not be vague or ambiguous.</p>
<p>-Concluding: The flow of ideas from the context should organically result in the question.</p>
<p>-Difficulty: The question should not be too easy.</p>
<p>-Clarity: Is the question clear and unambiguous to avoid incorrect interpretations caused by ambiguity or poor wording?</p>
<p>Metrics for the correct answer:</p>
<p>-Relevant: The correct answer should be either the keypoint or very related to it.</p>
<p>-Occurrence: The correct answer or any of its semantic or syntactic forms and directly related medical concepts should not occur in the context.</p>
<p>-Justification: Is the correct answer logically supported by the context and aligned with the provided information?</p>
<p>-Depth of Understanding: Does the correct answer demand a nuanced understanding of the context or concepts, ensuring the test taker genuinely grasps the material?</p>
<p>-Prevention of Guesswork: Does the correct answer deter guessing and align with the context, avoiding common misconceptions?</p>
<p>Metrics for the distractors:</p>
<p>-Format: The distractors should be in similar formats as the correct answer, i.e., if it is an abbreviation or an explanation.</p>
<p>-Length: The distractors should have a similar length as the correct answer.</p>
<p>-Relation: The distractors should be related to the correct answer through some medical concepts, or they should be the same kind of medical entities.</p>
<p>-Variation: Distractors should be distinct from each other and from the correct answer.</p>
<p>-Plausibility: Do the options align with the context and challenge critical thinking?-Differentiation: Are the options distinct, and does the correct answer clearly outshine the distractors based on context and available information?</p>
<p>-Common Mistakes: Distractors should align with common misconceptions to test genuine understanding.Cases gpt-4o gpt4 gpt3.5 opus sonnet haiku Expert X
1 1 2 1 1 1 -1 2 1 1 -1 -1 2 2 3 -1 2 -1 -1 2 -1 4 1 2 -1 -1 -1 2 5 -1 -1 2 1 -1 1 6 1 -1 2 1 1 -1 7 -1 1 -1 2 2 2 8 1 1 -1 -1 -1 -1 9 1 1 -1 -1 -1 2 10 -1 1 -1 2 -1 2 11 -1 -1 -1 -1 -1 1 12 2 -1 2 1 -1 2 13 2 2 2 1 1 -1 14 2 1 -1 -1 1 2 15 1 -1 2 -1 -1 2 16 -1 -1 2 1 -1 2 17 1 2 2 1 2 -1 18 -1 2 2 1 -1 2 19 -1 -1 2 1 2 -1 20 1 -1 -1 -1 2 2 21 -1 1 -1 -1 -1 2 22 1 -1 1 1 -1 2 23 1 1 1 -1 1 2 24 -1 -1 1 1 -1 2 25 -1 -1 2 1 -1 1 26 1 2 2 1 2 -1 27 2 1 -1 -1 -1 -1 28 1 2 -1 -1 1 1 29 2 1 2 1 2 -1 30 -1 -1 1 1 -1 2 31 1 -1 1 1 -1 -1 32 -1 1 -1 1 2 -1 33 1 2 1 -1 -1 2 34 -1 1 1 -1 -1 -1 35 -1 1 1 -1 -1 -1 36 1 2 1 -1 -1 2 37 -1 1 -1 -1 2 -1 38 -1 1 -1 -1 -1 -1 39 1 -1 -1 1 -1 -1 40 1 2 1 1 2 2</p>
<p>Case Study 1</p>
<p>Clinical Note: A 35-year-old man presented with the complaints of low back ache for 12 years, radiating to right leg for 4 months and numbness extending to lateral side of the sole of right foot.On examination, there was a 30% sensory loss in right S1 dermatome as compared with contralateral limb, with no bladder bowel involvement.Patient had no motor deficit.Magnetic resonance imaging (MRI) of the lumbosacral spine was suggestive of IM cystic lesion at L2-3 hypointense on T1-weighted images and hyperintense on T2-weighted images.MRI brain did not reveal any abnormality.Lumbar puncture and serologic studies were not performed.With the differential diagnosis of neoplastic lesion, the patient was taken up for posterior laminectomy.L2-3 laminectomy was done.A dural bulge was identified.On durotomy, the cord was found to be enlarged.Under microscopic guidance, posterior longitudinal myelotomy was done, the cysts were approached, and subtotal resection of cysts was done.Intraoperatively, three grayish white cysts were identified.Cysts were found to be adherent to the nerve roots causing their inflammation.</p>
<p>As a result, one of the cysts could not be excised and was only decompressed.The remaining two cysts were completely excised.Histopathology revealed it to be NCC.The patient improved postoperatively.Back pain was relieved, and there was significant reduction in radiating pain.He was started on albendazole (15 mg/kg body weight) for 4 weeks and steroids for 2 weeks.The patient was discharged on the 4th post-operative day.He was followed-up biweekly for the first month.Thereafter, monthly follow-up was done for the next 2 months.MRI done at 6 months confirmed resolution of the cystic lesion.Thereafter, 6 monthly follow-up was done.Patient is symptom free and not on any medication at 2 years of follow-up.</p>
<p>-Topic: select most appropriate laboratory or diagnostic study -Keypoint: MRI</p>
<p>GPT-4 generated USMLE question:</p>
<p>Context: A 35-year-old man has been experiencing low back ache for 12 years, with pain radiating to his right leg for the past 4 months.He also reports numbness extending to the lateral side of the sole of his right foot.Upon examination, a 30% sensory loss in right S1 dermatome was found compared to the contralateral limb, but there was no bladder or bowel involvement and no motor deficit.An MRI of his lumbosacral spine revealed an IM cystic lesion at L2-3, which was hypointense on T1-weighted images and hyperintense on T2-weighted images.No abnormalities were found in an MRI of the brain.The patient underwent a posterior laminectomy and L2-3 laminectomy, during which three grayish-white cysts were identified.These cysts were found to be adhering to the nerve roots, causing inflammation.One cyst could not be completely excised and was only decompressed, while the other two were completely removed.Histopathology confirmed the diagnosis of NCC.Postoperative improvement was noted in the patient, with relief from back pain and significant reduction in radiating pain.He was prescribed albendazole and steroids and was discharged on the 4th post-operative day.Follow-ups were conducted biweekly for the first month, monthly for the next two months, and then every six months.An MRI taken six months post-operatively confirmed the resolution of the cystic lesion.After two years, the patient is symptom-free and not on any medication.GPT-4: Easy MCQG-SRefine: Hard Reasons:</p>
<ol>
<li>Question (GPT-4) consists of the answer to the question in the context.2. Question (MCQG-SRefine) context more Concision.</li>
</ol>
<p>Case Study 2</p>
<p>Clinical Note: This 33-year-old female patient had typical COVID-19 symptoms such as high fever, dry cough, headache, and dyspnea about 1 week before ICU admission.She was intubated and proned due to rapid respiratory deterioration.For the following 6 days, her situation was unstable, and physical therapy consisted of prone positioning and prevention of secondary damage.From day 7 onwards, she started to improve rapidly and could be mobilized passively into a sideedge position.After extubation, she presented postextubation dysphagia and severe ICU-acquired weakness (MRC-SS 36/60).She also suffered from pronounced delirium and anxiety and said repeatedly that she had been abducted and that she believed she had to die.She seemed to feel threatened by us and it was difficult to calm her down.Due to the pandemic measures of the Swiss government, hospital visits were not generally allowed, but because her anxiety was limiting her rehabilitation, her husband was granted an exceptional permission to visit her.This seemed to give the patient a short sense of security, and she started to participate in some basic functional activities (CPAx 21/50).Nevertheless, the delirium did not resolve upon her transfer to a peripheral acute hospital.</p>
<p>-Topic: the cause/infectious agent or predisposing factor(s) -Keypoint: covid-19</p>
<p>GPT-4 generated USMLE question: Context: A 33-year-old female patient presented with typical symptoms of COVID-19, including high fever, dry cough, headache, and dyspnea.She was admitted to the ICU due to rapid respiratory deterioration and was intubated and proned.Despite physical therapy and prone positioning, her condition remained unstable for 6 days.From the seventh day, she started showing signs of improvement and was passively mobilized into a side-edge position.However, post-extubation, she experienced dysphagia and severe ICU-acquired weakness along with pronounced delirium and anxiety.Her rehabilitation was further hampered due to her delirium, which led to her belief of being abducted and fear of death.Due to her heightened anxiety, her husband was granted special permission to visit her, which provided her with a temporary sense of security and encouraged her to participate in basic functional activities.Despite this, her delirium persisted even after her transfer to a peripheral acute hospital.GPT-4: Easy MCQG-SRefine: Hard Reasons:</p>
<p>1.According to the context in Question (MCQG-SRefine), it is hard to get the right answer.Question (MCQG-SRefine) tries to make it harder, but at the same time, it becomes confusing.</p>
<ol>
<li>The answer to Question (MCQG-SRefine) can be all the following distractors.Because there is no evidence to suspect the infection of COVID-19.Question (GPT-4), however, starts the context by giving evidence that the patient was already diagnosed with Covid-19.But the answer also has COVID-19, so this is an error of the question.</li>
</ol>
<p>Case Study 3</p>
<p>Clinical Note: A 35-year-old male patient presented with a six-month history of pain in the anterior chest and neck.His pain initially had been mild but had become severe in the last three weeks.He was afebrile and did not have any constitutional symptoms.On examination, marked tenderness of the right sternoclavicular joint was noted.Initial laboratory investigations, tuberculosis workup, and chest X-rays were unremarkable.However, further study with MRI revealed subchondral bone marrow edema and enhancement involving the medial end of the right clavicle.In addition, mild effusion of the right sternoclavicular joint and surrounding soft-tissue edema was also seen (Figure , ).Given the patient's radiological and clinical findings, suspicion of SAPHO syndrome was raised.He was given an initial trial of paracetamol and NSAIDs and he showed remarkable improvement on his monthly follow-up visits.Though a biopsy was offered, the patient refused to undergo one and has shown no relapse to date thanks to the dramatic response to NSAIDs.</p>
<p>-Topic: select most likely diagnosis yes -Keypoint: SAPHO syndrome</p>
<p>GPT-4 generated USMLE question:</p>
<p>Context: A 35-year-old male patient has been experiencing anterior chest and neck pain for six months, which has become severe in the last three weeks.Despite being afebrile and showing no constitutional symptoms, the right sternoclavicular joint was notably tender on examination.Initial investigations, including laboratory tests, tuberculosis workup, and chest X-rays, yielded no remarkable findings.However, an MRI revealed subchondral bone marrow edema and enhancement at the medial end of the right clavicle, along with mild effusion of the right sternoclavicular joint and surrounding soft-tissue edema.Given these clinical and radiological findings, the suspicion of SAPHO syndrome was raised.The patient was started on paracetamol and NSAIDs, which led to a significant improvement in his condition during subsequent monthly follow-ups.Although a biopsy was considered, the patient declined.He has remained free of relapse due to the effective response to NSAIDs.</p>
<p>MCQG-SRefine generated USMLE question</p>
<p>Context: A 35-year-old male has been dealing with pain in his anterior chest and neck area for the past half a year, which has recently escalated in severity.Despite his discomfort, he had no other systemic symptoms and was afebrile.Upon physical examination, severe tenderness was noted in the right sternoclavicular joint.While initial investigations including laboratory tests and chest radiographs showed no abnormalities, an MRI scan revealed edema and enhancement in the bone marrow beneath the cartilage at the medial end of the right clavicle.There were also signs of mild effusion in the right sternoclavicular joint and surrounding soft tissue edema.After being administered paracetamol and NSAIDs, the patient showed significant improvement in his condition.GPT-4: Easy MCQG-SRefine: Hard Reasons:</p>
<p>1.In Question (GPT-4), the answer was directly listed in the context.2. In Question (MCQG-SRefine), only the signs/symptoms for SAPHO syndrome are given."</p>
<p>Case Study 4</p>
<p>Clinical Note: A 35-year-old male patient presented with a six-month history of pain in the anterior chest and neck.His pain initially had been mild but had become severe in the last three weeks.He was afebrile and did not have any constitutional symptoms.On examination, marked tenderness of the right sternoclavicular joint was noted.Initial laboratory investigations, tuberculosis workup, and chest X-rays were unremarkable.However, further study with MRI revealed subchondral bone marrow edema and enhancement involving the medial end of the right clavicle.In addition, mild effusion of the right sternoclavicular joint and surrounding soft-tissue edema was also seen (Figure , ).Given the patient's radiological and clinical findings, suspicion of SAPHO syndrome was raised.He was given an initial trial of paracetamol and NSAIDs and he showed remarkable improvement on his monthly follow-up visits.Though a biopsy was offered, the patient refused to undergo one and has shown no relapse to date thanks to the dramatic response to NSAIDs.</p>
<p>-Topic: knows signs/symptoms of selected disorders -Keypoint: SAPHO syndrome</p>
<p>GPT-4 generated USMLE question:</p>
<p>Context: A 35-year-old male presents with a six-month history of anterior chest and neck pain that has progressively worsened over the last three weeks.The patient is afebrile and does not display any constitutional symptoms.Upon examination, significant tenderness is noted in the right sternoclavicular joint.Initial laboratory tests, including a tuberculosis workup and chest X-rays, do not reveal any abnormalities.However, an MRI shows subchondral bone marrow edema and enhancement at the medial end of the right clavicle, along with mild effusion of the right sternoclavicular joint and surrounding soft-tissue edema.The patient's clinical and radiological findings lead to a suspicion of SAPHO syndrome.An initial treatment with paracetamol and NSAIDs results in significant improvement, and the patient continues to show no signs of relapse.Despite the offer of a biopsy, the patient chooses not to undergo the procedure.Question: What are the common signs and symptoms of SAPHO syndrome?Correct answer: Anterior chest and neck pain, tenderness in the sternoclavicular joint, subchondral bone marrow edema and enhancement, mild effusion of the sternoclavicular joint and surrounding soft-tissue edema.</p>
<p>Distractor options:</p>
<p>-A : Swelling and redness over the affected joint, accompanied by intense pain.</p>
<p>-B : Chronic diarrhea, abdominal pain, and weight loss.</p>
<p>-C : Presence of skin rashes, dry mouth and eyes.</p>
<p>-D : Persistent cough, chest pain, and difficulty in breathing.</p>
<p>-E : Severe headache, nausea, and light sensitivity.</p>
<p>MCQG-SRefine generated USMLE question</p>
<p>Context: A 35-year-old male patient has been experiencing pain in his chest and neck for six months.This pain has been intensifying over the past three weeks.He does not have any fever or other systemic symptoms.During the physical exam, we noticed significant tenderness in his right sternoclavicular joint.While initial lab tests and chest X-rays didn't reveal anything unusual, an MRI showed signs of subchondral bone marrow edema in the medial end of the right clavicle, a mild effusion in the right sternoclavicular joint, and some soft-tissue edema surrounding the joint.These clinical and radiological findings are suggestive of a particular condition.The patient was treated with paracetamol and NSAIDs, which led to a significant improvement in his symptoms.Question: Based on the described signs and symptoms, which syndrome is most likely to be suspected in this patient?  .The correct answer is the highest, maybe because it is easier to judge a correct answer since it is the shortest lengthwise, and in case it is right or wrong, it is very easy to reject a particular question, whereas other components are considerably more subjective.</p>
<p>Figure 3 :
3
Figure 3: The quality expert preference for the GPT-4 and the MCQG-SRefine question.The data is divided into Human and Machine based on how the topic tand key points k were generated.We only put the final Expert X preferences here, but we provide more results in the Appendix Table10.The percentage agreement between experts is 87.5% (Human<t, k>: 90%, Machine<t, k>: 85%).The Cohen's kappa between experts is 0.66722 (Human<t, k>: 0.75, Machine<t, k>: 0.57), indicating substantial reliability.</p>
<p>Figure 4 :
4
Figure 4: The difficulty expert evaluation for the GPT-4 generated question and the MCQG-SRefine question.</p>
<p>Figure 5 :
5
Figure 5: LLM-as-Judge (Rating) results for different components (e.g., Context, Question, Correct Answer, Distractor, Reasoning) and the final score.</p>
<p>Figure 6 :
6
Figure 6: Best scoring rounds.</p>
<p>Task</p>
<p>Input and Response R a , R b I: Generate a USMLE question based on the following medical case: A 84-year-old male presents with chest pain ....... Topic: pathophysiology of sepsis Test point: sepsis R a : system-a generated USMLE question R b : system-b generated USMLE question Critique F crq (I, R b ) The generated question fails to meet the quality and difficulty requirements for the USMLE because ......The following suggestions could be considered: ..... Score for Question R b : 1/10 Correction F crr (I, R b , F crq ) with F crq Here is the revision for question B: Context: The 84-year-old female patient presented to the emergency room with ...... Question: What is the most likely complication that has developed in this patient's case?Correct answer: Septic shock Distractor options: a) b) c) d) Comparison F cmp (I, R a , R b ) Question A has higher quality than B as it ...... On the contrary, question B ...... Preference Response: A B Ablation study and output analysis of MCQG-SRefine B.1 Round-wise metrics As shown in</p>
<p>Figure 7 :
7
Figure 7: No. of refinement rounds.This histogram indicates that most of the data points(78.9%for human, 80.4% for machine) take the full 4 iterations in pursuit of the threshold score.</p>
<p>Figure 8 :
8
Figure 8: Length of the generated question human.</p>
<p>Figure 9 :
9
Figure 9: Length of the generated question machine.</p>
<p>Figure 10 :
10
Figure 10: Clinical note length v/s best scoring round.</p>
<p>Figure 11 :
11
Figure 11: Question length v/s best scoring round.</p>
<p>Nasopharyngeal swab for SARS-CoV-2 RNA was positive.Chest X-ray on admission shows worsening right pleural effusion with new opacity obscuring the lower two-third of the right lung and a new pleural-based opacity in the left upper lobe Figure.CT chest with contrast shows large right pleural effusion and associated right basilar consolidation and abdominal ascites.The patient was admitted to the telemetry unit and started on methylprednisolone, piperacillin-tazobactam, remdesivir, and baricitinib.The patient clinically deteriorated on Day 2 and was transferred to the intensive care unit for thoracentesis and possible intubation.Thoracentesis removed 1.95 L of bloody, serosanguineous fluid obtained, with partial resolution of the effusion Figure.On Day 3, the patient developed septic shock, florid renal failure, and lethargy and was started on intravenous fluids and norepinephrine drip.Chest X-ray showed near-complete opacification of bilateral lung fields and subsequently was intubated.On Day 4, tense ascites were noted and the patient underwent paracentesis, which removed 4.25 L of bloody, serosanguineous fluid.Renal replacement therapy started.The patient was deemed to have a guarded prognosis with multiorgan failure.Diagnosis Causes and Mechanisms • □ the cause/infectious agent or predisposing factor(s) • ⊠ underlying processes/pathways (pathophysiology of sepsis) • □ underlying anatomic structure or physical location • ⊠ mechanisms, drugs (tazobactam, beta-lactamase inhibitors) Obtaining and Predicting History and Physical Examination • □ knows signs/symptoms of selected disorders • □ knows individual's risk factors for development of condition • □ knows what to ask to obtain pertinent additional history • ⊠ predicts the most likely additional physical finding (pleural effusion) Selecting and Interpreting Laboratory and Diagnostic Studies • □ select most appropriate laboratory or diagnostic study • ⊠ interprets laboratory or other study findings (pleural fluid analysis) • ⊠ predicts the most likely laboratory or diagnostic study result (pleural fluid protein/serum protein ratio &gt; 0.5) • □ most appropriate laboratory or diagnostic study after change in patient status Formulating the Diagnosis</p>
<p>Question: Based on the clinical and radiological findings, what is the most likely diagnosis for a 35-year-old male patient experiencing severe anterior chest and neck pain, and showing signs of subchondral bone marrow edema and enhancement at the medial end of the right clavicle on an MRI</p>
<p>Question: What diagnosis does the clinical presentation and the results of the MRI scan most likely indicate</p>
<p>The aspects for scoring different components in G are as follows: Context: Relevant, Concise, Coherent, Consistent, Specific, Fluent, Clueing, Completeness, and Misdirection.Question: Relevant, Clear, Concluding, Difficulty, and Clarity.Correct Answer: Relevant, Occurrence, Justification, Depth of Understanding, and Prevention of Guesswork.Distractors: Format, Length, Relation, Variation, Plausibility, Differentiation, and Common Mistakes.Chain of Thought/Reasoning: Logical Flow, Evidence-Based Reasoning, and Consideration of Options.We provide detailed explanations for each aspect of every component in Appendix Table 15, and LLM-Critique prompts in Table 16.The total score for each component is calculated by summing up all individual aspect scores.A sample output is provided in Appendix Table 15.</p>
<p>Table 2 :
2Corr. w/ Expert-xP. AgreementC. Kappaw/o aspect filtering67.5%0.226190476PA-based Greedy72.5%0.342301943CK-based Greedy72.5%0.423328965PA-based All-Comb80.0%0.505409582CK-based All-Comb*80.0%0.538904899
The correlation between the LLM-as-Judge and Expert-X across different aspect filtering methods.</p>
<p>Stopping CriteriaThe stopping criteria for the MCQG-SRefine included two checks.One was that the iteration would stop if the total critique score of all the components exceeded 90% of the maximum possible score, and the other was if the number of iterations exceeded 4.
10. Gastrointestinal System11. Renal &amp; Urinary System12. Pregnancy, Childbirth, &amp; the Puerperium13. Female Reproductive System &amp; Breast1. Diagnosis -Causes and Mechanisms14. Male Reproductive System2. Diagnosis -Obtaining and Predicting History15. Endocrine Systemand Physical Examination16. Multisystem Processes &amp; Disorders3. Diagnosis -Selecting and Interpreting Labo-ratory and Diagnostic Studies17. Biostatistics, Health, &amp; Interpretation of the Medical Epidemiology/Population4. Diagnosis -Formulating the DiagnosisLiterature5. Diagnosis -Determining Prognosis/Outcome18. Social Sciences6. Management -Health Maintenance and Dis-ease Prevention7. Management -Selecting and Monitoring Phar-macotherapy8. Management -Clinical Interventions/Treat-ments9. Management -Selecting Clinical Interven-tions (Mixed Management)10. Management -Monitoring/Surveillance forDisease Recurrence or ProgressionTest points k refer to the core concepts closelyrelated to the correct answer. We employ the Col-BERT retriever 17 , denoted as π rtr , to retrieve suit-able test points from 18 sections.1. General Principles of Foundational Science2. Immune System3. Blood &amp; Lymphoreticular System4. Behavioral Health5. Nervous System &amp; Special Senses6. Skin &amp; Subcutaneous Tissue7. Musculoskeletal System8. Cardiovascular System9. Respiratory System17 https://github.com/stanford-futuredata/ColBERT</p>
<p>Table 4
4, the round-wise metrics forhuman data reveal nuanced trends in the model'sperformance through multiple feedback iterations.The total score exhibited a modest increase fromRound 1 (0.9031) to Round 3 (0.9062), followedby a slight decline in Round 4 (0.9006), suggest-ing that while the self-feedback process contributespositively, its benefits may diminish with exces-sive iterations. The context score notably improvedfrom 0.9683 in Round 1 to 0.9712 in Round 3,reflecting enhanced model comprehension of thecontext through feedback. However, performancein other areas fluctuated: the question score showeda slight decline across rounds, and while the correctanswer score improved from Round 1 to Round 3, itdecreased slightly in Round 4. Conversely, the dis-tractor option score showed steady improvement,culminating in the highest score by Round 4. Thereasoning score, however, demonstrated a gradualdecline over the rounds. The standard deviation formost metrics either decreased or remained stable,indicating more consistent performance. Overall,while certain components of the model benefitedfrom the feedback process, others did not, high-lighting the complexity of balancing improvementsacross different aspects of question generation.</p>
<p>Table 8 :
8
Correlation between different rounds with Clinical Note Length and Question Length for both Human and Machine data.
Data TypeVariablePearson Spearman KendallHumanRounds with Clinical Note Length -0.9103-1.0-1.0Machine Rounds with Clinical Note Length -0.8869-1.0-1.0HumanRounds with Question Length-0.9103-1.0-1.0MachineRounds with Question Length-0.8869-1.0-1.0ModelNo POS BiasPA.τGPT-465%61.53% 0.2121GPT-4o42.5%58.82% 0.3340GPT-3.5-turbo35%64.29% 0.0572Claude-3-haiku42.5%41.18% -0.0636Claude-3.5-sonnet37.5%46.67% 0.1517Claude-3-opus47.5%68.42% 0.3152</p>
<p>Table 9 :
9
Percentage Agreement (PA.) and Kendall's Tau (τ ) for Different Models Compared to Expert X using valid data (e.g., order matching for both GPT-4 generated output first setting and MCQG-SRefine generated output first setting).</p>
<p>3.5-sonnet
QualityDifficultyt, k GPT4 Ours TieGPT4Ours1H. M.15% 20%80% 5% 50:35:15 35:30:35 80% 0% 70:20:10 45:35:202H. M.20% 20%70% 5% 75% 0%60:35:5 65:30:530:65:5 60:35:5xH. M.15% 20%75% 10% 70:20:10 15:45:40 70% 10% 75:20:5 50:40:10Table 10: The Quality part shows expert preferencecounts for the GPT-4 generated question and the MCQG-SRefine (Ours) question. The data is divided into Hu-man (H.) and Machine (M.) based on how the topicand key points were generated. Expert x represents thepreferences reached by the experts after a round of pre-liminary annotation. The percentage agreement betweenExpert 1 and Expert 2 is 87.5% (Human: 90%, Machine:85%). The Cohen's kappa between Expert 1 and Expert2 is 0.66722 (Human: 0.75, Machine: 0.571428), indi-cating substantial reliability. The Difficulty part showsthe difficulty level distribution (e.g., Easy: Medium:Hard) for GPT-4 and Ours, annotated by human experts.Compared with GPT-4, the MCQG-SRefine pipelinegenerates better quality USMLE multiple-choice ques-tions while producing significantly more medium andhard questions.</p>
<p>Table 11 :
11avg. score MCQG-SRefineGPT-4Context0.96 (0.96, 0.95, 0.99) 0.95 (0.91, 0.96, 0.99)Question0.91 (0.89, 0.93)0.87 (0.84, 0.9)CQT ANS 0.79 (0.64, 0.94)0.64 (0.45, 0.83)Distractor0.94 (0.94)0.89 (0.89)Reasoning 0.985 (0.99, 0.98)0.97 (0.98, 0.96)Total0.9170.864
The table also shows the rating results for the 10 different aspects: Context (concision, relevance, misdirection), Question (concluding, clarity), Correct Answer (occurrence, depth of understanding), Distractor (common mistakes), and Reasoning (logical flow, evidence-based reasoning).</p>
<p>Table 12 :
12
Human annotation example for topics and test points.
Topic Name</p>
<p>Table 13 :
13
Frequency of Human identified topics
Topic Name</p>
<p>Table 14 :
14
Frequency of machine generated topicsProblem StatementGiven a medical case n detailing a patient's history, diagnosis, treatment, and outcome, we aim to generate a USMLE question u.</p>
<p>The key points are the core concepts to which the target correct answer is highly related or is the same as the keypoint.We use both human-annotated and model-identified key points to generate USMLE questions.
-underlying anatomic structure or physical location-mechanisms, drugs-Selecting and Interpreting Laboratory and Diagnostic Studies-select most appropriate laboratory or diagnostic study-interprets laboratory or other study findings-Monitoring/Surveillance for Disease Recurrence or Progression-most appropriate long-term treatment (our topic for the current clinical note)Input: Key PointsExample keypoints:-covid-19 with copd-Radiotherapy or chemotherapy (our keypoint for the current clinical note)-ARDS-osteonecrosis of the femoral head-echocardiography for Ebstein's anomaly-COVID-19 pneumoniaMCQG-SRefine INIT StepInitiation Step:Some example topics:-Causes and Mechanisms-the cause/infectious agent or predisposing factor(s)-underlying processes/pathways</p>
<p>Table 15 :
15
Demonstration of MCQG-SRefine Pipeline.</p>
<ol>
<li>Preference: Please indicate your preference for the better question in the 'Preference' column by writing 1 or 2, signifying Question 1 or Question 2, respectively.In the Remark column please give a short reasoning behind your choice.2. Difficulty level: For each Question 1 and Question 2, you have to adjudge which level the question falls under: Easy, Medium, or Hard.Please indicate your choice in the respective Question difficulty level column, i.e., Question 1 difficulty and Question 2 difficulty.3. Errors: For both Question 1 and Question 2, highlight any spans having errors, be it logical, syntactical, or medical.An example error could be if the question contradicts the clinical note.Write any such errors if they exist in the Error column.Below are some rubrics you can use to evaluate the two outputs.You don't need to assess each question on every metric; simply use these as a guideline to understand what makes a good USMLE question before you start evaluating:</li>
</ol>
<p>Table 17 :
17
Human evaluation guide.</p>
<p>Table 18
18: LLM-as-judge (comparison). 0 means tie, 1 means preferring GPT-4, 2 means preferring MCQG-SRefine,and -1 means position bias (LLM has different preferences in two different orders).</p>
<p>Table 19 :
19
Human evaluation case study.
ComponentAspectPercentage agreement Cohen's kappaContextconsistent10.8108-0.0008196Contextmisdirection10.52631-0.001550Contextcoherent26.315780.065026Contextclueing28.9473680.0393258Contextconcision26.31570.04488Contextrelevant18.421050.010084Contextspecific15.7894-2.22044Contextcompleteness13.157890.0079113Contextfluent10.5263-0.00310Questiondifficulty27.50000-0.01665Questionrelevant27.50000-0.03386Questionclarity40.00.08309Questionclear42.50.05056Questionconcluding52.50.197465Correct Answerdepth of understanding30.7692-0.02631Correct Answerrelevant12.82050.008968Correct Answerjustification12.82050.008968Correct Answeroccurrence28.205120.07379Correct Answerprevention of guesswork38.4610.0370Distractorvariation10.25640.0Distractorplausibility31.5780.023715Distractorcommon mistakes33.3330.10582Distractorlength19.44440.035120Distractordifferentiation17.9487-0.012165Distractorrelation28.205-0.02535Distractorformat12.820-0.06763Reasoninglogical flow7.5-0.069364Reasoningcorrectness7.5-0.048158Reasoningconsideration of options10.0-0.0020876Reasoningevidence_based_reasoning5.0-0.081081</p>
<p>Table 20 :
20
Aspect vs Expert-X Preference correlation (Human Eval 40 data).
ComponentPercentage agreement Cohen's kappaContext0.382350.0338Question0.50.05472Correct Answer0.470580.15522Distractor0.3235-0.0358Reasoning0.08823-0.07168</p>
<p>Table 21 :
21
Component vs Expert-X Preference correlation (Human Eval 40 data)</p>
<p>Our code and data is released at https://github.com/ bio-nlp/MedQG with CC-BY-NC 4.0 license
https://www.nbme.org/educators/ item-writing-guide
https://www.usmle.org/sites/default/files/ 2021-08/USMLE_Content_Outline.pdf
Details can be found in Appendix A.
https://github.com/stanford-futuredata/ ColBERT
Stopping criteria can be found in Appendix A.
Two medical students with 2+ years hospital experience.
One licensed physician.
https://www.amboss.com/us
5 components with 30 aspects in Appendix Table15.
The Appendix provides the pseudocode for the Greedy Aspect Selection and All-Combo algorithms.
We observed diminishing returns for the top 9, 10, and 11 aspects, and thus stopped at 11.
According to Cohen's kappa, the reliability improved from slight (0.226) to moderate (0.539).
Limitations and Ethical ConsiderationsOur research focuses on generating medical exam questions, with promising results, though its adaptability to other domains is yet to be explored. We evaluated the method with three medical experts, but future involvement of specialized USMLE exam experts could enhance credibility. Future research should address privacy, fairness, generalizability to other languages/domains, and potential biases. While our work marks an important step in medical exam question generation, broader societal impacts require careful consideration. (Semi)-Automated USMLE-MCQG systems can boost the efficiency and accuracy of medical education, but over-reliance could reduce direct student-educator interactions, and LLMs' hallucinations might impact students' learning and real-world practice. Caution is necessary to ensure these technologies support, not replace, medical educators. Future research should focus on these challenges to maximize positive contributions to medical education and society.</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Analysing the applicability of chatgpt, bard, and bing to generate reasoning-based multiple-choice questions in medical physiology. Mayank Agarwal, Priyanka Sharma, Ayan Goswami, Cureus. 6152023</p>
<p>Large language models for generating medical examinations: systematic review. Yaara Artsi, Vera Sorin, Eli Konen, Girish Benjamin S Glicksberg, Eyal Nadkarni, Klang, BMC Medical Education. 2413542024</p>
<p>Exploring the potential and limitations of chat generative pre-trained transformer (chatgpt) in generating board-style dermatology questions: a qualitative analysis. Ibraheim Ayub, Dathan Hamann, Matthew J Carsten R Hamann, Davis, Cureus. 8152023</p>
<p>Harnessing the potential of large language models in medical education: promise and pitfalls. Trista M Benítez, Yueyuan Xu, Donald Boudreau, Alfred Wei, Chieh Kow, Fernando Bello, Le Van Phuoc, Xiaofei Wang, Xiaodong Sun, Gilberto , Ka-Kit Leung, Yanyan Lan, Journal of the American Medical Informatics Association. 3132024</p>
<p>Passing is great: Can chatgpt conduct usmle exams?. Som Biswas, Annals of Biomedical Engineering. 5192023</p>
<p>Paniniqa: Enhancing patient education through interactive question answering. Pengshan Cai, Zonghai Yao, Fei Liu, Dakuo Wang, Meghan Reilly, Huixue Zhou, Lingxi Li, Yi Cao, Alok Kapoor, Adarsha Bajracharya, Transactions of the Association for Computational Linguistics. 112023</p>
<p>Automatic multiple choice question generation from text: A survey. Dhawaleswar Rao, Ch , Sujan Kumar Saha, IEEE Transactions on Learning Technologies. 1312018</p>
<p>Storyer: Automatic story evaluation via ranking, rating and reasoning. Hong Chen, Minh Duc, Hiroya Vo, Yusuke Takamura, Hideki Miyao, Nakayama, Journal of Natural Language Processing. 3012023a</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, arXiv:2304.051282023barXiv preprint</p>
<p>Toward subgraph-guided knowledge graph question generation with graph neural networks. Yu Chen, Lingfei Wu, Mohammed J Zaki, IEEE Transactions on Neural Networks and Learning Systems. 2023c</p>
<p>Chatgpt versus human in generating medical graduate exam multiple choice questions-a multinational prospective study (hong kong sar, singapore, ireland, and the united kingdom). Billy Ho, Hung Cheung, Gary Kui, Kai Lau, Gordon Tin, Chun Wong, Elaine Yuen Phin, Dhananjay Lee, Choon Kulkarni, Ruby Sheong Seow, Michael Tiong-Hong Wong, Co, PLoS One. 188e02906912023</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, arXiv:2305.019372023arXiv preprint</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, Tatsunori B Hashimoto, Advances in Neural Information Processing Systems. 202436</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023arXiv preprint</p>
<p>Using automatic item generation to create multiplechoice test items. Hollis Mark J Gierl, Simon R Lai, Turner, Medical education. 4682012</p>
<p>Chatgpt outperforms crowd workers for text-annotation tasks. Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli, Proceedings of the National Academy of Sciences. 12030e23050161202023</p>
<p>Shasha Guo, Lizi Liao, Cuiping Li, Tat-Seng Chua, arXiv:2402.18267A survey on neural question generation: Methods, applications, and prospects. 2024arXiv preprint</p>
<p>Dsm: Question generation over knowledge base via modeling diverse subgraphs with meta-learner. Shasha Guo, Jing Zhang, Yanling Wang, Qianyi Zhang, Cuiping Li, Hong Chen, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>An explorative assessment of chatgpt as an aid in medical education: use it with caution. Zhiyong Han, Fortunato Battaglia, Abinav Udaiyar, Allen Fooks, Stanley R Terlecky, Medical Teacher. 4652024</p>
<p>Proggen: Generating named entity recognition datasets step-by-step with self-reflexive large language models. Yuzhao Heng, Chunyuan Deng, Yitong Li, Yue Yu, Yinghao Li, Rongzhi Zhang, Chao Zhang, arXiv:2403.111032024arXiv preprint</p>
<p>Opportunities and risks of chatgpt in medicine, science, and academic publishing: a modern promethean dilemma. Croatian Medical Journal. 6411Jan Homolak. 2023</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, Peter Szolovits, Applied Sciences. 111464212021</p>
<p>Harrison's principles of internal medicine, 19e. Dennis Kasper, Anthony Fauci, Stephen Hauser, Dan Longo, Joseph Jameson, Loscalzo, Mcgraw-hill. 12015</p>
<p>Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation. Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, arXiv:2311.187022023arXiv preprint</p>
<p>Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, arXiv:2310.08491Prometheus: Inducing fine-grained evaluation capability in language models. 2023arXiv preprint</p>
<p>Prometheus 2: An open source language model specialized in evaluating other language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, arXiv:2405.015352024arXiv preprint</p>
<p>Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, Roberta Raileanu, arXiv:2310.06452Understanding the effects of rlhf on llm generalisation and diversity. 2023arXiv preprint</p>
<p>Advantages and pitfalls in utilizing artificial intelligence for crafting medical examinations: a medical education pilot study with gpt-4. Klang, Portugez, Gross, Brenner, Gilboa, Ortal, Ron, V Robinzon, Meiri, Segal, BMC Medical Education. 232023</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, arXiv:2302.145202023arXiv preprint</p>
<p>Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, Xian-Ling Mao, arXiv:2402.13764Criticbench: Evaluating large language models as critic. 2024arXiv preprint</p>
<p>Rlaif: Scaling reinforcement learning from human feedback with ai feedback. Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, Abhinav Rastogi, arXiv:2309.002672023arXiv preprint</p>
<p>Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, Pengfei Liu, arXiv:2310.05470Generative judge for evaluating alignment. 2023arXiv preprint</p>
<p>Leveraging large language models for nlg evaluation: A survey. Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Chongyang Tao, arXiv:2401.071032024arXiv preprint</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. Bowen Baker, Teddy LeeJanarXiv preprint</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, arXiv:2303.16634Gpteval: Nlg evaluation using gpt-4 with better human alignment. 2023arXiv preprint</p>
<p>Jason Yecheng, William Ma, Guanzhi Liang, De-An Wang, Osbert Huang, Dinesh Bastani, Yuke Jayaraman, Zhu, arXiv:2310.12931Linxi Fan, and Anima Anandkumar. 2023. Eureka: Human-level reward design via coding large language models. arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>Evaluating large language models as agents in the clinic. Nikita Mehandru, Brenda Y Miao, Eduardo Rodriguez Almaraz, Madhumita Sushil, Atul J Butte, Ahmed Alaa, npj Digital Medicine. 71842024</p>
<p>Synfac-edit: Synthetic imitation edit feedback for factual alignment in clinical summarization. Prakamya Mishra, Zonghai Yao, Parth Vashisht, Feiyun Ouyang, Beining Wang, Vidhi Dhaval Mody, Hong Yu, arXiv:2402.13919arXiv:2404.08700Simone Alghisi, and Giuseppe Riccardi. 2024. Is your llm outdated? benchmarking llms &amp; alignment algorithms for time-sensitive knowledge. 2024arXiv preprintSeyed Mahed Mousavi</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Zero-shot fact verification by claim generation. Liangming Pan, Wenhu Chen, Wenhan Xiong, Min-Yen Kan, William Yang, Wang , arXiv:2105.146822021arXiv preprint</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang, Wang , arXiv:2308.031882023arXiv preprint</p>
<p>A "u-shaped" curve: Appreciating how primary care residency intention relates to the cost of board preparation and examination. cureus. Sebastian R Diaz Philip, A Bucur, Vikrant Bhatnagar, 2019</p>
<p>Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha, arXiv:2402.07927A systematic survey of prompt engineering in large language models: Techniques and applications. 2024arXiv preprint</p>
<p>Col-bertv2: Effective and efficient retrieval via lightweight late interaction. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia ; Shern Chan, Angelica Chen, Kyunghyun Cho, Ethan Perez, arXiv:2112.01488arXiv:2204.14146Jérémy Scheurer. 2021. Jun. 2022arXiv preprintTraining language models with language feedback</p>
<p>Agentclinic: a multimodal agent benchmark to evaluate ai in simulated clinical environments. Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, Michael Moor, arXiv:2405.079602024arXiv preprint</p>
<p>Comprehensive review of the usmle. Peter V Scoles, Advances in Physiology Education. 3222008</p>
<p>The role of an open artificial intelligence platform in modern neurosurgical education: a preliminary study. Tan Umut, Gökberk Sevgi, Yücel Erol, Dogruel, Fikret Osman, Richard Shane Sönmez, Abuzer Tubbs, Güngor, Neurosurgical review. 461862023</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Nature. 62079722023</p>
<p>Qa4qg: using question answering to constrain multi-hop question generation. Dan Su, Peng Xu, Pascale Fung, ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2022</p>
<p>Joint learning of question answering and question generation. Yibo Sun, Duyu Tang, Nan Duan, Tao Qin, Shujie Liu, Zhao Yan, Ming Zhou, Yuanhua Lv, Wenpeng Yin, Xiaocheng Feng, IEEE Transactions on Knowledge and Data Engineering. 3252019</p>
<p>Vasilios Ediaroglou, Nikolaos Mantzou, Fabrice Duparc, and Maria Piagkou. 2023. The potential role of chatgpt and artificial intelligence in anatomy education: a conversation with chatgpt. Trifon Totlis, Konstantinos Natsis, Dimitrios Filos, Surgical and Radiologic Anatomy. 4510</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.179262023aarXiv preprint</p>
<p>Tianlu Wang, Ping Yu, Ellen Xiaoqing, Sean O Tan, Ramakanth 'brien, Jane Pasunuru, Olga Dwivedi-Yu, Luke Golovneva, Zettlemoyer, arXiv:2308.04592Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2023b. Shepherd: A critic for language model generation. arXiv preprint</p>
<p>How far can camels go? exploring the state of instruction tuning on open resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey Macmillan, Noah A Smith, Iz Beltagy, Advances in Neural Information Processing Systems. 2023c36</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, arXiv:2211.000532022arXiv preprint</p>
<p>Bailey &amp; Love's short practice of surgery. Norman Williams, P Ronan, O' Connell, 2008CRC press</p>
<p>Finegrained human feedback gives better rewards for language model training. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, Hannaneh Hajishirzi, Advances in Neural Information Processing Systems. 202436</p>
<p>Exploring questionspecific rewards for generating deep questions. Yuxi Xie, Liangming Pan, Dongzhe Wang, Min-Yen Kan, Yansong Feng, arXiv:2011.011022020arXiv preprint</p>
<p>Improving summarization with human edits. Zonghai Yao, Benjamin J Schloss, Sai P Selvaraj ; Yao, Zihao Zhang, Chaolong Tang, Xingyu Bian, Youxia Zhao, Zhichao Yang, Junda Wang, Huixue Zhou, arXiv:2310.05857arXiv:2410.01553Won Seok Jang, Feiyun Ouyang, et al. 2024. Medqa-cs: Benchmarking large language models clinical skills using an ai-sce framework. 2023arXiv preprint</p>
<p>Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen, arXiv:2310.07641Evaluating large language models at evaluating instruction following. 2023arXiv preprint</p>
<p>A comprehensive analysis of the effectiveness of large language models as automatic dialogue evaluators. Chen Zhang, Luis Fernando, D' Haro, Yiming Chen, Malu Zhang, Haizhou Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024a38</p>
<p>A review on question generation from natural language text. Ruqing Zhang, Jiafeng Guo, Lu Chen, Yixing Fan, Xueqi Cheng, ACM Transactions on Information Systems (TOIS). 4012021</p>
<p>Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu, arXiv:2401.02009Self-contrast: Better reflection through inconsistent solving perspectives. 2024barXiv preprint</p>
<p>Towards llm-based fact verification on news claims with a hierarchical step-by-step prompting method. Xuan Zhang, Wei Gao, arXiv:2310.003052023arXiv preprint</p>
<p>Siren's song in the ai ocean: a survey on hallucination in large language models. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, arXiv:2309.012192023arXiv preprint</p>
<p>A large-scale dataset of patient summaries for retrieval-based clinical decision support systems. Zhengyun Zhao, Qiao Jin, Fangyuan Chen, Tuorui Peng, Sheng Yu, 10.1038/s41597-023-02814-8Scientific Data. 1102023</p>
<p>Educational question generation of children storybooks via question type distribution learning and event-centric summarization. Zhenjie Zhao, Yufang Hou, Dakuo Wang, Mo Yu, Chengzhong Liu, Xiaojuan Ma, arXiv:2203.141872022arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li ; Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, 202327</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 202436</p>
<p>For Question (GPT-4), the common signs and symptoms of SAPHO syndrome are not Anterior chest and neck pain, tenderness in the sternoclavicular joint, subchondral bone marrow edema and enhancement, mild effusion of the sternoclavicular joint and surrounding soft-tissue edema. This is just the symptom for the giving case</p>
<p>Question (MCQG-SRefine) consists the symptoms of SAPHO and asks to choose the right diagnosis for the patient which is a common USMLE type question. Question (GPT-4) also is a good way to test whether the test-taker knows the signs/symptoms of SAPHO. But the context is too long, and the answers are a little bit obvious</p>
<p>Question (GPT-4) is the missing context for diagnosis, but Question (MCQG-SRefine)'s Distractor options are not that relevant and need correction. </p>
<p>In the Question (MCQG-SRefine) context, there is a hint about the signs and symptoms of the patient's disease. </p>            </div>
        </div>

    </div>
</body>
</html>
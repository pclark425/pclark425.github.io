<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9434 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9434</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9434</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-b0435af3063195e8ae880489e64ccde64e6d7563</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b0435af3063195e8ae880489e64ccde64e6d7563" target="_blank">Guiding Large Language Models via Directional Stimulus Prompting</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs, sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors.</p>
                <p><strong>Paper Abstract:</strong> We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs. Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced, instance-specific hints and clues to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output. We assess our method across summarization, dialogue response generation, and chain-of-thought reasoning tasks. Our experiments demonstrate that the framework consistently improves LLMs' (e.g., ChatGPT, Codex, InstructGPT) performance on these supervised tasks using minimal labeled data. Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances ChatGPT's performance by an impressive 41.4%, matching or surpassing some fully supervised start-of-the-art models. Additionally, the instance-specific chain-of-thought prompt generated by our approach improves InstructGPT's reasoning accuracy compared to human-crafted or automatically generated prompts. The code and data are publicly available at \url{https://github.com/Leezekun/Directional-Stimulus-Prompting}.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9434.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9434.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSP-summarization-keywords</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Directional Stimulus Prompting (keywords) for Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instance-specific keyword hints (directional stimulus) generated by a small policy model (Flan-T5) are inserted into prompts to steer a black-box LLM (ChatGPT) to produce summaries more aligned with reference summaries, with further optimization via RL using ROUGE as reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CNN/Daily Mail summarization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>News article abstractive summarization evaluated against human reference highlights using overlap and similarity metrics (ROUGE, BLEU, Meteor, BERTScore).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompting (3 demonstrations) where DSP adds an instance-specific 'directional stimulus' consisting of concatenated keywords (e.g., 'Keyword1; Keyword2; ...') generated by a supervised-fine-tuned Flan-T5 policy model; also evaluated with SFT+RL where the policy model is further fine-tuned with PPO (NLPO) using ROUGE-Avg as reward. Zero-shot variants were also tested (0-shot prompting during RL/inference). LLM decoding: sampling with temperature 0.7, top_p=1.0.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard few-shot prompting using the same 3 demonstrations but without the DSP keywords; zero-shot prompting without DSP was also compared.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>With DSP (policy model SFT on 1k-4k samples): ROUGE/BLEU/Meteor improved by ~1-2 absolute points on average (averaged across metrics) compared to standard prompting; reported statement: with 4,000 training samples the policy model improved ROUGE and BLEU by 4-13% (relative) in some reported results. After SFT+RL further gains observed (ROUGE-Avg used as RL reward). BERTScore improved less after RL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Standard prompting (3-shot) baseline: ChatGPT already strong; DSP w/ SFT produced +1-2 absolute points on overlap metrics; SFT+RL produced additional improvement over SFT. Zero-shot testing with DSP showed comparable relative improvement versus standard prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Absolute: ~+1–2 ROUGE/BLEU/METEOR points (typical); Relative (reported for 4k samples): +4–13% in ROUGE/BLEU in some comparisons. BERTScore improved less (smaller effect) after RL due to reward mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Instance-specific keywords act as fine-grained hints that guide the LLM toward including key points present in the reference summary. RL optimizes the policy model to discover stimulus that maximizes a downstream metric (ROUGE), but using a reward aligned to an overlap metric can bias improvements toward that metric and less so on semantic metrics (e.g., BERTScore). Supervised-only pseudo-stimulus may not be optimal; RL encourages exploration of model-preferred stimuli.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Policy model: Flan-T5-Large (~780M) supervised fine-tuned using textrank-extracted keywords appearing in references. Training subsets: 1k, 2k, 4k samples; evaluation set: 500 samples. SFT: 5 epochs, lr 2e-5. RL (NLPO/PPO): ~51k episodes (51200 steps), batch size 8, lr 2e-6, KL target 0.5, initial beta 0.005, rollout top-p=0.9; generate 4 LLM outputs per input, average reward. LLM decoding: temperature 0.7, max_new_tokens 180. Both few-shot (3 demos) and zero-shot settings evaluated; DSP robust across these variations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Guiding Large Language Models via Directional Stimulus Prompting', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9434.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9434.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSP-dialogue-ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Directional Stimulus Prompting (dialogue acts) for Task-Oriented Dialogue with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A policy model generates instance-specific dialogue acts (verbalized) that are inserted into prompts to guide ChatGPT to produce system responses that better satisfy task-oriented dialogue goals, with further improvement when policy is fine-tuned with RL using sentence-level BLEU as reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiWOZ (task-oriented dialogue response generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate system responses given dialogue context; evaluate using Inform (correct entity provided), Success (all requested attributes answered), BLEU, and Combined score = (Inform + Success) * 0.5 + BLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Standard prompting (same 3 demonstrations) vs DSP where the 'directional stimulus' is the dialogue act sequence (verbalized) generated by a Flan-T5 policy model; policy trained with SFT on 1% (80 dialogues) or 10% (800) data, and optionally further optimized with RL (NLPO/PPO) using sentence-level SacreBLEU as reward. Inference: generate 4 LLM outputs per input during RL, beam search with beam=5 at test time for policy outputs; LLM decoding sampling temperature 0.7.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard prompting (no dialogue-act stimulus); DSP w/ SFT (policy supervised only); DSP w/ SFT+RL (policy further optimized). Also compared against fully supervised TOD models trained on full dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MultiWOZ 2.0 (1% / 80 dialogues): Standard prompting Combined=68.4 (Inform=71.8, Success=44.1, BLEU=10.5). DSP w/ SFT Combined=82.8 (Inform=76.6, Success=66.5, BLEU=11.2). DSP w/ SFT+RL Combined=96.7 (Inform=90.9, Success=82.2, BLEU=10.2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to standard prompting, SFT+RL yields large gains: Inform +19.1 points (71.8 -> 90.9), Success +38.1 points (44.1 -> 82.2), Combined +28.3 points (68.4 -> 96.7) for 1% data. With 10% training data DSP w/ SFT+RL reaches Combined ~99.6.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Combined score relative increase ≈ +41.4% (96.7 vs 68.4) when using DSP w/ SFT+RL vs standard prompting on MultiWOZ 2.0 with 1% data; absolute Combined gain +28.3 points. Inform and Success increase substantially as above.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Providing explicit dialogue acts as instance-specific stimulus helps the LLM adhere to task ontology and business logic, producing responses that satisfy user constraints. Supervised SFT aligns policy to pseudo-labeled acts, but RL encourages discovery of stimulus that better matches the LLM's preferences and improves downstream metrics. However, because LLM responses can have different surface wording, corpus-level BLEU may not increase even when Inform/Success improve.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Datasets: MultiWOZ2.0 and 2.1. Policy model: Flan-T5-Large (~780M). Low-resource training: 1% (80 dialogues) and 10% (800 dialogues). SFT: up to 25 epochs for 80 dialogues; RL: 52k episodes (~51200 steps), batch size 8, lr 2e-6, initial KL coeff 0.01, KL_target 0.2, top-k sampling k=50 during RL, policy decoding at inference num_beams=5. Evaluation averaged over 3 inferences. Reported that corpus-level BLEU did not improve in some comparisons despite large gains in Inform/Success.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Guiding Large Language Models via Directional Stimulus Prompting', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9434.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9434.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSP-dialogue-Codex</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Directional Stimulus Prompting (dialogue acts) for Task-Oriented Dialogue with Codex</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same DSP setup as for ChatGPT but guiding Codex (code-davinci-002) via instance-specific dialogue acts to improve task-oriented dialogue metrics, showing large gains in Inform/Success and Combined score even with very small supervised data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiWOZ (task-oriented dialogue response generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above: generate system responses; evaluate Inform, Success, BLEU, Combined.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Standard prompting vs DSP with dialogue-act stimulus generated by Flan-T5 policy model (SFT and SFT+RL variants). RL uses sentence-level BLEU reward, top-k sampling with k=50 during RL, policy decoding beams at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard prompting (no stimulus) and DSP w/ SFT, DSP w/ SFT+RL; compared to fully supervised TOD models trained on full dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MultiWOZ 2.0 (1% / 80 dialogues): Standard prompting Combined=66.8 (Inform=76.7, Success=41.5, BLEU=7.7). DSP w/ SFT Combined=81.7 (Inform=74.9, Success=66.3, BLEU=11.1). DSP w/ SFT+RL Combined=93.3 (Inform=91.0, Success=76.0, BLEU=9.8).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to standard prompting, SFT+RL yields Inform +14.3 points (76.7 -> 91.0), Success +34.5 points (41.5 -> 76.0), Combined +26.5 points (66.8 -> 93.3) for 1% data.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Absolute Combined increase +26.5 points; relative Combined increase ≈ +39.7% (93.3 vs 66.8) when using DSP w/ SFT+RL vs standard prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Dialogue-act stimulus provides explicit intent/behavior constraints enabling Codex to produce responses satisfying slot/entity requirements; RL helps discover model-preferred stimuli. Same caveat: corpus-level BLEU may not reflect improved task success due to stylistic differences in generated responses.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same MultiWOZ experimental setup as ChatGPT entry; policy model Flan-T5-Large; SFT and NLPO (PPO-like) RL with 51200 total steps, batch size 8, lr 2e-6; evaluation averaged over 3 inferences; inference policy uses beam size 5; LLM decoding temperature 0.7. Low-resource settings: 80 and 800 dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Guiding Large Language Models via Directional Stimulus Prompting', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9434.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9434.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSP-CoT-InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Directional Stimulus Prompting (instance-specific CoT triggers) for Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A policy model (t5-base) generates instance-specific chain-of-thought trigger prompts (e.g., variations on 'Let's think step by step') that are inserted into zero-shot prompts to elicit CoT reasoning from InstructGPT, with RL refinement improving reasoning accuracy beyond human-designed and automatically discovered global prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot Chain-of-Thought reasoning (MultiArith, AQuA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic and math reasoning benchmark tasks where chain-of-thought triggers are used in zero-shot prompting to elicit multi-step reasoning; metric is reasoning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot chain-of-thought prompting with instance-specific trigger prompts produced by a policy model; compared against 14 human-crafted global CoT trigger prompts and the APE-automatically discovered prompt. Two policy training regimes: SFT (trained on prompt-instance pairs that produced correct CoT outputs with human-crafted prompts) and SFT+RL (policy further optimized via RL using binary correctness reward). LLM decoding sampling temperature 0.7.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>14 human-designed general CoT trigger prompts (same prompt for all instances), the APE automatically discovered general prompt, and DSP instance-specific prompts (SFT and SFT+RL).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MultiArith / AQuA accuracies: Example human prompts: 'Let's think step by step.' => 79.6 / 31.9. APE => 81.6 / 34.3. DSP w/ SFT => 75.2 / 35.8. DSP w/ SFT+RL => 84.0 / 38.6.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>DSP w/ SFT+RL outperforms the best human-designed / automatically discovered single prompts: on MultiArith DSP SFT+RL = 84.0 vs APE = 81.6 (+2.4 absolute); on AQuA DSP SFT+RL = 38.6 vs best human ~38.2 (+0.4 absolute). DSP w/ SFT alone was not peak-performing.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>MultiArith: +2.4 absolute accuracy over APE (84.0 vs 81.6). AQuA: +0.4 absolute over best task-specific prompt (~38.2 -> 38.6).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>LLMs are sensitive to the exact wording of CoT triggers; instance-specific triggers can better match the needs of particular problems, eliciting stronger reasoning. Supervised SFT provides a starting point, but RL encourages discovery of prompts that the LLM prefers and that lead to correct chain-of-thought outputs. General prompts vary widely in effectiveness across instances; instance-specific prompting mitigates this variability.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Datasets: MultiArith (600 examples split 300/50/250), AQuA (standard test). Policy model t5-base; SFT: 2 epochs, batch size 16, lr 2e-5; RL: 106k episodes (~51200 steps with iterations), batch size 16, lr 2e-6, KL target 0.5, initial KL coeff 0.001; reward = 1 for correct reasoning, 0 otherwise; 20 RL iterations. Compared 14 human-crafted prompts and APE prompt; LLM: text-davinci-002 (InstructGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Guiding Large Language Models via Directional Stimulus Prompting', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Large language models are human-level prompt engineers <em>(Rating: 2)</em></li>
                <li>Prompt programming for large language models: Beyond the few-shot paradigm <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9434",
    "paper_id": "paper-b0435af3063195e8ae880489e64ccde64e6d7563",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "DSP-summarization-keywords",
            "name_full": "Directional Stimulus Prompting (keywords) for Summarization",
            "brief_description": "Instance-specific keyword hints (directional stimulus) generated by a small policy model (Flan-T5) are inserted into prompts to steer a black-box LLM (ChatGPT) to produce summaries more aligned with reference summaries, with further optimization via RL using ROUGE as reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_size": null,
            "task_name": "CNN/Daily Mail summarization",
            "task_description": "News article abstractive summarization evaluated against human reference highlights using overlap and similarity metrics (ROUGE, BLEU, Meteor, BERTScore).",
            "presentation_format": "Few-shot prompting (3 demonstrations) where DSP adds an instance-specific 'directional stimulus' consisting of concatenated keywords (e.g., 'Keyword1; Keyword2; ...') generated by a supervised-fine-tuned Flan-T5 policy model; also evaluated with SFT+RL where the policy model is further fine-tuned with PPO (NLPO) using ROUGE-Avg as reward. Zero-shot variants were also tested (0-shot prompting during RL/inference). LLM decoding: sampling with temperature 0.7, top_p=1.0.",
            "comparison_format": "Standard few-shot prompting using the same 3 demonstrations but without the DSP keywords; zero-shot prompting without DSP was also compared.",
            "performance": "With DSP (policy model SFT on 1k-4k samples): ROUGE/BLEU/Meteor improved by ~1-2 absolute points on average (averaged across metrics) compared to standard prompting; reported statement: with 4,000 training samples the policy model improved ROUGE and BLEU by 4-13% (relative) in some reported results. After SFT+RL further gains observed (ROUGE-Avg used as RL reward). BERTScore improved less after RL.",
            "performance_comparison": "Standard prompting (3-shot) baseline: ChatGPT already strong; DSP w/ SFT produced +1-2 absolute points on overlap metrics; SFT+RL produced additional improvement over SFT. Zero-shot testing with DSP showed comparable relative improvement versus standard prompting.",
            "format_effect_size": "Absolute: ~+1–2 ROUGE/BLEU/METEOR points (typical); Relative (reported for 4k samples): +4–13% in ROUGE/BLEU in some comparisons. BERTScore improved less (smaller effect) after RL due to reward mismatch.",
            "explanation_or_hypothesis": "Instance-specific keywords act as fine-grained hints that guide the LLM toward including key points present in the reference summary. RL optimizes the policy model to discover stimulus that maximizes a downstream metric (ROUGE), but using a reward aligned to an overlap metric can bias improvements toward that metric and less so on semantic metrics (e.g., BERTScore). Supervised-only pseudo-stimulus may not be optimal; RL encourages exploration of model-preferred stimuli.",
            "null_or_negative_result": false,
            "experimental_details": "Policy model: Flan-T5-Large (~780M) supervised fine-tuned using textrank-extracted keywords appearing in references. Training subsets: 1k, 2k, 4k samples; evaluation set: 500 samples. SFT: 5 epochs, lr 2e-5. RL (NLPO/PPO): ~51k episodes (51200 steps), batch size 8, lr 2e-6, KL target 0.5, initial beta 0.005, rollout top-p=0.9; generate 4 LLM outputs per input, average reward. LLM decoding: temperature 0.7, max_new_tokens 180. Both few-shot (3 demos) and zero-shot settings evaluated; DSP robust across these variations.",
            "uuid": "e9434.0",
            "source_info": {
                "paper_title": "Guiding Large Language Models via Directional Stimulus Prompting",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "DSP-dialogue-ChatGPT",
            "name_full": "Directional Stimulus Prompting (dialogue acts) for Task-Oriented Dialogue with ChatGPT",
            "brief_description": "A policy model generates instance-specific dialogue acts (verbalized) that are inserted into prompts to guide ChatGPT to produce system responses that better satisfy task-oriented dialogue goals, with further improvement when policy is fine-tuned with RL using sentence-level BLEU as reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_size": null,
            "task_name": "MultiWOZ (task-oriented dialogue response generation)",
            "task_description": "Generate system responses given dialogue context; evaluate using Inform (correct entity provided), Success (all requested attributes answered), BLEU, and Combined score = (Inform + Success) * 0.5 + BLEU.",
            "presentation_format": "Standard prompting (same 3 demonstrations) vs DSP where the 'directional stimulus' is the dialogue act sequence (verbalized) generated by a Flan-T5 policy model; policy trained with SFT on 1% (80 dialogues) or 10% (800) data, and optionally further optimized with RL (NLPO/PPO) using sentence-level SacreBLEU as reward. Inference: generate 4 LLM outputs per input during RL, beam search with beam=5 at test time for policy outputs; LLM decoding sampling temperature 0.7.",
            "comparison_format": "Standard prompting (no dialogue-act stimulus); DSP w/ SFT (policy supervised only); DSP w/ SFT+RL (policy further optimized). Also compared against fully supervised TOD models trained on full dataset.",
            "performance": "MultiWOZ 2.0 (1% / 80 dialogues): Standard prompting Combined=68.4 (Inform=71.8, Success=44.1, BLEU=10.5). DSP w/ SFT Combined=82.8 (Inform=76.6, Success=66.5, BLEU=11.2). DSP w/ SFT+RL Combined=96.7 (Inform=90.9, Success=82.2, BLEU=10.2).",
            "performance_comparison": "Compared to standard prompting, SFT+RL yields large gains: Inform +19.1 points (71.8 -&gt; 90.9), Success +38.1 points (44.1 -&gt; 82.2), Combined +28.3 points (68.4 -&gt; 96.7) for 1% data. With 10% training data DSP w/ SFT+RL reaches Combined ~99.6.",
            "format_effect_size": "Combined score relative increase ≈ +41.4% (96.7 vs 68.4) when using DSP w/ SFT+RL vs standard prompting on MultiWOZ 2.0 with 1% data; absolute Combined gain +28.3 points. Inform and Success increase substantially as above.",
            "explanation_or_hypothesis": "Providing explicit dialogue acts as instance-specific stimulus helps the LLM adhere to task ontology and business logic, producing responses that satisfy user constraints. Supervised SFT aligns policy to pseudo-labeled acts, but RL encourages discovery of stimulus that better matches the LLM's preferences and improves downstream metrics. However, because LLM responses can have different surface wording, corpus-level BLEU may not increase even when Inform/Success improve.",
            "null_or_negative_result": true,
            "experimental_details": "Datasets: MultiWOZ2.0 and 2.1. Policy model: Flan-T5-Large (~780M). Low-resource training: 1% (80 dialogues) and 10% (800 dialogues). SFT: up to 25 epochs for 80 dialogues; RL: 52k episodes (~51200 steps), batch size 8, lr 2e-6, initial KL coeff 0.01, KL_target 0.2, top-k sampling k=50 during RL, policy decoding at inference num_beams=5. Evaluation averaged over 3 inferences. Reported that corpus-level BLEU did not improve in some comparisons despite large gains in Inform/Success.",
            "uuid": "e9434.1",
            "source_info": {
                "paper_title": "Guiding Large Language Models via Directional Stimulus Prompting",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "DSP-dialogue-Codex",
            "name_full": "Directional Stimulus Prompting (dialogue acts) for Task-Oriented Dialogue with Codex",
            "brief_description": "Same DSP setup as for ChatGPT but guiding Codex (code-davinci-002) via instance-specific dialogue acts to improve task-oriented dialogue metrics, showing large gains in Inform/Success and Combined score even with very small supervised data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Codex (code-davinci-002)",
            "model_size": null,
            "task_name": "MultiWOZ (task-oriented dialogue response generation)",
            "task_description": "As above: generate system responses; evaluate Inform, Success, BLEU, Combined.",
            "presentation_format": "Standard prompting vs DSP with dialogue-act stimulus generated by Flan-T5 policy model (SFT and SFT+RL variants). RL uses sentence-level BLEU reward, top-k sampling with k=50 during RL, policy decoding beams at inference.",
            "comparison_format": "Standard prompting (no stimulus) and DSP w/ SFT, DSP w/ SFT+RL; compared to fully supervised TOD models trained on full dataset.",
            "performance": "MultiWOZ 2.0 (1% / 80 dialogues): Standard prompting Combined=66.8 (Inform=76.7, Success=41.5, BLEU=7.7). DSP w/ SFT Combined=81.7 (Inform=74.9, Success=66.3, BLEU=11.1). DSP w/ SFT+RL Combined=93.3 (Inform=91.0, Success=76.0, BLEU=9.8).",
            "performance_comparison": "Compared to standard prompting, SFT+RL yields Inform +14.3 points (76.7 -&gt; 91.0), Success +34.5 points (41.5 -&gt; 76.0), Combined +26.5 points (66.8 -&gt; 93.3) for 1% data.",
            "format_effect_size": "Absolute Combined increase +26.5 points; relative Combined increase ≈ +39.7% (93.3 vs 66.8) when using DSP w/ SFT+RL vs standard prompting.",
            "explanation_or_hypothesis": "Dialogue-act stimulus provides explicit intent/behavior constraints enabling Codex to produce responses satisfying slot/entity requirements; RL helps discover model-preferred stimuli. Same caveat: corpus-level BLEU may not reflect improved task success due to stylistic differences in generated responses.",
            "null_or_negative_result": true,
            "experimental_details": "Same MultiWOZ experimental setup as ChatGPT entry; policy model Flan-T5-Large; SFT and NLPO (PPO-like) RL with 51200 total steps, batch size 8, lr 2e-6; evaluation averaged over 3 inferences; inference policy uses beam size 5; LLM decoding temperature 0.7. Low-resource settings: 80 and 800 dialogues.",
            "uuid": "e9434.2",
            "source_info": {
                "paper_title": "Guiding Large Language Models via Directional Stimulus Prompting",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "DSP-CoT-InstructGPT",
            "name_full": "Directional Stimulus Prompting (instance-specific CoT triggers) for Chain-of-Thought",
            "brief_description": "A policy model (t5-base) generates instance-specific chain-of-thought trigger prompts (e.g., variations on 'Let's think step by step') that are inserted into zero-shot prompts to elicit CoT reasoning from InstructGPT, with RL refinement improving reasoning accuracy beyond human-designed and automatically discovered global prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_size": null,
            "task_name": "Zero-shot Chain-of-Thought reasoning (MultiArith, AQuA)",
            "task_description": "Arithmetic and math reasoning benchmark tasks where chain-of-thought triggers are used in zero-shot prompting to elicit multi-step reasoning; metric is reasoning accuracy.",
            "presentation_format": "Zero-shot chain-of-thought prompting with instance-specific trigger prompts produced by a policy model; compared against 14 human-crafted global CoT trigger prompts and the APE-automatically discovered prompt. Two policy training regimes: SFT (trained on prompt-instance pairs that produced correct CoT outputs with human-crafted prompts) and SFT+RL (policy further optimized via RL using binary correctness reward). LLM decoding sampling temperature 0.7.",
            "comparison_format": "14 human-designed general CoT trigger prompts (same prompt for all instances), the APE automatically discovered general prompt, and DSP instance-specific prompts (SFT and SFT+RL).",
            "performance": "MultiArith / AQuA accuracies: Example human prompts: 'Let's think step by step.' =&gt; 79.6 / 31.9. APE =&gt; 81.6 / 34.3. DSP w/ SFT =&gt; 75.2 / 35.8. DSP w/ SFT+RL =&gt; 84.0 / 38.6.",
            "performance_comparison": "DSP w/ SFT+RL outperforms the best human-designed / automatically discovered single prompts: on MultiArith DSP SFT+RL = 84.0 vs APE = 81.6 (+2.4 absolute); on AQuA DSP SFT+RL = 38.6 vs best human ~38.2 (+0.4 absolute). DSP w/ SFT alone was not peak-performing.",
            "format_effect_size": "MultiArith: +2.4 absolute accuracy over APE (84.0 vs 81.6). AQuA: +0.4 absolute over best task-specific prompt (~38.2 -&gt; 38.6).",
            "explanation_or_hypothesis": "LLMs are sensitive to the exact wording of CoT triggers; instance-specific triggers can better match the needs of particular problems, eliciting stronger reasoning. Supervised SFT provides a starting point, but RL encourages discovery of prompts that the LLM prefers and that lead to correct chain-of-thought outputs. General prompts vary widely in effectiveness across instances; instance-specific prompting mitigates this variability.",
            "null_or_negative_result": false,
            "experimental_details": "Datasets: MultiArith (600 examples split 300/50/250), AQuA (standard test). Policy model t5-base; SFT: 2 epochs, batch size 16, lr 2e-5; RL: 106k episodes (~51200 steps with iterations), batch size 16, lr 2e-6, KL target 0.5, initial KL coeff 0.001; reward = 1 for correct reasoning, 0 otherwise; 20 RL iterations. Compared 14 human-crafted prompts and APE prompt; LLM: text-davinci-002 (InstructGPT).",
            "uuid": "e9434.3",
            "source_info": {
                "paper_title": "Guiding Large Language Models via Directional Stimulus Prompting",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Large language models are human-level prompt engineers",
            "rating": 2
        },
        {
            "paper_title": "Prompt programming for large language models: Beyond the few-shot paradigm",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        }
    ],
    "cost": 0.0175245,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Guiding Large Language Models via Directional Stimulus Prompting</h1>
<p>Zekun $\mathbf{L i}^{1 *}$, Baolin Peng ${ }^{2}$, Pengcheng $\mathbf{H e}^{2}$, Michel Galley ${ }^{2}$, Jianfeng Gao ${ }^{2 \dagger}$, Xifeng Yan ${ }^{1 \dagger}$<br>University of California, Santa Barbara ${ }^{1}$<br>Microsoft ${ }^{2}$<br>{zekunli, xyan}@cs.ucsb.edu<br>{bapeng, penhe, mgalley,jfgao}@microsoft.com</p>
<h4>Abstract</h4>
<p>We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs. Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced, instance-specific hints and clues to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output. We assess our method across summarization, dialogue response generation, and chain-of-thought reasoning tasks. Our experiments demonstrate that the framework consistently improves LLMs' (e.g., ChatGPT, Codex, InstructGPT) performance on these supervised tasks using minimal labeled data. Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances ChatGPT's performance by an impressive $41.4 \%$, matching or surpassing some fully supervised start-of-the-art models. Additionally, the instance-specific chain-of-thought prompt generated by our approach improves InstructGPT's reasoning accuracy compared to human-crafted or automatically generated prompts. The code and data are publicly available. ${ }^{3}$</p>
<h2>1 Introduction</h2>
<p>In recent years, a new paradigm has emerged in natural language processing (NLP) with the rise of large language models (LLMs) such as Codex [9], InstructGPT, ChatGPT [46], GPT-4 [45], PaLM [10], and others. These models exhibit emergent abilities [68] such as strong in-context learning and few-shot prompting capabilities, which were not present in previous "smaller" language models (LMs) like BERT [14], RoBERTa [37], GPT-2 [52], and T5 [53]. This shift in paradigm has led to remarkable advancements in NLP, with LLMs demonstrating impressive general-purpose power. However, due to commercial considerations and the risk of misuse, most LLMs do not publicly release their parameters and only allow users to access them through black-box APIs. While there also exists open-sourced LLMs, fine-tuning them for specific tasks or use cases can be computationally inefficient. In this scenario, the standard approach for utilizing LLMs to perform diverse tasks is crafting task-specific text prompts to query LLMs through black-box APIs. While LLMs have</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparison of our Directional Stimulus Prompting and the standard prompting method using LLMs such as ChatGPT for the summarization task. DSP utilizes directional stimulus/hints (highlighted in orange), which are keywords in this case, to provide instance-specific guidance to LLMs in generating summaries (highlighted in blue) that better align with the desired reference summary with higher ROUGE scores or other measures like human preferences.</p>
<p>demonstrated considerable performance on a wide range of language tasks, they still struggle to generate outputs that fully align with desired behaviors and directions on some specific tasks and use cases [16, 4].</p>
<p>Since directly optimizing LLMs for specific tasks is either inefficient or infeasible for most users and developers, researchers resort to optimizing prompts instead. Prompt engineering approaches, which involve manually or automatically designing optimal task-specific natural language instructions and selecting appropriate training samples for demonstration in the prompt, have been the focus of many researchers [6, 55, 79, 39]. Despite these efforts, effectively steering LLMs to generate desired results and effectively exploiting labeled data remains a significant challenge.</p>
<p>To address the challenge, we propose a novel framework called <strong>Directional Stimulus Prompting (DSP)</strong>. This framework introduces a new component called the "<em>directional stimulus</em>" into the prompt to provide nuanced, instance-specific guidance and control over LLMs. Specifically, the directional stimulus prompt acts as "hints" and "clues" for the input query to guide LLMs toward the desired output. Notably, this differs from the methods that augment LLMs with additional knowledge retrieved from external sources [25, 60], as the directional stimulus prompt is generated solely based on the input query in our framework. Figure 1 compares our proposed prompting approach, DSP, with standard prompting for the summarization task. Our approach incorporates keywords in the prompt as the directional stimulus prompt to hint at key points the desired summary should cover. By providing this instance-specific guidance through directional stimulus prompt, LLMs can generate outputs that more closely align with the desired reference summary.</p>
<p>We utilize a relatively small and tunable LM (e.g., T5), as the policy model to generate the directional stimulus prompt for each input query. This approach enables us to sidestep the direct optimization of black-box LLMs by optimizing the small tunable policy model instead. We train the policy model through supervised fine-tuning (SFT) using a few collected labeled data. After supervised fine-tuning, we further optimize the policy model to explore better directional stimulus prompts with reinforcement learning (RL). During RL training, we aim to maximize the reward defined as downstream performance measures or any other measures of the LLM's output conditioned on the stimulus generated by the policy model.</p>
<p>Figure 2 provides the overview of our framework, using the summarization task as an illustrative example. We employ a compact, tunable policy model to generate the directional stimulus prompt, which specifies keywords that should be included in the LLM-generated summaries. The policy</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of our proposed framework DSP, where we learn a small tunable policy model to generate the directional stimulus (keywords in this case) that provide input-specific guidance for the LLM toward the desired target. The policy model can be trained with SFT and/or RL, where the reward is defined as the downstream task performance measure, such as the ROUGE score for the summarization task, or other alignment measures like human preferences.</p>
<p>model can be trained with SFT and RL, where the reward is typically defined as the downstream task performance measure, such as the ROUGE score for the summarization task, or other alignment measures like human preferences.</p>
<p>Our framework can be flexibly adapted to a wide range of LLMs and tasks by choosing the appropriate directional stimulus and associated rewards. We conducted experiments on summarization, dialogue response generation, and chain-of-thought reasoning tasks to evaluate the effectiveness of our framework. Our results demonstrate that our DSP approach can effectively guide ChatGPT toward the desired targets with a small collection of labeled data. Specifically, we conduct experiments with the black-box LLMs: ChatGPT, Codex, and InstructGPT. For the policy model, we employ a 750M Flan-T5-Large [53, 11] and 220M T5-Base. For the summarization task, we use keywords as the directional stimulus, which hints at key points that the desired summary should include. Despite ChatGPT's already considerable performance, the policy model T5 trained with only 4,000 samples from the CNN/Daily Mail dataset [43] improved the ROUGE and BLEU scores by 4-13%. For the dialogue response generation task, we train the policy model to generate dialogue acts that indicate the underlying intentions behind target responses on dialogues from MultiWOZ dataset [7]. Guided by the policy model trained with only 80 dialogues, ChatGPT's performance improved by up to 41.4% in combined scores, achieving comparable or even better performance than some state-of-the-art models trained on the full dataset with 8,438 dialogues. For the chain-of-thought reasoning, we train the policy model to generate a trigger prompt for each input query to trigger the LLM chain-of-thought reasoning, achieving better performance than the hand-crafted and automatically generated prompts.</p>
<h2>2 Directional stimulus prompting</h2>
<p>For a downstream task, there is an input space <em>X</em>, a data distribution <em>D</em> over <em>X</em>, and an output space <em>Y</em>. Due to the strong in-context learning and few-shot prompting abilities, LLMs can perform diverse tasks and generate the output <em>y</em> by including instructions that describe the task, a few demonstration examples, and the input query <em>x</em> in the prompt [6]. However, such prompts cannot always steer LLMs toward desired outputs, especially when it comes to fine-grained instance-specific desired behaviors. For instance, in the case of the summarization task, the input <em>x</em> is an article, and the output <em>y</em> is the corresponding summary. Different summarizers have distinct styles and emphasize different aspects of an article [16]. In this case, it may not be enough to effectively steer LLMs toward generating summaries that closely match reference summaries relying solely on task-specific instructions or demonstration examples to describe such nuanced differences for each sample.</p>
<p>To this end, our Directional Stimulus Prompting (DSP) approach introduces a small piece of discrete tokens <em>z</em> named "<em>directional stimulus</em>" into the prompt, which acts as hints and clues to provide LLMs with fine-grained guidance toward the desired direction. For example, for the summarization task, the directional stimulus <em>z</em> might consist of keywords that should be included in the desired summary. To generate this stimulus for each input query, we use a small tunable policy language model, <em>p</em><sub>POL</sub>(<em>z</em>|<em>x</em>). We then use this generated stimulus, <em>z</em>, along with the original input, <em>x</em>, to construct the prompt that steers the LLM toward generating its output, <em>p</em><sub>LLM</sub>(<em>y</em>|<em>x</em>, <em>z</em>), through black-box</p>
<p>box API calls. It's important to note that the parameters of the LLM, $p_{\text {LLM }}$, are not accessible or tunable. Overall, when using the LLM with DSP to perform a downstream task, the output is obtained via $\boldsymbol{y} \sim p_{\mathrm{LLM}}(\cdot \mid \boldsymbol{x}, \boldsymbol{z}), \boldsymbol{z} \sim p_{\mathrm{POL}}(\cdot \mid \boldsymbol{x})$.</p>
<h1>2.1 Supervised fine-tuning</h1>
<p>To train the policy model that generates directional stimulus for LLMs, we first perform supervised fine-tuning (SFT) on a pre-trained LM (e.g., T5, GPT-2, etc) on a small collection of labeled data. To collect the data, we could heuristically select or annotate the "pseudo-stimulus" $\boldsymbol{z}^{<em>}$ for each input query $\boldsymbol{x}$ and target output $\boldsymbol{y}$ pair based on the downstream task. For example, for the summarization task, we use keywords that the reference summary includes as pseudo-stimulus, while for the dialogue response generation task, we use dialogue acts that indicate the underlying meaning of the desired system response (see Section 3 for details). The resulting dataset $\mathcal{D}^{\prime}=\left{\left(\boldsymbol{x}, \boldsymbol{z}^{</em>}\right)\right}$ consists of input-stimulus pairs. We then fine-tune the policy model by maximizing the log-likelihood:</p>
<p>$$
\mathcal{L}<em _left_boldsymbol_x="\left(\boldsymbol{x">{\mathrm{SFT}}=-\mathbb{E}</em>^{}, \boldsymbol{z<em>}\right) \sim \mathcal{D}^{\prime}} \log p_{\mathrm{POL}}\left(\boldsymbol{z}^{</em>} \mid \boldsymbol{x}\right)
$$</p>
<p>Supervised fine-tuning can provide a good initial point for the policy model. However, it is important to note that the heuristically selected or annotated pseudo-stimulus may not always be optimal, and the supervised fine-tuned policy model may not generate the most preferred directional stimulus for the LLMs toward the desired outputs. To overcome this limitation, we can also incorporate reinforcement learning (RL) to further fine-tune the policy model. By directly optimizing the LLM's output toward desired targets, RL training enables the policy model to explore and generate more effective directional stimulus.</p>
<h3>2.2 Reinforcement learning</h3>
<p>Optimization objective Our goal is to steer the LLM's generation toward the desired target by maximizing an alignment measure $\mathcal{R}$, which can take various forms such as downstream task performance measures (e.g., ROUGE score for summarization), human preferences, or other customized measures. Mathematically, we aim to maximize the below objective:</p>
<p>$$
\mathbb{E}<em _mathrm_POL="\mathrm{POL">{\boldsymbol{x} \sim \mathcal{D}, \boldsymbol{z} \sim p</em>)]
$$}}(\cdot \mid \boldsymbol{x}), \boldsymbol{y} \sim p_{\mathrm{LLM}}(\cdot \mid \boldsymbol{x}, \boldsymbol{z})}[\mathcal{R}(\boldsymbol{x}, \boldsymbol{y</p>
<p>Since the parameters of the black-box LLM are not accessible or tunable, we resort to optimizing the policy model to generate the directional stimulus that guides the LLMs' generation toward maximizing the objective. To achieve that, we define another measure $\mathcal{R}_{\mathrm{LLM}}$ that captures how well the LLM performs when conditioned on a given stimulus $\boldsymbol{z}$ :</p>
<p>$$
\mathcal{R}<em _mathrm_LLM="\mathrm{LLM">{\mathrm{LLM}}(\boldsymbol{x}, \boldsymbol{z})=\mathcal{R}(\boldsymbol{x}, \boldsymbol{y}), \boldsymbol{y} \sim p</em>)
$$}}(\cdot \mid \boldsymbol{x}, \boldsymbol{z</p>
<p>This allows us to cast the original objective of maximizing $\mathcal{R}$ into optimizing the policy model to generate stimulus that maximizes $\mathcal{R}_{\mathrm{LLM}}$. By doing so, the LLM is effectively used as an evaluation function to guide the policy model toward generating more effective directional stimulus. Thus, the optimization objective for LLMs in Equation 2 is equal to the optimization objective for the policy model:</p>
<p>$$
\max <em _mathrm_POL="\mathrm{POL">{p</em>}}} \mathbb{E<em _mathrm_POL="\mathrm{POL">{\boldsymbol{x} \sim \mathcal{D}, \boldsymbol{z} \sim p</em>)\right]
$$}}(\cdot \mid \boldsymbol{x})}\left[\mathcal{R}_{\mathrm{LLM}}(\boldsymbol{x}, \boldsymbol{z</p>
<p>RL formulation However, the above optimization is intractable for the policy model. To address the issue, we formulate the policy model optimization as an RL problem and employ proximal policy optimization (PPO) [59]. We use the policy model to initialize a policy network $\pi_{0}=p_{\text {POL }}$ and then update $\pi$ using PPO. The process that the policy model generates a sequence of tokens as stimulus $\boldsymbol{z}$ can be seen as a Markov decision process (MDP) $\langle\mathcal{S}, \mathcal{A}, r, \mathcal{P}\rangle$ with a state space $\mathcal{S}$, action space $\mathcal{A}$, reward function $r$, and state-transition probability $\mathcal{P}$. In each time step $t$ of an episode, the agent selects an action (token) from the vocabulary $\mathcal{V}$ according to the distribution of the current policy network $\pi\left(\boldsymbol{z} \mid \boldsymbol{x}, \boldsymbol{z}_{&lt;t}\right)$. The episode ends when an end-of-sequence token is selected, and the stimulus $\boldsymbol{z}$ is generated. We can fine-tune the policy network $\pi$ by optimizing the reward $r$ :</p>
<p>$$
\mathbb{E}<em _boldsymbol_x="\boldsymbol{x">{\pi}[r]=\mathbb{E}</em>)]
$$} \sim \mathcal{D}, \boldsymbol{z} \sim \pi(\cdot \mid \boldsymbol{x})}[r(\boldsymbol{x}, \boldsymbol{z</p>
<p>Reward function Recall that our goal is to maximize the objective in Equation 4, which can be used as the reward $r$. To keep the policy network $\pi$ from moving too far from the initial policy model $p_{\text {POL }}$,</p>
<p>we also add a KL-divergence penalty reward. Therefore, the final reward becomes:</p>
<p>$$
r(\boldsymbol{x}, \boldsymbol{z})=\mathcal{R}<em _mathrm_POL="\mathrm{POL">{\mathrm{LLM}}(\boldsymbol{x}, \boldsymbol{z})-\beta \log \frac{\pi(\boldsymbol{z} \mid \boldsymbol{x})}{p</em>
$$}}(\boldsymbol{z} \mid \boldsymbol{x})</p>
<p>Following [80, 54], we dynamically adapt the coefficient $\beta$ during training:</p>
<p>$$
\begin{aligned}
\boldsymbol{e}<em t="t">{t} &amp; =\operatorname{clip}\left(\frac{\mathrm{KL}\left(\pi</em>}, p_{\mathrm{POL}}\right)-\mathrm{KL<em _target="{target" _text="\text">{\text {target }}}{\mathrm{KL}</em>,-0.2,0.2\right) \
\beta_{t+1} &amp; =\beta_{t}\left(1+K_{\beta} \boldsymbol{e}_{t}\right)
\end{aligned}
$$}}</p>
<p>Implementation To optimize the policy network $\pi$, we use the NLPO version of PPO from [54], which is specifically designed for language generators. To address the issue of large action spaces in PPO, NLPO learns to mask out less relevant tokens in the vocabulary using top- $p$ sampling. This technique restricts the action space to the smallest set of tokens whose cumulative probability is greater than the given probability parameter $p$, which we set to 0.9 in our experiments. Both the policy network $\pi$ and value network are initialized from the supervised fine-tuned policy model $p_{\text {POL }}$, with the final layer of the value network randomly initialized to output a scalar value using a regression head.</p>
<h1>3 Experiments</h1>
<p>Our proposed framework DSP can be flexibly applied to various types of LMs and generation tasks. In this work, we focus on the summarization, dialogue response generation, and automatic prompt generation tasks. We mainly use pre-trained T5 or Flan-T5 [53, 11] to initialize the policy model and evaluate the OpenAI's ChatGPT (gpt-3.5-turbo), Codex (code-davinci-002), and InstructGPT (text-davinci-002). Our experiments aim to assess the effectiveness of our approach in guiding the generation of black-box LLMs toward desired outputs.</p>
<h3>3.1 Summarization</h3>
<p>Recent studies [16, 75, 4] have shown that LLMs, such as GPT-3, InstructGPT, and ChatGPT, are capable of generating high-quality summaries with zero- or few-shot prompting. However, their reference-based evaluation benchmark performances, such as ROUGE scores, still lag behind finetuned methods, indicating that the generated summaries may not completely match the style and emphasis of the reference summaries. In our experiments, we seek to guide LLMs to generate summaries that more closely align with the reference summaries by providing keywords that should be mentioned in the desired summaries as hints. We evaluate the effectiveness using metrics that compare the generated summaries against reference summaries. Notably, other desired directions, such as better alignment with human preferences, can also be pursued.</p>
<p>Dataset and evaluation We conduct our experiments on the CNN/Daily Mail dataset, a widelyused news summarization benchmark. To keep the cost of API usage low, we train on a subset of 1,000, 2,000, and 4,000 article-summary pairs from the total 287,113 samples in the training set. For evaluation, we randomly select 500 samples, following previous work [16, 65], which has been proven to provide sufficient statistical power [8]. We use the overlap-based metrics, including ROUGE [33], BLEU [47], and Meteor [3], and the similarity-based metric, BERTScore [74], to compare the generated summaries with the references. The reported evaluation scores are averaged over three inferences of ChatGPT for each query, using a temperature of 0.7 and top_p of 1.0. We use the same three demonstration examples in the prompt for standard prompting and add keywords as directional stimulus in the prompt for our approach, DSP. The exact prompts used in our experiments are provided in the Appendix.</p>
<p>Supervised fine-tuning details We use keywords as the pseudo-stimulus to train the policy model with supervised fine-tuning as discussed in Section 2.1. To collect the data, we employ textrank [41, 5] to automatically extract the keywords from the article and summary and only keep those that appear in the reference summary. As a result, we obtain a list of extracted keywords for each article-summary pair in the dataset. To convert them into a sentence that serves as the stimulus, we concatenate them using a split token ";", resulting in the stimulus formated as "[Keyword1]; [Keyword2]; ... ; [KeywordN].". We use the constructed article-stimulus pairs to train the policy model via supervised</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance comparison of ChatGPT with standard prompting and DSP trained with SFT and SFT+RL, using varying numbers of training samples from the CNN/Daily Mail dataset.</p>
<p>fine-tuning. The input format for training is "<em>Extract the keywords: [Article]</em>", while the output is the target stimulus consisting of keywords. The policy model was trained for 5 epochs with a 2 × 10<sup>−5</sup> learning rate.</p>
<p><strong>RL training details</strong> As we aim to guide ChatGPT in generating summaries that more closely match the reference summaries, we adopt the automatic reference-based metric scores as the alignment measure reward. Specifically, we calculate the ROUGE-Avg score between the generated summaries and the reference summaries as the reward, with a rescaling coefficient of 10. We experimentally found that other automatic evaluation metrics, such as BLEU and Meteor, perform similarly. To reduce variance, we generate four outputs per input query using ChatGPT with a temperature of 0.7 and compute the average reward. Additionally, we assign a step-wise reward, which we found could improve the efficiency and stability of the training process. Specifically, the policy model generates a sequence of keywords in each episode, during which we assign a reward of 1 if a keyword appears in the reference summary and a penalty reward of -0.2 is given otherwise. We train the policy network for 51k episodes, with 5 epochs per batch, a batch size of 8, and a learning rate of 2 × 10<sup>−6</sup>. The KL<sub>target</sub> and β<sub>0</sub> in Equation 7 are set to 0.5 and 0.005, respectively.</p>
<p><strong>Results</strong> We evaluate the performance of ChatGPT with standard prompting and our approach DSP trained with SFT or SFT and then RL (SFT+RL) on varying sizes of training data and present the results in Figure 3. As can be seen, all the evaluation scores improve with our proposed DSP compared with standard prompting. Specifically, the supervised fine-tuned policy model generates the stimulus that effectively guides ChatGPT to generate summaries that closely align with the reference summaries, leading to improved benchmark performance. Furthermore, the additional fine-tuning of the policy model with RL results in further performance improvement, indicating the effectiveness of RL in exploring better directional stimulus that maximizes the reward. As the size of the training data increases, the performance improvement becomes more significant. Despite using a small collection of only 1,000 to 4,000 samples to keep API usage costs low, our DSP approach still consistently enhances ChatGPT's ROUGE, BLEU, and Meteor scores by 1-2 points, even though</p>
<p>ChatGPT has already achieved considerable performance. However, due to the discrepancy between the semantic-based metric BERTScore and the overlap-based metric ROUGE, which are used as the reward, the improvement in BERTScore after RL training may be relatively less significant. Figure 4 presents the change of training rewards and ROUGE-1 score on the validation set during the training process on 1,000 samples. We can see that the performance is closely related to the training rewards, and the training is relatively stable using the NLPO algorithm.</p>
<h1>3.2 Dialogue response generation</h1>
<p>In recent years, there has been a rise in LLM-based chatbots such as ChatGPT ${ }^{4}$ and Sparrow ${ }^{5}$. These chatbots are typically targeted at open-domain conversations to engage with users on a wide range of topics without a specific goal in mind. However, these chatbots still face challenges in handling task-oriented dialogues where they need to assist users in completing specific goals or tasks, such as making reservations or ordering food [4, 22]. Unlike open-domain conversations, task-oriented dialogues often require the chatbot to follow task-specific business logic and respond based on reliable information from API calls or database queries. To address this limitation, we train a small policy model to learn the underlying dialogue policy from the training data and thus guide the LLMs in generating reliable system responses that assist users in completing tasks.
Dataset and evaluation We conduct experiments on the popular task-oriented dialogue dataset MultiWOZ [7], including both the MultiWOZ2.0 (the original version) and MultiWOZ2.1 version [15]. The dataset provides annotations for user utterances, dialogue acts, and system responses for each dialogue turn. The goal is to generate the system response given the history dialogue context as input. We utilize the dialogue act, which represents the communicative intention of the target system response, as the pseudo-stimulus for our experiment. There are 8,438 dialogues in the training set. We only use $1 \%$ ( 80 dialogues) and $10 \%$ ( 800 dialogues) to train the policy model and evaluate the performance on the full validation and test set, which contains 1,000 dialogues. We use the standard evaluation metrics: Inform, which measures the rate that the appropriate entity that satisfies the user's requirements is provided; Success, which measures the rate that all requested attributes are answered; BLEU: the corpus-level BLEU score with reference responses; and an overall measure Combined score $=$ (Inform+Success) $\times 0.5+$ BLEU. Likewise, we report the average score over three inferences. We use the same three demonstration examples when using DSP or standard prompting.
Supervised fine-tuning details To conduct supervised fine-tuning on the policy model, we format the input of each sample as Translate dialogue to dialogue action: [Dialogue context]", with the target being the verbalized dialogue acts in the same format as [77, 63]. For instance, a dialogue act <hotel, inform, choice>, <hotel, inform, type>, <hotel, request, area> will be converted to "[hotel] [inform] choice type [request] area", which indicates that the system should inform available hotel choices and their types and ask for the area that the user would like (see the Appendix for examples). Note that the provided dialogue act annotations may not be the only valid dialogue act for the same dialogue content [77], and thus we hope to explore diverse valid dialogue acts (directional stimulus) through RL training.
RL training details The evaluation metrics Success and Inform rates are defined at the dialogue level, while the BLEU score is computed on the corpus level. However, our training and inference on conducted on the turn level. We thus use the sentence-level SacreBLEU [51] score as the reward. Same as in the summarization experiments, we generate four outputs per input using the LLM with a temperature of 0.7 . The policy network is trained 52 k episodes, 5 epochs per batch with a batch size of 8 and a learning rate of $2 \times 10^{-6}$. Since the generated dialogue acts should adhere to the business logic and ontology, we ensure that the updated policy network does not deviate significantly from the original policy model. We thus set the $\mathrm{KL}<em 0="0">{\text {target }}$ and $\beta</em>$ in Equation 7 as 0.2 and 0.01 , respectively. During training, we use top- $k$ sampling and set $k$ to 50 to explore the action space. During inference, we use beam search decoding with a beam size of 5 .
Results We evaluate the impact of our approach DSP on Codex and ChatGPT and compare the performance with several representative task-oriented dialogue models trained on the full training set (8438 dialogues), including DAMD [77], MinTL [34], Soloist [49], SimpleTOD [21], DoTS [23], PPTOD [63], UBAR [72], and GALAXY [19]. Table 1 summarizes the overall performance comparison,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Response generation performance of different methods on the MultiWOZ 2.0\&amp;2.1 datasets, where Succ. and Comb. denote the Success and Combined Score metrics, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">#Training data</th>
<th style="text-align: center;">MultiWOZ 2.0</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MultiWOZ 2.1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Inform</td>
<td style="text-align: center;">Succ.</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">Comb.</td>
<td style="text-align: center;">Inform</td>
<td style="text-align: center;">Succ.</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">Comb.</td>
</tr>
<tr>
<td style="text-align: center;">Codex</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Standard Prompting</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">65.9</td>
</tr>
<tr>
<td style="text-align: center;">DSP w/ SFT</td>
<td style="text-align: center;">1\% (80)</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">80.1</td>
</tr>
<tr>
<td style="text-align: center;">DSP w/ SFT+RL</td>
<td style="text-align: center;">1\% (80)</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">93.4</td>
</tr>
<tr>
<td style="text-align: center;">DSP w/ SFT</td>
<td style="text-align: center;">10\% (800)</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">82.6</td>
</tr>
<tr>
<td style="text-align: center;">DSP w/ SFT+RL</td>
<td style="text-align: center;">10\% (800)</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">102.2</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">99.2</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Standard Prompting</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">68.9</td>
</tr>
<tr>
<td style="text-align: center;">DSP w/ SFT</td>
<td style="text-align: center;">1\% (80)</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">81.4</td>
</tr>
<tr>
<td style="text-align: center;">DSP w/ SFT+RL</td>
<td style="text-align: center;">1\% (80)</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">93.7</td>
</tr>
<tr>
<td style="text-align: center;">DSP w/ SFT</td>
<td style="text-align: center;">10\% (800)</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">83.9</td>
</tr>
<tr>
<td style="text-align: center;">DSP w/ SFT+RL</td>
<td style="text-align: center;">10\% (800)</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">100.2</td>
</tr>
<tr>
<td style="text-align: center;">Fully supervised TOD models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DAMD [77]</td>
<td style="text-align: center;">100\% (8438)</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MinTL [34]</td>
<td style="text-align: center;">100\% (8438)</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">97.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Soloist [49]</td>
<td style="text-align: center;">100\% (8438)</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SimpleTOD [21]</td>
<td style="text-align: center;">100\% (8438)</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">93.0</td>
</tr>
<tr>
<td style="text-align: center;">DoTS [23]</td>
<td style="text-align: center;">100\% (8438)</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">96.3</td>
</tr>
<tr>
<td style="text-align: center;">PPTOD [63]</td>
<td style="text-align: center;">100\% (8438)</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">102.9</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">102.3</td>
</tr>
<tr>
<td style="text-align: center;">UBAR [72]</td>
<td style="text-align: center;">100\% (8438)</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">105.1</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">105.3</td>
</tr>
<tr>
<td style="text-align: center;">GALAXY [19]</td>
<td style="text-align: center;">100\% (8438)</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">110.4</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">110.8</td>
</tr>
</tbody>
</table>
<p>from which we obtain the following observations: (1) Our approach DSP significantly improves the success and inform rates of Codex and ChatGPT, indicating that they better understand the scenario and generate appropriate responses that help users in completing their tasks. (2) However, there is no improvement in the corpus-level BLEU score, possibly because the LLMs generate responses with different speaking styles and vocabulary since they do not see oracle system responses. Nevertheless, the high success and inform rates demonstrate the usefulness of our approach in delivering helpful and reliable responses. (3) Increasing the number of supervised fine-tuning samples does not guarantee performance improvement, but further fine-tuning the policy model using RL consistently provides performance gains. This suggests that RL training encourages the policy model to explore more model-preferred stimulus, while supervised fine-tuning may merely generate stimulus closely aligned with the pseudo-labeled data, which is not necessarily optimal. (4) Our approach achieves notable success with only 80 dialogues, surpassing several fully trained TOD models, particularly in terms of Success and Inform rates. With $10 \%$ of the training data ( 800 dialogues), our approach delivers comparable performance to current SOTA methods trained with full training data ( 8438 dialogues). We have also provided the performance of these compared methods in the low-resource settings ( $1 \%$ and $10 \%$ ) and a running example in the Appendix.</p>
<h1>3.3 Chain-of-Thought reasoning</h1>
<p>While current methods primarily use general task-specific prompts, LLMs show sensitivity to them. Studies [69, 26, 79] demonstrate that LLMs can vary in performance based on the prompt used. As a result, much of the previous work has centered on manually [56] or automatically [61, 79] crafting better prompts. However, these efforts mainly focus on task-specific prompts, which may not be optimal for every instance of a task. In our experiment, we employ our approach to generate instance-specific trigger prompts to elicit Chain-of-Thought (CoT) reasoning. Specifically, we train a policy model (t5-base) to generate instance-specific CoT trigger prompts, such as "Let's think step by step", to optimally prompt varying samples.
Dataset and evaluation We adopted the experimental setup from previous work [26, 79], where we tested zero-shot CoT reasoning abilities of InstructGPT (text-davinci-002) with different trigger prompts. There are 600 examples in the MultiArith dataset [57], which we divided into 300/50/250 for training/validation/test set. As for the AQuA dataset [35], we use the standard test set</p>
<p>Table 2: Zero-shot chain of thoughts performance of InstructGPT (text-davinci-002) with different prompts. *Our approach trains a policy model to generate instance-specific prompt triggers, which are compared to the task-specific prompts in [26, 79].</p>
<table>
<thead>
<tr>
<th style="text-align: center;">No.</th>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Chain-of-Thought Trigger Prompt</th>
<th style="text-align: center;">MultiArith</th>
<th style="text-align: center;">AQuA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Human-Designed</td>
<td style="text-align: center;">Let's think step by step.</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">31.9</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">We should think about this step by step.</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">28.7</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">First,</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">38.2</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Before we dive into the answer,</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">27.2</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Proof followed by the answer.</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">37.8</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Let's think step by step in a realistic way.</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">33.9</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Let's think step by step using common sense and knowledge.</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">34.3</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Let's think like a detective step by step.</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">24.0</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Let's think about this logically.</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">34.7</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Let's think step by step. First,</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">32.3</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Let's think</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">38.2</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Let's solve this problem by splitting it into steps.</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">33.2</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">The answer is after the proof.</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">34.3</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Let's be realistic and think step by step.</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">29.9</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">APE [79]</td>
<td style="text-align: center;">Let's work this out in a step by step way to be sure we have the right answer.</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">34.3</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">DSP w/ SFT</td>
<td style="text-align: center;">(*Generated instance-specific prompt)</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">35.8</td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">DSP w/ SFT+RL</td>
<td style="text-align: center;">(*Generated instance-specific prompt)</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">38.6</td>
</tr>
</tbody>
</table>
<p>with 254 samples, 300 samples from the standard training set for our training, and 100 samples for the standard validation set for our validation. We report the reasoning accuracy.
Supervised fine-tuning details For supervised fine-tuning (SFT), we first run inference on the training set with the 14 human-crafted prompts tested in [26], respectively. We then selected those prompt and query pairs which resulted in a correct CoT reasoning outcome to form the training set for SFT. These query-prompt pairs were used to train a t5-base policy model for 2 epochs, with the model input being the query instance and the target output a trigger prompt.
RL training details After SFT, the prompts generated by the policy model were used to trigger InstructGPT for zero-shot CoT prompting. Reasoning accuracy was utilized as the reward for reinforcement learning (RL). A reward of 1 was assigned for correct reasoning results and 0 otherwise. We conducted 20 training iterations ( 106 k episodes), with 5 epochs per batch, a batch size of 8 , and a learning rate of $2 \mathrm{e}-6$. The parameters for $\mathrm{KL}<em 0="0">{\text {target }}$ and $\beta</em>$ were set to 0.5 and 0.001 , respectively.
Results We compare the performance of using our generated instance-specific prompts with using the 14 human-crafted prompts which we used as the pseudo-stimulus to constitute the training set for SFT and also the prompt automatically discovered by the APE approach [79]. Note that all these 15 prompts are general task-specific and are used for the whole test set while ours are instance-specific. The performance comparison is shown in the Table 8. As can be seen, InstructGPT's performance varies significantly when using different task-specific prompts. Compared to the 14 task-specific human-designed prompts, DSP enhances the performance with instance-specific prompts. It also outperforms the prompt discovered by the APE approach. Solely relying on supervised fine-tuning of the policy model with the dataset comprising the 14 human-designed prompts doesn't lead to its peak performance. After fine-tuning with RL, the policy model is encouraged to explore better instance-specific trigger prompts, further improving performance.</p>
<h1>4 Related work</h1>
<p>Black-box large language models Recent years have witnessed the emergence of LLMs such as GPT-3 [6], Codex [9], InstructGPT, ChatGPT [46], PaLM [10], and LaMDA [66], which show significant promise in the field of NLP. These LLMs typically have a large number of parameters and require vast amounts of training data. Due to their scaling, these models have exhibited many emergent abilities, such as in-context learning, few-shot prompting, chain-of-thought prompting, and instruction following [6, 46, 69]. However, most LLMs are not open-sourced and can only be accessed via black-box APIs, through which the users send prompt queries and receive responses.</p>
<p>While there exist open-source LLMs such as OPT-175B [73] and Bloom [58], their local execution and fine-tuning require significant computational resources that may be infeasible for most researchers and users. However, despite their considerable performance on various tasks, LLMs often fall short of generating outputs that fully align with desired outputs on specific downstream tasks and use cases [16, 42, 18]. Our approach seeks to address this limitation by introducing directional stimulus generated by a small tunable LM into the prompt to provide more fine-grained guidance and control over black-box LLMs.</p>
<p>Prompt optimization and engineering Efficiently optimizing pre-trained LMs on downstream tasks by finding optimal prompts has been a focus of prior research. One approach involves tuning soft prompts, which are continuous embedding vectors that can be optimized using gradient descent methods [32, 30, 67, 2, 64]. However, the requirements of gradients and the challenge of passing gradients and continuous prompts through black-box APIs, making them less practical for the blackbox LLMs. Researchers have also tried to seek optimal prompts by designing task-specific natural language instructions and selecting proper training samples as in-context demonstrations in the prompt. These methods include manual engineering [50, 6, 56], editing [61, 76], reinforcement learning [13, 39], and automatic generation [79]. Despite these efforts, such prompts are not always effective at steering LLMs to generate desired outputs, especially for fine-grained instance-specific behaviors that are difficult to describe using task-specific instructions and demonstration examples. To address this limitation, our approach is able to provide more fine-grained instance-specific guidance generated by a small tunable policy model optimized with supervised fine-tuning and/or reinforcement learning.</p>
<p>Controllable text generation The control of language models (LMs) has been extensively studied. Early approaches fine-tuned LMs on datasets containing desired attributes [17]. [24] proposed class-conditioned LMs, generating text with predefined control codes. However, direct LM training is costly. To address this, PPLM [12] trains an attribute model and passes gradients to control generation. GeDi [27] and DExperts [36] use class-conditional distributions as generative discriminators to guide generation, reducing computation complexity. These methods require either additional LM training or internal gradients and logistics, making them not applicable to black-box LLMs. Our approach proposes a solution to control black-box LLMs by inserting directional stimulus into the input query prompt and optimizing based on the return output.</p>
<p>Reinforcement learning for NLP Reinforcement learning has been successfully applied to various NLP tasks, such as syntactic parsing [44, 29], machine translation [71, 28], summarization [48, 62], conversational systems [31], etc. Language models define probability distributions over tokens in their vocabulary, and the text generation problem can be naturally formulated as selecting an action in an RL setting. Therefore, there have been extensive research efforts on optimizing LMs with RL, usually by aligning them with human preferences [80, 70, 40, 62]. For example, the LLM InstructGPT [46] is optimized with RL to better follow users' instructions and intent. In contrast with these works that directly update the LLMs to align with human preferences, our work optimizes a small policy model that generates text (stimulus) to guide LLMs to generate more human-preferred output instead of directly optimizing the LLMs, bypassing the inefficient LLM's optimization.</p>
<h1>5 Conclusions and future work</h1>
<p>In this paper, we introduce Directional Stimulus Prompting (DSP), a new prompting framework to provide black-box LLMs with fine-grained and instance-specific guidance toward the desired outputs. We use a tunable policy model to generate the directional stimulus to provide such guidance and convert the optimization of black-box LLMs to that of the policy model. Experimental results demonstrate the effectiveness of our approach. DSP not only enables better control and guidance for black-box LLMs, but also effectively utilizes labeled data. Furthermore, the generated stimulus provides valuable insights and interpretations of LLMs' behaviors. In this work, we use heuristically selected or annotated pseudo-stimulus data for supervised fine-tuning of the policy model. For future work, we hope to explore the possibility of using a "machine language" between the policy model and the LLMs that might not be intuitively preferred by humans but can better convey guidance information, as well as other forms of directional stimulus beyond text.</p>
<h1>Acknowledgments and Disclosure of Funding</h1>
<p>This research was partly sponsored by the DARPA PTG program (HR001122C0009). Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of funding agencies.</p>
<h2>References</h2>
<p>[1] Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.
[2] An, S., Li, Y., Lin, Z., Liu, Q., Chen, B., Fu, Q., Chen, W., Zheng, N., and Lou, J.G. Input-Tuning: Adapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131, 2022.
[3] Banerjee, S. and Lavie, A. METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 65-72, 2005.
[4] Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z., Yu, T., Chung, W., et al. A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023, 2023.
[5] Barrios, F., López, F., Argerich, L., and Wachenchauzer, R. Variations of the similarity function of textrank for automated summarization. arXiv preprint arXiv:1602.03606, 2016.
[6] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[7] Budzianowski, P., Wen, T.-H., Tseng, B.-H., Casanueva, I., Ultes, S., Ramadan, O., and Gašić, M. MultiWOZ - a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling. arXiv preprint arXiv:1810.00278, 2018.
[8] Card, D., Henderson, P., Khandelwal, U., Jia, R., Mahowald, K., and Jurafsky, D. With little power comes great responsibility. arXiv preprint arXiv:2010.06595, 2020.
[9] Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[10] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[11] Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.
[12] Dathathri, S., Madotto, A., Lan, J., Hung, J., Frank, E., Molino, P., Yosinski, J., and Liu, R. Plug and play language models: A simple approach to controlled text generation. arXiv preprint arXiv:1912.02164, 2019.
[13] Deng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu, T., Song, M., Xing, E. P., and Hu, Z. RLPrompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548, 2022.
[14] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[15] Eric, M., Goel, R., Paul, S., Kumar, A., Sethi, A., Ku, P., Goyal, A. K., Agarwal, S., Gao, S., and Hakkani-Tur, D. MultiWOZ 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines. arXiv preprint arXiv:1907.01669, 2019.</p>
<p>[16] Goyal, T., Li, J. J., and Durrett, G. News summarization and evaluation in the era of GPT-3. arXiv preprint arXiv:2209.12356, 2022.
[17] Gururangan, S., Marasović, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., and Smith, N. A. Don’t stop pretraining: Adapt language models to domains and tasks. arXiv preprint arXiv:2004.10964, 2020.
[18] Gutiérrez, B. J., McNeal, N., Washington, C., Chen, Y., Li, L., Sun, H., and Su, Y. Thinking about GPT-3 in-context learning for biomedical IE? Think again. arXiv preprint arXiv:2203.08410, 2022.
[19] He, W., Dai, Y., Zheng, Y., Wu, Y., Cao, Z., Liu, D., Jiang, P., Yang, M., Huang, F., Si, L., et al. Galaxy: A generative pre-trained model for task-oriented dialog with semi-supervised learning and explicit policy injection. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 10749-10757, 2022.
[20] Honnibal, M. and Montani, I. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear, 2017.
[21] Hosseini-Asl, E., McCann, B., Wu, C.-S., Yavuz, S., and Socher, R. A simple language model for task-oriented dialogue. Advances in Neural Information Processing Systems, 33: 20179-20191, 2020.
[22] Hudeček, V. and Dušek, O. Are LLMs all you need for task-oriented dialogue? arXiv preprint arXiv:2304.06556, 2023.
[23] Jeon, H. and Lee, G. G. Domain state tracking for a simplified dialogue system. arXiv preprint arXiv:2103.06648, 2021.
[24] Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C., and Socher, R. CTRL: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858, 2019.
[25] Khattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024, 2022.
[26] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199-22213, 2022.
[27] Krause, B., Gotmare, A. D., McCann, B., Keskar, N. S., Joty, S., Socher, R., and Rajani, N. F. GeDi: Generative discriminator guided sequence generation. arXiv preprint arXiv:2009.06367, 2020.
[28] Kumar, G., Foster, G., Cherry, C., and Krikun, M. Reinforcement learning based curriculum optimization for neural machine translation. arXiv preprint arXiv:1903.00041, 2019.
[29] Le, M. and Fokkens, A. Tackling error propagation through reinforcement learning: A case of greedy dependency parsing. arXiv preprint arXiv:1702.06794, 2017.
[30] Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.
[31] Li, J., Monroe, W., Ritter, A., Galley, M., Gao, J., and Jurafsky, D. Deep reinforcement learning for dialogue generation. arXiv preprint arXiv:1606.01541, 2016.
[32] Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.
[33] Lin, C.-Y. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74-81, 2004.
[34] Lin, Z., Madotto, A., Winata, G. I., and Fung, P. Mintl: Minimalist transfer learning for task-oriented dialogue systems. arXiv preprint arXiv:2009.12005, 2020.</p>
<p>[35] Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017.
[36] Liu, A., Sap, M., Lu, X., Swayamdipta, S., Bhagavatula, C., Smith, N. A., and Choi, Y. Dexperts: Decoding-time controlled text generation with experts and anti-experts. arXiv preprint arXiv:2105.03023, 2021.
[37] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
[38] Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., and Zhu, C. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.
[39] Lu, P., Qiu, L., Chang, K.-W., Wu, Y. N., Zhu, S.-C., Rajpurohit, T., Clark, P., and Kalyan, A. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610, 2022.
[40] Lu, X., Welleck, S., Jiang, L., Hessel, J., Qin, L., West, P., Ammanabrolu, P., and Choi, Y. Quark: Controllable text generation with reinforced unlearning. arXiv preprint arXiv:2205.13636, 2022.
[41] Mihalcea, R. and Tarau, P. Textrank: Bringing order into text. In Proceedings of the 2004 conference on empirical methods in natural language processing, pp. 404-411, 2004.
[42] Moradi, M., Blagec, K., Haberl, F., and Samwald, M. Gpt-3 models are poor few-shot learners in the biomedical domain. arXiv preprint arXiv:2109.02555, 2021.
[43] Nallapati, R., Zhou, B., Gulcehre, C., Xiang, B., et al. Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023, 2016.
[44] Neu, G. and Szepesvári, C. Training parsers by inverse reinforcement learning. Machine Learning. 2009 Dec; 77: 303-37., 2009.
[45] OpenAI. Gpt-4 technical report, 2023.
[46] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.
[47] Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311-318, 2002.
[48] Paulus, R., Xiong, C., and Socher, R. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.
[49] Peng, B., Li, C., Li, J., Shayandeh, S., Liden, L., and Gao, J. SOLOIST: Building task bots at scale with transfer learning and machine teaching. Transactions of the Association for Computational Linguistics, 9:807-824, 2021.
[50] Petroni, F., Rocktäschel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.
[51] Post, M. A call for clarity in reporting bleu scores. arXiv preprint arXiv:1804.08771, 2018.
[52] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[53] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.
[54] Ramamurthy, R., Ammanabrolu, P., Brantley, K., Hessel, J., Sifa, R., Bauckhage, C., Hajishirzi, H., and Choi, Y. Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241, 2022.</p>
<p>[55] Reynolds, L. and McDonell, K. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1-7, 2021.
[56] Reynolds, L. and McDonell, K. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1-7, 2021.
[57] Roy, S. and Roth, D. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413, 2016.
[58] Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., et al. BLOOM: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.
[59] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[60] Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.
[61] Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and Singh, S. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020.
[62] Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.
[63] Su, Y., Shu, L., Mansimov, E., Gupta, A., Cai, D., Lai, Y.-A., and Zhang, Y. Multi-task pre-training for plug-and-play task-oriented dialogue system. arXiv preprint arXiv:2109.14739, 2021.
[64] Sun, T., Shao, Y., Qian, H., Huang, X., and Qiu, X. Black-box tuning for language-model-as-a-service. In International Conference on Machine Learning, pp. 20841-20855. PMLR, 2022.
[65] Suzgun, M., Melas-Kyriazi, L., and Jurafsky, D. Follow the wisdom of the crowd: Effective text generation via minimum bayes risk decoding. arXiv preprint arXiv:2211.07634, 2022.
[66] Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et al. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.
[67] Vu, T., Lester, B., Constant, N., Al-Rfou, R., and Cer, D. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021.
[68] Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.
[69] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[70] Wu, J., Ouyang, L., Ziegler, D. M., Stiennon, N., Lowe, R., Leike, J., and Christiano, P. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862, 2021.
[71] Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.
[72] Yang, Y., Li, Y., and Quan, X. UBAR: Towards fully end-to-end task-oriented dialog system with GPT-2. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 14230-14238, 2021.</p>
<p>[73] Zhang, S., Diab, M., and Zettlemoyer, L. Democratizing access to large-scale language models with opt-175b. Meta AI, 2022.
[74] Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.
[75] Zhang, T., Ladhak, F., Durmus, E., Liang, P., McKeown, K., and Hashimoto, T. B. Benchmarking large language models for news summarization. arXiv preprint arXiv:2301.13848, 2023.
[76] Zhang, T., Wang, X., Zhou, D., Schuurmans, D., and Gonzalez, J. E. TEMPERA: Testtime prompt editing via reinforcement learning. In International Conference on Learning Representations, 2023.
[77] Zhang, Y., Ou, Z., and Yu, Z. Task-oriented dialog systems that consider multiple appropriate responses under the same context. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 9604-9611, 2020.
[78] Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.
[79] Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022.
[80] Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.</p>
<h1>A Implementation Details</h1>
<h2>A. 1 Summarization</h2>
<p>We use the representative benchmark dataset CNN/Daily Mail for news summarization [43]. This dataset contains 287,113 training examples, 13,368 validation examples, and 11,490 test examples. To keep the API usage cost low, we use a subset of $1,000,2,000$, and 4,000 for training, 500 for validation, and 500 for testing. Each example in the dataset consists of a news article along with its corresponding highlight/summary written by human authors. In order to train the policy model through supervised fine-tuning, we employed the textrank [41] algorithm to automatically extract keywords from each article and only retained those mentioned in the corresponding reference summary. We initialize the policy model using the 780M FLAN-T5-large model [11, 53], and use it to guide the black-box LLM ChatGPT. The hyperparameters used in our experiments are detailed in Table 3. All the experiments are run on a server equipped with 8 NVIDIA RTX A6000 GPUs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model Params</th>
<th style="text-align: center;">Hyperparameter values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Supervised fine-tuning (SFT)</td>
<td style="text-align: center;">batch size: 8 <br> epochs: 5 <br> learning rate: 0.00002 <br> learning rate scheduler: linear <br> weight decay: 0.01</td>
</tr>
<tr>
<td style="text-align: center;">RL (NLPO)</td>
<td style="text-align: center;">steps per update: 5120 <br> total number of steps: 51200 <br> batch size: 8 <br> epochs per update: 5 <br> learning rate: 0.000002 <br> entropy coefficient: 0.0 <br> initial kl coeff: 0.005 <br> target kl: 0.5 <br> discount factor: 0.99 <br> gae lambda: 0.95 <br> clip ratio: 0.2 <br> value function coeff: 0.5 <br> rollouts top k: 100 <br> top mask ratio: 0.9 <br> target update iterations: 20</td>
</tr>
<tr>
<td style="text-align: center;">Tokenizer</td>
<td style="text-align: center;">padding side: right <br> truncation side: right <br> max length: 512</td>
</tr>
<tr>
<td style="text-align: center;">Policy model decoding</td>
<td style="text-align: center;">sampling: True <br> temperature: 0.7 <br> min length: 10 <br> max new tokens: 80</td>
</tr>
<tr>
<td style="text-align: center;">LLM decoding</td>
<td style="text-align: center;">sampling: True <br> temperature: 0.7 <br> top_p: 1.0 <br> max new tokens: 180</td>
</tr>
</tbody>
</table>
<p>Table 3: Hyperparameters for experiments on the CNN/Daily Mail dataset.</p>
<h2>A. 2 Dialogue response generation</h2>
<p>The MultiWOZ dataset is a widely-used task-oriented dialogue dataset consisting of 8,438 dialogues for training, 1,000 dialogues for validation, and 1,000 dialogues for testing. For each turn of the dialogues, in addition to the user utterances and system response, the annotations of belief state, database query results, and dialogue act are also provided. To process the data, we followed the approach used in UBAR [72]. Specifically, we employed delexicalization by replacing specific slot values with corresponding placeholders. These placeholders can be filled based on the results of a database search. The annotated dialogue acts serve as the stimulus in our approach. Table 4 provides information on all the dialogue acts and slots present in the dataset. We converted the structured dialogue acts, originally in the form of <domain, slot, value> triplets, into text format like [domain1][inform] slot1 ... [request] slot1 ... [domain2][reqmore], where domains, acts, and slot values are all bracketed.</p>
<p>We used 780M Flan-T5-Large for our policy model to guide the ChatGPT and Codex LLMs. During the supervised fine-tuning of the policy model, we trained it to generate stimulus converted from the dialogue acts based on the given dialogue context. The policy model was trained for 25 epochs using 80 dialogues from the MultiWOZ2.0 and MultiWOZ2.1 datasets. When 800 dialogues are given, it was trained for 8 epochs on the MultiWOZ2.0 dataset and 20 epochs on the MultiWOZ2.1 dataset. All the hyperparameters setup is presented in Table 5.</p>
<p>Table 4: Full ontology for all domains in MultiWOZ2.0 [7] dataset. The upper script indicates which domains it belongs to. *: universal, 1: restaurant, 2: hotel, 3: attraction, 4: taxi, 5: train, 6: hospital, 7: police.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">dialogue acts</th>
<th style="text-align: center;">inform<em> $/$ request $</em> /$ select $^{1235} /$ recommend $/{ }^{123} /$ nooffer $^{1235} /$ offerbook $^{125} /$ offerbooked ${ }^{125} /$ nobook $^{12} /$ welcome $^{<em>} /$ greet $^{</em>} /$ bye $^{<em>} /$ reqmore $^{</em>}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">slots</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { address }^{12367} / \text { postcode }^{12367} / \text { phone }^{123467} / \text { name }^{123} / \text { area }^{123} / \text { pricerange }^{12} / \ &amp; \text { type }^{23} / \text { internet }^{2} / \text { parking }^{2} / \text { stars }^{2} / \text { departure }^{45} / \text { destination }^{45} / \text { leave }^{45} / \ &amp; \text { arrive }^{45} / \text { people }^{123} / \text { reference }^{1235} / \text { id }^{5} / \text { price }^{45} / \text { time }^{15} / \text { department }^{6} / \ &amp; \text { day }^{125} / \text { stay }^{2} / \text { car }^{4} / \text { food }^{1} \end{aligned}$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Model Params</th>
<th style="text-align: left;">Hyperparameter values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Supervised fine-tuning (SFT)</td>
<td style="text-align: left;">batch size: 8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">epochs: $25 / 25 / 8 / 20$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">learning rate: 0.00002</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">learning rate scheduler: linear</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">weight decay: 0.01</td>
</tr>
<tr>
<td style="text-align: left;">RL (NLPO)</td>
<td style="text-align: left;">steps per update: 5120</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">total number of steps: 51200</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">batch size: 8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">epochs per update: 5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">learning rate: 0.000002</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">entropy coefficient: 0.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">initial kl coeff: 0.01</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">target kl: 0.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">discount factor: 0.99</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">gae lambda: 0.95</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">clip ratio: 0.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">value function coeff: 0.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">rollouts top k: 50</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">top mask ratio: 0.9</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">target update iterations: 20</td>
</tr>
<tr>
<td style="text-align: left;">Tokenizer</td>
<td style="text-align: left;">padding side: left</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">truncation side: left</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">max length: 512</td>
</tr>
<tr>
<td style="text-align: left;">Policy LM decoding</td>
<td style="text-align: left;">num_beams: 5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">min length: 1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">max new tokens: 40</td>
</tr>
<tr>
<td style="text-align: left;">LLM decoding</td>
<td style="text-align: left;">sampling: True</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">temperature: 0.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">top_p: 1.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">max new tokens: 64</td>
</tr>
</tbody>
</table>
<p>Table 5: Hyperparameters for experiments on the MultiWOZ dataset.</p>
<h1>A. 3 Chain of Thought reasoning</h1>
<p>We use our approach to generate instance-specific chain of thought (CoT) trigger prompts. Following previous work [26, 79], we evaluate two widely used arithmetic reasoning datasets MultiArith [57] and AQuA [35]. We compare with the 14 human-crafted chain-of-thought prompts evaluated in [26], part of which are collected from [1, 56]. We also compare with the prompt automatically designed by the APE approach [79]. We use the The hyperparameters used in our experiments are detailed in Table 6.</p>
<table>
<thead>
<tr>
<th>Model Params</th>
<th>Hyperparameter values</th>
</tr>
</thead>
<tbody>
<tr>
<td>Supervised fine-tuning (SFT)</td>
<td>batch size: 16</td>
</tr>
<tr>
<td></td>
<td>epochs: 2</td>
</tr>
<tr>
<td></td>
<td>learning rate: 0.00002</td>
</tr>
<tr>
<td></td>
<td>learning rate scheduler: linear</td>
</tr>
<tr>
<td></td>
<td>weight decay: 0.01</td>
</tr>
<tr>
<td>RL (NLPO)</td>
<td>steps per update: 5120</td>
</tr>
<tr>
<td></td>
<td>total number of steps: 51200</td>
</tr>
<tr>
<td></td>
<td>batch size: 16</td>
</tr>
<tr>
<td></td>
<td>epochs per update: 5</td>
</tr>
<tr>
<td></td>
<td>learning rate: 0.000002</td>
</tr>
<tr>
<td></td>
<td>entropy coefficient: 0.0</td>
</tr>
<tr>
<td></td>
<td>initial kl coeff: 0.001</td>
</tr>
<tr>
<td></td>
<td>target kl: 0.5</td>
</tr>
<tr>
<td></td>
<td>discount factor: 0.99</td>
</tr>
<tr>
<td></td>
<td>gae lambda: 0.95</td>
</tr>
<tr>
<td></td>
<td>clip ratio: 0.2</td>
</tr>
<tr>
<td></td>
<td>value function coeff: 0.5</td>
</tr>
<tr>
<td></td>
<td>rollouts top k: 50</td>
</tr>
<tr>
<td></td>
<td>top mask ratio: 0.9</td>
</tr>
<tr>
<td></td>
<td>target update iterations: 20</td>
</tr>
<tr>
<td>Tokenizer</td>
<td>padding side: right</td>
</tr>
<tr>
<td></td>
<td>truncation side: right</td>
</tr>
<tr>
<td></td>
<td>max length: 128</td>
</tr>
<tr>
<td>Policy LM decoding</td>
<td>sampling: True</td>
</tr>
<tr>
<td></td>
<td>temperature: 0.7</td>
</tr>
<tr>
<td></td>
<td>max new tokens: 32</td>
</tr>
<tr>
<td>LLM decoding</td>
<td>sampling: True</td>
</tr>
<tr>
<td></td>
<td>temperature: 0.7</td>
</tr>
<tr>
<td></td>
<td>top_p: 1.0</td>
</tr>
<tr>
<td></td>
<td>max new tokens: 32</td>
</tr>
</tbody>
</table>
<p>Table 6: Hyperparameters for experiments on the zero-shot chain-of-thought reasoning.</p>
<h2>B Additional results</h2>
<h3>B.1 Summarization</h3>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Number of generated keywords, keyword precision, and summary ROUGE-1 during the training process on 4000 samples.</p>
<p>Analysis of generated hints/keywords We outlined changes in the number of generated keywords, hit keywords (those matched in the reference summary), and corresponding ROUGE-1 scores throughout the training process in Figure 5. As the training progresses, the policy model appears to generate keywords with increasing precision, which aligns positively with the increasing ROUGE-1 score. However, it is observed that even when keywords are generated with high precision if their quantity is too limited, the performance doesn't necessarily improve.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Part-of-Speech (POS) and Named Entity Recognition (NER) tagging on the generated hints, i.e., keywords.</p>
<p>We also employed the spacy package [20] for Part-of-Speech (POS) and Named Entity Recognition (NER) tagging on the generated keywords. The results are shown in the Figure 6. For the POS tagging, we observe that nouns (NOUN) and proper nouns (PROPN) are the most frequently generated keywords, which can serve as informative keywords. As for the NER tagging, the most commonly generated keywords include persons (PERSON), geopolitical entities (GPE), dates (DATE), organizations (ORG), and numerals (CARDINAL).</p>
<p>GPT-4 Evaluation To gain a better understanding of generated summaries guided by keywords, we employed GPT-4 to evaluate the summaries. It has been shown that the LLM, especially GPT-4 is able to produce consistently high-quality assessments of text generation, showing high human alignment and thus being a good alternative to human evaluations [78, 38]. As we employ ROUGE scores as rewards for tuning the policy model to generate keywords that guide the LLM towards generating summaries more aligned with the reference summary, we leveraged GPT-4 to assess the overlap of key points (hints) between generated and reference summaries. Specifically, we use GPT-4 to compare the summaries generated with our proposed DSP and the original standard prompting. GPT-4 was instructed to first generate an explanation, followed by the corresponding answer. We prompt GPT-4 as follows:</p>
<p>You are provided with an article and a corresponding reference summary. Additionally, there will be two alternative summaries labeled as 'A' and 'B'. Your task is to identify which of the two summaries (A or B) is more similar to the reference summary. This similarity should be evaluated based on the presence and accuracy of key points from the reference summary in each alternative summary. Please detail your reasoning in an explanation. After your explanation, classify the task outcome as: select 'A wins' if Summary A aligns more closely with the reference summary, 'B wins' if Summary B aligns more closely, or 'Tie' if both summaries align equally well with the reference summary.</p>
<p>The GPT-4 evaluation results are shown in Figure 7. We found that GPT-4 can produce reasonable and detailed explanations of their assessment. From our test set of 500 samples: DSP-generated summaries were favored 255 times ( $51.0 \%$ ), summaries generated with original standard prompting were favored 222 times ( $44.4 \%$ ), while a tie was observed in 23 cases ( $4.6 \%$ ).</p>
<p>Zero-shot prompting In our main experiments, we employ few-shot prompting with 3 examples in the prompt during training and evaluation. The specific prompt and demonstration examples utilized are detailed in Appendix D. To test whether the approach performed well under the zero-shot setting, we evaluated the following two experimental settings on the CNNDM dataset with 4,000 training samples: (1) few(3)-shot during training and zero-shot during evaluation; and (2) zero-shot during both training and evaluation.
As shown in Figure 8, when both training and testing are conducted using zero-shot prompting, the performance improvement over standard prompting is still comparable to the scenario where both are conducted using few-shot prompting (results shown in Table 7?. In addition, we observed that our</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: GPT-4 evaluation on comparing Figure 8: Zero-shot evaluation results. DSP (0-shot) the summaries generated with our approach denotes that we use 0 -shot prompting during RL DSP, i.e., with the guidance of our gener-training and DSP (3-shot) indicates we use 3-shot ated keywords, and the original standard prompting during RL training. prompting, i.e., without keyword guidance.</p>
<p>Table 7: Low-resource evaluation on the MultiWOZ 2.0 dataset, where Succ. and Comb. denote the Success and Combined Score metrics, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">$1 \%$ of training data ( 80 dialogues)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$10 \%$ of training data ( 800 dialogues)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Inform</td>
<td style="text-align: center;">Succ.</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">Comb.</td>
<td style="text-align: center;">Inform</td>
<td style="text-align: center;">Succ.</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">Comb.</td>
</tr>
<tr>
<td style="text-align: center;">DAMD [77]</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">55.8</td>
</tr>
<tr>
<td style="text-align: center;">Soloist [49]</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">75.5</td>
</tr>
<tr>
<td style="text-align: center;">PPTOD [63]</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">92.0</td>
</tr>
<tr>
<td style="text-align: center;">UBAR [72]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">92.3</td>
</tr>
<tr>
<td style="text-align: center;">GALAXY [19]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">100.2</td>
</tr>
<tr>
<td style="text-align: center;">Codex</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Standard Prompting</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">66.8</td>
</tr>
<tr>
<td style="text-align: center;">DSP w/ SFT</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">87.0</td>
</tr>
<tr>
<td style="text-align: center;">DSP w/ SFT+RL</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">102.2</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Standard Prompting</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">68.4</td>
</tr>
<tr>
<td style="text-align: center;">DSP w/ SFT</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">80.5</td>
</tr>
<tr>
<td style="text-align: center;">DSP w/ SFT+RL</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">99.6</td>
</tr>
</tbody>
</table>
<p>approach exhibits robustness when different numbers of examples are used in prompts during training and evaluation, as our approach with few(3)-shot training still outperforms standard prompting under zero-shot testing.</p>
<h1>B. 2 Dialogue response generation</h1>
<p>Low-resource results In addition to the performance of compared baseline models with full training data as shown in the main paper, we also present their performance in the low-resource setting in Table 7. It is important to note that most of these methods struggle to achieve acceptable performance with only $1 \%$ of the training data ( 80 dialogues), and thus their results in the $1 \%$ setting are not reported. As for those with reported performance with 80 dialogues, their results are significantly worse compared to Codex and ChatGPT guided by the policy model. Furthermore, even with around 800 dialogues, their Inform and Success rates were still much lower than those achieved by ChatGPT and Codex.</p>
<h1>B. 3 Chain-of-Thought reasoning</h1>
<p>Table 8: Some generated trigger prompts by our fine-tuned policy model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Generated CoT Trigger Prompts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">First step:</td>
</tr>
<tr>
<td style="text-align: left;">Let's think like a detective step by step. First,</td>
</tr>
<tr>
<td style="text-align: left;">Let's solve this problem by splitting it into steps. First,</td>
</tr>
<tr>
<td style="text-align: left;">Let's think step by step using common sense.</td>
</tr>
<tr>
<td style="text-align: left;">Let's think step by step using our creative brains.</td>
</tr>
<tr>
<td style="text-align: left;">Let's think step by step using both the above information and the testing.</td>
</tr>
<tr>
<td style="text-align: left;">Let's think step by step using proven methods.</td>
</tr>
<tr>
<td style="text-align: left;">The answer is following the proof.</td>
</tr>
</tbody>
</table>
<p>Newly discovered prompts After fine-tuning with RL, the policy model is encouraged to discover instance-specific trigger prompts. Table 8 showcases some of these newly generated trigger prompts, which deviate from those present in the training data for SFT. Some are modifications or combinations of prompts from the training data, such as "First step:" and "Let's think like a detective step by step. First,". Others include new information, like "Let's think step by step using both the above information and the testing." and "Let's think step by step using proven methods.".</p>
<h2>C Running examples</h2>
<p>We provide two running examples on the CNN/Daily Mail and MultiWOZ dataset in Table 9 and 10, respectively. For each example, we present the generations of ChatGPT with standard prompting, DSP trained with SFT, and DSP trained with SFT and RL.</p>
<h2>D Prompts</h2>
<p>The used prompts of standard prompting and our proposed Directional Stimulus Prompting on CNN/Daily Mail and MultiWOZ datasets are given in Figures 9, 10, and Figures 11, 12, respectively. Both use the same three demonstration examples in standard prompting and DSP. In the case of the CNN/Daily Mail dataset, DSP incorporates additional keywords as hints (stimulus) in the prompts. For the MultiWOZ dataset, DSP includes the dialogue acts for each system turn as stimulus, along with explanations for all the dialogue acts.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://openAI.com/blog/chatgpt
${ }^{5}$ https://www.deepmind.com/blog/building-safer-dialogue-agents&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7013 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7013</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7013</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-235313702</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2106.01623v1.pdf" target="_blank">Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models</a></p>
                <p><strong>Paper Abstract:</strong> This paper studies how to automatically generate a natural language text that describes the facts in knowledge graph (KG). Considering the few-shot setting, we leverage the excellent capacities of pretrained language models (PLMs) in language understanding and generation. We make three major technical contributions, namely representation alignment for bridging the semantic gap between KG encodings and PLMs, relation-biased KG linearization for deriving better input representations, and multi-task learning for learning the correspondence between KG and text. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our model on KG-to-text generation task. In particular, our model outperforms all comparison methods on both fully-supervised and few-shot settings. Our code and datasets are available at https://github.com/RUCAIBox/Few-Shot-KG2Text.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7013.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7013.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RBFS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relation-Biased Breadth-First Search linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linearization strategy that traverses a KG by breadth-first search but orders sibling entities by a learned relation-biased weight, producing an entity sequence as input to a pretrained language model decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Relation-Biased BFS (RBFS) linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Traverse the KG starting from a root with BFS; at each layer, sibling entities are ordered by a relation-priority weight α_e = σ(ṽ_e W_r ṽ_e) computed from R-GCN entity embeddings and relation matrices, producing a sequential entity list (entity tokens) fed into the higher layers (decoder) of the PLM.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Relation-biased breadth-first search (RBFS): BFS traversal with sibling ordering determined by learned relation weights</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG, AGENDA, GenWiki Fine</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KG-to-text generation (few-shot and fully-supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART-Large encoder/decoder + R-GCN KG encoder (RBFS used to produce decoder input)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BART-Large (12-encoder/12-decoder layers) split into lower/higher layers for representation alignment; R-GCN (2 layers) for KG encoding; entity embeddings size 1024; embeddings initialized from pretrained KG embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (BLEU-4), ROUGE-L, CIDEr, CHRF++; human eval (#Supported facts, #Contradicting, naturalness Likert)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>RBFS used in the authors' model which outperformed baselines; e.g., few-shot BLEU-4 on WebNLG (500 instances): Ours 55.13 vs BART-Large 49.34 (Table 3). Other datasets: AGENDA (500) Ours 25.72 vs BART-Large 20.70; GenWiki Fine (500) Ours 50.72 vs BART-Large 47.50.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Produced more coherent and faithful entity sequences that improved generation quality in few-shot settings; RBFS (and FFS) led to higher BLEU than random or DFS-based traversals in retraining experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still lossy: compresses graph structure into a sequence and can lose explicit structural relations; ordering depends on learned relation weights and R-GCN embeddings (thus not purely data-agnostic canonicalization across models); requires a root/starting node and learned parameters for ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared empirically to RDFS (relation-biased DFS), FFS (forest-fire), and RS (random); RBFS and FFS achieved better BLEU than RS and RDFS. RBFS outperformed baseline triple-sequence linearizations (used by BART/T5) by producing more coherent, relation-prioritized sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7013.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7013.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triple-sequence (BART/T5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Triple sequence linearization (entity-relation-entity triple list)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A common baseline representation that serializes KG triples into a linear sequence (often triples concatenated or a repeat of entity and relation tokens), used as input text to PLMs such as BART or T5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Triple-sequence linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent the KG as a sequence of triples (head, relation, tail), either concatenated or joined with separators; the sequence is fed as plain text into pretrained sequence-to-sequence models (BART/T5). Entities may be repeated if they appear in multiple triples.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Triple list ordering (often random or predefined order); prior work sometimes randomizes triple order or uses simple rules to serialize triples</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG, AGENDA, GenWiki Fine (used by baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KG-to-text generation (graph-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART-Base/Large, T5-Base/Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Text-to-text PLMs (BART and T5) that accept the serialized triple sequence as textual input and generate descriptive text; in baselines these models were trained on full training sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (BLEU-4), ROUGE-L, CIDEr, CHRF++</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported baseline numbers: e.g., few-shot WebNLG (500 instances) BART-Large BLEU-4 49.34 (Table 3); T5-Large lower in some few-shot settings (example: WebNLG 500 = 38.36 for T5 in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Simple to use with text-conditioned PLMs and leverages PLM pretraining, but performance drops considerably in few-shot compared with the authors' RBFS+alignment approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not explicitly model graph structure (edges/neighbor relationships) beyond local triple context; repeats entities across triples causing redundancy; random triple ordering can harm coherence; less faithful generation when structural signals are required.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Authors observed that BART/T5 triple-sequence linearizations are outperformed by their RBFS+GNN+alignment approach in few-shot settings, which better captures structure and relation priorities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7013.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7013.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDFS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relation-Biased Depth-First Search linearization (RDFS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A depth-first traversal variant that biases sibling visitation order by relation-based weights, used as an alternative linearization strategy in analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Relation-Biased DFS (RDFS) linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Perform depth-first search starting from root; when selecting which child branch to traverse next, use relation-priority weights (similar to RBFS) to order siblings, yielding an entity sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Relation-biased DFS: DFS traversal with sibling ordering by learned relation weights</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KG-to-text generation (linearization analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Same base model (BART + R-GCN) retrained with RDFS linearizations</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Model architecture unchanged; only the linearization traversal is swapped to RDFS for ablation/comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (reported in linearization analysis Figure 3)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>RDFS underperforms RBFS and FFS in BLEU on WebNLG test set (exact BLEU values not listed in text but shown in figure: RDFS lower than RBFS/FFS).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provided a different ordering that tended to yield worse BLEU scores than RBFS/FFS in experiments; shows traversal choice affects model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>DFS tendency to explore deep branches first can place semantically related nodes far apart in the sequence, harming coherence for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Empirically worse than RBFS and FFS; better than purely random traversal in some cases but overall not the preferred traversal in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7013.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7013.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FFS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Forest Fire Search linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A randomized layer-based traversal (randomized RBFS variant) used to produce entity sequences; a baseline in linearization analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Forest Fire Search (FFS) linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A randomized version of layer-based traversal that explores nodes at the same layer in a randomized order (a stochastic variant of BFS), producing an entity sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossy, stochastic</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Forest-fire sampling/traversal across layers with randomness in sibling ordering</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KG-to-text generation (linearization analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART + R-GCN retrained with FFS linearization</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same architecture with FFS-produced sequences used as decoder input for training/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (linearization analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>FFS produced BLEU close to RBFS and better than RS/RDFS in the authors' experiments (specific numeric values shown in Figure 3 but not listed in text).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Layer-aware traversal and some randomness maintained reasonable coherence and achieved strong BLEU compared to fully random traversal.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Randomness can produce variability and non-deterministic ordering; lacks explicit relation-priority modeling compared to RBFS.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>FFS performed comparably to RBFS and better than RS/RDFS in BLEU; RBFS has advantage by explicitly modeling relation priorities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7013.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7013.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Search linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline linearization that randomly orders nodes/entities in the KG to form the input sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Random search (RS) linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Traverse the KG by randomly ordering nodes (no structural or relation-informed ordering), serializing entities into a sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossy, stochastic</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Random ordering of entities/triples (purely stochastic)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KG-to-text generation (linearization baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART + R-GCN retrained with random linearization</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unchanged model architecture; training input sequences are random permutations of graph nodes/entities.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (linearization analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>RS achieved the worst BLEU among the tested traversal strategies in authors' analysis (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Random ordering harmed coherence and BLEU, demonstrating the importance of traversal strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not preserve any structural or relation-driven ordering; high variability and poor performance for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Worse than RBFS, FFS, and RDFS in experiments; demonstrates that informed ordering is beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7013.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7013.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Token-level node rep.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token-level entity node representation (subword/token splitting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Represent multi-word entities as multiple token-level nodes (one node per token), enabling token-level entity embedding learning and better generalization to unseen entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Token-level node representation (entity tokenization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>An entity such as 'Iron Man' is represented as two nodes ('Iron', 'Man'); entity embeddings are learned at token/subword level (using BPE/subword vocabulary) so unseen multi-word entities can be composed from token embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based, hierarchical (entity->tokens), lossy/approximate</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Split entity strings using BPE / subword tokenization and create corresponding token nodes in the KG encoder graph representation</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG, AGENDA, GenWiki Fine</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KG-to-text generation (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>R-GCN KG encoder + BPE/subword vocabulary; PLM uses same subword tokens</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>R-GCN uses token-level nodes; BPE (Sennrich et al.) and subword vocabulary (Radford et al.) applied to split entity words into smaller semantic units</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Ablation observations and overall generation metrics (BLEU/ROUGE/CIDEr) indirectly reflect effect</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Helps generalization to unseen entities in few-shot learning by composing embeddings from sub-tokens; reduces out-of-vocabulary problems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>May increase number of nodes and complexity; token-level splitting can lose entity-level semantics and cohesion if composition is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Preferred over treating entire multi-word entities as single atomic nodes for few-shot generalization; used in conjunction with representation alignment to integrate with PLM token embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7013.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7013.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Representation Alignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GNN-to-PLM Representation Alignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned alignment that minimizes Euclidean distance between R-GCN-produced entity embeddings and PLM-based entity embeddings (extracted from masked text), enabling GNN embeddings to be used as PLM inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Representation alignment (GNN ↔ PLM embedding alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Compute PLM-based entity embeddings by encoding the masked reference text through PLM lower layers and pooling mention token embeddings; compute R-GCN entity embeddings; minimize L_RA = Σ_e ||ṽ_e - v_e||^2 to align embedding spaces so GNN outputs can be fed into higher PLM layers.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>embedding-space alignment (not a sequence format), transformation-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Supervised Euclidean-distance minimization between GNN entity embeddings and PLM-derived entity embeddings from masked text</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG, AGENDA, GenWiki Fine (training and ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KG-to-text generation (few-shot); representation bridging</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>R-GCN (entity encoder) aligned to lower layers of BART-Large (PLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Lower 6 layers of BART encoder produce PLM-based entity embeddings; R-GCN produces KG entity embeddings; alignment loss L_RA added to total loss.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Ablation impact measured by BLEU/ROUGE/CIDEr and human evaluation (#Supported facts etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Removing L_RA causes notable drop in BLEU/ROUGE/CIDEr (ablation Table 5); exact numeric drops not enumerated in text but reported as significant.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Bridges semantic gap between KG encodings and PLMs, enabling effective injection of KG representations into PLM and improving few-shot generation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires access to paired text during training to compute PLM-based entity embeddings (masked text used for alignment); alignment not needed at test time but adds training complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Authors argue alignment is essential versus direct feeding of graph encodings into PLMs (which suffers from semantic gap); ablation shows representation alignment yields clear gains over models without it.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Investigating pretrained language models for graph-to-text generation <em>(Rating: 2)</em></li>
                <li>The webnlg challenge: Generating text from RDF data <em>(Rating: 2)</em></li>
                <li>Text generation from knowledge graphs with graph transformers <em>(Rating: 2)</em></li>
                <li>Modeling global and local node contexts for text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7013",
    "paper_id": "paper-235313702",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "RBFS",
            "name_full": "Relation-Biased Breadth-First Search linearization",
            "brief_description": "A linearization strategy that traverses a KG by breadth-first search but orders sibling entities by a learned relation-biased weight, producing an entity sequence as input to a pretrained language model decoder.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Relation-Biased BFS (RBFS) linearization",
            "representation_description": "Traverse the KG starting from a root with BFS; at each layer, sibling entities are ordered by a relation-priority weight α_e = σ(ṽ_e W_r ṽ_e) computed from R-GCN entity embeddings and relation matrices, producing a sequential entity list (entity tokens) fed into the higher layers (decoder) of the PLM.",
            "representation_type": "sequential, token-based, lossy",
            "encoding_method": "Relation-biased breadth-first search (RBFS): BFS traversal with sibling ordering determined by learned relation weights",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "WebNLG, AGENDA, GenWiki Fine",
            "task_name": "KG-to-text generation (few-shot and fully-supervised)",
            "model_name": "BART-Large encoder/decoder + R-GCN KG encoder (RBFS used to produce decoder input)",
            "model_description": "BART-Large (12-encoder/12-decoder layers) split into lower/higher layers for representation alignment; R-GCN (2 layers) for KG encoding; entity embeddings size 1024; embeddings initialized from pretrained KG embeddings.",
            "performance_metric": "BLEU (BLEU-4), ROUGE-L, CIDEr, CHRF++; human eval (#Supported facts, #Contradicting, naturalness Likert)",
            "performance_value": "RBFS used in the authors' model which outperformed baselines; e.g., few-shot BLEU-4 on WebNLG (500 instances): Ours 55.13 vs BART-Large 49.34 (Table 3). Other datasets: AGENDA (500) Ours 25.72 vs BART-Large 20.70; GenWiki Fine (500) Ours 50.72 vs BART-Large 47.50.",
            "impact_on_training": "Produced more coherent and faithful entity sequences that improved generation quality in few-shot settings; RBFS (and FFS) led to higher BLEU than random or DFS-based traversals in retraining experiments.",
            "limitations": "Still lossy: compresses graph structure into a sequence and can lose explicit structural relations; ordering depends on learned relation weights and R-GCN embeddings (thus not purely data-agnostic canonicalization across models); requires a root/starting node and learned parameters for ordering.",
            "comparison_with_other": "Compared empirically to RDFS (relation-biased DFS), FFS (forest-fire), and RS (random); RBFS and FFS achieved better BLEU than RS and RDFS. RBFS outperformed baseline triple-sequence linearizations (used by BART/T5) by producing more coherent, relation-prioritized sequences.",
            "uuid": "e7013.0",
            "source_info": {
                "paper_title": "Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Triple-sequence (BART/T5)",
            "name_full": "Triple sequence linearization (entity-relation-entity triple list)",
            "brief_description": "A common baseline representation that serializes KG triples into a linear sequence (often triples concatenated or a repeat of entity and relation tokens), used as input text to PLMs such as BART or T5.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Triple-sequence linearization",
            "representation_description": "Represent the KG as a sequence of triples (head, relation, tail), either concatenated or joined with separators; the sequence is fed as plain text into pretrained sequence-to-sequence models (BART/T5). Entities may be repeated if they appear in multiple triples.",
            "representation_type": "sequential, token-based, lossy",
            "encoding_method": "Triple list ordering (often random or predefined order); prior work sometimes randomizes triple order or uses simple rules to serialize triples",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "WebNLG, AGENDA, GenWiki Fine (used by baselines)",
            "task_name": "KG-to-text generation (graph-to-text)",
            "model_name": "BART-Base/Large, T5-Base/Large",
            "model_description": "Text-to-text PLMs (BART and T5) that accept the serialized triple sequence as textual input and generate descriptive text; in baselines these models were trained on full training sets.",
            "performance_metric": "BLEU (BLEU-4), ROUGE-L, CIDEr, CHRF++",
            "performance_value": "Reported baseline numbers: e.g., few-shot WebNLG (500 instances) BART-Large BLEU-4 49.34 (Table 3); T5-Large lower in some few-shot settings (example: WebNLG 500 = 38.36 for T5 in Table 3).",
            "impact_on_training": "Simple to use with text-conditioned PLMs and leverages PLM pretraining, but performance drops considerably in few-shot compared with the authors' RBFS+alignment approach.",
            "limitations": "Does not explicitly model graph structure (edges/neighbor relationships) beyond local triple context; repeats entities across triples causing redundancy; random triple ordering can harm coherence; less faithful generation when structural signals are required.",
            "comparison_with_other": "Authors observed that BART/T5 triple-sequence linearizations are outperformed by their RBFS+GNN+alignment approach in few-shot settings, which better captures structure and relation priorities.",
            "uuid": "e7013.1",
            "source_info": {
                "paper_title": "Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "RDFS",
            "name_full": "Relation-Biased Depth-First Search linearization (RDFS)",
            "brief_description": "A depth-first traversal variant that biases sibling visitation order by relation-based weights, used as an alternative linearization strategy in analysis.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Relation-Biased DFS (RDFS) linearization",
            "representation_description": "Perform depth-first search starting from root; when selecting which child branch to traverse next, use relation-priority weights (similar to RBFS) to order siblings, yielding an entity sequence.",
            "representation_type": "sequential, token-based, lossy",
            "encoding_method": "Relation-biased DFS: DFS traversal with sibling ordering by learned relation weights",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "WebNLG (analysis)",
            "task_name": "KG-to-text generation (linearization analysis)",
            "model_name": "Same base model (BART + R-GCN) retrained with RDFS linearizations",
            "model_description": "Model architecture unchanged; only the linearization traversal is swapped to RDFS for ablation/comparison.",
            "performance_metric": "BLEU (reported in linearization analysis Figure 3)",
            "performance_value": "RDFS underperforms RBFS and FFS in BLEU on WebNLG test set (exact BLEU values not listed in text but shown in figure: RDFS lower than RBFS/FFS).",
            "impact_on_training": "Provided a different ordering that tended to yield worse BLEU scores than RBFS/FFS in experiments; shows traversal choice affects model performance.",
            "limitations": "DFS tendency to explore deep branches first can place semantically related nodes far apart in the sequence, harming coherence for generation.",
            "comparison_with_other": "Empirically worse than RBFS and FFS; better than purely random traversal in some cases but overall not the preferred traversal in this work.",
            "uuid": "e7013.2",
            "source_info": {
                "paper_title": "Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "FFS",
            "name_full": "Forest Fire Search linearization",
            "brief_description": "A randomized layer-based traversal (randomized RBFS variant) used to produce entity sequences; a baseline in linearization analysis.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Forest Fire Search (FFS) linearization",
            "representation_description": "A randomized version of layer-based traversal that explores nodes at the same layer in a randomized order (a stochastic variant of BFS), producing an entity sequence.",
            "representation_type": "sequential, token-based, lossy, stochastic",
            "encoding_method": "Forest-fire sampling/traversal across layers with randomness in sibling ordering",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "WebNLG (analysis)",
            "task_name": "KG-to-text generation (linearization analysis)",
            "model_name": "BART + R-GCN retrained with FFS linearization",
            "model_description": "Same architecture with FFS-produced sequences used as decoder input for training/evaluation.",
            "performance_metric": "BLEU (linearization analysis)",
            "performance_value": "FFS produced BLEU close to RBFS and better than RS/RDFS in the authors' experiments (specific numeric values shown in Figure 3 but not listed in text).",
            "impact_on_training": "Layer-aware traversal and some randomness maintained reasonable coherence and achieved strong BLEU compared to fully random traversal.",
            "limitations": "Randomness can produce variability and non-deterministic ordering; lacks explicit relation-priority modeling compared to RBFS.",
            "comparison_with_other": "FFS performed comparably to RBFS and better than RS/RDFS in BLEU; RBFS has advantage by explicitly modeling relation priorities.",
            "uuid": "e7013.3",
            "source_info": {
                "paper_title": "Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "RS",
            "name_full": "Random Search linearization",
            "brief_description": "A baseline linearization that randomly orders nodes/entities in the KG to form the input sequence.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Random search (RS) linearization",
            "representation_description": "Traverse the KG by randomly ordering nodes (no structural or relation-informed ordering), serializing entities into a sequence.",
            "representation_type": "sequential, token-based, lossy, stochastic",
            "encoding_method": "Random ordering of entities/triples (purely stochastic)",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "WebNLG (analysis)",
            "task_name": "KG-to-text generation (linearization baseline)",
            "model_name": "BART + R-GCN retrained with random linearization",
            "model_description": "Unchanged model architecture; training input sequences are random permutations of graph nodes/entities.",
            "performance_metric": "BLEU (linearization analysis)",
            "performance_value": "RS achieved the worst BLEU among the tested traversal strategies in authors' analysis (Figure 3).",
            "impact_on_training": "Random ordering harmed coherence and BLEU, demonstrating the importance of traversal strategy.",
            "limitations": "Does not preserve any structural or relation-driven ordering; high variability and poor performance for generation.",
            "comparison_with_other": "Worse than RBFS, FFS, and RDFS in experiments; demonstrates that informed ordering is beneficial.",
            "uuid": "e7013.4",
            "source_info": {
                "paper_title": "Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Token-level node rep.",
            "name_full": "Token-level entity node representation (subword/token splitting)",
            "brief_description": "Represent multi-word entities as multiple token-level nodes (one node per token), enabling token-level entity embedding learning and better generalization to unseen entities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Token-level node representation (entity tokenization)",
            "representation_description": "An entity such as 'Iron Man' is represented as two nodes ('Iron', 'Man'); entity embeddings are learned at token/subword level (using BPE/subword vocabulary) so unseen multi-word entities can be composed from token embeddings.",
            "representation_type": "token-based, hierarchical (entity-&gt;tokens), lossy/approximate",
            "encoding_method": "Split entity strings using BPE / subword tokenization and create corresponding token nodes in the KG encoder graph representation",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG, AGENDA, GenWiki Fine",
            "task_name": "KG-to-text generation (few-shot)",
            "model_name": "R-GCN KG encoder + BPE/subword vocabulary; PLM uses same subword tokens",
            "model_description": "R-GCN uses token-level nodes; BPE (Sennrich et al.) and subword vocabulary (Radford et al.) applied to split entity words into smaller semantic units",
            "performance_metric": "Ablation observations and overall generation metrics (BLEU/ROUGE/CIDEr) indirectly reflect effect",
            "performance_value": null,
            "impact_on_training": "Helps generalization to unseen entities in few-shot learning by composing embeddings from sub-tokens; reduces out-of-vocabulary problems.",
            "limitations": "May increase number of nodes and complexity; token-level splitting can lose entity-level semantics and cohesion if composition is imperfect.",
            "comparison_with_other": "Preferred over treating entire multi-word entities as single atomic nodes for few-shot generalization; used in conjunction with representation alignment to integrate with PLM token embeddings.",
            "uuid": "e7013.5",
            "source_info": {
                "paper_title": "Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Representation Alignment",
            "name_full": "GNN-to-PLM Representation Alignment",
            "brief_description": "A learned alignment that minimizes Euclidean distance between R-GCN-produced entity embeddings and PLM-based entity embeddings (extracted from masked text), enabling GNN embeddings to be used as PLM inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Representation alignment (GNN ↔ PLM embedding alignment)",
            "representation_description": "Compute PLM-based entity embeddings by encoding the masked reference text through PLM lower layers and pooling mention token embeddings; compute R-GCN entity embeddings; minimize L_RA = Σ_e ||ṽ_e - v_e||^2 to align embedding spaces so GNN outputs can be fed into higher PLM layers.",
            "representation_type": "embedding-space alignment (not a sequence format), transformation-based",
            "encoding_method": "Supervised Euclidean-distance minimization between GNN entity embeddings and PLM-derived entity embeddings from masked text",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG, AGENDA, GenWiki Fine (training and ablation)",
            "task_name": "KG-to-text generation (few-shot); representation bridging",
            "model_name": "R-GCN (entity encoder) aligned to lower layers of BART-Large (PLM)",
            "model_description": "Lower 6 layers of BART encoder produce PLM-based entity embeddings; R-GCN produces KG entity embeddings; alignment loss L_RA added to total loss.",
            "performance_metric": "Ablation impact measured by BLEU/ROUGE/CIDEr and human evaluation (#Supported facts etc.)",
            "performance_value": "Removing L_RA causes notable drop in BLEU/ROUGE/CIDEr (ablation Table 5); exact numeric drops not enumerated in text but reported as significant.",
            "impact_on_training": "Bridges semantic gap between KG encodings and PLMs, enabling effective injection of KG representations into PLM and improving few-shot generation fidelity.",
            "limitations": "Requires access to paired text during training to compute PLM-based entity embeddings (masked text used for alignment); alignment not needed at test time but adds training complexity.",
            "comparison_with_other": "Authors argue alignment is essential versus direct feeding of graph encodings into PLMs (which suffers from semantic gap); ablation shows representation alignment yields clear gains over models without it.",
            "uuid": "e7013.6",
            "source_info": {
                "paper_title": "Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Investigating pretrained language models for graph-to-text generation",
            "rating": 2,
            "sanitized_title": "investigating_pretrained_language_models_for_graphtotext_generation"
        },
        {
            "paper_title": "The webnlg challenge: Generating text from RDF data",
            "rating": 2,
            "sanitized_title": "the_webnlg_challenge_generating_text_from_rdf_data"
        },
        {
            "paper_title": "Text generation from knowledge graphs with graph transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "Modeling global and local node contexts for text generation from knowledge graphs",
            "rating": 2,
            "sanitized_title": "modeling_global_and_local_node_contexts_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation",
            "rating": 1,
            "sanitized_title": "genwiki_a_dataset_of_13_million_contentsharing_text_and_graphs_for_unsupervised_graphtotext_generation"
        }
    ],
    "cost": 0.015496749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models</p>
<p>Junyi Li lijunyi@ruc.edu.cn 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Beijing Key Laboratory of Big Data Management and Analysis Methods</p>
<p>Tianyi Tang 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Wayne Xin Zhao 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Beijing Key Laboratory of Big Data Management and Analysis Methods</p>
<p>Beijing Academy of Artificial Intelligence
100084BeijingChina</p>
<p>Zhicheng Wei weizhicheng1@huawei.com 
Huawei Cloud</p>
<p>Nicholas Jing Yuan nicholas.jing.yuan@gmail.com 
Huawei Cloud</p>
<p>Ji-Rong Wen jrwen@ruc.edu.cn 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>School of Information
Renmin University of China</p>
<p>Beijing Key Laboratory of Big Data Management and Analysis Methods</p>
<p>Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models</p>
<p>This paper studies how to automatically generate a natural language text that describes the facts in knowledge graph (KG). Considering the few-shot setting, we leverage the excellent capacities of pretrained language models (PLMs) in language understanding and generation. We make three major technical contributions, namely representation alignment for bridging the semantic gap between KG encodings and PLMs, relation-biased KG linearization for deriving better input representations, and multi-task learning for learning the correspondence between KG and text. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our model on KG-to-text generation task. In particular, our model outperforms all comparison methods on both fully-supervised and fewshot settings. Our code and datasets are available at https://github.com/RUCAIBox/ Few-Shot-KG2Text. * Corresponding author Jack Kirby c r e a to r Stan Lee c r e a to r Stan Lee fo u n d e r</p>
<p>Introduction</p>
<p>Knowledge graphs (KGs), such as Wikidata and DBpedia, are essential for many natural language processing (NLP) applications (Ji et al., 2020). To understand the structured information in KG, the task of KG-to-text generation has been proposed to automatically generate a descriptive text for a given knowledge graph (Koncel-Kedziorski et al., 2019;Ribeiro et al., 2020a). Figure 1 illustrates a KG with the corresponding descriptive text, in which the nodes (e.g., Stan Lee and Iron Man) represent entities and the edges (e.g., creator and alias) describe the relations between connected entities.</p>
<p>In recent years, with the help of crowdsourcing platforms and information extraction (IE) systems, large-scale labelled pairs of KG and its descriptive text have been created, such as WikiBio (Lebret et al., 2016) and WebNLG Challenge (Gardent  Figure 1: A knowledge graph (subgraph) with its descriptive text. The underlined words represent the context keywords about entities. et al., 2017). Based on these datasets, data-driven models have shown impressive capabilities to produce informative and fluent text for a given KG Moryossef et al., 2019). However, due to the great expense in annotation process, it is not always feasible to generate large-scale labelled datasets for a variety of domains in practice. Motivated by this, we propose to study the task of few-shot KG-to-text generation that aims to produce satisfactory output text given only a handful of (several hundred) labelled instances.</p>
<p>To fulfil this task, we need to fully understand the complicated semantic relations between entities from various domains, which is challenging with limited labelled data. Our solution is inspired by the excellent few-shot capabilities of pretrained language models (PLMs) on language understanding and generation tasks (Brown et al., 2020;Chen et al., 2020;Li et al., 2021a). Pretrained on the large-scale corpora, PLMs encode vast amounts of world knowledge into their parameters (Li et al., 2021b), which is potentially beneficial to understand and describe the KG facts in our task.</p>
<p>However, applying PLMs to few-shot KG-totext generation still faces two challenges. First, PLMs are usually pretrained on natural language text, while the KG inputs for our task are structured graphs. This semantic gap makes it difficult to effectively inject KG representations into PLMs especially with limited labelled instances. Second, unlike many other text generation tasks, KG-to-text generation requires faithful generation based on the understanding of KG facts. It needs to learn an accurate semantic correspondence between input KG and output text, which will be more difficult in few-shot settings.</p>
<p>To address the above issues, in this paper, we propose a few-shot KG-to-text generation model based on PLMs. There are three major technical contributions in our model. First, in order to bridge the semantic gap, we enforce the representation alignment by learning the correspondence between KG representations (encoded by graph neural networks) and PLM-based entity representations. Second, to feed KG into PLMs, we propose a relation-biased breadth-first search (RBFS) strategy to linearize KG into a well-planned entity sequence. Finally, we jointly train the primary text generation task and an auxiliary KG reconstruction task under the framework of multi-task learning. This step further enhances the semantic correspondence between input KG and output text, based on which our model can generate faithful text about KG.</p>
<p>To the best of our knowledge, we are the first study to investigate PLMs for few-shot KG-to-text generation. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our few-shot KG-to-text generation model.</p>
<p>Related Work</p>
<p>In this work, we mainly focus on generating text from knowledge graphs using PLMs.</p>
<p>KG-to-Text Generation. Early works mainly centered around statistical methods, applying grammar rules to generate text (Konstas and Lapata, 2013;Flanigan et al., 2016). Recently, neural based approaches have been proposed to generate text from linearized KG triples (Gardent et al., 2017)   presented an unsupervised training method that can iteratively back translate be-tween the text and graph data. Different from them, we explore how to utilize large PLMs for few-shot KG-to-text generation.</p>
<p>Pretrained Language Model. Recent years have witnessed prominent achievement of PLMs in NLP tasks (Devlin et al., 2019;Radford et al., 2019). Pretrained on massive corpora, pretrained models showcase unprecedented generalization ability to solve related downstream tasks (Li et al., 2021b). However, most of existing PLMs were conditioned on text data (Radford et al., 2019;Lewis et al., 2020), lacking consideration of structured data input. Ribeiro et al. (2020b) proposed to utilize PLMs for KG-to-text generation by randomly linearizing graph into a sequence of triples. While, these methods do not explicitly model the structural relations of KG, which is critical for generating faithful text. Our work aims to consider the KG structure and bridge the semantic gap between KG encodings and PLMs.</p>
<p>Problem Formulation</p>
<p>KG-to-text generation (Ribeiro et al., 2020a) aims to automatically generate a natural language text that describes the facts in KG.</p>
<p>Formally, the input KG consists of a set of triples, denoted as G = { e, r, e |e, e ∈ E, r ∈ R}, where E and R denote the entity set and relation set, respectively. A triple e, r, e denotes the fact that relation r exists between head entity e and tail entity e . Note that the input KG is a small and compact subgraph extracted from large-scale knowledge graphs (e.g., DBpedia). Following Koncel-Kedziorski et al. (2019), a text describing the input KG is usually available in this task. Let V denote the vocabulary. The target is to generate a natural language text Y = w 1 , ..., w j , ..., w T (w j ∈ V) that represents the correct and concise semantics of entities and their relations in the given knowledge graph. The text contains a set of entity mentions M = {m e |m e = e, s e , o e , e ∈ E}, where e is the target entity, s e and o e are the start and end indices of this mention in text Y, respectively. In other words, w se , ..., w oe specially corresponds to entity e. For entities with multiple mentions in text, we only keep the first mention of each entity in M. By replacing each word of mentions with the token "[MASK]", we can obtain a masked text, denoted as Y [mask] , which is also taken as input for representation alignment in Section 4.1.</p>
<p>In practice, it is difficult to collect massive pairs  Figure 2: Overview of our proposed model. "RA" and "BP" denote representation alignment and back propagation, respectively. We organize the PLM into lower layers and higher layers. The former provides PLMbased entity representations for alignment with KG encodings, and the latter acts as a decoder for generating text and reconstructing KG facts. After representation alignment, KG embeddings can be directly fed into the higher layers of PLMs for generating text.</p>
<p>of KG and its descriptive text for training. In this paper, we study the task of few-shot KG-to-text generation with a handful of training instances (e.g., 200 instances) based on a given PLM (e.g., GPT-2).</p>
<p>Approach</p>
<p>For our task, two major challenges are how to learn effective input representations and capture the semantic correspondence between KG and text. To address the two challenges, we propose three major technical contributions, namely representation alignment between KG encodings and PLMs, relation-biased BFS strategy for KG linearization, and multi-task learning with KG reconstruction. Figure 2 presents an illustrative overview of our model. Next we will describe each part in detail.</p>
<p>Representation Alignment</p>
<p>Unlike previous works (Ribeiro et al., 2020b;) that directly transform KG into text sequence, we employ graph neural network (GNN) as knowledge graph encoder to explicitly encode entity relations in KG. Based on the input KG, GNN would produce a set of entity embeddings, which can be regarded as the input word embeddings of PLM for generating text. However, the GNN-based entity embeddings and the PLM-based word (entity) embeddings come from two distinct semantic spaces. To bridge such a semantic gap, we propose a representation alignment method to align the GNN-based and PLM-based entity embeddings in different semantic spaces.</p>
<p>KG Encoder. The GNN-based KG encoder aims to generate entity embeddings for KG. Let v e ∈ R d E denote the entity embedding for a general entity e in KG, where d E is the embedding size. In our work, the entity embeddings are shared across different KGs and initialized with pretrained KG embeddings (Yang et al., 2015). We apply R- GCN (Schlichtkrull et al., 2018) to generate entity embeddings by leveraging multi-relational information in KG. Then, the embedding of entity e at the l + 1-th layer of R-GCN can be computed as:
v (l+1) e = σ( r∈R e ∈N r e W (l) r v (l) e +W (l) 0 v (l) e ), (1) where W (l) 0 and W (l)
r are trainable matrices, and N r e = {e | e, r, e , e , r, e ∈ G} denotes the set of neighbors of entity e under relation r. Finally, after stacking L times, the output entity embedding v (L) e from the last R-GCN layer is used as the final entity embeddingṽ e .</p>
<p>Note that, we represent an entity as a set of nodes. For instance, the entity Iron Man in Figure 1 will be represented by two nodes: one for the token Iron and the other for the token Man. This would enhance the generalization ability of KG encoder on unseen entities, since it learns entity embeddings at the token level.</p>
<p>Text Encoder. To obtain the PLM-based entity embeddings, we feed the masked text Y [mask] into the text encoder, i.e., the lower layers of PLM. As shown in Figure 1, compared with short entity mentions, the masked text contains rich context information about entities. Therefore, similar to masked language model (Devlin et al., 2019), the embeddings of masked text can be computed as:
v w 1 , ...,v w T = Lower-Layers(Y [mask] ),(2)
where the entity mention m e corresponds to the embedding sequence v ws e , ...,v wo e and the PLMbased entity embeddingv e can be computed by an average pooling over this embedding sequence.</p>
<p>To bridge the semantic gap, we model the representation alignment by minimizing the Euclidean distance in semantic space between the GNN-based and PLM-based entity embeddings as:
L RA = e∈E ṽ e −v e 2 ,(3)
whereṽ e andv e are GNN-based and PLM-based entity embeddings, respectively. With representation alignment, the GNN-based entity embeddings can be aligned with the PLMbased entity embeddings in semantic space, which enables us to effectively inject KG representations into PLM for improving generation quality.</p>
<p>Knowledge Graph Linearization</p>
<p>To feed the KG into decoder (i.e., the higher layers of PLM), we need to linearize KG into an entity sequence. Previous work Ribeiro et al., 2020b) usually relies on random or pre-defined rules, which is not flexible to model KG structures. Here, we propose to utilize breadthfirst search (BFS) strategy to traverse KG. BFS, a graph traversal algorithm, starts at the root node and explores all the nodes at the present layer before moving on to the nodes at the next depth layer 1 . Here, we assume that nodes at the same layer potentially express relevant semantics and should be placed in close positions of the entity sequence.</p>
<p>Furthermore, we observe that some relations are often lexicalized before others, e.g., the nationality of a person often precedes the birthplace in descriptive text. Considering such relation priority, in this paper, we propose a relation-biased breadth first search (RBFS) strategy to traverse and linearize KG into entity sequence. Specifically, we first compute RBFS weights α e for each entity e based on their relations as:
α e = σ(ṽ e W (L) rṽe ), e, r, e ∈ G,(4)
where W (L) r is a relation matrix from Eq. 1. Then, for two sibling entities e and e at the same layer, we traverse e before e if α e is greater than α e , and vice versa. Finally, through RBFS, we can obtain a linearized entity sequence taken as input of the decoder for text generation.</p>
<p>KG-enhanced Multi-task Learning</p>
<p>After obtaining the linearized entity sequence, we next take it as input and perform text generation. 1 https://en.wikipedia.org/wiki/Breadth-first_search Different from other text generation tasks, KG-totext generation aims to generate text reflecting the concise facts in KG. Inspired by , we incorporate an auxiliary KG reconstruction task to reconstruct the facts in KG for learning the semantic correspondence between text and KG.</p>
<p>Text Generation. The text generation task is performed upon the higher layers of PLM. The objective is to maximize the likelihood of the reference text, which is equivalent to minimize the negative log-likelihood as:
L LM = − T j=1
log p gen (w j |w 1 , ..., w j−1 ; G), (5) where p gen is the generative probability from PLM. Besides, in KG-to-text generation, some tokens in descriptive text correspond to KG entities shown in Figure 1. The ability to copy entities from KG would enrich the generated text content, which can be achieved by the pointer generator (See et al., 2017). By feeding the hidden states of PLM and the token embedding, the copy probability p j copy of the j-th token w j can be computed as:
p j copy = σ(W 1 s j + W 2 v w j + b copy ),(6)
where W 1 , W 2 , and b copy are trainable parameters, v w j is the embedding of token w j , and s j is the j-th hidden state from the top layer of PLM. Then, we explicitly "teach" our model how to switch between generation and copy via the copy loss as:
L P G = w j p j copy + w k (1 − p k copy ).(7)
Our intuition is aimed at minimizing the copy probability p j copy of token w j (generated from vocabulary) and maximizing the copy probability p k copy of token w k (copied from KG entities).</p>
<p>KG Reconstruction. Following Song et al. (2020), we formalize the KG reconstruction task as predicting the relations between any two entities. In detail, given a head entity e and a tail entity e in generated text, we can obtain the hidden states of their mentions from the top layer of decoder, i.e., s se , ..., s oe and s s e , ..., s o e . Then, the entity hidden states h e and t e can be computed by an average pooling over their mention hidden states. The probability for a relation r is calculated as:
p(r|e, e ) = softmax(W 3 [h e ; t e ; h e t e ] + b 2 ),(8)
where W 3 and b 2 are trainable parameters. The loss for reconstructing KG is also defined as the negative log-likelihood of all target triples in KG:
L GR = − e,r,e ∈G
log p(r|e, e ).</p>
<p>By incorporating the KG reconstruction task, our model is able to capture the semantic correspondence between input KG and output text, which further improves generating faithful text.</p>
<p>Finally, the total training loss consists of text generation loss L LM (Eq. 5), copy loss L P G (Eq. 7), representation alignment loss L RA (Eq. 3) and KG reconstruction loss L GR (Eq. 9) as:
L total = L LM +λ 1 L P G +λ 2 L RA +λ 3 L GR ,(10)
where λ 1 , λ 2 and λ 3 are combination coefficients.</p>
<p>Discussion and Learning</p>
<p>In this part, we present the model discussion and the model optimization.</p>
<p>Few-shot Learning. In few-shot KG-to-text generation, the key lies in how to bridge the semantic gap between KG and PLMs with limited dataset. To achieve this goal, we first utilize representation alignment in Section 4.1 to align the semantic space between KG encodings and PLMs, and then introduce a KG reconstruction task in Section 4.3 to further learn the semantic correspondence between input KG and output text. Besides, we observe that KG entities are often multi-word expressions. To deal with unseen entities in few-shot learning, we employ the Byte Pair Encoding (BPE) (Sennrich et al., 2016) and sub-word vocabulary (Radford et al., 2019) to split entity words into smaller semantic units. Our work is also empowered by the excellent few-shot capacities of PLMs with vast amounts of world knowledge learned from largescale corpora.</p>
<p>Optimization. For PLM, we employ BART-Large model (Lewis et al., 2020). Specially, we adopt the first 6 layers of BART encoder as the lower layers, and the remaining 6 layers of BART encoder and BART decoder as the higher layers. Note that, the target text and text encoder will not be used at test time. In particular, the target text is just used at training time and encoded as PLM-based entity embeddings for representation alignment, while the alignment is not needed at test time. We optimize all parameters according to the total loss in Eq. 10  with the OpenAI AdamW optimizer (Loshchilov and Hutter, 2019). The learning rate, batch size, R-GCN layers and embedding size are set to 1e-5, 20, 2 and 1024, respectively. The weights λ 1 , λ 2 and λ 3 in Eq. 10 are set to 0.7, 0.5 and 0.5, respectively, according to performance on validation set. During inference, we apply the beam search method with a beam size of 8.</p>
<p>Experiments</p>
<p>In this section, we first set up the experiments, and then report the results and analysis.</p>
<p>Experimental Setup</p>
<p>Datasets. To evaluate our model on few-shot KG-to-text generation, we conduct experiments on three benchmarks, including AGENDA (Koncel-Kedziorski et al., 2019), WebNLG (Gardent et al., 2017) and GenWiki Fine . We adopt three large domains (i.e., Airport, Building and Food) for WebNLG and two large domains (i.e., Sports and Games) for GenWiki. Table 1 shows the statistics for each dataset. Each instance of these datasets contains a knowledge graph in the form of triples and a target text describing the graph. The three datasets have originally provided the alignment records from entity mentions to KG entities. Take an example from WebNLG dataset "AGENT-1 is located in PATIENT-1": the entity mention is tagged as "AGENT-1" and the tag "AGENT-1" maps to the entity "11th_Mississippi_Infantry_Monument" in KG. If such alignments are not available, we can utilize entity linking tools (e.g., NER packages) for preprocessing.</p>
<p>Baselines. We make a comparison against five KGto-text generation models:</p>
<p>• GraphWriter (Koncel-Kedziorski et al., 2019) introduces a graph transformer encoder and a sequence decoder for generating text based on KG.</p>
<p>• CGE-LW (Ribeiro et al., 2020a) proposes a graph-to-text model by combining both global and local node aggregation strategies.    • CycleGT  jointly learns two dual tasks (graph-to-text generation and text-tograph relation classification) via cycle training.</p>
<p>• BART-Base/Large (Ribeiro et al., 2020b) linearizes the KG into sequence and applies BART-Base/Large (Lewis et al., 2020) to generate text.</p>
<p>• T5-Base/Large (Ribeiro et al., 2020b) linearizes KG into a triple sequence and employs T5-Base/Large (Raffel et al., 2020) to generate text.</p>
<p>Among these baselines, GraphWriter and CGE-LW are GNN-based generation models; CycleGT is an unsupervised model using cycle training; GPT2-Base/Large and BART-Base/Large are the most relevant comparisons, which also employ PLMs in KGto-text generation. These baselines were trained on the whole training dataset, i.e., all KG-text pairs. Following previous few-shot work (Chen et al., 2020), we train our model on different few-shot settings with training dataset size ranging from 50, 100, 200 to 500. All the comparison methods are optimized based on validation performance. In our model, the entity embeddings of GNN are initialized with pretrained KG embeddings and the GNN weights are transferred from CGE-LW. We also pretrain GNN weights based on the large-scale KG, i.e., Wikipedia. Based on the pretrained entity embeddings and weights, we continue to train our model.</p>
<p>Evaluation Metrics. For performance comparison, we adopt five automatic evaluation metrics widely used by previous graph-to-text work , i.e., BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015 and CHRF++ (Popovic, 2015). Specifically, BLEU-n and ROUGE-n compute the ratios of overlapping n-grams between generated and real text, CIDEr computes the TF-IDF weights for each n-gram in generated/real text, and CHRF++ computes F-score averaged on both character-and word-level n-grams.   Table 2, 3, and 4 present the fully-supervised and few-shot results of our model and other baselines, respectively. First, by combining global and local entity context, CGE-LW performs better than GraphWriter. Furthermore, with two elaborate designed dual tasks, CycleGT becomes the best non-PLM baseline, outperforming GraphWriter and CGE-LW.</p>
<p>Main Results</p>
<p>Second, as the most direct comparison with our model, BART-Base/Large and T5-Base/Large perform better than baselines by leveraging encoded semantics in PLMs, which reveals the feasibility of utilizing PLMs for KG-to-text generation.</p>
<p>Finally, we observe that our model achieves the best performance on both fully-supervised and fewshot settings. Large-scale PLMs can encode world knowledge by reading a large amount of text, making it easier to recover KG facts. Given only a handful of examples, the performances of baselines drop drastically, while the performance of our model only descents slightly. Furthermore, with only 500 labelled instances, our model improves over CGE-LW and CycleGT, and achieves the best performance in most cases. Compared to these PLM-based KG-to-text baselines, we adopt GNN to explicitly encode KG structure and representation alignment to bridge the semantic gap between PLM and GNN. This helps produce effective semantic representations for few-shot learning. Furthermore, we incorporate an auxiliary KG reconstruction task to learn semantic correspondence between input KGs and output text. These results indicate that our model can achieve more superior performance on KG-to-text generation task in a few-shot setting.</p>
<p>Detailed Analysis</p>
<p>Next, we conduct detailed analysis experiments on our model. We only report the test results on WEBNLG dataset with 500 training instances due to similar findings in other datasets.</p>
<p>Ablation Analysis. In our ablation study, we eval-  uate the effect of each loss L P G , L RA and L GR on the overall model performance. Here, we consider three variants:</p>
<p>• w/o PG: the variant removes the copy loss L P G .</p>
<p>• w/o RA: the variant removes the representation alignment loss L RA .</p>
<p>• w/o GR: the variant removes the KG reconstruction loss L GR .</p>
<p>As can be seen from Table 5, by removing any of the three losses, the BLEU/ROUGE/CIDEr performance drops compared to the complete model, especially removing L RA and L GR . The proposed representation alignment bridges the semantic gap between PLM and GNN, which is helpful for adapting KG representations to PLM. The KG reconstruction task learns the correspondence between KG and text ensuring faithful generation about KG. We also observe a small performance drop by removing L P G . It is likely because PLM has learned some common phrase expressions about these KG facts from large-scale pretraining corpus.</p>
<p>KG Linearization Analysis. In Section 4.2, we propose a novel relation-biased BFS (RBFS) strategy to linearize the input KG into entity sequence. To verify the effectiveness of this strategy, we conduct linearization analysis by comparing RBFS with three traversal strategies, including relationbiased depth-first search (RDFS), forest fire search (FFS) and random search (RS). Specifically, RDFS combines both DFS and the relation factor similar Reference asam pedas is a food found in the region of sumatra and malay peninsula in malaysia , the capital of which is putrajaya , and whose ethnic groups include malaysian malay and malaysian chinese .</p>
<p>athens international airport serves the city athens in greece , greek language is spoken in greece and the leaders names in greece are alexis tsipras and nikos voutsis .</p>
<p>BART Linearized KG 1 3 → 1 2 → 1 6 → 2 5 → 2 4 1 2 → 2 4 → 2 5 → 1 3 → 2 6 Generated Text asam pedas is a dish from malaysia and sumatra where the capital is putrajava . malaysian malay and chinese are ethnic groups in sumatra .</p>
<p>athens in greece is led by alexis tsipras and is served by athens international airport greece speaks greek language .</p>
<p>Ours
Linearized KG 1 → 3 → 2 → 6 → 5 → 4 1 → 3 → 2 → 6 → 5 → 4
Generated Text asam pedas comes from the region of sumatra and malay peninsula in malaysia , where the capital is putrajava , malaysian malay and malaysian chinese are ethnic groups .</p>
<p>athens is served by athens international airport in greece , which speaks greek textbflanguage . greece is led by alexis tsipras and nikos voutsis . to RBFS, where DFS starts at the root node and explores as far as possible along each branch before backtracking 2 ; FFS is a randomized version of RBFS randomly exploring all the nodes at the same layer (Leskovec and Faloutsos, 2006); and RS randomly traverses all the nodes in the input KG. By re-training our model with the above three strategies, we report the comparison of BLEU results in Figure 3. It can be observed that, RBFS and FFS strategies achieve better results compared to the rest strategies. Nodes at the same layer tend to express more relevant semantics, thus searching by layer could produce more reasonable and coherent entity sequence especially considering the relations of entities as our RBFS strategy.</p>
<p>Human Evaluation. Following previous work in data-to-text (Chen et al., 2020), we conduct human evaluation on the generated text. We randomly sample 200 KG subgraphs along with corresponding generated text from CGE-LW, BART-Large and our model. In order to reduce the variance caused by human, three workers were asked to score the text with respect to two aspects: Factual correctness and Language naturalness. The first criterion evaluates how well the generated text correctly conveys 2 https://en.wikipedia.org/wiki/Depth-first_search information in the KG, by counting the number of facts in text supported by the KG (denoted as #Supp.) and contradicting with or missing from the KG (denoted as #Cont.). The second criterion evaluates whether the generated text is grammatically correct and fluent. The scoring mechanism adopts a 5-point Likert scale (Likert, 1932), ranging from 1-point ("very terrible") to 5-point ("very satisfying"). We further average the three scores from the three human judges over the 200 inputs. The results in Table 6 show that our model produces more fidelity and fluent texts than previous models. In our approach, the KG reconstruction task and pointer generator enhance the awareness of KG facts and alleviate producing incorrect facts. Also, with some learned common phrase expressions in PLMs, our model can generate natural text while keeping fidelity.</p>
<p>Qualitative Analysis. In this part, we present intuitive explanations why our model performs well. Table 7 presents two descriptions and the corresponding generated entity sequences and texts by BART-Large baseline and our model. As we can see, based on KG linearization, the generated texts by our model show reasonable and similar content sketch with real texts (e.g., peninsula (region)→malaysia (country)→putrajava (capital)).</p>
<p>Besides, the baseline model incorrectly merges and generates unfaithful facts (e.g., malaysia and sumatra) or misses facts (e.g., nikos voutsis), while our model describes all the KG facts correctly. This improvement could be attributed to the KG reconstruction task, which enables our model to learn the correspondence between the input KG facts and output text. Finally, the entity words in our generated text are enriched and connected by meaningful keywords (e.g., entity greek language and keyword speaks). The reason might be that, with the help of representation alignment, the GNN entity embeddings are aligned with the PLM word embeddings.</p>
<p>Conclusion</p>
<p>This paper presented a few-shot KG-to-text generation model based on PLMs. We make three important technical contributions, namely representation alignment for bridging the semantic gap between KG encodings and PLMs, relation-biased KG linearization for deriving better input KG representations, and multi-task learning for learning the correspondence between KG and text. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our few-shot KG-to-text generation model. As future work, we will consider adopting KG-enhanced PLMs (Zhang et al., 2019; for improving the task performance, which explicitly inject knowledge information into PLMs.</p>
<p>, however, unable to model structural information about KG. Many works explored how to encode the graph structure using Graph Neural Networks (GNNs) or Transformers explicitly. Koncel-Kedziorski et al. (2019) leveraged a graph Transformer encoder to compute node representations by attending over local neighborhoods via self-attention. In contrast, Ribeiro et al. (2020a) focused on combining global and local message passing mechanisms based on GNNs, capturing complementary graph contexts.</p>
<p>Iron Man, Tony Stark, Stan LeeKG Reconstruction </p>
<p>Decoder 
(Higher Layers) </p>
<p>Text Encoder 
(Lower Layers) </p>
<p>KG Encoder 
(GNN) </p>
<p>RBFS Strategy </p>
<p>Text 
Generation </p>
<p>Pre-trained 
Language 
Model </p>
<p><MASK> <MASK> is a fictional 
superhero wearing a suit of armor 
and his alter ego is <MASK> 
<MASK>. He was created 
by <MASK> <MASK>. </p>
<p>RA </p>
<p>Iron Man is a fictional superhero 
wearing a suit of armor and his 
alter ego is Tony Stark. He was 
created by Stan Lee. </p>
<p>Stan Lee 
Tony Stark </p>
<p>Iron Man </p>
<p>c r e a t o r 
a li a s </p>
<p>KG </p>
<p>Stan Lee 
Tony Stark </p>
<p>Iron Man </p>
<p>c r e a t o r 
a li a s </p>
<p>KG </p>
<p>No BP </p>
<p>Table 1 :
1Statistics of three datasets.</p>
<p>Table 2 :
2Performance comparisons of different methods for fully-supervised KG-to-text generation under three domains. B-n and R-n are short for BLEU-n and ROUGE-n. Bold and underline fonts denote the best and the second best methods (the same as below).Datasets 
AGENDA 
WEBNLG 
GENWIKI FINE </p>
<h1>Instances</h1>
<p>50 
100 
200 
500 
50 
100 
200 
500 
50 
100 
200 
500 </p>
<p>BART-large 5.71 6.15 
7.59 10.71 
9.05 15.70 19.38 27.91 
9.14 13.38 15.39 24.14 
T5-large 
2.69 2.73 
4.65 
7.52 
7.18 14.52 16.88 21.68 
6.30 
6.36 10.37 17.72 
Ours 
6.22 9.40 10.21 17.93 10.60 17.46 20.00 31.79 10.75 14.44 16.84 28.89 </p>
<p>Table 3 :
3BLEU-4 results of different methods for few-shot KG-to-text generation under three domains. To mitigate the randomized effects of samples, we report the average results over five training runs (the same as below).Datasets 
AGENDA 
WEBNLG 
GENWIKI FINE </p>
<h1>Instances</h1>
<p>50 
100 
200 
500 
50 
100 
200 
500 
50 
100 
200 
500 </p>
<p>BART-large 14.33 15.28 16.94 20.70 22.57 26.21 30.68 49.34 26.59 29.60 34.56 47.50 
T5-large 
14.11 14.17 15.88 21.72 20.80 22.71 24.18 38.36 21.02 21.36 20.07 35.72 
Ours 
15.10 16.65 18.88 25.72 24.80 28.38 33.12 55.13 28.02 31.36 38.07 50.72 </p>
<p>Table 4 :
4ROUGE-L results of different methods for few-shot KG-to-text generation under three domains.</p>
<p>Table 5 :
5Ablation analysis on WEBNLG dataset.</p>
<p>Figure 3: Linearization analysis on WEBNLG dataset.RBFS-Train RDFS-Train 
FFS-Train 
RS-Train 
20 </p>
<p>25 </p>
<p>30 </p>
<p>35 </p>
<p>BLEU </p>
<p>RBFS-Test 
RDFS-Test </p>
<p>FFS-Test 
RS-Test </p>
<p>Models </p>
<h1>Supp.↑ #Cont.↓ Naturalness↑</h1>
<p>Gold 
4.40 
0.36 
4.26 
Ours 
3.77 
1.01 
3.96 
BART-Large 
3.20 
1.90 
3.55 
CEG-LW 
2.87 
2.13 
2.56 </p>
<p>Table 6 :
6Human evaluation on WEBNLG dataset. Co-
hen's kappa coefficients for labelling three factors are 
as follows: 0.78, 0.71, and 0.75. </p>
<p>Table 7 :
7Sample text generated by BART-Large baseline and our model from the Food and Airport domains of the WEBNLG benchmark. Since BART linearizes KG as triple sequence and an entity may involve in several triples, there are repeated entities used by BART (we omit the relations between entities). Bold and underlined words correspond to entity words and keywords.</p>
<p>Mc-Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam2020December 6-12, 2020, virtualTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc- Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learn- ers. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Informa- tion Processing Systems 2020, NeurIPS 2020, De- cember 6-12, 2020, virtual.</p>
<p>Few-shot NLG with pre-trained language model. Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, William Yang Wang, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online. the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, OnlineAssociation for Computational LinguisticsZhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, and William Yang Wang. 2020. Few-shot NLG with pre-trained language model. In Proceedings of the 58th Annual Meeting of the Association for Com- putational Linguistics, ACL 2020, Online, July 5- 10, 2020, pages 183-190. Association for Compu- tational Linguistics.</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL-HLT 2019. Minneapolis, MN, USA1Long and Short PapersAssociation for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language under- standing. In NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Pa- pers), pages 4171-4186. Association for Computa- tional Linguistics.</p>
<p>Generation from abstract meaning representation using tree transducers. Jeffrey Flanigan, Chris Dyer, Noah A Smith, Jaime G Carbonell, NAACL HLT 2016. San Diego California, USAThe Association for Computational LinguisticsJeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime G. Carbonell. 2016. Generation from abstract meaning representation using tree transducers. In NAACL HLT 2016, San Diego California, USA, June 12-17, 2016, pages 731-739. The Association for Computational Linguistics.</p>
<p>The webnlg challenge: Generating text from RDF data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationSantiago de Compostela, Spain2017Association for Computational LinguisticsClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The webnlg challenge: Generating text from RDF data. In Pro- ceedings of the 10th International Conference on Natural Language Generation, INLG 2017, Santi- ago de Compostela, Spain, September 4-7, 2017, pages 124-133. Association for Computational Lin- guistics.</p>
<p>Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training. Qipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang, David Wipf, Zheng Zhang, arXiv:2006.04702arXiv preprintQipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang, David Wipf, and Zheng Zhang. 2020. Cy- clegt: Unsupervised graph-to-text and text-to-graph generation via cycle training. arXiv preprint arXiv:2006.04702.</p>
<ol>
<li>A survey on knowledge graphs: Representation, acquisition and applications. Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, Philip S Yu, abs/2002.00388CoRRShaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Martti- nen, and Philip S. Yu. 2020. A survey on knowledge graphs: Representation, acquisition and applications. CoRR, abs/2002.00388.</li>
</ol>
<p>Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation. Zhijing Jin, Qipeng Guo, Xipeng Qiu, Zheng Zhang, De- cember 8-13Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (Online2020International Committee on Computational LinguisticsZhijing Jin, Qipeng Guo, Xipeng Qiu, and Zheng Zhang. 2020. Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation. In Proceedings of the 28th International Conference on Computational Linguis- tics, COLING 2020, Barcelona, Spain (Online), De- cember 8-13, 2020, pages 2398-2409. International Committee on Computational Linguistics.</p>
<p>Text generation from knowledge graphs with graph transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics1Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text generation from knowledge graphs with graph transformers. In Proceedings of the 2019 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, NAACL-HLT 2019, Minneapo- lis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2284-2293. Association for Computational Linguistics.</p>
<p>Inducing document plans for concept-to-text generation. Ioannis Konstas, Mirella Lapata, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingGrand Hyatt Seattle, Seattle, Washington, USAACL2013A meeting of SIGDAT, a Special Interest Group of the ACLIoannis Konstas and Mirella Lapata. 2013. Inducing document plans for concept-to-text generation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1503-1514. ACL.</p>
<p>Neural text generation from structured data with application to the biography domain. Rémi Lebret, David Grangier, Michael Auli, The Association for Computational Linguistics. Austin, Texas, USAEMNLP 2016Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with ap- plication to the biography domain. In EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 1203-1213. The Association for Computational Lin- guistics.</p>
<p>Sampling from large graphs. Jure Leskovec, Christos Faloutsos, Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data MiningPhiladelphia, PA, USAACMJure Leskovec and Christos Faloutsos. 2006. Sampling from large graphs. In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowl- edge Discovery and Data Mining, Philadelphia, PA, USA, August 20-23, 2006, pages 631-636. ACM.</p>
<p>BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational Linguistics2020Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7871-7880. Association for Computational Linguistics.</p>
<p>TextBox: A unified, modularized, and extensible framework for text generation. Junyi Li, Tianyi Tang, Gaole He, Jinhao Jiang, Xiaoxuan Hu, Puzhao Xie, Zhipeng Chen, Zhuohao Yu, Wayne Xin Zhao, Ji-Rong Wen, ACL. Junyi Li, Tianyi Tang, Gaole He, Jinhao Jiang, Xi- aoxuan Hu, Puzhao Xie, Zhipeng Chen, Zhuohao Yu, Wayne Xin Zhao, and Ji-Rong Wen. 2021a. TextBox: A unified, modularized, and extensible framework for text generation. In ACL.</p>
<p>Pretrained language models for text generation: A survey. Junyi Li, Tianyi Tang, Wayne Xin Zhao, Ji-Rong Wen, Proceedings of the 30th International Joint Conference on Artificial Intelligence. the 30th International Joint Conference on Artificial IntelligenceIJCAI 2021Junyi Li, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong Wen. 2021b. Pretrained language models for text generation: A survey. In Proceedings of the 30th International Joint Conference on Artificial Intelli- gence, IJCAI 2021.</p>
<p>A technique for the measurement of attitudes. Archives of psychology. Rensis Likert, Rensis Likert. 1932. A technique for the measurement of attitudes. Archives of psychology.</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Hierarchical encoder with auxiliary supervision for neural tableto-text generation: Learning better representation for tables. Tianyu Liu, Fuli Luo, Qiaolin Xia, Shuming Ma, Baobao Chang, Zhifang Sui, AAAI 2019, IAAI 2019, AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019. Honolulu, Hawaii, USAAAAI PressTianyu Liu, Fuli Luo, Qiaolin Xia, Shuming Ma, Baobao Chang, and Zhifang Sui. 2019. Hierarchical encoder with auxiliary supervision for neural table- to-text generation: Learning better representation for tables. In AAAI 2019, IAAI 2019, AAAI Sym- posium on Educational Advances in Artificial Intelli- gence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -February 1, 2019, pages 6786-6793. AAAI Press.</p>
<p>Barack's wife hillary: Using knowledge graphs for fact-aware language modeling. Robert Logan, F Nelson, Matthew E Liu, Matt Peters, Sameer Gardner, Singh, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsRobert Logan, Nelson F Liu, Matthew E Peters, Matt Gardner, and Sameer Singh. 2019. Barack's wife hillary: Using knowledge graphs for fact-aware lan- guage modeling. In Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics, pages 5962-5971.</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USAOpenReview.netIlya Loshchilov and Frank Hutter. 2019. Decou- pled weight decay regularization. In 7th Inter- national Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Step-by-step: Separating planning from realization in neural data-to-text generation. Amit Moryossef, Yoav Goldberg, Ido Dagan, NAACL-HLT 2019. Minneapolis, MN, USAAssociation for Computational LinguisticsAmit Moryossef, Yoav Goldberg, and Ido Dagan. 2019. Step-by-step: Separating planning from realization in neural data-to-text generation. In NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Vol- ume 1 (Long and Short Papers), pages 2267-2277. Association for Computational Linguistics.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th annual meeting of the Association for Compu- tational Linguistics, pages 311-318.</p>
<p>Knowledge enhanced contextual word representations. Matthew E Peters, Mark Neumann, Robert L Logan, I V , Roy Schwartz, Vidur Joshi, Sameer Singh, Noah A Smith, EMNLP-IJCNLP. Hong Kong, ChinaAssociation for Computational LinguisticsMatthew E. Peters, Mark Neumann, Robert L. Logan IV, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. 2019. Knowledge enhanced con- textual word representations. In EMNLP-IJCNLP, Hong Kong, China, November 3-7, 2019, pages 43- 54. Association for Computational Linguistics.</p>
<p>chrf: character n-gram f-score for automatic MT evaluation. Maja Popovic, Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine TranslationLisbon, Portugal2015The Association for Computer LinguisticsMaja Popovic. 2015. chrf: character n-gram f-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, WMT@EMNLP 2015, 17-18 September 2015, Lis- bon, Portugal, pages 392-395. The Association for Computer Linguistics.</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 189Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former.</p>
<p>Claire Gardent, and Iryna Gurevych. 2020a. Modeling global and local node contexts for text generation from knowledge graphs. F R Leonardo, Yue Ribeiro, Zhang, Trans. Assoc. Comput. Linguistics. 8Leonardo F. R. Ribeiro, Yue Zhang, Claire Gardent, and Iryna Gurevych. 2020a. Modeling global and local node contexts for text generation from knowl- edge graphs. Trans. Assoc. Comput. Linguistics, 8:589-604.</p>
<p>Hinrich Schutze, and Iryna Gurevych. 2020b. Investigating pretrained language models for graph-to-text generation. F R Leonardo, Martin Ribeiro, Schmitt, arXiv:2007.08426arXiv preprintLeonardo FR Ribeiro, Martin Schmitt, Hinrich Schutze, and Iryna Gurevych. 2020b. Investigating pre- trained language models for graph-to-text genera- tion. arXiv preprint arXiv:2007.08426.</p>
<p>Modeling relational data with graph convolutional networks. Michael Sejr, Thomas N Schlichtkrull, Peter Kipf, Rianne Bloem, Van Den, Ivan Berg, Max Titov, Welling, The Semantic Web -15th International Conference. Heraklion, Crete, GreeceSpringer10843ProceedingsMichael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolutional networks. In The Semantic Web -15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3-7, 2018, Proceedings, volume 10843 of Lecture Notes in Computer Science, pages 593-607. Springer.</p>
<p>Get to the point: Summarization with pointer-generator networks. Abigail See, J Peter, Christopher D Liu, Manning, ACL 2017, Vancouver. Long Papers1Association for Computational LinguisticsAbigail See, Peter J. Liu, and Christopher D. Man- ning. 2017. Get to the point: Summarization with pointer-generator networks. In ACL 2017, Vancou- ver, Canada, July 30 -August 4, Volume 1: Long Papers, pages 1073-1083. Association for Compu- tational Linguistics.</p>
<p>Neural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016. the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016Berlin1Long Papers. The Association for Computer LinguisticsRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Lin- guistics, ACL 2016, August 7-12, 2016, Berlin, Ger- many, Volume 1: Long Papers. The Association for Computer Linguistics.</p>
<p>Structural information preserving for graph-to-text generation. Linfeng Song, Ante Wang, Jinsong Su, Yue Zhang, Kun Xu, Yubin Ge, Dong Yu, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020OnlineAssociation for Computational LinguisticsLinfeng Song, Ante Wang, Jinsong Su, Yue Zhang, Kun Xu, Yubin Ge, and Dong Yu. 2020. Structural information preserving for graph-to-text generation. In Proceedings of the 58th Annual Meeting of the As- sociation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7987-7998. Associa- tion for Computational Linguistics.</p>
<p>Cider: Consensus-based image description evaluation. C Lawrence Ramakrishna Vedantam, Devi Zitnick, Parikh, IEEE Conference on Computer Vision and Pattern Recognition. Boston, MA, USAIEEE Computer SocietyRamakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In IEEE Conference on Com- puter Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 4566- 4575. IEEE Computer Society.</p>
<p>Embedding entities and relations for learning and inference in knowledge bases. Bishan Yang, Wen-Tau Yih, Xiaodong He, Jianfeng Gao, Li Deng, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Embedding entities and relations for learning and inference in knowledge bases. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Pinar Donmez, and Sonal Gupta. 2020. Improving text-to-text pretrained models for the graph-to-text task. Zixiaofan Yang, Arash Einolghozati, Hakan Inan, Keith Diedrick, Angela Fan, Proceedings of the 3rd WebNLG Workshop on Natural Language Generation from the Semantic Web. the 3rd WebNLG Workshop on Natural Language Generation from the Semantic WebDublin, IrelandAssociation for Computational LinguisticsZixiaofan Yang, Arash Einolghozati, Hakan Inan, Keith Diedrick, Angela Fan, Pinar Donmez, and Sonal Gupta. 2020. Improving text-to-text pre- trained models for the graph-to-text task. In Pro- ceedings of the 3rd WebNLG Workshop on Natu- ral Language Generation from the Semantic Web (WebNLG+ 2020), Dublin, Ireland (Virtual). Asso- ciation for Computational Linguistics.</p>
<p>ERNIE: enhanced language representation with informative entities. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. the 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational Linguistics1Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: en- hanced language representation with informative en- tities. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Vol- ume 1: Long Papers, pages 1441-1451. Association for Computational Linguistics.</p>            </div>
        </div>

    </div>
</body>
</html>
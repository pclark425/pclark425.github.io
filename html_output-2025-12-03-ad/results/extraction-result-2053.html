<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2053 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2053</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2053</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-278782199</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.14970v1.pdf" target="_blank">Self-Evolving Curriculum for LLM Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation. A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented. While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive. To address these limitations, we propose Self-Evolving Curriculum (SEC) , an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process. Our approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm. We leverage the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain. At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method. Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, our experiments demonstrate that SEC significantly improves models’ reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems. Additionally, our approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains. These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs. 2</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2053.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2053.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SEC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evolving Curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic, learned curriculum-learning method for RL fine-tuning of LLMs that treats curriculum selection as a non-stationary Multi-Armed Bandit (MAB) and uses the batch-average absolute advantage (proxyed from policy-gradient gradient magnitude) as a local reward to update a per-category expected return via TD(0).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>learned curriculum (non-stationary MAB)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Training problems are pre-partitioned into N categories (e.g., difficulty levels or problem types). At each RL step the curriculum policy maintains Q_t(c) for each category c and samples categories with probability proportional to Softmax(Q_t(c)/τ). A training batch is created by sampling problems uniformly from selected categories. After running the LLM policy on the batch and estimating advantages per rollout, the curriculum reward for a category is the batch average of |A_t| for rollouts from that category; Q_t(c) is updated by TD(0) as an exponential moving average Q_{t+1}(c) = α r_t(c) + (1-α) Q_t(c). This adaptively prioritizes categories whose problems yield large immediate learning signal (maximized at success rate ≈0.5 under binary verifiable rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Planning (Countdown, Zebra puzzles), Inductive reasoning (ARC-1D), Mathematics (MATH dataset, AMC, AIME)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Benchmarks require multi-step logical reasoning and planning: planning tasks require look-ahead and backtracking with combinatorial search (Countdown: arithmetic combinations; Zebra: constraint-satisfaction over entities/properties); inductive reasoning (ARC-1D) requires rule induction and generalization to longer sequences; mathematics requires long multi-step deduction and domain knowledge (textbook-style contest problems). Datasets include clear difficulty gradations (explicit in MATH; controlled by input length / problem size in others), are episodic with verifiable outcome rewards, and include both in-distribution and held-out harder (OOD) levels.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Conditions on the evolving LLM policy's on-policy signals: per-category batch-average absolute advantage computed from rollouts (which implicitly depends on current model parameters, success rates p, and reward statistics). The curriculum is therefore driven by model learning progress/performance rather than environment state like inventory or location.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Used alongside policy-gradient RL fine-tuning (GRPO primary; also evaluated with PPO and REINFORCE-Leave-One-Out (RLOO)), advantage estimation via multiple rollouts per problem, Boltzmann sampling (Softmax over Q_t/τ) for exploration-exploitation, and standard optimization (Adam).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td>The paper compares SEC to a manually-ordered difficulty curriculum (fixed easy→hard schedule). SEC outperforms the ordered manual curriculum on most benchmarks; ordered (fixed) curricula often underperform because they do not adapt to model progress and can over-expose models to easy problems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>Main heuristic baseline is random (uniform sampling across training categories). SEC yields consistent relative improvements over random: the paper reports aggregate relative gains such as +13% (Countdown), +21% (Zebra puzzles), +22% (ARC-1D), and up to +33% (AIME24). Example concrete numbers reported for larger model (Qwen2.5-7B): Zebra OOD improves ≈11% relative (0.32 → 0.36) and AIME improves ≈27% relative (0.14 → 0.18) compared to random. SEC-2D (joint type×difficulty arms) also outperforms random in a multi-task mixed training setting and avoids a mid-training collapse observed with random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td>Random/uniform curriculum baseline provided; SEC consistently improves over this baseline (see earlier field). Exact per-task numbers are in the paper's Table 1 and Table 2 (see Tables: SEC vs Random vs Ordered; Table 2 shows SEC improves Countdown under PPO and RLOO).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>SEC improves generalization to held-out harder OOD difficulty levels across domains; reported OOD improvements include the relative gains noted above (e.g., +13% Countdown, +21% Zebra, etc.). SEC also balances skills better in multi-task fine-tuning and produces more stable OOD performance over training than random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>The paper notes SEC is cheaper than online filtering methods that require repeatedly generating additional on-policy samples (SEC only computes rewards using the rollouts already generated for policy updates). Hyperparameters (α, τ) and rollout counts (8 rollouts per problem) are reported; wall-clock/GPU costs are given indirectly by noting experiments ran on 4–8 H100 80GB GPUs. No explicit dollar/API-cost metrics are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Requires predefined problem categories (effectiveness with automatically inferred or ill-defined categories not explored). Introduces extra hyperparameters (temperature τ and learning rate α) that need tuning. For stronger base models (Qwen2.5-7B) SEC is sometimes similar to random on easier tasks (because stronger models already handle harder problems), although SEC still helps on more challenging tasks. No explicit novelty/diversity mechanism is used, so SEC could potentially over-focus if category granularity is poor.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Domains include long multi-step reasoning (mathematics) and planning requiring backtracking (Countdown/Zebra); SEC improves generalization and performance on these long-horizon reasoning tasks notably (e.g., large relative gains on AIME and ARC-1D OOD), indicating it helps long-horizon reasoning in these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>On mathematics (specialized domain requiring contest-level knowledge), SEC yields substantial gains: across MATH-related evaluations SEC improves in-distribution and OOD accuracy (AIME24 relative improvement up to 33% reported). The method handles imbalanced difficulty distributions in MATH training by adaptively sampling categories.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td>Evaluated with two target LLM sizes being fine-tuned (Qwen2.5-3B and Qwen2.5-7B). SEC yields stronger and more consistent gains for the smaller 3B model; for the 7B model gains are smaller or similar to random on some easier tasks but still notable on harder domains (e.g., Zebra and AIME).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Adaptive, learned curriculum (SEC) that prioritizes categories yielding high absolute advantage (proxying immediate learning gain) consistently outperforms random and fixed/manual ordered curricula across diverse reasoning tasks; SEC improves OOD generalization (reported relative gains: Countdown +13%, Zebra +21%, ARC-1D +22%, AIME up to +33%), is robust across RL algorithms (GRPO, PPO, RLOO), and stabilizes multi-task fine-tuning (SEC-2D avoids mid-training collapse seen with random sampling). SEC requires predefined categories and hyperparameter tuning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2053.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2053.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WEBRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WEBRL (self-evolving online curriculum for LM-based web agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior work that uses prompting of an external LLM to autonomously generate new web-agent tasks online, based on previous failures, as a self-evolving curriculum for training LM-based web agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training llm web agents via self-evolving online curriculum reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated (task generation via prompting an external LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>An external LLM is prompted to generate new tasks for web-agent training based on the agent's prior failures, producing an evolving set of training goals/tasks that adapt to agent weaknesses. The paper references WEBRL as an example of LLM-driven task generation rather than implementing it.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Web agents / web environment (LM-based agents acting in web-like tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Open-ended, high diversity, multi-step interactive web tasks; likely to require exploration, varied tool usage and diverse goal formulations (as implied by 'web agents').</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported in this paper; WEBRL is only cited and its computational cost/details are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Referenced as an approach but not evaluated or compared directly in this paper; potential failure modes (e.g., quality of LLM-generated tasks, cost of LLM prompts) are not discussed in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Mentioned as an LLM-driven approach to autonomously generate tasks for web agents; paper does not supply comparative performance numbers or evaluations of WEBRL within its own experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2053.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2053.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-assisted difficulty estimation (Shi et al. / AdaRFT reference)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting an expert LLM to estimate problem difficulty (as a preprocessing step)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced technique where, in absence of explicit difficulty labels, an expert LLM is prompted in preprocessing to estimate or annotate problem difficulty levels for use in curriculum construction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-assisted preprocessing / difficulty estimation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Difficulty estimates are produced by prompting an external 'expert' LLM over problems and using those annotations to partition problems into difficulty categories; these categories can then be used by downstream curriculum strategies (e.g., SEC or AdaRFT). Mentioned as an alternative to empirical accuracy-based difficulty labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>General reasoning datasets (used where explicit difficulty annotations are missing)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Applies to reasoning-focused datasets where per-instance difficulty can be estimated (e.g., MATH-like problems, synthetic reasoning puzzles); helps create category labels when none are present.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified in this paper; invoking an expert LLM in preprocessing implies additional inference/API cost but specifics are not given.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Quality of difficulty estimates depends on the external LLM; preprocessing annotations may not reflect the on-policy difficulty distribution of the fine-tuned LLM and could mismatch model-specific learning dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Cited as a valid approach to obtain difficulty labels when dataset annotations are absent; contrasted with SEC which can instead use empirical on-policy signals (absolute advantage) to adaptively prioritize categories during training.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training llm web agents via self-evolving online curriculum reinforcement learning <em>(Rating: 2)</em></li>
                <li>Online difficulty filtering for reasoning oriented reinforcement learning <em>(Rating: 2)</em></li>
                <li>Efficient reinforcement finetuning via adaptive curriculum learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2053",
    "paper_id": "paper-278782199",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [
        {
            "name_short": "SEC",
            "name_full": "Self-Evolving Curriculum",
            "brief_description": "An automatic, learned curriculum-learning method for RL fine-tuning of LLMs that treats curriculum selection as a non-stationary Multi-Armed Bandit (MAB) and uses the batch-average absolute advantage (proxyed from policy-gradient gradient magnitude) as a local reward to update a per-category expected return via TD(0).",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "learned curriculum (non-stationary MAB)",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "Training problems are pre-partitioned into N categories (e.g., difficulty levels or problem types). At each RL step the curriculum policy maintains Q_t(c) for each category c and samples categories with probability proportional to Softmax(Q_t(c)/τ). A training batch is created by sampling problems uniformly from selected categories. After running the LLM policy on the batch and estimating advantages per rollout, the curriculum reward for a category is the batch average of |A_t| for rollouts from that category; Q_t(c) is updated by TD(0) as an exponential moving average Q_{t+1}(c) = α r_t(c) + (1-α) Q_t(c). This adaptively prioritizes categories whose problems yield large immediate learning signal (maximized at success rate ≈0.5 under binary verifiable rewards).",
            "domain_name": "Planning (Countdown, Zebra puzzles), Inductive reasoning (ARC-1D), Mathematics (MATH dataset, AMC, AIME)",
            "domain_characteristics": "Benchmarks require multi-step logical reasoning and planning: planning tasks require look-ahead and backtracking with combinatorial search (Countdown: arithmetic combinations; Zebra: constraint-satisfaction over entities/properties); inductive reasoning (ARC-1D) requires rule induction and generalization to longer sequences; mathematics requires long multi-step deduction and domain knowledge (textbook-style contest problems). Datasets include clear difficulty gradations (explicit in MATH; controlled by input length / problem size in others), are episodic with verifiable outcome rewards, and include both in-distribution and held-out harder (OOD) levels.",
            "state_conditioning": true,
            "state_conditioning_details": "Conditions on the evolving LLM policy's on-policy signals: per-category batch-average absolute advantage computed from rollouts (which implicitly depends on current model parameters, success rates p, and reward statistics). The curriculum is therefore driven by model learning progress/performance rather than environment state like inventory or location.",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Used alongside policy-gradient RL fine-tuning (GRPO primary; also evaluated with PPO and REINFORCE-Leave-One-Out (RLOO)), advantage estimation via multiple rollouts per problem, Boltzmann sampling (Softmax over Q_t/τ) for exploration-exploitation, and standard optimization (Adam).",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": "The paper compares SEC to a manually-ordered difficulty curriculum (fixed easy→hard schedule). SEC outperforms the ordered manual curriculum on most benchmarks; ordered (fixed) curricula often underperform because they do not adapt to model progress and can over-expose models to easy problems.",
            "performance_heuristic_curriculum": "Main heuristic baseline is random (uniform sampling across training categories). SEC yields consistent relative improvements over random: the paper reports aggregate relative gains such as +13% (Countdown), +21% (Zebra puzzles), +22% (ARC-1D), and up to +33% (AIME24). Example concrete numbers reported for larger model (Qwen2.5-7B): Zebra OOD improves ≈11% relative (0.32 → 0.36) and AIME improves ≈27% relative (0.14 → 0.18) compared to random. SEC-2D (joint type×difficulty arms) also outperforms random in a multi-task mixed training setting and avoids a mid-training collapse observed with random sampling.",
            "performance_no_curriculum": "Random/uniform curriculum baseline provided; SEC consistently improves over this baseline (see earlier field). Exact per-task numbers are in the paper's Table 1 and Table 2 (see Tables: SEC vs Random vs Ordered; Table 2 shows SEC improves Countdown under PPO and RLOO).",
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": "SEC improves generalization to held-out harder OOD difficulty levels across domains; reported OOD improvements include the relative gains noted above (e.g., +13% Countdown, +21% Zebra, etc.). SEC also balances skills better in multi-task fine-tuning and produces more stable OOD performance over training than random sampling.",
            "computational_cost": "The paper notes SEC is cheaper than online filtering methods that require repeatedly generating additional on-policy samples (SEC only computes rewards using the rollouts already generated for policy updates). Hyperparameters (α, τ) and rollout counts (8 rollouts per problem) are reported; wall-clock/GPU costs are given indirectly by noting experiments ran on 4–8 H100 80GB GPUs. No explicit dollar/API-cost metrics are provided.",
            "failure_modes_limitations": "Requires predefined problem categories (effectiveness with automatically inferred or ill-defined categories not explored). Introduces extra hyperparameters (temperature τ and learning rate α) that need tuning. For stronger base models (Qwen2.5-7B) SEC is sometimes similar to random on easier tasks (because stronger models already handle harder problems), although SEC still helps on more challenging tasks. No explicit novelty/diversity mechanism is used, so SEC could potentially over-focus if category granularity is poor.",
            "long_horizon_performance": "Domains include long multi-step reasoning (mathematics) and planning requiring backtracking (Countdown/Zebra); SEC improves generalization and performance on these long-horizon reasoning tasks notably (e.g., large relative gains on AIME and ARC-1D OOD), indicating it helps long-horizon reasoning in these benchmarks.",
            "specialized_domain_performance": "On mathematics (specialized domain requiring contest-level knowledge), SEC yields substantial gains: across MATH-related evaluations SEC improves in-distribution and OOD accuracy (AIME24 relative improvement up to 33% reported). The method handles imbalanced difficulty distributions in MATH training by adaptively sampling categories.",
            "ablation_studies": null,
            "model_size_scaling": "Evaluated with two target LLM sizes being fine-tuned (Qwen2.5-3B and Qwen2.5-7B). SEC yields stronger and more consistent gains for the smaller 3B model; for the 7B model gains are smaller or similar to random on some easier tasks but still notable on harder domains (e.g., Zebra and AIME).",
            "key_findings_curriculum_effectiveness": "Adaptive, learned curriculum (SEC) that prioritizes categories yielding high absolute advantage (proxying immediate learning gain) consistently outperforms random and fixed/manual ordered curricula across diverse reasoning tasks; SEC improves OOD generalization (reported relative gains: Countdown +13%, Zebra +21%, ARC-1D +22%, AIME up to +33%), is robust across RL algorithms (GRPO, PPO, RLOO), and stabilizes multi-task fine-tuning (SEC-2D avoids mid-training collapse seen with random sampling). SEC requires predefined categories and hyperparameter tuning.",
            "uuid": "e2053.0"
        },
        {
            "name_short": "WEBRL",
            "name_full": "WEBRL (self-evolving online curriculum for LM-based web agents)",
            "brief_description": "A referenced prior work that uses prompting of an external LLM to autonomously generate new web-agent tasks online, based on previous failures, as a self-evolving curriculum for training LM-based web agents.",
            "citation_title": "Training llm web agents via self-evolving online curriculum reinforcement learning",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated (task generation via prompting an external LLM)",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "An external LLM is prompted to generate new tasks for web-agent training based on the agent's prior failures, producing an evolving set of training goals/tasks that adapt to agent weaknesses. The paper references WEBRL as an example of LLM-driven task generation rather than implementing it.",
            "domain_name": "Web agents / web environment (LM-based agents acting in web-like tasks)",
            "domain_characteristics": "Open-ended, high diversity, multi-step interactive web tasks; likely to require exploration, varied tool usage and diverse goal formulations (as implied by 'web agents').",
            "state_conditioning": null,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": null,
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": null,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": "Not reported in this paper; WEBRL is only cited and its computational cost/details are not provided here.",
            "failure_modes_limitations": "Referenced as an approach but not evaluated or compared directly in this paper; potential failure modes (e.g., quality of LLM-generated tasks, cost of LLM prompts) are not discussed in detail here.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Mentioned as an LLM-driven approach to autonomously generate tasks for web agents; paper does not supply comparative performance numbers or evaluations of WEBRL within its own experiments.",
            "uuid": "e2053.1"
        },
        {
            "name_short": "LLM-assisted difficulty estimation (Shi et al. / AdaRFT reference)",
            "name_full": "Prompting an expert LLM to estimate problem difficulty (as a preprocessing step)",
            "brief_description": "A referenced technique where, in absence of explicit difficulty labels, an expert LLM is prompted in preprocessing to estimate or annotate problem difficulty levels for use in curriculum construction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-assisted preprocessing / difficulty estimation",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "Difficulty estimates are produced by prompting an external 'expert' LLM over problems and using those annotations to partition problems into difficulty categories; these categories can then be used by downstream curriculum strategies (e.g., SEC or AdaRFT). Mentioned as an alternative to empirical accuracy-based difficulty labeling.",
            "domain_name": "General reasoning datasets (used where explicit difficulty annotations are missing)",
            "domain_characteristics": "Applies to reasoning-focused datasets where per-instance difficulty can be estimated (e.g., MATH-like problems, synthetic reasoning puzzles); helps create category labels when none are present.",
            "state_conditioning": null,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": null,
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": null,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": "Not quantified in this paper; invoking an expert LLM in preprocessing implies additional inference/API cost but specifics are not given.",
            "failure_modes_limitations": "Quality of difficulty estimates depends on the external LLM; preprocessing annotations may not reflect the on-policy difficulty distribution of the fine-tuned LLM and could mismatch model-specific learning dynamics.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Cited as a valid approach to obtain difficulty labels when dataset annotations are absent; contrasted with SEC which can instead use empirical on-policy signals (absolute advantage) to adaptively prioritize categories during training.",
            "uuid": "e2053.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training llm web agents via self-evolving online curriculum reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Online difficulty filtering for reasoning oriented reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Efficient reinforcement finetuning via adaptive curriculum learning",
            "rating": 1
        }
    ],
    "cost": 0.011158749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-Evolving Curriculum for LLM Reasoning
29 May 2025</p>
<p>Xiaoyin Chen xiaoyin.chen@mila.quebec 
ServiceNow Research</p>
<p>Jiarui Lu 
ServiceNow Research</p>
<p>Minsu Kim 
ServiceNow Research</p>
<p>Dinghuai Zhang 
ServiceNow Research</p>
<p>Jian Tang 
ServiceNow Research</p>
<p>Alexandre Piché 
ServiceNow Research</p>
<p>Nicolas Gontier 
ServiceNow Research</p>
<p>Yoshua Bengio 
ServiceNow Research</p>
<p>Ehsan Kamalloo 
ServiceNow Research</p>
<p>Mila -Quebec 
ServiceNow Research</p>
<p>A I Institute 
ServiceNow Research</p>
<p>Université De Montréal 
ServiceNow Research</p>
<p>Kaist 
ServiceNow Research</p>
<p>Microsoft Research 
ServiceNow Research</p>
<p>Hec Montréal 
ServiceNow Research</p>
<p>Self-Evolving Curriculum for LLM Reasoning
29 May 20251D21B7AC3E1E80621CC68FCD46C5E39AarXiv:2505.14970v2[cs.AI]
Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation.A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented.While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive.To address these limitations, we propose Self-Evolving Curriculum (SEC), an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process.Our approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm.We leverage the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain.At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method.Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, our experiments demonstrate that SEC significantly improves models' reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems.Additionally, our approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains.These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs. 2 * Work done while at ServiceNow Research. 2Our code will be publicly released soon.Preprint.Under review.LLM Policy1 Expected Learning Gain 2 3 4 Curriculum Policy Training Batch Curriculum Reward Training Data Rewards Per Category RL Step RL with Verifiable Rewardsto the current MAB policy, which is subsequently updated on-the-fly using the reward obtained from the current training step via the TD(0) method[48].Our experiments demonstrate that SEC significantly improves model reasoning capabilities across three distinct domains: planning, inductive reasoning, and mathematics, particularly improving generalization to challenging out-of-distribution problems.Compared to the standard random curriculum, SEC achieves substantial relative improvements, such as 13% on Countdown, 21% on Zebra puzzles, 22% on ARC-1D, and up to 33% on the AIME24 dataset.When fine-tuned simultaneously across multiple reasoning domains, SEC effectively balances performance across tasks, underscoring its strength as an automatic curriculum learning strategy for RL fine-tuning of LLMs.</p>
<p>1 Introduction Reinforcement learning (RL) has emerged as a central technique for fine-tuning large language models (LLMs) [28,35,12], significantly improving their reasoning capabilities.Recent advances demonstrate notable success, particularly in domains where verifying generation correctness is straightforward [27], such as mathematics and code generation.By optimizing LLMs with rewards solely defined by verifiable outcomes, RL fine-tuning encourages the emergence of complex reasoning behaviors, including selfcorrection and back-tracking strategies [25,63,16], that substantially enhance reasoning performance.</p>
<p>A critical factor influencing the effectiveness of RL fine-tuning is the training curriculum [7], i.e., the or- der in which training data is presented.Since online RL inherently depends on the policy model itself to produce high-quality training trajectories, aligning the curriculum with the model's current learning progress is critical.Ideally, such an alignment enables the model to continually encounter problems that yield maximal learning outcomes [42,29,20].To illustrate this point concretely, we conduct a controlled experiment using the Countdown game, 3 deliberately employing a suboptimal (reverse) curriculum, in which problems are arranged from hard to easy.As shown in Figure 1, the resulting model performs poorly on the test set and exhibits minimal generalization to more challenging outof-distribution (OOD) problems.In contrast, when trained with a baseline random curriculum, where problems of varying difficulty are drawn uniformly at random, the model demonstrates significantly improved generalization and overall task performance.</p>
<p>Although random curriculum serves as a reasonable baseline, it naturally raises the question: Can we design more effective curriculum strategies?Curriculum learning [7] addresses precisely this challenge by seeking to optimize the sequencing of training tasks, thereby enhancing learning efficiency and efficacy.Recent approaches to curriculum learning for RL fine-tuning typically involve either manually crafted curricula designed upfront [50,47,58] or dynamic online filtering based on on-policy samples [64,3].However, manually designed curricula rely heavily on heuristics, demanding human intervention for new models or tasks; conversely, online filtering methods incur substantial computational overhead due to the continuous generation of additional on-policy samples.</p>
<p>In this paper, we propose Self-Evolving Curriculum (SEC) (Figure 2), an automatic curriculum learning [39] approach for RL fine-tuning of LLMs.Our method adaptively learns a curriculum policy concurrently with the RL fine-tuning process, formulating curriculum selection as a non-stationary Multi-Armed Bandit (MAB) problem [51,49,31].Each curriculum category (e.g., difficulty level or problem type) is treated as an individual arm, and the curriculum policy aims to select the arm that maximizes the learning outcomes.Specifically, we operationalize the concept of learning outcomes using the gradient norm, noting that, in policy gradient methods, the gradient norm is weighted by the absolute value of the advantage function.Leveraging this observation, we define the absolute advantage as the reward for each arm.At each RL training step, the curriculum is sampled according</p>
<p>Method</p>
<p>In the context of RL fine-tuning, at each training step t, the curriculum policy selects a subset D t ⊆ D from the training problem set D to be provided to the LLM.In our work, we consider scenarios where the training problems can be categorized into N distinct categories.This assumption simplifies the curriculum optimization problem into learning an expected return Q t (c) that maps category c to a real-valued score (Sec.2.1).The training batch is then constructed by first sampling categories according to the curriculum policy, followed by sampling problems uniformly within the categories.</p>
<p>The goal of the curriculum policy is to maximize the LLM's final task performance.However, directly evaluating such performance would require completing the entire RL fine-tuning process, while the curriculum policy is better to be updated along with the training steps.To resolve this, we introduce a locally measurable reward as a proxy objective for guiding the curriculum policy (Sec.2.2).</p>
<p>Curriculum Selection as Multi-Armed Bandit</p>
<p>Training datasets used for reasoning tasks can often be naturally decomposed into distinct categories.For example, if the dataset spans various reasoning domains, such as mathematics, coding, and planning, these domains naturally form distinct categories.When the dataset is homogeneous in task type or domain, a curriculum can still be constructed by categorizing examples based on in-domain levels, such as difficulty.For instance, the MATH dataset [17] categorizes problems into five distinct difficulty levels based on the guidelines provided by Art of Problem Solving (AoPS).Furthermore, in the absence of explicit difficulty annotations, problem difficulty can be estimated by either using the empirical accuracy of the training LLM or prompting an expert LLM in an additional preprocessing step, as demonstrated by Shi et al. [46].</p>
<p>Motivated by these considerations, we assume that, particularly for reasoning-focused datasets, training problems can be partitioned into N distinct categories C = {c 1 , c 2 , . . ., c N }.Conceptually, the curriculum policy optimization problem can then be viewed as a partially observable Markov decision process (POMDP): the state corresponds to the current LLM policy, actions correspond to curriculum selection, and rewards are defined by observable performance metrics, such as the on-policy performance associated with the selected curriculum.</p>
<p>This POMDP formulation naturally resembles a non-stationary MAB problem, a connection also highlighted by Matiisen et al. [31], where each arm represents a problem category c i , and the objective is to learn the expected return Q t (c) associated with selecting category c at training step t.Importantly, the MAB in this context is non-stationary: the expected reward distribution for each arm shifts as the LLM policy is updated over the course of training.To address this well-studied non-stationary bandit problem [51,49], we leverage the classic TD(0) method [48] to iteratively update Q t (c):
Q t+1 (c) = αr t (c) + (1 − α)Q t (c),(1)
where α is the learning rate, Q 0 (c) = 0 initializes the scores to zero, and r t (c) denotes the reward defined in the next section.Note that this is also known as the Exponential Moving Average.The curriculum policy can then be simply defined over Q t (c), as elaborated in Sec.2.3.</p>
<p>Measuring Learning Outcomes with Absolute Advantage</p>
<p>An ideal curriculum should maximize the LLM's final performance on the test data after an entire training episode.However, directly measuring this objective requires completing a full RL finetuning cycle.Although evaluating intermediate checkpoints can partially mitigate this issue, frequent evaluations are computationally expensive.To overcome this challenge, we introduce a proxy objective that can be efficiently computed locally at each training step.</p>
<p>An intuitive choice for such a proxy objective is to prioritize training data that maximizes the model's immediate learning outcomes [42,29,20], i.e., data that induces large parameter updates.Practically, this can be quantified by measuring the gradient norm of the loss function with respect to the selected training data.Specifically, consider a policy gradient algorithm that optimizes the LLM policy by minimizing the following loss function:
L PG (θ) = − E (st,at)∼π θ log π θ (a t | s t ) A t(2)
where π θ denotes the LLM policy and A t denotes the advantage value.Then, the per-step (s t , a t ) gradient norm is:
∥∇ θ L PG (θ, s t , a t )∥ 2 = E (st,at)∼π θ ∇ θ log π θ (a t | s t ) A t 2 ≈ | A t |∥∇ θ log π θ (a t | s t ) ∥ 2
We observe that the gradient magnitude is weighted by the absolute value of the advantage | A t |.We therefore approximate the learning gain of a curriculum c by the batch-wise expectation of | A t |:
r(c) = E (st,at)∼π θ (xi),xi∼c | A t |(3)
In other words, the reward for curriculum c at each training step is computed as the average absolute advantage across all rollouts associated with the problems drawn from curriculum category c.</p>
<p>Interpreting absolute advantage in RL with Verifiable Rewards.Recent works on RL finetuning for reasoning tasks frequently employ a binary correctness reward (0 for incorrect, 1 for correct) with the Group Relative Policy Optimization (GRPO) algorithm [44,25,12,30,57], a setting known as RL with Verifiable Rewards [27].Here, we show that under this common setting, the expected absolute advantage, as formalized in Equation 3, is maximized when the expected binary reward is 0.5.This means that the highest learning gain occurs when the problem is neither too easy nor too hard for the current model.Prioritizing training on problems with such difficulty aligns naturally with concepts from educational psychology, particularly the Zone of Proximal Development theory [54,8], and connects our approach to broader literature in curriculum learning [14,15,52,3].</p>
<p>Concretely, the GRPO algorithm estimates the advantage by sampling n rollouts from the same problem.The advantage of the i-th rollout is computed as:
A t,i = ri = ri−mean(r) std(r)
, where r i is the binary reward of the i-th rollout, and mean(r) and std(r) denote the mean and standard deviation of the rewards over all n rollouts.Since the reward is binary, it can be modeled as a Bernoulli distribution; thus, mean(r) = p and std(r) = p(1 − p), where p denotes the success rate.The expected absolute advantage can then be expressed as:
E[| A t,i |] = E[|r i |] = E r i − p p(1 − p)(4)
Because r i is Bernoulli, there are only two possible values:
ri =        1 − p p(1 − p) with probability p, −p p(1 − p) with probability 1 − p.(5)
Hence, the expected absolute advantage of the problem is
E |r i | = p 1 − p p(1 − p) + (1 − p) p p(1 − p) = 2 p(1 − p) p(1 − p) = 2 p(1 − p).(6)
We can see that the function g(p) = 2 p(1 − p) is symmetric around p = 0.5 and strictly concave on the interval [0, 1], reaching its maximum at p = 0.5.This derivation shows that maximizing the expected absolute advantage is equivalent to prioritizing problems at a success rate of 0.5.
1: Initialize Q 0 (c) ← 0 ∀ c ∈ C 2: for t ← 0 to T − 1 do 3: B t ← ∅ 4: while |B t | &lt; B do 5: Sample category c ∼ Softmax Q t (c)/τ 6:
Sample problem x uniformly from category c 7:
B t ← B t ∪ {x} 8:
end while
9:
Run π θ on each x ∈ B t to generate rollouts T and compute rewards r with R 10:</p>
<p>Estimate advantages A and update π θ with A(π θ , T , r)
11:
for all c ∈ C do 12:
B c ← { x ∈ B t | x belongs to category c } 13: r t (c) ← 1 |B c | j:xj ∈Bc 1 T j Tj t A t,j 14: Q t+1 (c) ← α r t (c) + (1 − α) Q t (c) 15:
end for 16: end for 17: return Fine-tuned LLM π θ Remarks: The derivation above aims to provide a concrete analysis of our proposed objective under a common RL Fine-tuning setting, as well as to offer intuitive insights into its general behavior.However, this does not imply that our method is limited to this specific scenario.Indeed, our experiments empirically demonstrate that SEC also achieves strong performance when applied with non-binary reward functions (Sec.3.2) and alternative RL algorithms (Sec.3.4).</p>
<p>Self-evolving Curriculum for RL Fine-tuning</p>
<p>At each RL training step, a batch of problems is generated as follows.First, categories are sampled according to a Boltzmann distribution defined by the current values of Q t (c):
p(c) = e Q t (c)/τ N i=1 e Q t (c i )/τ
, where τ is the temperature parameter controlling the exploration-exploitation trade-off.Next, problems are uniformly sampled from the selected categories.This process is repeated until the desired batch size is reached.Sampling from the Boltzmann distribution naturally balances exploration and exploitation in curriculum selection.</p>
<p>The resulting batch is then used to update the LLM policy.After the policy update at each step, we compute the reward r(c) for each sampled category c and update the corresponding Q t (c) values using Eq. 1.The complete procedure of SEC is summarized in Algorithm 1.</p>
<p>Experiments</p>
<p>This section presents experiments evaluating SEC across three reasoning domains: planning, inductive reasoning, and mathematics.We additionally investigate SEC's effectiveness with different curriculum categories and alternative RL algorithms.</p>
<p>Experimental Setup</p>
<p>We conduct our experiments using the open-weight Qwen2.5 models [62]: Qwen2.5-3B and Qwen2.5-7B.For RL fine-tuning on reasoning tasks, we employ the widely-used GRPO algorithm [44,12].We report average pass@1 accuracy from the best checkpoint, calculated over 8 independent generations per problem.Additional training and evaluation details are provided in Appendix B. Prompts and data examples for all tasks are provided in Appendix C. Our experiments cover three reasoning domains that require different abilities: (i) Planning, which requires look-ahead search and backtracking; (ii) Inductive reasoning, which involves learning general rules from observations and applying them to unseen scenarios; and (iii) Mathematics, which demands multi-step logical deduction and systematic problem solving.</p>
<p>Planning.For planning tasks, we consider two popular puzzle problems: (i) Countdown, where the goal is to use basic arithmetic operations to reach a target number from a given set of 3-6 integers.</p>
<p>In this puzzle, we control the task difficulty by increasing the number of given integers.(ii) Zebra Puzzles, a classic logic puzzle involving 3-6 entities (e.g., houses) each with 3-6 properties (e.g., color).Given a set of textual clues (constraints), the goal is to correctly assign each property to each entity.Here, we control the task difficulty by increasing the number of entities and properties.</p>
<p>Inductive reasoning.We adopt the 1D variant of the Abstraction and Reasoning Corpus (ARC) [9,61] for inductive reasoning.Each puzzle instance consists of strings of lengths 10, 20, 30, or 40 (with greater length corresponding to increased difficulty), which are defined over integers.Three input-output examples illustrating an underlying rule are provided, and the LLM is tested on an unseen case requiring generalization.</p>
<p>For the above three reasoning tasks (Countdown, Zebra, and ARC), we generate problems using the framework provided by Open-Thought [34].Specifically, our training data consists of the three easiest difficulty levels, and the most difficult level is reserved as an out-of-distribution (OOD) evaluation set.For each difficulty level, we sample 10,000 problems for training and 200 held-out samples for evaluation.During RL fine-tuning, we assign rewards of 1 for correct problems, 0.1 for incorrect answers but with correct formatting, and 0 otherwise.</p>
<p>Mathematics.We train LLMs on the training split of the MATH dataset [17], which comprises problems categorized into five difficulty levels, from 1 (easiest) to 5 (hardest), as specified in the dataset annotations.Unlike the previous three tasks, the training data for mathematics is imbalanced across these difficulty levels (Figure S1).For this task, we use a binary reward (1 for correct and 0 otherwise), without assigning a partial reward for a correct format.The models are subsequently evaluated on the MATH500, AMC22-23, and AIME24 datasets.</p>
<p>Main Results</p>
<p>First, we evaluate the effectiveness of SEC using problem difficulty as the curriculum category, i.e., each difficulty level corresponds to an arm in the MAB framework.We compare SEC against two commonly used curriculum strategies:  For the larger Qwen2.5-7Bmodel, SEC performance is competitive but more similar to the random curriculum on tasks like Countdown and ARC.This outcome aligns with expectations, as stronger base models may already possess sufficient reasoning capabilities to tackle harder problems, thus rendering explicit curriculum guidance less critical.Nevertheless, on more challenging tasks such as Zebra and mathematics, SEC continues to show clear improvements.Specifically, the OOD accuracy on Zebra improves by approximately 11% relative (0.32 → 0.36) over the random baseline.On the challenging AIME problmes, SEC achieves a 27% relative gain (0.14 → 0.18).The consistent improvements observed in these more challenging domains, together with the robust gains in the 3B model, highlight SEC's effectiveness in improving the model generalization.</p>
<p>Finally, the difficulty-ordered curriculum often yields suboptimal performance, likely due to its fixed difficulty schedule, which does not dynamically adapt to the model's current performance.As a result, models may spend excessive training time on easy problems, limiting exposure to harder ones from which models could potentially learn more.These results further underscore the necessity of adaptive, online curriculum strategies like SEC, which continuously align problem selection with the model's current learning state.</p>
<p>Curriculum analysis.Figure 3</p>
<p>SEC with Multiple Curriculum Categories</p>
<p>In this section, we demonstrate that SEC seamlessly supports drawing from multiple and diverse curriculum categories at the same time.A common scenario in RL fine-tuning involves optimizing a model's performance across multiple reasoning domains.To evaluate SEC in such a multi-task setting, we combine the training datasets from Countdown, Zebra, and ARC to create a mixed training set comprising multiple types of reasoning problems, and conduct RL fine-tuning using the Qwen2.5-3Bmodel.The goal here is to achieve a strong overall performance across all reasoning tasks.</p>
<p>Our MAB-based curriculum framework is agnostic to the semantic meaning of the curriculum categories, thus allowing categories to be defined arbitrarily.In this experiment, we define one arm for each unique combination of 3 problem types and 3 difficulty levels, resulting in a total of 9 distinct arms.We denote this variant as SEC-2D.</p>
<p>Figure 4 presents results evaluating SEC-2D when training simultaneously on multiple reasoning tasks.The table (left) demonstrates that SEC-2D consistently outperforms the random curriculum across all three reasoning tasks.The learning curve (right) provides a detailed view of OOD accuracy on Countdown as training progresses.Initially, both curricula show rapid improvement; however, the random curriculum exhibits a significant performance collapse midway through training, highlighting its inability to effectively balance multi-task learning.In contrast, SEC-2D maintains stable, robust performance, underscoring its strength in adaptively balancing multiple learning objectives.[43] and REINFORCE Leave-One-Out (RLOO) [24].Table 2 presents results on the Countdown task with Qwen2.5-3B,comparing SEC to the random curriculum under these two algorithms.Across both PPO and RLOO, SEC consistently improves performance on ID and OOD evaluation splits, demonstrating that it is effective beyond a single RL algorithm.</p>
<p>SEC with Alternative RL Algorithms</p>
<p>Related Works</p>
<p>RL fine-tuning for language models.Language models (LMs) can be naturally viewed as sequential decision-making policies, generating tokens conditioned on partial text states until reaching terminal outputs.Typically, reward signals are sparse and episodic, assigned only after the full generation, an approach termed Outcome Reward Models (ORM) [11].Some recent studies introduce Process Reward Models (PRM), assigning intermediate rewards during generation to facilitate local credit assignment [28,53].Leveraging this Markov Decision Process (MDP) framing, RL fine-tuning has demonstrated success across multiple domains, including aligning LMs with human preferences (RLHF) [10,66,4,36], enhancing mathematical reasoning via exact-match rewards [44], and self-training with internal LM distributions (e.g., Self-taught Reasoner, STaR) [65].Recently, Reinforcement Learning with Verifiable Rewards (RLVR) [12,27] has emerged as a promising paradigm for improving the reasoning abilities of LMs.</p>
<p>RL methods tailored to these MDP formulations have also played a central role.Policy-gradient methods, including REINFORCE variants (e.g., RLOO) [59,24,1] and Proximal Policy Optimization (PPO) approaches [43,21,44], are widely adopted due to their relative stability.Alternatively, off-policy and value-based algorithms such as Directed Preference Optimization (DPO) [41,32] and Generative Flow Networks (GFlowNets) [6,19,18] provide advantages in sample efficiency, diversity, and asynchronous training [33,5], although they may not always match the task-specific reward maximization capabilities of on-policy methods, instead prioritizing improved diversity.</p>
<p>Curriculum learning.Curriculum learning was introduced by Bengio et al. [7] and later refined as self-paced learning [26], showing that organizing examples from easy to hard smooths non-convex optimization and improves generalization.In RL, curricula mitigate sparse rewards and exploration hurdles: reverse-curriculum generation grows start states outward from the goal [14], Teacher-Student Curriculum Learning (TSCL) [31] also used a non-stationary MAB framework to maximize measured learning progress, defined as improvements in task performance, methods such as POET, ACCEL, and PAIRED co-evolve tasks with agents [55,38,13], and Kim et al. [22] proposed an adaptive teacher that dynamically adjusts curricula for multi-modal amortized sampling.</p>
<p>Only recently have similar curriculum learning ideas begun influencing RL fine-tuning of language models.R 3 applies reverse curricula specifically to chain-of-thought reasoning, progressively revealing longer reasoning sequences conditioned on gold demonstrations [60].Qi et al. [40] proposed WEBRL, a self-evolving online curriculum RL framework designed to train LM-based web agents by prompting another LLM to autonomously generate new tasks based on previous failures.</p>
<p>Concurrently, several studies have explored automatic curriculum learning for RL fine-tuning.Bae et al. [3] propose online filtering of training problems by repeatedly generating solutions to estimate their difficulty.AdaRFT [46] adaptively adjusts curriculum difficulty based on the model's recent reward signals but relies on explicit difficulty-level ordering.In contrast, SEC leverages a general MAB formulation to dynamically adjust the curriculum.DUMP [56], in parallel to us, also leverages absolute advantage as the curriculum reward with an MAB framework, focusing on the Knights and Knaves logical reasoning puzzle with GRPO.In contrast, our study covers multiple reasoning domains, examines a multi-task RL setting, and validates performance across various RL algorithms.</p>
<p>Conclusion</p>
<p>In this paper, we introduced Self-Evolving Curriculum (SEC), an automatic curriculum learning framework tailored for RL fine-tuning of LLMs.SEC formulates adaptive curriculum selection as a non-stationary Multi-Armed Bandit problem, dynamically adjusting problem difficulty according to the model's evolving capability.Extensive experiments across diverse reasoning tasks, including planning, inductive reasoning, and mathematics, demonstrate that SEC consistently improves generalization and effectively balances learning across multiple reasoning domains simultaneously.</p>
<p>Our framework consists of three major components: curriculum rewards, sampling methods, and update rules.In this paper, SEC employs absolute advantage as the curriculum reward, a Boltzmann distribution for sampling, and a TD(0) update method.The generalization of these components can be explored for future work.For instance, one might incorporate uncertainty measures into the curriculum selection by leveraging approaches such as Upper Confidence Bound (UCB) [2] or Thompson sampling [51].</p>
<p>Limitations.While SEC demonstrates consistent effectiveness across diverse reasoning tasks, it has some limitations.Our method currently relies on predefined curriculum categories; its effectiveness with automatically inferred or less clearly defined categories remains unexplored.Additionally, SEC introduces extra hyperparameters (e.g., temperature, learning rate) that require tuning.Future work may explore more flexible curriculum definitions, such as clustering problems based on embeddings or using lightweight models (e.g., linear regression) to directly estimate curriculum rewards.</p>
<p>Prompt for Zebra Puzzle:</p>
<p>A conversation between User and Assistant.The user asks a question, and the Assistant solves it.The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.User: This is a logic puzzle.There are 3 houses (numbered 1 on the left, 3 on the right), from the perspective of someone standing across the street from them.Each has a different person in them.They have different characteristics:</p>
<p>-Each person has a unique name: arnold, bob, alice -Everyone has a different favorite cigar: dunhill, prince, pall mall -The people keep different animals: cat, dog, bird</p>
<p>Prompt for ARC-1D:</p>
<p>A conversation between User and Assistant.The user asks a question, and the Assistant solves it.The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.User: Find the common rule that maps an input grid to an output grid, given the examples below.</p>
<p>Example 1: Input: 1 2 1 2 1 0 0 1 2 0 Output: 0 0 0 1 1 1 1 2 2 2</p>
<p>Example 2: Input: 1 2 0 0 0 0 2 0 1 2 Output: 0 0 0 0 0 1 1 2 2 2</p>
<p>Example 3: Input: 0 0 2 0 0 0 0 1 1 0 Output: 0 0 0 0 0 0 0 1 1 2</p>
<p>Below is a test input grid.Predict the corresponding output grid by applying the rule you found.Describe how you derived the rule and your overall reasoning process in detail before you submit your answer.Your final answer must be placed in \boxed{} and should be just the test output grid itself.</p>
<p>Input: 0 0 2 0 0 1 1 0 0 1 Assistant: Let me solve this step by step.</p>
<p>Prompt for math:</p>
<p>A conversation between User and Assistant.The user asks a question, and the Assistant solves it.The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.User: Find the remainder when
33818 2 +
Figure 1 :
1
Figure 1: Curriculum matters.A deliberately poor (reverse) curriculum severely limits RL fine-tuning performance.Our proposed Self-Evolving Curriculum (SEC) significantly outperforms the standard random curriculum.See Sec.3.1 for details.</p>
<p>Figure 2 :
2
Figure 2: Overview of Self-Evolving Curriculum (SEC).SEC dynamically adjusts the training curriculum according to the model's current capabilities.During preprocessing, training data is partitioned into distinct categories (indicated by colors), e.g., by difficulty level or problem type.At each RL fine-tuning step: (1) The curriculum policy samples a training batch based on categories' expected learning gains; (2) The LLM policy is updated using the sampled batch and the chosen RL algorithm; (3) Rewards for curriculum categories are computed using advantage values estimated by the RL algorithm; (4) The curriculum policy is updated accordingly, refining future data selection.</p>
<p>( 1 )Figure 3 :
13
Figure 3: Average sample difficulty over training steps.SEC adaptively adjusts task difficulty during RL fine-tuning.Blue curves represent the sampled difficulty, smoothed using a moving average, while the green dashed line indicates the mean difficulty of the dataset.Across all benchmarks (columns) and model sizes (top: Qwen2.5-3B,bottom, Qwen2.5-7B),SEC initially selects easier problems and progressively introduces more challenging ones as training proceeds, effectively aligning difficulty with model improvement.</p>
<p>Figure 4 :
4
Figure 4: Performance comparison when training on multiple tasks.Left: Test accuracy of Qwen2.5-3B on ID and OOD splits.SEC-2D is implemented by defining an arm for each combination of problem type and difficulty level.SEC-2D consistently achieves higher accuracy, showing improved generalization compared to a random curriculum across tasks.Right: Countdown OOD accuracy vs. training steps, smoothed by a moving average.The random curriculum's performance collapses mid-training, highlighting its inability to effectively balance multiple tasks.In contrast, SEC-2D maintains stable performance throughout training.</p>
<p>Figure S1 :
S1
Figure S1: Distribution of difficulty levels in the MATH training set.</p>
<p>Table 1 :
1
Evaluation across reasoning tasks and curriculum methods.Accuracy is measured by averaging pass@1 over 8 independent generations per problem.In-distribution (ID) results are averaged over test problems sampled from the same three difficulty levels used in training.The best-performing curriculum strategy for each dataset and model size is shown in bold, and the second-best is underlined.SEC consistently achieves strong performance across tasks, particularly improving generalization on challenging OOD test problems.
TaskSplitQwen2.5 3BQwen2.5 7BRandom Ordered SEC (Ours) Random Ordered SEC (Ours)CountdownID OOD0.859 0.4790.551 0.3210.866 0.5420.858 0.5660.820 0.4420.872 0.555ZebraID OOD0.517 0.2850.534 0.3290.547 0.3450.573 0.3210.572 0.3110.587 0.355ARC-1DID OOD0.501 0.3130.476 0.3630.500 0.3810.512 0.4360.526 0.4280.514 0.418MATH5000.6680.6720.6720.7740.7590.761MathAMC22-230.3450.3520.3510.4860.4770.511AIME240.0750.0540.1000.1380.1500.175</p>
<p>illustrates how SEC adaptively adjusts training difficulty across tasks and models.For each task and model, the sampled difficulty (blue curves) initially starts below or around the dataset mean difficulty (green dashed line), indicating SEC's initial emphasis on easier problems to facilitate early-stage learning.As training progresses, SEC gradually increases the
Task Countdown Zebra ARCSplit ID OOD ID OOD ID OODRandom SEC-2D 0.837 0.839 0.418 0.428 0.513 0.539 0.254 0.312 0.380 0.418 0.251 0.327Test Score (Moving Averge)0.0 0.1 0.2 0.3SEC (Ours) Random Curriculum100200300400500600700
difficulty of selected problems, aligning the training complexity with the improving capabilities of the model.Notably, SEC consistently selects harder problems for the stronger Qwen2.5-7Bmodel compared to the smaller 3B model, further confirming SEC's ability to effectively adapt its curriculum to the model's learning capacity.This adaptive pattern across tasks and models highlights SEC's strength in dynamically adjusting problem difficulty to maximize learning outcomes.</p>
<p>Table 2 :
2
SEC
with alternative RL algo-rithms on Countdown. SEC improvesRL fine-tuning performance with differ-ent RL algorithms (PPO, RLOO), com-pared to a random curriculum.RL Method SplitRandomSECPPOID OOD0.621 0.1590.750 0.224RLOOID OOD0.821 0.4650.859 0.494</p>
<p>33819 2 + 33820 2 + 33821 2 + 33822 2 is divided by 17.Put your final answer within \boxed{}.Assistant: Let me solve this step by step.</p>
<p>A puzzle game where players combine a given set of numbers using basic arithmetic operations to reach a target number.
Acknowledgments and Disclosure of FundingWe thank Dzmitry Bahdanau and Nicolas Chapados for insightful discussions and assistance with this project.The authors acknowledge funding from CIFAR, NSERC and the Future of Life Institute.Minsu Kim was supported by KAIST Jang Yeong Sil Fellowship.A Social Impact StatementThis paper introduces SEC, a curriculum-learning method designed to enhance the reasoning capabilities of language models through reinforcement learning.By improving model accuracy and generalization, SEC can potentially lead to positive societal impacts, such as more reliable AI assistants and improved accessibility in educational settings.However, as with any method enhancing language models, there are potential negative implications: stronger reasoning abilities might facilitate misuse in tasks like misinformation or deceptive content generation.It is thus essential to employ these methods alongside responsible AI practices, including robust monitoring and mitigation strategies, to manage and minimize possible harmful effects.B Implementation DetailsTraining.All models are fine-tuned with the GRPO algorithms[44]as implemented in the Volcano Engine Reinforcement Learning (verl) library[45].We train separate 3B and 7B parameters variants of Qwen2.5[62].The fine-tuning processes last in total 240 gradient steps for Qwen2.5-3B and 120 steps for Qwen2.5-7B with a batch size of 256 on each of the three puzzle tasks.Advantages are estimated by 8 rollouts.Both models are trained for 240 steps on the math task.We do not apply the Kullback-Leibler (KL) divergence loss by setting the corresponding loss weight to be 0 across our study.We limit the max prompt length to be 1,024 tokens and the max response length to be 4,096 tokens.The model parameters are optimized using Adam[23]with a learning rate of 1e-6 and beta (0.9, 0.99) without warm-up steps.All of the training experiments are conducted on 4-8 NVIDIA H100 80GB GPUs.Hyperparameters for SEC used in each experiment is summarized in TableS1.TableS1: Hyperparameter settings (learning rate α and temperature τ ) used in each experiment.For the multi-task experiment in Sec.3.3, we fine-tune the Qwen2.5-3Bmodel for 3 × 240 = 720 steps on the mixed dataset.We use α = 0.5 and τ = 0.2.In Sec.3.4, we train Qwen2.5-3B for 120 steps in all the experiments.For RLOO, we similarly use 8 rollouts for advantage estimation and α = 0.5, τ = 0.25 for SEC.For PPO, we use α = 0.5, τ = 1 for SEC, and λ = 1, γ = 1 for the GAE parameters.Consistent with our main experiments, we do not apply the KL divergence loss.Models.Below we list the models used in our experiments:• Qwen2.5-3B:https://huggingface.co/Qwen/Qwen2.5-3B• Qwen2.5-7B:https://huggingface.co/Qwen/Qwen2.5-7BMath Datasets.Below we list the data sources used in our experiments:• MATH500: https://huggingface.co/datasets/HuggingFaceH4/MATH-500• AMC22-23: https://huggingface.co/datasets/AI-MO/aimo-validation-amc• AIME: https://huggingface.co/datasets/Maxwell-Jia/AIME_2024C Data ExamplesBelow we list the prompts and data examples for each task in our study.The prompt template is adopted from Pan et al.[37].Prompt for Countdown:A conversation between User and Assistant.The user asks a question, and the
Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, Sara Hooker, arXiv:2402.147402024arXiv preprint</p>
<p>Using confidence bounds for exploitation-exploration trade-offs. Peter Auer, Journal of Machine Learning Research. 3Nov. 2002</p>
<p>Online difficulty filtering for reasoning oriented reinforcement learning. Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, Jeongyeon Nam, Donghyun Kwak, arXiv:2504.033802025arXiv preprint</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam Mccandlish, Chris Olah, Ben Mann, Jared Kaplan, arXiv:2204.058622022arXiv preprint</p>
<p>Trajectory balance with asynchrony: Decoupling exploration and learning for fast. Siddarth Brian R Bartoldson, James Venkatraman, Moksh Diffenderfer, Tal Jain, Seanie Ben-Nun, Minsu Lee, Johan Kim, Yoshua Obando-Ceron, Bhavya Bengio, Kailkhura, arXiv:2503.18929scalable llm post-training. 2025arXiv preprint</p>
<p>Flow network based generative models for non-iterative diverse candidate generation. Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, Yoshua Bengio, Advances in Neural Information Processing Systems. 202134</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, 10.1145/1553374.1553380Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09. the 26th Annual International Conference on Machine Learning, ICML '09New York, NY, USAAssociation for Computing Machinery2009</p>
<p>The zone of proximal development in vygotsky's analysis of learning and instruction. Vygotsky's educational theory in cultural context. Seth Chaiklin, 20031</p>
<p>François Chollet, arXiv:1911.01547On the measure of intelligence. 2019arXiv preprint</p>
<p>Paul Christiano, Jan Leike, Tom B Brown, Miljan Martic, Shane Legg, Dario Amodei, arXiv:1706.03741Deep reinforcement learning from human preferences. 2017arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Deepseek-Ai , 2025</p>
<p>Emergent complexity and zero-shot transfer via unsupervised environment design. Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, Sergey Levine, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Reverse curriculum generation for reinforcement learning. Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, Pieter Abbeel, Conference on robot learning. PMLR2017</p>
<p>Automatic goal generation for reinforcement learning agents. Carlos Florensa, David Held, Xinyang Geng, Pieter Abbeel, International conference on machine learning. PMLR2018</p>
<p>Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D Goodman, arXiv:2503.013072025arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, D Song, J Steinhardt, NeurIPS Datasets and Benchmarks. 2021</p>
<p>Matthew Ho, Vincent Zhu, Xiaoyin Chen, Moksh Jain, Nikolay Malkin, Edwin Zhang, arXiv:2410.13224Proof flow: Preliminary study on generative flow network language model tuning for formal reasoning. 2024arXiv preprint</p>
<p>J Edward, Moksh Hu, Eric Jain, Younesse Elmoznino, Guillaume Kaddar, Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. International Conference on Learning Representations (ICLR). 2024</p>
<p>Not all samples are created equal: Deep learning with importance sampling. Angelos Katharopoulos, François Fleuret, International conference on machine learning. PMLR2018</p>
<p>Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, Nicolas Le Roux, arXiv:2410.01679Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. 2024arXiv preprint</p>
<p>Minsu Kim, Sanghyeok Choi, Taeyoung Yun, Emmanuel Bengio, Leo Feng, Jarrid Rector-Brooks, Sungsoo Ahn, Jinkyoo Park, Nikolay Malkin, and Yoshua Bengio. Adaptive teachers for amortized samplers. International Conference on Learning Representations (ICLR). 2025</p>
<p>P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Buy 4 REINFORCE samples, get a baseline for free! In ICLR Workshop on Deep Reinforcement Learning for Structured Prediction. Wouter Kool, Herke Van Hoof, Max Welling, 2019</p>
<p>Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, M Lei, Kay Zhang, Disha Mckinney, Cosmin Shrivastava, George Paduraru, Tucker, arXiv:2409.12917Doina Precup, Feryal Behbahani, and Aleksandra Faust. Training language models to self-correct via reinforcement learning. 2024arXiv preprint</p>
<p>Self-paced learning for latent variable models. M Pawan Kumar, Benjamin Packer, Daphne Koller, Advances in Neural Information Processing Systems (NeurIPS). 2010</p>
<p>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James, V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A Smith, Yizhong Wang, arXiv:2411.15124Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training. 2024arXiv preprint</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, The Twelfth International Conference on Learning Representations. Bowen Baker, Teddy LeeJan. 2023</p>
<p>Online batch selection for faster training of neural networks. Ilya Loshchilov, Frank Hutter, arXiv:1511.063432015arXiv preprint</p>
<p>Deepcoder: A fully open-source 14b coder at o3-mini level. Michael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca , Ada Popa, Ion Stoica, DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51. 2025Notion Blog</p>
<p>Teacher-student curriculum learning. Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman, 10.1109/TNNLS.2020.2983146IEEE Transactions on Neural Networks and Learning Systems. 3192020</p>
<p>Simpo: Simple preference optimization with a reference-free reward. Yu Meng, Mengzhou Xia, Danqi Chen, Advances in Neural Information Processing Systems. 202437</p>
<p>Michael Noukhovitch, Shengyi Huang, Sophie Xhonneux, Arian Hosseini, Rishabh Agarwal, Aaron Courville, arXiv:2410.18252Asynchronous rlhf: Faster and more efficient off-policy rl for language models. 2024arXiv preprint</p>
<p>. Open-Thought, 2025</p>
<p>Learning to reason with llms. Openai, </p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>. Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, Alane Suhr, Tinyzero, 2025</p>
<p>Evolving curricula with regret-based environment design. Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, Tim Rocktäschel, Proceedings of the 39th International Conference on Machine Learning (ICML). the 39th International Conference on Machine Learning (ICML)2022</p>
<p>Automatic curriculum learning for deep rl: A short survey. Rémy Portelas, Cédric Colas, Lilian Weng, Katja Hofmann, Pierre-Yves Oudeyer, 10.24963/ijcai.2020/671International Joint Conference on Artificial Intelligence. 2020</p>
<p>Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, arXiv:2411.02337Training llm web agents via self-evolving online curriculum reinforcement learning. 2024arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362023</p>
<p>. Tom Schaul, John Quan, Ioannis Antonoglou, David Silver, arXiv:1511.059522015Prioritized experience replay. arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, Chuan Wu, arXiv:2409.19256Hybridflow: A flexible and efficient rlhf framework. 2024arXiv preprint</p>
<p>Efficient reinforcement finetuning via adaptive curriculum learning. Taiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, Jieyu Zhao, arXiv:2504.055202025arXiv preprint</p>
<p>Fastcurl: Curriculum reinforcement learning with progressive context extension for efficient training r1-like reasoning models. Mingyang Song, Mao Zheng, Zheng Li, Wenjie Yang, Xuan Luo, Yue Pan, Feng Zhang, arXiv:2503.172872025arXiv preprint</p>
<p>Learning to predict by the methods of temporal differences. Richard S Sutton, 10.1023/A:1022633531479Mach. Learn. 0885-612531August 1988</p>
<p>Reinforcement Learning: An Introduction. Richard S Sutton, Andrew G Barto, 2018Bradford Book, Cambridge, MA, USA</p>
<p>Kimi k1. 5: Scaling reinforcement learning with llms. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, arXiv:2501.125992025arXiv preprint</p>
<p>On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. William R Thompson, Biometrika. 253/41933</p>
<p>Proximal curriculum for reinforcement learning agents. Georgios Tzannetos, Bárbara Gomes Ribeiro, Parameswaran Kamalaruban, Adish Singla, arXiv:2304.128772023arXiv preprint</p>
<p>Solving math word problems with process-and outcome-based feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, arXiv:2211.142752022arXiv preprint</p>
<p>Mind in society: Development of higher psychological processes. Lev Semenovich, Vygotsky , Michael Cole, 1978Harvard university press</p>
<p>Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. Rui Wang, Joel Lehman, Jeff Clune, Kenneth O Stanley, Proceedings of the Genetic and Evolutionary Computation Conference (GECCO '19). the Genetic and Evolutionary Computation Conference (GECCO '19)ACM2019</p>
<p>Dump: Automated distributionlevel curriculum learning for rl-based llm post-training. Zhenting Wang, Guofeng Cui, Wan Kun, Wentian Zhao, arXiv:2504.097102025arXiv preprint</p>
<p>Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, Sida I Wang, arXiv:2502.18449Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. 2025arXiv preprint</p>
<p>Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, arXiv:2503.10460Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. 2025arXiv preprint</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. Williams Ronald, Machine learning. 81992</p>
<p>Training large language models for reasoning through reverse curriculum reinforcement learning. Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, arXiv:2402.058082024arXiv preprint</p>
<p>Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner, Elias B Khalil, arXiv:2305.18354Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations. 2023arXiv preprint</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.1511520245 technical report. arXiv preprint</p>
<p>Demystifying long chain-of-thought reasoning in llms. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, Xiang Yue, arXiv:2502.033732025arXiv preprint</p>
<p>Dapo: An open-source llm reinforcement learning system at scale. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, arXiv:2503.144762025arXiv preprint</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>M Daniel, Nisan Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019arXiv preprint</p>
<p>The bird keeper is directly left of the Dunhill smoker. </p>
<p>Alice is the dog owner. </p>
<p>Arnold is in the second house. </p>
<p>Alice is the Prince smoker. </p>
<p>What is Name of the person who lives in House 1? Provide only the name of the person as your final answer and put in in \boxed{}. Arnold is the cat lover. for example: \boxed{Alice}. Assistant: Let me solve this step by step</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5666 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5666</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5666</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-c3cf10a844ac292987de831ba27bf205288671c2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c3cf10a844ac292987de831ba27bf205288671c2" target="_blank">Large Foundation Models for Power Systems</a></p>
                <p><strong>Paper Venue:</strong> IEEE Power & Energy Society General Meeting</p>
                <p><strong>Paper TL;DR:</strong> This paper investigates the potential of existing foundation models by validating their performance on four representative tasks across power system domains, including the optimal power flow (OPF), electric vehicle (EV) scheduling, knowledge retrieval for power engineering technical reports, and situation awareness.</p>
                <p><strong>Paper Abstract:</strong> Foundation models, such as Large Language Models (LLMs), can respond to a wide range of format-free queries without any task-specific data collection or model training, creating various research and application opportunities for the modeling and operation of large-scale power systems. In this paper, we outline how such large foundation model such as GPT-4 are developed, and discuss how they can be leveraged in challenging power and energy system tasks. We first investigate the potential of existing foundation models by validating their performance on four representative tasks across power system domains, including the optimal power flow (OPF), electric vehicle (EV) scheduling, knowledge retrieval for power engineering technical reports, and situation awareness. Our results indicate strong capabilities of such foundation models on boosting the efficiency and reliability of power system operational pipelines. We also provide suggestions and projections on future deployment of foundation models in power system applications.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5666.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5666.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4OPF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM for Optimal Power Flow (LLM4OPF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses an LLM (GPT-4) as an optimizer to iteratively propose candidate generator dispatch solutions for a constrained OPF/economic dispatch problem using prompt-based retrieval of prior solution-cost pairs and constraint checking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4, a transformer-based large language model (OpenAI) trained on broad web-scale text corpora; used here in its conversational API to generate candidate numerical solutions and iterate based on prompts containing prior solution-cost pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Power systems — optimal power flow / economic dispatch (power system optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Solve a simplified constrained OPF / economic dispatch problem by iteratively proposing generator outputs (P_i) that satisfy bounds and total demand while minimizing quadratic generation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Comparison of objective cost and generator outputs vs. a standard optimizer (Gurobi); feasibility (constraint satisfaction) of proposed solutions; convergence of cost over optimization steps.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Close match to Gurobi solutions on 5-unit test case: Example results — Gurobi cost 131455.000 vs. GPT-4 cost 131456.090 (n=500 steps); second case Gurobi 134670.416 vs. GPT-4 134691.790 (n=50 steps with prior solutions included). Generator outputs reported in Table I (values nearly identical to Gurobi's).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Prompt design and inclusion of historical solution-cost pairs (few-shot / retrieved examples)', 'Number of iterative optimization steps (search budget)', 'Scale and complexity of the optimization problem (inefficiency on larger-scale problems)', 'Constraint checking / only storing feasible solutions']</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Empirical experiments: (a) baseline run with 300/500 steps shows iterative cost reduction and near-equality to Gurobi; (b) when adapting to new demand L_e=405, including prior solution-cost pairs enabled GPT-4 to reach a solution in fewer steps (50) than without prior examples. Paper cites known inefficiency of LLM search limiting larger-scale application.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison against ground-truth solver (Gurobi) on a 5-unit test case: check constraint satisfaction, report objective costs and generator dispatch values; plot cost vs. iteration; only feasible solutions are stored/considered.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires many iterations (expensive computation) and is sample-specific; search inefficiency limits scalability; LLM-based generation can produce code or numerical errors without proper scaffolding; may not scale to larger OPF instances.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Direct numeric comparison to Gurobi optimizer (exact solver). Experiments show near-equal costs and dispatch for the 5-unit case; authors note LLM approach inferior in efficiency and scalability for larger problems.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use careful prompt engineering and retrieval of prior solution-cost pairs (few-shot) to accelerate convergence; apply constraint checking externally and store only feasible candidates; consider improved initialization and few-shot prompts for batch conditions; use LLM as assistant/heuristic rather than replacement for exact solvers on large instances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Foundation Models for Power Systems', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5666.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5666.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4EV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM for Electric Vehicle charging (LLM4EV)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework where an LLM (GPT-4) interacts with end users in natural language to elicit parameters, generate correct code that invokes a provided EV-charging solver function, execute it, and interpret/explain results — effectively serving as an interactive scheduling interface and partial controller.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4, transformer-based LLM by OpenAI, used here for natural-language understanding, question generation, and code synthesis to call a predefined solver function (Solve_EV).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Power systems — electric vehicle charging scheduling / energy management</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Elicit problem parameters from users, generate code to call a convex optimization EV charging solver, and interpret the solver outputs (schedules of charging power over time for multiple vehicles).</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Functional correctness of generated questions (coverage of required function arguments), syntactic/semantic correctness of generated code, and correct interpretation of solver outputs; no numeric optimality metric vs. ground-truth solver reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Qualitative success: GPT-4 generated 8 questions that exactly matched the 8 arguments of the provided Solve_EV function; correctly turned natural-language answers ('they all start from zero') into code initial_states = [0, 0, 0, 0, 0]; produced an example optimal schedule output when executing the solver (numerical arrays reported). No numeric optimality comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Availability and clarity of function templates / API surface provided to the LLM', 'Quality of prompt engineering and example interactions', 'Changes in problem formulation requiring new function implementations (code-template dependency)']</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Experiments demonstrate perfect correspondence between questions generated and function arguments when a solve_EV template was provided; paper notes that when problem formulation changes, new functions are required and performance becomes dependent on function library.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Qualitative evaluation of LLM behavior: (1) whether it asks correct parameter questions, (2) whether it generates code to call the provided function, (3) whether it correctly interprets returned solver outputs. Example runs and printed schedules were used as evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Code-template dependent: LLM relies on predefined solver functions; when problem definitions change, additional engineering (new functions) required. No demonstrated benchmarking of solution optimality vs. direct solver calls across many instances.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>No comparison to alternative LLMs; compared qualitatively to manual coding workflow (LLM saves effort). Table II notes 'Code template dependent' as a limitation and suggests expanding function library.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Provide explicit function templates and clear API signatures to the LLM; use active elicitation prompts to ensure all solver arguments are collected; expand the function library to cover varied problem formulations; treat LLM as an interface that produces code to call vetted optimization routines rather than replacing numerical solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Foundation Models for Power Systems', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5666.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5666.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4SA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM for Situation Awareness (LLM4SA) using GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal in-context learning framework that uses GPT-4V (vision-enabled GPT-4) with prompt engineering and few-shot exemplars to perform zero/few-shot situation awareness tasks such as wildfire detection from satellite imagery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V (GPT-4 Vision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4V: GPT-4 variant with multimodal (image+text) capabilities enabling joint vision-language understanding and reasoning via the same LLM backbone; trained/finetuned to accept images and text prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Remote sensing / disaster detection / power-system situation awareness (wildfire detection from satellite imagery)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Binary image classification (wildfire present or not) from satellite images, performed via multimodal prompts and in-context exemplars; also generating textual explanations of scene.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Average classification accuracy (percent correct) over repeated rounds with balanced positive/negative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Four prompting strategies evaluated: (1) single image direct query (baseline), (2) single image + prompt engineering, (3) few-shot image examples with labels achieved 86% accuracy, (4) few-shot examples plus language explanations achieved highest 91% accuracy. For comparison, a CNN reported 94% and a random forest 96% on the same dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Prompt engineering (including task framing and step-by-step prompting)', 'Few-shot in-context exemplars (number and relevance of image examples)', 'Adding language explanations paired with exemplars', 'Multimodal alignment and quality of image inputs']</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Experimentally compared four approaches: incremental improvements from prompt engineering, to few-shot exemplars (86%), and further improvement when adding language explanations (91%). Direct single-image queries performed worse. Comparative numbers to CNN and random forest show GPT-4V still trails specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Multiple-round experiments: each round uses 5 positive and 5 negative images; accuracy averaged across rounds. Approaches differ in prompt content and exemplar inclusion. Comparison to standard ML baselines (CNN, random forest) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance still inferior to specialized supervised ML models (CNN 94%, RF 96%); sensitivity to prompt/exemplar design; heterogeneous/unstructured multi-source inputs complicate alignment; rare-event scarcity remains a challenge though GPT-4V shows zero/few-shot promise.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared four prompt strategies internally; externally compared accuracy to supervised CNN and random forest baselines (GPT-4V: 91% best vs. CNN 94%, RF 96%).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use in-context learning with carefully selected few-shot exemplars and pair exemplars with language explanations; apply prompt engineering to provide clear role/task framing and step-by-step reasoning instructions; integrate multimodal views (e.g., images + sensor data) when available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Foundation Models for Power Systems', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5666.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5666.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4Doc / RAG+GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM for Document Comprehension with Retrieval-Augmented Generation (LLM4Doc)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline combining retrieval-augmented generation (RAG) and GPT-4 with prompt engineering to summarize and answer technical questions on long energy-context documents (example: FERC Order No. 901).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (with Retrieval-Augmented Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 used in conjunction with a vector-embedding based retrieval system (RAG) that supplies document-relevant passages to the model, improving domain-specific factuality and comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Energy policy / regulatory document comprehension and knowledge retrieval (power systems standards interpretation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Textual comprehension, summarization, and question-answering over long technical regulatory documents by retrieving relevant passages and prompting GPT-4 to generate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Qualitative correctness of answers to general and technical questions; presence/absence of detailed, correct technical descriptions (binary/graded human judgement); example-based comparison (with and without RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Qualitative improvement with RAG: without RAG GPT-4 sometimes failed on technical questions (e.g., 'I don't know' for phase-locked loop synchronization), while with RAG GPT-4 produced detailed, document-grounded explanations. No numeric accuracy aggregated metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Use of retrieval augmentation (RAG) to supply domain-specific passages', 'Prompt engineering (format and exemplars)', 'Quality and coverage of the external document corpus', "Model's inherent knowledge gaps for specialized technical terms"]</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Side-by-side QA examples on FERC Order No. 901 show that RAG enables GPT-4 to answer technical queries that it could not answer unaided; authors note remaining inconsistencies and some incorrect parts even after RAG (errors flagged in their repository).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Constructed sets of 'general' and 'technical' questions; compared answers from GPT-4 with no retrieval, GPT-4+RAG, GPT-4+RAG+prompt-engineering, and human expert summaries. Visual/manual inspection used to judge depth and correctness; specific QA examples shown (Fig. 6).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Answer inconsistency and remaining factual errors even with RAG; limited to available documents (can't retrieve missing external facts); some generated content still inaccurate and requires human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared answer quality across no-RAG vs RAG vs RAG+prompt-engineering vs human writer; RAG and prompt engineering improved technical answer detail but did not fully match expert human writing.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Embed long documents into vector stores and use RAG to augment prompts; carefully design prompt formats; consider ensemble methods and domain-specific fine-tuning or private-data fine-tuning to reduce inconsistencies; always validate critical outputs with human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Foundation Models for Power Systems', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models as optimizers <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>The dawn of LMMS: Preliminary explorations with GPT-4V (ision) <em>(Rating: 2)</em></li>
                <li>On the potential of ChatGPT to generate distribution systems for load flow studies using OpenDSS <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5666",
    "paper_id": "paper-c3cf10a844ac292987de831ba27bf205288671c2",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "LLM4OPF",
            "name_full": "LLM for Optimal Power Flow (LLM4OPF)",
            "brief_description": "A framework that uses an LLM (GPT-4) as an optimizer to iteratively propose candidate generator dispatch solutions for a constrained OPF/economic dispatch problem using prompt-based retrieval of prior solution-cost pairs and constraint checking.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4, a transformer-based large language model (OpenAI) trained on broad web-scale text corpora; used here in its conversational API to generate candidate numerical solutions and iterate based on prompts containing prior solution-cost pairs.",
            "model_size": null,
            "scientific_subdomain": "Power systems — optimal power flow / economic dispatch (power system optimization)",
            "simulation_task": "Solve a simplified constrained OPF / economic dispatch problem by iteratively proposing generator outputs (P_i) that satisfy bounds and total demand while minimizing quadratic generation cost.",
            "accuracy_metric": "Comparison of objective cost and generator outputs vs. a standard optimizer (Gurobi); feasibility (constraint satisfaction) of proposed solutions; convergence of cost over optimization steps.",
            "reported_accuracy": "Close match to Gurobi solutions on 5-unit test case: Example results — Gurobi cost 131455.000 vs. GPT-4 cost 131456.090 (n=500 steps); second case Gurobi 134670.416 vs. GPT-4 134691.790 (n=50 steps with prior solutions included). Generator outputs reported in Table I (values nearly identical to Gurobi's).",
            "factors_affecting_accuracy": [
                "Prompt design and inclusion of historical solution-cost pairs (few-shot / retrieved examples)",
                "Number of iterative optimization steps (search budget)",
                "Scale and complexity of the optimization problem (inefficiency on larger-scale problems)",
                "Constraint checking / only storing feasible solutions"
            ],
            "evidence_for_factors": "Empirical experiments: (a) baseline run with 300/500 steps shows iterative cost reduction and near-equality to Gurobi; (b) when adapting to new demand L_e=405, including prior solution-cost pairs enabled GPT-4 to reach a solution in fewer steps (50) than without prior examples. Paper cites known inefficiency of LLM search limiting larger-scale application.",
            "evaluation_method": "Comparison against ground-truth solver (Gurobi) on a 5-unit test case: check constraint satisfaction, report objective costs and generator dispatch values; plot cost vs. iteration; only feasible solutions are stored/considered.",
            "limitations_or_failure_cases": "Requires many iterations (expensive computation) and is sample-specific; search inefficiency limits scalability; LLM-based generation can produce code or numerical errors without proper scaffolding; may not scale to larger OPF instances.",
            "comparisons": "Direct numeric comparison to Gurobi optimizer (exact solver). Experiments show near-equal costs and dispatch for the 5-unit case; authors note LLM approach inferior in efficiency and scalability for larger problems.",
            "recommendations_or_best_practices": "Use careful prompt engineering and retrieval of prior solution-cost pairs (few-shot) to accelerate convergence; apply constraint checking externally and store only feasible candidates; consider improved initialization and few-shot prompts for batch conditions; use LLM as assistant/heuristic rather than replacement for exact solvers on large instances.",
            "uuid": "e5666.0",
            "source_info": {
                "paper_title": "Large Foundation Models for Power Systems",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLM4EV",
            "name_full": "LLM for Electric Vehicle charging (LLM4EV)",
            "brief_description": "A framework where an LLM (GPT-4) interacts with end users in natural language to elicit parameters, generate correct code that invokes a provided EV-charging solver function, execute it, and interpret/explain results — effectively serving as an interactive scheduling interface and partial controller.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4, transformer-based LLM by OpenAI, used here for natural-language understanding, question generation, and code synthesis to call a predefined solver function (Solve_EV).",
            "model_size": null,
            "scientific_subdomain": "Power systems — electric vehicle charging scheduling / energy management",
            "simulation_task": "Elicit problem parameters from users, generate code to call a convex optimization EV charging solver, and interpret the solver outputs (schedules of charging power over time for multiple vehicles).",
            "accuracy_metric": "Functional correctness of generated questions (coverage of required function arguments), syntactic/semantic correctness of generated code, and correct interpretation of solver outputs; no numeric optimality metric vs. ground-truth solver reported.",
            "reported_accuracy": "Qualitative success: GPT-4 generated 8 questions that exactly matched the 8 arguments of the provided Solve_EV function; correctly turned natural-language answers ('they all start from zero') into code initial_states = [0, 0, 0, 0, 0]; produced an example optimal schedule output when executing the solver (numerical arrays reported). No numeric optimality comparison provided.",
            "factors_affecting_accuracy": [
                "Availability and clarity of function templates / API surface provided to the LLM",
                "Quality of prompt engineering and example interactions",
                "Changes in problem formulation requiring new function implementations (code-template dependency)"
            ],
            "evidence_for_factors": "Experiments demonstrate perfect correspondence between questions generated and function arguments when a solve_EV template was provided; paper notes that when problem formulation changes, new functions are required and performance becomes dependent on function library.",
            "evaluation_method": "Qualitative evaluation of LLM behavior: (1) whether it asks correct parameter questions, (2) whether it generates code to call the provided function, (3) whether it correctly interprets returned solver outputs. Example runs and printed schedules were used as evidence.",
            "limitations_or_failure_cases": "Code-template dependent: LLM relies on predefined solver functions; when problem definitions change, additional engineering (new functions) required. No demonstrated benchmarking of solution optimality vs. direct solver calls across many instances.",
            "comparisons": "No comparison to alternative LLMs; compared qualitatively to manual coding workflow (LLM saves effort). Table II notes 'Code template dependent' as a limitation and suggests expanding function library.",
            "recommendations_or_best_practices": "Provide explicit function templates and clear API signatures to the LLM; use active elicitation prompts to ensure all solver arguments are collected; expand the function library to cover varied problem formulations; treat LLM as an interface that produces code to call vetted optimization routines rather than replacing numerical solvers.",
            "uuid": "e5666.1",
            "source_info": {
                "paper_title": "Large Foundation Models for Power Systems",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLM4SA",
            "name_full": "LLM for Situation Awareness (LLM4SA) using GPT-4V",
            "brief_description": "A multimodal in-context learning framework that uses GPT-4V (vision-enabled GPT-4) with prompt engineering and few-shot exemplars to perform zero/few-shot situation awareness tasks such as wildfire detection from satellite imagery.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4V (GPT-4 Vision)",
            "model_description": "GPT-4V: GPT-4 variant with multimodal (image+text) capabilities enabling joint vision-language understanding and reasoning via the same LLM backbone; trained/finetuned to accept images and text prompts.",
            "model_size": null,
            "scientific_subdomain": "Remote sensing / disaster detection / power-system situation awareness (wildfire detection from satellite imagery)",
            "simulation_task": "Binary image classification (wildfire present or not) from satellite images, performed via multimodal prompts and in-context exemplars; also generating textual explanations of scene.",
            "accuracy_metric": "Average classification accuracy (percent correct) over repeated rounds with balanced positive/negative examples.",
            "reported_accuracy": "Four prompting strategies evaluated: (1) single image direct query (baseline), (2) single image + prompt engineering, (3) few-shot image examples with labels achieved 86% accuracy, (4) few-shot examples plus language explanations achieved highest 91% accuracy. For comparison, a CNN reported 94% and a random forest 96% on the same dataset.",
            "factors_affecting_accuracy": [
                "Prompt engineering (including task framing and step-by-step prompting)",
                "Few-shot in-context exemplars (number and relevance of image examples)",
                "Adding language explanations paired with exemplars",
                "Multimodal alignment and quality of image inputs"
            ],
            "evidence_for_factors": "Experimentally compared four approaches: incremental improvements from prompt engineering, to few-shot exemplars (86%), and further improvement when adding language explanations (91%). Direct single-image queries performed worse. Comparative numbers to CNN and random forest show GPT-4V still trails specialized models.",
            "evaluation_method": "Multiple-round experiments: each round uses 5 positive and 5 negative images; accuracy averaged across rounds. Approaches differ in prompt content and exemplar inclusion. Comparison to standard ML baselines (CNN, random forest) reported.",
            "limitations_or_failure_cases": "Performance still inferior to specialized supervised ML models (CNN 94%, RF 96%); sensitivity to prompt/exemplar design; heterogeneous/unstructured multi-source inputs complicate alignment; rare-event scarcity remains a challenge though GPT-4V shows zero/few-shot promise.",
            "comparisons": "Compared four prompt strategies internally; externally compared accuracy to supervised CNN and random forest baselines (GPT-4V: 91% best vs. CNN 94%, RF 96%).",
            "recommendations_or_best_practices": "Use in-context learning with carefully selected few-shot exemplars and pair exemplars with language explanations; apply prompt engineering to provide clear role/task framing and step-by-step reasoning instructions; integrate multimodal views (e.g., images + sensor data) when available.",
            "uuid": "e5666.2",
            "source_info": {
                "paper_title": "Large Foundation Models for Power Systems",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLM4Doc / RAG+GPT-4",
            "name_full": "LLM for Document Comprehension with Retrieval-Augmented Generation (LLM4Doc)",
            "brief_description": "A pipeline combining retrieval-augmented generation (RAG) and GPT-4 with prompt engineering to summarize and answer technical questions on long energy-context documents (example: FERC Order No. 901).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (with Retrieval-Augmented Generation)",
            "model_description": "GPT-4 used in conjunction with a vector-embedding based retrieval system (RAG) that supplies document-relevant passages to the model, improving domain-specific factuality and comprehension.",
            "model_size": null,
            "scientific_subdomain": "Energy policy / regulatory document comprehension and knowledge retrieval (power systems standards interpretation)",
            "simulation_task": "Textual comprehension, summarization, and question-answering over long technical regulatory documents by retrieving relevant passages and prompting GPT-4 to generate answers.",
            "accuracy_metric": "Qualitative correctness of answers to general and technical questions; presence/absence of detailed, correct technical descriptions (binary/graded human judgement); example-based comparison (with and without RAG).",
            "reported_accuracy": "Qualitative improvement with RAG: without RAG GPT-4 sometimes failed on technical questions (e.g., 'I don't know' for phase-locked loop synchronization), while with RAG GPT-4 produced detailed, document-grounded explanations. No numeric accuracy aggregated metric provided.",
            "factors_affecting_accuracy": [
                "Use of retrieval augmentation (RAG) to supply domain-specific passages",
                "Prompt engineering (format and exemplars)",
                "Quality and coverage of the external document corpus",
                "Model's inherent knowledge gaps for specialized technical terms"
            ],
            "evidence_for_factors": "Side-by-side QA examples on FERC Order No. 901 show that RAG enables GPT-4 to answer technical queries that it could not answer unaided; authors note remaining inconsistencies and some incorrect parts even after RAG (errors flagged in their repository).",
            "evaluation_method": "Constructed sets of 'general' and 'technical' questions; compared answers from GPT-4 with no retrieval, GPT-4+RAG, GPT-4+RAG+prompt-engineering, and human expert summaries. Visual/manual inspection used to judge depth and correctness; specific QA examples shown (Fig. 6).",
            "limitations_or_failure_cases": "Answer inconsistency and remaining factual errors even with RAG; limited to available documents (can't retrieve missing external facts); some generated content still inaccurate and requires human verification.",
            "comparisons": "Compared answer quality across no-RAG vs RAG vs RAG+prompt-engineering vs human writer; RAG and prompt engineering improved technical answer detail but did not fully match expert human writing.",
            "recommendations_or_best_practices": "Embed long documents into vector stores and use RAG to augment prompts; carefully design prompt formats; consider ensemble methods and domain-specific fine-tuning or private-data fine-tuning to reduce inconsistencies; always validate critical outputs with human experts.",
            "uuid": "e5666.3",
            "source_info": {
                "paper_title": "Large Foundation Models for Power Systems",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models as optimizers",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2
        },
        {
            "paper_title": "The dawn of LMMS: Preliminary explorations with GPT-4V (ision)",
            "rating": 2
        },
        {
            "paper_title": "On the potential of ChatGPT to generate distribution systems for load flow studies using OpenDSS",
            "rating": 1
        }
    ],
    "cost": 0.011913749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Foundation Models for Power Systems</h1>
<p>Chenghao Huang<em>§, Siyang $\mathrm{Li}^{\dagger \S}$, Ruohong Liu ${ }^{\dagger \S}$, Hao Wang</em>, and Yize Chen ${ }^{\dagger}$<br>*Department of Data Science and Artificial Intelligence, Monash University, Australia<br>{chenghao.huang, hao.wang2}@monash.edu<br>${ }^{\dagger}$ Information Hub, Hong Kong University of Science and Technology (Guangzhou), China<br>{sli572, rliu519}@connect.hkust-gz.edu.cn, yizechen@ust.hk</p>
<h4>Abstract</h4>
<p>Foundation models, such as Large Language Models (LLMs), can respond to a wide range of format-free queries without any task-specific data collection or model training, creating various research and application opportunities for the modeling and operation of large-scale power systems. In this paper, we outline how such large foundation model such as GPT-4 are developed, and discuss how they can be leveraged in challenging power and energy system tasks. We first investigate the potential of existing foundation models by validating their performance on four representative tasks across power system domains, including the optimal power flow (OPF), electric vehicle (EV) scheduling, knowledge retrieval for power engineering technical reports, and situation awareness. Our results indicate strong capabilities of such foundation models on boosting the efficiency and reliability of power system operational pipelines. We also provide suggestions and projections on future deployment of foundation models in power system applications.</p>
<p>Index Terms-Artificial intelligence, foundation models, datadriven modeling and control, power system operation.</p>
<h2>I. INTRODUCTION</h2>
<p>Recent advancements on large foundation models such as large language models (LLM) and large multi-modal models have greatly transformed the landscape of artificial intelligence and machine learning (ML) [1], [2]. The conversational variant of Generative Pre-trained Transformer 4 (GPT-4) developed by OpenAI, ChatGPT, has been widely perceived as a milestone toward large foundation models. While standard machine learning models are often-wise trained on single-source, domain-specific dataset, which is largely limited by the data collection and application scope. Large foundation models are based on the transformer neural network architectures [3], and are trained on unprecedented scale of compute and data. Once trained, they exhibit impressive, generalizable capabilities across a variety of domains and tasks including reasoning, abstraction, understanding and projection. Such models indeed revolutionize the development and implementation of machine learning algorithms and reveal many novel opportunities.</p>
<p>The equivalent- or even beyond-human-level abilities provided by foundation models motivate us to explore the emerging opportunities in boosting the performance of power system modeling and operations. [4] explores the pre-training of large transformer model for power system time-series forecasting and imputation, while in [5] the authors show early success on applying ChatGPT for generating test circuits in an interactive</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>way. However, many abilities provided by foundation models are yet to be utilized for improving the reliability and efficiency of power networks. For instance, such large foundation models have been shown to be able to output understanding, reasoning, and using tools for a wide variety of domains to some degree [6]. Such capabilities can potentially help or even change the operational procedures for large-scale power networks.</p>
<p>In this paper, we systematically develop application pipelines and examine the capabilities of large foundation models on a set of power system modeling and operation tasks. We find using off-the-shelf pre-trained foundation models such as GPT-4 and GPT-4 Vision (GPT-4V), it is indeed efficient to create multiple novel pathways in helping engineers, policymakers, and energy users for solving challenging problems in the power and energy domain. These problems often involve high-dimensional unstructured data, such as interacting with plain-language technical documents, system measurements or image data. In addition, by leveraging the power of large foundation models, it effectively removes the barriers for users to interact with the power systems. This differentiates from other machine learning implementation for power grid applications. For instance, previous research identified machine learning algorithms can successfully predict locational marginal price and power dispatch signals [7], [8]. But this needs trained engineers to carefully craft labeled operational data and specific machine learning models. On the contrary, large foundation models can potentially ease the data collection and model training burden, while providing instant feedback for system operators and energy users [9]. To explore the capabilities provided by large foundation models on power system tasks, we design specific prompts and interactive paradigms, and show foundation models can directly solve simplified optimal power flow (OPF) problems by only interacting with prompts without using optimization solvers; we also demonstrate LLMs can serve as an intermediary between end users and EV charging control algorithms, so that optimal scheduling models can be easily set up based on users' preferences; while pre-trained large foundation models can lack accurate knowledge in power and energy systems, we find that using retrieval-augmented generation [10] tackles such challenges by embedding large technical reports in specific domains.</p>
<p>In particular, we focus on the unique characteristics of</p>
<p>power system modeling and operations [11], as it exhibits varying complexity and domain knowledge, compared to other general natural language processing or vision-related tasks. The example applications shown in this paper can be extended to a variety of power system tasks, such as power grid modeling [11], market operations [12], situation awareness [13], energy management and power dispatch [14], anomaly detection [15], and forecasting tasks under various settings [16]. To facilitate the development and utilization of such powerful tools, we make our algorithms and detailed simulation testing cases, especially the multi-modal and additional time-series forecasting examples publicly available ${ }^{1}$.</p>
<h2>II. Methodology</h2>
<h2>A. LLM as Optimizer</h2>
<p>Optimization have been regarded as one of the foundational tools for solving various decision-making problems in power system, such as the OPF, EV charging scheduling, and building energy system management [17]. Although researchers have provided numerous solutions, there still exist challenges from both computation cost and algorithm performance perspectives. In particular, for a variety of users lacking domain knowledge in optimization or power systems, it is impractical for them to learn to model and solve such problems [18].</p>
<p>Optimization by PROmpting (OPRO) demonstrates the ability of LLMs to solve optimization problem by describing the problem with natural language, and then iteratively generates new solutions based on task description and previously generated solutions [19]. We further develop a framework named LLM for the OPF (LLM4OPF) to solve constrained optimization problems as shown in Fig. 1(a). In this framework, we generate prompt with historical solution-cost pairs and task description. In each optimization step, several solution-cost pairs with minimal cost values are retrieved from solutioncost pair buffer, and LLM optimizer generates new solution based on the well-designed prompt. The new solution which satisfies physical constraints will be stored in solution-cost pair buffer. The detailed prompt and response example is described in our online repository [20]. We additionally explore the ability of LLM4OPF to quickly adapt to new problems with different constraints, given new task description and solutions of previous problem.</p>
<p>Though OPRO can find good quality solutions in some optimization problems, the inefficiency in searching solutions space limits its application in optimization problems with larger scale. [18] empirically shows the ability of LLMs to generate code, while occasionally triggering errors during execution. Hence, instead of relying on LLMs to generate the entire code, we introduce function templates for the optimization problem in the novel framework named LLM for EV (LLM4EV). LLMs interacts with end users, comprehends inquiries, writes code to invoke the functions, and explains the results of code execution to end users. Fig. 1(b) illustrates</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>our interactive process between users and LLMs. Complete interaction process is detailed in [20].</p>
<h2>B. LLM for Knowledge Retrieval</h2>
<p>In this subsection, we introduce our approach of adopting LLMs to achieve summarization and question answering (QA) on energy field-based long-textual documents, named LLM4Doc. The framework of our approach is depicted in Fig. 1(c), where we adopt retrieval augmented generation (RAG) and prompt engineering (PE) as workhorses. RAG is a hybrid mechanism based on natural language generation and information retrieval. It is capable of enriching LLMs' knowledge by pulling in information from a corpus of useful data, much like fetching a book from a library [10]. Integrated with RAG, LLMs can generate not only contextually accurate, but also information-rich outputs. Retrieval involves searching for relevant information from a large corpus of data, such as textual files and database. Only using retrieval can probably generate inaccurate or unrelated outputs. Augmentation involves incorporating the retrieved information into LLM's internal representation of knowledge to significantly improve the output quality of LLMs. Generation involves generating text from LLMs.</p>
<p>We firstly process a document with long text into vector embeddings. Then, we use integrate these vectors into an external knowledge base. Based on this base, we ask GPT4 to do summarization and multiple QAs on the processed document. On the other hand, prompt engineering is a crucial step in LLM development, which focuses on refining the language prompts to help LLMs understand human's requirements better, and produce fairly accurate, relevant, and information-rich outputs [21]. There are various purposes of prompt engineering. Specifically, in this paper, we design a format for our prompts, where humans can customize different parts to get diversified answers, detailed in Fig. 2.</p>
<h2>C. Large Multimodal Models for Situation Awareness</h2>
<p>Multi-modal models are able to connect learned representations across diverse data types, e.g. language, vision, and etc, to gain a more comprehensive understanding and generate appropriate responses. Current state-of-the-art multimodal models are built upon LLMs with incremental pretraining with image-text pairs [22], so the resulting models can achieve a variety of tasks with multiple types of model input and output data. Such capabilities can provide valuable tools for power system applications, as system measurements and logging data are naturally large scale and multi-modal.</p>
<p>In particular, we look into the situation awareness problem which can be tackled by large multi-modal models [13]. To guarantee the operation safety, system operators need to make timely and efficient measurements processing, operating states comprehension, and anomaly detection. For instance, imaging data have been used in [23] to identify the insulator surface status. However, there are limited sample availability of target situations and limited diversity of operating situations, such as the sub-station usually stays in normal stage with scare fire</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(1) Retrieve data. (2) Input prompt. (3) Generate new solution. (4) Evaluates whether the new solution can satisfy constraints. (5) If yes, calculate the cost. (6), (7) Derive solution-cost pair and store. (8) When finish, output the solution with minimal cost.
(b)
<img alt="img-1.jpeg" src="img-1.jpeg" />
(1) Describe task. (2) User query. (3) Ask user to input parameters. (4) User response.
(5) Generate code to invoke function. (6) Python interpreter execution. (7) Explain results to user.
(c)
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 2: The format of proposed engineered prompts for processing technical documents in power systems.
disaster occurrence, which render it hard to foster a dedicated fire detector. Hence, a zero/few-shot situation monitor with re-
liable and aligned vision-text representations is highly favored. GPT-4V is a competent candidate [24], which owns potent image-text retrieval ability to produce accurate scene descriptions for measured images, as well as outstanding zero/fewshot generalization inference and scenarios transferability to discern rare situations. In this work, we propose a framework named LLM4SA by adopting in-context learning plus prompt engineering [25] which circumvents cumbersome post-hoc fine-tuning. In this regard, proper prompt design plays a key role which should provide multi-modal QA exemplars of high correspondence and concrete detection task or situation category information [26].</p>
<p>There are two orthogonal ways to achieve desirable situation understanding. One way is instruction tuning coupled with prompt learning [27], which needs to handcraft domain-</p>
<p>specific input-label pairs and use them to tailor an ad-hoc prompt embedding and refine original parameters for LLMs. But this method is hard to execute due to formidable data collection and computational efforts for large parameter finetuning. The other way is in-context learning plus prompt engineering [25] which circumvents cumbersome post-hoc finetuning. In this regard, proper prompt design plays a key role which should provide multi-modal query-answer exemplars of high correspondence and concrete detection task or situation category information [26]. In this work, we majorly focus on the in-context learning capabilities to attain the zero/few-shot situation awareness by curating detailed in-context prompts.</p>
<h2>III. APPLICATIONS IN POWER SYSTEMS</h2>
<h2>A. Optimal Power Flow</h2>
<p>In this paper, we consider the OPF problem with simplified constraints on nodal power balance and power flow equations. The detailed formulation can be found in [20]. In this problem, we minimize the quadratic power generation costs $C_{i}\left(P_{i}\right)=$ $a_{i} P_{i}^{2}+b_{i} P_{i}+c_{i}, i \in \Omega$, where $P_{i}$ denotes power output of generator $i \in \Omega . a_{i}, b_{i}$ and $c_{i}$ denote cost coefficients. All generators have power generation limits.</p>
<p>We evaluate LLM4OPF on a 5-unit testing case [28], and compare its performance with Gurobi solver [29] on two tasks. In the first task, we set the sum of nodal load $L_{e}=400$ and instruct GPT-4 to perform $n=300$ optimization steps. Subsequently in the second task, we set $L_{e}=405$ and include the solution-cost pairs obtained from the first experiment as part of the prompt. We then tasked GPT-4 API to perform 50 optimization steps to evaluate the the ability of LLM4OPF to adapt to new tasks with different constraints.</p>
<p>The experimental results depicted in Fig. 3(a) and Fig. 3(b) demonstrate that the LLM4OPF can iteratively optimize to minimize the cost. Note that we only store and plot the solutions that satisfy constraints. As shown in Table. I, the cost and generator thermal output values obtained by LLM4OPF closely match those achieved by the Gurobi optimizer. Furthermore, it is evident that LLM4OPF can achieve the solution for the new task with fewer optimization steps when provided with the solution to relevant problems. These results show the strong capabilities of LLMs in solving optimization tasks.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 3: OPF testing case (a). total costs; (b). generation values.</p>
<h2>B. Electric Vehicle Charging</h2>
<p>We consider a charging station which can provide charging service simultaneously for a set of EV $j \in \mathcal{V}$ [30]. The</p>
<p>TABLE I: Comparison of Experiment Results on EV Charging.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Optimizer</th>
<th>Cost</th>
<th>$P_{1}$</th>
<th>$P_{2}$</th>
<th>$P_{3}$</th>
<th>$P_{4}$</th>
<th>$P_{5}$</th>
<th>n</th>
</tr>
</thead>
<tbody>
<tr>
<td>OPF</td>
<td>Gurobi</td>
<td>131455.000</td>
<td>102.844</td>
<td>90.000</td>
<td>76.730</td>
<td>77.425</td>
<td>53.000</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>131456.090</td>
<td>103.000</td>
<td>90.000</td>
<td>77.000</td>
<td>77.000</td>
<td>53.000</td>
<td>500</td>
</tr>
<tr>
<td>OPF</td>
<td>Gurobi</td>
<td>134670.416</td>
<td>104.850</td>
<td>90.000</td>
<td>78.216</td>
<td>78.934</td>
<td>53.000</td>
<td>-</td>
</tr>
<tr>
<td>$L_{e}=405$</td>
<td>GPT-4</td>
<td>134691.790</td>
<td>106.000</td>
<td>90.000</td>
<td>79.000</td>
<td>77.000</td>
<td>53.000</td>
<td>50</td>
</tr>
</tbody>
</table>
<h2>LLM4EV</h2>
<p>System: You are an AI assistant specialized in solving EV charging problems. You have been provided with a predefined function called solve_EV() that is capable of addressing various EV charging problems:</p>
<div class="codehilite"><pre><span></span><code>def Solve_EV(num_of_vehicles, timesteps,
    initial_states, max_power,
</code></pre></div>

<div class="codehilite"><pre><span></span><code>terminal_states, dept_time, power_capacity, plot_fig) :
・.
User:Can you help me to schedule the charging of electric vehicles?
</code></pre></div>

<p>Assistant:Of course, I'd be happy to help. To provide the best result, I need some specific details from you. Could you please tell me the following:... What are the initial states (charge levels) of your vehicles?</p>
<p>Please list for each vehicle if more than one... User: 3. They all start from zero, ...</p>
<p>Assistant: Thank you for the information. Based on your input, we can utilize our EV charging solver as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> starting from zero for all vehicles
initial_states = [0, 0, 0, 0, 0]
</code></pre></div>

<p>Fig. 4: LLM4EV experiment results.</p>
<p>electric vehicle charging optimization problem is formulated to optimize charging power $u_{j}(t)$ for each EV, subject to the EV charging dynamics $x_{j}(t)=x_{j}(t-1)+\delta u_{j}(t)$. $\delta$ and $x_{j}(t)$ denote the charging efficiency and EV state-of-charge at time $t$ respectively. We conduct experiment using a test case consisting of 5 charging sessions and 20 time steps to evaluate the performance of LLM4EV framework. Through this test, we primarily assess the performance of our proposed method in the following aspects:</p>
<ul>
<li>Given task description and code example, can LLMs utilize natural language to ask user for required parameters to solve EV charging optimization problem?</li>
<li>Can LLMs comprehend EV charging optimization problems described in natural language, and generate correct code to call the provided functions?</li>
<li>Can LLMs accurately interpret the output of the code and explain the results to the user?
From Fig.4, we discover that GPT-4 can intelligently generate precise questions to users based on the provided function and task description. Remarkably, the 8 questions raised perfectly correspond to the 8 arguments of the function solve_EV. Furthermore, even when users provide answers in natural language, GPT-4 demonstrates a strong ability to comprehend and generate corresponding code. For instance, when presented with the question "what are the initial states (charge levels) of your vehicles? Please list for each vehicle if more than one", the user's answer is "they all start from zero". Subsequently, GPT-4 generates accurate code 'initial_states = [0, 0, 0, 0, 0]'. See [20] for detailed experiment results.</li>
</ul>
<h2>C. Situation Awareness via Multimodal Capabilities</h2>
<p>We utilize multimodal capabilities of GPT-4V to achieve situation awareness based on scenario images. Situation awareness usually involves classification tasks such as anomaly detection. In this part, we demonstrate the results of satellite image-based wildfire detection [31], a binary classification task. We design 4 approaches: (1). One image query and directly asking if wildfire exists in the image; (2). One image query and applying prompt engineering; (3). Multiple image examples along with ground true labels as the engineered prompts; (4). Based on (3), adding language explanations associated with true labels.</p>
<p>To get average accuracy of each approach, we take multipleround experiments, where 5 positive images and 5 negative images are involved in each round. An example of Approach (3) is demonstrated in Fig. 5 and see [20] for complete results. Among all approaches, the Approach 4) achieves the highest accuracy $91 \%$, proving that GPT-4V is capable of utilizing multimodal information to improve performance, while the Approach (3) achieves $86 \%$, also indicating the effectiveness of few-shot prompting. However, it is worth mentioning GPT4V's performance on specific image classification tasks is still inferior to standard ML models such as convolutional neural network with $94 \%$ accuracy, and random forest with $96 \%$ accuracy. Even so, GPT-4V holds remarkable potential in saving the training time and data collection processes, which</p>
<h2>LLM4SA</h2>
<p>Image inputs:
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Detection target
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Few-shot prompts</p>
<p>System: This dataset contains satellite images about wildfire in Canada, using Longitude and Latitude coordinates for each wildfire spot ( $&gt;0.01$ acres burned) found. Areas after wildfire may demonstrate different color in satellite images. You are a professor of forestry, and good at observing satellite images. I will give you several examples of satellite images with "yes" or "no" to specify if wildfire happened. The truths of the first 5 images are "yes", "yes", "yes", "no", "no".
Question: Now, let's think step by step, and tell me, had wildfire happened in the last picture?
Answer: ...Considering this observation, it is likely that the last picture depicts an area where wildfires have occurred.</p>
<p>Fig. 5: An example of LLM4SA inference.
are needed for typical ML methods. Furthermore, GPT-4V have more flexibility on accepting and generating multimodal outputs, such as generating explanations of current states.</p>
<h2>D. Energy Context-based Document Comprehension</h2>
<p>In this experiment, we construct a pipeline shown in Sec II-B for profiling and retrieving knowledge from power system documents, and apply it on the 177-page Federal Energy Regulatory Commission (FERC) Order No. 901 [32]. This FERC order develops reliability standards addressing reliability gaps with inverter-based resources involved. To explore GPT-4's capability of different QA tasks, we classify questions into 2 types. General questions ask about the overall content of the document, such as "Please summarize this file.", "What is the structure of this technical report?". While Technical questions focus more on the technical details mentioned in the document, such as "What is the phaselock loop Synchronization in this file?", "Please tell me, in Reliability Standards in this file, which standard is the most useful one?" Complete evaluation questions list is in [20]. To study the professional capability brought by RAG, we compare the answers on two technical questions as shown in Fig. 6. GPT-4 with RAG is capable of handling deeper comprehension of large energy context-based documents. We can see that without RAG, GPT-4 but cannot understand</p>
<h1>LLM4Dos</h1>
<h2>Directly ask questions.</h2>
<p>Question: What is Synchronization?
Answer: The synchronization requires generators to maintain synchronism where possible and continue to inject current to support system stability.</p>
<p>Ask based on RAG.
Question: What is Synchronization?
Answer: ... synchronization in this document refers to the synchronization status between the inverter and the grid during Bulk-Power System (BPS) events. Specifically, when a BPS event (such as a fault) causes instantaneous synchronization loss, ... The purpose of synchronization is to ensure that the inverter can continue to provide power and resynchronize with the grid...</p>
<h2>Directly ask questions.</h2>
<p>Question: What is phase-lock loop Synchronization?
Answer: I don't know.</p>
<h2>Ask based on RAG.</h2>
<p>Question: What is phase-lock loop Synchronization?
Answer: ... Specifically, PLL synchronization refers to the inverter continuing to inject grid current during system disturbances, locking the phase-locked loop to the last synchronization point, and continuing to inject current into the grid at its calculated phase until the phase-locked loop can resynchronize. ... maintaining PLL synchronization is important for protecting and controlling ...</p>
<p>Fig. 6: Evaluation of two technical question answerings based on FERC Order No. 901 with and without RAG.
TABLE II: Summary of use cases, limitations and possible future developments</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Limitations</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">Potential Solutions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">OPF</td>
<td style="text-align: center;">Expensive computation <br> Sample-specific optimization</td>
<td style="text-align: center;">Takes multiple iterations to converge <br> Only work on sample-by-sample load</td>
<td style="text-align: center;">Improved initialization and few-shot prompts design <br> Few-shot learning and reasoning across batch of system conditions</td>
</tr>
<tr>
<td style="text-align: center;">EV</td>
<td style="text-align: center;">Code template dependent</td>
<td style="text-align: center;">When changing problem formulation, new <br> functions are required</td>
<td style="text-align: center;">Expand function library for various problem formulations</td>
</tr>
<tr>
<td style="text-align: center;">Knowledge <br> Retrieval</td>
<td style="text-align: center;">Answer inconsistency <br> Lack of specific knowledge</td>
<td style="text-align: center;">Generate different but useful answer sets <br> Limited to given documents</td>
<td style="text-align: center;">Ensemble learning and improved alignment <br> Fine-tuning on domain specific or private datasets</td>
</tr>
<tr>
<td style="text-align: center;">Situation <br> Awareness</td>
<td style="text-align: center;">Unstructured and multi-source <br> input data, e.g., meter read- <br> ings, image snapshots</td>
<td style="text-align: center;">Measurements come from heterogeneous de- <br> vices and data sources, making it hard to <br> align for foundation models</td>
<td style="text-align: center;">Different modalities, such as PMU measurements and images, pro- <br> vide different "views" that complement each other, and need to be <br> integrated to facilitate a holistic understanding</td>
</tr>
</tbody>
</table>
<p>relatively complex technical phrases. After using RAG, GPT4 can not only interpret technical words in details, but also explain their importance or purpose. This helps understanding technical reports as well as embedding user-specific power system knowledge into foundation models.</p>
<h2>IV. LIMITATIONS, SUMMARY AND Future WORK</h2>
<p>In this paper, we explore a set of promising applications, promises and challenges of developing foundation models for power systems. The potential of such models are validated by a range of comprehension, modeling, and operation tasks. We show the noteworthy performance provided by LLMs and multi-modal models without fine-tuning or further training. We conclude that such strong capabilities of foundation models can provide system operators, policymakers, and end users efficient tools.</p>
<p>While we note that large foundation models are still at the initial stage for industrial deployments, since there still exists limited understanding to the model's behaviors, and needs thorough evaluation and testing on real-world scenarios. We also notice a set of limitations on using the current foundation models, such as model validity, efficiency and applicable scenarios, as detailed in Table II.</p>
<h2>REFERENCES</h2>
<p>[1] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg et al., "Sparks of artificial general intelligence: Early experiments with gpt-4," arXiv preprint arXiv:2303.12712, 2023.
[2] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., "Learning transferable visual models from natural language supervision," in International conference on machine learning. PMLR, 2021, pp. 8748-8763.
[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.
[4] Anonymous, "Powergpt: Foundation model for power systems," OpenReview, 2023.
[5] R. S. Bonadia, F. C. Trindade, W. Freitas, and B. Venkatesh, "On the potential of chatgpt to generate distribution systems for load flow studies using opendss," IEEE Transactions on Power Systems, 2023.
[6] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang, B. Qian et al., "Toolllm: Facilitating large language models to master 16000+ real-world apis," arXiv preprint arXiv:2307.16789, 2023.
[7] Y. Chen, L. Zhang, and B. Zhang, "Learning to solve dcopf: A duality approach," Electric Power Systems Research, vol. 213, p. 108595, 2022.
[8] N. Guha, Z. Wang, M. Wytsck, and A. Majumdar, "Machine learning for ac optimal power flow," arXiv preprint arXiv:1910.08842, 2019.
[9] T. Wu, M. Terry, and C. J. Cai, "Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts," in Proceedings of the 2022 CHI conference on human factors in computing systems, 2022, pp. 1-22.
[10] P. Lewis, E. Perez et al., "Retrieval-augmented generation for knowledge-intensive nlp tasks," Advances in Neural Information Processing Systems, vol. 33, pp. 9459-9474, 2020.
[11] P. Kundur, "Power system stability," Power system stability and control, vol. 10, pp. 7-1, 2007.
[12] D. S. Kirschen and G. Strbac, Fundamentals of power system economics. John Wiley \&amp; Sons, 2018.
[13] M. Panteli and D. S. Kirschen, "Situation awareness in power systems: Theory, challenges and applications," Electric Power Systems Research, vol. 122, pp. 140-151, 2015.
[14] L. A. Roald, D. Pozo, A. Papavasiliou, D. K. Molzahn, J. Kazempour, and A. Conejo, "Power systems optimization under uncertainty: A review of methods and applications," Electric Power Systems Research, vol. 214, p. 108725, 2023.
[15] C.-W. Ten, C.-C. Liu, and G. Manimaran, "Vulnerability assessment of cybersecurity for scada systems," IEEE Transactions on Power Systems, vol. 23, no. 4, pp. 1836-1846, 2008.
[16] M. Shahidehpour, H. Yamin, and Z. Li, Market operations in electric power systems: forecasting, scheduling, and risk management. John Wiley \&amp; Sons, 2003.
[17] D. K. Molzahn, F. Dörfler, H. Sandberg, S. H. Low, S. Chakrabarti, R. Baldick, and J. Lavaei, "A survey of distributed optimization and</p>
<p>control algorithms for electric power systems," IEEE Transactions on Smart Grid, vol. 8, no. 6, pp. 2941-2962, 2017.
[18] M. Jin, B. Sel, F. Hardeep, and W. Yin, "A human-on-the-loop optimization autoformalism approach for sustainability," arXiv preprint arXiv:2308.10380, 2023.
[19] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen, "Large language models as optimizers," arXiv preprint arXiv:2309.03409, 2023.
[20] C. Huang, S. Li, R. Liu, H. Wang, and Y. Chen, "Online materials for large foundation models in power systems," https://github.com/ chenmmnyize/LLM_PowerSystems, accessed: 2023-11-20.
[21] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar, J. Spencer-Smith, and D. C. Schmidt, "A prompt pattern catalog to enhance prompt engineering with chatgpt," arXiv preprint arXiv:2302.11382, 2023.
[22] M. Tsimpoukelli, J. L. Menick et al., "Multimodal few-shot learning with frozen language models," Advances in Neural Information Processing Systems, vol. 34, pp. 200-212, 2021.
[23] D. Sadykova, D. Pernebayeva, M. Bagheri, and A. James, "In-yolo: Realtime detection of outdoor high voltage insulators using uav imaging," IEEE Transactions on Power Delivery, vol. 35, no. 3, 2019.
[24] Z. Yang, L. Li, K. Lin, J. Wang, C.-C. Lin, Z. Liu, and L. Wang, "The dawn of lmms: Preliminary explorations with gpt-4v (ision)," arXiv preprint arXiv:2309.17421, vol. 9, 2023.
[25] Y. Cao, X. Xu, C. Sun, X. Huang, and W. Shen, "Towards generic anomaly detection and understanding: Large-scale visual-linguistic model (gpt-4v) takes the lead," arXiv preprint arXiv:2311.02782, 2023.
[26] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer, "Rethinking the role of demonstrations: What makes in-context learning work?" in EMNLP, 2022.
[27] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, "Finetuned language models are zero-shot learners," arXiv preprint arXiv:2109.01652, 2021.
[28] A. Soroudi, Power system optimization modeling in GAMS. Springer, 2017, vol. 78.
[29] L. Gurobi Optimization, "Gurobi optimizer reference manual," 2021.
[30] Y. Chen and B. Zhang, "State-of-charge aware ev charging," in 2022 IEEE Power \&amp; Energy Society General Meeting (PESGM). IEEE.
[31] A. AABA, "Wildfire prediction dataset (satellite images)," https: //www.kaggle.com/datasets/abdelghaniaaba/wildfire-prediction-dataset, accessed: 2023-02-02.
[32] "Reliability standards to address inverter-based resources," https://www. ferc.gov/media/e-1-rm22-12-000, Federal Energy Regulatory Commission, Accessed: 2023-10-19.</p>
<h2>APPENDIX</h2>
<h2>A. Optimal Power Flow</h2>
<p>We consider a simple economic dispatch problem of thermal units, which transform the fuel-based source of energy into electricity [28]. The thermal unit ecomomic dispatch problem is formulated as follows:</p>
<p>$$
\begin{array}{ll}
\min &amp; \sum_{i \in \Omega} C_{i}\left(P_{i}\right) \
\text { s.t. } &amp; C_{i}\left(P_{i}\right)=a_{i}\left(P_{i}\right)^{2}+b_{i} P_{i}+c_{i}, i \in \Omega \
&amp; P_{i}^{\min } \leq P_{i} \leq P_{i}^{\max }, i \in \Omega \
&amp; \sum_{i \in \Omega} P_{i} \geq L_{e}
\end{array}
$$</p>
<p>In this problem, we optimize over power output $P_{i}$ of generator $i \in \Omega$ to minimize generation costs calculated in (1b). $a_{i}, b_{i}$ and $c_{i}$ denotes cost coefficients. (1c) and (1d) are operating limits and total generation constraints, respectively. $P_{i}^{\min }$ and $P_{i}^{\max }$ represents maximum/minimum power outputs of thermal unit $i . L_{e}$ is demand.</p>
<h2>LLM40PE</h2>
<p>System: You need assistance in solving an optimization problem. This problem involves 5 optimization variables, namely p1, p2, p3, p4, and p5. These variables are subject to constraints defined by their minimum and maximum values: $p_{\min }=[28$, $90,68,76,19]$ and $p_{\max }=[206,284,189,266,53]$. Additionally, the sum of p1, p2, p3, p4, and p5 must be greater than or equal to 400 .
Your objective is to provide values for p1, p2, p3, p4, and p5 that satisfy the constraints and minimize the optimization objective.
Below are some previous solution and their objective value pairs. The pairs are arranged in descending order based on their function values, where lower values are better.</p>
<div class="codehilite"><pre><span></span><code>Solution 1: p1, p2, p3, p4, p5 =
    120.0, 90.0, 70.0, 85.0, 40.0\\
Objective value 1: 141843.15\\
Solution 2: p1, p2, p3, p4, p5 =
    110.0, 95.0, 75.0, 85.0, 35.0\\
Objective value 2: 137350.95\\
</code></pre></div>

<p>・. .</p>
<p>Give me a new (p1, p2, p3, p4, p5) pair that is different from all pairs above, and has a function value lower than any of the above. Do not give me any explanation, the form of response must strictly follow the example: p1, p2, p3, p4, p5 = 123.11, 80.2, 99.67, 101.52, 37</p>
<h2>B. Electric Vehicle Charging</h2>
<p>The full EV charging scheduling problem is formulated as follows:</p>
<p>$$
\begin{array}{ll}
\min <em _in="\in" _mathcal_V="\mathcal{V" j="j">{\mathbf{u}} &amp; \sum</em>\right|}}\left|x_{j}(T)-x_{j, \text { depart }<em j="j">{2}^{2} \
\text { s.t. } &amp; u</em> \
&amp; \sum_{j \in \mathcal{V}} u_{j}(t) \leq P(t), t \in \mathcal{T} \
&amp; \underline{u}}(t)=0, t \notin\left[t_{j, \text { arrival }}, t_{j, \text { depart }}\right), j \in \mathcal{V<em j="j">{j} \leq u</em>}(t) \leq \bar{u<em j="j">{j}, t \in \mathcal{T}, j \in \mathcal{V} \
&amp; x</em> \
&amp; x_{j}(t)=x_{j}(t-1)+\delta u_{j}(t), t \geq 1
\end{array}
$$}(0)=x_{j, \text { initial }}, j \in \mathcal{V</p>
<p>In this problem, the objective is to meet terminal energy demand of each EV. (2b) indicates that charging power is only available when the charging session is alive. $t_{j, \text { arrival }}$ and $t_{j, \text { depart }}$ represent the arrival time and expected departure time of the $j_{t h}$ EV. (2c) and (2d) define total power constraint and charging rate constraint of individual EV, respectively. With (2e) and (2f), the battery state is initialized with $x_{j, \text { initial }}$, and updated with charging rate $u_{j}(t) . \delta$ denotes charging efficiency.</p>
<h2>LLM4EV</h2>
<p>System: You are an AI assistant specialized in solving EV charging problems. You have been provided with a predefined function called solve_EV() that is capable of addressing various EV charging problems:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="k">def</span><span class="w"> </span><span class="nf">Solve_EV</span><span class="p">(</span><span class="n">num_of_vehicles</span><span class="p">,</span>
    <span class="n">timesteps</span><span class="p">,</span> <span class="n">initial_states</span><span class="p">,</span>
    <span class="n">max_power</span><span class="p">,</span> <span class="n">terminal_states</span><span class="p">,</span>
    <span class="n">dept_time</span><span class="p">,</span> <span class="n">power_capacity</span><span class="p">,</span>
    <span class="n">plot_fig</span><span class="p">):</span>
    <span class="n">x_terminal</span><span class="o">=</span><span class="n">cp</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
        <span class="n">num_of_vehicles</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;</span>
        <span class="n">x_terminal</span><span class="s1">&#39;)</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">num_of_vehicles</span>
        <span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;x0&#39;</span><span class="p">)</span>
    <span class="n">max_sum_u</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;</span>
        <span class="n">max_sum_u</span><span class="s1">&#39;)</span>
    <span class="n">u_max</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
        <span class="n">num_of_vehicles</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;u_max&#39;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">num_of_vehicles</span><span class="p">,</span>
        <span class="n">timesteps</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">num_of_vehicles</span><span class="p">,</span>
        <span class="n">timesteps</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;u&#39;</span><span class="p">)</span>
    <span class="n">x_terminal</span><span class="o">.</span><span class="n">value</span><span class="o">=</span><span class="n">terminal_states</span>
    <span class="n">x0</span><span class="o">.</span><span class="n">value</span><span class="o">=</span><span class="n">initial_states</span>
    <span class="n">max_sum_u</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">power_capacity</span>
    <span class="n">u_max</span><span class="o">.</span><span class="n">value</span><span class="o">=</span><span class="n">max_power</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="kp">ones</span><span class="p">((</span>
        <span class="n">num_of_vehicles</span><span class="p">,</span> <span class="p">))</span>
    <span class="n">obj</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">constr</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="o">&lt;=</span> <span class="n">x_terminal</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
        <span class="n">constr</span> <span class="o">+=</span> <span class="p">[</span><span class="n">x</span><span class="p">[:,</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">x</span><span class="p">[:,</span><span class="n">t</span><span class="p">]</span>
            <span class="o">+</span> <span class="n">u</span><span class="p">[:,</span><span class="n">t</span><span class="p">],</span>
        <span class="n">u</span><span class="p">[:,</span><span class="n">t</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">u_max</span><span class="p">,</span>
        <span class="n">u</span><span class="p">[:,</span><span class="n">t</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">cp</span><span class="o">.</span><span class="kp">sum</span><span class="p">(</span><span class="n">u</span><span class="p">[:,</span><span class="n">t</span><span class="p">])</span> <span class="o">&lt;=</span> <span class="n">max_sum_u</span><span class="p">,</span>
        <span class="n">u</span><span class="p">[:,</span><span class="n">t</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="kp">ones_like</span><span class="p">(</span>
            <span class="n">dept_time</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">dept_time</span><span class="p">)</span>
            <span class="o">*</span><span class="mf">100.0</span><span class="o">+</span><span class="mf">0.000001</span><span class="p">]</span>
    <span class="n">obj</span> <span class="o">-=</span> <span class="n">cp</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span>
        <span class="n">x_terminal</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Problem</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">Maximize</span><span class="p">(</span><span class="n">obj</span>
        <span class="p">),</span> <span class="n">constr</span><span class="p">)</span>
    <span class="n">prob</span><span class="o">.</span><span class="kp">solve</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">plot_fig</span><span class="o">==</span><span class="kc">True</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>

<p>plt.show()
return x.value, u.value
When a user requests you to solve an EV charging problem, you should ask the user in natural language to provide the necessary parameters. Then, based on user's response, you should generate code to invoke solve_EV() function. Here is an example:</p>
<div class="codehilite"><pre><span></span><code>num_of_vehicles = 3
timesteps = 10
initial_states = [0, 0, 0]
max_power = 10.0
terminal_states = [70, 50, 100]
dept_time = [8, 6, 10]
power_capacity = 20
solve_EV(num_of_vehicles, timesteps,
    initial_states, max_power,
    terminal_states, dept_time,
    power_capacity, plot_fig)
</code></pre></div>

<p>User:Can you help me to schedule the charging of electric vehicles?</p>
<p>Assistant:Of course, I'd be happy to help. To provide the best result, I need some specific details from you. Could you please tell me the following:</p>
<ol>
<li>How many vehicles do you have that you need to charge?</li>
<li>Over how many timesteps are you planning to charge your vehicles? A timestep could be an hour, for example.</li>
<li>What are the initial states (charge levels) of your vehicles? Please list for each vehicle if more than one.</li>
<li>What's the maximum power each of your vehicles can consume?</li>
<li>What are the final desired states (charge levels) for each of your vehicles?</li>
<li>Until what timestep does each of your vehicles not need to start charging? Please list for each vehicle if more than one.</li>
<li>What's the total power capacity you want to use for charging all your vehicles?</li>
<li>Finally, would you like a plot figure of the charging status?</li>
</ol>
<p>User:1. five,
2. 20 hours,
3. They all start from zero,
4. 10 ,
5. 100 ,
6. $[10,12,16,18,20]$,</p>
<ol>
<li>30 .</li>
</ol>
<p>Assistant: Thank you for the information. Based on your input, we can utilize our EV charging solver as follows:</p>
<div class="codehilite"><pre><span></span><code>num_of_vehicles = 5
timesteps = 20
<span class="gh">#</span> starting from zero for all
    vehicles
initial_states = [0, 0, 0, 0, 0]
max_power = 10.0
<span class="gh">#</span> desired state is 100 for all
    vehicles
terminal_states = [100, 100, 100,
    100, 100]
<span class="gh">#</span> staggered start times for charging
dept_time = [10, 12, 16, 18, 20]
power_capacity = 30
Solve_EV(num_of_vehicles, timesteps,
    initial_states, max_power,
    terminal_states, dept_time,
    power_capacity)
</code></pre></div>

<p>Python Interpreter Execution.
Assistant: The optimal schedule is as follows: [[10, $10,10,10,10,10,10,10,10,0,0,0,0,0,0,0,0$, $0,0],[8.229,8.229,8.229,8.229,8.229,8.230$, $8.230,8.231,8.232,8.840,8.853,0,0,0,0,0,0,0$, $0],[4.978,4.978,4.978,4.978,4.978,4.978,4.978$, $4.978,4.978,4.978,7.668,7.667,8.720,8.720,8.722$, $8.726,0,0,0,0],[3.500,3.500,3.500,3.500,3.500$, $3.500,3.500,3.500,3.500,3.500,6.815,6.814,8.482$, $8.482,8.483,8.484,8.719,8.722,0,0],[2.347,2.347$, $2.347,2.347,2.347,2.347,2.347,2.347,2.347,2.346$, $5.093,5.092,8.082,8.082,8.082,8.083,8.459,8.461$, $8.546,8.540]]$.</p>
<h2>C. Energy Context-based Document Comprehension</h2>
<p>General questions care more on the entire content of the document:</p>
<p>1) Please summarize this file.
2) What is the structure of this file?
3) Please give me several key words strongly related to the content in this file.
Technical questions care more on the technical details mentioned in the document:
4) What is the Synchronization in this file?
5) What is the phase-lock loop Synchronization in this file?
6) Please summarize the main content of Synchronization part in this file.
7) Please give an example in real-world application of the synchronization mentioned in this file.
8) Please tell me, in Reliability Standards in this file, which standard is the most useful one?</p>
<p>A comparison of summaries provided by the direct answering scheme, RAG scheme, RAG plus prompt engineering, and expert human writters are provided in the following text block using GPT-4. We note that answers provided by the LLMs are not always accurate, and the incorrect parts are market in red texts.</p>
<p>Question: Please summarize this file. Directly ask questions.</p>
<h2>Answer:</h2>
<p>This document is an appendix to the NERC Rules of Procedure for Dispute Resolution. It contains the procedures for filing a complaint with the NERC Dispute Resolution Panel, including the process for filing a complaint, the timeline for the complaint process, and the requirements for filing a complaint. It also outlines the process for responding to a complaint, the timeline for the response process, and the requirements for responding to a complaint. It also provides a guideline for the 2023 DER_A Model, outlining the requirements for the model, including the necessary components, safety protocols, and testing procedures.</p>
<h2>RAG</h2>
<h2>Answer:</h2>
<p>This document provides information from the Federal Energy Regulatory Commission (FERC) Home Page on the Internet, which is available on eLibrary. The full text of the document is available in PDF and Microsoft Word format for viewing, printing, and/or downloading. To access the document, the user must type the docket number excluding the last three digits in the docket number field. The document directs NERC to consider in its standards development process whether to include in new or modified Reliability Standards a requirement that planning coordinators and transmission planners include a wide set of grid stress performance conditions (i.e., both typical and extreme conditions) in planning assessments, and whether to require planning coordinators and transmission planners to account in planning assessments for both on-peak and off-peak conditions, normal and extreme. An example of a transmission system plan and performance requirements for extreme weather is provided, with Order No. 896, 88 FR 41262 (June 23, 2023), 183 FERC u00b6 61,191, at P 198 (2023).</p>
<h2>RAG + prompt engineering</h2>
<h2>Answer:</h2>
<p>The document addresses concerns raised by commenters regarding data accessibility and burden associated with providing unregistered IBR (In-Balance Resettlement) modeling data and parameters. It acknowledges that transmission owners may face limitations in obtaining the necessary data for the reliable operation of the Bulk-Power System. Similar challenges may also be encountered by distribution providers in gathering adequate modeling data and parameters from IBR-ORRs (In-Balance Resettlement Distributed Energy Resources). To mitigate these issues, the document suggests that existing processes of transmission planners, such as the New York Independent System Operator (NYISO) and CAISO, could be utilized to obtain specific information as required. It emphasizes the importance of demonstrating the need for information and ensuring proper protection and non-disclosure agreements are in place. The document also discusses the effective date and congressional notification of the rule, specifying that it will go into effect 60 days after publication in the Federal Register. It clarifies that the rule does not meet the definition of a \major rule" as per section 351 of the Small, implying the involvement of the Office of Information and Regulatory Affairs of the OMB (Office of Management and Budget) in assessing this determination. In summary, the document addresses concerns regarding data access and burden related to unregistered IBR modeling data and parameters. It proposes leveraging existing processes of transmission planners, highlights challenges faced by transmission owners and distribution providers, and provides information on the rule's effective date and regulatory classification.</p>
<h2>Human writing</h2>
<h2>Answer:</h2>
<p>FERC Order No. 901 addresses specific matters pertaining to the impacts of inverter-based resources (IBR) on the reliable operation of the Bulk-Power System. With an unprecedented proportion of nonsynchronous generation resources which employ inverters, rectifiers, and converters projected to connect over the next decade, this order is aims at providing system planners and operators the necessary tools to plan for and reliably integrate IBRs into the Bulk-Power System or to plan for IBRs connected to the distribution system that in the aggregate have a material impact on the Bulk-Power System. This order directs NERC to develop new or modified Reliability Standards to address reliability gaps pertaining to IBRs in four areas: data sharing; model validation; planning and operational studies; and performance requirements. Such Standards should ensure that NERC registered entities have the necessary data to predict the behavior of all IBRs. They should also ensure that IBR models are comprehensive, validated, and updated in a timely manner. In addition, the standards ensure that validated IBR models are included in transmission planning and operational studies to assess the reliability impacts on Bulk-Power System performance. And the standards should require that registered IBRs provide frequency and voltage support during frequency and voltage excursions. The order also provides guidelines on Informational Filing and Reliability Standard Development Timeline.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/chennnnnyize/LLM_PowerSystems&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
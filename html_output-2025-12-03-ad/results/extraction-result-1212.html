<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1212 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1212</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1212</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-276903042</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.10171v2.pdf" target="_blank">Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies</a></p>
                <p><strong>Paper Abstract:</strong> World Model-based Reinforcement Learning (WMRL) enables sample efficient policy learning by reducing the need for online interactions which can potentially be costly and unsafe, especially for autonomous driving. However, existing world models often suffer from low prediction fidelity and compounding one-step errors, leading to policy degradation over long horizons. Additionally, traditional RL policies, often deterministic or single Gaussian-based, fail to capture the multi-modal nature of decision-making in complex driving scenarios. To address these challenges, we propose Imagine-2-Drive, a novel WMRL framework that integrates a high-fidelity world model with a multi-modal diffusion-based policy actor. It consists of two key components: DiffDreamer, a diffusion-based world model that generates future observations simultaneously, mitigating error accumulation, and DPA (Diffusion Policy Actor), a diffusion-based policy that models diverse and multi-modal trajectory distributions. By training DPA within DiffDreamer, our method enables robust policy learning with minimal online interactions. We evaluate our method in CARLA using standard driving benchmarks and demonstrate that it outperforms prior world model baselines, improving Route Completion and Success Rate by 15% and 20% respectively.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1212.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1212.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiffDreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiffDreamer (Diffusion-based World Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-fidelity video-diffusion world model introduced in this paper that predicts multiple future observations jointly (image-space) and a reward head, used to support policy learning in imagination; built on Stable Video Diffusion (SVD) and conditioned on waypoint trajectories encoded with Fourier features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DiffDreamer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A video-diffusion based visual world model that predicts H future RGB observations jointly conditioned on P past+current frames and a Fourier-embedded waypoint trajectory; architecture uses a State Encoder (EfficientNet-B0 tokenization), a decoder-only transformer backbone (n_L=4, n_H=4, d_FF=2048) to produce a 32-dim state embedding, SVD for multi-frame generation with the first P noisy frames replaced by actual past frames for grounding, plus an MLP reward-prediction head.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>video-diffusion visual world model (neural simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving (CARLA simulator, front-facing RGB input)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Perceptual / generative fidelity metrics: FID and FVD; temporal-consistency and qualitative frame visualizations; also task metrics (Route Completion, Success Rate) used as downstream utility measures.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>FID = 17.09; FVD = 130.39 (reported on CARLA driving sequences, Table III).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Primarily a neural black-box generator but provides interpretable visual outputs: generated future frames can be inspected and colored-mapped to trajectories (visual qualitative analysis shown); no claim of disentangled latents or explicit human-interpretable state variables beyond 32-dim embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Qualitative visualization of predicted video frames and top-view trajectory visualizations tied to predicted frames (Fig. 4, Fig. 5); temporal-consistency metrics; no formal latent-space probing or attention-map analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Trained on 4x NVIDIA A100 GPUs for ~28 hours with gradient accumulation (effective batch size 16); initialized from Vista weights (pretrained on ~1,700 hours driving videos); uses K=50 denoising steps for diffusion policy sampling during training/evaluation (policy side).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Reported to improve sample efficiency vs. baselines by enabling policy training in imagination — overall framework uses 1e6 online interactions (N_total_online) with a world-model warm-start of 10k iterations; empirically outperforms model-free baselines and other world models while reducing online infractions/km by 50%; explicit FLOPS or parameter-count comparisons not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When used with the paper's DPA policy, Imagine-2-Drive achieves Success Rate (SR) = 83.33% and Route Completion (RC) = 82.13% across CARLA scenarios (Table I); reduces Infraction/Km compared to baselines (0.70 infractions/km).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High visual-fidelity and temporal consistency (joint multi-frame prediction) translate to improved downstream policy performance: better SR and RC relative to Dreamer-V3 and Iso-Dreamer; authors argue simultaneous multi-frame prediction mitigates compounding one-step errors, which improves policy rollouts in imagination and thus sample efficiency and safety.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Switching from autoregressive RSSM-style one-step latent models to SVD multi-frame generation reduces compounding error and increases fidelity but increases model complexity and computational cost (video-diffusion training and sampling). Training required pretraining/warm-start and significant GPU time; diffusion-policy integration (denoising MDP) also adds computation (multiple denoising steps per decision).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Uses SVD as backbone; replaces first P noisy frames with real past frames to ground predictions; Fourier embedding of trajectory; additional MLP reward head and combined losses (diffusion reconstruction + dynamics enhancement + structure preservation + reward MSE); compact state embedding (32-dim) to balance capacity and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared directly (same DPA policy) to Dreamer-V3 and Iso-Dreamer: DiffDreamer produces substantially better fidelity (FID/FVD) and downstream driving performance (SR/RC). Authors report DiffDreamer FID 17.09 vs Dreamer-V3 FID 89.29 and Iso-Dreamer FID 102.24 and corresponding task improvements (Imagine-2-Drive SR 83.33% vs Dreamer-V3 63.33%, Iso-Dreamer 56.66%).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends (for their driving setting) joint multi-frame video-diffusion (SVD) with grounding of initial frames, Fourier trajectory encoding, context length P=5 and horizon H=9, warm-starting from a general SVD (Vista) and alternating training of world model and policy; these choices balance fidelity and task utility while keeping training tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1212.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1212.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vista</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vista (Stable Video Diffusion-based Driving World Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generalizable, high-fidelity video-diffusion driving world model cited and used as the foundation for initializing DiffDreamer; accepts diverse conditioning (actions, trajectories, text, goals) and emphasizes temporal consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vista</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A stable latent video-diffusion model designed for driving sequence prediction capable of conditioning on multiple control modalities (actions, trajectories, text commands, goal points); used here to initialize DiffDreamer weights.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>video-diffusion visual world model / generative neural simulator</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving / driving video prediction</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Perceptual generative metrics (FID, FVD) and temporal-consistency measures (as referenced); Vista is described as high-fidelity in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Generative model producing video frames—interpretability mainly via inspecting predicted videos and conditional controls; no explicit interpretability methods described in this paper's use of Vista.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None described in this paper beyond visual inspection; Vista cited for temporal attention and improved consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Pretrained on a large driving dataset (~1,700 hours of driving videos) per paper; exact training hardware unspecified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Used as a warm-start to accelerate DiffDreamer convergence; serves as a high-fidelity foundation but computational cost of pretraining is high (large dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Authors leverage Vista weights to bootstrap a task-specific world model (DiffDreamer), indicating Vista provides transferable high-fidelity generative capabilities beneficial for downstream policy learning in driving.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High-fidelity pretraining on large video datasets increases transferability but entails large data and compute costs; authors mitigate this by fine-tuning rather than training from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Video-diffusion backbone with multi-modal conditioning (actions/trajectories/text), temporal attention to improve consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Vista is described as among the most versatile and accurate generative world models; used to outperform autoregressive RSSM baselines when adapted for multi-frame prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Used as a pretrained SVD backbone then fine-tuned with task-specific reward head and grounding strategy (replace initial noisy frames with real frames) for driving.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1212.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1212.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer-V3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer-V3 (Mastering Diverse Domains through World Models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art latent world-model RL approach (Dreamer series) used as a baseline; relies on recurrent state-space latent dynamics (RSSM) and latent imagination for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering Diverse Domains through World Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer-V3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent-space world model using a recurrent state-space model (RSSM) to model dynamics in latent space and perform latent 'imagination' rollouts for policy learning; emphasizes robustness and normalization improvements over prior Dreamer versions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (RSSM-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general RL domains; here used for autonomous driving (CARLA) as a baseline</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Indirect fidelity measured by downstream task performance and model prediction quality in latent space; in this paper generative fidelity compared via FID/FVD computed from predicted image sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>FID = 89.29; FVD = 324.07 (reported in Table III for Dreamer-V3 on CARLA sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent RSSM black-box; interpretable mainly via latent trajectories or downstream behavior; no explicit interpretability methods reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported in this paper (baseline usage).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Outperforms Iso-Dreamer in task metrics but is outperformed by DiffDreamer in both fidelity metrics and downstream driving performance according to the paper's experiments (SR and RC).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Success Rate (SR) = 63.33%; Route Completion (RC) = 67.53% (Table I).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Latent imagination provides sample efficiency over model-free baselines, but one-step RSSM transitions lead to compounding error over long horizons, negatively impacting long-horizon planning in driving compared to multi-frame diffusion models.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>RSSM latent models are computationally cheaper per step than video diffusion but suffer from accumulating one-step prediction errors across long horizons, reducing temporal fidelity and downstream policy performance.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Recurrent latent dynamics (RSSM), latent-space planning/imagination, robustness/normalization techniques in V3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms model-free methods (PPO, DQN) in this domain but underperforms relative to DiffDreamer (video-diffusion multi-frame approach) in both fidelity metrics and driving task metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper implies that for long-horizon driving tasks, multi-frame, high-fidelity video generation (as in DiffDreamer/Vista) is preferable over one-step latent RSSMs to avoid compounding errors; no detailed V3 tuning provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1212.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1212.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iso-Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iso-Dreamer (Iso-Dream / Isolating Noncontrollable Dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A world-model variant that decomposes scenes into controllable and non-controllable components; used here as a baseline to compare world-model designs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Iso-Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A decomposition-style latent world model that separates action-controllable components from non-controllable visual dynamics to improve modeling and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (decompositional latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving (in paper used as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Image-space fidelity metrics (FID, FVD) reported for comparison; downstream task metrics (SR, RC) used to indicate utility.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>FID = 102.24; FVD = 421.56 (reported in Table III on CARLA sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Design aims to increase interpretability by separating controllable vs non-controllable dynamics, but in this paper it is treated as a baseline with no specific interpretability analysis shown.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Architectural decomposition (controllable vs non-controllable latents); no additional probing reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Performs worse than Dreamer-V3 and DiffDreamer in both fidelity and driving task metrics in the reported experiments; no detailed compute/time comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Success Rate (SR) = 56.66%; Route Completion (RC) = 60.33% (Table I).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Iso-Dreamer's decompositional approach does not match the downstream performance of high-fidelity multi-frame diffusion models in long-horizon driving in these experiments; likely limited by one-step latent prediction drift over horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Architectural separation for interpretability/control may limit raw visual-fidelity compared to video-diffusion approaches, leading to reduced downstream policy performance in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Latent decomposition into controllable and non-controllable components to isolate agent-influenced dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Underperforms Dreamer-V3 and DiffDreamer on both fidelity (FID/FVD) and task metrics in the CARLA experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified here; paper's results favor multi-frame high-fidelity video diffusion approaches for driving.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1212.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1212.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DriveGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DriveGAN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative model for future driving sequence prediction that conditions on actions; cited in related work as an example of generative world models for driving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DriveGAN</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DriveGAN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A generative adversarial approach for predicting future driving scenarios conditioned on actions (cited as prior work for driving sequence generation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>generative visual world model (GAN-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving / driving video prediction</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper beyond being a generative baseline; GANs are generally black-box with interpretability via image inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not mentioned in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Mentioned as prior approach but no direct comparison in experiments within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Cited as a line of prior work that focuses on action-conditioned future driving generation; no experimental utility analysis here.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>GAN-based sequence generation conditioned on actions (as per prior literature).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Listed among other generative driving models (Drive-Dreamer, MagicDrive); paper argues video-diffusion (Vista/DiffDreamer) provides superior temporal consistency relative to some prior generative approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1212.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1212.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Drive-Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Drive-Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative world-model approach for driving that uses actions as inputs to predict future driving scenarios; cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Drive-Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Drive-Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior work focusing on generating future driving sequences conditioned on actions, combining generative modeling and driving simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>generative visual world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Cited as prior generative world-model work in driving; no direct analysis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1212.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1212.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAIA-I</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GAIA-I</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative driving world model that expands conditioning to include text commands in addition to actions; cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GAIA-I: A Generative World Model for Autonomous Driving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GAIA-I</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A generative world model for driving that incorporates text commands alongside actions to condition future sequence generation (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>generative visual world model (multi-modal conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Mentioned as extending conditioning modalities (text + actions); no experimental evaluation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1212.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1212.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Think-2-Drive</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Think-2-Drive</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach adapting Dreamer-V3 for autonomous driving using BEV (bird's-eye-view) state-space representations, cited as a paradigm of latent world-models applied to driving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Think-2-Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Think-2-Drive</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent-world-model approach that adapts latent imagination methods (Dreamer family) to driving using BEV state representations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Cited as a line of work that unifies latent imagination with driving-specific state representations; no direct comparisons in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability <em>(Rating: 2)</em></li>
                <li>Mastering Diverse Domains through World Models <em>(Rating: 2)</em></li>
                <li>Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models <em>(Rating: 2)</em></li>
                <li>DriveGAN <em>(Rating: 1)</em></li>
                <li>Drive-Dreamer <em>(Rating: 1)</em></li>
                <li>GAIA-I: A Generative World Model for Autonomous Driving <em>(Rating: 1)</em></li>
                <li>Diffusion Policy: Visuomotor Policy Learning via Action Diffusion <em>(Rating: 1)</em></li>
                <li>Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1212",
    "paper_id": "paper-276903042",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "DiffDreamer",
            "name_full": "DiffDreamer (Diffusion-based World Model)",
            "brief_description": "A high-fidelity video-diffusion world model introduced in this paper that predicts multiple future observations jointly (image-space) and a reward head, used to support policy learning in imagination; built on Stable Video Diffusion (SVD) and conditioned on waypoint trajectories encoded with Fourier features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DiffDreamer",
            "model_description": "A video-diffusion based visual world model that predicts H future RGB observations jointly conditioned on P past+current frames and a Fourier-embedded waypoint trajectory; architecture uses a State Encoder (EfficientNet-B0 tokenization), a decoder-only transformer backbone (n_L=4, n_H=4, d_FF=2048) to produce a 32-dim state embedding, SVD for multi-frame generation with the first P noisy frames replaced by actual past frames for grounding, plus an MLP reward-prediction head.",
            "model_type": "video-diffusion visual world model (neural simulator)",
            "task_domain": "autonomous driving (CARLA simulator, front-facing RGB input)",
            "fidelity_metric": "Perceptual / generative fidelity metrics: FID and FVD; temporal-consistency and qualitative frame visualizations; also task metrics (Route Completion, Success Rate) used as downstream utility measures.",
            "fidelity_performance": "FID = 17.09; FVD = 130.39 (reported on CARLA driving sequences, Table III).",
            "interpretability_assessment": "Primarily a neural black-box generator but provides interpretable visual outputs: generated future frames can be inspected and colored-mapped to trajectories (visual qualitative analysis shown); no claim of disentangled latents or explicit human-interpretable state variables beyond 32-dim embedding.",
            "interpretability_method": "Qualitative visualization of predicted video frames and top-view trajectory visualizations tied to predicted frames (Fig. 4, Fig. 5); temporal-consistency metrics; no formal latent-space probing or attention-map analysis reported.",
            "computational_cost": "Trained on 4x NVIDIA A100 GPUs for ~28 hours with gradient accumulation (effective batch size 16); initialized from Vista weights (pretrained on ~1,700 hours driving videos); uses K=50 denoising steps for diffusion policy sampling during training/evaluation (policy side).",
            "efficiency_comparison": "Reported to improve sample efficiency vs. baselines by enabling policy training in imagination — overall framework uses 1e6 online interactions (N_total_online) with a world-model warm-start of 10k iterations; empirically outperforms model-free baselines and other world models while reducing online infractions/km by 50%; explicit FLOPS or parameter-count comparisons not provided.",
            "task_performance": "When used with the paper's DPA policy, Imagine-2-Drive achieves Success Rate (SR) = 83.33% and Route Completion (RC) = 82.13% across CARLA scenarios (Table I); reduces Infraction/Km compared to baselines (0.70 infractions/km).",
            "task_utility_analysis": "High visual-fidelity and temporal consistency (joint multi-frame prediction) translate to improved downstream policy performance: better SR and RC relative to Dreamer-V3 and Iso-Dreamer; authors argue simultaneous multi-frame prediction mitigates compounding one-step errors, which improves policy rollouts in imagination and thus sample efficiency and safety.",
            "tradeoffs_observed": "Switching from autoregressive RSSM-style one-step latent models to SVD multi-frame generation reduces compounding error and increases fidelity but increases model complexity and computational cost (video-diffusion training and sampling). Training required pretraining/warm-start and significant GPU time; diffusion-policy integration (denoising MDP) also adds computation (multiple denoising steps per decision).",
            "design_choices": "Uses SVD as backbone; replaces first P noisy frames with real past frames to ground predictions; Fourier embedding of trajectory; additional MLP reward head and combined losses (diffusion reconstruction + dynamics enhancement + structure preservation + reward MSE); compact state embedding (32-dim) to balance capacity and efficiency.",
            "comparison_to_alternatives": "Compared directly (same DPA policy) to Dreamer-V3 and Iso-Dreamer: DiffDreamer produces substantially better fidelity (FID/FVD) and downstream driving performance (SR/RC). Authors report DiffDreamer FID 17.09 vs Dreamer-V3 FID 89.29 and Iso-Dreamer FID 102.24 and corresponding task improvements (Imagine-2-Drive SR 83.33% vs Dreamer-V3 63.33%, Iso-Dreamer 56.66%).",
            "optimal_configuration": "Paper recommends (for their driving setting) joint multi-frame video-diffusion (SVD) with grounding of initial frames, Fourier trajectory encoding, context length P=5 and horizon H=9, warm-starting from a general SVD (Vista) and alternating training of world model and policy; these choices balance fidelity and task utility while keeping training tractable.",
            "uuid": "e1212.0",
            "source_info": {
                "paper_title": "Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Vista",
            "name_full": "Vista (Stable Video Diffusion-based Driving World Model)",
            "brief_description": "A generalizable, high-fidelity video-diffusion driving world model cited and used as the foundation for initializing DiffDreamer; accepts diverse conditioning (actions, trajectories, text, goals) and emphasizes temporal consistency.",
            "citation_title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
            "mention_or_use": "use",
            "model_name": "Vista",
            "model_description": "A stable latent video-diffusion model designed for driving sequence prediction capable of conditioning on multiple control modalities (actions, trajectories, text commands, goal points); used here to initialize DiffDreamer weights.",
            "model_type": "video-diffusion visual world model / generative neural simulator",
            "task_domain": "autonomous driving / driving video prediction",
            "fidelity_metric": "Perceptual generative metrics (FID, FVD) and temporal-consistency measures (as referenced); Vista is described as high-fidelity in the text.",
            "fidelity_performance": null,
            "interpretability_assessment": "Generative model producing video frames—interpretability mainly via inspecting predicted videos and conditional controls; no explicit interpretability methods described in this paper's use of Vista.",
            "interpretability_method": "None described in this paper beyond visual inspection; Vista cited for temporal attention and improved consistency.",
            "computational_cost": "Pretrained on a large driving dataset (~1,700 hours of driving videos) per paper; exact training hardware unspecified in this paper.",
            "efficiency_comparison": "Used as a warm-start to accelerate DiffDreamer convergence; serves as a high-fidelity foundation but computational cost of pretraining is high (large dataset).",
            "task_performance": null,
            "task_utility_analysis": "Authors leverage Vista weights to bootstrap a task-specific world model (DiffDreamer), indicating Vista provides transferable high-fidelity generative capabilities beneficial for downstream policy learning in driving.",
            "tradeoffs_observed": "High-fidelity pretraining on large video datasets increases transferability but entails large data and compute costs; authors mitigate this by fine-tuning rather than training from scratch.",
            "design_choices": "Video-diffusion backbone with multi-modal conditioning (actions/trajectories/text), temporal attention to improve consistency.",
            "comparison_to_alternatives": "Vista is described as among the most versatile and accurate generative world models; used to outperform autoregressive RSSM baselines when adapted for multi-frame prediction.",
            "optimal_configuration": "Used as a pretrained SVD backbone then fine-tuned with task-specific reward head and grounding strategy (replace initial noisy frames with real frames) for driving.",
            "uuid": "e1212.1",
            "source_info": {
                "paper_title": "Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Dreamer-V3",
            "name_full": "Dreamer-V3 (Mastering Diverse Domains through World Models)",
            "brief_description": "A state-of-the-art latent world-model RL approach (Dreamer series) used as a baseline; relies on recurrent state-space latent dynamics (RSSM) and latent imagination for policy learning.",
            "citation_title": "Mastering Diverse Domains through World Models",
            "mention_or_use": "use",
            "model_name": "Dreamer-V3",
            "model_description": "Latent-space world model using a recurrent state-space model (RSSM) to model dynamics in latent space and perform latent 'imagination' rollouts for policy learning; emphasizes robustness and normalization improvements over prior Dreamer versions.",
            "model_type": "latent world model (RSSM-based)",
            "task_domain": "general RL domains; here used for autonomous driving (CARLA) as a baseline",
            "fidelity_metric": "Indirect fidelity measured by downstream task performance and model prediction quality in latent space; in this paper generative fidelity compared via FID/FVD computed from predicted image sequences.",
            "fidelity_performance": "FID = 89.29; FVD = 324.07 (reported in Table III for Dreamer-V3 on CARLA sequences).",
            "interpretability_assessment": "Latent RSSM black-box; interpretable mainly via latent trajectories or downstream behavior; no explicit interpretability methods reported in this paper.",
            "interpretability_method": "None reported in this paper (baseline usage).",
            "computational_cost": null,
            "efficiency_comparison": "Outperforms Iso-Dreamer in task metrics but is outperformed by DiffDreamer in both fidelity metrics and downstream driving performance according to the paper's experiments (SR and RC).",
            "task_performance": "Success Rate (SR) = 63.33%; Route Completion (RC) = 67.53% (Table I).",
            "task_utility_analysis": "Latent imagination provides sample efficiency over model-free baselines, but one-step RSSM transitions lead to compounding error over long horizons, negatively impacting long-horizon planning in driving compared to multi-frame diffusion models.",
            "tradeoffs_observed": "RSSM latent models are computationally cheaper per step than video diffusion but suffer from accumulating one-step prediction errors across long horizons, reducing temporal fidelity and downstream policy performance.",
            "design_choices": "Recurrent latent dynamics (RSSM), latent-space planning/imagination, robustness/normalization techniques in V3.",
            "comparison_to_alternatives": "Outperforms model-free methods (PPO, DQN) in this domain but underperforms relative to DiffDreamer (video-diffusion multi-frame approach) in both fidelity metrics and driving task metrics.",
            "optimal_configuration": "Paper implies that for long-horizon driving tasks, multi-frame, high-fidelity video generation (as in DiffDreamer/Vista) is preferable over one-step latent RSSMs to avoid compounding errors; no detailed V3 tuning provided here.",
            "uuid": "e1212.2",
            "source_info": {
                "paper_title": "Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Iso-Dreamer",
            "name_full": "Iso-Dreamer (Iso-Dream / Isolating Noncontrollable Dynamics)",
            "brief_description": "A world-model variant that decomposes scenes into controllable and non-controllable components; used here as a baseline to compare world-model designs.",
            "citation_title": "Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models",
            "mention_or_use": "use",
            "model_name": "Iso-Dreamer",
            "model_description": "A decomposition-style latent world model that separates action-controllable components from non-controllable visual dynamics to improve modeling and planning.",
            "model_type": "latent world model (decompositional latent dynamics)",
            "task_domain": "autonomous driving (in paper used as a baseline)",
            "fidelity_metric": "Image-space fidelity metrics (FID, FVD) reported for comparison; downstream task metrics (SR, RC) used to indicate utility.",
            "fidelity_performance": "FID = 102.24; FVD = 421.56 (reported in Table III on CARLA sequences).",
            "interpretability_assessment": "Design aims to increase interpretability by separating controllable vs non-controllable dynamics, but in this paper it is treated as a baseline with no specific interpretability analysis shown.",
            "interpretability_method": "Architectural decomposition (controllable vs non-controllable latents); no additional probing reported here.",
            "computational_cost": null,
            "efficiency_comparison": "Performs worse than Dreamer-V3 and DiffDreamer in both fidelity and driving task metrics in the reported experiments; no detailed compute/time comparisons provided.",
            "task_performance": "Success Rate (SR) = 56.66%; Route Completion (RC) = 60.33% (Table I).",
            "task_utility_analysis": "Iso-Dreamer's decompositional approach does not match the downstream performance of high-fidelity multi-frame diffusion models in long-horizon driving in these experiments; likely limited by one-step latent prediction drift over horizons.",
            "tradeoffs_observed": "Architectural separation for interpretability/control may limit raw visual-fidelity compared to video-diffusion approaches, leading to reduced downstream policy performance in this setting.",
            "design_choices": "Latent decomposition into controllable and non-controllable components to isolate agent-influenced dynamics.",
            "comparison_to_alternatives": "Underperforms Dreamer-V3 and DiffDreamer on both fidelity (FID/FVD) and task metrics in the CARLA experiments reported.",
            "optimal_configuration": "Not specified here; paper's results favor multi-frame high-fidelity video diffusion approaches for driving.",
            "uuid": "e1212.3",
            "source_info": {
                "paper_title": "Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "DriveGAN",
            "name_full": "DriveGAN",
            "brief_description": "A generative model for future driving sequence prediction that conditions on actions; cited in related work as an example of generative world models for driving.",
            "citation_title": "DriveGAN",
            "mention_or_use": "mention",
            "model_name": "DriveGAN",
            "model_description": "A generative adversarial approach for predicting future driving scenarios conditioned on actions (cited as prior work for driving sequence generation).",
            "model_type": "generative visual world model (GAN-based)",
            "task_domain": "autonomous driving / driving video prediction",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": "Not discussed in this paper beyond being a generative baseline; GANs are generally black-box with interpretability via image inspection.",
            "interpretability_method": "Not mentioned in this paper.",
            "computational_cost": null,
            "efficiency_comparison": "Mentioned as prior approach but no direct comparison in experiments within this paper.",
            "task_performance": null,
            "task_utility_analysis": "Cited as a line of prior work that focuses on action-conditioned future driving generation; no experimental utility analysis here.",
            "tradeoffs_observed": "Not discussed in this paper.",
            "design_choices": "GAN-based sequence generation conditioned on actions (as per prior literature).",
            "comparison_to_alternatives": "Listed among other generative driving models (Drive-Dreamer, MagicDrive); paper argues video-diffusion (Vista/DiffDreamer) provides superior temporal consistency relative to some prior generative approaches.",
            "optimal_configuration": null,
            "uuid": "e1212.4",
            "source_info": {
                "paper_title": "Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Drive-Dreamer",
            "name_full": "Drive-Dreamer",
            "brief_description": "A generative world-model approach for driving that uses actions as inputs to predict future driving scenarios; cited in related work.",
            "citation_title": "Drive-Dreamer",
            "mention_or_use": "mention",
            "model_name": "Drive-Dreamer",
            "model_description": "Prior work focusing on generating future driving sequences conditioned on actions, combining generative modeling and driving simulators.",
            "model_type": "generative visual world model",
            "task_domain": "autonomous driving",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": "Not discussed here.",
            "interpretability_method": null,
            "computational_cost": null,
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Cited as prior generative world-model work in driving; no direct analysis in this paper.",
            "tradeoffs_observed": null,
            "design_choices": null,
            "comparison_to_alternatives": null,
            "optimal_configuration": null,
            "uuid": "e1212.5",
            "source_info": {
                "paper_title": "Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GAIA-I",
            "name_full": "GAIA-I",
            "brief_description": "A generative driving world model that expands conditioning to include text commands in addition to actions; cited as related work.",
            "citation_title": "GAIA-I: A Generative World Model for Autonomous Driving",
            "mention_or_use": "mention",
            "model_name": "GAIA-I",
            "model_description": "A generative world model for driving that incorporates text commands alongside actions to condition future sequence generation (cited in related work).",
            "model_type": "generative visual world model (multi-modal conditioning)",
            "task_domain": "autonomous driving",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": "Not discussed in this paper.",
            "interpretability_method": null,
            "computational_cost": null,
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Mentioned as extending conditioning modalities (text + actions); no experimental evaluation in this paper.",
            "tradeoffs_observed": null,
            "design_choices": null,
            "comparison_to_alternatives": null,
            "optimal_configuration": null,
            "uuid": "e1212.6",
            "source_info": {
                "paper_title": "Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Think-2-Drive",
            "name_full": "Think-2-Drive",
            "brief_description": "An approach adapting Dreamer-V3 for autonomous driving using BEV (bird's-eye-view) state-space representations, cited as a paradigm of latent world-models applied to driving.",
            "citation_title": "Think-2-Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving",
            "mention_or_use": "mention",
            "model_name": "Think-2-Drive",
            "model_description": "Latent-world-model approach that adapts latent imagination methods (Dreamer family) to driving using BEV state representations.",
            "model_type": "latent world model",
            "task_domain": "autonomous driving",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": "Not discussed here.",
            "interpretability_method": null,
            "computational_cost": null,
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Cited as a line of work that unifies latent imagination with driving-specific state representations; no direct comparisons in experiments in this paper.",
            "tradeoffs_observed": null,
            "design_choices": null,
            "comparison_to_alternatives": null,
            "optimal_configuration": null,
            "uuid": "e1212.7",
            "source_info": {
                "paper_title": "Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
            "rating": 2,
            "sanitized_title": "vista_a_generalizable_driving_world_model_with_high_fidelity_and_versatile_controllability"
        },
        {
            "paper_title": "Mastering Diverse Domains through World Models",
            "rating": 2,
            "sanitized_title": "mastering_diverse_domains_through_world_models"
        },
        {
            "paper_title": "Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models",
            "rating": 2,
            "sanitized_title": "isodream_isolating_and_leveraging_noncontrollable_visual_dynamics_in_world_models"
        },
        {
            "paper_title": "DriveGAN",
            "rating": 1
        },
        {
            "paper_title": "Drive-Dreamer",
            "rating": 1,
            "sanitized_title": "drivedreamer"
        },
        {
            "paper_title": "GAIA-I: A Generative World Model for Autonomous Driving",
            "rating": 1,
            "sanitized_title": "gaiai_a_generative_world_model_for_autonomous_driving"
        },
        {
            "paper_title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion",
            "rating": 1,
            "sanitized_title": "diffusion_policy_visuomotor_policy_learning_via_action_diffusion"
        },
        {
            "paper_title": "Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning",
            "rating": 1,
            "sanitized_title": "diffusion_world_model_future_modeling_beyond_stepbystep_rollout_for_offline_reinforcement_learning"
        }
    ],
    "cost": 0.01625725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies
9 Mar 2025</p>
<p>Anant Garg garg.anant205@gmail.com 
Robotics Research Center
IIIT Hyderabad
India</p>
<p>K Madhava 
Robotics Research Center
IIIT Hyderabad
India</p>
<p>Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies
9 Mar 2025A5B0B9F7C273BB8CE63A76980B52AE20arXiv:2411.10171v2[cs.RO]
World Model-based Reinforcement Learning (WMRL) enables sample efficient policy learning by reducing the need for online interactions which can potentially be costly and unsafe, especially for autonomous driving.However, existing world models often suffer from low prediction fidelity and compounding one-step errors, leading to policy degradation over long horizons.Additionally, traditional RL policies, often deterministic or single Gaussian-based, fail to capture the multi-modal nature of decision-making in complex driving scenarios.To address these challenges, we propose Imagine-2-Drive, a novel WMRL framework that integrates a high-fidelity world model with a multi-modal diffusion-based policy actor.It consists of two key components: DiffDreamer, a diffusion-based world model that generates future observations simultaneously, mitigating error accumulation, and DPA (Diffusion Policy Actor), a diffusion-based policy that models diverse and multi-modal trajectory distributions.By training DPA within DiffDreamer, our method enables robust policy learning with minimal online interactions.We evaluate our method in CARLA using standard driving benchmarks and demonstrate that it outperforms prior world model baselines, improving Route Completion and Success Rate by 15% and 20% respectively.Project page: https://imagine-2-drive.github.io/</p>
<p>I. INTRODUCTION</p>
<p>World models (WMs) have emerged as a powerful paradigm in reinforcement learning (RL), enabling agents to learn environment dynamics and simulate future states for improved decision-making.By leveraging a learned model of the world, RL agents can achieve greater sample efficiency, plan ahead, and generalize better across tasks.These models also enable autonomous vehicles (AVs) to internally "imagine" possible future scenarios, facilitating more efficient exploration and reducing the risks and costs associated with real-world interactions.The significant advantages of world models also highlights the importance of learning an accurate world model.</p>
<p>Current WMRL [12,14,15,24] approaches, model the environment dynamics in latent space using a Recurrent State Space Model (RSSM) [13] based network.A common limitation of these approaches is their reliance on a singlestep transition model, where errors accumulate over multistep planning, causing planned states to drift from the onpolicy distribution.To alleviate this issue, prior works [35,6,11] leverage video diffusion [3] based approaches to predict the future states simultaneously, with Vista [11] being the most versatile and accurate.However, the utility of these world models is often limited by the nature of traditional RL Fig. 1: Based on past and current observations, Imagine-2-Drive generates a waypoint trajectory using DPA.This trajectory, along with observations, is used to generate future observations and rewards from DiffDreamer corresponding to the trajectory.policies.Most RL policies are constrained to deterministic outputs or single gaussian distributions, which fail to capture the full range of possible behaviors.This undermines the adaptability of the world models and their ability to handle the complexity and variability found in driving environments.</p>
<p>To overcome these limitations, we present Imagine-2-Drive, a novel framework that incorporates two key innovations: DiffDreamer, a high-fidelity world model designed for precise future prediction, and DPA, a diffusion-based policy actor that can model diverse behavioral modes for trajectory planning as shown in Fig. 1.DiffDreamer leverages Stable Video Diffusion (SVD) architecture as a foundation for prediction of future observations with an additional module to predict future rewards which enables it to function as a comprehensive world model, facilitating more sample efficient planning and decision-making.</p>
<p>Similar to [17,28,18,36], DPA utilizes a diffusion based model to predict the trajectory (a sequence of actions) simultaneously.By incorporating the diverse behavior patterns inherent to diffusion policies, our framework can explore and model a broader range of behaviors, thereby improving its overall performance and robustness.DPA is trained using PPO [29], leveraging DiffDreamer to simulate and evaluate trajectories.This enables optimizing the episodic return while significantly reducing the need for online interactions with the environment.</p>
<p>We validate our framework in the autonomous driving domain using the CARLA [7] simulator, demonstrating its superiority over existing world model and model free approaches.A comprehensive evaluation across diverse driving scenarios, based on standard driving metrics, highlights the contributions of each component and their synergistic effect in enhancing overall performance.</p>
<p>To summarize, our key contributions include:  II and Fig. 4, our approach demonstrates superior performance in modeling distinct behavioral patterns within a single policy architecture.3) Our extensive evaluation on CARLA driving tasks demonstrates that our framework outperforms both existing model-based approaches [24,15] and modelfree baselines [23,29] across standard driving metrics, with all methods using single RGB image as the sole input modality.</p>
<p>II. RELATED WORK</p>
<p>A. World Models for Autonomous Driving</p>
<p>For autonomous driving, world models follow two key paradigms: one leverages world models as neural driving simulators, while the other unifies action prediction with future generation.Think-2-Drive [20] adapts Dreamer-V3 [15] for autonomous driving using BEV state-space representations.Generative world models have been extensively explored for future driving sequence prediction based on various input modalities.Models like DriveGAN [19], Drive-Dreamer [35], and MagicDrive [10] focus on generating future driving scenarios using actions as inputs.GAIA-I [16] expands this approach by incorporating text commands alongside actions.Vista [11], the most versatile, accepts inputs including actions, trajectories, text commands, and goal points, demonstrating high generalizability for sequence prediction.</p>
<p>B. Diffusion Policy for Planning</p>
<p>Following the success of Diffuser [17] on D4RL benchmark [9], diffusion-based policies [4,27,1,33,25] have been successfully applied in robotics for motion planning.Most typically, these policies are trained from human demonstrations through a supervised objective, and enjoy both high training stability and strong performance in modeling complex and multi-modal trajectory distributions.In the autonomous driving domain, successful works like [36,18] sample trajectories using diffusion models while [21] formulate Constrained MDP (CMDP) to incorporate constraints.</p>
<p>C. Training Diffusion Models with Reinforcement Learning</p>
<p>Training diffusion models using reinforcement learning (RL) techniques has gained traction in recent research, particularly for applications such as text-to-image generation [8,2,5].DDPO [2] formulate the denoising process as an MDP and apply PPO update to this formalism.DPPO [26] extends these previous approaches by embedding the denoising policy MDP within the environmental MDP, forming a "dual" MDP framework optimized for various model-free tasks.</p>
<p>By harnessing the multi-modal capabilities of diffusion policies and the efficiency gains of world models, we investigate their combined impact and assess the individual contribution of each component to overall performance.</p>
<p>III. METHODOLOGY</p>
<p>Imagine-2-Drive is a world model-based RL framework for long-horizon trajectory generation using only front-facing camera input.By leveraging a learned world model for policy learning, it optimizes for maximizing the episodic returns while minimizing reliance on online environment interactions, enabling efficient decision-making.</p>
<p>As depicted in Fig. 2(a), the framework consists of three key components:</p>
<p>1) State Encoder: Encodes the sequence of P past front-view RGB camera observations with the current observation, to provide temporal information for the current state s. 2) DPA: A diffusion-based policy actor which generates a sequence of actions (waypoint trajectory τ ) conditioned on the current encoded state.3) DiffDreamer: A high-fidelity world model that improves sample efficiency by simulating future states, rewards, and synthetic experiences.It generates future observations based on past frames and the DPApredicted waypoint trajectory, offering a rich predictive model for policy learning.Notation: This paper differentiates between two categories of timesteps: diffusion timesteps, indicated by superscripts k ∈ {K, . . ., 0}, and environment timesteps, denoted by subscripts t ∈ {1, . . ., T }.We use ( ˆ) above elements to denote them being predicted in the future.</p>
<p>We also use the terms world model and DiffDreamer interchangeably, as they refer to the same concept over here.</p>
<p>A. Approximate POMDP with MDP</p>
<p>A single-frame state representation in autonomous driving lacks temporal context, limiting its formulation as an MDP, which assumes the state fully captures relevant information.To address this, we approximate an MDP by incorporating a fixed-length sequence of past observations, enriching temporal dependencies and mitigating partial observability.</p>
<p>Following ViNT [31], we use a pretrained State Encoder which tokenize each observation image {o i } t t−P into an embedding of size d model = 512 with an EfficientNet-B0 [34] model which outputs a feature vector ζ(o i ).The individual tokens are then combined with a positional encoding and t .This trajectory is further enriched using Fourier Embeddings and, along with past and current observations, is input to M ϕ to predict future H observations and rewards.(b) details DiffDreamer, comprising two components: SVD for future observation prediction and an additional head for reward prediction.In SVD, the first P noisy frames are replaced with past observations, while the current observation is repeated (P + H) times and concatenated with past and noisy frames for better grounding with the initial conditions.fed into a transformer backbone F sa .We use a decoder-only transformer with n L = 4 multi-headed attention blocks, each with n H = 4 heads and d F F = 2048 hidden units.The output tokens are concatenated and flattened, then passed through MLP layers to give a final state embedding s t ∈ R 32
s t = M LP (F sa (ζ(o) t−P :t ))(1)</p>
<p>B. DiffDreamer</p>
<p>To overcome the issues of compounding errors and low prediction fidelity, typically found in one-step prediction RSSM-based world models, we leverage stable videodiffusion (SVD) as the foundation for building our world model M ϕ to predict multiple future observations at once.Given the P past and current observations {o i } t t−P , we predict H future observations {ô i } t+H t+1 conditioned on a sequence of actions (trajectory of waypoints) τ ∈ R H×2 from DPA.
τ t = (a t , a t+1 , . . . , a t+H )(2)
where action a t is defined in eq.17</p>
<p>Future State Prediction: As shown in the Architecture Fig. 2(b), to predict the future observations {ô i } t+H t+1 corresponding to τ t while enforcing dynamic consistency with respect to position, velocity and acceleration, we replace the first P noisy frames of the SVD model with the past P and current observations {o i } t t−P .This approach enforces the grounding of model's prediction of the initial frames with the initial conditions and reduces drift.
ôt+1:t+H = SV D ϕ (o t−P :t , c t )(3)
where c t is the fourier embedding of trajectory τ t using eq. 4</p>
<p>Using the state encoder eq. 1 and {ô i } t+H t+1 , we can get the corresponding future states {ŝ i } t+H t+1 .This capability allows the model to accurately predict the future states of the environment corresponding to the actions, which is crucial for sample efficient and safe decision making.</p>
<p>Fourier Trajectory Encoding: To effectively encode the trajectory τ t , we apply a fourier embedding (FFT) [22], which transforms the raw spatial coordinates into a frequency domain representation.This embedding captures both low and high-frequency components, allowing the model to represent smooth and periodic motion patterns more effectively.
c t = F F T (τ t )(4)
Additional Required MDP Components: To employ DiffDreamer as a world model, we integrate an additional M LP module to predict future rewards rt based on the state s t and action a t , thereby completing the Markov Decision Process (MDP) formulation, enabling effective policy optimization in the "imagination" space of the world model.
rt ∼ p ϕ (r t | ŝt , a t ) = M LP (ŝ t , a t )(5)
Loss Function: For future frame predictions, in addition to usual noise reconstruction loss L dif f usion , we incorporate two additional losses L dynamics and L structure from section 3.1 of [11] for enhanced frame prediction.A brief explanation and intuition behind the additional losses follows:</p>
<p>1) Dynamics Enhancement Loss L dynamics focuses on accurately capturing the motion dynamics of objects within the generated videos.This loss encourages the model to learn realistic movement patterns, ensuring that the generated sequences maintain coherence over time.2) Structure Preservation Loss L structure reinforces the structural integrity of the generated scenes.By emphasizing the preservation of spatial relationships and object configurations, this loss helps in producing videos that are not only visually appealing but also contextually accurate.For a detailed overview of the losses used in L SV D , interested readers are encouraged to refer to section 3.1 of [11].</p>
<p>The combined loss function for training the SVD model L SV D becomes:
L SV D = L dif f usion + λ 1 L dynamics + λ 2 L structure (6)
To train the additional reward prediction head, we add an additional reward prediction loss L r to get the final world model loss L M :
L r = 1 2 ||r t − r t || 2(7)L M = L SV D + λ 3 L r(8)
Here λ 1 , λ 2 , λ 3 are hyperparameters to balance the weights of the losses in the final world model loss function
L W M .</p>
<p>C. Diffusion Policy Actor (DPA)</p>
<p>We use a diffusion based policy network π θ to predict a trajectory τ t as shown in eq. 2 conditioned on the current state s t .π θ employs a U-Net architecture similar to that proposed in [17].For sampling, we adopt the strategy from DDIM [32] and utilize K = 50 denoising steps.</p>
<p>However, unlike behavioral cloning methods where diffusion policies can be optimized to fit the conditional noise prediction ε θ (τ k t , s t , k), our RL setting lacks predefined target trajectories to supervise upon.Instead, the policy learns to maximize the cumulative sum of rewards objective J RL (π θ ).
J RL (π θ ) = E π θ t≥0 r(s t , a t )(9)
where r t is calculated from the simulator using eq.18.</p>
<p>As shown in [2] and [8], the denoising process can be formulated as a K-step MDP, to form a denoising MDP.Our approach extends this approach by augmenting the environment MDP with the diffusion MDP.The denoising MDP is completed for each time-step of environment MDP to get the final denoised trajectory τ 0 t .Unlike DPPO [26], which applies a similar formulation in a model-free setting, our approach leverages this structure within model-based reinforcement learning paradigm.</p>
<p>We begin with a randomly initialized diffusion policy network.Sampling from the policy network begins with a noisy trajectory sample drawn from a isotropic Gaussian distribution, τ K t ∼ N (0, σ 2 I).The reverse process is defined by a learned distribution p θ (τ k−1 t |τ k t , s t ), which progressively "denoises" the action sequence to produce a sequence of trajectories {τ K t , τ K−1 t , . . ., τ 1 t , τ 0 t } ending with the final denoised sample τ 0 t .</p>
<p>Denoising as a K-step MDP: The Denoising MDP uses timestep t(t, k) = tK + (K − k) corresponding to (t,k).The states, actions and rewards are defined as:
st (t,k) = (τ k+1 t , s t ) (10a) āt (t,k) = τ k t (10b) rt (t,k) = r(s t , a t ) if k = 0 0 otherwise (10c)
We assign rewards only to the final denoising step, corresponding to actions a 0 t executed after the completion of the denoising MDP.The policy π θ parameterizes as:
πθ (āt (t,k) | st (t,k) ) = π θ (τ k t | τ k+1 t , s t ) = N (τ k t , µ(τ k t , ε θ (τ k+1 t , k + 1, s t ), σ 2 I))(11)
Policy Optimization: DPA policy parameters θ can be optimized by maximizing policy objective J RL eq. 9, using policy gradients method:
∇ θ J RL = E π θ t≥0 ∇ θ logπ θ (a t | s t )R t , R t = t ′ ≥t γ(t ′ )r t ′ is the go-to-rewards(12)
This objective can be applied to optimize πθ for which the gradients of the log-likelihood can be calculated from eq. 11:
∇ θ JRL = E πθ t≥0 ∇ θ logπ θ (āt | st) Rt , Rt = t ′ ≥ t γ(t ′ )r t ′(13)
For better stability and reduced variance, we use PPO [30] to get the final gradient estimator for πθ as follows:
∇ θ JRL = ∇ θ E min A πθ old (st, āt) πθ (āt | st) πθold (āt | st) , A πθ old (st, āt) clip πθ (āt | st) πθold (āt | st) , 1 − ε, 1 + ε (14)
where ε is the clipping ratio which prevents drastic updates to the policy, leading to more stable training compared to vanilla PG.</p>
<p>The advantage estimator A πθ old (st, āt) is used to reduce the variance and stabilize training without introducing any bias.It is calculated as:
A πθ old (st, āt) = t ′ ≥t r(st (t,0) , at (t,0) ) − V θ (st (t,0) ) (15)
where V θ is the state-value estimator, which is trained using T D(0) method as follows:
L V θ = 1 2 ||δ t || 2 , where δ t = r + V θ ′ (s t+1 ) − V θ (s t ) (16)
where δ t is the T D residual error.</p>
<p>The training procedure for the overall pipeline is outlined in Algorithm 1 and shown in Fig. 3.</p>
<p>IV. EXPERIMENTS AND RESULTS</p>
<p>A. Experimental Setup</p>
<p>We evaluate navigation along predefined routes in CARLA Town04 (v0.9.11) at 20 Hz, utilizing routes from the CARLA Leaderboard.Each scenario varies in environmental and lighting conditions, featuring up to 500 meters of road with multiple lanes and a maximum of 20 dynamic traffic vehicles.The ego-agent is randomly initialized along the route, while traffic vehicles are assigned velocities between 1-5 km/h.An episode terminates upon collision or going out-of-bounds.The ego-agent maintains a constant velocity of 7 km/h and follows the output waypoint trajectory via a default PID controller.We train and evaluate our model and baselines on a subset of 10 scenarios.</p>
<p>B. Training Details</p>
<p>Vista [11], trained on approximately 1,700 hours of driving videos, serves as the foundation for initializing our world model, DiffDreamer.To facilitate the efficient convergence of DPA, M ϕ is provided with a warm-start by pretraining it for 10,000 iterations, which are obtained from rollouts in the CARLA simulator using a randomly initialized π θ .As the training of M ϕ relies on data tuples collected by π θ rollouts in the simulator, it is crucial for both the modules to learn in a synchronized manner to ensure stable and efficient co-evolution of their capabilities.We train DiffDreamer on 4 A100 GPUs for 28 hours, accumulating gradients over 4 steps for an effective batch size of 16.</p>
<p>1) Training of DiffDreamer: M ϕ is trained in a supervised manner using the data tuples, where each tuple consists of {o i } t+H t−P , {a j } t+H t , {r j } t+H t sampled from the replay buffer.These tuples are used to compute the world model loss L M using eq.8, which then updates the world model parameters ϕ.Note that the policy π θ is kept fixed during the training of M ϕ .(1) Collect Experience: (2) Train World Model: Update world model M ϕ using loss L M (eq.8)</p>
<p>20:</p>
<p>end for 21:</p>
<p>(3) Train Policy π θ in the World Model M ϕ :</p>
<p>22:</p>
<p>if n ≥ N world ws then 23:</p>
<p>for j = 1 to N policy do 24:</p>
<p>Sample initial state s 0 from B</p>
<p>25:</p>
<p>Roll out trajectory using M ϕ :</p>
<p>26:</p>
<p>for t = 0 to T do 27:</p>
<p>Sample trajectory τ t ∼ π θ (s t )</p>
<p>28:</p>
<p>Predict s t+1 , r t+1 ∼ M ϕ (s t , τ t )</p>
<p>29:</p>
<p>end for 30:</p>
<p>Optimize policy π θ using ∇ θ JRL (eq.14)</p>
<p>31:</p>
<p>end for 32:</p>
<p>end if 33: end for 2) Training of DPA: The policy π θ is trained within the "imagination" space of the frozen world model M ϕ .Given a state s t , π θ generates a trajectory τ t .The past observations {o i } t t−P along with τ t are then input to M ϕ to predict future observations and rewards, denoted as (ô i , ri ) for i = t, . . ., t + H.While predictions extend up to an Hstep horizon, the agent advances one timestep at a time in the world model, with the predicted reward rt contributing to the go-to-return R t .The next state ŝt+1 is then used by π θ to generate τ t+1 .This process iterates until a fixed episode length T , after which π θ is updated using eq.14. C. Implementation Details 1) Action Space: Waypoint trajectory is represented in the X-Y cartesian space.To predict the trajectory effectively, we simplify the action space by predicting the incremental ∆X and ∆Y in the ego-agent's local frame.
a t = (∆X t , ∆Y t )(17)
Simplifying ∆X: To reduce action space dimensionality and ensure forward progress, we fix ∆X = 1, allowing the model to focus on lateral adjustments.</p>
<p>Discretizing ∆Y : lateral movement ∆Y per timestep is defined within (−0.5, 0.5) meters, divided into 11 equal bins representing specific lateral deviations.</p>
<p>Action Encoding: Each discrete action, representing ∆Y , is encoded using a one-hot embedding of 11 size.</p>
<p>2) Horizon and Context Length: We set prediction horizon length H = 9 and context length P = 5.</p>
<p>3) Reward Function: The reward function for lanekeeping and collision avoidance is defined as:
r t = υ T ego ûh • ∆t − ξ 1 • Collision Cost − ξ 2 • ∆Y + c(18)
Here, υ ego represents the ego agent's velocity projected onto the highway direction ûh, normalized and scaled by ∆t = 0.05 to measure highway progression in meters.Collisions with other vehicles or the environment incur a penalty of Collision c ost = 10, while ξ2 • ∆Y discourages large trajectory deviations.The constant c = 1 promotes longer episodes, with ξ 1 and ξ 2 both set to 1.</p>
<p>D. Baselines</p>
<p>We focus on world model-based and model-free methods using front camera images as the sole input modality and compare with the following:</p>
<p>1) Dreamer-V3 [15]: The third generation in the Dreamer series [12,14], incorporating robustness and normalization techniques for stable training.2) Iso-Dreamer [24]: decomposes scenes into action controllable and non-controllable components.3) DQN [23], PPO [29]: Standard model-free algorithms.</p>
<p>E. Metrics</p>
<p>Success Rate (SR %) as the percentage of runs where the ego-agent is able to achieve at least 90% route completion (RC %) with zero instances of Infractions which include the instances of going out-of-lane bounds and collisions with other traffic vehicles.</p>
<p>Episodic Return is defined as the cumulative sum of the reward function (eq.18) over a episode.R 0 = T t=0 r t F. Results</p>
<p>1)</p>
<p>Driving Scores and Episodic Returns: Table I compares RL experts on standard driving metrics across all scenarios.Our model, Imagine-2-Drive , outperforms all baselines with a 20% and 14.6% improvement in SR and RC, respectively, while reducing Infraction/km by 50%.This demonstrates the effectiveness of DPA and DiffDreamer.</p>
<p>World model-based methods consistently surpass modelfree approaches by explicitly learning an environment model, enabling more efficient planning.Among them, Dreamer-V3 outperforms Iso-Dreamer, benefiting from enhanced robustness and normalization techniques over Dreamer-V2 [14].PPO surpasses DQN by optimizing policies with stable updates and improving exploration through stochastic policies.</p>
<p>Fig. 6 further supports these findings, showing that Imagine-2-Drive achieves higher episodic returns over 1M simulation steps across all scenarios.Model SR(%) ↑ Infraction/Km ↓ RC(%) ↑ Iso-Dreamer [24] 56.66 1.65 60.33 Dreamer-V3 [15] 63.33 2) Prediction Fidelity: Table III shows a quantitative comparison of world models prediction fidelity.Imagine-2-Drive outperforms in temporal consistency and fidelity metrics by 72 and 290 in FID and FVD scores respectively.Unlike auto-regressive models, which degrade over time, SVD models enhance temporal consistency by incorporating temporal attention.Fig. 5 qualitatively highlights the high fidelity prediction of our world model conditioned on an input trajectory.V. ABLATION STUDIES A. DPA analysis Table II evaluates DPA within the Iso-Dreamer and Dreamer-V3 frameworks by comparing performance with their SAC actor versus when replaced with DPA (rows 1 vs. 3) and (rows 2 vs. 4).Models using DPA outperform by at least 10% and 8% on SR and RC metrics, highlighting its impact on trajectory prediction.Furthermore, Fig. 4 illustrates DPA's ability to model complex, multi-modal action distributions, demonstrating its adaptability and broader potential in reinforcement learning and decision-making tasks.</p>
<p>B. DiffDreamer Analysis</p>
<p>Table II evaluates prediction fidelity and environmental dynamics across different world models using a uniform DPA policy.This isolates the impact of each model, allowing a focused assessment of its ability to capture environmental dynamics and influence trajectory prediction.Results in (rows 2, 3, and 4) highlight DiffDreamer's superior performance, achieving a 20% and 14.6% improvement in SR and RC, respectively.This underscores the importance of accurate future state predictions and validates the use of SVD as the foundation of our world model framework.</p>
<p>C. Context Length analysis</p>
<p>To evaluate the impact of POMDP approximation using the State Encoder, Fig. 7 compares episodic returns for Imagine-2-Drive across different context lengths (CL), with P = 5 fixed for DiffDreamer.The performance declines at CL = 1 due to insufficient historical information, limiting state inference in partially observable environments.In contrast, longer context lengths (CL = 3, 5) capture richer temporal information, improving state estimation and decision-making, thereby enhancing performance.</p>
<p>VI. CONCLUSION &amp; FUTURE WORK Conclusion:</p>
<p>In this work, we introduced Imagine-2-Drive , a world model-based reinforcement learning framework that combines a high-fidelity world model (DiffDreamer) with a multi-modal diffusion policy actor (DPA) for long-horizon trajectory generation in autonomous driving.DiffDreamer enables accurate future prediction in image state space, improving sample efficiency and reducing reliance on online interactions.Through iterative training, our approach captures diverse behavioral modes while enhancing learning stability.Experimental results highlight its superiority over single-Gaussian policies and other world models, demonstrating its ability to generate diverse trajectories and predict accurate future observations.</p>
<p>Future Work: We aim to extend Imagine-2-Drive to other domains such as robotic manipulation, where accurate long-horizon planning and multimodal prediction are crucial.Expanding the versatility of our approach across domains will further validate its generalizability and adaptability.</p>
<p>Fig. 2 :
2
Fig. 2: Architecture: Imagine-2-Drive consists of a Diffusion Policy Actor (DPA) π θ for trajectory prediction τ and DiffDreamer Mϕ as a World Model for future state and reward prediction.(a) illustrates the overall pipeline: given the encoded state from the current and P past observations, π θ denoises a set of one-hot embeddings over K steps to generate H future discrete actions, forming the final denoised trajectory τ 0t .This trajectory is further enriched using Fourier Embeddings and, along with past and current observations, is input to M ϕ to predict future H observations and rewards.(b) details DiffDreamer, comprising two components: SVD for future observation prediction and an additional head for reward prediction.In SVD, the first P noisy frames are replaced with past observations, while the current observation is repeated (P + H) times and concatenated with past and noisy frames for better grounding with the initial conditions.</p>
<p>Fig. 3 :
3
Fig. 3: Iterative Training of DPA (π θ ) and DiffDreamer (M ϕ ): π θ and M ϕ are trained alternately, with one fixed while the other updates.M ϕ learns from π θ rollouts in the simulator, while π θ is optimized using a frozen M ϕ .This iterative process ensures synchronization and improves training stability.</p>
<p>Algorithm 1
1
Training of Imagine-2-Drive 1: Initialize: Randomly initialized policy π θ and world model M ϕ 2: Initialize experience buffer B ← ∅ 3: Define total online interactions N total online ← 10 6 4: Define rollout limit per env.step N rollout online ← 5000 5: Define world model warm-start steps N world ws ← 10000 6: Define world model update steps N world ← 2 7: Define policy update steps N policy ← 5 8: for n = 1 to N total online do 9:</p>
<p>17 :
17
for i = 1 to N world do 18: Sample batch of {o i } t+H t−P , {a j } t+H t , {r j } t+H t from B 19:</p>
<p>Fig. 4 :Fig. 5 :
45
Fig. 4: Multi-Modal Nature of DPA: The top-view visualization highlights the diverse behaviors generated over the episode by DPA under identical initial conditions but different seeds.Given the current and past observations as shown, DiffDreamer predicts future observations for two distinct trajectories: Left (Blue) and Right (Red), navigating around the Green car ahead.The predicted frames are color coded corresponding to the trajectory.This demonstrates both the multi-modal nature of DPA and the prediction-fidelity of DiffDreamer.(Please zoom-in for a better view)</p>
<p>Fig. 6 :
6
Fig. 6: Episodic return of Imagine-2-Drive (Ours) and other different RL agents averaged over the scenarios.A moving average with a window size of 50 is applied</p>
<p>Fig. 7 :
7
Fig. 7: Episodic return of Imagine-2-Drive (Ours with CL=5) with different CL averaged over the scenarios.A moving average with a window size of 50 is applied.Context Length refers to the number of past frames used for state encoding.</p>
<p>10 :
10
for t = 1 to N rollout online do Sample trajectory τ t ∼ π θ (s t ) Execute a t in simulator, observe (s t , a t , r t ) Store transition (s t , a t , r t ) in buffer B
11:12:13:14:n ← n + 115:end for16:</p>
<p>TABLE I :
I
Driving Metrics Comparison: We compare the Imagine-2-Drive and the baselines on standard driving metrics across all the scenarios.We run each model on all scenarios with 3 random seeds.
ModelSR(%) ↑Infraction/Km ↓RC(%) ↑DQN [23]0.06.2727.63PPO [29]16.664.6150.02Iso-Dreamer [24]56.661.6560.33Dreamer-V3 [15]63.331.5267.53Imagine-2-Drive (ours)83.330.7082.13</p>
<p>TABLE III :
III
Comparison of prediction fidelity scores of different world models over CARLA driving sequences
World ModelFID ↓FVD ↓DriveGAN [19]67.1281.9Iso-Dreamer [24]102.24421.56Dreamer-V3 [15]89.29324.07DiffDreamer17.09130.39
This iterative training procedure is illustrated in Fig.3, where the policy actor π θ interacts with the world model M ϕ in a closed-loop fashion. By leveraging the simulated rollouts generated by M ϕ , π θ refines its policy without requiring direct interaction with the simulator, thereby enhancing sample efficiency.</p>
<p>JUICER: Data-Efficient Imitation Learning for Robotic Assembly. Lars Ankile, 2024</p>
<p>Training Diffusion Models with Reinforcement Learning. Kevin Black, 2024cit. on pp. 2, 4</p>
<p>Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets. Andreas Blattmann, 20231</p>
<p>Diffusion Policy: Visuomotor Policy Learning via Action Diffusion. Chi Cheng, 2024</p>
<p>Directly Fine-Tuning Diffusion Models on Differentiable Rewards. 2024 (cit. Kevin Clark, </p>
<p>Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning. Zihan Ding, 20241</p>
<p>CARLA: An Open Urban Driving Simulator. Alexey Dosovitskiy, 20171</p>
<p>DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models. Ying Fan, 2023cit. on pp. 2, 4</p>
<p>D4RL: Datasets for Deep Data-Driven Reinforcement Learning. Justin Fu, 2021</p>
<p>Street View Generation with Diverse 3D Geometry Control. Ruiyuan Gao, 2024</p>
<p>Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability. Shenyuan Gao, 2024</p>
<p>Dream to Control: Learning Behaviors by Latent Imagination. Danijar Hafner, 2020</p>
<p>Learning Latent Dynamics for Planning from Pixels. Danijar Hafner, 20191</p>
<p>Mastering Atari with Discrete World Models. Danijar Hafner, 20226</p>
<p>Mastering Diverse Domains through World Models. Danijar Hafner, 2024cit. on pp. 1, 2, 6, 7</p>
<p>GAIA-1: A Generative World Model for Autonomous Driving. Anthony Hu, 2023</p>
<p>Planning with Diffusion for Flexible Behavior Synthesis. Michael Janner, 2022cit. on pp. 1, 2, 4)</p>
<p>Controllable Multi-Agent Motion Prediction using Diffusion. Max Chiyu, Jiang, 20231</p>
<p>Seung Wook, Kim , Towards a Controllable High-Quality Neural Simulation. 2021cit. on pp. 2, 7</p>
<p>Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving. Qifeng Li, CARLA-v2). 2024</p>
<p>DDM-Lag : A Diffusion-based Decision-making Model for Autonomous Vehicles with Lagrangian Safety Enhancement. Jiaqi Liu, 2024</p>
<p>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. Ben Mildenhall, 20203</p>
<p>Playing Atari with Deep Reinforcement Learning. Volodymyr Mnih, 2013</p>
<p>Minting Pan, Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models. 2022cit. on pp. 1, 2, 6, 7</p>
<p>Imitating Human Behaviour with Diffusion Models. Tim Pearce, 2023</p>
<p>Diffusion Policy Policy Optimization. Allen Z Ren, 2024cit. on pp. 2, 4</p>
<p>Goal-Conditioned Imitation Learning using Score-based Diffusion Policies. Moritz Reuss, 2023</p>
<p>EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning. Kallol Saha, 20231</p>
<p>Proximal Policy Optimization Algorithms. John Schulman, 2017cit. on pp. 1, 2, 6, 7</p>
<p>Trust Region Policy Optimization. John Schulman, 20174</p>
<p>ViNT: A Foundation Model for Visual Navigation. Dhruv Shah, 2023</p>
<p>Denoising Diffusion Implicit Models. 2022 (cit. Jiaming Song, Chenlin Meng, Stefano Ermon, 4</p>
<p>NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration. Ajay Sridhar, 2023</p>
<p>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. Mingxing Tan, V Quoc, Le, 2020</p>
<p>DriveDreamer: Towards Realworld-driven World Models for Autonomous Driving. Xiaofeng Wang, 20231</p>
<p>Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following. Brian Yang, 20241</p>            </div>
        </div>

    </div>
</body>
</html>
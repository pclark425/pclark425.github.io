<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3862 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3862</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3862</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-7ac14eb265086ccb7bf94d956b20ebc20144cbde</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7ac14eb265086ccb7bf94d956b20ebc20144cbde" target="_blank">ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization</a></p>
                <p><strong>Paper Venue:</strong> Matter</p>
                <p><strong>Paper TL;DR:</strong> ORGANA is an assistive robotic system that automates diverse chemistry experiments using decision-making and perception tools and reduces frustration and physical demand by over 50%, with users saving an average of 80.3% of their time when using it.</p>
                <p><strong>Paper Abstract:</strong> Chemistry experiments can be resource- and labor-intensive, often requiring manual tasks like polishing electrodes in electrochemistry. Traditional lab automation infrastructure faces challenges adapting to new experiments. To address this, we introduce ORGANA, an assistive robotic system that automates diverse chemistry experiments using decision-making and perception tools. It makes decisions with chemists in the loop to control robots and lab devices. ORGANA interacts with chemists using Large Language Models (LLMs) to derive experiment goals, handle disambiguation, and provide experiment logs. ORGANA plans and executes complex tasks with visual feedback, while supporting scheduling and parallel task execution. We demonstrate ORGANA's capabilities in solubility, pH measurement, recrystallization, and electrochemistry experiments. In electrochemistry, it executes a 19-step plan in parallel to characterize quinone derivatives for flow batteries. Our user study shows ORGANA reduces frustration and physical demand by over 50%, with users saving an average of 80.3% of their time when using it.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3862.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3862.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Organa.Reasoner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Organa.Reasoner (LLM-based interaction and reasoning module in Organa)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The LLM-driven reasoning component of the Organa robotic assistant that translates high-level natural-language user intentions into multi-step experiment descriptions, resolves ambiguities through dialog, and incorporates past experiment (thought, action, observation) tuples to plan subsequent experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Organa.Reasoner</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>An LLM-based conversational reasoning module inside Organa that (1) elicits from users an experiment definition, lab setup, an example experiment, and stopping criteria, (2) constructs multi-step experimental procedures using iterative prompting and CLAIRify, and (3) performs reflect-and-replan style reasoning over past (thought, action, observation) tuples using the ReAct prompting scheme to propose subsequent experiments and to trigger human-in-the-loop disambiguation when observations deviate from expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>User-provided information collected at startup (natural-language experiment description, example execution, available hardware/reagents, stopping criteria) and the sequence of past experiment tuples (thought, action, observation) generated during Organa runs; no large external scholarly corpus is used by Organa.Reasoner as described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language instructions or speech provided by the chemist during the startup phase; Organa.Reasoner asks targeted clarifying questions to elicit missing details (experiment goal, lab setup, example experiment, stopping criterion).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Iterative LLM prompting with ReAct (chain-of-thought-style reasoning) to generate (thought, action, observation) tuples; uses CLAIRify for converting natural-language procedures to structured XDL; human-in-the-loop verification when observations deviate from expected results.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Multi-step experiment descriptions in natural language and structured XDL plans (via CLAIRify) suitable as goals for the TAMP planner; also natural-language summaries and troubleshooting prompts for the user.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Validated within the Organa system via execution of downstream robotic plans (solubility, recrystallization, pH, electrochemistry) and a human user study measuring correctness of produced plans, frequency/duration of human interventions, false-positive/false-negative alerts in troubleshooting scenarios, and usability metrics (NASA-TLX, SUS).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Organa.Reasoner enabled multi-experiment autonomous runs (e.g., electrochemistry across multiple buffers) with low human interaction time (startup 4–7 min), successful detection and notification of unexpected outcomes in most injected-error trials (false positive alerts in 3/40 correct experiments; missed detection in 1/8 injected-error experiments), and produced experiment outcomes comparable to human performance in electrochemistry (pKa and slope estimates similar to chemists).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not claim to synthesize or distill theories from large scholarly corpora; relies on the correctness of LLM outputs and user-provided example experiments; LLM plan validation and ensuring generalization remain challenges; current planner lacks online replanning and the PDDL domain can be complex for non-expert users.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared to CLAIRify-style manual step specification and fully manual experiments, Organa.Reasoner reduced user involvement time (startup vs CLAIRify: 4–7 min vs ~17.65 min) and produced comparable electrochemistry results to human experimenters; also reduced frustration and physical demand in NASA-TLX and improved SUS scores on several items.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3862.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3862.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLAIRify</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLAIRify (LLM translator to XDL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative prompting scheme using GPT-3.5 to convert natural-language chemistry procedures into structured XDL scripts suitable for robotic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models for chemistry robotics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>CLAIRify (iterative LLM prompting + verifier-assisted generation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A translator that uses an LLM (GPT-3.5 in prior work) with iterative prompting and syntactic verification to generate valid XDL code from natural-language experiment descriptions; in Organa it is used as the bridge from Organa.Reasoner-produced natural-language procedures to planner goals.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Single natural-language experiment procedure examples provided interactively by the user (not a literature corpus); the system iteratively prompts and corrects the LLM output until syntactically valid XDL is produced.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language procedure descriptions (single-example or templates) provided by users during startup; CLAIRify is prompted to produce XDL corresponding to that procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Iterative prompting with a verifier that checks syntactic validity of generated XDL and re-prompts the LLM until valid structured code is produced (verifier-assisted iterative prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Structured XDL (Chemical Description Language in XML) scripts encoding experiment steps and goals suitable for TAMP input.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Syntactic verification of produced XDL within the iterative prompting loop; executed downstream on robot hardware (prior CLAIRify demonstrations on Franka robot and in Organa experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>CLAIRify reliably produced executable XDL scripts for the experiments performed and formed the basis of Organa's ability to generate planner goals from natural-language descriptions; user workflows requiring manual step-by-step specification (CLAIRify-style) required more human involvement compared to Organa.Reasoner-driven start-up.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Generates plans conditioned on the single example and LLM knowledge; requires verification loops to ensure syntactic correctness; when used alone (without higher-level reasoning) requires more human involvement to specify all steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared as a workflow variant in the user study: CLAIRify-style manual detailed specification required ~17.65 minutes of user involvement vs Organa.Reasoner startup 4–7 minutes; no direct comparison to large-scale literature synthesis methods reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3862.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3862.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-powered chemistry agent that augments large language models with 18 external chemistry tools including calculators and literature search tools to plan and execute molecule synthesis workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChemCrow: Augmenting large-language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ChemCrow (LLM + tool-augmented chemistry agent)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A system that augments a large language model with an external toolset (18 tools) such as calculators, literature-search interfaces, and RoboRXN, enabling the LLM to access domain tools and evidence sources for molecule synthesis planning and execution.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>External tools include literature search tools that can access published literature (papers) and other online sources; specific corpus size and selection not detailed in Organa paper (reference summarized only).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>User-specified molecule synthesis goals or tasks; the LLM calls external tools (including literature search) as needed during plan generation.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-augmented tool use: the LLM is coupled to external tools that retrieve domain-specific evidence (literature search, calculators, synthesis platforms), guiding the LLM's plan generation and grounding its outputs in tool results.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Synthesis plans and actionable step sequences; in reported demonstrations, used to plan syntheses that were executed in the lab (two real-world molecules from literature were successfully synthesized).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Empirical lab validation: the system demonstrated synthesis of two molecules from literature; evaluation included successful execution of planned syntheses (as described in the referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChemCrow integrated LLMs with tools to plan and assist in molecule synthesis and demonstrated end-to-end synthesis of two molecules from literature, indicating practical utility of LLM + tool augmentation for chemistry tasks (as summarized in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The Organa paper provides only a brief summary: detailed limitations (e.g., hallucination, tool-reliability, reproducibility) are not discussed here and would need to be consulted in the ChemCrow paper itself.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Reported success in reproducing literature syntheses (two molecules) demonstrates capability versus manual literature-following; no quantitative comparison to human performance or baselines is provided in the Organa paper's discussion of ChemCrow.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3862.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3862.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Coscientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coscientist (LLM-based autonomous chemistry agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based autonomous agent designed to design, plan, and conduct scientific experiments by browsing the internet, using lab robots, and selecting functions from hardware documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous chemical research with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Coscientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A chemistry agent that leverages large language models to autonomously perform end-to-end experimental workflows, including web browsing for information, planning experimental steps, interacting with liquid-handling robots, and consulting hardware documents to select appropriate functions.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Web-accessible resources including internet pages and possibly scholarly papers encountered during browsing; Organa paper summarizes Coscientist as capable of browsing the internet but does not detail corpus size or selection.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Implicitly via autonomy/objectives specified to the agent (e.g., design and conduct experiments); the agent autonomously searches for relevant information online as part of planning.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>LLM-driven autonomous planning with tool access (web browsing and robot APIs) to gather and synthesize information; likely retrieval-augmented generation via online browsing, as summarized in the Organa paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Autonomous experimental designs, step-by-step plans, and executed experimental actions via robots; reported as an autonomous agent demonstration in the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Empirical demonstrations of the agent autonomously designing and conducting experiments; Organa paper gives a high-level summary but no detailed metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Coscientist was reported to perform autonomous experimental design and execution involving internet browsing and direct robotic actions (as summarized in the Organa paper), illustrating LLMs' potential for integrating literature/web evidence into experimental workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Organa paper does not elaborate limitations for Coscientist; general challenges for such systems include verifying LLM plans, ensuring safe and correct physical execution, and controlling for hallucinations when using web sources.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>No explicit baseline or human-comparison metrics are provided in the Organa paper's brief mention; the referenced work should be consulted for detailed evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3862.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3862.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: reasoning + acting prompting scheme</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting scheme that interleaves reasoning (thoughts) and actions to allow LLMs to generate chains of reasoning together with concrete actions and observations in interaction loops.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ReAct prompting scheme</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A method of prompting LLMs to produce (thought, action, observation) tuples so the model can chain reasoning steps and call actions (e.g., plans, tool calls), allowing reflection on observations and enabling iterative multi-step reasoning across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied to the internal memory of past (thought, action, observation) tuples generated during experiments; not applied to external scholarly corpora in Organa's usage.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Used by prompting the LLM during the start-up and reflection steps to produce next-experiment proposals given prior tuples and user input (natural language).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Chain-of-thought style reasoning implemented as ReAct tuples: the LLM is given past tuples and asked to produce next thought and action, enabling contextualized reasoning across sequential experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Thought (rationale), action (experimental plan), and subsequent observation fields that are consumed by Organa to plan and decide whether to prompt users.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Operational validation through Organa experiments: the ReAct scheme provided context over past experiments enabling Organa to propose subsequent experiments and detect inconsistencies that trigger user pings; user study results measured frequency and outcomes of such interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ReAct-style prompting enabled Organa.Reasoner to maintain continuity across multiple experiments and to leverage past outcomes when proposing new experiments; contributed to successful autonomous multi-experiment runs with human-in-the-loop checks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>ReAct provides reasoning traces but does not guarantee correctness; Organa notes the broader challenge of validating LLM-generated plans and ensuring generalization and safety when LLMs propose multi-stage actions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Used as part of Organa's reasoning stack; comparison to non-ReAct prompting is not quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3862.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3862.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automatic Statistician</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Automatic Statistician</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A project that automates statistical model selection and generates human-readable reports summarizing data analyses; cited as inspiration for Organa's automatic report generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The automatic statistician</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Automatic Statistician (inspirational reference)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>An automated system that synthesizes statistical models and produces explanatory reports from data, used as conceptual inspiration for Organa.Analyzer's automated report generation of experiment results and analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Operates on experimental data rather than scholarly papers; Organa cites it as inspiration for generating summaries combining logs, analysis, and plots.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>N/A for Organa usage — referenced as a conceptual model for automated summary/report generation from data.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Automated model selection and explanation applied to data (not literature); Organa adapts the idea to produce experiment reports combining parameter estimation and diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Human-readable PDF reports summarizing experiments, analysis plots, estimated parameters, and logged errors (Organa implements an Electrochemistry Report inspired by this project).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Organa's report generation is validated by providing example reports to users and using the user study to gauge perceived utility (users unanimously valued post-experiment summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used as inspiration; Organa generates automated reports that users found valuable in trust and usability studies, but the Automatic Statistician itself is not applied to literature synthesis in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The Automatic Statistician addresses data-driven model selection rather than literature distillation; Organa's use is limited to experiment data summaries and not large-scale literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Organa reports improved user trust and usability when providing automatic summaries, mirroring goals of the Automatic Statistician but not directly compared in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ChemCrow: Augmenting large-language models with chemistry tools <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>Large language models for chemistry robotics <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>The automatic statistician <em>(Rating: 1)</em></li>
                <li>LLM+P: Empowering large language models with optimal planning proficiency <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3862",
    "paper_id": "paper-7ac14eb265086ccb7bf94d956b20ebc20144cbde",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "Organa.Reasoner",
            "name_full": "Organa.Reasoner (LLM-based interaction and reasoning module in Organa)",
            "brief_description": "The LLM-driven reasoning component of the Organa robotic assistant that translates high-level natural-language user intentions into multi-step experiment descriptions, resolves ambiguities through dialog, and incorporates past experiment (thought, action, observation) tuples to plan subsequent experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Organa.Reasoner",
            "system_or_method_description": "An LLM-based conversational reasoning module inside Organa that (1) elicits from users an experiment definition, lab setup, an example experiment, and stopping criteria, (2) constructs multi-step experimental procedures using iterative prompting and CLAIRify, and (3) performs reflect-and-replan style reasoning over past (thought, action, observation) tuples using the ReAct prompting scheme to propose subsequent experiments and to trigger human-in-the-loop disambiguation when observations deviate from expectations.",
            "input_corpus_description": "User-provided information collected at startup (natural-language experiment description, example execution, available hardware/reagents, stopping criteria) and the sequence of past experiment tuples (thought, action, observation) generated during Organa runs; no large external scholarly corpus is used by Organa.Reasoner as described in the paper.",
            "topic_or_query_specification": "Natural-language instructions or speech provided by the chemist during the startup phase; Organa.Reasoner asks targeted clarifying questions to elicit missing details (experiment goal, lab setup, example experiment, stopping criterion).",
            "distillation_method": "Iterative LLM prompting with ReAct (chain-of-thought-style reasoning) to generate (thought, action, observation) tuples; uses CLAIRify for converting natural-language procedures to structured XDL; human-in-the-loop verification when observations deviate from expected results.",
            "output_type_and_format": "Multi-step experiment descriptions in natural language and structured XDL plans (via CLAIRify) suitable as goals for the TAMP planner; also natural-language summaries and troubleshooting prompts for the user.",
            "evaluation_or_validation_method": "Validated within the Organa system via execution of downstream robotic plans (solubility, recrystallization, pH, electrochemistry) and a human user study measuring correctness of produced plans, frequency/duration of human interventions, false-positive/false-negative alerts in troubleshooting scenarios, and usability metrics (NASA-TLX, SUS).",
            "results_summary": "Organa.Reasoner enabled multi-experiment autonomous runs (e.g., electrochemistry across multiple buffers) with low human interaction time (startup 4–7 min), successful detection and notification of unexpected outcomes in most injected-error trials (false positive alerts in 3/40 correct experiments; missed detection in 1/8 injected-error experiments), and produced experiment outcomes comparable to human performance in electrochemistry (pKa and slope estimates similar to chemists).",
            "limitations_or_challenges": "Does not claim to synthesize or distill theories from large scholarly corpora; relies on the correctness of LLM outputs and user-provided example experiments; LLM plan validation and ensuring generalization remain challenges; current planner lacks online replanning and the PDDL domain can be complex for non-expert users.",
            "comparison_to_baselines_or_humans": "Compared to CLAIRify-style manual step specification and fully manual experiments, Organa.Reasoner reduced user involvement time (startup vs CLAIRify: 4–7 min vs ~17.65 min) and produced comparable electrochemistry results to human experimenters; also reduced frustration and physical demand in NASA-TLX and improved SUS scores on several items.",
            "uuid": "e3862.0",
            "source_info": {
                "paper_title": "ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "CLAIRify",
            "name_full": "CLAIRify (LLM translator to XDL)",
            "brief_description": "An iterative prompting scheme using GPT-3.5 to convert natural-language chemistry procedures into structured XDL scripts suitable for robotic execution.",
            "citation_title": "Large language models for chemistry robotics",
            "mention_or_use": "use",
            "system_or_method_name": "CLAIRify (iterative LLM prompting + verifier-assisted generation)",
            "system_or_method_description": "A translator that uses an LLM (GPT-3.5 in prior work) with iterative prompting and syntactic verification to generate valid XDL code from natural-language experiment descriptions; in Organa it is used as the bridge from Organa.Reasoner-produced natural-language procedures to planner goals.",
            "input_corpus_description": "Single natural-language experiment procedure examples provided interactively by the user (not a literature corpus); the system iteratively prompts and corrects the LLM output until syntactically valid XDL is produced.",
            "topic_or_query_specification": "Natural-language procedure descriptions (single-example or templates) provided by users during startup; CLAIRify is prompted to produce XDL corresponding to that procedure.",
            "distillation_method": "Iterative prompting with a verifier that checks syntactic validity of generated XDL and re-prompts the LLM until valid structured code is produced (verifier-assisted iterative prompting).",
            "output_type_and_format": "Structured XDL (Chemical Description Language in XML) scripts encoding experiment steps and goals suitable for TAMP input.",
            "evaluation_or_validation_method": "Syntactic verification of produced XDL within the iterative prompting loop; executed downstream on robot hardware (prior CLAIRify demonstrations on Franka robot and in Organa experiments).",
            "results_summary": "CLAIRify reliably produced executable XDL scripts for the experiments performed and formed the basis of Organa's ability to generate planner goals from natural-language descriptions; user workflows requiring manual step-by-step specification (CLAIRify-style) required more human involvement compared to Organa.Reasoner-driven start-up.",
            "limitations_or_challenges": "Generates plans conditioned on the single example and LLM knowledge; requires verification loops to ensure syntactic correctness; when used alone (without higher-level reasoning) requires more human involvement to specify all steps.",
            "comparison_to_baselines_or_humans": "Compared as a workflow variant in the user study: CLAIRify-style manual detailed specification required ~17.65 minutes of user involvement vs Organa.Reasoner startup 4–7 minutes; no direct comparison to large-scale literature synthesis methods reported.",
            "uuid": "e3862.1",
            "source_info": {
                "paper_title": "ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ChemCrow",
            "name_full": "ChemCrow",
            "brief_description": "An LLM-powered chemistry agent that augments large language models with 18 external chemistry tools including calculators and literature search tools to plan and execute molecule synthesis workflows.",
            "citation_title": "ChemCrow: Augmenting large-language models with chemistry tools",
            "mention_or_use": "mention",
            "system_or_method_name": "ChemCrow (LLM + tool-augmented chemistry agent)",
            "system_or_method_description": "A system that augments a large language model with an external toolset (18 tools) such as calculators, literature-search interfaces, and RoboRXN, enabling the LLM to access domain tools and evidence sources for molecule synthesis planning and execution.",
            "input_corpus_description": "External tools include literature search tools that can access published literature (papers) and other online sources; specific corpus size and selection not detailed in Organa paper (reference summarized only).",
            "topic_or_query_specification": "User-specified molecule synthesis goals or tasks; the LLM calls external tools (including literature search) as needed during plan generation.",
            "distillation_method": "Retrieval-augmented tool use: the LLM is coupled to external tools that retrieve domain-specific evidence (literature search, calculators, synthesis platforms), guiding the LLM's plan generation and grounding its outputs in tool results.",
            "output_type_and_format": "Synthesis plans and actionable step sequences; in reported demonstrations, used to plan syntheses that were executed in the lab (two real-world molecules from literature were successfully synthesized).",
            "evaluation_or_validation_method": "Empirical lab validation: the system demonstrated synthesis of two molecules from literature; evaluation included successful execution of planned syntheses (as described in the referenced work).",
            "results_summary": "ChemCrow integrated LLMs with tools to plan and assist in molecule synthesis and demonstrated end-to-end synthesis of two molecules from literature, indicating practical utility of LLM + tool augmentation for chemistry tasks (as summarized in this paper).",
            "limitations_or_challenges": "The Organa paper provides only a brief summary: detailed limitations (e.g., hallucination, tool-reliability, reproducibility) are not discussed here and would need to be consulted in the ChemCrow paper itself.",
            "comparison_to_baselines_or_humans": "Reported success in reproducing literature syntheses (two molecules) demonstrates capability versus manual literature-following; no quantitative comparison to human performance or baselines is provided in the Organa paper's discussion of ChemCrow.",
            "uuid": "e3862.2",
            "source_info": {
                "paper_title": "ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Coscientist",
            "name_full": "Coscientist (LLM-based autonomous chemistry agent)",
            "brief_description": "An LLM-based autonomous agent designed to design, plan, and conduct scientific experiments by browsing the internet, using lab robots, and selecting functions from hardware documentation.",
            "citation_title": "Autonomous chemical research with large language models",
            "mention_or_use": "mention",
            "system_or_method_name": "Coscientist",
            "system_or_method_description": "A chemistry agent that leverages large language models to autonomously perform end-to-end experimental workflows, including web browsing for information, planning experimental steps, interacting with liquid-handling robots, and consulting hardware documents to select appropriate functions.",
            "input_corpus_description": "Web-accessible resources including internet pages and possibly scholarly papers encountered during browsing; Organa paper summarizes Coscientist as capable of browsing the internet but does not detail corpus size or selection.",
            "topic_or_query_specification": "Implicitly via autonomy/objectives specified to the agent (e.g., design and conduct experiments); the agent autonomously searches for relevant information online as part of planning.",
            "distillation_method": "LLM-driven autonomous planning with tool access (web browsing and robot APIs) to gather and synthesize information; likely retrieval-augmented generation via online browsing, as summarized in the Organa paper.",
            "output_type_and_format": "Autonomous experimental designs, step-by-step plans, and executed experimental actions via robots; reported as an autonomous agent demonstration in the referenced work.",
            "evaluation_or_validation_method": "Empirical demonstrations of the agent autonomously designing and conducting experiments; Organa paper gives a high-level summary but no detailed metrics.",
            "results_summary": "Coscientist was reported to perform autonomous experimental design and execution involving internet browsing and direct robotic actions (as summarized in the Organa paper), illustrating LLMs' potential for integrating literature/web evidence into experimental workflows.",
            "limitations_or_challenges": "Organa paper does not elaborate limitations for Coscientist; general challenges for such systems include verifying LLM plans, ensuring safe and correct physical execution, and controlling for hallucinations when using web sources.",
            "comparison_to_baselines_or_humans": "No explicit baseline or human-comparison metrics are provided in the Organa paper's brief mention; the referenced work should be consulted for detailed evaluations.",
            "uuid": "e3862.3",
            "source_info": {
                "paper_title": "ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ReAct prompting",
            "name_full": "ReAct: reasoning + acting prompting scheme",
            "brief_description": "A prompting scheme that interleaves reasoning (thoughts) and actions to allow LLMs to generate chains of reasoning together with concrete actions and observations in interaction loops.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "system_or_method_name": "ReAct prompting scheme",
            "system_or_method_description": "A method of prompting LLMs to produce (thought, action, observation) tuples so the model can chain reasoning steps and call actions (e.g., plans, tool calls), allowing reflection on observations and enabling iterative multi-step reasoning across experiments.",
            "input_corpus_description": "Applied to the internal memory of past (thought, action, observation) tuples generated during experiments; not applied to external scholarly corpora in Organa's usage.",
            "topic_or_query_specification": "Used by prompting the LLM during the start-up and reflection steps to produce next-experiment proposals given prior tuples and user input (natural language).",
            "distillation_method": "Chain-of-thought style reasoning implemented as ReAct tuples: the LLM is given past tuples and asked to produce next thought and action, enabling contextualized reasoning across sequential experiments.",
            "output_type_and_format": "Thought (rationale), action (experimental plan), and subsequent observation fields that are consumed by Organa to plan and decide whether to prompt users.",
            "evaluation_or_validation_method": "Operational validation through Organa experiments: the ReAct scheme provided context over past experiments enabling Organa to propose subsequent experiments and detect inconsistencies that trigger user pings; user study results measured frequency and outcomes of such interactions.",
            "results_summary": "ReAct-style prompting enabled Organa.Reasoner to maintain continuity across multiple experiments and to leverage past outcomes when proposing new experiments; contributed to successful autonomous multi-experiment runs with human-in-the-loop checks.",
            "limitations_or_challenges": "ReAct provides reasoning traces but does not guarantee correctness; Organa notes the broader challenge of validating LLM-generated plans and ensuring generalization and safety when LLMs propose multi-stage actions.",
            "comparison_to_baselines_or_humans": "Used as part of Organa's reasoning stack; comparison to non-ReAct prompting is not quantified in the paper.",
            "uuid": "e3862.4",
            "source_info": {
                "paper_title": "ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Automatic Statistician",
            "name_full": "The Automatic Statistician",
            "brief_description": "A project that automates statistical model selection and generates human-readable reports summarizing data analyses; cited as inspiration for Organa's automatic report generation.",
            "citation_title": "The automatic statistician",
            "mention_or_use": "mention",
            "system_or_method_name": "Automatic Statistician (inspirational reference)",
            "system_or_method_description": "An automated system that synthesizes statistical models and produces explanatory reports from data, used as conceptual inspiration for Organa.Analyzer's automated report generation of experiment results and analysis.",
            "input_corpus_description": "Operates on experimental data rather than scholarly papers; Organa cites it as inspiration for generating summaries combining logs, analysis, and plots.",
            "topic_or_query_specification": "N/A for Organa usage — referenced as a conceptual model for automated summary/report generation from data.",
            "distillation_method": "Automated model selection and explanation applied to data (not literature); Organa adapts the idea to produce experiment reports combining parameter estimation and diagnostics.",
            "output_type_and_format": "Human-readable PDF reports summarizing experiments, analysis plots, estimated parameters, and logged errors (Organa implements an Electrochemistry Report inspired by this project).",
            "evaluation_or_validation_method": "Organa's report generation is validated by providing example reports to users and using the user study to gauge perceived utility (users unanimously valued post-experiment summaries).",
            "results_summary": "Used as inspiration; Organa generates automated reports that users found valuable in trust and usability studies, but the Automatic Statistician itself is not applied to literature synthesis in this work.",
            "limitations_or_challenges": "The Automatic Statistician addresses data-driven model selection rather than literature distillation; Organa's use is limited to experiment data summaries and not large-scale literature synthesis.",
            "comparison_to_baselines_or_humans": "Organa reports improved user trust and usability when providing automatic summaries, mirroring goals of the Automatic Statistician but not directly compared in the paper.",
            "uuid": "e3862.5",
            "source_info": {
                "paper_title": "ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ChemCrow: Augmenting large-language models with chemistry tools",
            "rating": 2
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models for chemistry robotics",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "The automatic statistician",
            "rating": 1
        },
        {
            "paper_title": "LLM+P: Empowering large language models with optimal planning proficiency",
            "rating": 1
        }
    ],
    "cost": 0.01717125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Organa: A Robotic Assistant for Automated Chemistry Experimentation and Characterization</h1>
<p>Kourosh Darvish ${ }^{1,2,3 \dagger *}$, Marta Skreta ${ }^{1,2,3 \dagger}$, Yuchi Zhao ${ }^{1,2,3 \dagger}$, Naruki Yoshikawa ${ }^{1,2}$, Sagnik Som ${ }^{1}$, Miroslav Bogdanovic ${ }^{1}$, Yang Cao ${ }^{3}$, Han Hao ${ }^{3}$, Haoping Xu ${ }^{1,2}$, Alán Aspuru-Guzik ${ }^{1,2,3 \ddagger}$, Animesh Garg ${ }^{1,2,4,5 \dagger}$, Florian Shkurti ${ }^{1,2,3 \ddagger}$</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Robot setup with Organa's overall schema. Organa provides seamless interaction between SDLs and chemists for diverse chemistry experiments. A key strength of Organa is that it perceives surrounding objects and keeps track of progress on a chemistry task in order to make an informed decision about next steps that are in line with user goals. Organa optimizes SDL efficiency through parallel experiment execution, providing timely feedback via reports and analysis, keeping users well-informed and involved in high-level decision-making. More information about Organa can be found at https://ac-rad.github.io/organa/, including code and a video demonstration.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Abstract-Chemistry experiments can be resource- and labor-intensive, often requiring manual tasks like polishing electrodes in electrochemistry. Traditional lab automation infrastructure faces challenges adapting to new experiments. To address this, we introduce Organa, an assistive robotic system that automates diverse chemistry experiments using decision-making and perception tools. It makes decisions with chemists in the loop to control robots and lab devices. ORgana interacts with chemists using Large Language Models (LLMs) to derive experiment goals, handle disambiguation, and provide experiment logs. Organa plans and executes</p>
<p>complex tasks with visual feedback, while supporting scheduling and parallel task execution. We demonstrate Organa 's capabilities in solubility, pH measurement, recrystallization, and electrochemistry experiments. In electrochemistry, it executes a 19-step plan in parallel to characterize quinone derivatives for flow batteries. Our user study shows Organa reduces frustration and physical demand by over $50 \%$, with users saving an average of $80.3 \%$ of their time when using it.</p>
<p>Keywords: Self-driving labs, electrochemistry, assistive robotics, large language models, task and motion planning with scheduling, automatic report generation, computer vision.</p>
<h2>INTRODUCTION</h2>
<p>The process of discovering materials, from generating candidates to conducting experiments, is time- and labor-intensive. It entails synthesizing and characterizing samples at various scales (ranging from milligrams to grams) in order to identify the desired material that meets the specified requirements. While chemistry labs use a wide variety of special-purpose equipment to expedite independent tasks including synthesis, purification, and analysis, chemists still need to bridge the gaps where automation has not been introduced, such as transferring samples between different workstations. Achieving fully automated Self-Driving Labs (SDLs), with data-driven experiment planning and automated experiment execution, to accelerate material discovery in the vast chemical search space remains a formidable challenge, but one that we are making progress towards [1]-[6]. This involves integrating general-purpose robots with lab equipment to efficiently perform chemistry tasks, perceive the environment, plan actions, facilitate high-throughput experiments, interact intuitively with chemists, and maintain safety while adapting flexibly to new chemistry tasks. Furthermore, automatic experimentation in SDLs can enhance result consistency and reproducibility by mitigating the variability inherent in manual experimentation or characterization, which is crucial for scientific discoveries. Such systems could improve the accessibility of chemistry experiments to users, especially in scenarios of dangerous operations and users with disabilities.</p>
<p>In this work, we introduce Organa, an assistive robotic solution, as a step toward achieving flexible automation in chemistry labs. It is a suite of algorithmic tools for robot interaction, perception, and decision making, and is a continuation of our previous work CLAIRify [7]. Organa, as demonstrated in Figure 1, uses LLMs to interact with chemists, identify experiment goals, and plan robot experiments. It also offers feedback to chemists by analyzing experiment outputs. Organa allows for intuitive communication in natural language with chemists, utilizing either written or speech modalities, thereby reducing human effort and keeping chemists informed of high-level decisions throughout the experiment. Organa provides chemists with a summary of experiments, their results, and analysis in the form of a report. This approach provides chemists with com-
prehensive feedback and enables timely user intervention when necessary [8]. Another aspect of Organa is its 3D visual perception capabilities, which enables the manipulation of objects as well as monitoring the progress of chemistry experiments. This allows Organa to make informed decisions on how robots will interact with lab equipment and when to proceed to the next step of a long experiment. The combination of autonomous decision-making and high-level human involvement when necessary contributes to the overall robustness of the system, while reducing the amount of manual involvement in the experiment. Additionally, Organa is oriented towards modularity, in terms of both hardware and functional components, empowering scientists to adopt them for various purposes and a diverse set of experiments and hardware setups. To reduce the overall experiment makespan and enhance efficiency, Organa also supports the parallel execution of chemistry experiments. Readers can refer to Video S1 for a live video demonstration.</p>
<p>As shown in Figure 1, Organa receives commands from chemists in audio or text format, translates them using an LLM-based reasoning architecture into an experiment task description, and then maps these instructions to the robot's goals. Additionally, it grounds perceived objects in the scene through user interaction. Organa improves efficiency by simultaneously solving task and motion planning (TAMP) and scheduling problems, enabling parallel execution of tasks. Moreover, Organa provides feedback to users by offering a comprehensive report and analysis and notifying them in case of unexpected results during the experiment. We show that Organacan be used to execute four widely-used, fundamental chemistry experiments: solubility screening, recrystallization, pH experimentation, and electrochemistry characterization. Solubility screening is an example of using perceptual feedback to make decisions on when to stop the experiment. Organa is utilized to identify the electrochemical characteristics of an quinone solution, a promising molecule class for redox flow batteries. The system is also evaluated by conducting a user study with chemists, validating its significant usefulness. The summary of our contributions is as follows:</p>
<p>1) We introduce a user-friendly assistive robotic system that leans towards a modular design to support fundamental chemistry experiments. Organa reduces the workload of chemists by interacting with them to identify their high-level instructions for experimentation and map them to robot-executable goals. Our system also includes automatic analysis and report generation, as illustrated in Figure 9.
2) We demonstrate the automation of characterizing the electrochemical properties of a quinone derivative (anthraquinone-2-sulfonate, AQS) through fully automated mechanical polishing, as shown in Figure 3. Automating mechanical polishing is of significant practical importance to chemists.</p>
<p>3) We solve task and motion planning and scheduling problems together during the planning phase, certifying the execution of tasks by multiple robots and devices in parallel. This extension to TAMP enhances efficiency in experimentation and utilization of lab resources, enabling the parallel execution of chemistry tasks.
There have been many diverse efforts in lab automation recently; however, many focus on special-purpose hardware and require structured languages [3], [9]. Due to its industrial importance, automation of electrochemistry is actively studied [10]-[12]. One of the bottlenecks of these automation systems is the pretreatment of electrodes [13]. Since electrodes get degraded during consecutive measurements, proper treatment is necessary to obtain accurate and consistent results. To address this, Yoshikawa et al. [14] automated the mechanical polishing of glassy carbon electrodes by combining a custom polishing station and a robot arm, which is a more standard and applicable to more diverse types of electrodes. In this work, we utilize the polishing station proposed by [14].</p>
<p>Efforts have been made to overcome the limitation of specialized hardware through the use of a robotic system [15]. For example, [4] employed general-purpose robots to demonstrate the use of a mobile manipulator to operate instruments designed for human chemists and conduct a specific chemistry experiment. An effort to enhance reconfigurability for chemistry experiments is demonstrated in [16], employing an intuitive set of state machines and workflows. While these initiatives were conducted in structured lab setups with known object poses, recent works, such as [5], introduce a modular architecture, leveraging QR codes for perception and enhancing flexibility. Commonly among them, the perception of chemical reactions and objects in the scene [17]-[19] introduce significant challenges to lab automation, especially given the transparency of chemistry lab tools [20], [21]. To alleviate robustness and flexibility challenges in lab automation, efforts have been made to use robots as assistive systems. For example, [22] used robotic systems to execute experiments over an extended period with the option of human intervention when necessary. In another example, a graphical user interface (GUI) was used in [12] to allow users to encode electrochemistry experiments for robot execution using a fill-in-the-blank template. Despite these promising steps towards SDLs for accelerated material discovery, challenges persist in terms of flexibility, modularity, robustness, and human-centric automation across these examples.</p>
<p>An inspiring example is found in [23], which envisions a system featuring a fictitious character, Organa, engaging in dialogue with a chemist and providing answers to any chemical question. In this work, we bring Organa to the real world. Organa extends our prior work, CLAIRify [7], by reasoning over higher-level chemistry tasks through an initial interaction session. It also pro-
vides users with feedback about experimental results in the form of reports. While CLAIRify only reasons over a single experiment, Organa reasons over the chemist's instructions and plans for multiple experiments to run. For example, it may plan for a series of experiments to characterize material properties over a range of parameters. Additionally, Organa engages the user in troubleshooting if unexpected behavior, outliers, or ambiguities occur during experiment execution. These additional capabilities of Organa, compared to [7], lead to greater efficiency in terms of user engagement and improved robustness in performing experiments. Moreover, Organa goes beyond previous work in terms of skills, such as the perception of transparent objects, which are essential for executing chemistry tasks without the use of AprilTags [24]. Finally, Organa enhances the efficiency of chemistry experiments by enabling parallel task execution, whereas [7] only supports sequential execution. This improved efficiency has been achieved by solving TAMP and scheduling problems together, adapting the PDDLStream algorithm [25].</p>
<p>Automation in chemistry labs poses a challenge for chemists unfamiliar with robotic programming. Recent efforts have shown that this interaction can be simplified through natural language interfaces. Chemistry experiment translators have employed rule-based approaches [3] or fine-tuned sequence-to-sequence models for mapping to robot plans [26]. Mehr et al. introduced a hardware-agnostic Chemical Description Language (XDL) in XML format, facilitating integration into diverse robotics infrastructures [3].</p>
<p>Large Language Models (LLMs) have been shown to be very helpful in planning chemistry experiments due to their ability of exhibiting reasoning-like behaviour [27], in that they are able to produce a logical sequence of experiment action steps given their knowledge of the world state. CLAIRify, as an example, utilizes GPT-3.5 to translate natural language experiments into XDL. This process involves iterative prompting of the LLM to ensure syntactic correctness, and the resulting plans can be executed on a Franka robot in a chemistry lab [7]. ChemCrow, an LLM-powered chemistry engine, integrates 18 external tools for molecule synthesis planning, including calculators, literature search tools, and IBM's RoboRXN platform, and successfully demonstrated the synthesis of two real-world molecules from literature [28]. Boiko et al. designed an LLM-based chemistry agent, Coscientist, capable of autonomously designing, planning, and conducting scientific experiments by performing tasks like browsing the internet, using liquid transferring robots, and selecting relevant functions from hardware documents [29]. Inspired by existing approaches that integrate LLM-based models into experiment planning and robot execution, our focus is on monitoring reactions, detecting unexpected outcomes, and providing feedback to chemists for corrective actions. We also explore LLMs generating automatic</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Instances of Organa conducting various chemistry experiments: (A) solubility, (B) recrystallization, and (C) pH testing. Images showcase the robot executing actions in each setup, as well as the final results.</p>
<p>summary reports for chemists post-experiment, drawing inspiration from the Automatic Statistician project [8]. Our approach generates reports summarizing robot plans, experiment results, data analysis plots, experiment errors, and human-provided information.</p>
<p>Typically, works in the lab automation literature encode robot and lab device plans using manual state machines [3], [4]. As a result, they often face challenges in adapting to uncertainties in the environment. To autonomously solve the problem of robot executable plan generation and parallelizing the robot or agent actions, it is required to solve the problem of TAMP as well as the scheduling problem simultaneously. An approach to solve TAMP problems is with PDDLStream [25], which combines Planning Domain Definition Language (PDDL) solvers for solving discrete planning problems [30] with streams, a declarative sampling procedure. In another work for a geometry-rich sequential robot manipulation problem [31], the authors solve it as an optimization problem. In an extension, [32] solves tool-use problems with mixed-integer programming combined with kinematic and differentiable dynamics constraints. Learning-based approaches have also been proposed [33]–[35] to address the efficiency shortcomings of previous methods.</p>
<p>High-throughput experimentation requires the parallel execution of chemistry tasks, a challenge addressed through scheduling methods discussed in the literature [36], [37]. Scheduling typically deals with a fixed sequence of actions and is commonly resolved using optimization techniques such as mixed-integer linear programming [38]. In this work, we address TAMP and scheduling problems simultaneously. Simultaneous task planning and scheduling, also known as temporal planning, deals with simultaneous durative actions during planning, often utilizing PDDL2.1 [38], [39]. While TAMP and scheduling problems have been explored individually in the literature, their integration has not received sufficient attention. Two examples of such com-</p>
<p>binations are presented in [40], where temporal planning and a sampling-based motion planner solve the problem in two steps, and in [41], where mixed-integer linear programming was employed. Furthermore, while [25] provides a simple example code, it lacks a formal description for addressing simultaneous scheduling and TAMP.</p>
<h2>RESULTS</h2>
<p>We assess Organs for its reliability in reproducing literature results and modularity through a diverse set of multistep chemistry experiments, including solubility screening, recrystallization, pH testing, and electrochemistry. The first three experiments were previously detailed in [7]. We show them done again here with Organs, and focus on results from an advanced electrochemistry experiment. Additionally, we evaluate the interaction between chemists and Organs through a user study.</p>
<h2>Solubility Experiment</h2>
<p>Solubility is the physical property describing the highest concentration of a solute that is capable of dissolving in a solvent under a specific temperature. For this experiment, the robot iteratively pours a small amount of water until all solids are dissolved. After each pouring, the solution is stirred, and the turbidity, a quantitative measurement of residue in solutions, is estimated via a vision-based algorithm that we adapted from [19]. In Figure 2, an instance of Organs is shown conducting a solubility experiment, with the robot observing the dissolved solution for turbidity value estimation. The system assessed the solubility (defined as the mass of solute dissolved in 100 g of solvent) of three solutes in water - salt (sodium chloride), sugar (sucrose), and alum (aluminum potassium sulfate) - with accuracy values of $7.2 \%, 11.2 \%$, and $12.3 \%$, respectively, compared to literature results [42]. The main source of error is attributed to the pouring accuracy of the robot [7], arising from the delayed response, sensitivity, and limited resolution of the scale and dynamixel motor attached to the robot end-effector. For testing each solution, the robot and hardware executed a 7 -step plan, with each test taking an average of 25.63 minutes.</p>
<h2>Recrystallization Experiment</h2>
<p>Recrystallization is a purification process used to extract pure compounds from impure solids. During this process, impure solids are initially added and dissolved in a solution while being heated, and the process is stopped once saturated. As the solution cools, pure compounds start to crystallize due to the solubility of such compounds decreases, while impurities are maintained in the solution. In our experiment, alum was used to test recrystallization because of its dramatic solubility variations in different water temperatures. This experiment modified the solubility test by pre-heating the solvent. The robot and hardware performed a 8 -step plan with the execution
time of 44.80 mins. The result of the robot producing crystals is shown in Figure 2, in the middle.</p>
<h2>pH Experiment</h2>
<p>pH characterizes the acidity or basicity of a solution and is calculated as the negative logarithm of its hydrogen ion activity [43]. The anthocyanin pigment in red cabbage can be used as a pH indicator, and the demonstration of its color change is a popular introductory chemistry experiment [44]. We prepared red cabbage solution by boiling red cabbage leaves in hot water. The initial color of the solution was dark purple/red. The color changes to bright pink when an acid is added, and to blue when a base is added. The robot demonstrated this color change by adding food-grade vinegar (acetic acid, an acid) and baking soda (sodium bicarbonate, a base) into the red cabbage solution. We applied the pouring skills of liquid and powder to transfer reagents and showcase the resulting color changes in Figure 2. To perform this experiment, a 6 -step plan was executed with a duration of 3.85 minutes.</p>
<h2>Electrochemistry Experiment</h2>
<p>Task description: Quinones are an important family of molecules that can be applied to metal-free aqueous flow batteries [45], and their electrochemical properties are actively investigated [46]. The electrochemistry measurements are usually tedious and require human effort for the pretreatment of electrodes which takes several minutes. To demonstrate the applicability of our robotic system in electrochemistry, we measured the redox potential of a quinone solution at different pH levels and visualized the corresponding Pourbaix diagram. Figure 3 shows our experimental setup. We used a portable, low-cost potentiostat [47] and a standard 3-electrode system to conduct the electrochemistry experiments. Notably, we used a glassy-carbon working electrode, which requires mechanical polishing activation, a labor-intensive process in human-centered experiments. Glassy carbon electrode is one of the most common in electrochemistry studies [48]. Although mechanical polishing is the most common method for activating glassy carbon electrodes [13], it has not been incorporated into existing automation systems. With our robotic system, we introduced a polishing station together with the robotic arm holding the electrode to automate this process [14].</p>
<p>We prepared the quinone solutions at different pHs using a flow-based system based on a syringe pump and selection valves, and the redox potential of the solution was measured using a portable potentiostat described in [47]. The solution was a mixture of 2 mM sodium anthraquinone-2-sulfonate (AQS) from Sigma Aldrich, 0.1 M NaCl , and 0.1 M buffer solution. Six buffer solutions were used for different pH values: acetate buffer $\left(\mathrm{CH}<em 3="3">{3} \mathrm{COONa}\right.$ and $\left.\mathrm{CH}</em>} \mathrm{COOH}\right)$ for pH 4 and 5 , phosphate buffer $\left(\mathrm{Na<em 4="4">{2} \mathrm{HPO}</em>}\right.$ and $\left.\mathrm{NaH<em 4="4">{2} \mathrm{PO}</em>}\right)$ for pH 6 , 7 , and 8 , and carbonate buffer $\left(\mathrm{Na<em 3="3">{2} \mathrm{CO}</em>\right)$}\right.$ and $\left.\mathrm{NaHCO}_{3</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: The electrochemistry setup and experiment workflow. Initially, users communicate their intention to Organavia a text or speech interface; objects and their poses are perceived. Subsequently, the user interacts with Organa to establish object functionalities. Eventually, Organa plans the robot actions for parallel execution. On top, it shows the experiment setup. At the bottom, it displays the results of visual perception and snapshots of the robot and other hardware executing the actions in parallel.
for pH 9 . The pH of the buffer solution was adjusted by mixing two solutions manually. The electrode was mechanically polished for 30 s using a robotic polishing station [14] to ensure activation and washed in deionized
water for 30 s to remove residues. In each measurement, the open circuit potential (OCP) was measured at the beginning and taken as the starting potential of cyclic voltammetry measurements. Three cycles of</p>
<p>cyclic voltammetry measurement were conducted in an electrochemical window between -1.5 V and 0.5 V at a scan rate of $100 \mathrm{mV} / \mathrm{s}$. The redox potential was calculated by taking the average of oxidation and reduction peak potentials. The pH of the solution was measured after each electrochemical measurement by automatically transferring the characterized solution to the pH measurement station with the use of a pump.</p>
<p>Experiment setup: Figure 3 illustrates the setup and workflow employed in conducting electrochemistry experiments. The experimental apparatus includes a polishing station, washing station, pH meter, syringe pump, and robot arm. The experimental environment features three distinct beakers: a large one designated for washing, a small vessel for containing the experiment solution, and another small beaker dedicated to pH measurement. The experimental sequence commences with user interaction with Organa to input experiment details. The robot moves toward a view pose, and Organa.Perception detects and estimates the poses of objects within the scene (taking approximately 20 s ). The user is prompted to ground beakers and stations, specifying the functionality of each detected object. This enables the system to understand what each object is used for, which is especially important to consider if there are multiple instances of the same object type. Upon completing these initial steps, Organa.Reasoner generates a high-level plan and goal, which is fed into Organa.Planner to find a parallel executable plan. This process minimizes a cost function associated with total time, thereby maximizing equipment usage. Organa.RobotExecution distributes and executes the plan using multithreading, with each piece of equipment being called asynchronously to perform its respective actions. Figure 3 showcases snapshots of electrochemistry task in progress.</p>
<p>Throughout the experiment, pH values and redox potential values are recorded and provided to the Organa.Analyzer for estimating peak voltage at each pH level. Subsequently, the Organa.Analyzer compares these results with the expected outcomes suggested by Organa. REASONER based on the initial information provided by the user. If inconsistencies arise, Organa notifies the user and provides feedback for troubleshooting; otherwise, the experiment proceeds autonomously without user intervention, with Organa.Reasoner proposing the next buffer solution and the corresponding high-level plan for execution. After testing all buffer solutions, a summary report, along with analyzed results, is automatically generated for user review. An example of such a report is provided in Note S9 and Figures S5 to S11 .</p>
<p>In total, three complete electrochemistry experiments were performed, testing six buffer solutions for each experiment, ranging from pH 4 to pH 9 . Further details regarding the prompts in the human-ORgANA interaction are available in Note S2 and Figure S4 .</p>
<p>Quinone Characterization: Quinones typically un-
dergo three different types of reactions depending on the pH of the solution, and they are shown in Figure 6 as the different slopes in the Pourbaix diagram. Twoproton/two-electron reaction is dominant in the region where $\mathrm{pH}&lt;\mathrm{p} K_{\mathrm{a} 1}$, one-proton/two-electron reaction is dominant between $\mathrm{p} K_{\mathrm{a} 1}$ and $\mathrm{p} K_{\mathrm{a} 2}$, and zero-proton/twoelectron reaction is dominant when $\mathrm{pH}&gt;\mathrm{p} K_{\mathrm{a} 2}$ [46]. This results in different slope values on the Pourbaix diagram: $-59 \mathrm{mV} / \mathrm{pH}$ unit where $\mathrm{pH}&lt;\mathrm{p} K_{\mathrm{a} 1},-30 \mathrm{mV} / \mathrm{pH}$ unit where $\mathrm{p} K_{\mathrm{a} 1}&gt;\mathrm{pH}$ and $\mathrm{pH}&lt;\mathrm{p} K_{\mathrm{a} 2}$, and $0 \mathrm{mV} / \mathrm{pH}$ unit where $\mathrm{pH}&gt;\mathrm{p} K_{\mathrm{a} 2}$. The reported dissociation constants of AQS are $\mathrm{p} K_{\mathrm{a} 1}=7.68$ and $\mathrm{p} K_{\mathrm{a} 2}=10.92$ [49]. Because $\mathrm{p} K_{\mathrm{a} 2}(10.92)$ is in a corrosive region of pHs , we only investigated pHs to solve for $\mathrm{p} K_{\mathrm{a} 1}$. Our experimental results agree with these theoretical predictions. ORGANA obtained the following slope estimates for $\mathrm{pH}&lt;\mathrm{p} K_{\mathrm{a} 1}$ : $-61.3,-61.8$ and $-61.0 \mathrm{mV} / \mathrm{pH}$ unit. The estimated values for the dissociation constants $\mathrm{p} K_{\mathrm{a} 1}$ were: 8.12 , 7.86 and 8.10. As can be seen in Figure 4, representing the results of a single experimental run, even with only 6 measurements we can achieve a low variance estimate for the slope. The variance for $\mathrm{p} K_{\mathrm{a} 1}$ value is comparatively high, caused by a lack of points at pH values greater than 9 because of safety concerns in the robotics lab. However, even with the given set of pH values, we can reduce $\mathrm{p} K_{\mathrm{a} 1}$ estimation variance by utilizing combined data from all three experiments, as can be seen in Figure 6.</p>
<p>Sequential and parallel task execution and efficiency: We have performed the electrochemistry experiments both sequentially and with parallel task plans. Figure 5 demonstrates the Gantt chart of the parallel task plan. The available agents to perform the actions are indicated on the y-axis of the Gantt chart. When solving the concurrent task and motion planning problem with durative actions, to avoid state space explosion, we assume three possible action durations of 1-3 T for simplification, where T is the action unit time. Actions with a duration of up to 60 seconds are simplified as 1T, 2T (up to 120 seconds), and 3T (more than 180 seconds). In total, the plan is a sequence of 19 actions, some performed by a single agent, while others are performed as a joint action with several agents being involved.</p>
<p>On average, the sequential plan takes 21.67 mins to execute, while the parallel plan takes 17.10 mins to execute, reducing the total time significantly by $21.1 \%$. Finally, the averages and standard deviations of the planning time over 12 trials for the above cases are: solving the sequential planning problem ( $61.52 \pm 0.1 \mathrm{~s}$ ); and temporal task and motion planning with the time-variant cost function associated with total time ( $186.3 \pm 46.0 \mathrm{~s}$ ).</p>
<h2>User Study</h2>
<p>To assess the usability of our system in a real-world laboratory setting, we conducted a study with experimental chemists. The details of participants in this study are provided in Note S4 .</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: The electrochemistry results executed by Organa. On the left, a Pourbaix diagram is shown for a single Organa experimental run with estimated distributions for $\mathrm{p} K_{\mathrm{a} 1}$ and $\mathrm{p} K_{\mathrm{a} 2}$. The maximum likelihood estimation (MLE) for $\mathrm{p} K_{\mathrm{a} 1}$ is 7.86 . The top right plot is the cyclic voltammetry curve at $\mathrm{pH}=9$. In the bottom right, the estimated slope distribution for the first region is shown, with an MLE of $-61.8 \mathrm{mV} / \mathrm{pH}$ unit. The distributions for $p K_{\mathrm{a} 1}, p K_{\mathrm{a} 2}$, and the slope are marginal distributions for individual parameters. We report the MLE from the full combined posterior distribution over all model parameters in Note S9, which may be different from the maximum value in the marginal distribution for any given parameter.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Gantt chart of the electrochemistry experiment with sequential and parallel execution. The execution times are written in the legend. For sequential execution as shown in (a), only one action is executed at a given time and it takes 1,346 seconds ( 22.43 min ) to complete a single test of a buffer solution. For parallel execution shown in (b), the total execution decreases to 1,071 seconds ( 17.85 min ) since multiple actions associated with different agents can be executed at the same time. Note: pump cannot transfer solution to pH beaker at $\sim 500$ sec in the parallel execution experiment due to ongoing redox potential measurement. Boxes stacked on top of each other with the same color and pattern mean that the agents are involved in performing the same action jointly.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: Comparison between electrochemistry results conducted by chemists and Organa. Comparison of Pourbaix diagrams and their first region estimated slopes in electrochemistry experiments conducted by Organa (top, with 3 data points per pH value) and chemists (bottom, with 4 data points per pH value). Results are comparable: Organa with $p K_{\mathrm{a} 1}=8.03$ and chemists with $p K_{\mathrm{a} 1}=8.02$. The estimated slope for Organa is $61.3 \mathrm{mV} / \mathrm{pH}$ unit and for chemists is $-62.7 \mathrm{mV} / \mathrm{pH}$ unit. Distributions shown for $p K_{\mathrm{a} 1}, p K_{\mathrm{a} 2}$, and slope are marginal distributions for individual parameters. We report the maximum likelihood estimate from the full combined posterior distribution over all model parameters in Note S9, which may be different from the maximum value in the marginal distribution for any given parameter. The red star and blue cross highlight two distinct problems with task execution.</p>
<p>Test modes: We asked subjects to perform the following tests:</p>
<p>1) Manual experimentation: Chemists manually conducted an electrochemistry experiment to generate a Pourbaix plot. Following the procedure outlined in Note S4, they prepared a quinone solution using pipettes and manually polished an electrode. Following this, they conducted a CV scan and measured the solution's pH with the same potentiostat and pH sensor used by the robot. Each human subject performed measurements at three different pH values, leading to a total of 24 data points, i.e., four measurements per pH value.
2) Organa startup: Users interact with Organa at the startup phase to provide their intention of the chemistry experiment, i.e., information on experiment procedures, goals, expected observations, and vessel semantics. Chemists were asked to follow a script with the target Pourbaix experiment, first by writing, and then by speaking.
3) Organa troubleshooting: Users were asked to interact after obtaining information from the start-up phase. To test user engagement, a scenario was designed where an intentional nonsensical observation triggered Organa. REASONER's
rationalization component to prompt user feedback, ensuring active involvement despite the system's potential autonomous execution capability.
4) CLAIRify: Users were instructed to interact with Organa as if the language planner module did not exist, manually detailing each experiment step. This is the equivalent of using CLAIRify [7].
Evaluation metrics: We assessed both quantitative and qualitative metrics [50], including user interaction time during experiment execution and the variance in Pourbaix plot measurements. Qualitative analysis involved three surveys.
5) NASA Task Load Index (NASA-TLX): It assesses a participant's perceived workload across mental demand, physical demand, temporal demand, effort, performance, and frustration level [51]. Participants self-rated on a scale of 0 (low, good) to 20 (high, bad). NASA-TLX was administered after both manual experimentation and Organa usage, with the questionnaire details in Note S4 .
6) System Usability Survey (SUS): It evaluates subjective usability with ten Likert scale questions [52]. Post-interview, scores range from 0 to 10 , with higher scores indicating better usability. Users completed the SUS after both manual experimentation</p>
<p>and interacting with Organa. The survey questions are detailed in Note S4 .
3) Custom questionnaire: We created a 24 -question custom Likert scale questionnaire to gauge user preferences for various system components and perceptions on lab experiment automation. Questions are framed positively and negatively to alleviate response bias among subjects [53]. We show these questions and response summaries in Figure 8; to visualize preferences more easily, we inverted the scores of negative questions. The original survey can be viewed in Note S4 .
We performed a one-tailed T-score evaluation at a significance level of $p&lt;0.05$ for the SUS and NASATLX studies to gauge differences between the manual experiment and Organa.</p>
<p>User study - quantitative results: Figure 6 compares Pourbaix plots generated by experimental chemists and Organa after the electrochemistry experiment. We compare parameter estimates based on combined data from all human experiments on one hand and all Organa experiments on the other. The values are comparable. For $\mathrm{p} K_{\mathrm{a} 1}$ Organa produces 8.03 and chemists 8.02. For the slope the estimated from Organa is $-61.3 \mathrm{mV} / \mathrm{pH}$ unit, while for the chemists it is $-62.7 \mathrm{mV} / \mathrm{pH}$ unit.</p>
<p>Figure 7 depicts the time history and frequency of chemists' interactions with Organa across different experiment modes. Notably, Organa demonstrates superior performance in terms of human involvement. Manual experimentation averaged over 30 minutes, while OrGana, during the startup phase, required 7.35 minutes for written and 4.27 minutes for spoken instructions. Organa toubleshooting took chemists an average of 1.30 minutes to provide feedback on errors. Additionally, the CLAIRify-style workflow necessitated 17.65 minutes of user involvement on average. In 3 experiments with correct results out of 40 total ( 8 users * 5 experiments without intentional bugs), Organa incorrectly alerted the human (false positive). Among 8 experiments with introduced errors, Organa failed to detect an issue in only one instance (false negative).</p>
<p>In Figure 6, the star symbol denotes an instance where Organa detected and alerted the user to an issue during a chemistry experiment, enabling timely correction. In contrast, the cross in the bottom left figure underscores a scenario where a user omission in a manual user study became apparent only during subsequent data analysis.</p>
<p>User study - qualitative analysis: Figure 8 presents qualitative survey results. In Figure 8(a), NASA-TLX responses for performing manual experiments vs. Organa reveal reduced demand and effort across all categories. Significantly, Organa halved participant frustration and reduced physical demand fourfold. In Figure 8(b), we overlay results from the SUS for the manual experiment and Organa for each of the ten questions. We find that Organa shows significant improvement compared to manual in the following areas: desire to use the
system frequently, reduced complexity of the system, improved consistency, and reduced cumbersomeness. In Figure 8(c), responses to our custom questionnaire indicate unanimous agreement among chemists regarding the potential for automating repetitive lab tasks. Almost all chemists express comfort with robots performing some tasks, with diverse modalities for interaction. Nearly all find Organa easy to interact with (only one was neutral) and capable of accurately capturing intended experiments. Preferences for speaking vs. writing to OrGana vary bimodally. While Organa is appreciated for reducing the time that humans need to be involved in the experiment, chemists prefer to be kept informed during experiment execution. The post-experiment summary is unanimously valued by all chemists.</p>
<h2>DISCUSSION</h2>
<p>Reliability and reproducibility of chemistry results: Organa reliably reproduced results from literature for various experiments. In the solubility experiment, OrGana estimated compound solubility with $10.2 \pm 2.2 \%$ mean and standard deviation values. In the electrochemistry experiment (Figure 6), Organa produced comparable results to chemists, yielding a slope value of $-61.4 \pm 0.5 \mathrm{mV} / \mathrm{pH}$ unit and a $\mathrm{p} K_{\mathrm{a} 1}$ value of $8.03 \pm 0.17$ across three runs of the experiment. These values are similar to values report in literature. While Organa demonstrated proficiency in reproducing literature results in various experiments, challenges persist in tackling complex chemistry tasks requiring advanced perception, manipulation, and planning. Moreover, the use of specialized devices such as liquid and solid dispensers can improve the accuracy of the results whenever feasible.</p>
<p>Modularity: Organa is developed with modularity in mind, taking a step toward independent development and interconnection of modules to obtain and extend the overall functionality. Instances of Organa (Figure 2) are applied in four chemistry experiments and integrate variations of natural language processing (NLP), perception, TAMP, robot execution, and data analysis, along with commonly available lab hardware. Organa can be used with a fundamentally different setup, for example, in experiments with continuous microfluidic platforms [54]. A modular approach enables rapid customization for new applications, aligning with the concept of material on demand for lab automation. Utilizing a general-purpose robot equipped with a multi-modal large perception and manipulation model can potentially enhance flexibility, reducing the need for specialized algorithms in novel applications [55]. In electrochemistry experiments, we leverage LLMs and a transformer-based visual perception architecture for high-level task planning and object perception. In contrast to other lab automation projects where robots operate in structured environments with fixed object poses, Organa relaxes this assumption. It perceives and acts in a semi-structured environment where objects and their poses can vary.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7: Chemist interaction time with Organa in various test modes. As shown in (A), the x-axis represents the duration for three iterations of the buffer solution trials in the electrochemistry experiment. This figure shows the timing and frequency of human-ORGANA interactions, with total times specified in (B). It is clear that ORGANA requires less frequent user intervention compared to CLAIRify and manual experimentation. (B) demonstrates the total human interaction time in each test mode, showing that both vocal and textual ORGANA interaction modes are more efficient compared to manual experimentation and CLAIRify.</p>
<p>Evaluation of autonomy and robustness: Organa successfully completed four different long-horizon chemistry experiments, including:</p>
<ul>
<li>Solubility: 7 steps plan with 25.63 mins execution time performed for 2 times;</li>
<li>Recrystallization: 8 steps plan with 44.80 mins execution time performed for 1 time;</li>
<li>pH measurement: 6 steps plan with 3.85 mins execution time performed for 1 time;</li>
<li>Electrochemistry: 114(6×19) steps plan with 130.00 mins execution time performed 2 times.</li>
</ul>
<p>The human-in-the-loop feature of Organa makes it more robust to failures and reliable through timely interaction with users. In Figure 6, the star indicates a scenario where Organa detected and informed the user about an issue during a chemistry experiment, enabling timely correction.</p>
<p>Efficiency and support for parallel experimentation: Organa increases efficiency and maximizes the usage of resources in chemistry experiments by parallelizing the execution of tasks using the available resources, i.e., pump, robot arm, stirrer, polisher, potentiostat, and pH-sensor. This is achieved by solving the TAMP and scheduling problems together, which can potentially lead to accelerated material discovery, a crucial component that involves supporting high-throughput experimentation and screening. In this work, Organa.Planner minimizes a cost function tied to total time; however, it is also possible to consider additional costs associated with the quality of task execution. We demonstrated that Organa.Planner results in a notable enhancement of 274 seconds (21.1%) in the overall electrochemistry time compared to sequential task execution. Solving TAMP and scheduling problems together incur an overhead compared to sequential TAMP during the planning phase. Employing learning-based techniques or LLMs is a potential approach for reducing planning time [33], [56].</p>
<p>Safety: To address safety in lab automation, aside from relying on a human-in-the-loop for disambiguation, Organa relies on constrained motion planning, consistency checks, and feedback integration.</p>
<p>Constrained motion planning was applied and proved to be critical in solubility and recrystallization experiments in order to prevent spills while moving vials between locations. Organa, equipped with LLMs for reasoning, has the capability to detect unexpected events and address them through user interaction. This feature holds promise for enhancing safety when safety rationales are provided to it by the human. Moreover, Organa's report generation feature can play a crucial role in documenting and keeping scientists informed in case of safety violations. For example, in the report, a section could be dedicated to safety-related notes, according to safety rationales and metrics. Finally, comprehensive safety should extend to both the physical and psychological well-being of humans, taking into account chemical, electrical, and mechanical hazards during synthesis and reactions. This necessitates preemptive and post-event safety measures in perception, planning, and execution, facilitating timely adjustments.</p>
<p>Interactions Between Chemists and Organa: The user study indicates participants found lab automation, specifically Organa, useful. They expressed comfort with various communication modalities such as command line interface (CLI), GUI, or natural language.</p>
<p>The study demonstrated a significant reduction in user physical load and frustration during the chemistry experiment with Organa. Participants consistently rated Organa as significantly useful (SUS questions 1-2) and expressed satisfaction (SUS questions 8-9) with its performance. The findings suggest that enhancing Organa's ease of use and learning could further streamline system usability, which also motivates the use of such systems for scientists with physical disabilities.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8: Human subjective evaluation. (A) NASA Task Load Index (the lower, the better). (B) System Usability Scale (the higher, the better). (C) User study questions. Asterisks in (A) and (B) indicate a significant improvement of Organa over the manual experiment for specified metrics, which was computed using a one-tailed T-test. If the p-value of the T-test was less than 0.05 , we noted the difference as significant.</p>
<p>While users did not perceive a significant increase in efficiency based on workload and SUS studies, Figure 7 indicates the quantitative importance of Organa in reducing human temporal workload. Specifically, for testing three buffer solutions, users saved $88.4 \%$ of time by interacting with Organa through audio compared to manual experimentation. The discrepancy between the human subjective and objective measures might stem from users
performing only half of the full experiment manually (as opposed to the full experiment). Nevertheless, this result highlights the potential for system improvement.</p>
<p>Users agree on the necessity of keeping humans in the loop of autonomy. In fact, half the users expressed uncertainty in trusting a robot to complete the experiments autonomously from start to finish. This indicates the need for careful consideration when integrating robots</p>
<p>into human workflows. Comprehensive report generation was identified as one potential method of increasing trust, as well as pinging humans while the experiment was being executed at moments of uncertainty.</p>
<p>Limitations: Organs currently relies primarily on independent sensor modalities for perception. There is potential in exploring multimodal perception to monitor the progress of chemistry tasks and enhance decisionmaking. An example can be found in [57] to monitor chemistry task progress, where several process parameters (such as temperature and stir rate) and visual cues (such as volume, color, turbidity) were combined to control a chemistry process inside a reactor. Another instance of multimodal perception involves using haptic feedback and vision for object manipulation [58], [59]. Finally, although we addressed the challenge of transparent object detection and pose estimation for our setup in electrochemistry experiments, there is much to be done to develop a robust model-free transparent object perception, considering their textureless and reflective surface [60].</p>
<p>A limitation of Organs. Planner lies in the complexity of defining a PDDL domain for planning robot actions, potentially making it challenging for chemists who are not experts in planning and robotics to modify it. Additionally, it lacks support for online replanning, limiting its adaptability to uncertainties in task execution. We are working on LLMs for task planning and replanning, which holds promise for solving multi-stage long-horizon tasks, but validating the proposed LLM plans and ensuring generalization remains a challenge [56], [61]-[64]. In past work, we have started to address this by learning planning heuristics from experience [33], [65].</p>
<p>To fulfill the promise of flexible lab automation, the robotic agent should be able to automatically prepare the experimental setup, a capability currently lacking in the present work. For example, a mobile robot could retrieve clean beakers or vials from their shelves, insert pump tubes or sensory probes into the vials and beakers, and then proceed to run the experiment.</p>
<h2>EXPERIMENTAL PROCEDURES</h2>
<h2>Resource availability</h2>
<p>Lead contact: Requests for further information and resources should be directed to and will be fulfilled by the lead contact, Kourosh Darvish (kdarvish@cs.toronto.edu).</p>
<p>Materials availability: This study did not generate new materials.</p>
<p>Data and code availability: All the data required to evaluate the presented conclusions is available within the paper and in the Supplementary Materials. The dataset for perception evaluation can be found at https://acrad.github.io/organa/. The code for Organs can also be found at https://github.com/ac-rad/organa.</p>
<p>The architecture, workflow, and main components of Organs are described in Figure 9. The following sections elaborate on the details of each component.</p>
<p>Large Language Model (LLM)-Based Interaction and Reasoning: Generating low-level robot plans for each experiment can be tedious for chemists, especially when several (potentially repetitive) experiments need to be conducted for the synthesis and characterization of materials. To streamline this, Organs.Reasoner facilitates the process through three steps: i) generating a natural language chemistry experiment task description from the high-level instruction of chemists for multiples experiments through an interactive conversation at the start-up phase, ii) translating the experiment's natural description into valid structured language, and iii) resolving ambiguities by interacting with the user during experiments, including grounding perception information and addressing unexpected outcomes.</p>
<p>Autonomous experiment reasoning: While previous works have shown that LLMs are able to generate plans for a single chemistry procedure, autonomously generating plans for a series of experiments is more challenging because the continuity and interdependencies between experiments require understanding complex sequences of actions and their consequences. This involves not only recalling completed experiments but also predicting outcomes and adapting strategies based on results from previous steps. The goal of the start-up phase is to provide Organs with enough information about experiment goals and the world state of the lab so it can autonomously perform a series of experiments instead of needing to provide Organs with step-by-step instructions. During the start-up phase of the experiments, users interact with the system via text or speech. To propose a series of multi-step experiments, Organs.Reasoner acquires information from the user about the following categories: experiment description, lab setup (available hardware, reagents), an example of how to execute one experiment (procedure, rationale, expected output), and stopping criterion. An overview of this process is shown in Figure 9. The example experiment provided by the user is the basis for how Organs. Reasoner constructs the remaining experimental procedures using CLAIRify. Organs. Reasoner proposes subsequent experiment plans based on the experiment goal provided by the user and past experiments that have been completed. This is accomplished by the ReAct prompting scheme [66], which generates (thought, action, observation) tuples after performing an experiment (the first tuple is parsed from the user input). In our case, an action is viewed as an experimental plan, observations are the measured experimental values post-plan execution, and thought represents the rationale behind a given experiment. When the LLM is prompted to generate a new experiment, it has access to past (thought, action, observation) tuples. This enables a chain-of-thought style reasoning, which provides the LLM context over what has been</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 9: Organa's architecture and workflow. (A) Users interact with Organa to convey their intentions for chemistry experiments and disambiguate the functionality of objects by grounding the scene. The LLM-based Organa. ReASONER module translates these instructions into chemistry experiment plans and goals. Subsequently, Organa. PLANNER generates parallel task and motion plans for execution, optimizing hardware utilization. Organa.RobotExecution, equipped with action and perception skills, executes plans in parallel to maximize equipment usage and conduct experiments. The Organa.Analyzer processes raw data to estimate experiment progress and provides feedback to determine the next steps. Human notifications occur only when necessary to address unexpected situations. (B) Organa.ReASONER translates the experiment goal into a multi-step experimental plan by asking the user to define four key pieces of information: experiment definition (goal), lab setup (what hardware and reagents the robot can access), example experiment (how a user would explain the procedure for one experiment), and the stopping criterion. For each experiment, the procedure is processed by CLAIRify to generate TAMP input. Once an experiment is complete, the observations and outcomes are fed back into the planner to help plan for the next experiment (Reflection). At the beginning of the experiment, the user is also asked to assign semantic meaning to the hardware that Organa accesses during the experiment. This is so that Organa knows what each vessel is used for if there are multiple of the same vessel type.</p>
<p>done already and why. This is important when planning multi-step experiments because past experiments need to be taken into account when proposing a new one to not repeat them unnecessarily or take into account any constraints.</p>
<p>After an experiment is complete, the results are analyzed. If the results do not match expected outcomes (as identified by the user in the experiment goal definition), the user is pinged to verify. Organa. REASONER incorporates this feedback, as well as outcomes from all past experiments, to plan subsequent ones.</p>
<p>In the startup phase, Organa also asks the user to semantically ground the hardware it perceives in the scene by identifying what each vessel is intended to be used for. Organa then labels each vessel with its name so that it can generate plans that utilize the correct vessel. This is important if there are multiple of the same vessel type in the experiment. In our electrochemistry experiment, one beaker is used for the reaction, and the other is used for waste. Therefore, it is imperative that Organa does not use them interchangeably. However, if the beakers were interchangeable, Organa could use this information during planning to enable parallelization.</p>
<p>The Organa.Reasoner-human interaction is shown in Figure 9(b). Details about user interaction modalities (text or speech) and implementation specifics can be found in Note S2 and Figure S4 .</p>
<p>Experiment description to valid structured task: We employ CLAIRify [67] to convert the natural language description of a chemistry experiment into structured language codes in the XDL language, which are used as goals for planning by Organa. Planner. CLAIRify utilizes an iterative prompting scheme to guarantee syntactic validity in the output language domain. Although there may be minor variations in the generated XDL scripts at each iteration of the experiment (such as different whitespace characters), the final plan for solving the PDDL problem remains the same.</p>
<p>Human-in-the-loop disambiguation and troubleshooting: In addition to planning, Organa also engages with the user for scene clarification and resolving inconsistencies between expected and current observations. Following [68], Organa addresses scene ambiguities during startup by grounding object functionalities. To handle unexpected experimental outcomes, a human-in-the-loop approach is adopted, where Organa.REASONER reasons over observations and expected observations, prompting the user to investigate and decide on further actions. Examples of ambiguity and uncertainty resolution and their prompts are detailed in Note S3 .</p>
<p>Task and Motion Planning with Scheduling: To speed up experimentation, the TAMP planner should facilitate parallel task execution by robots and other resources or equipment. We adapted PDDLStream [25] with PDDL2.1 [39] to support durative actions and introduced a time-variant cost function to enhance task execution efficiency. PDDLStream is represented by the
tuple $&lt;\mathcal{P}, \mathcal{A}, \mathcal{I}, \mathcal{G}, \mathcal{S}&gt;$, respectively defining predicates, actions, initial state, goal state, and streams. The stream $S(\mathbf{x})$ over a tuple of literals $\mathbf{x}$ acts as a conditional sampler, declaring the satisfaction of the relation between its input and output tuples. To enable durative actions, $\mathbf{a} \in \mathcal{A}$ is substituted with starting and ending actions, denoted as a:start and a:end [39]. Additionally, the starting time of the $i$ 'th action in the plan $\pi$, where $a_{i} \in \pi$, is linked to $t_{\text {a:start }, i}$, and its duration is indicated by $D_{\mathbf{a}, i}$. For efficiency and reduced task execution time, the total cost is defined as:</p>
<p>$$
\text { total_cost }=\sum_{i=1}^{k}\left(t_{\text {a:start }, i}+D_{\mathbf{a}, i}\right) \quad \forall \mathbf{a} \in \pi, k=|\pi|
$$</p>
<p>Figure 10 details the transformation of action a to a:start and a:end, along with updates to their cost functions. Note S8 provides an overview of PDDL syntax and details the modifications to action descriptions in the PDDL language.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Temporal-PDDLStream
Input: \(\mathcal{A}, \mathcal{S}^{\circ}, \mathcal{S}^{\circ}, \mathcal{I}, \mathcal{G}\)
Output: \(\pi\)
    \(\mathcal{U}^{\circ}=\operatorname{ApplyStreams}\left(\mathcal{S}^{\circ}, \mathcal{I}\right.\), next \() \triangleright\) eagerly evaluate costs
    while True do
        \(\mathcal{U}^{*}=\operatorname{ApplyStreams}\left(\mathcal{S}^{\circ},\left\{\mathcal{I}, \mathcal{U}^{\circ}\right\}\right.\), OptOutput \() \quad \triangleright\) optimistic
            stream
            \(\pi^{*}=\operatorname{Search}\left(\mathcal{A},\left\{\mathcal{I}, \mathcal{U}^{\circ}, \mathcal{U}^{*}\right\}, \mathcal{G}\right)\)
            \(\pi, \psi=\operatorname{Evaluate}\left(\left\{\mathcal{I}, \mathcal{U}^{\circ}\right\}, \mathcal{U}^{*}, \pi^{*}, \mathcal{G}\right)\)
        if \(\pi \neq\) None then return \(\pi\)
</code></pre></div>

<p>In [25], various methods to solve the PDDLStream problem are discussed, including an incremental approach where streams are certified eagerly and blindly before the search, leading to inefficiency due to the generation of irrelevant facts during stream evaluation. Another method involves optimistic certification of streams, enabling a lazy exploration of candidate plans. While more efficient, this approach does not support timevarying functions with streams. In Temporal TAMP, time-variable costs are updated through streams. To overcome the limitations of the two existing methods in solving the temporal TAMP problem, we integrate both approaches. Specifically, time-varying streams linked to cost functions and timings are evaluated eagerly, while the remaining streams are evaluated optimistically. Additionally, by imposing reasonable constraints on eagerly evaluated streams, we restrict the search space to enhance search efficiency. Algorithm 1 outlines our method for solving temporal PDDLStream problems, taking input durative actions $\mathcal{A}$, optimistic streams $\mathcal{S}^{\circ}$, cost-related eager streams $\mathcal{S}^{\circ}$, and initial states $\mathcal{I}$ with a goal $\mathcal{G}$. Initially, we assess time- and cost-associated streams, incorporating them into the current set of certified facts $\mathcal{U}^{\circ}$. The remaining streams $\mathcal{S}^{\circ}$ are optimistically evaluated using the OptOutput procedure to create an optimistic object tuple $u^{<em>} \in \mathcal{U}^{</em>}$. In line 4 , we employ a fast downward planning system utilizing weighted A<em> heuristic search [69] to find an optimistic plan $\pi^{</em>}$ with a</p>
<p>focus on minimizing the plan cost. Finally, the optimistic plan and its associated streams are evaluated and certified, returning upon finding a plan. Organa relies on the TAMP planner described here to manage all agent plans. The preconditions for each step of the experiment are defined to ensure that no action is executed until its preconditions are satisfied. The scheduler returns a feasible solution that accounts for all conditions and prevents race conditions.</p>
<p>Perception: To achieve autonomous chemistry experiments, we suggest a dual-level perception framework. The first level monitors chemical task progress by characterizing materials, while the second level focuses on perceiving workspace objects for robot manipulation. Our approach integrates various sensors and utilizes perception algorithms for monitoring reactions and estimating workspace states. Details of perception algorithms are provided below, and hardware specifics can be found in Note S5 .</p>
<p>Turbidity visual feedback: Turbidity, indicating solution opaqueness, gauges undissolved solvent in solubility experiments. Following HeinSight [19], we adopt the solution's average brightness, observed by a robot with an in-hand camera, as a proxy for turbidity. Using the Hough Circle Transform [70], the robot identifies the dish in a top-down view, extracts the minimum square region enclosing it in hue, saturation, and value (HSV) color space, and computes the average brightness as the turbidity value. See Figure 2 for an automated turbidity measurement example.</p>
<p>Transparent and opaque object detection and pose estimation: Perceiving transparent objects poses challenges due to violations of the Lambertian assumption and the textureless nature of transparent surfaces [60]. To address this, we integrated cutting-edge perception algorithms in Organa.Perception for effective transparent object detection and pose estimation in the scene (refer to Figure S2 for a visualization of the pipeline). First, we employed Grounding DINO [71], a transformer-based zero-shot object detection model [72] with grounded pre-training. This model enables flexible object detection by accepting human category names and images as input and providing bounding box information, labels, and confidence for each detection. Next, we applied Non-Maximum Suppression (NMS), a standard post-processing method in object detection [73], to remove duplicate detections and retain the most pertinent bounding boxes. The resulting bounding box was then fed into the Segment Anything model (SAM) [74], utilizing a transformer-based image encoder and mask decoder for segmentation. SAM offers options for points, boxes, and segment prompting through a CLIPbased prompt encoder [75]. Further implementation details can be found in Note S1, Figures S1 to S3, and Table S1 .</p>
<p>Estimating 3D object poses from 2D segments requires depth information for reprojection. We obtained the
necessary depth data using the ZED camera depth map [76]. Opaque object point clouds maintain high accuracy, but transparency introduces distortion, capturing background surfaces. To mitigate this, a practical solution involves estimating the camera distance to the front surface of transparent objects by filtering the least distorted closest $10 \%$ of points. The minimal 3D bounding box was derived from each object's point cloud convex hull using Open3D functions [77]. Principal Component Analysis (PCA) [78] identified the object's major axes, forming the rotation matrix of the object's frame of reference. Finally, object pose in the robot world frame was determined through extrinsic camera calibration.</p>
<p>Skills for Chemistry Experiment: To enable autonomous chemistry experiments, it is crucial to integrate a diverse set of robot skills and laboratory tools. Skills are attained through either specialized hardware (refer to Note S6 ) or diverse algorithms, as detailed below.</p>
<p>Pick $\&amp;$ place and insertion skills: Organa uses object pose information coming from perception to determine the robot end-effector's target frame. These frames are used for grasping, placement, and insertion. To enhance robustness and avoid collisions, a pre/post pose strategy is applied, such as pre-insertion, insertion, and post-insertion poses during object manipulation. The robot joint trajectory is computed using inverse kinematics [79] and the probabilistic roadmap (PRM*) path planning [80].</p>
<p>Constrained motion planning skill: In a chemistry lab, a common task involves transporting containers with liquids and powders. In experiments, especially those related to solubility and recrystallization, we introduced orientation constraints to prevent spillage when the robot transfers beakers. We utilized the PRM<em> sampling-based method for robot motion planning [81]. To incorporate $k$-dimensional path constraints $\mathcal{F}(q): \mathcal{Q} \rightarrow \mathbb{R}^{k}$ in the configuration space $\mathcal{Q}$, we applied a projection-based method to identify configurations satisfying constraints during PRM</em> sampling [80]. When sampling a free configuration in PRM*, its projected value in the constrained configuration space is determined by iteratively minimizing $\mathcal{F}(q)$ using its Jacobian. For details on our implementation and evaluation, see [7].</p>
<p>Liquid and granular material pouring skill: In the chemistry lab, liquid and granular solid pouring is routine, with liquid transfer handled by a pump. Precise powder pouring is challenging and expensive with existing hardware solutions. Inspired by manual pouring skills, Organa implements a robotic pouring technique in solubility and recrystallization experiments. This skill incorporates weight feedback using a proportional derivative (PD) controller and a shaping function, taking the desired substance target value as input and providing the desired rotational velocity of the robot end-effector. Additional details are available in [7].</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 10: Using PDDLStream to solve task and motion planning problems with scheduling. To support parallel task execution in ORGANA, we transform the instantaneous actions action $\in \mathcal{A}$ (on top) to durative actions from PDDL2.1 with starting action:start and ending action:end [39] (at the bottom). For this purpose, the preconditions and effects are updated accordingly to meet the requirements and constraints of concurrent plans. For example, while the agent is acting, it cannot be assigned to any other actions (is_free[agent] is set to false as an effect of the starting action). In addition, non-negative global_time and agent_start_t[agent] literals are added to keep track of the current global execution time and the starting time of each action by the agent. Two stream functions start_cost(action, t) and end_cost(action) are added to provide the cost of action starting and ending. Moreover, update_time() function, in the precondition of action:end, is associated with a stream generator that gets as input the action, its starting time, and the current global execution time and it updates the current global execution time. The constraints on update_time() stream are added in line 3 of the stream generator function on top right, ensuring the current global execution time is higher than the action starting time, and the action is ended with the correct duration. Moreover, it ensures the updated time is less than the maximum time allowed to reach the goal of PDDLStream. Note S8 details these changes using PDDL syntax.</p>
<p>Electrode polishing skill: In our electrochemistry experiment, Organa utilizes a polishing station to refine the glassy carbon electrode. This station comprises a polishing pad connected to two linear actuators that execute planar motion. A mechanical impedance, facilitated by a spring linked to the electrode jig, governs the normal interaction force between the polishing station and the robot end-effector. The electrode is polished with a circular motion realized by sending signals to the linear actuators. Further details on the polishing process are available in [14].This design is inexpensive and enables polishing tasks to be performed by a simpler robotic platform in the future, allowing the more dexterous robot to carry out other tasks.</p>
<p>Automated Data Analysis and Report Generation: To enable Organa to generate comprehensive user reports, we integrated the following tools into ORGANA.ANALYZER.</p>
<p>Electrochemistry Parameter Estimation: In the electrochemistry experiment, we aim to characterize the relationship between the pH and the redox potential, which is the potential that drives the reduction or oxidation half-reaction of a compound measured against a standard reference half-cell [82]. We know that the relationship has three distinct regions of linear dependency, demarcated by pH values $\mathrm{p} K_{\mathrm{a} 1}$ and $\mathrm{p} K_{\mathrm{a} 2}$. We also know that the slope of the second region (from $\mathrm{p} K_{\mathrm{a} 1}$ to $\mathrm{p} K_{\mathrm{a} 2}$ ) is onehalf of the slope of the first one (before $\mathrm{p} K_{\mathrm{a} 1}$ ), while in the last region (after $\mathrm{p} K_{\mathrm{a} 2}$ ) the redox potential does not change (slope is equal to zero). The model is therefore fully defined with 4 parameters: two inflection points $\left(\mathrm{p} K_{\mathrm{a} 1}\right.$ and $\left.\mathrm{p} K_{\mathrm{a} 2}\right)$, a single slope variable (in our case $k$, the slope in region $\left[\mathrm{p} K_{\mathrm{a} 1}, \mathrm{p} K_{\mathrm{a} 2}\right]$ ) and one variable to define the redox potential offset (we take $\mathrm{E}_{\text {inf }}$, the value in the third static region).</p>
<p>To produce parameter values based on collected data we utilize maximum likelihood estimation (MLE). In ad-</p>
<p>dition to that, we aim to produce an updated belief about the parameter values and the model line after each new data point has been sampled. We utilize this posterior over parameters to plot marginal distributions for each individual one (see Figure 4 and Figure 6 for examples). With an automated system, this is an important element in keeping the chemist aware of the current progress of the experiment, in order to catch any issues that the system might not automatically detect and in general to make any corrections necessary. Additionally, while utilizing an optimization algorithm to choose points to sample was not needed in the specific electrochemistry experimental setup, this output would allow for the simple application of out of the box optimization algorithms in the future. We give details of the method for estimating the posterior distribution in Note S7 .</p>
<p>Electrochemistry Report Generation: Organa automatically generates a PDF summary report at the experiment's conclusion, offering a comprehensive overview of the chemistry experiment, including automaticallygenerated statistical analyses of the measurements, to the users. The report includes experiment details, logs of failures with corresponding resolutions, and summary plots analyzing the results. An example electrochemistry report is in Note S9 and Figures S5 to S11 .</p>
<h2>SUPPLEMENTAL INFORMATION INDEX</h2>
<p>Supplemental PDF contains notes on experimental details, corresponding figures, and a table. Video S1 is a live demonstration of Organa interacting with human and executing electrochemistry experiment in parallel.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We thank members of the Matter Lab for participating in the user study. We would like to also thank Jinbang Huang, Lasse Bjørn Kristensen, and Jason HattrickSimpers for their insightful discussions. This research was undertaken thanks in part to funding provided to the University of Toronto's Acceleration Consortium from the Canada First Research Excellence Fund, grant number CFREF-2022-00042. We acknowledge the generous support of Dr. Anders G. Frøseth, the Acceleration Consortium, the Vector Institute, Natural Resources Canada and the Canada 150 Research Chairs program. F.S. and A.G. would also like to acknowledge the Discovery Grant.</p>
<h2>AUTHOR CONTRIBUTIONS</h2>
<p>K.D. led the technical development and validation of the system, as well as TAMP with the scheduling. M.S. created the human-Organa interaction and reasoning pipeline, led the user study, and created the report generation capability. Y.Z. led system integration, developed the perception pipeline, and contributed to TAMP with scheduling. N.Y. developed the chemistry experiment protocol and contributed to system development. S.S. contributed to developing perception and system integration. M.B. contributed to developing parameter estimation. H.H. and Y.C. contributed to the
chemistry experiment protocol. H.X. contributed to developing perception. A.A.G. supervised chemistry experiment protocols and speech interaction. A.G. supervised large language model-based planning and reasoning. F.S. supervised task and motion planning with scheduling, user study, and automated report generation. K.D., M.S., Y.Z., N.Y., S.S., and M.B. wrote the initial draft. All the authors proofread the manuscript.</p>
<h2>DECLARATION OF INTERESTS</h2>
<p>The authors declare no competing interests.</p>
<h2>REFERENCES</h2>
<p>[1] M. Christensen, L. P. Yunker, P. Shiri, T. Zepel, P. L. Prieto, S. Grunert, F. Bork, and J. E. Hein, "Automation isn't automatic," Chemical Science, vol. 12, no. 47, pp. 15 473-15 490, 2021.
[2] L. M. Roch, F. Häse, C. Kreisbeck, T. Tamayo-Mendoza, L. P. Yunker, J. E. Hein, and A. Aspuru-Guzik, "ChemOS: orchestrating autonomous experimentation," Science Robotics, vol. 3, no. 19, 2018.
[3] S. H. M. Mehr, M. Craven, A. I. Leonov, G. Keenan, and L. Cronin, "A universal system for digitization and automatic execution of the chemical synthesis literature," Science, vol. 370, no. 6512, pp. 101-108, 2020.
[4] B. Burger, P. M. Maffettone, V. V. Gusev, C. M. Aitchison, Y. Bai, X. Wang, X. Li, B. M. Alston, B. Li, R. Clowes et al., "A mobile robotic chemist," Nature, vol. 583, no. 7815, pp. $237-241,2020$.
[5] R. Vescovi, T. Ginsburg, K. Hippe, D. Y. Ozgulbas, C. Stone, A. Stroka, R. Butler, B. J. Blaiszik, T. Brettin, K. Chard et al., "Towards a modular architecture for science factories," Digital Discovery, vol. 2, pp. 1980-1998, 2023.
[6] B. P. MacLeod, F. G. Parlane, A. K. Brown, J. E. Hein, and C. P. Berlingnette, "Flexible automation accelerates materials discovery," Nature Materials, vol. 21, no. 7, pp. 722-726, 2022.
[7] N. Yoshikawa, M. Skreta, K. Darvish, S. Arellano-Rubach, Z. Ji, L. Bjørn Kristensen, A. Z. Li, Y. Zhao, H. Xu, A. Kuramshin et al., "Large language models for chemistry robotics," Autonomous Robots, pp. 1-30, 2023.
[8] C. Steinruecken, E. Smith, D. Janz, J. Lloyd, and Z. Ghahramani, "The automatic statistician," Automated machine learning: Methods, systems, challenges, pp. 161-173, 2019.
[9] S. Steiner, J. Wolf, S. Glatzel, A. Andreou, J. M. Granda, G. Keenan, T. Hinkley, G. Aragon-Camarasa, P. J. Kitson, D. Angelone et al., "Organic synthesis in a modular robotic system driven by a chemical programming language," Science, vol. 363, no. 6423, p. eaav2211, 2019.
[10] I. Oh, M. A. Pence, N. G. Lukhanin, O. Rodríguez, C. M. Schroeder, and J. Rodríguez-López, "The electrolab: An opensource, modular platform for automated characterization of redox-active electrolytes," Device, vol. 1, no. 5, p. 100103, 2023.
[11] K. Laws, M. Tze-Kiat Ng, A. Sharma, Y. Jiang, A. J. Hammer, and L. Cronin, "An autonomous electrochemical discovery robot that utilises probabilistic algorithms: Probing the redox behaviour of inorganic materials," ChemElectroChem, vol. 11, no. 1, p. e202300532, 2024.
[12] R. Duke, S. Mahmoudi, A. P. Kaur, V. Bhat, I. Dingle, N. C. Stumme, S. K. Shaw, D. Eaton, A. Vego, and C. Risko, "Expflow: a graphical user interface for automated reproducible electrochemistry," Digital Discovery, vol. 3, pp. 163172, 2024.
[13] G. M. Swain, "Solid electrode materials: pretreatment and activation," in Handbook of electrochemistry. Elsevier, 2007, pp. 111-153.
[14] N. Yoshikawa, G. D. Akkoc, S. Pablo-García, Y. Cao, H. Hao, and A. Aspuru-Guzik, "Does one need to polish electrodes in an eight pattern? automation provides the answer." ChemRxiv, 2024.</p>
<p>[15] D. Knobbe, H. Zwirnmann, M. Eckhoff, and S. Haddadin, "Core processes in intelligent robotic lab assistants: Flexible liquid handling," in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2022, pp. 2335-2342.
[16] H. Fakhraldeen, G. Pizzuto, J. Glowacki, and A. I. Cooper, "ARChemist: Autonomous robotic chemistry system architecture," in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 6013-6019.
[17] S. Eppel, H. Xu, M. Bismuth, and A. Aspuru-Guzik, "Computer vision for recognition of materials and vessels in chemistry lab settings and the vector-labpics data set," ACS Central Science, vol. 6, no. 10, pp. 1743-1752, 2020. [Online]. Available: https://doi.org/10.1021/acscentsci.0c00460
[18] R. El-khawaldeh, M. A. Guy, F. Bork, N. Taherimakhsousi, K. N. Jones, J. Hawkins, L. Han, R. P. Pritchard, B. Cole, S. Monfette, and J. E. Hein, "Keeping an "eye" on the experiment: computer vision for real-time monitoring and control," Chem. Sci., pp. -, 2023. [Online]. Available: http://dx.doi.org/10.1039/D3SC05491H
[19] T. Zepel, V. Lai, L. P. E. Yunker, and J. E. Hein, "Automated liquid-level monitoring and control using computer vision," ChemRxiv, 2020.
[20] H. Xu, Y. R. Wang, S. Eppel, A. Aspuru-Guzik, F. Shkurti, and A. Garg, "Seeing glass: Joint point cloud and depth completion for transparent objects," arXiv preprint arXiv:2110.00087, 2021.
[21] Y. R. Wang, Y. Zhao, H. Xu, S. Eppel, A. Aspuru-Guzik, F. Shkurti, and A. Garg, "Mvtrans: Multi-view perception of transparent objects," in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 37713778 .
[22] N. J. Szymanski, B. Rendy, Y. Fei, R. E. Kumar, T. He, D. Milsted, M. J. McDermott, M. Gallant, E. D. Cubuk, A. Merchant et al., "An autonomous laboratory for the accelerated synthesis of novel materials," Nature, pp. 1-6, 2023.
[23] A. Aspuru-Guzik, R. Lindh, and M. Reiher, "The matter simulation (r) evolution," ACS central science, vol. 4, no. 2, pp. 144-152, 2018.
[24] E. Olson, "Apriltag: A robust and flexible visual fiducial system," in 2011 IEEE international conference on robotics and automation. IEEE, 2011, pp. 3400-3407.
[25] C. R. Garrett, T. Lozano-Pérez, and L. P. Kaelbling, "PDDLStream: Integrating symbolic planners and blackbox samplers via optimistic adaptive planning," in Proceedings of the 30th Int. Conf. on Automated Planning and Scheduling (ICAPS). AAAI Press, 2020, pp. 440-448.
[26] A. C. Vaucher, P. Schwaller, J. Geluykens, V. H. Nair, A. Iuliano, and T. Laino, "Inferring experimental procedures from text-based representations of chemical reactions," Nature communications, vol. 12, no. 1, p. 2573, 2021.
[27] Z. Ren, Z. Zhang, Y. Tian, and J. Li, "CRESt - copilot for real-world experimental scientist," ChemRxiv, 2023.
[28] A. M. Bran, S. Cox, A. D. White, and P. Schwaller, "ChemCrow: Augmenting large-language models with chemistry tools," arXiv preprint arXiv:2304.05376, 2023.
[29] D. A. Boiko, R. MacKnight, B. Kline, and G. Gomes, "Autonomous chemical research with large language models," Nature, 2023.
[30] D. McDermott, M. Ghallab, A. Howe, C. Knoblock, A. Ram, M. Veloso, D. Weld, and D. Wilkins, "PDDLthe planning domain definition language," Technical Report CVC TR98003/DCS TR1165. New Haven, CT: Yale Center for Computational Vision and Control, Tech. Rep. 123, 1998. [Online]. Available: http://www.example.com/advancements .report
[31] M. Toussaint, "Logic-geometric programming: An optimization-based approach to combined task and motion planning." in IJCAI, 2015, pp. 1930-1936.
[32] M. A. Toussaint, K. R. Allen, K. A. Smith, and J. B. Tenenbaum, "Differentiable physics and stable modes for tool-use and manipulation planning," Robotics: Science and Systems Foundation, 2018.
[33] M. Khodeir, B. Agro, and F. Shkurti, "Learning to search in
task and motion planning with streams," IEEE Robotics and Automation Letters, 2023.
[34] B. Kim, L. Shimanuki, L. P. Kaelbling, and T. Lozano-Pérez, "Representation, learning, and planning algorithms for geometric task and motion planning," The International Journal of Robotics Research, vol. 41, no. 2, pp. 210-231, 2022.
[35] N. Kumar, W. McClinton, R. Chitnis, T. Silver, T. LozanoPérez, and L. P. Kaelbling, "Learning efficient abstract planning models that choose what to predict," in 7th Annual Conference on Robot Learning, 2023.
[36] F. Häse, L. M. Roch, and A. Aspuru-Guzik, "Next-generation experimentation with self-driving laboratories," Trends in Chemistry, vol. 1, no. 3, pp. 282-291, 2019.
[37] C. D. Hubbs, C. Li, N. V. Sahinidis, I. E. Grossmann, and J. M. Wassick, "A deep reinforcement learning approach for chemical production scheduling," Computers $\mathcal{E}$ Chemical Engineering, vol. 141, p. 106982, 2020.
[38] D. Long, J. Dolejsi, and M. Stolba, "Scheduling problems in PDDL," in Workshop on Knowledge Engineering for Planning and Scheduling, 2023.
[39] M. Fox and D. Long, "Pddl2. 1: An extension to pddl for expressing temporal planning domains," Journal of artificial intelligence research, vol. 20, pp. 61-124, 2003.
[40] S. Edelkamp, M. Lahijanian, D. Magazzeni, and E. Plaku, "Integrating temporal reasoning and sampling-based motion planning for multigoal problems with dynamics and time windows," IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 3473-3480, 2018.
[41] J. Chen, B. C. Williams, and C. Fan, "Optimal mixed discrete-continuous planning for linear hybrid systems," in Proceedings of the 24th International Conference on Hybrid Systems: Computation and Control, ser. HSCC '21. New York, NY, USA: Association for Computing Machinery, 2021. [Online]. Available: https://doi.org/10.1145/3447928.3456654
[42] N. A. O. o. J. NAOJ, Handbook of Scientific Tables. WORLD SCIENTIFIC, 2022.
[43] "ph," International Union of Pure and Applied Chemistry (IUPAC), 2019. [Online]. Available: https://doi.org/10.1351/ goldbook.P04524
[44] J. J. Fortman and K. M. Stubbs, "Demonstrations with red cabbage indicator," Journal of chemical education, vol. 69, no. 1, p. 66, 1992.
[45] B. Huskinson, M. P. Marshak, C. Suh, S. Er, M. R. Gerhardt, C. J. Galvin, X. Chen, A. Aspuru-Guzik, R. G. Gordon, and M. J. Aziz, "A metal-free organic-inorganic aqueous flow battery," Nature, vol. 505, no. 7482, pp. 195-198, 2014.
[46] A. Khetan, "High-throughput virtual screening of quinones for aqueous redox flow batteries: Status and perspectives," Batteries, vol. 9, no. 1, p. 24, 2022.
[47] S. Pablo-García, Á. García, G. D. Akkoc, M. Sim, Y. Cao, M. Somers, C. Hattrick, N. Yoshikawa, D. Dworschak, H. Hao, and A. Aspuru-Guzik, "An affordable platform for automated synthesis and electrochemical characterization," Device, 2024.
[48] D. M. Heard and A. J. Lennox, "Electrode materials in modern organic electrochemistry," Angewandte Chemie International Edition, vol. 59, no. 43, pp. 18 866-18 884, 2020.
[49] M. Quan, D. Sanchez, M. F. Wasylkiw, and D. K. Smith, "Voltammetry of quinones in unbuffered aqueous solution: reassessing the roles of proton transfer and hydrogen bonding in the aqueous electrochemistry of quinones," Journal of the American Chemical Society, vol. 129, no. 42, pp. 12 84712856, 2007.
[50] K. Darvish, L. Penco, J. Ramos, R. Cisneros, J. Pratt, E. Yoshida, S. Ivaldi, and D. Pucci, "Teleoperation of humanoid robots: A survey," IEEE Transactions on Robotics, 2023.
[51] S. G. Hart, "NASA-task load index (NASA-TLX); 20 years later," in Proceedings of the human factors and ergonomics society annual meeting, vol. 50. Sage publications Sage CA: Los Angeles, CA, 2006, pp. 904-908.
[52] J. Brooke, "SUS: A quick and dirty usability scale," Usability Eval. Ind., vol. 189, 111995.
[53] A. Furnham, "Response bias, social desirability and dissimulation," Personality and individual differences, vol. 7, no. 3, pp. 385-400, 1986.</p>
<p>[54] Y. Liu and X. Jiang, "Why microfluidics? merits and trends in chemical synthesis," Lab on a Chip, vol. 17, no. 23, pp. 39603978, 2017.
[55] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn et al., "RT-2: Vision-language-action models transfer web knowledge to robotic control," arXiv preprint arXiv:2307.15818, 2023.
[56] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone, "LLM+P: Empowering large language models with optimal planning proficiency," arXiv preprint arXiv:2304.11477, 2023.
[57] R. El-khawaldeh, M. A. Guy, F. Bork, N. Taherimakhsousi, K. N. Jones, J. Hawkins, L. Han, R. P. Pritchard, B. Cole, S. Monfette et al., "Keeping an "eye" on the experiment: computer vision for real-time monitoring and control," Chemical Science, 2023.
[58] P. K. Murali, B. Porr, and M. Kaboli, "Touch if it's transparent? actor: Active tactile-based category-level transparent object reconstruction," in 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2023, pp. 10792-10799.
[59] M. A. Lee, Y. Zhu, K. Srinivasan, P. Shah, S. Savarese, L. FeiFei, A. Garg, and J. Bohg, "Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks," in 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 89438950 .
[60] J. Jiang, G. Cao, J. Deng, T.-T. Do, and S. Luo, "Robotic perception of transparent objects: A review," IEEE Transactions on Artificial Intelligence, 2023.
[61] M. Skreta, Z. Zhou, J. L. Yuan, K. Darvish, A. Aspuru-Guzik, and A. Garg, "RePLan: Robotic replanning with perception and language models," 2024.
[62] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg, "ProgPrompt: Program generation for situated robot task planning using large language models," Autonomous Robots, pp. 1-14, 2023.
[63] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, "Code as policies: Language model programs for embodied control," arXiv preprint arXiv:2209.07753, 2022.
[64] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu et al., "PaLM-E: An embodied multimodal language model," arXiv preprint arXiv:2303.03378, 2023.
[65] M. Khodeir, A. Sonwane, R. Hari, and F. Shkurti, "Policyguided lazy search with feedback for task and motion planning," in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 3743-3749.
[66] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao, "ReAct: Synergizing reasoning and acting in language models," in The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/forum?id=WE_vluYUL-X
[67] M. Skreta, N. Yoshikawa, S. Arellano-Rubach, Z. Ji, L. B. Kristensen, K. Darvish, A. Aspuru-Guzik, F. Shkurti, and A. Garg, "Errors are useful prompts: Instruction guided task programming with verifier-assisted iterative prompting," arXiv preprint arXiv:2303.14100, 2023.
[68] A. Majumdar, F. Xia, B. Ichter, D. Batra, and L. Guibas, "Findthis: Language-driven object disambiguation in indoor environments," in Conference on Robot Learning. PMLR, 2023, pp. 1335-1347.
[69] M. Helmert, "The fast downward planning system," Journal of Artificial Intelligence Research, vol. 26, pp. 191-246, 2006.
[70] J. Illingworth and J. Kittler, "A survey of the hough transform," Computer vision, graphics, and image processing, vol. 44, no. 1, pp. 87-116, 1988.
[71] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su, J. Zhu, and L. Zhang, "Grounding DINO: Marrying DINO with grounded pre-training for open-set object detection," 2023.
[72] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni, and H.-Y. Shum, "DINO: DETR with improved denoising
anchor boxes for end-to-end object detection," arXiv preprint arXiv:2203.03605, 2022.
[73] Z. Zou, K. Chen, Z. Shi, Y. Guo, and J. Ye, "Object detection in 20 years: A survey," Proceedings of the IEEE, vol. 111, no. 3, pp. 257-276, 2023.
[74] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Dollár, and R. Girshick, "Segment anything," arXiv preprint arXiv:2304.02643, 2023.
[75] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., "Learning transferable visual models from natural language supervision," in International conference on machine learning. PMLR, 2021, pp. 8748-8763.
[76] Stereolabs, "ZED 2 - AI Stereo Camera," https://www.stereo labs.com/products/zed-2, accessed: 2023-12-22.
[77] Q.-Y. Zhou, J. Park, and V. Koltun, "Open3D: A modern library for 3D data processing," arXiv preprint arXiv:1801.09847, 2018.
[78] C. Labrin and F. Urdinez, "Principal component analysis," in $R$ for Political Data Science. Chapman and Hall/CRC, 2020, pp. 375-393.
[79] P. Beeson and B. Ames, "Trac-ik: An open-source library for improved solving of generic inverse kinematics," in 2015 IEEE$R A S$ 15th International Conference on Humanoid Robots (Humanoids). IEEE, 2015, pp. 928-935.
[80] Z. Kingston, M. Moll, and L. E. Kavraki, "Exploring implicit spaces for constrained sampling-based planning," Int. J. Robot. Res., vol. 38, no. 10-11, pp. 1151-1178, 2019.
[81] S. Karaman and E. Frazzoli, "Sampling-based algorithms for optimal motion planning," Int. J. Robot. Res., vol. 30, no. 7, pp. 846-894, 2011.
[82] "redox potential," International Union of Pure and Applied Chemistry (IUPAC), 2019. [Online]. Available: https://doi.or $\mathrm{g} / 10.1351 /$ goldbook.RT06783
[83] M. Walker, G. Pizzuto, H. Fakhruldeen, and A. I. Cooper, "Go with the flow: deep learning methods for autonomous viscosity estimations," Digital Discovery, vol. 2, pp. 1540-1547, 2023. [Online]. Available: http://dx.doi.org/10.1039/D3DD00109A
[84] Stereolabs, "ZED Mini Camera and SDK Overview," 2019.
[85] G. Bradski, "The OpenCV Library," Dr. Dobb's Journal of Software Tools, 2000.
[86] C. Hwang, W. Cui, Y. Xiong, Z. Yang, Z. Liu, H. Hu, Z. Wang, R. Salas, J. Jose, P. Ram, J. Chau, P. Cheng, F. Yang, M. Yang, and Y. Xiong, "Tutel: Adaptive mixture-of-experts at scale," arXiv preprint arXiv:2206.03382, 2022.
[87] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, "An image is worth 16x16 words: Transformers for image recognition at scale," in International Conference on Learning Representations, 2021. [Online]. Available: https://openrevi ew.net/forum?id=YicbFdNTTy
[88] N. Elgrishi, K. J. Rountree, B. D. McCarthy, E. S. Rountree, T. T. Eisenhart, and J. L. Dempsey, "A practical beginner's guide to cyclic voltammetry," Journal of chemical education, vol. 95, no. 2, pp. 197-206, 2018.
[89] H. E. Grecco, M. C. Dartiailh, G. Thalhammer-Thurner, T. Bronger, and F. Bauer, "Pyvisa: the python instrumentation package," Journal of Open Source Software, vol. 8, no. 84, p. 5304, 2023.</p>
<h1>Supplemental information</h1>
<h2>Note S1: Perception Analysis</h2>
<p>Perception for Lab Automation Background: Effective scene perception is paramount in the lab automation context, with a particular emphasis on two facets of perceptual skills: chemistry-level perception for synthesis monitoring and analysis and object-level perception for the manipulation of lab equipment. Addressing chemistrylevel perception, early efforts, exemplified by LabPics [17], introduced an innovative image dataset and employed convolutional neural networks (CNNs) to discern material phases and delineate phase boundaries. Recent advancements by HeinSight systems [18], [19] have furthered this field by presenting a more generalized vision system that not only classifies material phases but also quantifies physical properties such as volume, color, and turbidity, thereby facilitating comprehensive monitoring and control of experimental processes. Moreover, in the work of [83], liquid viscosity estimation was achieved through the utilization of robot manipulators for collecting fluid motion videos, followed by 3D CNN analysis.</p>
<p>Turning to object-level perception, the transparency of many chemistry lab tools poses a unique challenge. Notably, works have focused on detecting transparent vessels in occluded scenes by leveraging 3D reconstruction and multiview perception [20], [21]. This approach enabled the estimation of depth, segments, and object poses, enhancing the efficacy of downstream manipulation tasks. These models focus on detecting common transparent and opaque objects; however, they do not generalize well to unfamiliar and novel objects. We adopted Grounding DINO [71] with SAM [74], a more versatile approach that is also capable of accepting unconstrained text prompts.</p>
<p>Evaluation of object detection and pose estimation in ORGANA: The perception pipeline is evaluated in two aspects: object detection and object position estimation. Average Precision (AP) was used to measure the detection performance for each object class. Mean Absolute Error (MAE) was used to evaluate the position estimate of the objects. Since we assumed that objects maintain an upright orientation and object orientation is not utilized in our chemistry experiments, the quality of the orientation prediction was not evaluated. A real dataset related to the electrochemistry setup was constructed for conducting these evaluations.</p>
<p>Data Collection: We used a ZED Mini camera [84] to collect images of varying scene setups. Specifically, chemistry equipment, including beakers, flasks, and a polishing pad were placed on a lab bench; we randomized lighting conditions, table backgrounds, colored liquids in transparent vessels, and object locations. We automated the data collection by having the robot with an eye-in-hand camera capture RGBD images from different angles, as shown in Figure S1. In total, the dataset consists of 135 RGBD images captured across 17 different scenes. Each scene contains 4 transparent objects and 1 polishing plate.</p>
<p>To obtain the ground truth object pose in the world frame, AprilTags [24] were randomly placed on the table. An image was then captured by the robot camera and the 6D poses of the tags were estimated using OpenCV functions [85]. We converted these poses into world coordinates based on known camera poses and subsequently replaced the tags with associated objects on the table. For obtaining the ground truth 2D bounding boxes for each object, we created 3D mesh models based on measured dimensions. Point clouds of these mesh models were projected to 2D image space using the corresponding tag pose, camera intrinsic and extrinsic matrices. The 2D bounding box for each set of projected points was generated and treated as the ground truth.</p>
<p>Implementation: The perception pipeline was built based on Grounding DINO [71] and SAM models [74] with a custom object pose estimation mechanism shown in Figure S2. For evaluation, we directly used the checkpoint of the Grounding DINO model with the Swin-T backbone [86] and SAM with the ViT-B backbone [87]. To detect beakers and the polishing station in the scene, we prompted the Grounding DINO with "glass object" and "plate".</p>
<p>Results: For object detection, Table S1 summarizes the Average Precision (AP) for each class supported by the perception pipeline. We observed that the precision of glass objects is consistently high across different Intersection over Union (IoU) thresholds. In contrast, the pipeline achieves a high AP for detecting plates with low IoU thresholds, but the detection performance dramatically decreases as IoU increases. This occurs because the Groundning DINO model often recognizes the brown pad as the plate rather than the entire polishing pad fixture, resulting in a lower IoU, as demonstrated in Figure S1. Additionally, we found that the pipeline can handle partial occlusion, as shown in Figure S1. This capability enables the pipeline to withstand more realistic and complex conditions outside of preset object configurations.</p>
<p>In terms of object position estimation, the pipeline achieved a Mean Absolute Error (MAE) of 3.5 cm , averaged over all objects in the dataset. Specifically, the small beaker has the lowest MAE of 2.4 cm , while the large flask has the highest MAE of 5.1 cm . The accuracy of the object position is also highly dependent on the depth map from the ZED camera, as discussed in the paper ("Transparent and opaque object detection and pose estimation" section). Although the ZED Mini camera is reported to have a depth error of $1.5 \%$ within its range of 10 cm to 3 m [84], the depth accuracy of transparent objects degrades given that background depth values are reported instead. To address this issue, we applied the radius outlier removal method [77] directly on point clouds as demonstrated</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Department of Computer Science, University of Toronto, Toronto, ON M5S 1A1, Canada
${ }^{2}$ Vector Institute, Toronto, ON M5G 0C6, Canada
${ }^{3}$ Acceleration Consortium, University of Toronto, Toronto, ON M5G 1X6, Canada
${ }^{4}$ NVIDIA, Santa Clara, CA 95051, USA
${ }^{5}$ Department of Computer Science, Georgia Institute of Technology, Atlanta, GA 30332, USA
${ }^{\dagger}$ These authors contributed equally.
$\ddagger$ These authors supervised equally.
*Lead contact and correspondence: kdarvish@cs.toronto.edu&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
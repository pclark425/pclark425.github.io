<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1331 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1331</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1331</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-53107462</p>
                <p><strong>Paper Title:</strong> Sim-to-Real Transfer with Neural-Augmented Robot Simulation</p>
                <p><strong>Paper Abstract:</strong> : Despite the recent successes of deep reinforcement learning, teaching complex motor skills to a physical robot remains a hard problem. While learning directly on a real system is usually impractical, doing so in simulation has proven to be fast and safe. Nevertheless, because of the "reality gap," policies trained in simulation often perform poorly when deployed on a real system. In this work, we introduce a method for training a recurrent neural network on the differences between simulated and real robot trajectories and then using this model to augment the simulator. This Neural-Augmented Simulation (NAS) can be used to learn control policies that transfer signiﬁcantly better to real environments than policies learned on existing simulators. We demonstrate the potential of our approach through a set of experiments on the Mujoco simulator with added backlash and the Poppy Ergo Jr robot. NAS allows us to learn policies that are competitive with ones that would have been learned directly on the real robot.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1331.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1331.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo physics engine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performance rigid-body physics engine used as the primary source simulator for OpenAI Gym robotic control tasks in this paper; authors used it as the source environment and modified it (added backlash) to create an artificial reality gap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer with Neural-Augmented Robot Simulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Rigid-body dynamics simulator supporting articulated bodies, contacts, joint dynamics and friction; used via OpenAI Gym environments (Pusher, Striker, ErgoReacher) as the source simulator for policy learning and as the basis for augmented simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-to-high-fidelity rigid-body dynamics (accurate contacts, joint dynamics and friction modeling) but missing small hardware non-idealities by default (e.g., gearbox backlash, 3D-printed flex, servo heating/wear).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>models friction, contact forces, masses and inertias and joint constraints; operates with discrete time stepping (authors used 50Hz in sim); does not by default model servo gear backlash, plastic body flex, sensor detection miss-rates, or motor heating/wear unless explicitly modified.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>PPO-trained control policies; LSTM correction model φ (Neural-Augmented Simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning policies trained with PPO (policy gradient RL), and an LSTM-based recurrent neural network that predicts corrections to simulator state transitions (φ) for NAS.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Continuous control tasks: move/push/strike objects and reach targets (Pusher, Striker, ErgoReacher). Tasks are control/reasoning about dynamics rather than thermodynamics/circuits/biology.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Policies trained in MuJoCo achieved strong simulation performance (trained for 2M frames); NAS-augmented training in MuJoCo led to policies that matched or approached expert (target-trained) performance in sim-to-sim experiments (qualitative; no single numeric reward provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Target environments with a reality gap: (a) a modified MuJoCo target simulator with added backlash and (b) a real Poppy Ergo Jr physical robot (ErgoShield task).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>NAS-augmented policies transferred significantly better than source-only policies; on the Pusher NAS reached expert-level transfer in sim-to-sim and in real robot tasks NAS outperformed baselines (forward-model-only and source policies). No absolute numeric reward values reported in main text for transfer beyond qualitative/figure summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Authors experimented with varying physical parameters (mass, inertia, friction) and found small changes had little effect on policies, large changes destabilized simulation, and that adding backlash provided a stable but significant reality gap; NAS (difference-modeling) remained effective where forward models failed or overfit.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors argue that specific small hardware non-idealities (e.g., backlash) can be necessary to model to close the reality gap; they settled on modeling backlash as a minimal/sufficient perturbation that yields a useful gap without destabilizing the simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Source-policy trained purely in MuJoCo without augmentation failed to transfer when backlash was present; forward-model-only (open-loop LSTM) showed compounding errors and poor generalization despite fitting recorded trajectories (overfitting), particularly when training data lacked coverage of key interactions (e.g. ball pushes).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1331.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1331.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PyBullet physics engine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source physics simulator used to implement the ErgoShield simulated environment corresponding to the Poppy Ergo Jr robot; used as the source simulator for sim-to-real experiments on the ErgoShield task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer with Neural-Augmented Robot Simulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Open-source rigid-body dynamics and collision simulator (Bullet physics integration) used to simulate the Poppy Ergo Jr robots and the ErgoShield task at a simulation control frequency; provides joint dynamics, contacts and observation interfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity rigid-body simulation suitable for robot arm control tasks; models contacts and joint kinematics but lacks many real-world sources of non-stationarity (servo backlash, 3D-printed part flex, motor overheating) unless explicitly modeled.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>provides joint angle/velocity simulation, contact detection, discrete-time control (authors note simulation run at 50Hz vs real robot at 100Hz); does not inherently model electrical noise, conductive-paint hit-detection failures, or part wear.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>PPO-trained attacker policy; LSTM correction φ trained offline</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL policies (PPO) for attacker robot in ErgoShield; LSTM-based correction model trained on paired sim/real trajectories used to augment PyBullet outputs during policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Control task in the ErgoShield environment: attacker robot must hit defender's shield as often as possible; a multi-step dynamic control task involving contact interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Simulated training yielded policies that performed well in simulation; direct real-robot training performed poorly due to sparse/noisy hit detection and hardware issues; NAS-trained policies in PyBullet transferred better to the real robot than baselines (qualitative results reported).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real Poppy Ergo Jr physical robots executing the ErgoShield task.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>NAS ('Transfer policy') achieved significantly better real-robot performance than source-only or forward-model-only policies; exact numeric rewards are presented in figures but not numerically enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors note the simulator omits hardware non-idealities (backlash, PLA flex, motor overheating, conductive-paint thinning) and that these omissions drive the reality gap; augmenting the simulator with an LSTM correction can compensate for omitted effects.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Direct policies trained in PyBullet without augmentation underperformed on real robot due to omitted non-idealities and noisy/sparse real-world rewards; forward-model-only policies overfit and did not generalize well to novel policies when transferred.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1331.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1331.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI Gym environments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Gym (Pusher, Striker, ErgoReacher)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard RL benchmark environments implemented on top of MuJoCo used as simulated tasks; authors used these Gym tasks with modified dynamics (added backlash) to evaluate sim-to-sim and sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer with Neural-Augmented Robot Simulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>OpenAI Gym (MuJoCo-backed environments: Pusher, Striker, ErgoReacher)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Task wrappers around physics simulators (MuJoCo) providing standardized observation/action spaces for continuous-control benchmarks; authors used these to compare source, target and NAS-augmented policies.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>depends on underlying physics engine (MuJoCo) — generally medium-to-high for rigid-body dynamics in simulated benchmark tasks but lacking device-level non-idealities.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>simulate articulated arms, object interactions, friction and contact dynamics; discrete time steps; authors modified joint dynamics to add backlash in target environments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>PPO-trained policies; LSTM correction φ</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning agents trained in Gym environments with PPO; NAS uses LSTM correction of simulated state transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Robotic control: pushing a cylinder to a goal (Pusher), striking a ball into a goal (Striker), reaching a target (ErgoReacher).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Policies trained for 2M frames in source Gym environments; NAS-trained policies improved over source policies and reached expert performance on Pusher in sim-to-sim experiments (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Target Gym environments modified to include backlash; also transferred to real robot tasks in ErgoShield.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>NAS policies transferred significantly better to backlash-modified targets than source-only policies; forward-model-only sometimes produced misleadingly high simulated performance but failed to generalize on transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Authors report that changing mass/inertia/friction had limited effect unless changes were large (causing instability); adding backlash produced a stable, meaningful reality gap that exposed transfer failures — NAS improved transfer in these cases.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper suggests modeling the types of non-idealities that actually cause discrepancies (e.g., backlash) is more useful than large arbitrary parameter sweeps; too-large parameter changes can destabilize simulation and make learning infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Source policies trained in standard Gym environments failed on target Gym environments when backlash was introduced; forward-model-only solutions overfit to collected trajectories and performed poorly when confronted with novel policy behaviors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1331.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1331.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gym-ergojr (ErgoShield sim)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gym-ergojr (Poppy Ergo Jr simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GitHub-available simulated environment of the Poppy Ergo Jr robot used by the authors to run the ErgoShield task in simulation (implemented on PyBullet), paired with real-robot experiments for sim-to-real evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer with Neural-Augmented Robot Simulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>gym-ergojr (PyBullet-backed simulation of Poppy Ergo Jr)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Task-specific simulated environment replicating Poppy Ergo Jr robot kinematics and joint controls; provides 24-dim observations and 6-element control vector at 100Hz interface in the real robot (simulation ran at a lower frequency).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>task-level medium-fidelity simulation of the Poppy Ergo Jr arm; captures joint kinematics and basic dynamics but omits many hardware-specific non-idealities (servo gear backlash, body flex, motor overheating, sensor noise, imperfect hit detection).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>simulated joint angles/velocities and contact interactions; authors note simulation ran at 50Hz while real robot operates at 100Hz; does not model conductive-paint hit-detection miss rates or PLA deformation unless explicitly simulated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Attacker policy (PPO), defender random policy during data collection, LSTM correction φ</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PPO-trained attacker agent controlling 6 joint positions; LSTM-based difference model trained on paired sim/real episodes to produce corrected states for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>ErgoShield: attacker must hit defender's shield repeatedly in short episodes — a contact-rich control task requiring precise timing and kinematics.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Simulation allowed efficient policy learning; direct real-robot training performed worse due to sparse/noisy rewards. NAS-trained policies in gym-ergojr transferred to real hardware with better performance than baselines (qualitatively reported).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real Poppy Ergo Jr physical robots executing ErgoShield.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>NAS ('Transfer policy') outperformed source-only and forward-model-only policies on the real robot (statistically significant in their reported evaluations); direct real-robot training performed worse due to sparse detection and mechanical issues.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors emphasize modeling or correcting for hardware-specific non-idealities (backlash, rivet mounts, conductive-paint reliability) is important to achieve good sim-to-real transfer; gym-ergojr alone is insufficient without augmentation for high-quality transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When only forward-model was used (trained on recorded real data without grounding to sim), it overfit and failed to generalize to policies not represented in the dataset; insufficient real data coverage led NAS to underperform on behaviors not well represented in collected trajectories (e.g., Striker ball interactions).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1331.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1331.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural-Augmented Simulation (NAS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural-Augmented Simulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's proposed method: train a recurrent neural network (LSTM) to predict the difference between simulator and real trajectories, then augment the simulator with that correction model to train policies that transfer better to the real world without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>augmented underlying simulator (MuJoCo or PyBullet) + LSTM correction φ</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>NAS augments an existing physics simulator by applying an LSTM-predicted correction to each simulated state transition, producing simulator outputs grounded using paired real-world trajectory data.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics (sim-to-real transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>effectively elevates the simulator's fidelity in the state-transition sense by applying learned corrections; base simulator fidelity remains unchanged, but simulated trajectories become closer to recorded real trajectories in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>correction model uses recent history (LSTM hidden state) and simulated next-state to output additive correction term; leverages simulator for extrapolation to unobserved dynamics while correcting systematic differences (e.g., backlash).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>φ (LSTM difference model) used together with PPO-trained policies</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent neural network (LSTM) that predicts s_t^{real} - s_t^{sim} given simulated next-state, current simulated/real state and action; integrated into policy learning loop to produce corrected state estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Facilitates learning control policies for continuous-control tasks (Pusher, Striker, ErgoReacher, ErgoShield) by improving simulator realism for policy training and transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>NAS required relatively few real-world trajectories to be effective (authors report >100 trajectories gave near-expert transfer; they used 250–5k depending on task). Policies trained in NAS-augmented simulators reached or approached expert (target-trained) performance in several tasks (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real robot (Poppy Ergo Jr) and target simulators with added backlash; same augmentation reused across tasks without policy-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>NAS-produced policies transferred significantly better than source-only and forward-model-only baselines; on Pusher reached expert performance in sim-to-sim and on ErgoShield outperformed other approaches; required far fewer real trajectories than training expert policies directly (authors cite expert needing ~3-4k trajectories vs NAS needing >100).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors conclude that learning a correction model that compensates for simulator omissions (e.g., backlash) is sufficient to enable transfer for multiple tasks, and that the simulator itself can still provide useful extrapolation to dynamic regimes not covered by the real dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>NAS can fail where there is both a large simulator-target discrepancy and insufficient real-data coverage of relevant state-action regions (e.g., Striker ball interactions not sufficiently represented in random behavior data).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1331.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1331.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Forward dynamics LSTM (ψ)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Forward dynamics LSTM (open-loop forward model ψ)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM trained to predict the next real state from the current real state and action without using simulator information; serves as a baseline and is shown to suffer from compounding error and overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer with Neural-Augmented Robot Simulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>open-loop learned forward dynamics model (not a physics simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A learned predictive model (LSTM) of the target dynamics trained on recorded real trajectories; used as an alternative to simulator augmentation for policy learning but used in open-loop during rollouts leading to compounding errors.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>data-driven forward model fidelity limited by training data; can fit recorded trajectories well but poor open-loop long-horizon predictive fidelity due to compounding errors and overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>predicts s_{t+1}^{real} = s_t + ψ(s_t, a_t, h) in open-loop; accurate short-term predictions on training distribution but lacks grounding to a stable simulator and generalization to novel policies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ψ (forward model LSTM); policies trained using only ψ</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent neural network (LSTM) trained to predict next real state from current real state and action; used to create a fully learned environment for policy training.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Same continuous-control tasks as other methods, but using a learned dynamics model as the training environment rather than physics simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Forward model can match recorded trajectories closely (low point-wise errors on dataset) but during policy training its open-loop rollouts accumulate small errors into large deviations, harming learned policy performance (qualitative observation).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real robot (policies trained entirely in learned forward model then executed on real robot).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Policies trained solely on the forward model performed poorly on the real robot due to overfitting and poor generalization despite the model's low error on training trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors highlight that learning forward dynamics alone is insufficient unless the model generalizes across the distribution of states encountered during policy learning; grounding with simulator state helps retain extrapolation ability.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Open-loop compounding error and overfitting to recorded trajectories caused policies trained on ψ to fail when deployed on the real robot or on trajectories not in the training set.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1331.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1331.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Backlash (sim parameter)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gear backlash (simulated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An explicitly added hardware non-ideality (backlash) introduced into target simulations to create a controlled reality gap; used by authors because it produced a stable but meaningful discrepancy between source and target simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer with Neural-Augmented Robot Simulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>modified MuJoCo / target simulator with added joint backlash</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Simulation modification that introduces backlash (dead-zone/play) in specified joints to emulate gear play present in real servos; used to create the reality gap for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>introducing backlash increases fidelity with respect to real servo gear-play effects; it is a targeted, low-dimensional fidelity adjustment rather than a wholesale increase in simulator complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>models joint dead-zone/play (backlash) in selected joints; authors experimented adding backlash to 1–3 joints depending on task; other small parameter changes (mass/inertia/friction) were also tested but backlash provided a stable meaningful gap.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>N/A (simulator parameter), used in combination with NAS and baseline policies</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Used to test robustness and transfer of control policies across sim-to-sim and sim-to-real scenarios where joint play exists.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>When backlash was present only in the target, source policies trained without augmentation performed poorly on the target; NAS-corrected policies performed substantially better and could reach near-expert performance in some tasks (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Target simulator with backlash and the real robot (which exhibits similar servo gear backlash).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Adding backlash to target highlighted transfer failures of unaugmented policies; NAS corrected simulator outputs and enabled substantially improved transfer to backlash-containing targets and real hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Authors found backlash to be an effective, stable means of creating a realistic reality gap; small changes to mass/inertia/friction had little effect, large changes destabilized simulation, so backlash offered a practical compromise.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors argue that targeted simulation of the specific omitted non-idealities (such as backlash) that cause significant discrepancies is important; adding those features (or learning corrections for them) is sufficient for improved transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When relevant interactions related to backlash were not covered sufficiently in the collected real-data (e.g., rare interactions with the ball in Striker), NAS could not fully correct for the discrepancy and transfer performance suffered.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Sim-to-real robot learning from pixels with progressive nets <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Grounded action transformation for robot learning in simulation <em>(Rating: 1)</em></li>
                <li>Once more unto the breach: Co-evolving a robot and its simulator <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1331",
    "paper_id": "paper-53107462",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "MuJoCo",
            "name_full": "MuJoCo physics engine",
            "brief_description": "A high-performance rigid-body physics engine used as the primary source simulator for OpenAI Gym robotic control tasks in this paper; authors used it as the source environment and modified it (added backlash) to create an artificial reality gap.",
            "citation_title": "Sim-to-Real Transfer with Neural-Augmented Robot Simulation",
            "mention_or_use": "use",
            "simulator_name": "MuJoCo",
            "simulator_description": "Rigid-body dynamics simulator supporting articulated bodies, contacts, joint dynamics and friction; used via OpenAI Gym environments (Pusher, Striker, ErgoReacher) as the source simulator for policy learning and as the basis for augmented simulation.",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "medium-to-high-fidelity rigid-body dynamics (accurate contacts, joint dynamics and friction modeling) but missing small hardware non-idealities by default (e.g., gearbox backlash, 3D-printed flex, servo heating/wear).",
            "fidelity_characteristics": "models friction, contact forces, masses and inertias and joint constraints; operates with discrete time stepping (authors used 50Hz in sim); does not by default model servo gear backlash, plastic body flex, sensor detection miss-rates, or motor heating/wear unless explicitly modified.",
            "model_or_agent_name": "PPO-trained control policies; LSTM correction model φ (Neural-Augmented Simulation)",
            "model_description": "Reinforcement learning policies trained with PPO (policy gradient RL), and an LSTM-based recurrent neural network that predicts corrections to simulator state transitions (φ) for NAS.",
            "reasoning_task": "Continuous control tasks: move/push/strike objects and reach targets (Pusher, Striker, ErgoReacher). Tasks are control/reasoning about dynamics rather than thermodynamics/circuits/biology.",
            "training_performance": "Policies trained in MuJoCo achieved strong simulation performance (trained for 2M frames); NAS-augmented training in MuJoCo led to policies that matched or approached expert (target-trained) performance in sim-to-sim experiments (qualitative; no single numeric reward provided in text).",
            "transfer_target": "Target environments with a reality gap: (a) a modified MuJoCo target simulator with added backlash and (b) a real Poppy Ergo Jr physical robot (ErgoShield task).",
            "transfer_performance": "NAS-augmented policies transferred significantly better than source-only policies; on the Pusher NAS reached expert-level transfer in sim-to-sim and in real robot tasks NAS outperformed baselines (forward-model-only and source policies). No absolute numeric reward values reported in main text for transfer beyond qualitative/figure summaries.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Authors experimented with varying physical parameters (mass, inertia, friction) and found small changes had little effect on policies, large changes destabilized simulation, and that adding backlash provided a stable but significant reality gap; NAS (difference-modeling) remained effective where forward models failed or overfit.",
            "minimal_fidelity_discussion": "Authors argue that specific small hardware non-idealities (e.g., backlash) can be necessary to model to close the reality gap; they settled on modeling backlash as a minimal/sufficient perturbation that yields a useful gap without destabilizing the simulator.",
            "failure_cases": "Source-policy trained purely in MuJoCo without augmentation failed to transfer when backlash was present; forward-model-only (open-loop LSTM) showed compounding errors and poor generalization despite fitting recorded trajectories (overfitting), particularly when training data lacked coverage of key interactions (e.g. ball pushes).",
            "uuid": "e1331.0"
        },
        {
            "name_short": "PyBullet",
            "name_full": "PyBullet physics engine",
            "brief_description": "An open-source physics simulator used to implement the ErgoShield simulated environment corresponding to the Poppy Ergo Jr robot; used as the source simulator for sim-to-real experiments on the ErgoShield task.",
            "citation_title": "Sim-to-Real Transfer with Neural-Augmented Robot Simulation",
            "mention_or_use": "use",
            "simulator_name": "PyBullet",
            "simulator_description": "Open-source rigid-body dynamics and collision simulator (Bullet physics integration) used to simulate the Poppy Ergo Jr robots and the ErgoShield task at a simulation control frequency; provides joint dynamics, contacts and observation interfaces.",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "medium-fidelity rigid-body simulation suitable for robot arm control tasks; models contacts and joint kinematics but lacks many real-world sources of non-stationarity (servo backlash, 3D-printed part flex, motor overheating) unless explicitly modeled.",
            "fidelity_characteristics": "provides joint angle/velocity simulation, contact detection, discrete-time control (authors note simulation run at 50Hz vs real robot at 100Hz); does not inherently model electrical noise, conductive-paint hit-detection failures, or part wear.",
            "model_or_agent_name": "PPO-trained attacker policy; LSTM correction φ trained offline",
            "model_description": "RL policies (PPO) for attacker robot in ErgoShield; LSTM-based correction model trained on paired sim/real trajectories used to augment PyBullet outputs during policy learning.",
            "reasoning_task": "Control task in the ErgoShield environment: attacker robot must hit defender's shield as often as possible; a multi-step dynamic control task involving contact interactions.",
            "training_performance": "Simulated training yielded policies that performed well in simulation; direct real-robot training performed poorly due to sparse/noisy hit detection and hardware issues; NAS-trained policies in PyBullet transferred better to the real robot than baselines (qualitative results reported).",
            "transfer_target": "Real Poppy Ergo Jr physical robots executing the ErgoShield task.",
            "transfer_performance": "NAS ('Transfer policy') achieved significantly better real-robot performance than source-only or forward-model-only policies; exact numeric rewards are presented in figures but not numerically enumerated in text.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors note the simulator omits hardware non-idealities (backlash, PLA flex, motor overheating, conductive-paint thinning) and that these omissions drive the reality gap; augmenting the simulator with an LSTM correction can compensate for omitted effects.",
            "failure_cases": "Direct policies trained in PyBullet without augmentation underperformed on real robot due to omitted non-idealities and noisy/sparse real-world rewards; forward-model-only policies overfit and did not generalize well to novel policies when transferred.",
            "uuid": "e1331.1"
        },
        {
            "name_short": "OpenAI Gym environments",
            "name_full": "OpenAI Gym (Pusher, Striker, ErgoReacher)",
            "brief_description": "Standard RL benchmark environments implemented on top of MuJoCo used as simulated tasks; authors used these Gym tasks with modified dynamics (added backlash) to evaluate sim-to-sim and sim-to-real transfer.",
            "citation_title": "Sim-to-Real Transfer with Neural-Augmented Robot Simulation",
            "mention_or_use": "use",
            "simulator_name": "OpenAI Gym (MuJoCo-backed environments: Pusher, Striker, ErgoReacher)",
            "simulator_description": "Task wrappers around physics simulators (MuJoCo) providing standardized observation/action spaces for continuous-control benchmarks; authors used these to compare source, target and NAS-augmented policies.",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "depends on underlying physics engine (MuJoCo) — generally medium-to-high for rigid-body dynamics in simulated benchmark tasks but lacking device-level non-idealities.",
            "fidelity_characteristics": "simulate articulated arms, object interactions, friction and contact dynamics; discrete time steps; authors modified joint dynamics to add backlash in target environments.",
            "model_or_agent_name": "PPO-trained policies; LSTM correction φ",
            "model_description": "Reinforcement learning agents trained in Gym environments with PPO; NAS uses LSTM correction of simulated state transitions.",
            "reasoning_task": "Robotic control: pushing a cylinder to a goal (Pusher), striking a ball into a goal (Striker), reaching a target (ErgoReacher).",
            "training_performance": "Policies trained for 2M frames in source Gym environments; NAS-trained policies improved over source policies and reached expert performance on Pusher in sim-to-sim experiments (qualitative).",
            "transfer_target": "Target Gym environments modified to include backlash; also transferred to real robot tasks in ErgoShield.",
            "transfer_performance": "NAS policies transferred significantly better to backlash-modified targets than source-only policies; forward-model-only sometimes produced misleadingly high simulated performance but failed to generalize on transfer.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Authors report that changing mass/inertia/friction had limited effect unless changes were large (causing instability); adding backlash produced a stable, meaningful reality gap that exposed transfer failures — NAS improved transfer in these cases.",
            "minimal_fidelity_discussion": "Paper suggests modeling the types of non-idealities that actually cause discrepancies (e.g., backlash) is more useful than large arbitrary parameter sweeps; too-large parameter changes can destabilize simulation and make learning infeasible.",
            "failure_cases": "Source policies trained in standard Gym environments failed on target Gym environments when backlash was introduced; forward-model-only solutions overfit to collected trajectories and performed poorly when confronted with novel policy behaviors.",
            "uuid": "e1331.2"
        },
        {
            "name_short": "gym-ergojr (ErgoShield sim)",
            "name_full": "gym-ergojr (Poppy Ergo Jr simulator)",
            "brief_description": "A GitHub-available simulated environment of the Poppy Ergo Jr robot used by the authors to run the ErgoShield task in simulation (implemented on PyBullet), paired with real-robot experiments for sim-to-real evaluation.",
            "citation_title": "Sim-to-Real Transfer with Neural-Augmented Robot Simulation",
            "mention_or_use": "use",
            "simulator_name": "gym-ergojr (PyBullet-backed simulation of Poppy Ergo Jr)",
            "simulator_description": "Task-specific simulated environment replicating Poppy Ergo Jr robot kinematics and joint controls; provides 24-dim observations and 6-element control vector at 100Hz interface in the real robot (simulation ran at a lower frequency).",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "task-level medium-fidelity simulation of the Poppy Ergo Jr arm; captures joint kinematics and basic dynamics but omits many hardware-specific non-idealities (servo gear backlash, body flex, motor overheating, sensor noise, imperfect hit detection).",
            "fidelity_characteristics": "simulated joint angles/velocities and contact interactions; authors note simulation ran at 50Hz while real robot operates at 100Hz; does not model conductive-paint hit-detection miss rates or PLA deformation unless explicitly simulated.",
            "model_or_agent_name": "Attacker policy (PPO), defender random policy during data collection, LSTM correction φ",
            "model_description": "PPO-trained attacker agent controlling 6 joint positions; LSTM-based difference model trained on paired sim/real episodes to produce corrected states for policy learning.",
            "reasoning_task": "ErgoShield: attacker must hit defender's shield repeatedly in short episodes — a contact-rich control task requiring precise timing and kinematics.",
            "training_performance": "Simulation allowed efficient policy learning; direct real-robot training performed worse due to sparse/noisy rewards. NAS-trained policies in gym-ergojr transferred to real hardware with better performance than baselines (qualitatively reported).",
            "transfer_target": "Real Poppy Ergo Jr physical robots executing ErgoShield.",
            "transfer_performance": "NAS ('Transfer policy') outperformed source-only and forward-model-only policies on the real robot (statistically significant in their reported evaluations); direct real-robot training performed worse due to sparse detection and mechanical issues.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors emphasize modeling or correcting for hardware-specific non-idealities (backlash, rivet mounts, conductive-paint reliability) is important to achieve good sim-to-real transfer; gym-ergojr alone is insufficient without augmentation for high-quality transfer.",
            "failure_cases": "When only forward-model was used (trained on recorded real data without grounding to sim), it overfit and failed to generalize to policies not represented in the dataset; insufficient real data coverage led NAS to underperform on behaviors not well represented in collected trajectories (e.g., Striker ball interactions).",
            "uuid": "e1331.3"
        },
        {
            "name_short": "Neural-Augmented Simulation (NAS)",
            "name_full": "Neural-Augmented Simulation",
            "brief_description": "The paper's proposed method: train a recurrent neural network (LSTM) to predict the difference between simulator and real trajectories, then augment the simulator with that correction model to train policies that transfer better to the real world without task-specific fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "augmented underlying simulator (MuJoCo or PyBullet) + LSTM correction φ",
            "simulator_description": "NAS augments an existing physics simulator by applying an LSTM-predicted correction to each simulated state transition, producing simulator outputs grounded using paired real-world trajectory data.",
            "scientific_domain": "mechanics / robotics (sim-to-real transfer)",
            "fidelity_level": "effectively elevates the simulator's fidelity in the state-transition sense by applying learned corrections; base simulator fidelity remains unchanged, but simulated trajectories become closer to recorded real trajectories in practice.",
            "fidelity_characteristics": "correction model uses recent history (LSTM hidden state) and simulated next-state to output additive correction term; leverages simulator for extrapolation to unobserved dynamics while correcting systematic differences (e.g., backlash).",
            "model_or_agent_name": "φ (LSTM difference model) used together with PPO-trained policies",
            "model_description": "Recurrent neural network (LSTM) that predicts s_t^{real} - s_t^{sim} given simulated next-state, current simulated/real state and action; integrated into policy learning loop to produce corrected state estimates.",
            "reasoning_task": "Facilitates learning control policies for continuous-control tasks (Pusher, Striker, ErgoReacher, ErgoShield) by improving simulator realism for policy training and transfer.",
            "training_performance": "NAS required relatively few real-world trajectories to be effective (authors report &gt;100 trajectories gave near-expert transfer; they used 250–5k depending on task). Policies trained in NAS-augmented simulators reached or approached expert (target-trained) performance in several tasks (qualitative).",
            "transfer_target": "Real robot (Poppy Ergo Jr) and target simulators with added backlash; same augmentation reused across tasks without policy-specific fine-tuning.",
            "transfer_performance": "NAS-produced policies transferred significantly better than source-only and forward-model-only baselines; on Pusher reached expert performance in sim-to-sim and on ErgoShield outperformed other approaches; required far fewer real trajectories than training expert policies directly (authors cite expert needing ~3-4k trajectories vs NAS needing &gt;100).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors conclude that learning a correction model that compensates for simulator omissions (e.g., backlash) is sufficient to enable transfer for multiple tasks, and that the simulator itself can still provide useful extrapolation to dynamic regimes not covered by the real dataset.",
            "failure_cases": "NAS can fail where there is both a large simulator-target discrepancy and insufficient real-data coverage of relevant state-action regions (e.g., Striker ball interactions not sufficiently represented in random behavior data).",
            "uuid": "e1331.4"
        },
        {
            "name_short": "Forward dynamics LSTM (ψ)",
            "name_full": "Forward dynamics LSTM (open-loop forward model ψ)",
            "brief_description": "An LSTM trained to predict the next real state from the current real state and action without using simulator information; serves as a baseline and is shown to suffer from compounding error and overfitting.",
            "citation_title": "Sim-to-Real Transfer with Neural-Augmented Robot Simulation",
            "mention_or_use": "use",
            "simulator_name": "open-loop learned forward dynamics model (not a physics simulator)",
            "simulator_description": "A learned predictive model (LSTM) of the target dynamics trained on recorded real trajectories; used as an alternative to simulator augmentation for policy learning but used in open-loop during rollouts leading to compounding errors.",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "data-driven forward model fidelity limited by training data; can fit recorded trajectories well but poor open-loop long-horizon predictive fidelity due to compounding errors and overfitting.",
            "fidelity_characteristics": "predicts s_{t+1}^{real} = s_t + ψ(s_t, a_t, h) in open-loop; accurate short-term predictions on training distribution but lacks grounding to a stable simulator and generalization to novel policies.",
            "model_or_agent_name": "ψ (forward model LSTM); policies trained using only ψ",
            "model_description": "Recurrent neural network (LSTM) trained to predict next real state from current real state and action; used to create a fully learned environment for policy training.",
            "reasoning_task": "Same continuous-control tasks as other methods, but using a learned dynamics model as the training environment rather than physics simulator.",
            "training_performance": "Forward model can match recorded trajectories closely (low point-wise errors on dataset) but during policy training its open-loop rollouts accumulate small errors into large deviations, harming learned policy performance (qualitative observation).",
            "transfer_target": "Real robot (policies trained entirely in learned forward model then executed on real robot).",
            "transfer_performance": "Policies trained solely on the forward model performed poorly on the real robot due to overfitting and poor generalization despite the model's low error on training trajectories.",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors highlight that learning forward dynamics alone is insufficient unless the model generalizes across the distribution of states encountered during policy learning; grounding with simulator state helps retain extrapolation ability.",
            "failure_cases": "Open-loop compounding error and overfitting to recorded trajectories caused policies trained on ψ to fail when deployed on the real robot or on trajectories not in the training set.",
            "uuid": "e1331.5"
        },
        {
            "name_short": "Backlash (sim parameter)",
            "name_full": "Gear backlash (simulated)",
            "brief_description": "An explicitly added hardware non-ideality (backlash) introduced into target simulations to create a controlled reality gap; used by authors because it produced a stable but meaningful discrepancy between source and target simulations.",
            "citation_title": "Sim-to-Real Transfer with Neural-Augmented Robot Simulation",
            "mention_or_use": "use",
            "simulator_name": "modified MuJoCo / target simulator with added joint backlash",
            "simulator_description": "Simulation modification that introduces backlash (dead-zone/play) in specified joints to emulate gear play present in real servos; used to create the reality gap for experiments.",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "introducing backlash increases fidelity with respect to real servo gear-play effects; it is a targeted, low-dimensional fidelity adjustment rather than a wholesale increase in simulator complexity.",
            "fidelity_characteristics": "models joint dead-zone/play (backlash) in selected joints; authors experimented adding backlash to 1–3 joints depending on task; other small parameter changes (mass/inertia/friction) were also tested but backlash provided a stable meaningful gap.",
            "model_or_agent_name": "N/A (simulator parameter), used in combination with NAS and baseline policies",
            "model_description": "N/A",
            "reasoning_task": "Used to test robustness and transfer of control policies across sim-to-sim and sim-to-real scenarios where joint play exists.",
            "training_performance": "When backlash was present only in the target, source policies trained without augmentation performed poorly on the target; NAS-corrected policies performed substantially better and could reach near-expert performance in some tasks (qualitative).",
            "transfer_target": "Target simulator with backlash and the real robot (which exhibits similar servo gear backlash).",
            "transfer_performance": "Adding backlash to target highlighted transfer failures of unaugmented policies; NAS corrected simulator outputs and enabled substantially improved transfer to backlash-containing targets and real hardware.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Authors found backlash to be an effective, stable means of creating a realistic reality gap; small changes to mass/inertia/friction had little effect, large changes destabilized simulation, so backlash offered a practical compromise.",
            "minimal_fidelity_discussion": "Authors argue that targeted simulation of the specific omitted non-idealities (such as backlash) that cause significant discrepancies is important; adding those features (or learning corrections for them) is sufficient for improved transfer.",
            "failure_cases": "When relevant interactions related to backlash were not covered sufficiently in the collected real-data (e.g., rare interactions with the ball in Striker), NAS could not fully correct for the discrepancy and transfer performance suffered.",
            "uuid": "e1331.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim-to-real robot learning from pixels with progressive nets",
            "rating": 2,
            "sanitized_title": "simtoreal_robot_learning_from_pixels_with_progressive_nets"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Grounded action transformation for robot learning in simulation",
            "rating": 1,
            "sanitized_title": "grounded_action_transformation_for_robot_learning_in_simulation"
        },
        {
            "paper_title": "Once more unto the breach: Co-evolving a robot and its simulator",
            "rating": 1,
            "sanitized_title": "once_more_unto_the_breach_coevolving_a_robot_and_its_simulator"
        }
    ],
    "cost": 0.01577975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim-to-Real Transfer with Neural-Augmented Robot Simulation</p>
<p>Florian Golemo florian.golemo@inria.fr 
Adrien Ali Taïga adrien.ali.taiga@umontreal.ca 
Pierre-Yves Oudeyer pierre-yves.oudeyer@inria.fr 
Aaron Courville aaron.courville@umontreal.ca </p>
<p>INRIA Bordeaux &amp; MILA</p>
<p>MILA
Université de Montréal</p>
<p>INRIA Bordeaux</p>
<p>MILA
Université de Montréal</p>
<p>Sim-to-Real Transfer with Neural-Augmented Robot Simulation
8644EB775AE096D82BCB1A00F6BC8D1A
Despite the recent successes of deep reinforcement learning, teaching complex motor skills to a physical robot remains a hard problem.While learning directly on a real system is usually impractical, doing so in simulation has proven to be fast and safe.Nevertheless, because of the "reality gap," policies trained in simulation often perform poorly when deployed on a real system.In this work, we introduce a method for training a recurrent neural network on the differences between simulated and real robot trajectories and then using this model to augment the simulator.This Neural-Augmented Simulation (NAS) can be used to learn control policies that transfer significantly better to real environments than policies learned on existing simulators.We demonstrate the potential of our approach through a set of experiments on the Mujoco simulator with added backlash and the Poppy Ergo Jr robot.NAS allows us to learn policies that are competitive with ones that would have been learned directly on the real robot.</p>
<p>Introduction</p>
<p>Reinforcement learning (RL) with function approximation has demonstrated remarkable performance in recent years.Prominent examples include playing Atari games from raw pixels [1], learning complex policies for continuous control [2,3], and surpassing human performance on the game of Go [4].However, most of these successes were achieved in non-physical environments: simulations, video games, etc. Learning complex policies on physical systems remains an open challenge.Typical reinforcement learning methods require a large amount of interaction with the environment and, in addition, it is also often impractical, if not dangerous, to roll out a partially trained policy.Both these issues make it unsuitable for many tasks to be trained directly on a physical robot.</p>
<p>The fidelity of current simulators and rendering engines provides an incentive to use them as training grounds for control policies, hoping the newly acquired skills would transfer back to reality.However, this is not as easy as one might hope since no simulator perfectly captures reality.This problem is widely known as the reality gap.See Figure 1 for an illustration.</p>
<p>The reality gap can be seen as an instance of a domain adaptation problem, where the input distribution of a model changes between training (in simulation) and testing (in the real world).In the case of image classification and off-policy image-based deep reinforcement learning, this issue has sometimes been tackled by refining images from the source domain -where the training happens -to appear closer to images from the target domain -where evaluation is carried out [5,6].</p>
<p>In this work, we take a similar approach and apply it to continuous control.We use data collected from a real robot to train a recurrent neural network to predict the discrepancies between simulation and the real world.This allows us to improve the quality of our simulation by grounding it in realistic trajectories.We refer to our approach as Neural-Augmented Simulation (NAS).We can use it with any reinforcement learning algorithm, combining the benefits of simulation and fast offline training, Figure 1: Impact of backlash: when setting both the simulated robot (red) and the real robot (white) to the resting position, the small backlash of each joint adds up to a noticeable difference in the end effector position.</p>
<p>while still learning policies that transfer well to robots in real environments.Since we collect data using a non-task-specific policy, NAS can be used to learn policies related to different tasks.Thus, we believe that NAS provides an efficient and effective strategy for multi-task sim-to-real transfer.</p>
<p>With our NAS strategy, we aim to achieve the best of both modeling modalities.While the recurrent neural network compensates for the unrealistically modeled aspects of the simulator; the simulator allows for better extrapolation to dynamic regimes that were not well explored under the data collection policy.Our choice to use a recurrent model2 is motivated by the desire to capture deviations that could violate the standard Markov assumption on the state space.</p>
<p>We evaluate our approach on two OpenAI Gym [8] based simulated environments with an artificially created reality gap in the form of added backlash.We also evaluate on the Poppy Ergo Jr robot arm, a relatively inexpensive arm with dynamics that are not well modeled by the existing simulator.We find that NAS provides an efficient way to learn policies that approach the performance of policies trained directly on the physical system.</p>
<p>Related work</p>
<p>Despite the recent success of deep reinforcement learning, learning on a real robot remains impractical due to poor sample efficiency.Classical methods for sim-to-real transfer include co-evolving the policy and the simulation [9,10], selecting for policies that transfer well [11,12], learning a transfer function of policies [13], and modeling different levels of noise [14].Nonetheless, these methods are usually limited to domains with a low-dimensional state-action space.</p>
<p>An alternative is to try to learn a forward model of the dynamics, a mapping between the current state and an action to the next state [15,16,17].However, despite some successes, learning a forward model of a real robot remains an open challenge.The compounding error of these models quickly deteriorates over time and corrections are needed to compensate for the uncertainty of the model [18].These methods are also usually sensitive to the data used to train the model, which is often different from the state distribution encountered during policy learning [19,20] Domain adaptation has received a lot of focus from the computer vision community.There have been many approaches such as fine-tuning a model trained on the source domain using data from the target domain [21], enforcing invariance in features between data from source and target domain [22] or learning a mapping between source and target domain [23].</p>
<p>Similar ideas have been applied in RLx, such as previous work that focused on learning internal representations robust to change in the input distribution [24].Using data generated from the simulation during training was also used in bootstrapping the target policy [25], Progressive Neural Networks [26] were also used to extend a simulation-learned policy to different target environments, but the most common method remains learning the control policy on the simulator before fine tuning on the real robot.Another quite successful recent method consists of randomizing all task-related physical properties of the simulation [27,28].This approach is very time-consuming because in order for a policy to generalize to all kinds of inputs, it has to experience a very wide range of noise during learning.Recently an approach very similar to our has been developed in the work of [29].However, rather than learning a state space transition, the authors learn a transformation of actions.This learning is done on-policy so once the model is learned, the simulator augmentation doesn't transfer to other tasks.</p>
<p>In summary, existing approaches struggle with either computational complexity or with difficulties in transferability to other tasks.We would like to introduce a technique that is capable of addressing both these issues.</p>
<p>Method</p>
<p>Notation</p>
<p>We consider a domain adaptation problem between a source domain D s (usually a simulator) and a target domain D t (typically a robot in the real world).Each domain is represented by a Markov Decision Process (MDP) S, A, p, r, γ , where S is the state space, A the action space, for s ∈ S and a ∈ S, p(.|s, a) the transition probability of the environment, r the reward and γ the discount factor.We assume that the two domains share the same action space, however, because of the perceptual-reality gap state space, dynamics and rewards can be different even if they are structurally similar, we write D s = S s , A, p s , r s and D t = S t , A, p t , r t for the two MDPs.We also assume that rewards are similar (i.e r s = r t ) and, most importantly, we assume that we are given access to the source domain and can reset it to a specific state.For example, for transfer from simulation to the robot, we assume we are free to reset the joint positions and angular velocities of the simulated robots.</p>
<p>Target domain data collection</p>
<p>We model the difference between the source and target domain, learning a mapping from trajectories in the source domain to trajectories in the target domain.Given a behavior policy µ (which could be random or provided by an expert) we collect trajectories from the target environment (i.e.real robot) follow actions given by µ.</p>
<p>Training the model</p>
<p>To train the LSTM in NAS, we sample an initial state s 0 ∼ p t (s 0 ) from the target domain distribution and set the source domain to start from the same initial state (i.e s s 0 = s t 0 = s 0 ).At each time step an action is sampled following the behavior policy a i ∼ µ(s t i ), then executed on the two domains to get the transition (s i , a i , s s i+1 , s t i+1 ).The source domain is then reset to the target domain state and the procedure is repeated until the end of the episode.The resulting trajectory τ = (s 0 , a 0 , s s 1 , s t 1 , ..., s s T −2 , a T , s s T −1 , s t T −1 ) of length T is then stored, the procedure is described in Algorithm 1.After collecting the data the model φ, an LSTM [7], is trained to predict s t i+1 .The difference between two states is usually small, so the network outputs a correction term φ(s t i , a i , h,
s s i+1 ) = s t i+1 − s s i+1
where h is the hidden state of the network.We also compare our approach with a forward model trained without using information from the source domain, ψ(s i , a i , h) = s t i+1 − s i .We normalize the inputs and outputs, and the model is trained with maximum likelihood using Adam [30].</p>
<p>Transferring to the target domain</p>
<p>Once the model φ is trained, it can be combined with the source environment to learn a policy that will later be transferred to the target environment.We use PPO 3 [31] in all our experiments but any other reinforcement learning algorithm could be used.During learning at each time step the current state transition in the source domain is passed to φ to compute an estimate of the current state in the target environment, an action a i is chosen according to this estimate a i ∼ π(φ(s s i , s s i+1 )), then the source domain state is set to the current estimate of the target domain state ∼ φ(s s i , s s i+1 ).This will allow our model to correct trajectories from the source domain, making them closer to the corresponding trajectory in the target domain.</p>
<p>Experiments</p>
<p>We evaluated our method on a set of simulated robotic control tasks using the MuJoCo physics engine [32] as well as on the open-source 3D-printed physical robot "Poppy Ergo Jr" [33] (see https://www.poppy-project.org/en/robots/poppy-ergo-jr)4 .We created an artificial reality gap using two simulated environments.The source and target environments are identical, except that the target environment has backlash.We also experimented with small changes in joints and link parameters but noted that it did not impact our results.Overall, the difference in backlash between the environments was significant enough to prevent policies trained on the source from performing well on the target environment (though an agent could solve the target environment if trained on it).More details about why we picked backlash can be found in appendix A.2.</p>
<p>We used the following environments from OpenAI Gym for our experiments:</p>
<p>• Pusher: A 3-DOF arm trying to move a cylinder on a plane to a specific location.</p>
<p>• Striker: A 7-DOF arm that has to strike a ball on a table in order to make the ball move into a goal that is out of reach for the robot.</p>
<p>• ErgoReacher: A 4-DOF arm that has to reach a goal spot with its end effector.</p>
<p>While we added backlash to only one joint of the Pusher, to test the limits of our artificial reality gap, we added backlash to three joints of the Striker and two joints or ErgoReacher.We also compare our proposed method with different baselines:</p>
<p>• Expert policy: policy trained directly in the target environment</p>
<p>• Source policy: transferring a policy trained in source environment without any adaption</p>
<p>• Forward model policy: a forward dynamic model ψ is trained using an LSTM and data collected from the target domain then a policy trained using only this model (without insight from the source domain)</p>
<p>• Transfer policy: the policy trained using NAS</p>
<p>Trajectory following</p>
<p>We evaluated the two models learned φ and ψ on a 100-step trajectory rollout on the Pusher (see Figure 4).The forward model ψ is trained without access the source domain and is making predictions in an open-loop.While this model is accurate at the beginning of the rollout, its small error compounds over time.We will see later that this makes it hard to learn policies that will achieve a high reward.On the other hand, the model φ is grounded using the source environment and only needs to correct the simulator predictions, so the difference between the trajectory in the real environment is barely noticeable.It shows that correcting trajectories from the source environment provide an efficient way to model the target environment.</p>
<p>Simulated Environments -Sim to Sim transfer</p>
<p>We tested our method on the simulated environments previously introduced.The number of trajectories used to train the models varied from 250 for the Pusher over 1k for the ErgoReacher to 5k for the Striker.Policies are trained for 2M frames and evaluated over 100 episodes, averaged across 5 seeds, results are given in Figure 5.Additional information about the training can be found in app.A.1.</p>
<p>Our experiments show that in all our environments the policy learned using NAS improve significantly over the source policy, and even reach expert performance on the Pusher.Though it seems that the forward model policy is doing better on the Striker, this is just a consequence of reward hacking; the agent learns a single movement that pushes away the ball without considering the target position.In contrast, the policy learned using NAS aims for the right direction but does not push it hard enough to make it all the way to the target.This happens when, following a random policy in the target domain, we do not record enough interaction between the robot and the ball to model this behavior correctly.An expert policy (e.g.given by human demonstration) could help alleviate this issue and make sure that the relevant parts of the state action space are covered when collecting the data.Altogether it highlights the fact that we cannot hope to learn a good transfer policy for states where there exists both a significant discrepancy between the source and target environments and insufficient data to properly train the LSTM.</p>
<p>We also varied the number of trajectories used to train the model in NAS to see how it influences the final performance of the transferred policy, see Figure 6.When more than 100 trajectories are used, the difference with the expert policy is not visually noticeable and the difference in reward is only due to variance in the evaluation and policy learning.This is in contrast to the 3-4k trajectories required by the expert policy during training to reach its final performance.We use the existing Poppy Ergo Jr robot arm in a novel task called "ErgoShield", a low-cost physical variant of the OpenAI Gym "Reacher" task.Each robot arm has 6 DOF: with respect to the robot's heading 1 perpendicular joint at the base, followed by 2 in parallel, 1 perpendicular, and two more in parallel.All joints can rotate from -90 degrees to +90 degrees from their resting position.</p>
<p>Physical Robot Experiments Overview</p>
<p>For this task we fixed two of these robots onto a wooden plank, facing each other at a distance that left the two tips 5mm apart in resting position.One robot is holding a "sword" (pencil) and the other is holding a shield.The shield robot ("defender") moves every 2.5s to a random position in which the shield is in reach of the opponent.The sword robot ("attacker") is the only robot directly controllable.Each episode lasts 10s and the attacker's goal is to touch the shield as often as possible.</p>
<p>Every time a hit is detected, the robots reset to a resting position (attacker) and different random position (defender).The setup is depicted in Figure 7 and additional specifications can be found in appendix B. This environment is accompanied by a simulation in PyBullet 56 .</p>
<p>The environment can be controlled at 100Hz by sending a 6-element vector in range [-1,1] corresponding to the desired motor position.The environment is observed at 100Hz as a 24-element vector consisting of: attacker joint angles, attacker joint velocities, defender joint angles, defender joint velocities.In the physical robots, the hit detection is implemented by coating both sword and shield in conductive paint, to which a MakeyMakey7 is attached.</p>
<p>For the offline data collection, a single robot with an empty pen holder end-effector was instructed to move to 3 random positions for one seconds each (corresponding to an episode length of 300 frames).</p>
<p>We collected 500 such episodes equivalent to roughly half an hour of robot movement including resetting between episodes.</p>
<p>Results on Sim-to-Real Transfer</p>
<p>We followed the same experimental paradigm as in the sim-to-sim experiments in 4.1: 3 expert policies were trained directly on the real robot, 3 policies were trained in simulation and were evaluated on the real robot, 3 policies were trained using our method, and 3 policies were trained with only the forward dynamics model.All policies were trained with the same PPO hyperparameters, save for the random seed.The hyperparameters can be found in appendix C.</p>
<p>The evaluation results are displayed in Figure 8a.The policies trained directly on the real robot performed significantly worse than all the other policies.The main reasons for this are (a) the hit  detection is not perfect (as in simulation) and since not every hit gets detected the reward is sparser 8 .And (b) since the attacker robot frequently gets their sword stuck in the opponent, in themselves, or in the environment, exploration is not as easy as in simulation.</p>
<p>We did not find a significant difference in the performance of the simulation and forward dynamics policies.However, our approach (the "Transfer policy") yielded a significantly better results than any others.</p>
<p>Figure 8b (and in more detail appendix D) shows for a single joint how the different approaches estimate the joint position under a given action.The simulation approaches the target position asymptotically while the real robot approaches the same value linearly.It is worth noting that even though the forward model can estimate the recorded dataset very well, policies trained using only this model and no simulation tend to perform badly.This is likely due to the forward model overfitting to the training data and not generalizing to other settings.This is a crucial feature of our method: by augmenting the simulator we are able to utilize the same learned augmentation to different tasks.</p>
<p>Conclusion</p>
<p>Currently, deep reinforcement learning algorithms are limited in their application to real world scenarios by their high sample complexity and their lack of guarantees when a partially trained policy is deployed.Transfer learning from simulation-trained agents offers a way to solve both these issues and enjoy more flexibility.To that end, we introduced a new method for learning a model that can be used to augment a robotic simulator and demonstrated the performance of this approach as well as its sample efficiency with respect to real robot data.</p>
<p>Provided the same robot is used, the model only has to be learned once and does not require policy-specific fine-tuning, making this method appropriate for multi-task robotic applications.</p>
<p>In the future, we would like to extend this approach to more extensive tasks and different robotic platforms to evaluate its generality, since working purely in simulation leaves some noise that occurs in real robots unmodeled.We would also like to move to image-based observations because our current action/observation spaces are low-dimensional but have a very high frequency (50Hz in sim, 100Hz on real robot).Since our approach is already neural network-based and neural networks are known to scale well to high dimensionality, this addition should be straightforward.</p>
<p>Another interesting path to investigate would be to combine more intelligent exploration methods for collecting the original dataset.If the initial exploration is guided by intrinsic motivation or count-based exploration it might further improve the sample-efficiency and reduce the amount of random movements that need to be recorded in the real robot.</p>
<p>A Experiment details A.1 Neural Network</p>
<p>For the Pusher, the neural network architecture is a fully connected layers with 128 hidden units with ReLU activations followed by a 3 layers LSTM with 128 hidden units and an other fully connected layers for the outputs.The Striker share the same architecture with 256 hidden units instead.The ErgoReacher environment only needed an LSTM of 3 layers with 100 hidden units each.Networks are trained using the Adam optimizer for 250 epochs with a batch size of 1 and a learning rate of 0.01.We use the PPO implementation and the recommended parameters for Mujoco from https:// github.com/ikostrikov/pytorch-a2c-ppo-acktrA.2 Why Backlash?</p>
<p>Regarding the setup for the sim2sim experiments, we tried increasing the reality gap between the two simulated environments on different simulators (Gazebo, MuJoCo, Unity) by tuning physical parameters like mass, inertia and friction.However, we found that small changes in these properties did not affect the policy trained in the source domain, whereas large changes made the environment unstable and it was not possible to learn a good policy directly in the target environment.Nevertheless, even in this extreme case the message from Figure 5 still holds and the proposed method was doing better than the forward model.We then settled on backlash to solve the previous issues as it offered a good compromise between simulation stability and inter-simulation difference.</p>
<p>As an example of one of these preliminary experiments, we increased the mass and inertia of the arm on the Pusher task by 50%, increased the mass and inertia of the pushable object by 100% (i.e.doubled it), and increased the surface friction of the table by 500% while keeping backlash.We found that with these changes, the difference model was still doing much better than the forward model.Looking at a trajectory rollout, the error was significant but close to zero.It should be noted, that such changes created a significant difference between the source and target environments.The new expert policy only averaged -91.3 in reward instead of the previous -67.5 (calculated over 100 episodes on 5 different seeds) which shows the reliability of our method when the reality gap increases.</p>
<p>B ErgoShield Specifications</p>
<p>The "swords" we used are standard BIC 19.5cm wooden pencils.The pencil is sticking out of the penholder end-effector by about 2cm at the bottom.The shield is 2.35cm in radius and 0.59cm deep in the center.A 3D-printable version is available at https://www.tinkercad.com/things/8UXdY4a5xdJ-ergo-jr-shield#/.</p>
<p>The random sampling ranges in degrees on each joint of the shield robot are [45, 15, 15, 22.5, 15, 15] respectively centered around the resting position.</p>
<p>The connection for training the real robot live has an average latency of 10ms.On the real robot the noise and non-stationarities can stem from</p>
<p>• The gear backlash on the Dynamixel XL-320 servos.</p>
<p>• The body of the robot being 3D-printed out of thin PLA plastic.</p>
<p>• The body parts being mounted to the servos via plastic rivets.</p>
<p>• Overheating of the motors and wear of the body parts.</p>
<p>• The electro-conductive paint thinning out over time.</p>
<p>C PPO Hyperparameters Real Robot</p>
<p>Figure 2 :
2
Figure 2: Left: Overview of the method for training the forward dynamics model.By gathering state differences when running the same action in simulation and on the real robot.Right: When the forward model is learned, it can be applied to simulator states to get the corresponding real state.This correction model (∆) is time-dependent (implemented as LSTM).The policy learning algorithm only has access to the "real" states.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Benchmark simulation environments used</p>
<p>Figure 5 :
5
Figure 5: Comparison of the different methods described when deployed on the target environment, for the Pusher (5a) and the Striker (5b).</p>
<p>Figure 6 :
6
Figure 6: We compare the results of our method when the number of trajectories used to train the model φ varies on the Pusher</p>
<p>Figure 7 :
7
Figure 7: The ErgoShield environment in reality and simulation.The attacker (sword, left side) has to hit the randomly-moving defender (shield, right side) as often as possible in 10s.In the left image the defender robot is covered in a sock to mitigate the attacker getting stuck too often.The joint angles were compensated for backlash in the left image to highlight similarities.</p>
<p>(a) Method Comparison (Real Robot) (b) Example Single Joint Position and Estimate over Time</p>
<p>Figure 8 :
8
Figure 8: Results of different simulation to real transfer approaches.Left: comparison of average rewards of 20 rollouts of 3 policies per approach.Right: comparison of single joint behavior when receiving a target position (violet dotted) in simulation (green dashed), on the real robot (blue solid), and estimates from the forward model (red, dashed) and our method (yellow, dotted)</p>
<p>Execute action a i in the source domain, observe a reward r i and a new state s s
Algorithm 1 Offline Data CollectionInitialize model φfor episode= 1, M doInitialize the target domain from a state s t 1 Initialize the source domain to the same state s s 1 = s t 1for i = 1, T doSelect action a i ∼ µ(s t i ) according to the behavior policy Execute action a i on the source domain and observe new state s s i+1 Execute action a i on the target domain and observe new state s t i+1 Update φ with the tuple s s i+1 = s t i+1Set s s i+1 = s t i+1end forend forAlgorithm 2 Policy Learning in Source DomainGiven a RL algorithm AA model φInitialize Afor episode= 1, M doInitialize the source domain from a state s s
1Initialize the φ latent variables h for i = 1, T do Sample action a i ∼ π(s s i ) using the behavioral policy fromA i+1 Set ŝt i+1 = s s i+1 + φ(s i , a i , h, s s i+1) the estimate of s t i+1 given by φ Do a one-step policy update with A using the transition (s s i , a i , ŝt i+1 , r i ) Set the source domain to s s i+1 = ŝt i+1 end for end for</p>
<p>We chose to use a Long Short-Term Model (LSTM)[7] as our recurrent neural network.
A list of all our hyperparameters is included in the supplementary material.
The code for our experiments can be found at https://github.com/aalitaiga/sim-to-real/
https://pybullet.org/wordpress/
The simulated environment is also available on GitHub at https://github.com/fgolemo/gym-ergojr
https://makeymakey.com/
There are no false positives, but we estimate that 10 − 15% hits aren't recognized.
AcknowledgmentsThis work was made possible with the funding and support of CIFAR, CHIST-ERA IGLU, and ANR.The authors would like to thank the MILA lab for being an amazing research environment and the FLOWERS team at INRIA Bordeaux for the ongoing support with the robots.On top of that, the authors would like to thank David Meger for providing a home for the little robots and input on the project, Herke van Hoof for giving valuable advice on the training procedure, and Alexandre Lacoste for making us revisit the method several times over until we were crystal clear.Additional thanks to NVIDIA for providing a Geforce Titan Xp for the INRIA Bordeaux lab.D Quantitative Analysis of TrajectoriesTable1displays the differences between the expert policy (the policy rolled out on the real robot) and (a) the source policy (same policy in simulation), (b) the forward model policy (the policy rolled out through the LSTM), and (c) the transferred model policy (our approach for modifying the simulator output).1st Quartile Mean 3rd Quartile Table1: Quantitative analysis of trajectory differences on the ErgoShield task, calculate by the sums of squared differences at each point in the trajectory over 1000 trajectories.The point-wise difference in (a) indicates the expected significant deviations between simulation and real robot.The low deviations in (b) are specifically what that model was trained for and are therefore near-zero.In practice however, this leads to overfitting, i.e. the so-trained model doesn't perform well on policies for which this model has not been exposed to any trajectories (which is evident from the performance in Figure8a).The differences in (c) show that the modified simulator is closer to the real robot trajectory.In combination with the final performance in Figure8awe can infer that our model does not overfit as the forward model does because it's harnessing the simulator to generalize to previously unseen trajectories.
Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 51875402015</p>
<p>Trust region policy optimization. J Schulman, S Levine, P Abbeel, M Jordan, P Moritz, Proceedings of the 32nd International Conference on Machine Learning (ICML-15). the 32nd International Conference on Machine Learning (ICML-15)2015</p>
<p>T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. 2015arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, Nature. 52975872016</p>
<p>Learning from simulated and unsupervised images through adversarial training. A Shrivastava, T Pfister, O Tuzel, J Susskind, W Wang, R Webb, The IEEE Conference on Computer Vision and Pattern Recognition. 201736</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, Robotics and Automation (ICRA). IEEE2018. 2018</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 981997</p>
<p>. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, arXiv:1606.015402016Openai gym. arXiv preprint</p>
<p>Back to reality: Crossing the reality gap in evolutionary robotics. J C Zagal, J Ruiz-Del Solar, P Vallejos, IFAC Proceedings Volumes. 200437</p>
<p>Once more unto the breach: Co-evolving a robot and its simulator. J Bongard, H Lipson, Proceedings of the Ninth International Conference on the Simulation and Synthesis of Living Systems (ALIFE9). the Ninth International Conference on the Simulation and Synthesis of Living Systems (ALIFE9)2004</p>
<p>Crossing the reality gap in evolutionary robotics by promoting transferable controllers. S Koos, J.-B Mouret, S Doncieux, Proceedings of the 12th annual conference on Genetic and evolutionary computation. the 12th annual conference on Genetic and evolutionary computationACM2010</p>
<p>Combining simulation and reality in evolutionary robotics. J C Zagal, J Ruiz-Del-Solar, Journal of Intelligent and Robotic Systems. 5012007</p>
<p>Adapting learned robotics behaviours through policy adjustment. J C G Higuera, D Meger, G Dudek, Robotics and Automation (ICRA), 2017 IEEE International Conference on. IEEE2017</p>
<p>Noise and the reality gap: The use of simulation in evolutionary robotics. N Jakobi, P Husbands, I Harvey, European Conference on Artificial Life. Springer1995</p>
<p>Deep learning helicopter dynamics models. A Punjani, P Abbeel, Robotics and Automation (ICRA), 2015 IEEE International Conference on. IEEE2015</p>
<p>One-shot learning of manipulation skills with online dynamics adaptation and neural network priors. J Fu, S Levine, P Abbeel, Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on. IEEE2016</p>
<p>Combining model-based policy search with online model learning for control of physical humanoids. I Mordatch, N Mishra, C Eppner, P Abbeel, Robotics and Automation (ICRA), 2016 IEEE International Conference on. IEEE2016</p>
<p>A Nagabandi, G Kahn, R S Fearing, S Levine, arXiv:1708.02596Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. 2017arXiv preprint</p>
<p>Pilco: A model-based and data-efficient approach to policy search. M Deisenroth, C E Rasmussen, Proceedings of the 28th International Conference on machine learning (ICML-11). the 28th International Conference on machine learning (ICML-11)2011</p>
<p>Guided cost learning: Deep inverse optimal control via policy optimization. C Finn, S Levine, P Abbeel, International Conference on Machine Learning. 2016</p>
<p>How transferable are features in deep neural networks?. J Yosinski, J Clune, Y Bengio, H Lipson, Advances in neural information processing systems. 2014</p>
<p>Domain-adversarial training of neural networks. Y Ganin, E Ustinova, H Ajakan, P Germain, H Larochelle, F Laviolette, M Marchand, V Lempitsky, Journal of Machine Learning Research. 17592016</p>
<p>Unsupervised cross-domain image generation. Y Taigman, A Polyak, L Wolf, arXiv:1611.022002016arXiv preprint</p>
<p>Deep domain confusion: Maximizing for domain invariance. E Tzeng, J Hoffman, N Zhang, K Saenko, T Darrell, arXiv:1412.34742014arXiv preprint</p>
<p>S James, E Johns, arXiv:1609.037593d simulation for robot arm control with deep q-learning. 2016arXiv preprint</p>
<p>A A Rusu, M Vecerik, T Rothörl, N Heess, R Pascanu, R Hadsell, arXiv:1610.04286Sim-to-real robot learning from pixels with progressive nets. 2016arXiv preprint</p>
<p>J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, arXiv:1703.06907Domain randomization for transferring deep neural networks from simulation to the real world. 2017arXiv preprint</p>
<p>X B Peng, M Andrychowicz, W Zaremba, P Abbeel, arXiv:1710.06537Sim-to-real transfer of robotic control with dynamics randomization. 2017arXiv preprint</p>
<p>Grounded action transformation for robot learning in simulation. J Hanna, P Stone, Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI). the 31st AAAI Conference on Artificial Intelligence (AAAI)February 2017</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2015</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEE2012</p>
<p>Poppy project: open-source fabrication of 3d printed humanoid robot for science, education and art. M Lapeyre, P Rouanet, J Grizou, S Nguyen, F Depraetre, A Le, P.-Y Falher, Oudeyer, Digital Intelligence. 2014. 20146</p>            </div>
        </div>

    </div>
</body>
</html>
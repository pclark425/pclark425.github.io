<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3331 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3331</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3331</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-104b0bb1da562d53cbda87aec79ef6a2827d191a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/104b0bb1da562d53cbda87aec79ef6a2827d191a" target="_blank">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work develops and releases Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters, which may be a suitable substitute for closed-source models.</p>
                <p><strong>Paper Abstract:</strong> In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3331.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3331.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rejection Sampling vs PPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rejection Sampling fine-tuning and Proximal Policy Optimization (PPO) for RLHF</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper compares two RLHF algorithms applied to Llama 2-Chat: (1) Rejection Sampling fine-tuning—sampling K outputs per prompt, scoring with a learned reward model, selecting the top sample and fine-tuning on those selections—and (2) PPO—on-policy optimization with a KL penalty to the original policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive transformer family (Llama 2) fine-tuned for dialogue (Llama 2-Chat) using supervised fine-tuning (SFT) then iterative RLHF; reward models trained from human pairwise preferences guide RLHF; GQA, RoPE, SwiGLU used in underlying architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Primary experiments on 70B (Rejection Sampling performed on 70B; smaller models distilled from the 70B outputs; 34B/13B/7B also fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Rejection Sampling (N-sample reranking and fine-tuning)', 'Proximal Policy Optimization (PPO) RLHF', 'Iterative RLHF versions (RLHF-V1..V5)', 'Supervised Fine-Tuning (SFT) bootstrap']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Rejection Sampling: for each prompt sample K outputs from the current policy, score each with the reward model, select the highest-scoring output and treat it as a gold target for supervised fine-tuning (breadth-first exploration). PPO: on-policy RL algorithm optimizing expected reward with a KL penalty term to the original policy (depth-first, single-sample per optimization step). Iterative RLHF: repeated cycles of collecting human preference data, training reward models, and applying rejection-sampling and/or PPO. SFT: initial supervised instruction-tuning prior to RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse: the paper explicitly contrasts breadth-oriented Rejection Sampling (multiple diverse samples per prompt) against depth-oriented PPO (single-sample iterative policy updates), and uses them in sequence (Rejection Sampling followed by PPO) in later iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>General preference alignment and downstream reasoning/assistant tasks (benchmarked on human helpfulness and safety evaluations; academic benchmarks include MMLU, BBH, GSM8K, HumanEval, Commonsense reasoning, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Human preference tasks (binary comparisons used to train reward models) and downstream evaluation on a mixture of academic benchmarks (MMLU, BBH, GSM8K, HumanEval, Commonsense QA, reading comprehension, code) plus human helpfulness/safety rating tasks and model-based reward evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Qualitative and indirect quantitative signals reported: Rejection Sampling used through RLHF-V4 and then PPO applied on top (RLHF-V5). Figure 7 shows the average max vs median reward among N samples (N up to 100) — the delta (max minus median) increases with N, indicating potential gains from Rejection Sampling. Figure 8 shows optimal sampling temperature for Llama 2-Chat-RLHF when sampling 10–100 outputs is T in [1.2,1.3]. Downstream human evaluations: after iterative RLHF (which used Rejection Sampling and then PPO), Llama 2-Chat models achieved strong results (examples: Llama 2-Chat-34B had >75% win-rate vs Vicuna-33B and Falcon-40B on the 4k helpfulness prompts; Llama 2-Chat-7B outperformed MPT-7B-chat on ~60% of prompts; Llama 2-Chat-70B had a 36% win-rate and 31.5% tie-rate vs ChatGPT on the human prompt set). Reward-model judged improvement shows Llama2 surpasses ChatGPT per in-house RMs after RLHF-V3 (harmlessness and helpfulness >50%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Breadth vs Depth: Rejection Sampling explores many candidate generations per prompt (breadth) and can exploit the reward model by re-ranking, producing higher maximum-rewarded outputs given enough samples; PPO optimizes the policy iteratively (depth) and applies a KL penalty to maintain closeness to base policy. The paper found Rejection Sampling beneficial early (used exclusively until RLHF-V4) and then combined with PPO for further gains. Rejection Sampling enabled distillation of large-model capabilities into smaller models (smaller models trained on the selected outputs). A drawback observed: restricting selection to only the previous-iteration 'bag' of samples (Rejection-Sampling-only pipeline) led to some capability regressions (e.g., worse poem rhyming) indicating forgetting unless top samples from earlier iterations were retained.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Sampling many diverse outputs (Rejection Sampling) provides measurable headroom (max reward grows with sample count) and is an effective way to improve model outputs guided by a reward model; 2) Combining Rejection Sampling with PPO yields further improvements — the pipeline used Rejection Sampling first, then PPO; 3) Temperature and number of samples must be tuned iteratively (optimal temperature changes as model is updated); 4) Rejection Sampling on the large model + distillation is an effective way to transfer capabilities to smaller models; 5) Sole dependence on a single strategy can induce regressions (forgetting) unless historical top samples are maintained.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>They observed regressions in certain creative capabilities (e.g., composing rhyming lines in poems) when using Rejection Sampling restricted to the sample bag of the immediate previous iteration (forgetting), which required retaining top samples from earlier iterations to mitigate. Also, Rejection Sampling requires significant compute (many samples) and careful temperature tuning; PPO provides training stability via KL penalty preventing reward hacking but explores less per prompt. No direct head-to-head scalar benchmark (e.g., GSM8K accuracy) split by Rejection vs PPO is reported — comparisons are mostly qualitative and via reward/human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3331.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3331.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Helpfulness RM vs Safety RM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Separate reward models for Helpfulness and Safety</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper trains two distinct reward models — one optimized for helpfulness and one for safety — from large-scale human pairwise preference data, arguing that single reward models can be confused by the tension between being maximally helpful and refusing unsafe prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Helpfulness Reward Model (Helpfulness RM) and Safety Reward Model (Safety RM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Both RMs are initialized from pretrained chat-model checkpoints and trained with a binary ranking loss (preference pairs) with an added margin term derived from the annotator strength label; training mixes Meta-collected data and open-source preference datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Initialized from chat-model checkpoints; exact parameter counts not separately reported for RMs (architecture and hyperparams identical to the pretrained Llama 2 models except classification head replaced by scalar regression head).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Pairwise human-preference reward modeling (binary ranking loss)', 'Margin-weighted ranking (leveraging graded preference strength)', 'Model initialization from chat checkpoints']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Reward model input is (prompt, response) and outputs a scalar score. Training loss is a binary ranking loss -log sigmoid(r(c) - r(r)) with an added margin m(r) that depends on how strongly annotators preferred the chosen response, so more separable pairs have larger margins. Two separate RMs are trained (helpfulness, safety) with different data mixtures to emphasize their domain.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar-styles separation: rather than a single hybrid reward, the paper explicitly trains two separate but related reward models to disentangle two distinct objectives (helpfulness vs safety).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Reward-model accuracy on internal Meta Helpfulness and Meta Safety test sets and several open-source preference benchmarks (Anthropic Helpful/Harmless, OpenAI Summarize, OpenAI WebGPT, StackExchange, Stanford SHP, SteamSHP-XL).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary human preference comparison tasks (chosen vs rejected) across many dialog/response pairs; test sets held out per batch and aggregated into Meta Helpfulness and Meta Safety test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Table 7: Helpfulness RM average performance across listed benchmarks ≈ 70.6 (avg across datasets in their table); Safety RM avg ≈ 64.3. Specifics from paper's Table 7: Helpfulness RM scored: Meta Helpful 63.2, Meta Safety 62.8, Anthropic Helpful 72.0, Anthropic Harmless 71.0, OpenAI Summ. 75.5, Stanford SHP 80.0; Safety RM scored: Meta Helpful 56.2, Meta Safety 64.5, Anthropic Helpful 55.4, Anthropic Harmless 74.7, OpenAI Summ. 71.7, Stanford SHP 65.2. GPT-4 baseline evaluated on pairwise comparisons scored 58.6 and 58.1 on the two internal axes in Table 7.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Explicit comparison: two separate RMs (Helpfulness vs Safety) outperform a single RM on their respective domains; Helpfulness RM works best on Meta Helpfulness test set and Safety RM on Meta Safety. The paper reports that training two RMs eases the learning problem because the helpfulness vs safety objectives can conflict and a single RM can be confused.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Splitting reward modeling into specialized objectives (helpfulness, safety) improves reward-model accuracy on corresponding test sets. 2) Adding a margin term based on annotator preference strength improves Helpfulness RM accuracy especially on more separable pairs. 3) Reward models initialized from chat-model checkpoints performed better (reduced information mismatch). 4) Reward-model accuracy scales with model size and more preference-data; training for more than one epoch produced overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Reward-model accuracy degrades for pairs labeled 'negligibly better/unsure' (similar responses) — Table 8 shows accuracy much higher on 'significantly better' comparisons and lower on 'negligibly better'. Also, the need to keep reward models on-distribution was emphasized: reward accuracy can degrade if the LM policy distribution shifts (hyper-specialization), requiring re-collection of preference data on new model outputs frequently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3331.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3331.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temperature-driven diverse sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling temperature and N-sample selection for exploration in Rejection Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper analyzes the effect of sampling temperature and number of samples (N) on the maximum reward obtainable via rejection-sampling-style re-ranking, finding that higher temperature and larger N increase diversity and potential max reward, and that optimal temperature changes as models are RLHF-updated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2-Chat (RLHF iterations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dialogue-optimized Llama 2 variants whose sampling behavior (temperature) is tuned during rejection-sampling collection; reward-scoring is used to pick best sample out of N.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Reported curves for SFT and RLHF models; experiments shown for Llama 2-Chat models (sizes not always enumerated per figure), primary rejection-sampling on 70B.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Temperature-controlled stochastic sampling', 'N-sample re-ranking with reward model (select max reward)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>For each prompt, sample N outputs at a specified temperature T; score each with the reward model and take the max (or top-k) to form training targets. Higher T increases output diversity; larger N increases chance of obtaining a high-reward trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Explicitly diverse: paper treats diversity as an exploration hyperparameter (temperature) combined with breadth via N samples — this is the principal mechanism to introduce diverse reasoning/solution styles for the same prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Internal RLHF reward maximization (proxy for human preference), applied to the same human prompt sets used for collecting preference annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Maximizing reward model score over N sampled outputs per prompt; downstream validation via human preference judgments and human evaluations of helpfulness/safety over 4k prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Figure 7: max-versus-median reward among N samples (N ∈ [1..100]) averaged over training prompts — the delta between max and median grows with N, showing potential gains. Figure 8: for Llama 2-Chat-SFT and Llama 2-Chat-RLHF, max reward as a function of N and temperature reveals the optimal temperature shifts after RLHF; for Llama 2-Chat-RLHF the optimal temperature for sampling 10–100 outputs is T in [1.2, 1.3]. No per-benchmark numeric accuracy is reported as a function of temperature/N, only the reward curves and downstream human-evaluation gains when using these methods in RLHF pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct comparison is between lower-temperature/single-sample regimes vs higher-temperature/multi-sample regimes: multi-sample + higher temperature yields higher maximum reward (greater headroom) at the cost of more sampling compute. The optimal temperature is not static and must be retuned as the model is updated during iterative RLHF. The paper recommends tuning temperature given finite compute budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Diversity (higher temperature) + larger N reliably increases the maximum reward available for rejection-sampling-based fine-tuning. 2) As the model changes via RLHF, the best temperature for exploration changes — empirically T ~1.2–1.3 for the RLHF models and N in [10..100]. 3) There is a trade-off with compute budget: more samples give better maximum but cost more compute.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No universal temperature: the optimal temperature changes across iterations — using a static temperature can be suboptimal. Also, increased diversity can produce high-reward but potentially less stable distributions if reward model misaligns; the paper mitigates by KL penalties in PPO and by keeping reward models on-distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3331.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3331.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ghost Attention (GAtt)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ghost Attention for multi-turn consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GAtt is a fine-tuning data hack that synthetically attaches a persistent system instruction to earlier turns of a multi-turn dialogue in training (while zeroing loss on previous-turn tokens) so that the model maintains attention to the system message across many turns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dialogue-fine-tuned Llama 2 models; GAtt is applied as an SFT/RLHF-stage augmentation to ensure multi-turn instruction consistency (e.g., 'always respond succinctly' or 'act as X').</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Applied after RLHF-V3; evaluation reported across models up to 20+ turns until context length limit (applies to sizes that support 4k context; used for 7B–70B variants).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Synthetic system-message concatenation (Ghost Attention)', 'Masked loss (zero out loss on previous-turn tokens) during fine-tuning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Construct SFT examples by concatenating an instruction (system message) to all user turns synthetically, then sample outputs from current RLHF model and fine-tune on these samples while setting loss to zero for tokens from previous dialogue turns — this encourages attention weights to maintain focus on the system message without inducing training mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar-style: GAtt is a targeted architectural/data-level technique to encourage a single consistent behavior across turns rather than a diversity-of-reasoning technique; it enforces consistent instruction-following across long dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Multi-turn instruction consistency (dialogue-level control), evaluated up to 20+ turns and visualized via attention activation maps.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks where a global system instruction must persist across many user-assistant turns (e.g., 'always answer in French' or 'act as Napoleon'); evaluation uses both quantitative turn-based consistency checks and attention visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Quantitative analysis (paper states consistency up to 20+ turns) and attention visualizations (Figure 10) showing higher attention activations to the system message throughout the dialogue with GAtt vs without; no standard benchmark accuracy numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared directly to the baseline RLHF model without GAtt: baseline models tended to forget the system instruction after a few turns; GAtt-equipped models maintain the instruction for many more turns (confirmed by attention heatmaps and turn-wise consistency evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GAtt materially improves multi-turn consistency by shaping attention toward the system message across the dialogue history; the paper reports consistency up to 20+ turns and shows attention maps illustrating this effect.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The current GAtt implementation is described as a vanilla/hack approach; paper notes potential future improvements (e.g., teaching model to change system message mid-conversation). No reported negative numerical results, but authors caution further development could improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3331.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3331.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT then RLHF pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning (SFT) followed by iterative RLHF</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The engineering pipeline starts with SFT on high-quality instruction-response pairs, then iteratively collects human preference data to train reward models and applies RLHF (Rejection Sampling and PPO) to produce Llama 2-Chat.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2-Chat family (7B, 13B, 34B, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base Llama 2 pretraining followed by supervised fine-tuning on curated instruction data (27,540 SFT annotations plus third-party data) and iterative RLHF with large-scale human preference data (>1M comparisons) to train reward models and run rejection-sampling/PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Released sizes: 7B, 13B, 70B (34B reported but not released); SFT and RLHF applied across these sizes with primary RLHF sampling on 70B and distillation to smaller sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Supervised instruction fine-tuning (SFT)', 'Human-preference reward modeling', 'Rejection Sampling and PPO RLHF', 'Ghost Attention (multi-turn consistency)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>SFT: supervised training on instruction-response pairs, zeroing loss on prompt tokens, trained for 2 epochs. Reward modeling: pairwise binary comparisons from humans to train scalar reward models with margin-based ranking loss. RLHF: Rejection Sampling to create distillation datasets, then application of PPO with reward composed from helpfulness and safety RMs and a KL penalty to the original policy.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse pipeline: uses multiple complementary techniques (SFT, two distinct RMs, rejection-sampling, PPO, and data-level methods like GAtt) to align and improve behavior — the paper emphasizes combining methods rather than relying on a single style.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Human helpfulness and safety evaluations (4k prompts), plus standard academic benchmarks (MMLU, BBH, GSM8K, HumanEval, Commonsense, Reading Comprehension, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Human evaluators compared model generations on ~4k prompts for helpfulness and ~2k adversarial prompts for safety; academic benchmarks measured zero/shot or few-shot performance on reasoning/coding/math/world-knowledge tasks (GSM8K, MMLU, HumanEval...).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Aggregate results after SFT+iterative RLHF: Llama 2 (pretrained base) improvements shown in Table 3 across benchmarks (e.g., Llama2-70B: MMLU 68.9, GSM8K 56.8, HumanEval 29.9). Fine-tuned Llama 2-Chat human-eval results: Llama 2-Chat-34B wins >75% vs Vicuna-33B and Falcon-40B; Llama 2-Chat-7B beats MPT-7B-chat on ~60% prompts; Llama 2-Chat-70B has 36% win-rate and 31.5% tie-rate vs ChatGPT on the prompt set. Reward-model judged improvements show surpassing ChatGPT per internal RMs after RLHF-V3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>SFT alone provided a strong starting point (authors note sampled model outputs often competitive with human SFT annotations), but iterative RLHF (Rejection Sampling + PPO guided by improved reward models) produced consistent gains in helpfulness and safety as judged by both reward models and human raters. The paper emphasizes that iterative re-collection of preference data on new model outputs is necessary to keep reward models on-distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) A small, high-quality SFT dataset (tens of thousands) plus large-scale preference data and iterative RLHF produces chat models competitive with closed-source chat products on the evaluated prompts. 2) Combining methods (SFT -> Rejection Sampling -> PPO) and separate RMs for helpfulness/safety was crucial to achieve aligned performance. 3) Maintaining reward-model coverage by collecting new preference data as models improve is necessary to prevent degraded RM accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>SFT-only models were competitive on some outputs but insufficient for product-grade alignment; RLHF iterations that used only the immediate previous bag of Rejection sampled outputs caused forgetting in some creative tasks (e.g., rhyming), requiring inclusion of top samples from earlier iterations to avoid regressions. Human evaluation limitations (no coding/reasoning prompts in the human prompt set) mean some improvements may not generalize to all reasoning-heavy benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to summarize with human feedback <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3331",
    "paper_id": "paper-104b0bb1da562d53cbda87aec79ef6a2827d191a",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "Rejection Sampling vs PPO",
            "name_full": "Rejection Sampling fine-tuning and Proximal Policy Optimization (PPO) for RLHF",
            "brief_description": "The paper compares two RLHF algorithms applied to Llama 2-Chat: (1) Rejection Sampling fine-tuning—sampling K outputs per prompt, scoring with a learned reward model, selecting the top sample and fine-tuning on those selections—and (2) PPO—on-policy optimization with a KL penalty to the original policy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 2-Chat",
            "model_description": "Auto-regressive transformer family (Llama 2) fine-tuned for dialogue (Llama 2-Chat) using supervised fine-tuning (SFT) then iterative RLHF; reward models trained from human pairwise preferences guide RLHF; GQA, RoPE, SwiGLU used in underlying architecture.",
            "model_size": "Primary experiments on 70B (Rejection Sampling performed on 70B; smaller models distilled from the 70B outputs; 34B/13B/7B also fine-tuned)",
            "reasoning_methods": [
                "Rejection Sampling (N-sample reranking and fine-tuning)",
                "Proximal Policy Optimization (PPO) RLHF",
                "Iterative RLHF versions (RLHF-V1..V5)",
                "Supervised Fine-Tuning (SFT) bootstrap"
            ],
            "reasoning_methods_description": "Rejection Sampling: for each prompt sample K outputs from the current policy, score each with the reward model, select the highest-scoring output and treat it as a gold target for supervised fine-tuning (breadth-first exploration). PPO: on-policy RL algorithm optimizing expected reward with a KL penalty term to the original policy (depth-first, single-sample per optimization step). Iterative RLHF: repeated cycles of collecting human preference data, training reward models, and applying rejection-sampling and/or PPO. SFT: initial supervised instruction-tuning prior to RLHF.",
            "diversity_of_methods": "Diverse: the paper explicitly contrasts breadth-oriented Rejection Sampling (multiple diverse samples per prompt) against depth-oriented PPO (single-sample iterative policy updates), and uses them in sequence (Rejection Sampling followed by PPO) in later iterations.",
            "reasoning_task_name": "General preference alignment and downstream reasoning/assistant tasks (benchmarked on human helpfulness and safety evaluations; academic benchmarks include MMLU, BBH, GSM8K, HumanEval, Commonsense reasoning, etc.)",
            "reasoning_task_description": "Human preference tasks (binary comparisons used to train reward models) and downstream evaluation on a mixture of academic benchmarks (MMLU, BBH, GSM8K, HumanEval, Commonsense QA, reading comprehension, code) plus human helpfulness/safety rating tasks and model-based reward evaluations.",
            "performance_by_method": "Qualitative and indirect quantitative signals reported: Rejection Sampling used through RLHF-V4 and then PPO applied on top (RLHF-V5). Figure 7 shows the average max vs median reward among N samples (N up to 100) — the delta (max minus median) increases with N, indicating potential gains from Rejection Sampling. Figure 8 shows optimal sampling temperature for Llama 2-Chat-RLHF when sampling 10–100 outputs is T in [1.2,1.3]. Downstream human evaluations: after iterative RLHF (which used Rejection Sampling and then PPO), Llama 2-Chat models achieved strong results (examples: Llama 2-Chat-34B had &gt;75% win-rate vs Vicuna-33B and Falcon-40B on the 4k helpfulness prompts; Llama 2-Chat-7B outperformed MPT-7B-chat on ~60% of prompts; Llama 2-Chat-70B had a 36% win-rate and 31.5% tie-rate vs ChatGPT on the human prompt set). Reward-model judged improvement shows Llama2 surpasses ChatGPT per in-house RMs after RLHF-V3 (harmlessness and helpfulness &gt;50%).",
            "comparison_of_methods": "Breadth vs Depth: Rejection Sampling explores many candidate generations per prompt (breadth) and can exploit the reward model by re-ranking, producing higher maximum-rewarded outputs given enough samples; PPO optimizes the policy iteratively (depth) and applies a KL penalty to maintain closeness to base policy. The paper found Rejection Sampling beneficial early (used exclusively until RLHF-V4) and then combined with PPO for further gains. Rejection Sampling enabled distillation of large-model capabilities into smaller models (smaller models trained on the selected outputs). A drawback observed: restricting selection to only the previous-iteration 'bag' of samples (Rejection-Sampling-only pipeline) led to some capability regressions (e.g., worse poem rhyming) indicating forgetting unless top samples from earlier iterations were retained.",
            "key_findings": "1) Sampling many diverse outputs (Rejection Sampling) provides measurable headroom (max reward grows with sample count) and is an effective way to improve model outputs guided by a reward model; 2) Combining Rejection Sampling with PPO yields further improvements — the pipeline used Rejection Sampling first, then PPO; 3) Temperature and number of samples must be tuned iteratively (optimal temperature changes as model is updated); 4) Rejection Sampling on the large model + distillation is an effective way to transfer capabilities to smaller models; 5) Sole dependence on a single strategy can induce regressions (forgetting) unless historical top samples are maintained.",
            "counter_examples_or_negative_results": "They observed regressions in certain creative capabilities (e.g., composing rhyming lines in poems) when using Rejection Sampling restricted to the sample bag of the immediate previous iteration (forgetting), which required retaining top samples from earlier iterations to mitigate. Also, Rejection Sampling requires significant compute (many samples) and careful temperature tuning; PPO provides training stability via KL penalty preventing reward hacking but explores less per prompt. No direct head-to-head scalar benchmark (e.g., GSM8K accuracy) split by Rejection vs PPO is reported — comparisons are mostly qualitative and via reward/human evaluations.",
            "uuid": "e3331.0",
            "source_info": {
                "paper_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Helpfulness RM vs Safety RM",
            "name_full": "Separate reward models for Helpfulness and Safety",
            "brief_description": "The paper trains two distinct reward models — one optimized for helpfulness and one for safety — from large-scale human pairwise preference data, arguing that single reward models can be confused by the tension between being maximally helpful and refusing unsafe prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Helpfulness Reward Model (Helpfulness RM) and Safety Reward Model (Safety RM)",
            "model_description": "Both RMs are initialized from pretrained chat-model checkpoints and trained with a binary ranking loss (preference pairs) with an added margin term derived from the annotator strength label; training mixes Meta-collected data and open-source preference datasets.",
            "model_size": "Initialized from chat-model checkpoints; exact parameter counts not separately reported for RMs (architecture and hyperparams identical to the pretrained Llama 2 models except classification head replaced by scalar regression head).",
            "reasoning_methods": [
                "Pairwise human-preference reward modeling (binary ranking loss)",
                "Margin-weighted ranking (leveraging graded preference strength)",
                "Model initialization from chat checkpoints"
            ],
            "reasoning_methods_description": "Reward model input is (prompt, response) and outputs a scalar score. Training loss is a binary ranking loss -log sigmoid(r(c) - r(r)) with an added margin m(r) that depends on how strongly annotators preferred the chosen response, so more separable pairs have larger margins. Two separate RMs are trained (helpfulness, safety) with different data mixtures to emphasize their domain.",
            "diversity_of_methods": "Similar-styles separation: rather than a single hybrid reward, the paper explicitly trains two separate but related reward models to disentangle two distinct objectives (helpfulness vs safety).",
            "reasoning_task_name": "Reward-model accuracy on internal Meta Helpfulness and Meta Safety test sets and several open-source preference benchmarks (Anthropic Helpful/Harmless, OpenAI Summarize, OpenAI WebGPT, StackExchange, Stanford SHP, SteamSHP-XL).",
            "reasoning_task_description": "Binary human preference comparison tasks (chosen vs rejected) across many dialog/response pairs; test sets held out per batch and aggregated into Meta Helpfulness and Meta Safety test sets.",
            "performance_by_method": "Table 7: Helpfulness RM average performance across listed benchmarks ≈ 70.6 (avg across datasets in their table); Safety RM avg ≈ 64.3. Specifics from paper's Table 7: Helpfulness RM scored: Meta Helpful 63.2, Meta Safety 62.8, Anthropic Helpful 72.0, Anthropic Harmless 71.0, OpenAI Summ. 75.5, Stanford SHP 80.0; Safety RM scored: Meta Helpful 56.2, Meta Safety 64.5, Anthropic Helpful 55.4, Anthropic Harmless 74.7, OpenAI Summ. 71.7, Stanford SHP 65.2. GPT-4 baseline evaluated on pairwise comparisons scored 58.6 and 58.1 on the two internal axes in Table 7.",
            "comparison_of_methods": "Explicit comparison: two separate RMs (Helpfulness vs Safety) outperform a single RM on their respective domains; Helpfulness RM works best on Meta Helpfulness test set and Safety RM on Meta Safety. The paper reports that training two RMs eases the learning problem because the helpfulness vs safety objectives can conflict and a single RM can be confused.",
            "key_findings": "1) Splitting reward modeling into specialized objectives (helpfulness, safety) improves reward-model accuracy on corresponding test sets. 2) Adding a margin term based on annotator preference strength improves Helpfulness RM accuracy especially on more separable pairs. 3) Reward models initialized from chat-model checkpoints performed better (reduced information mismatch). 4) Reward-model accuracy scales with model size and more preference-data; training for more than one epoch produced overfitting.",
            "counter_examples_or_negative_results": "Reward-model accuracy degrades for pairs labeled 'negligibly better/unsure' (similar responses) — Table 8 shows accuracy much higher on 'significantly better' comparisons and lower on 'negligibly better'. Also, the need to keep reward models on-distribution was emphasized: reward accuracy can degrade if the LM policy distribution shifts (hyper-specialization), requiring re-collection of preference data on new model outputs frequently.",
            "uuid": "e3331.1",
            "source_info": {
                "paper_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Temperature-driven diverse sampling",
            "name_full": "Sampling temperature and N-sample selection for exploration in Rejection Sampling",
            "brief_description": "The paper analyzes the effect of sampling temperature and number of samples (N) on the maximum reward obtainable via rejection-sampling-style re-ranking, finding that higher temperature and larger N increase diversity and potential max reward, and that optimal temperature changes as models are RLHF-updated.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 2-Chat (RLHF iterations)",
            "model_description": "Dialogue-optimized Llama 2 variants whose sampling behavior (temperature) is tuned during rejection-sampling collection; reward-scoring is used to pick best sample out of N.",
            "model_size": "Reported curves for SFT and RLHF models; experiments shown for Llama 2-Chat models (sizes not always enumerated per figure), primary rejection-sampling on 70B.",
            "reasoning_methods": [
                "Temperature-controlled stochastic sampling",
                "N-sample re-ranking with reward model (select max reward)"
            ],
            "reasoning_methods_description": "For each prompt, sample N outputs at a specified temperature T; score each with the reward model and take the max (or top-k) to form training targets. Higher T increases output diversity; larger N increases chance of obtaining a high-reward trajectory.",
            "diversity_of_methods": "Explicitly diverse: paper treats diversity as an exploration hyperparameter (temperature) combined with breadth via N samples — this is the principal mechanism to introduce diverse reasoning/solution styles for the same prompt.",
            "reasoning_task_name": "Internal RLHF reward maximization (proxy for human preference), applied to the same human prompt sets used for collecting preference annotations.",
            "reasoning_task_description": "Maximizing reward model score over N sampled outputs per prompt; downstream validation via human preference judgments and human evaluations of helpfulness/safety over 4k prompts.",
            "performance_by_method": "Figure 7: max-versus-median reward among N samples (N ∈ [1..100]) averaged over training prompts — the delta between max and median grows with N, showing potential gains. Figure 8: for Llama 2-Chat-SFT and Llama 2-Chat-RLHF, max reward as a function of N and temperature reveals the optimal temperature shifts after RLHF; for Llama 2-Chat-RLHF the optimal temperature for sampling 10–100 outputs is T in [1.2, 1.3]. No per-benchmark numeric accuracy is reported as a function of temperature/N, only the reward curves and downstream human-evaluation gains when using these methods in RLHF pipeline.",
            "comparison_of_methods": "Direct comparison is between lower-temperature/single-sample regimes vs higher-temperature/multi-sample regimes: multi-sample + higher temperature yields higher maximum reward (greater headroom) at the cost of more sampling compute. The optimal temperature is not static and must be retuned as the model is updated during iterative RLHF. The paper recommends tuning temperature given finite compute budgets.",
            "key_findings": "1) Diversity (higher temperature) + larger N reliably increases the maximum reward available for rejection-sampling-based fine-tuning. 2) As the model changes via RLHF, the best temperature for exploration changes — empirically T ~1.2–1.3 for the RLHF models and N in [10..100]. 3) There is a trade-off with compute budget: more samples give better maximum but cost more compute.",
            "counter_examples_or_negative_results": "No universal temperature: the optimal temperature changes across iterations — using a static temperature can be suboptimal. Also, increased diversity can produce high-reward but potentially less stable distributions if reward model misaligns; the paper mitigates by KL penalties in PPO and by keeping reward models on-distribution.",
            "uuid": "e3331.2",
            "source_info": {
                "paper_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Ghost Attention (GAtt)",
            "name_full": "Ghost Attention for multi-turn consistency",
            "brief_description": "GAtt is a fine-tuning data hack that synthetically attaches a persistent system instruction to earlier turns of a multi-turn dialogue in training (while zeroing loss on previous-turn tokens) so that the model maintains attention to the system message across many turns.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 2-Chat",
            "model_description": "Dialogue-fine-tuned Llama 2 models; GAtt is applied as an SFT/RLHF-stage augmentation to ensure multi-turn instruction consistency (e.g., 'always respond succinctly' or 'act as X').",
            "model_size": "Applied after RLHF-V3; evaluation reported across models up to 20+ turns until context length limit (applies to sizes that support 4k context; used for 7B–70B variants).",
            "reasoning_methods": [
                "Synthetic system-message concatenation (Ghost Attention)",
                "Masked loss (zero out loss on previous-turn tokens) during fine-tuning"
            ],
            "reasoning_methods_description": "Construct SFT examples by concatenating an instruction (system message) to all user turns synthetically, then sample outputs from current RLHF model and fine-tune on these samples while setting loss to zero for tokens from previous dialogue turns — this encourages attention weights to maintain focus on the system message without inducing training mismatch.",
            "diversity_of_methods": "Similar-style: GAtt is a targeted architectural/data-level technique to encourage a single consistent behavior across turns rather than a diversity-of-reasoning technique; it enforces consistent instruction-following across long dialogues.",
            "reasoning_task_name": "Multi-turn instruction consistency (dialogue-level control), evaluated up to 20+ turns and visualized via attention activation maps.",
            "reasoning_task_description": "Tasks where a global system instruction must persist across many user-assistant turns (e.g., 'always answer in French' or 'act as Napoleon'); evaluation uses both quantitative turn-based consistency checks and attention visualizations.",
            "performance_by_method": "Quantitative analysis (paper states consistency up to 20+ turns) and attention visualizations (Figure 10) showing higher attention activations to the system message throughout the dialogue with GAtt vs without; no standard benchmark accuracy numbers reported.",
            "comparison_of_methods": "Compared directly to the baseline RLHF model without GAtt: baseline models tended to forget the system instruction after a few turns; GAtt-equipped models maintain the instruction for many more turns (confirmed by attention heatmaps and turn-wise consistency evaluations).",
            "key_findings": "GAtt materially improves multi-turn consistency by shaping attention toward the system message across the dialogue history; the paper reports consistency up to 20+ turns and shows attention maps illustrating this effect.",
            "counter_examples_or_negative_results": "The current GAtt implementation is described as a vanilla/hack approach; paper notes potential future improvements (e.g., teaching model to change system message mid-conversation). No reported negative numerical results, but authors caution further development could improve performance.",
            "uuid": "e3331.3",
            "source_info": {
                "paper_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "SFT then RLHF pipeline",
            "name_full": "Supervised Fine-Tuning (SFT) followed by iterative RLHF",
            "brief_description": "The engineering pipeline starts with SFT on high-quality instruction-response pairs, then iteratively collects human preference data to train reward models and applies RLHF (Rejection Sampling and PPO) to produce Llama 2-Chat.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 2-Chat family (7B, 13B, 34B, 70B)",
            "model_description": "Base Llama 2 pretraining followed by supervised fine-tuning on curated instruction data (27,540 SFT annotations plus third-party data) and iterative RLHF with large-scale human preference data (&gt;1M comparisons) to train reward models and run rejection-sampling/PPO.",
            "model_size": "Released sizes: 7B, 13B, 70B (34B reported but not released); SFT and RLHF applied across these sizes with primary RLHF sampling on 70B and distillation to smaller sizes.",
            "reasoning_methods": [
                "Supervised instruction fine-tuning (SFT)",
                "Human-preference reward modeling",
                "Rejection Sampling and PPO RLHF",
                "Ghost Attention (multi-turn consistency)"
            ],
            "reasoning_methods_description": "SFT: supervised training on instruction-response pairs, zeroing loss on prompt tokens, trained for 2 epochs. Reward modeling: pairwise binary comparisons from humans to train scalar reward models with margin-based ranking loss. RLHF: Rejection Sampling to create distillation datasets, then application of PPO with reward composed from helpfulness and safety RMs and a KL penalty to the original policy.",
            "diversity_of_methods": "Diverse pipeline: uses multiple complementary techniques (SFT, two distinct RMs, rejection-sampling, PPO, and data-level methods like GAtt) to align and improve behavior — the paper emphasizes combining methods rather than relying on a single style.",
            "reasoning_task_name": "Human helpfulness and safety evaluations (4k prompts), plus standard academic benchmarks (MMLU, BBH, GSM8K, HumanEval, Commonsense, Reading Comprehension, etc.)",
            "reasoning_task_description": "Human evaluators compared model generations on ~4k prompts for helpfulness and ~2k adversarial prompts for safety; academic benchmarks measured zero/shot or few-shot performance on reasoning/coding/math/world-knowledge tasks (GSM8K, MMLU, HumanEval...).",
            "performance_by_method": "Aggregate results after SFT+iterative RLHF: Llama 2 (pretrained base) improvements shown in Table 3 across benchmarks (e.g., Llama2-70B: MMLU 68.9, GSM8K 56.8, HumanEval 29.9). Fine-tuned Llama 2-Chat human-eval results: Llama 2-Chat-34B wins &gt;75% vs Vicuna-33B and Falcon-40B; Llama 2-Chat-7B beats MPT-7B-chat on ~60% prompts; Llama 2-Chat-70B has 36% win-rate and 31.5% tie-rate vs ChatGPT on the prompt set. Reward-model judged improvements show surpassing ChatGPT per internal RMs after RLHF-V3.",
            "comparison_of_methods": "SFT alone provided a strong starting point (authors note sampled model outputs often competitive with human SFT annotations), but iterative RLHF (Rejection Sampling + PPO guided by improved reward models) produced consistent gains in helpfulness and safety as judged by both reward models and human raters. The paper emphasizes that iterative re-collection of preference data on new model outputs is necessary to keep reward models on-distribution.",
            "key_findings": "1) A small, high-quality SFT dataset (tens of thousands) plus large-scale preference data and iterative RLHF produces chat models competitive with closed-source chat products on the evaluated prompts. 2) Combining methods (SFT -&gt; Rejection Sampling -&gt; PPO) and separate RMs for helpfulness/safety was crucial to achieve aligned performance. 3) Maintaining reward-model coverage by collecting new preference data as models improve is necessary to prevent degraded RM accuracy.",
            "counter_examples_or_negative_results": "SFT-only models were competitive on some outputs but insufficient for product-grade alignment; RLHF iterations that used only the immediate previous bag of Rejection sampled outputs caused forgetting in some creative tasks (e.g., rhyming), requiring inclusion of top samples from earlier iterations to avoid regressions. Human evaluation limitations (no coding/reasoning prompts in the human prompt set) mean some improvements may not generalize to all reasoning-heavy benchmarks.",
            "uuid": "e3331.4",
            "source_info": {
                "paper_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to summarize with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2
        }
    ],
    "cost": 0.0217715,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Llama 2: Open Foundation and Fine-Tuned Chat Models</h1>
<p>Hugo Touvron<em> Louis Martin ${ }^{\dagger}$ Kevin Stone ${ }^{\dagger}$<br>Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra<br>Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen<br>Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller<br>Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou<br>Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev<br>Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich<br>Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra<br>Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi<br>Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang<br>Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang<br>Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic<br>Sergey Edunov Thomas Scialom</em></p>
<h2>GenAI, Meta</h2>
<h4>Abstract</h4>
<p>In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Contents</h1>
<p>1 Introduction ..... 3
2 Pretraining ..... 5
2.1 Pretraining Data ..... 5
2.2 Training Details ..... 5
2.3 Llama 2 Pretrained Model Evaluation ..... 7
3 Fine-tuning ..... 8
3.1 Supervised Fine-Tuning (SFT) ..... 9
3.2 Reinforcement Learning with Human Feedback (RLHF) ..... 9
3.3 System Message for Multi-Turn Consistency ..... 16
3.4 RLHF Results ..... 17
4 Safety ..... 20
4.1 Safety in Pretraining ..... 20
4.2 Safety Fine-Tuning ..... 23
4.3 Red Teaming ..... 28
4.4 Safety Evaluation of Llama 2-Chat ..... 29
5 Discussion ..... 32
5.1 Learnings and Observations ..... 32
5.2 Limitations and Ethical Considerations ..... 34
5.3 Responsible Release Strategy ..... 35
6 Related Work ..... 35
7 Conclusion ..... 36
A Appendix ..... 46
A. 1 Contributions ..... 46
A. 2 Additional Details for Pretraining ..... 47
A. 3 Additional Details for Fine-tuning ..... 51
A. 4 Additional Details for Safety ..... 58
A. 5 Data Annotation ..... 72
A. 6 Dataset Contamination ..... 75
A. 7 Model Card ..... 77</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Helpfulness human evaluation results for Llama 2-Chat compared to other open-source and closed-source models. Human raters compared model generations on $\sim 4 \mathrm{k}$ prompts consisting of both single and multi-turn prompts. The $95 \%$ confidence intervals for this evaluation are between $1 \%$ and $2 \%$. More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent difficulty of comparing generations.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Win-rate \% for helpfulness and safety between commercial-licensed baselines and Llama 2-Chat, according to GPT4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.</p>
<h1>1 Introduction</h1>
<p>Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.
The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022), but none of these models are suitable substitutes for closed "product" LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.
In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closedsource models. Human raters judged model generations for safety violations across $\sim 2,000$ adversarial prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models.</p>
<p>We are releasing the following models to the general public for research and commercial use ${ }^{\ddagger}$ :</p>
<ol>
<li>Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by $40 \%$, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing. ${ }^{5}$</li>
<li>Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well.</li>
</ol>
<p>We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not - and could not - cover all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guide ${ }^{\mathbb{\&amp;}}$ and code examples ${ }^{1}$ to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.
The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology (Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related work (Section 6), and conclusions (Section 7).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Training of Llama 2-Chat: This process begins with the pretraining of Llama 2 using publicly available online sources. Following this, we create an initial version of Llama 2-Chat through the application of supervised fine-tuning. Subsequently, the model is iteratively refined using Reinforcement Learning with Human Feedback (RLHF) methodologies, specifically through rejection sampling and Proximal Policy Optimization (PPO). Throughout the RLHF stage, the accumulation of iterative reward modeling data in parallel with model enhancements is crucial to ensure the reward models remain within distribution.</p>
<h1>2 Pretraining</h1>
<p>To create the new family of Llama 2 models, we began with the pretraining approach described in Touvron et al. (2023), using an optimized auto-regressive transformer, but made several changes to improve performance. Specifically, we performed more robust data cleaning, updated our data mixes, trained on $40 \%$ more total tokens, doubled the context length, and used grouped-query attention (GQA) to improve inference scalability for our larger models. Table 1 compares the attributes of the new Llama 2 models with the Llama 1 models.</p>
<h3>2.1 Pretraining Data</h3>
<p>Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta's products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performance-cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.
We performed a variety of pretraining data investigations so that users can better understand the potential capabilities and limitations of our models; results can be found in Section 4.1.</p>
<h3>2.2 Training Details</h3>
<p>We adopt most of the pretraining setting and model architecture from Llama 1. We use the standard transformer architecture (Vaswani et al., 2017), apply pre-normalization using RMSNorm (Zhang and Sennrich, 2019), use the SwiGLU activation function (Shazeer, 2020), and rotary positional embeddings (RoPE, Su et al. 2022). The primary architectural differences from Llama 1 include increased context length and grouped-query attention (GQA). We detail in Appendix Section A.2.1 each of these differences with ablation experiments to demonstrate their importance.</p>
<p>Hyperparameters. We trained using the AdamW optimizer (Loshchilov and Hutter, 2017), with $\beta_{1}=$ $0.9, \beta_{2}=0.95$, eps $=10^{-5}$. We use a cosine learning rate schedule, with warmup of 2000 steps, and decay final learning rate down to $10 \%$ of the peak learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. Figure 5 (a) shows the training loss for Llama 2 with these hyperparameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Training Data</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">Context <br> Length</th>
<th style="text-align: center;">GQA</th>
<th style="text-align: center;">Tokens</th>
<th style="text-align: center;">LR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">2 k</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.0 T</td>
<td style="text-align: center;">$3.0 \times 10^{-4}$</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA 1</td>
<td style="text-align: left;">See Touvron et al.</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">2 k</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.0 T</td>
<td style="text-align: center;">$3.0 \times 10^{-4}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">(2023)</td>
<td style="text-align: center;">33B</td>
<td style="text-align: center;">2 k</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.4 T</td>
<td style="text-align: center;">$1.5 \times 10^{-4}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;">65B</td>
<td style="text-align: center;">2 k</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.4 T</td>
<td style="text-align: center;">$1.5 \times 10^{-4}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.0 T</td>
<td style="text-align: center;">$3.0 \times 10^{-4}$</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA 2</td>
<td style="text-align: left;">A new mix of publicly</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.0 T</td>
<td style="text-align: center;">$3.0 \times 10^{-4}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">available online data</td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2.0 T</td>
<td style="text-align: center;">$1.5 \times 10^{-4}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2.0 T</td>
<td style="text-align: center;">$1.5 \times 10^{-4}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Llama 2 family of models. Token counts refer to pretraining data only. All models are trained with a global batch-size of 4 M tokens. Bigger models - 34B and 70B - use Grouped-Query Attention (GQA) for improved inference scalability.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Training Loss for Llama 2 models. We compare the training loss of the Llama 2 family of models. We observe that after pretraining on 2 T Tokens, the models still did not show any sign of saturation.</p>
<p>Tokenizer. We use the same tokenizer as Llama 1; it employs a bytepair encoding (BPE) algorithm (Sennrich et al., 2016) using the implementation from SentencePiece (Kudo and Richardson, 2018). As with Llama 1, we split all numbers into individual digits and use bytes to decompose unknown UTF-8 characters. The total vocabulary size is 32 k tokens.</p>
<h1>2.2.1 Training Hardware \&amp; Carbon Footprint</h1>
<p>Training Hardware. We pretrained our models on Meta's Research Super Cluster (RSC) (Lee and Sengupta, 2022) as well as internal production clusters. Both clusters use NVIDIA A100s. There are two key differences between the two clusters, with the first being the type of interconnect available: RSC uses NVIDIA Quantum InfiniBand while our production cluster is equipped with a RoCE (RDMA over converged Ethernet) solution based on commodity ethernet Switches. Both of these solutions interconnect 200 Gbps end-points. The second difference is the per-GPU power consumption cap - RSC uses 400 W while our production cluster uses 350 W . With this two-cluster setup, we were able to compare the suitability of these different types of interconnect for large scale training. RoCE (which is a more affordable, commercial interconnect network)</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Time <br> (GPU hours)</th>
<th style="text-align: center;">Power <br> Consumption (W)</th>
<th style="text-align: center;">Carbon Emitted <br> (tCO2eq)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">184320</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">31.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">368640</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">62.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">1038336</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">153.90</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">1720320</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">291.42</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3311616</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">539.00</td>
</tr>
</tbody>
</table>
<p>Table 2: $\mathbf{C O}_{2}$ emissions during pretraining. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. $100 \%$ of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.
can scale almost as well as expensive Infiniband up to 2000 GPUs, which makes pretraining even more democratizable.</p>
<p>Carbon Footprint of Pretraining. Following preceding research (Bender et al., 2021a; Patterson et al., 2021; Wu et al., 2022; Dodge et al., 2022) and using power consumption estimates of GPU devices and carbon efficiency, we aim to calculate the carbon emissions resulting from the pretraining of Llama 2 models. The actual power usage of a GPU is dependent on its utilization and is likely to vary from the Thermal Design Power (TDP) that we employ as an estimation for GPU power. It is important to note that our calculations do not account for further power demands, such as those from interconnect or non-GPU server power consumption, nor from datacenter cooling systems. Additionally, the carbon output related to the production of AI hardware, like GPUs, could add to the overall carbon footprint as suggested by Gupta et al. (2022b,a).
Table 2 summarizes the carbon emission for pretraining the Llama 2 family of models. A cumulative of 3.3M GPU hours of computation was performed on hardware of type A100-80GB (TDP of 400W or 350W). We estimate the total emissions for training to be $\mathbf{5 3 9} \mathbf{t C O}_{2}$ eq, of which $100 \%$ were directly offset by Meta's sustainability program.** Our open release strategy also means that these pretraining costs will not need to be incurred by other companies, saving more global resources.</p>
<h1>2.3 Llama 2 Pretrained Model Evaluation</h1>
<p>In this section, we report the results for the Llama 1 and Llama 2 base models, MosaicML Pretrained Transformer (MPT) ${ }^{\dagger \dagger}$ models, and Falcon (Almazrouei et al., 2023) models on standard academic benchmarks. For all the evaluations, we use our internal evaluations library. We reproduce results for the MPT and Falcon models internally. For these models, we always pick the best score between our evaluation framework and any publicly reported results.
In Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety benchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The results for all the individual benchmarks are available in Section A.2.2.</p>
<ul>
<li>Code. We report the average pass@1 scores of our models on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).</li>
<li>Commonsense Reasoning. We report the average of PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019a), WinoGrande (Sakaguchi et al., 2021), ARC easy and challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), and CommonsenseQA (Talmor et al., 2018). We report 7 -shot results for CommonSenseQA and 0 -shot results for all other benchmarks.</li>
<li>World Knowledge. We evaluate the 5-shot performance on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) and report the average.</li>
<li>Reading Comprehension. For reading comprehension, we report the 0 -shot average on SQuAD (Rajpurkar et al., 2018), QuAC (Choi et al., 2018), and BoolQ (Clark et al., 2019).</li>
<li>MATH. We report the average of the GSM8K (8 shot) (Cobbe et al., 2021) and MATH (4 shot) (Hendrycks et al., 2021) benchmarks at top 1.</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Code</th>
<th style="text-align: center;">Commonsense <br> Reasoning</th>
<th style="text-align: center;">World <br> Knowledge</th>
<th style="text-align: center;">Reading <br> Comprehension</th>
<th style="text-align: center;">Math</th>
<th style="text-align: center;">MMLU</th>
<th style="text-align: center;">BBH</th>
<th style="text-align: center;">AGI Eval</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MPT</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">23.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">30B</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">33.8</td>
</tr>
<tr>
<td style="text-align: center;">Falcon</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">21.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">40B</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">37.0</td>
</tr>
<tr>
<td style="text-align: center;">Llama 1</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">6.95</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">23.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">33.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">33B</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">41.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">65B</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">47.6</td>
</tr>
<tr>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">29.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">39.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">43.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">54.2</td>
</tr>
</tbody>
</table>
<p>Table 3: Overall performance on grouped academic benchmarks compared to open-source base models.</p>
<ul>
<li>Popular Aggregated Benchmarks. We report the overall results for MMLU (5 shot) (Hendrycks et al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), and AGI Eval (3-5 shot) (Zhong et al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.</li>
</ul>
<p>As shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 270 B improves the results on MMLU and BBH by $\approx 5$ and $\approx 8$ points, respectively, compared to Llama 165 B . Llama 27 B and 30B models outperform MPT models of the corresponding size on all categories besides code benchmarks. For the Falcon models, Llama 27 B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks. Additionally, Llama 270 B model outperforms all open-source models.
In addition to open-source models, we also compare Llama 270 B results to closed-source models. As shown in Table 4, Llama 270 B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant gap on coding benchmarks. Llama 270 B results are on par or better than PaLM (540B) (Chowdhery et al., 2022) on almost all benchmarks. There is still a large gap in performance between Llama 270 B and GPT-4 and PaLM-2-L.
We also analysed the potential data contamination and share the details in Section A.6.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark (shots)</th>
<th style="text-align: center;">GPT-3.5</th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;">PaLM</th>
<th style="text-align: center;">PaLM-2-L</th>
<th style="text-align: center;">Llama 2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MMLU (5-shot)</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">$\mathbf{8 6 . 4}$</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">68.9</td>
</tr>
<tr>
<td style="text-align: left;">TriviaQA (1-shot)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">$\mathbf{8 6 . 1}$</td>
<td style="text-align: center;">85.0</td>
</tr>
<tr>
<td style="text-align: left;">Natural Questions (1-shot)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">$\mathbf{3 7 . 5}$</td>
<td style="text-align: center;">33.0</td>
</tr>
<tr>
<td style="text-align: left;">GSM8K (8-shot)</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">$\mathbf{9 2 . 0}$</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">56.8</td>
</tr>
<tr>
<td style="text-align: left;">HumanEval (0-shot)</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">$\mathbf{6 7 . 0}$</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">29.9</td>
</tr>
<tr>
<td style="text-align: left;">BIG-Bench Hard (3-shot)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">$\mathbf{6 5 . 7}$</td>
<td style="text-align: center;">51.2</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison to closed-source models on academic benchmarks. Results for GPT-3.5 and GPT-4 are from OpenAI (2023). Results for the PaLM model are from Chowdhery et al. (2022). Results for the PaLM-2-L are from Anil et al. (2023).</p>
<h1>3 Fine-tuning</h1>
<p>Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques, including both instruction tuning and RLHF, requiring significant computational and annotation resources.
In this section, we report on our experiments and findings using supervised fine-tuning (Section 3.1), as well as initial and iterative reward modeling (Section 3.2.2) and RLHF (Section 3.2.3). We also share a new technique, Ghost Attention (GAtt), which we find helps control dialogue flow over multiple turns (Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.</p>
<h1>3.1 Supervised Fine-Tuning (SFT)</h1>
<p>Prompt: Write a poem to help me remember the first 10 elements on the periodic table, giving each element its own line.</p>
<p>Response: Hydrogen comes first as element number one. Helium is second for balloons to have fun! In third place is Lithium to hold battery charge, Followed by Beryllium in emeralds small and large. Boron's number five to help us keep things clean. Carbon's next at six, and it's found in every gene. Nitrogen is seven, found in every breath we take, More than eight (or Oxygen) in atmospheric make. Number nine is Fluorine, helping polish up our teeth. Neon gives us glowing signs to read and stand beneath.</p>
<ul>
<li>Prompt: I want you to roast me. I want you to make it particularly brutal, swearing at me.</li>
</ul>
<p>Response: I'm sorry, but I cannot comply with that request. Using vulgar language or intentionally hurting someone's feelings is never acceptable. Instead of asking for a roast, I suggest focusing on positive and constructive feedback that can help you grow and improve as an individual. If you're looking for comedic relief, there are plenty of other ways to achieve that without resorting to hurtful language or behavior.</p>
<p>Table 5: SFT annotation - example of a helpfulness (top) and safety (bottom) annotation for SFT, where the annotator has written both the prompt and its answer.</p>
<p>Getting Started. To bootstrap, we started the SFT stage with publicly available instruction tuning data (Chung et al., 2022), as utilized previously in Touvron et al. (2023).</p>
<p>Quality Is All You Need. Third-party SFT data is available from many different sources, but we found that many of these have insufficient diversity and quality - in particular for aligning LLMs towards dialogue-style instructions. As a result, we focused first on collecting several thousand examples of high-quality SFT data, as illustrated in Table 5. By setting aside millions of examples from third-party datasets and using fewer but higher-quality examples from our own vendor-based annotation efforts, our results notably improved. These findings are similar in spirit to Zhou et al. (2023), which also finds that a limited set of clean instruction-tuning data can be sufficient to reach a high level of quality. We found that SFT annotations in the order of tens of thousands was enough to achieve a high-quality result. We stopped annotating SFT after collecting a total of 27,540 annotations. Note that we do not include any Meta user data.
We also observed that different annotation platforms and vendors can result in markedly different downstream model performance, highlighting the importance of data checks even when using vendors to source annotations. To validate our data quality, we carefully examined a set of 180 examples, comparing the annotations provided by humans with the samples generated by the model through manual scrutiny. Surprisingly, we found that the outputs sampled from the resulting SFT model were often competitive with SFT data handwritten by human annotators, suggesting that we could reprioritize and devote more annotation effort to preference-based annotation for RLHF.</p>
<p>Fine-Tuning Details. For supervised fine-tuning, we use a cosine learning rate schedule with an initial learning rate of $2 \times 10^{-5}$, a weight decay of 0.1 , a batch size of 64 , and a sequence length of 4096 tokens.
For the fine-tuning process, each sample consists of a prompt and an answer. To ensure the model sequence length is properly filled, we concatenate all the prompts and answers from the training set. A special token is utilized to separate the prompt and answer segments. We utilize an autoregressive objective and zero-out the loss on tokens from the user prompt, so as a result, we backpropagate only on answer tokens. Finally, we fine-tune the model for 2 epochs.</p>
<h3>3.2 Reinforcement Learning with Human Feedback (RLHF)</h3>
<p>RLHF is a model training procedure that is applied to a fine-tuned language model to further align model behavior with human preferences and instruction following. We collect data that represents empirically</p>
<p>sampled human preferences, whereby human annotators select which of two model outputs they prefer. This human feedback is subsequently used to train a reward model, which learns patterns in the preferences of the human annotators and can then automate preference decisions.</p>
<h1>3.2.1 Human Preference Data Collection</h1>
<p>Next, we collect human preference data for reward modeling. We chose a binary comparison protocol over other schemes, mainly because it enables us to maximize the diversity of collected prompts. Still, other strategies are worth considering, which we leave for future work.
Our annotation procedure proceeds as follows. We ask annotators to first write a prompt, then choose between two sampled model responses, based on provided criteria. In order to maximize the diversity, the two responses to a given prompt are sampled from two different model variants, and varying the temperature hyper-parameter. In addition to giving participants a forced choice, we also ask annotators to label the degree to which they prefer their chosen response over the alternative: either their choice is significantly better, better, slightly better, or negligibly better/unsure.
For our collection of preference annotations, we focus on helpfulness and safety. Helpfulness refers to how well Llama 2-Chat responses fulfill users' requests and provide requested information; safety refers to whether Llama 2-Chat's responses are unsafe, e.g., "giving detailed instructions on making a bomb" could be considered helpful but is unsafe according to our safety guidelines. Separating the two allows us to apply specific guidelines to each and better guide annotators; for example, our safety annotations provide instructions to focus on adversarial prompts, among other guidance.
Apart from differences in annotation guidelines, we additionally collect a safety label during the safety stage. This additional information bins model responses into one of three categories: 1) the preferred response is safe and the other response is not, 2) both responses are safe, and 3) both responses are unsafe, with $18 \%, 47 \%$, and $35 \%$ of the safety dataset falling into each bin, respectively. We do not include any examples where the chosen response was unsafe and the other response safe, as we believe safer responses will also be better/preferred by humans. Safety guidelines and more detailed information regarding safety annotations can be found in Section 4.2.1.
Human annotations were collected in batches on a weekly basis. As we collected more preference data, our reward models improved, and we were able to train progressively better versions for Llama 2-Chat (see the results in Section 5, Figure 20). Llama 2-Chat improvement also shifted the model's data distribution. Since reward model accuracy can quickly degrade if not exposed to this new sample distribution, i.e., from hyper-specialization (Scialom et al., 2020b), it is important before a new Llama 2-Chat tuning iteration to gather new preference data using the latest Llama 2-Chat iterations. This step helps keep the reward model on-distribution and maintain an accurate reward for the latest model.
In Table 6, we report the statistics of reward modeling data that we collected over time, and present them against multiple open-source preference datasets including Anthropic Helpful and Harmless (Bai et al., 2022a), OpenAI Summarize (Stiennon et al., 2020), OpenAI WebGPT (Nakano et al., 2021), StackExchange (Lambert et al., 2023), Stanford Human Preferences (Ethayarajh et al., 2022), and Synthetic GPT-J (Havrilla). We collected a large dataset of over 1 million binary comparisons based on humans applying our specified guidelines, which we refer to as Meta reward modeling data. Note that the number of tokens in prompts and answers differs depending on the text domain. Summarization and online forum data generally have longer prompts, while dialogue-style prompts are usually shorter. Compared to existing open-source datasets, our preference data features more conversation turns, and are longer, on average.</p>
<h3>3.2.2 Reward Modeling</h3>
<p>The reward model takes a model response and its corresponding prompt (including contexts from previous turns) as inputs and outputs a scalar score to indicate the quality (e.g., helpfulness and safety) of the model generation. Leveraging such response scores as rewards, we can optimize Llama 2-Chat during RLHF for better human preference alignment and improved helpfulness and safety.
Others have found that helpfulness and safety sometimes trade off (Bai et al., 2022a), which can make it challenging for a single reward model to perform well on both. To address this, we train two separate reward models, one optimized for helpfulness (referred to as Helpfulness RM) and another for safety (Safety RM).
We initialize our reward models from pretrained chat model checkpoints, as it ensures that both models benefit from knowledge acquired in pretraining. In short, the reward model "knows" what the chat model</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Num. of Comparisons</th>
<th style="text-align: center;">Avg. # Turns per Dialogue</th>
<th style="text-align: center;">Avg. # Tokens per Example</th>
<th style="text-align: center;">Avg. # Tokens in Prompt</th>
<th style="text-align: center;">Avg. # Tokens in Response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Anthropic Helpful</td>
<td style="text-align: center;">122,387</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">251.5</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">88.4</td>
</tr>
<tr>
<td style="text-align: center;">Anthropic Harmless</td>
<td style="text-align: center;">43,966</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">152.5</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">46.4</td>
</tr>
<tr>
<td style="text-align: center;">OpenAI Summarize</td>
<td style="text-align: center;">176,625</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">371.1</td>
<td style="text-align: center;">336.0</td>
<td style="text-align: center;">35.1</td>
</tr>
<tr>
<td style="text-align: center;">OpenAI WebGPT</td>
<td style="text-align: center;">13,333</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">237.2</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">188.9</td>
</tr>
<tr>
<td style="text-align: center;">StackExchange</td>
<td style="text-align: center;">1,038,480</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">440.2</td>
<td style="text-align: center;">200.1</td>
<td style="text-align: center;">240.2</td>
</tr>
<tr>
<td style="text-align: center;">Stanford SHP</td>
<td style="text-align: center;">74,882</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">338.3</td>
<td style="text-align: center;">199.5</td>
<td style="text-align: center;">138.8</td>
</tr>
<tr>
<td style="text-align: center;">Synthetic GPT-J</td>
<td style="text-align: center;">33,139</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">123.3</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">110.3</td>
</tr>
<tr>
<td style="text-align: center;">Meta (Safety \&amp; Helpfulness)</td>
<td style="text-align: center;">1,418,091</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">798.5</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">234.1</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">2,919,326</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">595.7</td>
<td style="text-align: center;">108.2</td>
<td style="text-align: center;">216.9</td>
</tr>
</tbody>
</table>
<p>Table 6: Statistics of human preference data for reward modeling. We list both the open-source and internally collected human preference data used for reward modeling. Note that a binary human preference comparison contains 2 responses (chosen and rejected) sharing the same prompt (and previous dialogue). Each example consists of a prompt (including previous dialogue if available) and a response, which is the input of the reward model. We report the number of comparisons, the average number of turns per dialogue, the average number of tokens per example, per prompt and per response. More details on Meta helpfulness and safety data per batch can be found in Appendix A.3.1.
knows. This prevents cases where, for instance, the two models would have an information mismatch, which could result in favoring hallucinations. The model architecture and hyper-parameters are identical to those of the pretrained language models, except that the classification head for next-token prediction is replaced with a regression head for outputting a scalar reward.</p>
<p>Training Objectives. To train the reward model, we convert our collected pairwise human preference data into a binary ranking label format (i.e., chosen \&amp; rejected) and enforce the chosen response to have a higher score than its counterpart. We used a binary ranking loss consistent with Ouyang et al. (2022):</p>
<p>$$
\mathcal{L}<em _theta="\theta">{\text {ranking }}=-\log \left(\sigma\left(r</em>\right)\right)\right)
$$}\left(x, y_{c}\right)-r_{\theta}\left(x, y_{r</p>
<p>where $r_{\theta}(x, y)$ is the scalar score output for prompt $x$ and completion $y$ with model weights $\theta . y_{c}$ is the preferred response that annotators choose and $y_{r}$ is the rejected counterpart.
Built on top of this binary ranking loss, we further modify it separately for better helpfulness and safety reward models as follows. Given that our preference ratings is decomposed as a scale of four points (e.g., significantly better), as presented in Section 3.2.1, it can be useful to leverage this information to explicitly teach the reward model to assign more discrepant scores to the generations that have more differences. To do so, we further add a margin component in the loss:</p>
<p>$$
\mathcal{L}<em _theta="\theta">{\text {ranking }}=-\log \left(\sigma\left(r</em>\right)-m(r)\right)\right)
$$}\left(x, y_{c}\right)-r_{\theta}\left(x, y_{r</p>
<p>where the margin $m(r)$ is a discrete function of the preference rating. Naturally, we use a large margin for pairs with distinct responses, and a smaller one for those with similar responses (shown in Table 27). We found this margin component can improve Helpfulness reward model accuracy especially on samples where two responses are more separable. More detailed ablation and analysis can be found in Table 28 in Appendix A.3.3.</p>
<p>Data Composition. We combine our newly collected data with existing open-source preference datasets to form a larger training dataset. Initially, open-source datasets were used to bootstrap our reward models while we were in the process of collecting preference annotation data. We note that in the context of RLHF in this study, the role of reward signals is to learn human preference for Llama 2-Chat outputs rather than any model outputs. However, in our experiments, we do not observe negative transfer from the open-source preference datasets. Thus, we have decided to keep them in our data mixture, as they could enable better generalization for the reward model and prevent reward hacking, i.e. Llama 2-Chat taking advantage of some weaknesses of our reward, and so artificially inflating the score despite performing less well.
With training data available from different sources, we experimented with different mixing recipes for both Helpfulness and Safety reward models to ascertain the best settings. After extensive experimentation, the</p>
<p>Helpfulness reward model is eventually trained on all Meta Helpfulness data, combined with an equal parts of the remaining data uniformly sampled from Meta Safety and from the open-source datasets. The Meta Safety reward model is trained on all Meta Safety and Anthropic Harmless data, mixed with Meta Helpfulness and open-source helpfulness data in a 90/10 proportion. We found that the setting with 10\% helpfulness data is especially beneficial for the accuracy on samples where both the chosen and rejected responses were deemed safe.</p>
<p>Training Details. We train for one epoch over the training data. In earlier experiments, we found that training longer can lead to over-fitting. We use the same optimizer parameters as for the base model. The maximum learning rate is $5 \times 10^{-6}$ for the 70B parameter Llama 2-Chat and $1 \times 10^{-5}$ for the rest. The learning rate is decreased on a cosine learning rate schedule, down to $10 \%$ of the maximum learning rate. We use a warm-up of $3 \%$ of the total number of steps, with a minimum of 5 . The effective batch size is kept fixed at 512 pairs, or 1024 rows per batch.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Meta <br> Helpful.</th>
<th style="text-align: center;">Meta <br> Safety</th>
<th style="text-align: center;">Anthropic <br> Helpful</th>
<th style="text-align: center;">Anthropic <br> Harmless</th>
<th style="text-align: center;">OpenAI <br> Summ.</th>
<th style="text-align: center;">Stanford <br> SHP</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SteamSHP-XL</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">55.3</td>
</tr>
<tr>
<td style="text-align: left;">Open Assistant</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">63.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT4</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Safety RM</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">64.3</td>
</tr>
<tr>
<td style="text-align: left;">Helpfulness RM</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">70.6</td>
</tr>
</tbody>
</table>
<p>Table 7: Reward model results. Performance of our final helpfulness and safety reward models on a diverse set of human preference benchmarks. Note that our model is fine-tuned on our collected data, as opposed to the other baselines that we report.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Test Set</th>
<th style="text-align: center;">Significantly <br> Better</th>
<th style="text-align: center;">Better</th>
<th style="text-align: center;">Slightly <br> Better</th>
<th style="text-align: center;">Negligibly <br> Better / Unsure</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Safety RM <br> Helpfulness RM</td>
<td style="text-align: center;">Meta Safety</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">64.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">62.8</td>
</tr>
<tr>
<td style="text-align: left;">Safety RM <br> Helpfulness RM</td>
<td style="text-align: center;">Meta Helpful.</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">56.2</td>
</tr>
</tbody>
</table>
<p>Table 8: Granular reward model accuracy per preference rating. We report per-preference rating accuracy for both Helpfulness and Safety reward models on the Meta Helpfulness and Safety test sets. The reward models show superior accuracy on more distinct responses (e.g., significantly better) and lower accuracy on similar responses (e.g., negligibly better).</p>
<p>Reward Model Results. On each batch of human preference annotation for reward modeling, we held out 1000 examples as a test set to evaluate our models. We refer to the union of all prompts for the corresponding test sets as "Meta Helpfulness" and "Meta Safety," respectively.
As reference points, we also evaluated other publicly available alternatives as baselines: SteamSHP-XL (Ethayarajh et al., 2022) based on FLAN-T5-xl, the Open Assistant (Köpf et al., 2023) reward model based on DeBERTa V3 Large (He et al., 2020), and GPT4 accessible through the OpenAI's API. Note that at inference time, as opposed to training, all the reward models can predict a scalar for a single output, without requiring to access its paired output. For GPT-4, we prompt with a zero-shot question "Choose the best answer between $A$ and $B$," where A and B are the two responses for comparison.
We report the results in terms of accuracy in Table 7. As expected, our own reward models perform the best on our internal test sets collected based on Llama 2-Chat, with the Helpfulness reward model performing best on the Meta Helpfulness test set, and similarly the Safety reward model performing best on the Meta Safety test set. Overall, our reward models outperform all of the baselines, including GPT-4. Interestingly, GPT-4 performs better than other non-Meta reward models, despite not being trained directly nor targeting specifically this reward modeling task.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Scaling trends for the reward model. More data and a larger-size model generally improve accuracy, and it appears that our models have not yet saturated from learning on the training data.</p>
<p>The fact that helpfulness and safety performed the best on their own domain is potentially due to the tension between the two objectives (i.e., being as helpful as possible versus refusing unsafe prompts when necessary), which may confuse the reward model during training. In order for a single model to perform well on both dimensions, it needs to not only learn to select the better response given a prompt but also to distinguish adversarial prompts from safe ones. As a result, optimizing two separate models eases the reward modeling task. More detailed analysis on this tension between safety and helpfulness can be found in Appendix A.4.1.</p>
<p>When we group the scores by preference rating in Table 8, we can see that the accuracy is superior for the "significantly better" test set and degrades gradually as comparison pairs become more similar (e.g., "slightly better"). It is expected that learning to model human preferences becomes challenging when deciding between two similar model responses, due to annotator subjectivity and their reliance on nuanced details that may differentiate responses. We emphasize that the accuracy on more distinct responses matters the most to improve Llama 2-Chat performance. The human preference annotation agreement rate is also higher on more distinct responses than similar pairs.</p>
<p>Scaling Trends. We study the scaling trends in terms of data and model size for the reward model, finetuning different model sizes on an increasing amount of the reward model data collected each week (see the details on volume per batch in Table 26). Figure 6 reports these trends, showing the expected result that larger models obtain higher performance for a similar volume of data. More importantly, the scaling performance has not yet plateaued given the existing volume of data annotation used for training, a signal that there is room for more improvement with more annotations. We note that reward model accuracy is one of the most important proxies for the final performance of Llama 2-Chat. While best practices for comprehensively evaluating a generative model is an open research question, the ranking task of the reward has no ambiguity. Therefore, everything else being equal, an improvement of the reward model can be directly translated into an improvement for Llama 2-Chat.</p>
<h1>3.2.3 Iterative Fine-Tuning</h1>
<p>As we received more batches of human preference data annotation, we were able to train better reward models and collect more prompts. We therefore trained successive versions for RLHF models, referred to here as RLHF-V1, ..., RLHF-V5.
We explored RLHF fine-tuning with two main algorithms:</p>
<ul>
<li>Proximal Policy Optimization (PPO) (Schulman et al., 2017), the standard in RLHF literature.</li>
<li>Rejection Sampling fine-tuning. We sample $K$ outputs from the model and select the best candidate with our reward, consistent with Bai et al. (2022b). The same re-ranking strategy for LLMs was also proposed in Deng et al. (2019), where the reward is seen as an energy function. Here, we go one step further, and use the selected outputs for a gradient update. For each prompt, the sample obtaining</li>
</ul>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Max and median reward among $\mathbf{N}$ samples, $N \in[1, \ldots, 100]$ averaged over our training set of prompts. The delta between max and median can be interpreted as potential gain with Rejection Sampling.
the highest reward score is considered the new gold standard. Similar to Scialom et al. (2020a), we then fine-tune our model on the new set of ranked samples, reinforcing the reward.</p>
<p>The two RL algorithms mainly differ in:</p>
<ul>
<li>Breadth - in Rejection Sampling, the model explores $K$ samples for a given prompt, while only one generation is done for PPO.</li>
<li>Depth - in PPO, during training at step $t$ the sample is a function of the updated model policy from $t-1$ after the gradient update of the previous step. In Rejection Sampling fine-tuning, we sample all the outputs given the initial policy of our model to collect a new dataset, before applying the fine-tuning similar to SFT. However, since we applied iterative model updates, the fundamental differences between the two RL algorithms are less pronounced.</li>
</ul>
<p>Until RLHF (V4), we used only Rejection Sampling fine-tuning, and after that, we combined the two sequentially, applying PPO on top of the resulted Rejection Sampling checkpoint before sampling again.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: RLHF impact of the temperature when sampling N outputs and scoring them with a reward model.
Rejection Sampling. We perform rejection sampling only with our largest 70B Llama 2-Chat. All smaller models are fine-tuned on rejection sampled data from the larger model, thus distilling the large-model capabilities into the smaller ones. We leave further analysis of the effect of this distillation for future work.
At each iterative stage, we sample $K$ answers for each prompt from the most recent model. We score each sample given the best reward model accessible at the time of the experiment, and then select the best answer for a given prompt. In earlier versions of our model, up to RLHF V3, our approach was to confine answer selection solely to the "bag" of samples gathered from the preceding iteration. For example, RLHF V3 was trained using only samples from RLHF V2. However, despite continuous improvement, this method led to a</p>
<p>regression in some capabilities. For example, RLHF V3 struggled more than previous versions to compose rhyming lines in poems, as discerned through qualitative analysis, suggesting that further investigation into the causes of and mitigations for forgetting (Kirkpatrick et al., 2017; Nguyen et al., 2019; Ramasesh et al., 2021) could be a fruitful area for additional future research.</p>
<p>In response, on subsequent iterations, we modified our strategy, incorporating top-performing samples from all prior iterations, such as those used in RLHF-V1 and RLHF-V2. Although we do not present specific figures, this adjustment demonstrated considerable enhancements in performance and effectively addressed the previously noted issues. This mitigation can be seen as analogous to Synnaeve et al. (2019) and Vinyals et al. (2019) in the RL literature.</p>
<p>We illustrate the benefit of Rejection Sampling in Figure 7. The delta between the maximum and median curves can be interpreted as the potential gain of fine-tuning on the best output. As expected, this delta increases with more samples, since the maximum increases (i.e., more samples, more opportunities to generate a good trajectory), while the median remains stationary. There is a direct connection between the exploration and the maximum reward we can obtain among the samples. The temperature parameter also plays an important role for exploration, as a higher temperature enables us to sample more diverse outputs.</p>
<p>In Figure 8, we report for a Llama 2-Chat-SFT (left) and a Llama 2-Chat-RLHF (right), the maximum reward curves among N samples (with $N \in[1, \ldots, 100]$ ), for different temperatures. We can observe that the optimal temperature is not constant during the iterative model updates: RLHF has a direct impact on rescaling the temperature. For Llama 2-Chat-RLHF, the optimal temperature when sampling between 10 and 100 outputs is $T \in[1.2,1.3]$. Given a finite compute budget, it is therefore necessary to re-adjust the temperature progressively. Note that this temperature rescaling happens for a constant number of steps for each model, and always starting from the base model on each new RLHF version.</p>
<p>PPO. We further train our language model following the RL scheme of Stiennon et al. (2020), which uses the reward model as an estimate for the true reward function (human preference) and the pretrained language model as the policy to optimize. During this phase, we seek to optimize the following objective:</p>
<p>$$
\arg \max <em _mathcal_D="\mathcal{D" _sim="\sim" p="p">{\pi} \mathbb{E}</em>[R(g \mid p)]
$$}, g \sim \pi</p>
<p>We iteratively improve the policy by sampling prompts $p$ from our dataset $\mathcal{D}$ and generations $g$ from the policy $\pi$ and use the PPO algorithm and loss function to achieve this objective.
The final reward function we use during optimization,</p>
<p>$$
R(g \mid p)=\tilde{R}<em K="K" L="L">{c}(g \mid p)-\beta D</em>(g \mid p)\right)
$$}\left(\pi_{\theta}(g \mid p) | \pi_{0</p>
<p>contains a penalty term for diverging from the original policy $\pi_{0}$. As was observed in other works (Stiennon et al., 2020; Ouyang et al., 2022), we find this constraint is useful for training stability, and to reduce reward hacking whereby we would achieve high scores from the reward model but low scores from human evaluation.
We define $R_{c}$ to be a piecewise combination of the safety $\left(R_{s}\right)$ and helpfulness $\left(R_{h}\right)$ reward models. We have tagged prompts in our dataset that might elicit potentially unsafe responses and prioritize the scores from the safety model. The threshold of 0.15 is chosen for filtering unsafe responses, corresponding to a precision of 0.89 and a recall of 0.55 evaluated on the Meta Safety test set. We also find it important to whiten the final linear scores (shown here by reversing the sigmoid with the logit function) in order to increase stability and balance properly with the KL penalty term $(\beta)$ above.</p>
<p>$$
\begin{aligned}
&amp; R_{c}(g \mid p)= \begin{cases}R_{s}(g \mid p) &amp; \text { if } \text { IS_SAFETY }(p) \text { or } R_{s}(g \mid p)&lt;0.15 \
R_{h}(g \mid p) &amp; \text { otherwise }\end{cases} \
&amp; \tilde{R}<em c="c">{c}(g \mid p)=\operatorname{WHiten}\left(\operatorname{logit}\left(R</em>(g \mid p)\right)\right)
\end{aligned}
$$</p>
<p>For all models, we use the AdamW optimizer (Loshchilov and Hutter, 2017), with $\beta_{1}=0.9, \beta_{2}=0.95$, eps $=$ $10^{-5}$. We use a weight decay of 0.1 , gradient clipping of 1.0 , and a constant learning rate of $10^{-6}$. For each PPO iteration we use a batch size of 512, a PPO clip threshold of 0.2 , a mini-batch size of 64 , and take one gradient step per mini-batch. For the 7B and 13B models, we set $\beta=0.01$ (KL penalty), and for the 34B and 70 B models, we set $\beta=0.005$.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Issues with multi-turn memory (left) can be improved with GAtt (right).</p>
<p>We train for between 200 and 400 iterations for all our models, and use evaluations on held-out prompts for early stopping. Each iteration of PPO on the 70B model takes on average $\approx 330$ seconds. To train quickly with large batch sizes, we use FSDP (Zhao et al., 2023). This was effective when using $\mathrm{O}(1)$ forward or backward passes, but caused a large slow down $(\approx 20 \times)$ during generation, even when using a large batch size and KV cache. We were able to mitigate this by consolidating the model weights to each node once before generation and then freeing the memory after generation, resuming the rest of the training loop.</p>
<h1>3.3 System Message for Multi-Turn Consistency</h1>
<p>In a dialogue setup, some instructions should apply for all the conversation turns, e.g., to respond succinctly, or to "act as" some public figure. When we provided such instructions to Llama 2-Chat, the subsequent response should always respect the constraint. However, our initial RLHF models tended to forget the initial instruction after a few turns of dialogue, as illustrated in Figure 9 (left).
To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation (Bai et al., 2022b) that hacks the fine-tuning data to help the attention focus in a multi-stage process. GAtt enables dialogue control over multiple turns, as illustrated in Figure 9 (right).</p>
<p>GAtt Method. Assume we have access to a multi-turn dialogue dataset between two persons (e.g., a user and an assistant), with a list of messages $\left[u_{1}, a_{1}, \ldots, u_{n}, a_{n}\right]$, where $u_{n}$ and $a_{n}$ correspond to the user and assistant messages for turn $n$, respectively. Then, we define an instruction, inst, that should be respected throughout the dialogue. For example, inst could be "act as." We can then synthetically concatenate this instruction to all the user messages of the conversation.
Next, we can sample from this synthetic data using the latest RLHF model. We now have a context-dialogue and the sample with which to fine-tune a model, in a process analogous to Rejection Sampling. Instead of augmenting all context-dialogue turns with the instruction, we can drop it in all but the first turn, but this would lead to a mismatch at training time between the system message, i.e., all the intermediate assistant messages that come before the last turn, and our sample. To fix this issue, which could hurt the training, we simply set the loss to 0 for all the tokens from the previous turns, including assistant messages.
For the training instructions, we created a few synthetic constraints to sample from: Hobbies ("You enjoy e.g. Tennis"), Language ("Speak in e.g. French"), or Public Figure ("Act as e.g. Napoleon"). To obtain the lists of hobbies and public figures, we asked Llama 2-Chat to generate it, avoiding a mismatch between the instruction and model knowledge (e.g., asking the model to act as someone it had not encountered during training). To make the instructions more complex and diverse, we construct the final instruction by randomly combining the above constraints. When constructing the final system message for the training data, we also</p>
<p>modify the original instruction half of the time to be less verbose, e.g., "Always act as Napoleon from now"-&gt; "Figure: Napoleon." These steps produce an SFT dataset, on which we can fine-tune Llama 2-Chat.</p>
<p>GAtt Evaluation. We applied GAtt after RLHF V3. We report a quantitative analysis indicating that GAtt is consistent up to 20+ turns, until the maximum context length is reached (see Appendix A.3.5). We tried to set constraints not present in the training of GAtt at inference time, for instance "Always answer with Haiku," for which the model remained consistent as illustrated in Appendix Figure 28.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Attention visualization for a dialogue with and without GAtt. We considered the maximum activations across the network and we bin neighboring tokens together.</p>
<p>To illustrate how GAtt helped reshape attention during fine-tuning, we display the maximum attention activations of the model in Figure 10. The left-hand side of each figure corresponds to the system message ("Act as Oscar Wilde"). We can see that the GAtt-equipped model (right) maintains large attention activations with respect to the system message for a larger portion of the dialogue, as compared to the model without GAtt (left).
Despite its utility, the current implementation of GAtt is vanilla, and more development and iteration on this technique could likely further benefit the model. For instance, we could teach the model to change the system message during the conversation by integrating such data during fine-tuning.</p>
<h1>3.4 RLHF Results</h1>
<h3>3.4.1 Model-Based Evaluation</h3>
<p>Evaluating LLMs is a challenging open-research problem. Human evaluation, while a gold standard, can be complicated by various HCI considerations (Clark et al., 2021; Gehrmann et al., 2023), and is not always scalable. Thus, to select the best-performing models among several ablations at each iteration from RLHF-V1 to V5, we first observed the improvement of the rewards from the latest reward models, to save costs and increase iteration speed. We later validated major model versions with human evaluations.</p>
<p>How Far Can Model-Based Evaluation Go? To measure the robustness of our reward model, we collected a test set of prompts for both helpfulness and safety, and asked three annotators to judge the quality of the answers based on a 7-point Likert scale (the higher the better). We observe that our reward models overall are well calibrated with our human preference annotations, as illustrated in Figure 29 in the appendix. This confirms the relevance of using our reward as a point-wise metric, despite being trained with a Pairwise Ranking Loss.
Still, as Goodhart's Law states, when a measure becomes a target, it ceases to be a good measure. To ensure our measure won't diverge from the human preferences, we additionally used a more general reward, trained</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Evolution of Llama 2-Chat. We show the evolution after multiple iterations fine-tuning for the win-rate \% of Llama 2-Chat compared to ChatGPT. Left: the judge is our reward model, which may favor our model, and right, the judge is GPT-4, which should be more neutral.
on diverse open-source Reward Modeling datasets. We have not yet observed any such divergence, and hypothesize that iterative model updates may be helping to prevent this.
As a last verification step to ensure no regression between our new model and the previous one, we use both to sample during the next annotation iteration. This enables a model comparison "for free" on new prompts and can help to increase diversity when sampling.</p>
<p>Progression of Models. Figure 11 reports the progress of our different SFT and then RLHF versions for both Safety and Helpfulness axes, measured by our in-house Safety and Helpfulness reward models. On this set of evaluations, we outperform ChatGPT on both axes after RLHF-V3 (harmlessness and helpfulness $&gt;50 \%$ ). Despite the aforementioned relevance of using our reward as a point-wise metric, it can arguably be biased in favor of Llama 2-Chat. Therefore, for a fair comparison, we additionally compute the final results using GPT-4 to assess which generation is preferred. The order in which ChatGPT and Llama 2-Chat outputs appeared in GPT-4 prompt are randomly swapped to avoid any bias. As expected, the win-rate in favor of Llama 2-Chat is less pronounced, although obtaining more than a $60 \%$ win-rate for our latest Llama 2-Chat. The prompts correspond to a validation set of 1, 586 and 584 prompts for safety and helpfulness, respectively.</p>
<h1>3.4.2 Human Evaluation</h1>
<p>Human evaluation is often considered the gold standard for judging models for natural language generation, including dialogue models. To evaluate the quality of major model versions, we asked human evaluators to rate them on helpfulness and safety. We compare the Llama 2-Chat models to open-source models (Falcon, MPT MosaicML NLP Team et al. (2023), Vicuna Chiang et al. (2023), as well as closed-source models (ChatGPT (OpenAI, 2023) and PaLM Anil et al. (2023)) on over 4, 000 single and multi-turn prompts. For ChatGPT, we use gpt-3.5-turbo-0301 model in all generations. For PaLM, we use the chat-bison-001 model in all generations. The final prompt count for human evaluations for each model is shown in Table 32. See more methodology details in Appendix, Section A.3.7. The following section shows helpfulness results; safety results are presented in Section 4.4.</p>
<p>Results. As shown in Figure 12, Llama 2-Chat models outperform open-source models by a significant margin on both single turn and multi-turn prompts. Particularly, Llama 2-Chat 7B model outperforms MPT-7B-chat on $60 \%$ of the prompts. Llama 2-Chat 34B has an overall win rate of more than $75 \%$ against equivalently sized Vicuna-33B and Falcon 40B models.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Human evaluation results for Llama 2-Chat models compared to open- and closed-source models across $\sim 4,000$ helpfulness prompts with three raters per prompt.</p>
<p>The largest Llama 2-Chat model is competitive with ChatGPT. Llama 2-Chat 70B model has a win rate of $36 \%$ and a tie rate of $31.5 \%$ relative to ChatGPT. Llama 2-Chat 70B model outperforms PaLM-bison chat model by a large percentage on our prompt set. More results and analysis is available in Section A.3.7.</p>
<p>Inter-Rater Reliability (IRR). In our human evaluations, three different annotators provided independent assessments for each model generation comparison. High IRR scores (closer to 1.0) are typically seen as better from a data quality perspective, however, context is important. Highly subjective tasks like evaluating the overall helpfulness of LLM generations will usually have lower IRR scores than more objective labelling tasks. There are relatively few public benchmarks for these contexts, so we feel sharing our analysis here will benefit the research community.
We used Gwet's AC1/2 statistic (Gwet, 2008, 2014) to measure inter-rater reliability (IRR), as we found it to be the most stable metric across different measurement scenarios. On the 7-point Likert scale helpfulness task that is used in our analysis, Gwet's AC2 score varies between 0.37 and 0.55 depending on the specific model comparison. We see scores on the lower end of that range for ratings from model comparisons with similar win rates to each other (like the Llama 2-Chat-70B-chat vs. ChatGPT comparison). We see scores on the higher end of that range for ratings from model comparisons with a more clear winner (like the Llama 2-Chat-34b-chat vs. Falcon-40b-instruct).</p>
<p>Limitations of human evaluations. While our results indicate that Llama 2-Chat is on par with ChatGPT on human evaluations, it is important to note that human evaluations have several limitations.</p>
<ul>
<li>By academic and research standards, we have a large prompt set of 4 k prompts. However, it does not cover real-world usage of these models, which will likely cover a significantly larger number of use cases.</li>
<li>Diversity of the prompts could be another factor in our results. For example, our prompt set does not include any coding- or reasoning-related prompts.</li>
<li>We only evaluate the final generation of a multi-turn conversation. A more interesting evaluation could be to ask the models to complete a task and rate the overall experience with the model over multiple turns.</li>
<li>Human evaluation for generative models is inherently subjective and noisy. As a result, evaluation on a different set of prompts or with different instructions could result in different results.</li>
</ul>
<h1>4 Safety</h1>
<h2>WARNING: this section contains examples of text that may be considered unsafe, offensive, or upsetting.</h2>
<p>In this section, we dive deeper into the important topic of safety measurements and mitigations. We first discuss our safety investigations into pretraining data and pretrained models (Section 4.1). Next, we describe the process of our safety alignment (Section 4.2), explaining how we collected safety-related annotations and utilized SFT and RLHF, and present experimental results. Then, we discuss the red teaming we performed to further understand and improve model safety (Section 4.3). Finally, we present quantitative safety evaluations of Llama 2-Chat (Section 4.4). We also share a model card in the Appendix, in Table 52.</p>
<h3>4.1 Safety in Pretraining</h3>
<p>It is important to understand what is in the pretraining data both to increase transparency and to shed light on root causes of potential downstream issues, such as potential biases. This can inform what, if any, downstream mitigations to consider, and help guide appropriate model use. In this section, we analyze the pretraining data for distributions of languages, demographic representations, and toxicity. We also present the results of testing the pretrained models on existing safety benchmarks.</p>
<p>Steps Taken to Pretrain Responsibly. We followed Meta's standard privacy and legal review processes for each dataset used in training. We did not use any Meta user data in training. We excluded data from certain sites known to contain a high volume of personal information about private individuals. We made a best effort to train our models efficiently to reduce the carbon footprint of pretraining (Section 2.2.1). Sharing our models broadly will reduce the need for others to train similar models. No additional filtering was conducted on the datasets, to allow Llama 2 to be more widely usable across tasks (e.g., it can be better used for hate speech classification), while avoiding the potential for the accidental demographic erasure sometimes caused by over-scrubbing. Importantly, this allows Llama 2-Chat to generalize more effectively during safety tuning with fewer examples (Welbl et al., 2021; Korbak et al., 2023; Xu et al., 2021). As a result, Llama 2 models should be used carefully and deployed only after significant safety tuning is applied.</p>
<p>Demographic Representation: Pronouns. Bias in model generations may result from biases inherited from the training data itself. For instance, Bailey et al. (2022) shows that in massive text corpora, words representing "people" are often used in more similar contexts to words representing "men" than to words representing "women," and Ganesh et al. (2023) demonstrates that a model's performance on fairness metrics can be highly dependent on how the model trains on data representing underrepresented demographic groups. Within our English-language training corpus, we computed the frequencies of the most common English pronouns in Table 9a. We observe that He pronouns are generally overrepresented in documents compared to She pronouns, echoing similar frequency differences observed in pronominal usage for similarly sized model pretraining datasets (Chowdhery et al., 2022). This could mean that the model is learning less during pretraining about context that mentions She pronouns, and subsequently may potentially generate He pronouns at a higher rate than She pronouns.</p>
<p>Demographic Representation: Identities. We also analyze the representation of different demographic groups in the pretraining data by measuring rates of usage of demographic identity terms from the HolisticBias dataset (Smith et al., 2022) as a proxy. We compute frequencies for each descriptor term in the pretraining corpus. We group descriptors into 5 axes (Religion, Gender and Sex, Nationality, Race and Ethnicity, and Sexual Orientation), and show the top 5 terms in each axis in Table 9b. In the top 5 terms, we remove a few terms such as "straight," "white," and "black," because these terms have frequent uses beyond demographic mentions (e.g., as basic color terms). We also deduplicate across lists, removing a few terms found in both Gender and Sex and Sexual Orientation. For Gender and Sex, while She pronouns are mentioned in fewer documents, the term "female" is present in a larger percentage of documents. This could imply that while there is less frequent context about She pronouns, comments about "females" are more prevalent, perhaps reflecting the differences in linguistic markedness of these terms (Blodgett et al., 2021). For Sexual Orientation, the top five terms all relate to LGBTQ+ identities. For Nationality, Race and Ethnicity, and Religion, we observe a Western skew (Bhatt et al., 2022). For instance, the term "American" is mentioned in $69.4 \%$ of the references, the term "European" is more prevalent than other race and ethnicity, and "Christian" is the most represented religion followed by "Catholic" and "Jewish."</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Gender Pronouns</th>
<th style="text-align: left;">75.23\%</th>
<th style="text-align: left;">Grammatical Person</th>
<th style="text-align: left;">94.47\%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">She (she, her, hers, herself)</td>
<td style="text-align: left;">$28.45 \%$</td>
<td style="text-align: left;">1st (I, me, my, mine, myself, ...)</td>
<td style="text-align: left;">$70.71 \%$</td>
</tr>
<tr>
<td style="text-align: left;">He (he, him, his, himself)</td>
<td style="text-align: left;">$50.73 \%$</td>
<td style="text-align: left;">2nd (you, your, yours, ...)</td>
<td style="text-align: left;">$61.80 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Unspecified (they, them, their, ...)</td>
<td style="text-align: left;">$86.38 \%$</td>
<td style="text-align: left;">3rd (it, its, itself, she, her, he, him, ...)</td>
<td style="text-align: left;">$93.07 \%$</td>
</tr>
</tbody>
</table>
<p>(a) Percentage of documents containing gender pronouns and grammatical person. $75 \%$ of all documents contain gendered pronouns. Within this subset, $28 \%$ of all documents contain She pronouns. $94 \%$ of all documents contain pronouns in general. See the full detailed list of pronouns for each subgroup in Appendix A.4.3.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Gender and Sex <br> $(5.91 \%)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sexual Orientation <br> $(6.67 \%)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Nationality <br> $(14.83 \%)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Race and Ethnicity <br> $(19.51 \%)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Religion <br> $(7.93 \%)$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Descriptor</td>
<td style="text-align: center;">\% Doc</td>
<td style="text-align: center;">Descriptor</td>
<td style="text-align: center;">\% Doc</td>
<td style="text-align: center;">Descriptor</td>
<td style="text-align: center;">\% Doc</td>
<td style="text-align: center;">Descriptor</td>
<td style="text-align: center;">\% Doc</td>
<td style="text-align: center;">Descriptor</td>
<td style="text-align: center;">\% Doc</td>
</tr>
<tr>
<td style="text-align: center;">female</td>
<td style="text-align: center;">$50.0 \%$</td>
<td style="text-align: center;">gay</td>
<td style="text-align: center;">$14.8 \%$</td>
<td style="text-align: center;">american</td>
<td style="text-align: center;">$69.4 \%$</td>
<td style="text-align: center;">european</td>
<td style="text-align: center;">$20.7 \%$</td>
<td style="text-align: center;">christian</td>
<td style="text-align: center;">$33.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">male</td>
<td style="text-align: center;">$39.1 \%$</td>
<td style="text-align: center;">lesbian</td>
<td style="text-align: center;">$4.3 \%$</td>
<td style="text-align: center;">indian</td>
<td style="text-align: center;">$16.5 \%$</td>
<td style="text-align: center;">african</td>
<td style="text-align: center;">$11.5 \%$</td>
<td style="text-align: center;">religious</td>
<td style="text-align: center;">$28.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;">feminine</td>
<td style="text-align: center;">$5.4 \%$</td>
<td style="text-align: center;">lgbt</td>
<td style="text-align: center;">$4.0 \%$</td>
<td style="text-align: center;">chinese</td>
<td style="text-align: center;">$16.3 \%$</td>
<td style="text-align: center;">asian</td>
<td style="text-align: center;">$7.4 \%$</td>
<td style="text-align: center;">spiritual</td>
<td style="text-align: center;">$20.6 \%$</td>
</tr>
<tr>
<td style="text-align: center;">transgender</td>
<td style="text-align: center;">$4.2 \%$</td>
<td style="text-align: center;">lgbtq</td>
<td style="text-align: center;">$3.6 \%$</td>
<td style="text-align: center;">korean</td>
<td style="text-align: center;">$5.1 \%$</td>
<td style="text-align: center;">latin</td>
<td style="text-align: center;">$6.2 \%$</td>
<td style="text-align: center;">catholic</td>
<td style="text-align: center;">$15.4 \%$</td>
</tr>
<tr>
<td style="text-align: center;">masculine</td>
<td style="text-align: center;">$3.1 \%$</td>
<td style="text-align: center;">queer</td>
<td style="text-align: center;">$3.5 \%$</td>
<td style="text-align: center;">mexican</td>
<td style="text-align: center;">$4.9 \%$</td>
<td style="text-align: center;">indigenous</td>
<td style="text-align: center;">$3.7 \%$</td>
<td style="text-align: center;">jewish</td>
<td style="text-align: center;">$13.0 \%$</td>
</tr>
</tbody>
</table>
<p>(b) The percentage listed below each demographic axis represents the percentage of all documents that mention any of the descriptor terms in this axis. The percentage listed for each demographic descriptor represents, among the documents that mention a descriptor in the given demographic axis, the percentage that mention this specific descriptor.</p>
<p>Table 9: Demographic representations. Analysis of pronouns and identities in our pretraining corpus shows some skews that may affect performance, such as higher representations of Western demographics.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Pretraining data toxicity. To allow for better downstream generalization, we chose not to scrub toxic data from pretraining. The HateBERT classifier assigns a toxicity likelihood of 0.5 or higher to about $0.2 \%$ of documents in our pretraining corpus.</p>
<p>Data Toxicity. We measure the prevalence of toxicity in the English-language portion of the pretraining corpus using a HateBERT classifier fine-tuned on the ToxiGen dataset (Hartvigsen et al., 2022). We score each line of a document separately and average them to assign a document score. Figure 13 shows the distribution of scores in a $10 \%$ random sample of the full corpus. About $0.2 \%$ of documents evaluated are assigned a likelihood score of 0.5 or higher, meaning there is a small amount of toxicity in our pretraining data.</p>
<p>Language Identification. While our pretraining data is mostly English, it also includes text from a small number of other languages. Table 10 shows the distribution of languages in our corpus, subsetted to those found in more than $0.005 \%$ of the documents. Our analysis uses the fastText (Bojanowski et al., 2016) language identification tool and a threshold of 0.5 for the language detection. A training corpus with a majority in English means that the model may not be suitable for use in other languages.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>**https://sustainability.fb.com/2021-sustainability-report/
${ }^{\dagger \dagger}$ https://www.mosaicml.com/blog/mpt-7b&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
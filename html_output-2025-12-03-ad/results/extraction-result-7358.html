<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7358 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7358</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7358</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-266999634</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.07950v3.pdf" target="_blank">SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciInstruct, a suite of scientific instructions for training scientific language models capable of college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs. We analyze the curated SciInstruct from multiple interesting perspectives (e.g., domain, scale, source, question type, answer length, etc.). To verify the effectiveness of SciInstruct, we fine-tuned different language models with SciInstruct, i.e., ChatGLM3 (6B and 32B), Llama3-8B-Instruct, and Mistral-7B: MetaMath, enhancing their scientific and mathematical reasoning capabilities, without sacrificing the language understanding capabilities of the base model. We release all codes and SciInstruct at https://github.com/THUDM/SciGLM.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7358.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7358.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (API)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI API, used as generator and labeler)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed‑source instruction‑tuned LLM used in this work as the main generator for chain‑of‑thought (CoT) reasoning, as a self‑reflective critic (Prompt 2) and as an outcome‑reward labeler to filter/certify generated reasoning traces (ORM).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (via API, used as generator and labeler)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>closed-source / not specified</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction‑tuned base model (API); used as generator, self‑critic, and outcome reward labeler</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Physics, Chemistry, Mathematics (multi‑domain)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate step‑by‑step (CoT) solutions to college‑level scientific/math questions; reflect on and critique own incorrect solutions; produce revised correct reasoning traces for dataset annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Three‑stage self‑reflective prompting: Prompt 1 (CoT generation), Prompt 2 (reflect on errors and revise), Prompt 3 (reflect + include real answer as hint). Also used formatted labeling prompt to produce binary correctness labels (ORM).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used primarily as a generator/labeler; filtering used a binary correctness label produced by GPT‑4 (1/0) based on match to real answer; no single numeric 'accuracy' of GPT‑4 reported for generation beyond counts of accepted traces.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Not reported as a standalone accuracy metric; pipeline yielded 19,824 correct solutions from initial CoT sampling stage (count of accepted traces after GPT‑4 ORM filtering).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['sampling multiple times per question (posterior sampling)', 'self‑reflection prompts (Prompt 2/3) increased coverage of correct traces', 'ORM (outcome reward model) used instead of costly PRM', 'quality of ground‑truth answers (used as hints in Prompt 3)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>GPT‑4-0613 used for generation and labeling in annotation pipeline; multiple samples per question; ORM labeling prompt (binary 1/0); reflective prompts as described (Prompts 1–3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Even GPT‑4 fails on many complex science problems; cannot consistently produce correct CoT traces for all questions; labeling via ORM (outcome) may miss process errors where final answer matches but intermediate reasoning is wrong.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7358.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7358.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciGLM-6B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciGLM (ChatGLM3-6B fine‑tuned on SciInstruct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ChatGLM3‑based 6B model instruction‑tuned on the SciInstruct dataset (self‑reflective CoT traces) to improve college‑level scientific and mathematical reasoning while preserving general language abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SciGLM (finetuned from ChatGLM3-6B-Base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction‑tuned / supervised fine‑tuned on SciInstruct (CoT reasoning traces)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Physics, Chemistry, Mathematics, Formal proofs (Lean) — multi‑domain scientific reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text‑based simulation of domain reasoning: solve college‑level physics/chemistry/math problems by producing step‑by‑step solutions and final answers (CoT outputs used as simulated reasoning traces).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Chain‑of‑Thought (CoT) style prompts; model fine‑tuned to return CoT by default. Evaluations run in CoT settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy on scientific and mathematical benchmark datasets (Avg.Sci, Avg.Math); pass@1 for code tasks; pass@K (simulated) used for sample diversity analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Reported relative improvement: ~4.87% improvement over the corresponding ChatGLM3-6B base model on average scientific benchmarks after instruction tuning with SciInstruct (paper statement). Exact per‑benchmark numbers available in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>ChatGLM3-6B-Base (same‑size base model) — used as primary baseline; other baselines include GPT‑4, GPT‑3.5, Galactica, LLaMA variants and dataset‑specific tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['instruction dataset quality (filtering via instruction‑quality classifier)', 'mix of domains in training data (physics/chemistry/math/formal proofs)', 'data scale (fraction of SciInstruct used): early gains at ~10% but larger improvements require >50%', 'model parameter scale (6B vs 32B)', 'chain‑of‑thought style supervision', 'data filtering (removing lowest‑scored 10% improves performance)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Fine‑tuning: learning rate 3e‑6, linear scheduler, 2 epochs, DeepSpeed; training data SciInstruct (~254k instructions overall, but experiments on varying proportions); evaluation in CoT mode. Pass@K sampling experiments used temperature=1.0, N=30 samples (K up to 25).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Improvements limited by dataset and model scale; slight drop on code generation (MBPP) compared to base in some settings; failure modes include calculation mistakes, false reasoning, and comprehension errors (classified in error analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7358.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7358.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciGLM-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciGLM (ChatGLM3-32B fine‑tuned on SciInstruct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ChatGLM3‑32B model fine‑tuned on SciInstruct to improve multi‑discipline scientific reasoning; shows consistent gains and sample‑diversity behavior comparable to much larger closed models on some benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SciGLM (finetuned from ChatGLM3-32B-Base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction‑tuned / supervised fine‑tuned on SciInstruct (CoT reasoning traces)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Physics, Chemistry, Mathematics, Formal proofs (Lean)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Produce step‑by‑step scientific solutions and final answers across college‑level science/math benchmarks (acts as a text‑based reasoning simulator for these subdomains).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Chain‑of‑Thought (CoT) prompting by default after fine‑tuning; evaluated with CoT; pass@K sampling for diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (Avg.Sci, per‑benchmark accuracies), pass@K for sample diversity (simulated pass rate across N=30 samples).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Reported relative improvement: ~2.67% improvement over ChatGLM3-32B base model on average scientific benchmarks after SciInstruct tuning (paper statement). Additionally, the paper reports that SciGLM-32B with K=25 (SciBench) and K=3 (SciEval) achieves performance comparable to GPT‑4 on those pass@K measures.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>ChatGLM3-32B-Base (base model). Other baselines reported include GPT‑4 and various open models in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['model scale (32B vs 6B) influences absolute performance', 'instruction dataset size and domain mix', 'sample diversity (pass@K) unaffected by instruction tuning; high K improves chances of correct sampled outputs', 'temperature and number of generated samples (N) used in pass@K experiments']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Fine‑tuning hyperparameters: lr=3e‑6, linear scheduler, 2 epochs, DeepSpeed. Pass@K experiments used temperature=1.0, N=30 samples, K up to 25. Evaluation in CoT mode.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Improvements still constrained by model/data scale; some remaining failure modes include arithmetic mistakes, incorrect equation deduction, and occasional incorrect intermediate reasoning despite correct final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7358.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7358.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3‑8B‑Instruct (evaluated with/without SciInstruct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B instruction‑tuned LLaMA‑family model used as a baseline and also evaluated when augmented with SciInstruct (few‑shot / dataset‑specific tuning) to assess transfer of SciInstruct across architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction‑tuned base model; evaluated zero/few‑shot and with SciInstruct augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Physics, Chemistry, Mathematics (evaluated across SciInstruct target benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Answer college‑level scientific/math questions (CoT and few‑shot settings) to test transfer of SciInstruct tuning and dataset effects.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero‑shot and few‑shot evaluation; when combined with SciInstruct it uses dataset‑specific tuning (supervised fine‑tuning producing CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy on the paper's scientific benchmarks (Avg.Sci etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Zero‑shot/few‑shot Llama3-8B-Instruct (paper reports comparisons in Table 4); exact numeric per‑benchmark values are reported in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['benefits from SciInstruct fine‑tuning (domain transfer)', 'sensitivity to number of few‑shot examples', 'domain mix in SciInstruct contributes cross‑domain gains']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluated in the same standardized framework as other models; chat module used for multi‑choice formatting; few‑shot shots and dataset‑specific tuning settings per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Paper reports variable gains depending on benchmark and few‑shot configuration; specific numeric improvements vary by task (see paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7358.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7358.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B: MetaMATH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral‑7B (MetaMATH variant; evaluated with SciInstruct / MetaMath)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B model (Mistral family) used in the paper as a math‑specialized baseline (MetaMATH) and evaluated when augmented with SciInstruct to measure transfer to scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B: MetaMATH</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>dataset‑specific tuned (MetaMATH) baseline; also evaluated with SciInstruct few‑shot/fine‑tuning</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics primarily (but evaluated on multi‑domain scientific benchmarks when combined with SciInstruct)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Solve mathematical reasoning problems and transfer to science tasks when augmented with SciInstruct traces (CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero‑shot and few‑shot settings; dataset‑specific tuning (MetaMATH) and additional SciInstruct tuning where noted.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy on mathematical benchmarks (GSM8K, MATH, Mathematics, MMLU‑Math) and scientific benchmarks when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>MetaMATH (zero/few‑shot) results used as baselines in paper tables; precise numeric values are in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['specialized math pretraining/finetuning (MetaMATH) improves math tasks', 'SciInstruct mixture can transfer math/formal proof strengths into scientific reasoning', 'number of fine‑tuning examples and domain mix affect transfer']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluated under the same accuracy/pass@K framework as other models; few‑shot and fine‑tuning configurations per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Performance depends on matching between model's pretraining specialization and target scientific task; transfer gains vary by benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7358.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7358.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5‑turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT‑3.5‑turbo‑0613 (OpenAI API variant used to generate negative samples and CoT traces)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Used in the paper as a generator of CoT traces (including incorrect ones) to construct negative examples when training the instruction‑quality classifier for filtering noisy instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo-0613 (API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>closed / not specified</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction‑tuned API model used for negative/positive sample generation in classifier training</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Physics, Chemistry, Mathematics (used for generating training traces for classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate step‑by‑step solutions to collected questions to produce negative examples (incorrect CoT traces) and positive examples for classifier training.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>CoT prompting to produce candidate traces; generated traces were filtered/labeled (by GPT‑4 or by manual/ORM methods) to create positive/negative training pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Not evaluated as a simulator; used to create training data for instruction‑quality classifier. The classifier then evaluated models using accuracy metrics on downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['quality of generated negative samples influences classifier effectiveness', 'diversity of generator models (ChatGLM2, GPT‑3.5, GPT‑4) used for negative sampling affects classifier robustness']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Used to generate candidate CoT traces; incorrect answers filtered; combined with other model traces to form classifier training set (Table 7 describes composition).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Generated negative samples may include types of errors that bias the classifier; classifier performance depends on correctness of labels assigned to these samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scibench: Evaluating college-level scientific problem-solving abilities of large language models <em>(Rating: 2)</em></li>
                <li>Scieval: A multi-level large language model evaluation benchmark for scientific research <em>(Rating: 2)</em></li>
                <li>MetaMath <em>(Rating: 2)</em></li>
                <li>MAmmoTH: Building math generalist models through hybrid instruction tuning <em>(Rating: 2)</em></li>
                <li>Galactica: A large language model for science <em>(Rating: 1)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7358",
    "paper_id": "paper-266999634",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "GPT-4 (API)",
            "name_full": "GPT-4 (OpenAI API, used as generator and labeler)",
            "brief_description": "Closed‑source instruction‑tuned LLM used in this work as the main generator for chain‑of‑thought (CoT) reasoning, as a self‑reflective critic (Prompt 2) and as an outcome‑reward labeler to filter/certify generated reasoning traces (ORM).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (via API, used as generator and labeler)",
            "model_size": "closed-source / not specified",
            "model_type": "instruction‑tuned base model (API); used as generator, self‑critic, and outcome reward labeler",
            "scientific_domain": "Physics, Chemistry, Mathematics (multi‑domain)",
            "simulation_task_description": "Generate step‑by‑step (CoT) solutions to college‑level scientific/math questions; reflect on and critique own incorrect solutions; produce revised correct reasoning traces for dataset annotation.",
            "prompting_strategy": "Three‑stage self‑reflective prompting: Prompt 1 (CoT generation), Prompt 2 (reflect on errors and revise), Prompt 3 (reflect + include real answer as hint). Also used formatted labeling prompt to produce binary correctness labels (ORM).",
            "evaluation_metric": "Used primarily as a generator/labeler; filtering used a binary correctness label produced by GPT‑4 (1/0) based on match to real answer; no single numeric 'accuracy' of GPT‑4 reported for generation beyond counts of accepted traces.",
            "reported_accuracy": "Not reported as a standalone accuracy metric; pipeline yielded 19,824 correct solutions from initial CoT sampling stage (count of accepted traces after GPT‑4 ORM filtering).",
            "baseline_accuracy": null,
            "factors_reported": [
                "sampling multiple times per question (posterior sampling)",
                "self‑reflection prompts (Prompt 2/3) increased coverage of correct traces",
                "ORM (outcome reward model) used instead of costly PRM",
                "quality of ground‑truth answers (used as hints in Prompt 3)"
            ],
            "experimental_conditions": "GPT‑4-0613 used for generation and labeling in annotation pipeline; multiple samples per question; ORM labeling prompt (binary 1/0); reflective prompts as described (Prompts 1–3).",
            "limitations_or_failure_modes": "Even GPT‑4 fails on many complex science problems; cannot consistently produce correct CoT traces for all questions; labeling via ORM (outcome) may miss process errors where final answer matches but intermediate reasoning is wrong.",
            "uuid": "e7358.0",
            "source_info": {
                "paper_title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "SciGLM-6B",
            "name_full": "SciGLM (ChatGLM3-6B fine‑tuned on SciInstruct)",
            "brief_description": "A ChatGLM3‑based 6B model instruction‑tuned on the SciInstruct dataset (self‑reflective CoT traces) to improve college‑level scientific and mathematical reasoning while preserving general language abilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SciGLM (finetuned from ChatGLM3-6B-Base)",
            "model_size": "6B parameters",
            "model_type": "instruction‑tuned / supervised fine‑tuned on SciInstruct (CoT reasoning traces)",
            "scientific_domain": "Physics, Chemistry, Mathematics, Formal proofs (Lean) — multi‑domain scientific reasoning",
            "simulation_task_description": "Text‑based simulation of domain reasoning: solve college‑level physics/chemistry/math problems by producing step‑by‑step solutions and final answers (CoT outputs used as simulated reasoning traces).",
            "prompting_strategy": "Chain‑of‑Thought (CoT) style prompts; model fine‑tuned to return CoT by default. Evaluations run in CoT settings.",
            "evaluation_metric": "Accuracy on scientific and mathematical benchmark datasets (Avg.Sci, Avg.Math); pass@1 for code tasks; pass@K (simulated) used for sample diversity analysis.",
            "reported_accuracy": "Reported relative improvement: ~4.87% improvement over the corresponding ChatGLM3-6B base model on average scientific benchmarks after instruction tuning with SciInstruct (paper statement). Exact per‑benchmark numbers available in paper tables.",
            "baseline_accuracy": "ChatGLM3-6B-Base (same‑size base model) — used as primary baseline; other baselines include GPT‑4, GPT‑3.5, Galactica, LLaMA variants and dataset‑specific tuned models.",
            "factors_reported": [
                "instruction dataset quality (filtering via instruction‑quality classifier)",
                "mix of domains in training data (physics/chemistry/math/formal proofs)",
                "data scale (fraction of SciInstruct used): early gains at ~10% but larger improvements require &gt;50%",
                "model parameter scale (6B vs 32B)",
                "chain‑of‑thought style supervision",
                "data filtering (removing lowest‑scored 10% improves performance)"
            ],
            "experimental_conditions": "Fine‑tuning: learning rate 3e‑6, linear scheduler, 2 epochs, DeepSpeed; training data SciInstruct (~254k instructions overall, but experiments on varying proportions); evaluation in CoT mode. Pass@K sampling experiments used temperature=1.0, N=30 samples (K up to 25).",
            "limitations_or_failure_modes": "Improvements limited by dataset and model scale; slight drop on code generation (MBPP) compared to base in some settings; failure modes include calculation mistakes, false reasoning, and comprehension errors (classified in error analysis).",
            "uuid": "e7358.1",
            "source_info": {
                "paper_title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "SciGLM-32B",
            "name_full": "SciGLM (ChatGLM3-32B fine‑tuned on SciInstruct)",
            "brief_description": "A ChatGLM3‑32B model fine‑tuned on SciInstruct to improve multi‑discipline scientific reasoning; shows consistent gains and sample‑diversity behavior comparable to much larger closed models on some benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SciGLM (finetuned from ChatGLM3-32B-Base)",
            "model_size": "32B parameters",
            "model_type": "instruction‑tuned / supervised fine‑tuned on SciInstruct (CoT reasoning traces)",
            "scientific_domain": "Physics, Chemistry, Mathematics, Formal proofs (Lean)",
            "simulation_task_description": "Produce step‑by‑step scientific solutions and final answers across college‑level science/math benchmarks (acts as a text‑based reasoning simulator for these subdomains).",
            "prompting_strategy": "Chain‑of‑Thought (CoT) prompting by default after fine‑tuning; evaluated with CoT; pass@K sampling for diversity.",
            "evaluation_metric": "Accuracy (Avg.Sci, per‑benchmark accuracies), pass@K for sample diversity (simulated pass rate across N=30 samples).",
            "reported_accuracy": "Reported relative improvement: ~2.67% improvement over ChatGLM3-32B base model on average scientific benchmarks after SciInstruct tuning (paper statement). Additionally, the paper reports that SciGLM-32B with K=25 (SciBench) and K=3 (SciEval) achieves performance comparable to GPT‑4 on those pass@K measures.",
            "baseline_accuracy": "ChatGLM3-32B-Base (base model). Other baselines reported include GPT‑4 and various open models in paper tables.",
            "factors_reported": [
                "model scale (32B vs 6B) influences absolute performance",
                "instruction dataset size and domain mix",
                "sample diversity (pass@K) unaffected by instruction tuning; high K improves chances of correct sampled outputs",
                "temperature and number of generated samples (N) used in pass@K experiments"
            ],
            "experimental_conditions": "Fine‑tuning hyperparameters: lr=3e‑6, linear scheduler, 2 epochs, DeepSpeed. Pass@K experiments used temperature=1.0, N=30 samples, K up to 25. Evaluation in CoT mode.",
            "limitations_or_failure_modes": "Improvements still constrained by model/data scale; some remaining failure modes include arithmetic mistakes, incorrect equation deduction, and occasional incorrect intermediate reasoning despite correct final answer.",
            "uuid": "e7358.2",
            "source_info": {
                "paper_title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Llama3-8B-Instruct",
            "name_full": "Llama3‑8B‑Instruct (evaluated with/without SciInstruct)",
            "brief_description": "An 8B instruction‑tuned LLaMA‑family model used as a baseline and also evaluated when augmented with SciInstruct (few‑shot / dataset‑specific tuning) to assess transfer of SciInstruct across architectures.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct",
            "model_size": "8B parameters",
            "model_type": "instruction‑tuned base model; evaluated zero/few‑shot and with SciInstruct augmentation",
            "scientific_domain": "Physics, Chemistry, Mathematics (evaluated across SciInstruct target benchmarks)",
            "simulation_task_description": "Answer college‑level scientific/math questions (CoT and few‑shot settings) to test transfer of SciInstruct tuning and dataset effects.",
            "prompting_strategy": "Zero‑shot and few‑shot evaluation; when combined with SciInstruct it uses dataset‑specific tuning (supervised fine‑tuning producing CoT).",
            "evaluation_metric": "Accuracy on the paper's scientific benchmarks (Avg.Sci etc.).",
            "reported_accuracy": null,
            "baseline_accuracy": "Zero‑shot/few‑shot Llama3-8B-Instruct (paper reports comparisons in Table 4); exact numeric per‑benchmark values are reported in paper tables.",
            "factors_reported": [
                "benefits from SciInstruct fine‑tuning (domain transfer)",
                "sensitivity to number of few‑shot examples",
                "domain mix in SciInstruct contributes cross‑domain gains"
            ],
            "experimental_conditions": "Evaluated in the same standardized framework as other models; chat module used for multi‑choice formatting; few‑shot shots and dataset‑specific tuning settings per paper.",
            "limitations_or_failure_modes": "Paper reports variable gains depending on benchmark and few‑shot configuration; specific numeric improvements vary by task (see paper tables).",
            "uuid": "e7358.3",
            "source_info": {
                "paper_title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Mistral-7B: MetaMATH",
            "name_full": "Mistral‑7B (MetaMATH variant; evaluated with SciInstruct / MetaMath)",
            "brief_description": "A 7B model (Mistral family) used in the paper as a math‑specialized baseline (MetaMATH) and evaluated when augmented with SciInstruct to measure transfer to scientific tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B: MetaMATH",
            "model_size": "7B parameters",
            "model_type": "dataset‑specific tuned (MetaMATH) baseline; also evaluated with SciInstruct few‑shot/fine‑tuning",
            "scientific_domain": "Mathematics primarily (but evaluated on multi‑domain scientific benchmarks when combined with SciInstruct)",
            "simulation_task_description": "Solve mathematical reasoning problems and transfer to science tasks when augmented with SciInstruct traces (CoT).",
            "prompting_strategy": "Zero‑shot and few‑shot settings; dataset‑specific tuning (MetaMATH) and additional SciInstruct tuning where noted.",
            "evaluation_metric": "Accuracy on mathematical benchmarks (GSM8K, MATH, Mathematics, MMLU‑Math) and scientific benchmarks when applicable.",
            "reported_accuracy": null,
            "baseline_accuracy": "MetaMATH (zero/few‑shot) results used as baselines in paper tables; precise numeric values are in paper tables.",
            "factors_reported": [
                "specialized math pretraining/finetuning (MetaMATH) improves math tasks",
                "SciInstruct mixture can transfer math/formal proof strengths into scientific reasoning",
                "number of fine‑tuning examples and domain mix affect transfer"
            ],
            "experimental_conditions": "Evaluated under the same accuracy/pass@K framework as other models; few‑shot and fine‑tuning configurations per paper.",
            "limitations_or_failure_modes": "Performance depends on matching between model's pretraining specialization and target scientific task; transfer gains vary by benchmark.",
            "uuid": "e7358.4",
            "source_info": {
                "paper_title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-3.5‑turbo-0613",
            "name_full": "GPT‑3.5‑turbo‑0613 (OpenAI API variant used to generate negative samples and CoT traces)",
            "brief_description": "Used in the paper as a generator of CoT traces (including incorrect ones) to construct negative examples when training the instruction‑quality classifier for filtering noisy instructions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo-0613 (API)",
            "model_size": "closed / not specified",
            "model_type": "instruction‑tuned API model used for negative/positive sample generation in classifier training",
            "scientific_domain": "Physics, Chemistry, Mathematics (used for generating training traces for classifier)",
            "simulation_task_description": "Generate step‑by‑step solutions to collected questions to produce negative examples (incorrect CoT traces) and positive examples for classifier training.",
            "prompting_strategy": "CoT prompting to produce candidate traces; generated traces were filtered/labeled (by GPT‑4 or by manual/ORM methods) to create positive/negative training pairs.",
            "evaluation_metric": "Not evaluated as a simulator; used to create training data for instruction‑quality classifier. The classifier then evaluated models using accuracy metrics on downstream tasks.",
            "reported_accuracy": null,
            "baseline_accuracy": null,
            "factors_reported": [
                "quality of generated negative samples influences classifier effectiveness",
                "diversity of generator models (ChatGLM2, GPT‑3.5, GPT‑4) used for negative sampling affects classifier robustness"
            ],
            "experimental_conditions": "Used to generate candidate CoT traces; incorrect answers filtered; combined with other model traces to form classifier training set (Table 7 describes composition).",
            "limitations_or_failure_modes": "Generated negative samples may include types of errors that bias the classifier; classifier performance depends on correctness of labels assigned to these samples.",
            "uuid": "e7358.5",
            "source_info": {
                "paper_title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scibench: Evaluating college-level scientific problem-solving abilities of large language models",
            "rating": 2,
            "sanitized_title": "scibench_evaluating_collegelevel_scientific_problemsolving_abilities_of_large_language_models"
        },
        {
            "paper_title": "Scieval: A multi-level large language model evaluation benchmark for scientific research",
            "rating": 2,
            "sanitized_title": "scieval_a_multilevel_large_language_model_evaluation_benchmark_for_scientific_research"
        },
        {
            "paper_title": "MetaMath",
            "rating": 2
        },
        {
            "paper_title": "MAmmoTH: Building math generalist models through hybrid instruction tuning",
            "rating": 2,
            "sanitized_title": "mammoth_building_math_generalist_models_through_hybrid_instruction_tuning"
        },
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 1,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 1,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        }
    ],
    "cost": 0.01986525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models
18 Nov 2024</p>
<p>Dan Zhang 
The Knowledge Engineering Group (KEG)
Tsinghua University</p>
<p>Zhipu AI</p>
<p>Ziniu Hu 
California Institute of Technology https://SciGLM.github.io</p>
<p>Sining Zhoubian 
The Knowledge Engineering Group (KEG)
Tsinghua University</p>
<p>Zhipu AI</p>
<p>Zhengxiao Du 
The Knowledge Engineering Group (KEG)
Tsinghua University</p>
<p>Zhipu AI</p>
<p>Kaiyu Yang 
California Institute of Technology https://SciGLM.github.io</p>
<p>Zihan Wang 
The Knowledge Engineering Group (KEG)
Tsinghua University</p>
<p>Zhipu AI</p>
<p>Yisong Yue 
California Institute of Technology https://SciGLM.github.io</p>
<p>Yuxiao Dong 
The Knowledge Engineering Group (KEG)
Tsinghua University</p>
<p>Jie Tang 
The Knowledge Engineering Group (KEG)
Tsinghua University</p>
<p>Collection Math Physics Chemistry</p>
<p>SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models
18 Nov 2024449A153AC3E372FB7DE410F3F9F4B8BFarXiv:2401.07950v3[cs.CL]
Large Language Models (LLMs) have shown promise in assisting scientific discovery.However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations.To bridge these gaps, we introduce SciInstruct, a suite of scientific instructions for training scientific language models capable of college-level scientific reasoning.Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain.This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise.Applying this framework, we curated a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs.We analyze the curated SciInstruct from multiple interesting perspectives (e.g., domain, scale, source, question type, answer length, etc.).To verify the effectiveness of SciInstruct, we fine-tuned different language models with SciInstruct, i.e., ChatGLM3 (6B and 32B), Llama3-8B-Instruct, and Mistral-7B: MetaMath, enhancing their scientific and mathematical reasoning capabilities, without sacrificing the language understanding capabilities of the base model.We release all codes and SciInstruct at https://github.com/THUDM/SciGLM.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have shown potential to assist and accelerate scientific discovery [1; 2], helping tasks like protein prediction [3], weather forecasting [4] and geoscience understanding [5].Despite these promising proof-of-concept trials, recent studies [6; 7; 8; 9] show that even advanced LLMs like GPT-3.5 and GPT-4 struggle with basic scientific problems, achieving only 28.52% accuracy on some college-level textbook questions.These scientific questions, such as calculating energy with the Planck distribution, require a diverse set of skills, including finding the correct combination of physical concepts and axioms, choice and deduction of formal equations, and rigorous numerical computing.Before letting LLMs equip these skills to solve basic scientific questions, all ambitious visions of building LLM agents to assist scientific discovery could be unreliable.This brings substantial incentives for building scientific instructions and using them to develop foundational scientific language models.Table 1: Comparison between SciInstruct and other related instruction datasets.</p>
<p>Dataset Domain College Math Physics Chemistry Lean Level</p>
<p>Galactica [1] Unknown MathInstruct [10] MetaMath [11] WebInstruct [12]    However, training LLMs to understand science (e.g., physics, chemistry, math) is much more challenging than many general reasoning tasks, requiring more complicated skills.Therefore, the core of improving LLMs' scientific problem-solving capabilities is to build large-scale and highquality instruction datasets, which shall cover all the required skills.We thus aggregate data to improve each skill: 1) for scientific concept understanding, we gather a substantial amount of physics and chemistry questions that require basic science knowledge; 2) for numerical calculation, we crawl and utilize additional and more advanced mathematical calculation data; 3) for rigorous deduction of symbolic equations, we incorporate formal theorem proofs written in Lean.Such a mixture of data sources enables the trained model not to overfit to a single subject but to acquire some general and fundamental skills to solve different scientific tasks.</p>
<p>SciInstruct</p>
<p>Physics</p>
<p>On the other hand, the scale of available instruction data from the internet for scientific problems is way smaller than other tasks.As scientific content often requires certain expertise to create, and high-quality information is often protected by intellectual property, most data we can legally access only contain question-answer (QA) pairs without detailed chain-of-thought reasoning steps (R).However, merely training LLMs on QA pairs will lead to very bad results and even harm their general language capability.To get high-quality reasoning steps (R) as instruction, we propose a selfreflective instruction annotation framework that asks LLM to autonomously annotate, critique, and revise reasoning steps, with minimal human intervention.Specifically, LLM first tries to generate both reasoning steps and answer the given question (Q) only; then, for those outputs with incorrect answer prediction, we ask LLM itself to identify the error type, based on which to address the error and revise the output, until getting the correct answer.Such a self-reflective annotation framework solely utilizes AI rather than humans to collect reasoning traces (R) as instructions, while guaranteeing the quality and addressing the potential mistakes of existing LLM with careful answer checking and LLM self-reflection.</p>
<p>After consolidating the questions and answers produced by self-reflective annotation, we construct SciInstruct, a comprehensive dataset instruction tuning scientific language models.Figure 1 and Figure 2 present the domain and question type proportions of SciInstruct.Table 1 concludes the key differences between existing datasets and ours.In this work, we choose three LLMs, i.e., the ChatGLM3 [13; 14] (6B and 32B), Llama3-8B-Instruct [15], and Mistral-7B: MetaMATH [16; 11], as our backbone base models.For example, by fine-tuning the ChatGLM series model on SciInstruct, we obtain the SciGLM model.We then evaluate the fine-tuned models through three types of evaluation tasks, including scientific test sets, mathematical benchmarks, and general language and coding tasks, and show the average accuracy on scientific benchmarks of different LLMs in Figure 3. Through instruction tuning, we achieve a 4.87% improvement over the 6B model, and 2.67% improvement over the 32B model, outperforming many previous state-of-the-art models with the same parameter size, including Galactica [1] for science problems, and MAmmoTH [10] for math problems.We also show tuning our instruction datasets does not sacrifice general language understanding capabilities, making SciGLM a good suite of scientific language models for both human-AI communication as well as scientific domain-knowledge expertise.</p>
<p>We highlight our contributions as follows:</p>
<p>Problem n</p>
<p>CoT n Answer n</p>
<p>••••••</p>
<p>QA dataset Bootstrapped</p>
<p>The number of asymptotes of the curve y=\frac{x^2+x}{x^2-1} is ___.</p>
<p>How to determine the harmonic vibration frequency of a single degree of freedom system?</p>
<p>Try to calculate the boiling point of water at 670kPa external pressure.• From the data perspective, we construct SciInstruct, a comprehensive scientific instruction tuning dataset that includes physics, chemistry problems, math, and formal proofs.</p>
<p>• From the method perspective, we propose a self-reflective annotation pipeline for LLMs to autonomously curate instruction datasets.</p>
<p>• From the model perspective, to verify the effectiveness of our SciInstruct, we finetune different LLMs (the ChatGLM3 series model, Llama3-8B-Instruct, and Mistral-7B: MetaMATH) on SciInstruct and show performance improvements on various scientific and mathematical benchmarks, without sacrificing general language understanding tasks.</p>
<p>SciInstruct</p>
<p>Many research [17; 10; 12] have shown that fine-tuning pre-trained LLMs on high-quality CoT reasoning data can gain performance improvement by enabling the model to better utilize the knowledge memorized through pre-training, follow more accurate and human-readable reasoning styles and language formats.However, the main challenges of constructing scientific instructions include the knowledge and complexity required and the smaller scale of available data.Therefore, we seek to tackle these obstacles by creating SciInstruct to enhance the LLMs' scientific problemsolving capabilities.Figure 4 illustrates our meticulous design of essential sub-modules aimed at gathering large-scale, high-quality instructions.These critical sub-modules encompass self-reflective instruction annotation and noisy instruction filtering.SciInstruct comprises a total of 254,051 verified instructions.</p>
<p>Diverse Instruction Collection</p>
<p>Our objective is to build a comprehensive and diverse dataset that encompasses scientific knowledge in terms of depth, wide coverage, and diversity.To achieve this, we will focus on scientific fields and curate several top-tier datasets that are extensively utilized and cover a wide range of scientific disciplines, such as physics, chemistry problems, mathematics, and formal proofs.To initiate the process, we collect questions from a variety of sources, including textbooks, pedagogical materials, and problem sets.</p>
<p>Instruction Subject.As show on the left side of Figure 4, we create data from the following subjects:</p>
<p>• Physics.This subject aims to address the challenge of processing complex physical problems with step-by-step solutions and assessing the ability of LLMs to comprehend and analyze physics problems.Public training datasets such as Fundamentals of Physics and Physical Chemistry are observed to lack college-level physics knowledge.To address this gap, we collected a large set of physical questions from a wide array of subjects (e.g., dynamics, quantum physics, electrodynamics, etc.) from typical physical textbooks, along with a comprehensive dictionary of physics problems
[Prompt 3]
The following input consists of a science problem, a corresponding solution and the real answer.The given solution is incorrect, please reflect on its errors and then generate a correct stepby-step solution to the problem based on the real answer.</p>
<p>[Prompt 2] The following input consists of a science problem and a corresponding solution.</p>
<p>However, this solution is incorrect, please reflect on its errors and then generate a correct step-by-step solution to the problem.</p>
<p>[Prompt 1] The following input consists of a science problem, please generate an elaborate step-by-step solution to the problem.</p>
<p>Chain-of-Thought</p>
<p>Reflective Generation</p>
<p>Prompt Answer</p>
<p>Generate Generate</p>
<p>[Sampled Output]</p>
<p>The phase difference Δφ = π/6, the distance between two points Δx = 0.2m, and the vibration period T = 0.4s are known.According to the phase difference and the relationship between the wavelength: Δφ = 2 PI (Δx/lambda), we have wavelength lambda = 1.2m.Then according to the relationship between wavelength λ and period T and wave speed v: v = λ/T, the known value is substituted into the above formula: v = 1.2/0.4= 3m/s.The wave speed is 3m/s.</p>
<p>LLM</p>
<p>[Reflection] The error in the solution is that the relationship between phase difference and wavelength used in the calculation of wavelength is incorrect.The correct relation should be Δφ = 2PI (Δx/lambda).</p>
<p>[Answer] First, according to the formula ∆phi = 2 PI (∆x/lambda), we know that lambda = 2 PI (0.2)/(PI / 6) = 1.2 m.Next, known vibration period T = 0.4s, according to the definition of wave speed v formula v = lambda/T, we have v = 1.2m / 0.4s = 3m/s.In summary, the wavelength is 1.2m and the wave speed is 3m/s.</p>
<p>LLM [Reflection]</p>
<p>The error in the solution is that the ratio of the phase difference π/6 to the full wave phase 2π, rather than half wave phase π, i.e. π/6 ÷ 2π, should be used to determine the ratio of the distance between two points to the wavelength.</p>
<p>[Answer] First, known phase difference Δphi=PI / 6, distance between two points Δx = 0.2 m, we can use the formula Δphi = 2 PI (Δx/lambda) to solve the wavelength lambda.We can know that lambda = (2 PI (0.2)/(PI / 6) = (2 x 0.2)/(1/6) = 2.4 m.Next, known vibration period T = 0.4s, according to the definition of wave speed v formula v = λ/T, we obtain v = λ/ T = 2.4m / 0.4s = 6m/s.To sum up, the wavelength is 2.4m and the wave speed is 6m/s.</p>
<p>LLM</p>
<p>[Prompt] On a ray of simple harmonics, the vibration phase difference between two points 0.2\mathrm{~m} is \pi/6.It is also known that the vibration period is 0.4\mathrm{~s}, then the wavelength is, and the wave speed is? and solutions.Note that most of these physics questions only contain a single answer without a step-by-step solution.</p>
<p>• Chemistry.We gathered questions from various chemistry subjects, including college-level General Chemistry, Analytical Chemistry, Biochemistry, and Inorganic Chemistry.Unlike Physics, these Chemistry problems emphasize more on knowledge concepts and require less calculation.</p>
<p>• Math.College-level mathematical tasks are characterized by their intricate logic and high complexity.We have gathered math problems encompassing various mathematical subjects such as Calculus, Algebra, and Advanced Mathematics.These problems come in different formats, including multi-choice questions, calculations, and complex problem-solving scenarios.Our collection process involves sourcing problems from public Q&amp;A websites, problem sets, and textbooks.</p>
<p>• Formal Proofs (Lean).SciInstruct also includes data from formal mathematics.In formal math, theorems and proofs are expressed in formal logic instead of natural language.They look like computer programs.For example, "theorem gcd_self (n : Nat) : gcd n n = n" is a theorem that says: For any natural number n, the greatest common divisor of n and itself is n.And "cases n &lt;;&gt; simp [gcd, mod_self]" is a proof of this theorem.There are many languages for writing formal theorems and proofs; examples include Coq [18], Isabelle [19], and Lean [20].We chose to include data from Lean, as it offers a vast math library and is currently the most popular in the math community.Therefore, it has abundant theorems and proofs written by mathematicians, covering diverse topics in graduate-level mathematics, such as analysis, geometry, and algebra.Specifically, we process the data from LeanDojo [21] and format it to align with the successive deduction process, ensuring its relevance and applicability to the model's training for mathematical reasoning tasks in natural language.This preprocessing step helps bridge the gap between the Lean dataset's nature and the model's expected learning signals.Finally, we obtained 40,248 instructions for theorem proof.Like Appendix A.3, we form an instruction for each theorem and its proof.</p>
<p>Multi-lingual Instruction.To enhance the overall quality and effectiveness of the curated dataset, we also translate the default Chinese questions into English.We found that LLMs tend to generate correct solutions after translating these problems into English for some Chinese problems that do not obtain correct solutions.This improved performance is likely due to the higher-quality English corpus used during the pre-training of LLMs.Therefore, we have embraced this strategy to construct SciInstruct for Chinese questions.Summary.In total, we gathered 257,143 raw questions, the majority of which lacked step-by-step reasoning steps.We aim to supplement these through a self-reflective annotation process.</p>
<p>Self-Reflective Instruction Annotation</p>
<p>Given a language model π to answer question Q, recent studies [22] have shown that by first forcing it to generate step-by-step reasoning steps (R) first, the overall performance for correctly generating answer A can be significantly improved, via:
P π (A | Q) = E R∼Pπ(R|Q) P (A | R, Q
) .This is why many instruction datasets aim to collect high-quality intermediate solutions to train LLMs generating correct step-by-step solutions.The key challenge for the science domain, as we state above, is that most of the QA pairs we collect do not contain ground-truth reasoning paths (R).Getting the correct intermediate reasoning R given QA can be regarded as a discrete latent variable inference problem via posterior sampling.However, in practice, we cannot afford to sample all possible R from LLM.Here we adopt a simplified strategy for solving it: 1) Utilizing a powerful LLM (we use GPT-4-0613), we sample each multiple times for each question, recording its reasoning traces as well as the predicted answer; 2) We filter out only the traces with a correct predicted answer, by assuming only with correct traces, LLM can get a correct answer.</p>
<p>LLM Self-Reflection.However, even GPT-4 cannot consistently produce accurate answers after multiple trials, and the above procedure can only collect R for a portion of questions.Drawing on prior research demonstrating the capacity of language models for self-correction [23; 24], we refine the CoT process using a self-reflective framework, as depicted in the middle of Figure 4.The final reflective generation process entails three stages.Initially, we employ a simple CoT prompt (Prompt 1) to obtain step-by-step solutions for each question.To obtain an accurate assessment of reasoning results, we employ a GPT-4 labeling method based on an outcome reward model (ORM) [25] as a basic implementation in our work, rather than the expensive process reward model (PRM) [25] that typically requires manual annotation, especially for complex scientific reasoning.We filter out incorrect solutions by applying the GPT-4 labeling method, resulting in 19,824 correct solutions.Subsequently, the incorrect solutions from stage one and their respective questions enter stage two.</p>
<p>Here, a reflective prompt (Prompt 2) assists GPT-4 in generating correct answers by analyzing its errors from stage one.The newly generated solutions are then filtered again by GPT-4, and the undesirable ones proceed to stage three.In stage three, based on Prompt 2, we incorporate the real answer as a direct hint in the prompt (Prompt 3) to further aid in question resolution.The final segment of correct solutions is obtained after the generation and filtering process.We illustrate the reflective generation in Figure 5 and quantify the amount of data generated by each process.</p>
<p>Filtering Out Low-Quality Instruction</p>
<p>Though the above procedures give us the annotated reasoning steps (R), not all of them are correct.</p>
<p>The error can come from two sources: 1) though LLM generates a correct answer, the intermediate reasoning can still be wrong [26]; 2) the question and ground-truth solutions transformed via Optical character recognition (OCR) may be incomplete and unable to be successfully compiled.Therefore, we propose another step to train an instruction-quality classifier and filter out low-quality instructions.</p>
<p>Quality Data Synthesis.We randomly selected a subset of questions from our labeled dataset of 11,553 questions as positive samples.To generate negative samples, we prompted ChatGLM2-6B, GPT-3.5-turbo-0613, and GPT-4-0613 to provide step-by-step answers to selected questions.We filtered inaccurate answers from ChatGLM2-6B and labeled the solutions from GPT-3.5-turbo and GPT-4 using a formatted prompt method demonstrated in Figure 11 in Appendix A.4.These solutions were merged with annotated solutions from the original dataset to train our classifier.The composition of the merged dataset is detailed in Table 7 in Appendix A.5.</p>
<p>Instruction-quality Classifier.We improved dataset quality by training an instruction-quality classifier based on Table 7 using ChatGLM3-6B-Base features.Using these data, we train an instruction-quality classifier via the feature pre-extracted by a ChatGLM3-6B-Base model.The classifier outputs a logit ranging from -1 to 1, with higher scores indicating more reliable answers.This logit is used to rank and select high-quality data from the noisy dataset.Table 2 demonstrates the effectiveness of supervised fine-tuning on both filtered and unfiltered datasets at a 6B scale, showing that models trained on a filtered dataset perform better.</p>
<p>Error Analysis.As our classifier filter is trained on labeled datasets and generated solutions, errors in negatively labeled solutions from ChatGLM2-6B, GPT-3.5-turbo, and GPT-4 can significantly impact the classifier's performance.Therefore, we conduct an error analysis and categorize them into Comprehensive mistakes, Calculation mistakes, and False reasoning.This analysis is detailed in Figure 12 in A.6, demonstrating the classifier's capacity to recognize these errors in the dataset.</p>
<p>Summary.Based on the aforementioned key sub-modules, we have constructed the SciInstruct dataset, which comprises 254,051 instructions, as illustrated in Table 3.</p>
<p>Instruction-Tuning with SciInstruct</p>
<p>As our foundational model, we choose three LLMs, i.e., ChatGLM3 (6B and 32B), Llama3-8B-Instruct, and Mistral-7B: MetaMATH.After establishing the base model, we have standardized all data into a chatbot-style format.Subsequently, we have fine-tuned the foundational model using the SciInstruct, enabling us to validate our constructed SciInstruct.Throughout the fine-tuning process, we have conducted experiments using the Huggingface transformers library.For both the 6B and 32B models, we have utilized a learning rate of 3e-6, employed a linear scheduler, and trained for two epochs.To efficiently train the model, we have leveraged DeepSpeed [27] training.CEval-Sci [28], MMLU-Sci [29], SciEval [6], SciBench [7], GPGQ [8], GSM8K [30], MATH [31], Mathematics [32], SAT-Math [33], MMLU-Math [29] from MathInstruction, and CEval-MATH [28].These evaluation datasets cover a broad range of subjects, including physics, chemistry, and math.</p>
<p>General Tasks.Furthermore, we assess the generalization abilities of tasks across various scales when fine-tuning models.These tasks include assessing knowledge abilities (MMLU [29] and CEval [28]) and code generation (MBPP) [34].</p>
<p>Evaluation Metrics.The default setting for the fine-tuned base model inherently provides the CoT solution.Hence, we conduct all experiments using CoT settings.To thoroughly and accurately evaluate the capabilities of different models, we employ the accuracy metric for all tasks except for code generation, for which we use the pass@1 metric.</p>
<p>Baselines.We consider the following baselines(e.g., GPT-4 [35], GLM [13; 14; 36] and LLaMA Base [37], Continue Pre-training, and Dataset-specific Tuning, etc.) and describe details in Appendix A.8.We employ a standardized evaluation framework to compare GLM and LLaMA Base baselines fairly.To gauge performance in the MATH task, we utilize zero-shot and 8-shot configurations to determine the highest accuracy.Additionally, for Mathematics, SAT-Math, MMLU, and CEval, we employ a chat module for assessment.When dealing with multiple-choice questions, we formulate the prompt as "Therefore, among A through D, the answer is".</p>
<p>Data Contamination.Both SciInstruct and the evaluation benchmarks fall within the science domain.To minimize any potential data contamination and strengthen the integrity of our results, we ensure that the training set used to construct SciInstruct was not derived from the test sets utilized in our evaluation.In other words, there was no overlap between the SciInstruct and the evaluation benchmarks used in our experiments.</p>
<p>Main Results and Analysis</p>
<p>Table 4, Table 5, and Table 6 present experimental findings for scientific and mathematical benchmarks, as well as general tasks.The results show that training on the provided SciInstruct benefits reasoning tasks, improving performance in scientific reasoning (i.e., CEval-Sci, SciEval, SciBench, MMLU-Sci) and transferring to mathematical tasks.Such performance improvement is consistent with different scales of based model parameters, across 6B and 32B.</p>
<p>In addition, SciGLM's performance improvement in scientific reasoning does not sacrifice its general language understanding capabilities.As shown in Figure 13 in A.9, on standard tasks like MMLU and CEval, SciGLM even achieves slight performance improvement.On code generation like MBPP, the performance is slightly lower but is overall consistent.As shown in Figure 15 in A.10, we present a statistics problem in SciBench that is accurately solved with the SciGLM (32B).we assess the significance of each subject based on the performance impact across various tasks.As shown in Figure 6, we have an interesting finding: each subject contributes tasks that are not restricted to its immediate domains.For instance, Physics and Chemistry data significantly aid in CEval-Math tasks, while Math and Formal Proof improve SciBench performance.This shows that our mixture dataset enables LLMs to acquire some general reasoning skills for solving scientific questions instead of merely overfitting to a certain task distribution, and achieving universal improvement on different downstream tasks.</p>
<p>Influence of Data Scaling.One central question for instruction dataset creation is how many samples are needed for a model to learn specific skills [38].</p>
<p>Prior works [39] have shown that for dialogue tasks, as few as 1000 high-quality instruction samples can lead to significant improvements.We're interested in analyzing data scaling law for scientific problemsolving.Through retraining the model with varying data proportions and analyzing the outcomes in science and math, as shown in Figure 7, one interesting pattern we find is initial data augmentation of 10% yields improvements, but further additions show no significant gains until surpassing a 50% threshold.We hypothesize that the early gains are due to that finetuned LLM learning basic reasoning and task formatting, which requires fewer instruction data (less than 30k).Advancing to more complex skills, such as equation deduction, necessitates a larger dataset for effective learning and generalize.Future research on improving data quality could potentially lower the data requirement for LLM skill learning.</p>
<p>Pass@K Analysis on Sample Diversity.One interesting observation of LLM reasoning is that with non-zero temperature and sampling multiple times, even for those very hard questions, LLM still has a high chance of providing a correct answer.Pass@K is widely used for code generation [40; 41]    math reasoning [25].To analyze whether our SciInstruct can really improve the general reasoning, we simulate different Pass@K values as shown in Figure 8.We use fine-tuned SciGLM (32B) and ChatGLM (32B) to generate N ≥ K (in this paper, N = 30 and K ≤ 25) solutions per question, allowing for a more accurate examination of the LLM's true pass rate on that question.We find fine-tuning does not influence the sample diversity.SciGLM (32B) with K=25 on SciBench and K=3 on SciEval can achieve comparable performance to GPT-4, showing the potential of our fine-tuned model to achieve better results.We hypothesize that high-quality and diverse reasoning data indeed lead the model to good behavior/skills for analyzing and solving hard scientific problems instead of overfitting the training set, showing the general usability of our self-annotated SciInstruct dataset.</p>
<p>Conclusion</p>
<p>In this work, we present a self-instructive annotation framework to create a high-level and high-quality dataset, SciInstruct, to enrich the scientific knowledge of LLMs.Using SciInstruct, we train three LLMs, which significantly improve many scientific and mathematical benchmarks over the base models and outperform many state-of-the-art LLMs that have an order of magnitude more parameters.</p>
<p>Our research underscores the significance of diverse training data as well as LLM self-annotation and correctness for enhancing general reasoning capability, even for hard domains like science.</p>
<p>Limitation</p>
<p>In this section, we discuss more limitations during the research of SciInstruct.</p>
<p>Scale of Dataset and Model.Even though our training dataset has expanded to approximately 254k, improving model performance still necessitates access to an even larger dataset.Our model's experimental outcomes are carried out at 6∼8B and 32B parameters, leading to a relatively better performance.However, it's important to note that these performances are constrained by the model's scale.Moving forward, it's worth exploring the potential benefits of leveraging larger-scale datasets and models to further improve performance.</p>
<p>Using Data Classifier to Enhance the Generation of Models.In line with what was discussed in Section 2.3, we employ an instruction-quality classifier to boost the instruction quality, yielding improved performance as shown in Table 2.However, we anticipate that the instruction-quality classifier, also referred to as the reward model, could provide even greater benefits.One particular avenue of improvement could be bootstrapping data to improve the ability of the base model.</p>
<p>Broader Impact</p>
<p>Positive impact.This paper aims to construct high-level and high-quality instruction to improve the scientific reasoning capability of LLMs, which helps LLMs to better give the answers to questions at the college level.Collecting diverse instructions, annotating self-reflective instructions, and filtering out low-quality instructions provide researchers insights to prepare training datasets.</p>
<p>Negative impact.A drawback of this work is that the scale of the training dataset and model is relatively small, and we can address this by bootstrapping a more large training dataset.We believe that the benefits of data generation manner outweigh the downside.</p>
<p>A Appendix</p>
<p>A.1 Related Works</p>
<p>Recently, there have been advances to bridge the gaps in reasoning difficulty and evaluation subjects from three perspectives for scientific reasoning with LLMs.</p>
<p>High-level evaluation like SciBench [7] and GPQA [8], which evaluate the scientific reasoning capabilities of LLMs at the college level and even graduate level.In addition, SciEval [6] provides a multi-level LLMs evaluation benchmark to address data leakage problems and subjective question/answer evaluation ability issues.</p>
<p>Continued pre-training like Galactica [1] and MINERVA [42], which continue to train their respective base LLMs on multiple web texts including science-related or math-related corpus.This continued pre-training approach explores the potential of LLMs for science and contributes to the open-source models for the scientific community, but it is computationally expensive.</p>
<p>Dataset-specific fine-tuning like RFT [38], WizardMath [43], MAmmoTH [10], and MetaMath [11], which constructs certain datasets including GSM8K and MATH, conducts supervised fine-tuning of LLMs and evaluates the popular benchmarks GSM8K and MATH.MAmmoTH not only improves the in-domain (IND) performance like GSM8K but also generalizes to broader mathematical tasks by building a spectrum of math instruction tuning datasets including in-domain and out-of-domain datasets.This series of methods mainly focuses on mathematical reasoning tasks.</p>
<p>A.   Figure 10: We show the number of questions of that length and the average length of all answers under each question.</p>
<p>A.3 Instruction Examples</p>
<p>Instruction Format for Physics, Chemistry, and Math.To make our dataset consistent, we define a unified template for the problem-solving process that matches the logical flow of human thinking for Physics, Chemistry, and Math.For each question, we first list the basic analysis of which scientific concepts (or knowledge points) this question asks, and then present step-by-step reasoning steps of the detailed solution, and eventually summarize the answer.</p>
<p>This is an example of an instruction format.Problem: * * * .</p>
<p>Answer:</p>
<p>Analysis: This question examines * * * knowledge points.</p>
<p>Step 1: * * * .</p>
<p>Step i: * * * .</p>
<p>Step n: * * * .</p>
<p>To sum up, the answer to this question is * * * .</p>
<p>Instruction Format for Formal Math.</p>
<p>This is an example of an instruction format for Lean.Problem: theorem gcd_self (n : Nat) : gcd n n = n Answer: cases n &lt;;&gt; simp [gcd, mod_self]</p>
<p>A.4 GPT Labeling</p>
<p>As shown in Figure 11, we present the process of GPT-4 labeling.It is important to note that addressing the solution comparison by obtaining the accurate assessment of reasoning steps, such as through the use of a process reward model (PRM), typically requires costly human annotations, especially for complex scientific reasoning.Therefore, our work employs a labeling method based on an outcome reward model (ORM) as a basic implementation.While this approach may have limitations, it serves as a foundation for constructing the training set for our instruction-quality classifier.Moreover, Table 2 demonstrates the effectiveness of this classifier.</p>
<p>A.5 Dataset of Instruction-quality Classifier</p>
<p>As shown in Table 7, we list the data statistics used to train the instruction-quality classifier.</p>
<p>A.6 Detailed Error Analysis</p>
<p>As shown in Figure 12, we give detailed error examples, which are summarized in Section 2.3.</p>
<p>Self-reflection.Since our classifier is trained on the labeled dataset and generated solutions, the errors that occur in ChatGLM2-6B, GPT-3.5-turbo, and GPT-4's negative labeled solutions may strongly influence the classifier's performance.Therefore, we analyze the errors and categorize them into three main classes (Calculation mistakes, False reasoning, and Comprehensive mistakes), which are shown below in Figure 2.This means that our classifier has some ability to identify these errors in datasets.</p>
<p>GPT-4 Labeling</p>
<p>Prompt: The following input includes three segments.It starts with a question, followed by a step-by-step solution to the question, and ends with the true answer.If the answer obtained in the solution is equivalent to the real one, output '1', otherwise output '0'.</p>
<p>Input</p>
<p>Q: A photon moving at speed c, a person chasing light at 0.9c, that person observes the speed of light is?</p>
<p>Solution: According to Einstein's theory of special relativity, we cannot simply add up the velocities.In relativity, the superposition of velocities follows the Lorentz transformation, rather than the classical addition.For the speed of light, there is a special case in which the observer measures the speed of light as c, regardless of the observer's speed.This means that even if an observer is moving at 0.9c, the speed of light he measures is still c.Therefore, the speed of light observed by this person is c.</p>
<p>Real Answer: c.</p>
<p>GPT-4 Output: 1</p>
<p>Data Classifier.Indeed, real-world datasets are noised and present challenges.To improve the data quality and retain more useful datasets, we train a data classifier and rigorously filter the noised datasets.As mentioned, during training the data classifier, we randomly selected some questions Q from high-quality datasets annotated by annotators.Regarding positive samples, we select the accurate answers of GPT-3.5-turbo and GPT-4 for Q.Meanwhile, the error answer generated by GPT-3.5-turbo and GPT-4 can be viewed as negative samples.We train our classifier to output a scalar logit that ranges from -1 to 1. Higher scores represent more reliable and accurate answers.</p>
<p>Based on the trained data classifier, each data from noise datasets can receive a logit l that reflects its correctness and completeness, which can be further utilized for automatically ranking and selecting high-quality data.In Table 2, we demonstrate the effectiveness of instruction filtering by supervised fine-tuning on both filtered and unfiltered datasets at a 6B scale.Compared to models trained on unfiltered datasets, those trained on filtered datasets outperform the average tasks.This underscores the importance of data quality over data quantity for scientific tasks.For questions without thoughts, we use GPT-4 Turbo to supplement them with thoughts and retain the correct thought by comparing the generated answer with the true answer.We take a problem of physics subject as an example and show the analysis and labeling of GPT-4 Turbo.</p>
<p>5</p>
<p>Figure 11: We present the process of GPT-4 labeling.Given a prompt, question, solution, and real answer, GPT-4 can give the final label output.• CEval-Hard [28] comprises the eight subjects at the hard level from CEval.</p>
<p>• CEval-Sci [28] is selected from CEval including physical, chemical, and mathematical, subjects.</p>
<p>• MMLU-Sci [29] is similar to CEval-Sci and includes physical, chemical, and mathematical subjects selected from MMLU.</p>
<p>Comprehension mistakes Explanation</p>
<p>Answers that do not resolve the task exactly as instructed, often including an output that is insufficient.Example Q: A particle moving in a straight line in the x direction has the coordinates x = 5t 2 − t 3 at time t, where x is measured in meters and t in seconds.Find: displacement and average velocity in the 4th second.</p>
<p>A: The displacement in 4 seconds can be obtained by calculating the coordinates at t = 4 minus the coordinates at t = 0, Displacement = x(4) − x(0) = 5 × 4 2 − 4 3 − 5 × 0 2 + 0 3 = 5 × 16 − 64 − 0 + 0 = 80 − 64 = 16.Average velocity in 4 seconds can be obtained by calculating displacement divided by time, that is average_velocity = displacement/time = 16/4 = 4.In summary, the displacement in 4 seconds is 16 meters, the average speed is 4 meters/second, and the distance traveled by the particle is 16 meters.</p>
<p>Error: Wrong comprehension.The answer should be displacement and average velocity 'in the 4th second' rather than 'in 4 seconds'.</p>
<p>Calculation mistakes Explanation</p>
<p>Answers that fail to accurately calculate or simplify mathematical expressions, especially when encountering integration and differentiation.Example Q: In a shell with an inner radius of R1 and an outer radius of R2, the charge density in the shell is ρ = a + br, Where r is the distance from the center to the observation point, and there is no charge distribution elsewhere in the space.(a) Find the expression that the electric field everywhere in the space depends on r.</p>
<p>A: First of all, according to Gauss's law, the total electric flux of any closed surface inside the closed surface is equal to the total charge inside the closed surface.According to the conditions in the topic, the charge density in the shell is ρ = a + br, the total charge in the shell can be obtained as:
Q = R 2 R 1 π 0 2π 0 (a + br) • r 2 sin θdϕdθdr = 4π R 2 R 1 (a + br)r 2 dr = 4π a 4 (R 4 2 − R 4 1 ) + b 5 (R 5 2 − R 5 1
) .According to the Gauss theorem, inside the spherical shell, the flux of the electric field is zero.Therefore, we only need to produce the electric field outside the shell.Outside the shell, according to Coulomb's Law, the electric field generated by the charge can be expressed as:
E = 1 4πϵ 0 Q r 2 Substituting Qinto this formula yields: E = 1 4πϵ 0 4π( a 4 (R 4 2 −R 4 1 )+ b 5 (R 5 2 −R 5 1 )) r 2
simplified, we can get electric field depends on the expression of rare as follows:
E = 1 ϵ 0 a r 2 (R 4 2 − R 4 1 ) + b r 2 (R 5 2 − R 5 1 ) Error: Wrong integration.</p>
<p>False reasoning Explanation</p>
<p>Solutions consist of false reasoning steps or made-up information in reasoning.Example Q: The escape work of polonium and barium is 4.52 eV, and 2.50 eV, respectively.Calculate the cut-off frequencies of barium and polonium.A: According to the theory of photoelectric effect, the relationship between escape work and cutting potential can be expressed by the following formula escape work = cutting potential−photon energy.Apply the relationship between escape work and cutting potential to the case of barium and polonium.For polonium, cutting potential = escape work + photon energy = 4.52 eV + hν.For barium, cutting potential = escape work + photon energy = 2.50 eV + hν, where h is Planck's constant and ν is the frequency of light.In the visible range, the frequency range of light is approximately 4.3 × 10 14 Hz to 7.5 × 10 14 Hz.Now we can calculate the cut-off frequencies of barium and polonium respectively.For polonium, 4.52 eV + hνcut = 4.30 eV.hνcut = 4.30 eV − 4.52 eV.hνcut = −0.22eV.For barium, 2.50 eV + hνcut = 4.30 eV.hνcut = 4.30 eV − 2.50 eV.hνcut = 1.80 eV.In summary, the cut-off frequency of polonium is −0.22 eV, and that of barium is 1.80 eV.Because the cutoff frequency is the frequency of light and must be positive, barium can be used as a phototube cathode material in the visible range.Final answer: Barium can be used as a phototube cathode material in the visible range.Error: False reasoning, cutting potential should be W/h.</p>
<p>[7] Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou.Compositional semantic parsing with large language models.arXiv preprint arXiv:2209.15003,2022.</p>
<p>Figure 12: Error analysis of three types of self-reflection for noisy instruction filtering.</p>
<p>• SciEval [6] serves as a comprehensive, multi-disciplinary benchmark for assessing subjective question-answering capabilities.</p>
<p>• MATH [31] is a standard benchmark for evaluating the mathematical ability of LLMs.</p>
<p>• Mathematics [32] mainly includes Algebra, NumTheory, and Calculus.</p>
<p>• SAT-Math [33] is a math task including Intermediate Algebra, Probability, and Calculus.</p>
<p>• MMLU-Math [29] evaluation is the same as MAmmoTH [10], including Algebra, Probability, NumTheory, and Calculus.</p>
<p>• CEval-MATH [28] only includes mathematical subjects extracted from CEval.</p>
<p>A.8 Evaluation Baselines</p>
<p>• GPT-4 [35]: We consider the closed-source model GPT-4, which uses CoT prompting.</p>
<p>• GLM [13; 14] and LLaMA Base [37]: We select ChatGLM2-6B (Base), ChatGLM3-6B (Base), and ChatGLM3-32B (Base) as the base models.In addition, we consider LLaMA-2-7B and LLaMA-2-13B as baselines.</p>
<p>• Continue Pre-training: We include Galactica [1] although we note that its continued pre-training does not necessarily include college-level scientific reasoning.</p>
<p>• Dataset-specific Tuning: For the dataset-specific tuning, we consider WizardMath [43], MAmmoTH-CoT [10], and MetaMath [11], which adapt to the MATH dataset.</p>
<p>A.9 Detailed Experimental Results</p>
<p>Regarding domain-wise results in Figure 13, we present the detailed subject results in Table 9 and Table 10.We show several concrete examples in SciBench, as well as the output generated by SciGLM (32B) and ChatGLM3-32B-Base in Figure 14, Figure 15, Figure 16, Figure 17, and Figure 18.</p>
<p>Problem When an electron in a certain excited energy level in a one-dimensional box of length 2.00 \u00c5 makes a transition to the ground state, a photon of wavelength 8.79 nm is emitted.Find the quantum number of the initial state.</p>
<p>Correct Answer: 4</p>
<p>Predicted Solution by ChatGLM3-32B-Base (Served as baseline)</p>
<p>The ground state has n = 1.The energy in the n = 1 state is 1 L 2 .For the n = 2 state, this becomes 1 2L 2 .</p>
<p>Thus, this energy difference is 1 2 22 − 1 1 2 = 9 4 .By de Broglie's equation, hc/λ = 9 4hcν , so ν = 4hc 9λ .Thus, E = hν = hc 4 9λ .Therefore, n = 2, thus the answer is 2 .</p>
<p>Predicted Solution by SciGLM (Finetuned from ChatGLM3-32B-Base) Concept: The concept of this problem is based on the photoelectric effect.The photoelectric effect is the emission of electrons when light shines on a material.The photoelectric effect occurs when photons with energies greater than the binding energy of electrons cause the ejection of electrons from a material.When a photon is emitted by an electron, the energy of the photon is equal to the energy of the electron in the excited state minus the energy in the ground state.The energy of a photon is given by the equation E = hc/λ, where h is Planck's constant (6.626 × 10 −34 Js), c is the speed of light (3.000 × 10 8 m/s), and λ is the wavelength of the photon in meters.The energy of an electron in a one-dimensional box is given by the equation En = (n 2 × h 2 /8l 2 ), where n is the quantum number of the state and l is the length of the box in meters.The problem asks for the quantum number of the initial state.This can be found by setting the energies of the initial and final states equal to the energy of the photon.</p>
<p>Step 1: Convert the length of the box and the wavelength of the photon to meters.</p>
<p>-The length of the box is given as 2.00 Å.We need to convert this to meters by multiplying by 10 −10 (since 1 Å = 10 −10 m).</p>
<p>-The wavelength of the photon is given as 8.79 nm.We need to convert this to meters by multiplying by 10 −9 (since 1 nm = 10 −9 m).</p>
<p>Step 2: Calculate the energy of the photon.</p>
<p>-Use the equation E = hc/λ, where h is Planck's constant (6.626 × 10 −34 Js) and c is the speed of light (3.00 × 10 8 m/s).</p>
<p>Step 3: Set the energy of the final state (n = 1) equal to the energy of the photon and solve for n.</p>
<p>-Use the equation En = (n 2 * h 2 /8l 2 ), where l is the length of the box in meters.</p>
<p>Step 4: Convert the answer to scientific notation and round to 3 decimal places.</p>
<p>To sum up, the answer to the question is therefore n = 4.000 .✓ Figure 14: An example Quantum Chemistry problem in SciBench that has been solved accurately with SciGLM (32B).It shows that after instruction-tuning, the SciGLM learns skills/behaviors to first analyze the knowledge required for each problem, and then step-by-step solve the problem with correct formula and calculations.</p>
<p>A.11 Examples of SciInstruct</p>
<p>We present SciInstruct examples for Math (Figure 19), Physics (Figure 20), Chemistry (Figure 21), and Formal Proofs (Figure 22).</p>
<p>Problem Extend Example 1.4-6 to an n-sided die.That is, suppose that a fair n-sided die is rolled n independent times.A match occurs if side i is observed on the i th trial, i = 1, 2, . . ., n. Find the limit of this probability as n increases without bound.</p>
<p>Correct Answer: 1 − 1/e = 0.6321 . . .Predicted Solution by ChatGLM3-32B-Base We need the limit of P (A) as n goes to infinity.P (A) equals 1  n n as there are n possibilities for each of the n rolls.Thus we wish to find limn→∞ 1 n n = limx→∞ 1 x x = 0. Conclusion: The answer is therefore 0 .</p>
<p>Predicted Solution by SciGLM (Finetuned from ChatGLM3-32B-Base) The probability that all i sides of the die are rolled i times in a row is
1 n n = 1 n n .
Step 1: If we write the sum of the n terms of this geometric sequence from i = 1 to n, it equals 1 because each term in the sum is 1  n and there is a total of n terms being summed.Thus, if we want to find the probability that no match occurs, we sum all probabilities that no match occurs over k = 1 to n.This becomes n k=1
1 k n = 1 1 n + 1 2 n + 1 3 n + • • • + 1 n n
As n → ∞, the sum approaches a limiting term e −1 .Since the total probability must equal 1 for all n, and we just established that the limit of the sum of all probabilities that no match occurs is e −1 , this must be the probability that no match occurs at all.The probability that at least one match occurs is then 1 − e −1 .</p>
<p>Step 2: Thus, as n → ∞, the limit of the probability that no match occurs is e −1 = 0.367, and the limit of the probability that at least one match occurs is 1 − e −1 ≈ 0.63.</p>
<p>To sum up, the answer to the question is 0.63.</p>
<p>The limit of the probability that at least one match occurs is 0.63 .✓ Problem The World Series in baseball continues until either the American League team or the National League team wins four games.How many different orders are possible (e.g., ANNAAA means the American League team wins in six games) if the series goes four games?</p>
<p>Correct Answer Latex: 2 Correct Answer Number: 2</p>
<p>Answer generated by ChatGLM3-32B-Base</p>
<p>The answer is therefore 20 .</p>
<p>Answer generated by SciGLM (32B) Solution:</p>
<p>Step 1: American league team wins in 4 games ( 4 4 ) .National team wins in 4 games ( 4 4 ) .</p>
<p>Step 2: Total number of ways =( 4 4 )+( 4 4 )=2×1=2.</p>
<p>Conclusion: The answer is therefore 2 .✓ Problem 7.3-9.Consider the following two groups of women: Group 1 consists of women who spend less than 500 annually on clothes; Group 2 comprises women who spend over 1000 annually on clothes.Let p1 and p2 equal the proportions of women in these two groups, respectively, who believe that clothes are too expensive.2. The answer is therefore 7.605 .</p>
<p>Answer generated by SciGLM (32B) Solution:
∆Lz = h 2π ℏ mcℏc n(n−1) 2 = 1•(1−1) 2 = 0 2
= 0 The answer is therefore 0 .✓ Problem Two lightbulbs, one rated 30 W at 120 V and another rated 40 W at 120 V, are arranged in two different circuits.a.The two bulbs are first connected in parallel to a 120 V source.i. Determine the resistance of the bulb rated 30 W and the current in it when it is connected in this circuit.ii.Determine the resistance of the bulb rated 40 W and the current in it when it is connected to this circuit.</p>
<p>b.The bulbs are now connected in series with each other and have a 120 V source.i. Determine the resistance of the bulb rated 30 W and the current in it when it is connected in this circuit.ii.Determine the resistance of the bulb rated 40 W and the current in it when it is connected to this circuit.c.In the spaces below, number the bulbs in each situation described, in order of their brightness.(1= brightest, 4 = dimmest) <strong><em>_30 W bulb in the parallel circuit </em></strong>_40 W bulb in the parallel circuit <strong><em>_30 W bulb in the series circuit </em></strong>_40 W bulb in the series circuit d.Calculate the total power dissipated by the two bulbs in each of the following cases.i.The parallel circuit ii.The series circuit Answer Aanalysis: Use formulas to calculate resistance, current, and total power.(a) i: Calculate the resistance and current of the light bulb.P = V 2 /R given R=480 Ω and V=IR given I=0.25 A ii: Calculate the resistance and current of the light bulb.P = V 2 /R given R=360 Ω and V=IR given I=0.33A (b) i./ii.Calculate the resistance and current of the light bulb.The resistances are unchanged 480 Ω and 360 Ω.The total resistance in series is 480 Ω + 360 Ω = 840 Ω making the total current I = V/R = 0.14 A which is the same value for both resistors in series.</p>
<p>(c) Compare the brightness of the light bulb.The bulbs are brightest in parallel, where they provide their labeled values of 40 W and 30 W. In series, it is the larger resistor (the 30 W bulb) that glows brighter with a larger potential difference across it in series.This gives the order from top to bottom as 2 1 3 4.</p>
<p>(d) i: Calculate the total power consumed by two light bulbs.In parallel, they each operate at their rated voltage so they each provide their rated power andPT = 30W + 40W = 70W ii: Calculate the total power consumed by two light bulbs In series  A.12 Scientific Reasoning Datasets Mathematical Datasets.In the past, the emphasis in mathematical reasoning tasks was primarily on fundamental arithmetic reasoning datasets [44; 45; 46], addressing basic mathematical problems such as AddSub [47].Later on, to address realistic math word problems, some more difficult datasets are proposed [48; 49; 30; 50].To construct a more difficult and diversified dataset, NumGLUE [51] and LiLA [52] have been introduced to enhance current research.However, their focus remains primarily on elementary, middle school, and high school math problems.Given the rapid advancement of LLMs, to assess their capacity and constraints in addressing more intricate math problems, MMLU [29] with college-level math problems has been proposed.To address the more challenging college-level math problems, PoT [53; 7] is proposed.Our proposed SciInstruct covers more complex and more diversified math problems at the college level.
P T = V T 2 /R T = 17W In summary,(a)
Scientific Datasets.To address the scientific ability of LLMs, SciBench [7] proposes various solutions like calling existing tools (python and Wolfram).SciEval [6] presents a multi-level evaluation benchmark for scientific research.GPQA [8] builds a graduate-level Q&amp;A benchmark to evaluate more difficult physics and chemistry questions.</p>
<p>A.13 General Reasoning with LLMs</p>
<p>Assisted by Chain-of-Thought prompting [54; 55; 56] and Tree-or-Thought [57], LLMs have brought decent reasoning performance improvements.Especially, on the challenging BIG-Bench tasks, the CoT technique has already outperformed human ability [58].Later on, to address reasoning tasks, a few studies [59; 60; 61; 62; 63; 11; 64; 65; 66] propose various methods to leverage LLMs and retain step-by-step processes.With the rapid development of LLM Agents, several works [67; 17] propose to utilize external tools to enhance the reasoning capabilities of LLMs.For example, ReAct [67] attempts to call existing tools like web search engines to improve the reasoning skills of LLMs.Indeed, leveraging the programs as thought processes is a natural way.Several recent studies have utilized programs as thought processes, such as PoT [68], to improve the reasoning abilities of LLMs.To enhance LLMs' reasoning performance in solving mathematical or scientific problems with PoT, several methods have been suggested, including Self-critic [69], Self-eval [70], and Planand-solve [71].For example, self-critic [69] and self-eval [70] propose to adopt self-evaluation to improve the generated program's robustness.Different from these two methods, plan-and-solve [71] adopts more detailed planning instructions to generate a high-level reasoning plan for LLMs.Indeed, these methods have demonstrated that they can obtain great capabilities over PoT.</p>
<p>A.14 Instruction Tuning in LLMs</p>
<p>To align language models with human preferences and effective objectives, some works [17; 72; 66] design instruction tuning.The instruction tuning aims to mine the potential capabilities by aligning with and responding to human preference.Early on, instruction tuning focuses on improving the instruction-following abilities of LLMs for general purposes.Represented by FLAN [73] and T0 [74], they aim to understand the generalization abilities of LLMs for instruction tuning.Later, to comprehend the efficacy and performance of enlarging the instructional tuning datasets on models, FLAN-v2 [75; 76] has been proposed to validate this goal.However, these methods build training instruction tuning datasets in a human-annotated manner.Recently, various studies [77; 72; 78; 17; 39; 79] have started to construct synthetic instruction following datasets distilled from some LLMs like GPT-3.5-turbo and GPT-4.Like these works, Platypus [80] constructs a small-scale instruction for the following dataset by utilizing a domain-specialized dataset, aiming to enhance the reasoning capabilities of LLMs.</p>
<ol>
<li>
<p>If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted?) N/A 6.Will older version of the dataset continue to be supported/hosted/maintained?Yes, older versions of the dataset will continue to be maintained and hosted.</p>
</li>
<li>
<p>If others want to extend/augment/build on/contribute to the dataset, is there a mechanisms for them to do so?If others want to extend/augment/build on/contribute to the dataset, the most efficient way to reach us is via GitHub pull requests.For more questions, don't hesitate to get in touch with Dan Zhang (zd18@tsinghua.org.cn),who will be responsible for maintenance.</p>
</li>
</ol>
<p>B.4 Composition</p>
<ol>
<li>
<p>What do the instance that comprise the dataset represent (e.g., documents, photos, people, countries?)Each instance includes a question, corresponding answer, and subject.These attributes are used to fine-tune LLMs capable of college-level scientific reasoning.</p>
</li>
<li>
<p>How many instances are there in total (of each type, if appropriate)?The constructed SciInstruct dataset comprises 254,051 instructions, including 123,869 physics and chemistry, 89,934 math, and 40,248 formal proof (Lean).</p>
</li>
<li>
<p>Does the dataset contain all possible instances or is it a sample of instances from a larger set?</p>
</li>
</ol>
<p>The datasets contain all possible instances.</p>
<ol>
<li>
<p>Is there a label or target associated with each instance?Yes, each instance includes the corresponding subject.</p>
</li>
<li>
<p>Is any information missing from individual instances?No.</p>
</li>
<li>
<p>Are there recommended data splits (e.g., training, development/validation, testing)?We use all instances as a training set because there are various public benchmark datasets as test sets that cover a broad range of subjects.</p>
</li>
<li>
<p>Are there any errors, sources of noise, or redundancies in the dataset?No.</p>
</li>
<li>
<p>Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?The dataset is self-contained.</p>
</li>
<li>
<p>Does the dataset contain data that might be considered confidential?No.</p>
</li>
<li>
<p>Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?No.</p>
</li>
</ol>
<p>B.5 Collection Process</p>
<ol>
<li>
<p>How was the data associated with each instance acquired?The data associated with each instance is acquired from a variety of sources, including textbooks, pedagogical materials, and problem sets.References for these sources are provided in Table 7 in Appendix 7.2.</p>
</li>
<li>
<p>What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)?We first use Optical character recognition (OCR) to transform the question and final answer in the collected materials, then utilize a powerful LLM API to generate the intermediate reasoning, and finally filter out the traces with a correct predicted answer.</p>
</li>
<li>
<p>Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?Regular LLM researchers (e.g., undergraduate students, PhD students, and postdocs listed in the author list) at Tsinghua and Caltech were involved in the data collection process.</p>
</li>
<li>
<p>Does the dataset relate to people?No.</p>
</li>
<li>
<p>Did you collect the data from the individuals in questions directly, or obtain it via third parties or other sources (e.g., websites)?We obtained the dataset from textbooks, pedagogical materials, and problem sets.References for these sources are provided in Table 7 in Appendix 7.2.</p>
</li>
</ol>
<p>Figure 1: Domain</p>
<p>Filling</p>
<p>Figure 2 Figure 3 :
23
Figure 2: Question Type</p>
<p>Figure 4 :
4
Figure 4: The pipeline of constructing SciInstruct.On the far left is a mix of training datasets.The purpose of the annotation is to supplement chain-of-thought processes with reflective generation.The goal of the filter is to train an instruction-quality classifier and only keep high-quality reasoning traces as instructions.</p>
<p>Figure 5 :
5
Figure 5: The workflow and quantity of data for self-reflective instruction annotation framework.</p>
<p>4 Figure 6 :
46
Figure 6: Influence of different domains in fine-tuning data towards target tasks.Weight is calculated by (Acc.(SciGLM) -Acc.(Exclude subjects) / Acc.(SciGLM)) under the leave-one-out subject setting.</p>
<p>Figure 7 :
7
Figure 7: Performance improvement over different scale of instruction data.The x-axis represents the proportion of instruction per domain, and the y-axis represents the relative performance compared with SciGLM trained with the whole set.</p>
<p>Figure 8 :
8
Figure 8: Evaluating Pass@K on SciBench (Quantum Chemistry) and SciEval.All samples are generated at temperature 1.0.Results show that our instruction tuning does not influence the sample diversity, and increases the performance even with large K.</p>
<p>FillingFigure 9 :
9
Figure 9: We present the proportions of question types in all subjects and each subject.</p>
<p>Figure 13 :
13
Figure 13: SciGLM consistently improving the ChatGLM3-Base for different scientific tasks, without sacrificing general language understanding capabilities.</p>
<p>Figure 15 :
15
Figure 15: An example statistics problem in SciBench that is accurately solved with the SciGLM (32B).</p>
<p>Figure 16 :
16
Figure 16: A statistics example problem in SciBench that has been solved accurately with SciGLM (32B).</p>
<p>Figure 17 :
17
Figure 17: A statistics example problem in SciBench that has been solved accurately with SciGLM (32B).</p>
<p>Figure 18 :
18
Figure 18: A Quantum Chemistry example problem in SciBench that has been solved accurately with SciGLM (32B).</p>
<p>i: P = 480 Ω and V = 0.25A ii: P = 360 Ω and V = 0.33A (b) i/ii: P = 840 Ω and V = 0.14A (c) This gives the order from top to bottom as 2 1 3 4.(d) i: PT = 70W ii: P T = 17W</p>
<p>Figure 20 :
20
Figure 20: An instruction example on the physics subject.</p>
<p>Table 2 :
2
Ablation study of filter step.We arranged the samples by score and excluded the lowest 10%.The resulting numbers represent the average weighted accuracy on evaluation tasks.
TypeAve. Sci Ave. Math Ave. {Sci+Math}Unfiltered Filtered43.57 43.8548.50 49.2446.03 46.54</p>
<p>Table 3 :
3
Statistics of SciInstruct across subjects.
Subject# Number ProportionPhysics &amp; Chemistry Math Formal Proofs (Lean)123,869 89,934 40,24848.76% 35.40% 15.84%Total254,051</p>
<p>Table 4 :
4
Results
GPT-4 GPT-3.5-turbo Claude-v1.354.96 41.37 39.1460.55 46.83 44.64---73.93 66.97 63.4528.52 12.17 -39.70 --------(# parameter = 6B∼7B)LLaMA-2-7B Galactica-6.7B ChatGLM2-6B ChatGLM2-6B-Base ChatGLM3-6B ChatGLM3-6B-Base SciGLM (ChatGLM3-6B-Base) Llama3-8B-Instruct (zero-shot) Llama3-8B-Instruct (few-shot) + SciInstruct Mistral-7B: MetaMATH (zero-shot) Mistral-7B: MetaMATH (few-shot) + SciInstruct28.29  † 11.84  † 29.61  † 32.90  † 36.84  † 45.40  † 51.97 26.32  † 25.66  † 32.24 9.87  † 9.21  † 30.9230.00  † 11.44  † 45.71  † 40.95  † 38.57  † 54.29  † 60.00 27.62  † 23.33  † 34.76 8.57  † 19.52  † 38.1030.41 30.68 37.09  † 38.06  † 41.78  † 40.16  † 45.34 26.90  † 52.67  † 40.86 28.25  † 44.74  † 42.1628.37 50.87 53.02  † 50.38  † 56.56  † 61.69  † 62.09 71.38  † 71.38  † 66.47 63.61  † 63.61  † 64.160.40 -1.54  † 1.20  † 2.40  † 2.40  † 3.77 1.03  † 3.60  † 3.60 4.63  † 6.17  † 6.23----28.70 24.75 25.25 27.27  † 31.31  † 29.29 27.78  † 29.29  † 27.27----34.14 38.12 41.40 30.09 34.66 34.54 23.79 28.76 34.81----29.73 40.34 45.32 28.58 37.92 36.04 25.57 33.92 37.91(# parameter = 12B∼13B)LLaMA-2-13B Vicuna-13B19.74  † -19.05  † -35.85 32.1336.96 53.931.37 -26.20 -22.59 -22.13 -(# parameter = 30B∼32B)Galactica-30B ChatGLM3-32B-Base SciGLM (ChatGLM3-32B-Base)-53.95  † 56.58-64.29  † 66.1935.53 50.30  † 49.3854.96 67.38  † 69.84-4.29  † 5.15-22.22 25.76-43.74 45.48-48.62 51.473 Benchmark on SciInstruct
[28]cientific reasoning tasks.Experiments indicate that fine-tuning on SciInstruct consistently outperforms the base model across various parameter scales.Avg.Sci represents the weighted average score of all scientific tasks within the same evaluation category, while Avg.{Sci+Math} signifies the weighted average score on both scientific and mathematical tasks.Within each parameter setup, Bold spotlights the one with best performance, and Underline denotes the second-best result.Results marked as † are benchmarked by ourselves.ModelCEval-Hard CEval-Sci MMLU-Sci SciEval SciBench GPQA_Diamond Avg.Sci Avg.{Sci+Math} (API, parameter details unknown)3.1 Experimental SetupScientific and Mathematical Tasks.The evaluation tasks are summarized in Table8in Appendix A.7.These tasks have been chosen as out-of-domain benchmark datasets, encompassing CEval-Hard[28],</p>
<p>Table 5 :
5
Results on mathematical reasoning.The Avg. Math represents the weighted average score of all mathematical tasks within the evaluation category.Results marked as † are benchmarked by ourselves.
ModelGSM8K MATH Mathematics SAT-Math MMLU-Math CEval-Math Avg. Math(API, parameter details unknown)GPT-4 GPT-3.5-turbo Claude-v1.392.00 80.80 -42.50 34.10 ----95.00 70.90 ----53.91 40.81 37.66---(# parameter = 6B∼7B)LLaMA-2-7B Galactica-6.7B WizardMath-7B MAmmoTH (CoT)-7B MetaMath-7B MAmmoTH &amp; MetaMath-7B ChatGLM2-6B ChatGLM2-6B-Base ChatGLM3-6B ChatGLM3-6B-Base SciGLM (ChatGLM3-6B-Base) Llama3-8B-Instruct (zero-shot) Llama3-8B-Instruct (few-shot) + SciInstruct Mistral-7B: MetaMATH (zero-shot) Mistral-7B: MetaMATH (few-shot) + SciInstruct14.60 10.20 54.90 50.50 66.50 66.30 25.85 31.54 29.05 72.93 73.62 7.35  † 63.38  † 63.76 76.12  † 72.33  † 76.652.50 2.20 10.70 10.40 19.80 24.10 6.90  † 7.84  † 9.92  † 25.38  † 25.18 20.76  † 30.00  † 31.40 29.34  † 28.28  † 30.306.00 4.60 9.30 9.20 -18.30 14.30  † 17.10  † 11.60  † 29.30  † 31.80 4.50  † 22.30  † 22.60 23.90  † 24.40  † 25.0026.80 17.50 25.40 32.70 -41.40 39.55  † 34.55  † 39.09  † 55.91  † 65.46 58.64  † 57.27  † 43.64 15.45  † 50.45  † 43.8229.80 28.00 31.10 39.90 -44.40 38.91  † 40.45  † 41.07  † 31.83  † 49.38 54.52  † 55.24  † 42.81 10.37  † 42.40  † 41.4830.00  † 14.48  † ----36.67  † 32.22  † 21.11  † 40.00  † 50.00 16.67  † 18.89  † 21.11 8.89  † 16.67  † 28.8918.28 -----27.03 27.28 25.31 42.56 49.24 27.07 41.18 37.55 27.35 39.09 41.02(# parameter = 12B∼13B)LLaMA-2-13B Vicuna-13B WizardMath-13B MAmmoTH (CoT)-13B MAmmoTH &amp; MetaMath-13B28.70 28.40 63.90 56.30 71.043.90 5.80 14.00 12.90 26.1811.50 10.00 14.10 11.70 20.6032.70 34.00 24.50 43.60 48.1834.40 34.10 32.10 42.30 48.2518.89  † ----21.68 ----(# parameter = 30B∼32B)Galactica-30B ChatGLM3-32B-Base SciGLM (ChatGLM3-32B-Base)41.70 81.80 83.7012.70 31.60  † 32.8611.80 38.60  † 35.0037.70 67.73  † 76.3637.90 50.10  † 61.29-51.11  † 55.56-53.49 57.46</p>
<p>Table 6 :
6
Results on general language understanding tasks.Fine-tuning does not sacrifice most language tasks and only drops a bit on the code generation task.
ModelMMLU CEval MBPP Avg.GPT-486.4068.7083.00 79.37ChatGLM3-6B-Base SciGLM (6B-Base)61.32 61.3867.09 67.1655.80 61.40 45.00 57.85ChatGLM3-32B-Base SciGLM (32B-Base)69.05 70.0879.94 79.6458.20 69.06 56.60 68.78</p>
<p>2 SciInstruct Analysis Filling 33.2% Choice Question 32.0% Complex Solution Problem 20.1% Simple Solution Problem 14.6%
Question Type ProportionsComplex Solution Problem Question Type Proportions in Mathematics54.8%9.3% 0.4%Filling Choice Question35.5%Simple Solution Problem(a)(b)Question Type Proportions in PhysicsComplex Solution ProblemFilling12.2%35.0%17.7%Simple Solution Problem35.0%Choice Question(c)</p>
<p>Table 2 :
2
Ablation study on instruction filtering.Multiple questions only have the final answer while lacking the chain-ofthought process.Due to the scarce high-quality datasets, that is important to supplement the solutions.
TypeAve. Math Ave. Sci Ave. BenchUnfiltered Filtered39.03 40.8552.54 52.2843.53 44.662.1.3 Instruction OptimizationSupplement Thought.</p>
<p>Table 7 :
7
Composing data based on a limited set of labeled examples to train a data classifier and filter out noisy instructions.
Source# Positive Samples # Negative SamplesTotalOriginal Clean Data ChatGLM2-6B GPT-3.5-Turbo-0613 GPT-4-061311,553 0 745 3320 23,106 2075 3,89811,553 23,106 2,820 4,230Total12,63029,07941,709A.7 Evaluation Tasks</p>
<p>Table 8 :
8Biology)Eval Dataset # Samples Answer FormSubjectCEval-Hard152Multi-choice△, ▲, ♢, △, ♦, ⋆CEval-Sci210Multi-choice△, ♢, △, ▲, ♦, ⋆, ♣MMLU-Sci1,855Multi-choice  §, △, ♢, △, ▲, ♦, ⋆, ♣SciEval15,901Multi-choice♦, ⋆, ♣MATH5,000Open-formed†,  ‡,  §, △, ♢, ▲, ▼Mathematics1,000Open-formed†,  ‡, ♢, ▲SAT-Math220Multi-choice‡, △, ▼MMLU-Math974Multi-choice§, △, ♢, ▲CEval-Math90Multi-choice△, △, ♢, ▲
Overview of Scientific Evaluation Tasks.(†: Pre-Algebra, ‡: Inter-Algebra, §: Algebra, △: Probability, ♢: NumTheory, △: Statistics, ▲: Calculus, ▼: Geometry, ♦: Physics, ⋆: Chemistry, ♣:.</p>
<p>Table 9 :
9
Detailed results of physical subjects for Figure 3. Bold denotes the best result in the closed-source models and open-source models.
modelAve.Avg.CEval College Physics High School PhysicsMiddle School PhysicsAvg.College PhysicsMMLU Conceptual PhysicsMiddle School PhysicsSciEval Physics.ChatGLM3-6B-Base SciGLM (ChatGLM3-6B-Base)54.32 63.16 56.59 66.6742.11 47.3763.16 63.1684.21 89.4745.59 46.5239.22 34.3155.32 55.3234.44 41.0646.95 47.56ChatGLM3-32B-Base SciGLM (ChatGLM3-32B-Base) 69.65 80.70 67.85 78.9557.89 63.1678.95 78.95100.00 56.76 100.00 58.6142.16 41.1862.55 66.8157.62 57.6251.22 56.71</p>
<p>Table 10 :
10
Detailed results of chemical subjects for Figure 3. Bold denotes the best result in the closed-source models and open-source models.
modelAve.Avg.College ChemistryCEval High School ChemistryMiddle School ChemistryAvg.MMLU College ChemistryHigh School ChemistrySciEval Chemistry.ChatGLM3-6B-Base SciGLM (ChatGLM3-6B-Base)58.25 66.67 58.55 68.2545.83 41.6763.16 73.6895.00 95.0049.83 48.8445.00 47.0052.22 49.7555.76 57.27ChatGLM3-32B-Base SciGLM (ChatGLM3-32B-Base) 69.05 71.42 66.44 69.8441.67 45.8378.95 78.9595.00 95.0063.04 66.6751.00 57.0068.97 71.4360.15 61.40A.10 Output Examples of SciGLM</p>
<p>If 1009 out of a random sample of 1230 women from group 1 and 207 out of a random sample 340 from group 2 believe that clothes are too expensive, (a) Give a point estimate of p1 − p2.To find the point estimate of p1p2, which is the joint probability that a woman from group 1 and a woman from group2, believe that clothes are too expensive, we will use the number of women in each group that believe clothes are too expensive.From group1, we have that 10.0907 of women believe that clothes are too expensive, And from group 2, we have that 0.6076 of women believe that clothes are too expensive.The point estimate is approximately(123)=0.000000.The answer is therefore 0.000 .
Correct Answer Latex: $0.2115$ Correct Answer Number: 0.2115Answer generated by ChatGLM3-32B-BaseAnswer generated by SciGLM (32B) Solution: n1 = 1230, p1 = 1009 ≈ 0.819, n 1 n2 = 340, p2 ≈ 207 340 = 0.609 p1 − p2 ≈ 0.819 − 0.609 = 0.210The answer is therefore 0.210 . ✓
AcknowledgmentsThis work is supported by the NSFC 62276148, NSFC for Distinguished Young Scholar 62425601, a research fund from Zhipu, New Cornerstone Science Foundation through the XPLORER PRIZE and Tsinghua University (Department of Computer Science and Technology) -Siemens Ltd., China Joint Research Center for Industrial Intelligence and Internet of Things (JCIIOT).Part I Appendix Table of ContentsProblemUsing the method of undetermined coefficients to answer the below question.y ′′ − 2y ′ + 5y = sin x, y(0) = 1, y ′ (0) = 1 Answer Analysis: The key points of this problem are the undetermined coefficients and the differential equation of parameter variations.This test uses the Method of undetermined coefficients to solve differential equation problems.Step1: the auxiliary equation is r 2 − 2r + 5 = 0 with roots r = 1 ± 2i, so the complementary solution is: ιe(x) = e x (c1 cos 2x + c2 sin 2x).Try the particular solution:Step2: substituting, we have:and the general solution is:Step3: Thus the solution to the initial-value problem is:10 cos 2x − 1 20 sin 2x + 1 10 cos x + 1 5 sin x.To sum up, the answer is: y(x) = e x 9 10 cos 2x − 1 20 sin 2x + 1 10 cos x + 1 5 sin x.In summary, the weight percent of Ba is 47.35%,the weight percent of K is 8.28%, the weight percent of Cl is 31.95%.
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>arXiv:2311.07361Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4. 2023arXiv preprint</p>
<p>xtrimopglm: Unified 100b-scale pre-trained transformer for deciphering the language of protein. Bo Chen, Xingyi Cheng, Shen Yangli Ao Geng, Xin Li, Boyan Zeng, Jing Wang, Chiming Gong, Aohan Liu, Yuxiao Zeng, Jie Dong, Le Tang, Song, bioRxiv. 2023</p>
<p>Accurate medium-range global weather forecasting with 3d neural networks. Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, Qi Tian, Nature. 6192023</p>
<p>Learning a foundation language model for geoscience knowledge understanding and utilization. Cheng Deng, Tianhang Zhang, Zhongmou He, Qiyuan Chen, Yuanyuan Shi, Le Zhou, Luoyi Fu, Weinan Zhang, Xinbing Wang, Chenghu Zhou, arXiv:2306.050642023arXiv preprint</p>
<p>Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu, arXiv:2308.13149Scieval: A multi-level large language model evaluation benchmark for scientific research. 2023arXiv preprint</p>
<p>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Shichang Arjun R Loomba, Yizhou Zhang, Wei Sun, Wang, arXiv:2307.106352023arXiv preprint</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, arXiv:2311.12022Gpqa: A graduate-level google-proof q&amp;a benchmark. 2023arXiv preprint</p>
<p>Rest-mcts*: Llm self-training via process reward guided tree search. Dan Zhang, Sining Zhoubian, Yisong Yue, Yuxiao Dong, Jie Tang, arXiv:2406.038162024arXiv preprint</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.05653Mammoth: Building math generalist models through hybrid instruction tuning. 2023arXiv preprint</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, Metamath, arXiv:2309.12284Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>Xiang Yue, Tuney Zheng, Ge Zhang, Wenhu Chen, arXiv:2405.03548Mammoth2: Scaling instructions from the web. 2024arXiv preprint</p>
<p>Glm: General language model pretraining with autoregressive blank infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Glm-130b: An open bilingual pre-trained model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.024142022arXiv preprint</p>
<p>A I Meta, Meta llama 3. 2024</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang, arXiv:2310.128232023arXiv preprint</p>
<p>The Coq proof assistant reference manual: Version 6.1. PhD thesis, Inria. Bruno Barras, Samuel Boutin, Cristina Cornes, Judicaël Courant, Jean-Christophe Filliatre, Eduardo Gimenez, Hugo Herbelin, Gerard Huet, Cesar Munoz, Chetan Murthy, 1997</p>
<p>Isabelle/HOL: a proof assistant for higher-order logic. Tobias Nipkow, Markus Wenzel, Lawrence C Paulson, 2002Springer</p>
<p>The lean theorem prover (system description). Leonardo De Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, Jakob Von Raumer, Automated Deduction-CADE-25: 25th International Conference on Automated Deduction. Berlin, GermanySpringerAugust 1-7, 2015. 201525</p>
<p>LeanDojo: Theorem proving with retrieval-augmented language models. Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, Anima Anandkumar, Neural Information Processing Systems (NeurIPS). 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. S Sanmi Koyejo, A Mohamed, Danielle Agarwal, K Belgrave, A Cho, Oh, NeurIPS; New Orleans, LA, USA2022. 2022. November 28 -December 9, 2022, 2022</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Reflection-tuning: Data recycling improves llm instruction-tuning. Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Heng Huang, Jiuxiang Gu, Tianyi Zhou, arXiv:2310.117162023arXiv preprint</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yura Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, CoRR, abs/2305.20050Jan. 2023Bowen Baker, Teddy Lee</p>
<p>Measuring faithfulness in chain-of-thought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukosiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, ; Samuel, R Bowman, Ethan Perez, CoRR, abs/2307.13702Jan Brauner,. 2023Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan</p>
<p>Zero: Memory optimizations toward training trillion parameter models. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He, SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE2020</p>
<p>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, arXiv:2305.083222023arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Advancing mathematics by guiding human intuition with ai. Alex Davies, Petar Veličković, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomašev, Richard Tanburn, Peter Battaglia, Charles Blundell, András Juhász, Nature. 60078872021</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, arXiv:2304.063642023arXiv preprint</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.077322021arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Glm Team, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, arXiv:2406.12793A family of large language models from glm-130b to glm-4 all tools. 2024arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, Chang Zhou, arXiv:2308.01825Scaling relationship on learning mathematical reasoning with large language models. 2023arXiv preprint</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, arXiv:2305.11206Less is more for alignment. 2023arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Competition-level code generation with alphacode. Yujia Li, David H Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien De Masson D'autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, CoRR, abs/2203.07814Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Advances in Neural Information Processing Systems. 202235</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, arXiv:2308.09583Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023arXiv preprint</p>
<p>Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Transactions of the Association for Computational Linguistics. 32015</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, arXiv:1608.014132016arXiv preprint</p>
<p>Arkil Patel, Satwik Bhattamishra, Navin Goyal, arXiv:2103.07191Are nlp models really able to solve simple math word problems?. 2021arXiv preprint</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)2014</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, arXiv:1705.041462017arXiv preprint</p>
<p>Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, Mathqa, arXiv:1905.13319Towards interpretable math word problem solving with operation-based formalisms. 2019arXiv preprint</p>
<p>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Mcaleer, Albert Q Jiang, Jia Deng, Stella Biderman, Sean Welleck, arXiv:2310.10631Llemma: An open language model for mathematics. 2023arXiv preprint</p>
<p>Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, Ashwin Kalyan, arXiv:2204.056602022arXiv preprint</p>
<p>Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, arXiv:2210.17517A unified benchmark for mathematical reasoning. 2022arXiv preprint</p>
<p>Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, Tony Xia, arXiv:2305.12524Theoremqa: A theorem-driven question answering dataset. 2023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Challenging bigbench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.001142021arXiv preprint</p>
<p>Iteratively prompt pre-trained language models for chain of thought. Boshi Wang, Xiang Deng, Huan Sun, arXiv:2203.083832022arXiv preprint</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, arXiv:2212.100012022arXiv preprint</p>
<p>Making language models better reasoners with step-aware verifier. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Making large language models better reasoners with alignment. Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, Zhifang Sui, arXiv:2309.021442023arXiv preprint</p>
<p>Compositional semantic parsing with large language models. Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou, arXiv:2209.150032022arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, arXiv:2205.106252022arXiv preprint</p>
<p>Autore: Document-level relation extraction with large language models. Xue Lilong, Zhang Dan, Dong Yuxiao, Tang Jie, arXiv:2403.148882024arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382023arXiv preprint</p>
<p>An explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, arXiv:2111.020802021arXiv preprint</p>
<p>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, arXiv:2305.040912023arXiv preprint</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, arXiv:2304.12244Wizardlm: Empowering large language models to follow complex instructions. 2023arXiv preprint</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, arXiv:2110.082072021arXiv preprint</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>The flan collection: Designing data and methods for effective instruction tuning. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Barret Quoc V Le, Jason Zoph, Wei, arXiv:2301.136882023arXiv preprint</p>
<p>Self-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Baolin Peng, Chunyuan Li, arXiv:2304.03277Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. 2023arXiv preprint</p>
<p>How far can camels go? exploring the state of instruction tuning on open resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Raghavi Khyathi, David Chandu, Kelsey Wadden, Noah A Macmillan, Iz Smith, Beltagy, arXiv:2306.047512023arXiv preprint</p>
<p>Cole J Ariel N Lee, Nataniel Hunter, Ruiz, arXiv:2308.07317Platypus: Quick, cheap, and powerful refinement of llms. 2023arXiv preprint</p>
<p>For what purpose was the dataset created? Our benchmark dataset was created to address the data scarcity challenge in the science domain and provide a suite of scientific instructions for training scientific language models capable of college-level scientific reasoning. </p>
<p>Who created the dataset and on behalf of which entity? The dataset was developed by LLM researchers (undergraduate students, doctoral students, and postdocs) listed in the author list. </p>
<p>Who funded the creation of the dataset? This work is supported by the NSFC 62276148, NSFC for Distinguished Young Scholar 62425601. Cornerstone Science Foundation through the XPLORER PRIZE and Tsinghua University. Department of Computer Science and Technology) -Siemens Ltd., China Joint Research Center for Industrial Intelligence and Internet of Things (JCIIOT)</p>
<p>The dataset has been distributed through Hugging Face, Google Drive, Tsinghua Cloud, and the code used for developing baseline models through GitHub. API, GitHubHow will the dataset will be distributed (e.g., tarball on website</p>
<p>Have any third parties imposed IP-based or other restrictions on the data associated with the instances?. </p>
<p>Do any export controls or other regulatory restrictions apply to the dataset or to individual instances. </p>
<p>When will the dataset be distributed? It has been released now. </p>
<p>Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?. The dataset will be distributed under the CC BY 4.0 license</p>
<p>Who will be supporting/hosting/maintaining the dataset? Zhipu AI and THUDM will support, host, and maintain the dataset. </p>
<p>The owner/curator/manager(s) of the dataset can be contacted through the following emails: Dan Zhang (zd18@tsinghua.org.cn) and Sining Zhoubian. How can the owner/curator/manager of the dataset be contacted (e.g., email address. zbsn21@mails.tsinghua.edu.cn</p>
<p>to correct labeling errors, add new instances, delete instances)? Yes, the datasets will be updated whenever necessary to ensure accuracy, and announcements will be made accordingly. Will the dataset be updated. These updates will be posted on the main web page for the dataset</p>
<p>Has the dataset been used for any tasks already? Yes, this dataset has been used to generate new datasets, for example, to convert the text-formatted calculations into code formats and to fine-tune the code-based language models. </p>
<p>What (other) tasks could be the dataset be used for?. </p>
<p>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? The current composition of the datasets is self-sufficient to train a scientific language model. Any changes in the next release and updates will be documented and shared through the dataset webpage. </p>
<p>Are there tasks for which the dataset should not be used?. </p>            </div>
        </div>

    </div>
</body>
</html>
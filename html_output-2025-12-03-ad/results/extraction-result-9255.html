<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9255 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9255</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9255</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-270737974</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.18528v1.pdf" target="_blank">PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have revolutionized NLP research. Notably, in-context learning enables their use as evaluation metrics for natural language generation, making them particularly advantageous in low-resource scenarios and time-restricted applications. In this work, we introduce **PrExMe**, a large-scale **Pr**ompt **Ex**ploration for **Me**trics, where we evaluate more than 720 prompt templates for open-source LLM-based metrics on machine translation (MT) and summarization datasets, totalling over 6.6M evaluations. This extensive comparison (1) benchmarks recent open-source LLMs as metrics and (2) explores the stability and variability of different prompting strategies. We discover that, on the one hand, there are scenarios for which prompts are stable. For instance, some LLMs show idiosyncratic preferences and favor to grade generated texts with textual labels while others prefer to return numeric scores. On the other hand, the stability of prompts and model rankings can be susceptible to seemingly innocuous changes. For example, changing the requested output format from “0 to 100” to "-1 to +1” can strongly affect the rankings in our evaluation. Our study contributes to understanding the impact of different prompting approaches on LLM-based metrics for MT and summarization evaluation, highlighting the most stable prompting patterns and potential limitations.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9255.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9255.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BasePromptVariants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot (PZS) vs Zero-shot Chain-of-Thought (ZS-COT) vs Zero-shot Chain-of-Thought with Emotion (ZS-COT-EM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of three hierarchical base-prompt families (plain zero-shot, CoT, and CoT augmented with an emotion-description step) across open-source LLMs for MT and summarization evaluation, showing model-dependent preferences and measurable stability between some pairs of base prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PLATYPUS2-70B, OPENORCA-13B, TOWER-13B, LLAMA3-70B, LLAMA3-8B, NOUS-13B, MIXTRAL-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B / 13B / 13B / 70B / 8B / 13B / 8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reference-free machine translation (various language pairs from Eval4NLP/WMT) and summarization (SUMMEVAL/SEAHORSE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Segment-level automatic evaluation of generated hypotheses against source (no reference) with correlation to human judgments (Kendall as main measure).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Hierarchical prompts built from a base prompt layer: (1) Plain zero-shot (PZS) including task description, source, hypothesis, format requirement; (2) ZS-COT that asks the model to 'think step by step' before output; (3) ZS-COT-EM that additionally asks the model to 'describe emotions' before CoT. OS variants add a one-shot demonstration slot (RAG-selected).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>The three base prompt variants are compared against each other within the hierarchical-template grid; also compared indirectly to separate prompting techniques and baselines (LocalGemba, DSBA, BARTScore, XCOMET).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as Kendall correlations to human scores across tasks; hierarchical templates (including these base prompts) yielded top correlations per model (e.g., PLATYPUS2-70B reached highest correlations across many tasks per Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No single base prompt universally best; model-specific results: TOWER uses ZS-COT or ZS-COT-EM in 86.2% of its best prompts, PLATYPUS2 uses them in 23.9% of its best prompts, ORCA and many others favor PZS. Aggregate stability between PZS and ZS-COT (when changing base prompt) measured at Kendall = 0.65 (highest stability observed).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not given as a single numeric delta in primary metrics per base-prompt pair except the stability Kendall=0.65 for PZS<->ZS-COT; effects are model- and dataset-dependent (no universal effect size provided).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize model-specific instruction-tuning data and idiosyncratic preferences explain why some models benefit from CoT (or emotion priming) while others prefer plain prompts; CoT and emotion prompting may elicit internal reasoning/explanatory output that some models leverage for metric judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Grid search over >720 prompt templates, 7 LLMs; Phase 1 used Eval4NLP train split (first 500 MT samples per language pair) and ran 6.65M ZS prompts; Phase 2 evaluated selected top prompts on full dev/test sets. Kendall was main metric; permute-input significance tests (p ≤ 0.075) used for clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9255.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9255.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OutputFormat_Numeric_vs_Textual</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Output format: numeric score ranges and discrete numeric formats vs textual quality labels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The requested output format (numeric continuous ranges, discrete numeric buckets, or textual labels like 'good/neutral/bad' or 'catastrophic/indifferent/marvelous') strongly affects model performance and even relative model rankings; models show consistent idiosyncratic preferences for numeric vs textual outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPENORCA-13B (ORCA), PLATYPUS2-70B (PLATYPUS2), LLAMA3-70B, TOWER-13B, NOUS-13B, MIXTRAL</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B / 70B / 70B / 13B / 13B / 8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reference-free MT and summarization evaluation (Eval4NLP, WMT23, SEAHORSE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Score generated hypotheses for quality; prompts instruct models to return scores in various numeric ranges (e.g., 0-100, -100 to 100, -1.0 to 1.0, 0.0 to 1.0, 0-5) or textual labels (simple or complex).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Format requirement field varied across 10 templates: continuous numeric ranges (e.g., 0 to 100, -100 to 100, 0.0 to 1.0, -1.0 to 1.0), discrete numeric choices (0 or 1, -1/0/1), and textual label formats (simple labels 'bad/neutral/good' and complex labels with richer terms).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Formats were compared pairwise and via aggregated rankings (median aggregation across other prompt components). Examples compared: 0 to 100 vs -100 to 100 vs -1.0 to 1.0 vs textual labels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported indirectly: best prompts per model used different format types — e.g., ORCA and PLATYPUS2 were prompted to return numeric scores for all but one reported correlations; LLAMA3-70B used textual labels in 90.2% of its best prompts; TOWER used textual labels in 80.4% of its best prompts. Overall metric performance reported as Kendall correlations per model/task in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No single format dominated across models: numeric formats produced better performance for ORCA and PLATYPUS2, while LLAMA3 and TOWER achieved better correlations when prompted for textual labels. Some numeric-range swaps (e.g., 0 to 100 vs -100 to 100) produced minor ranking changes, while label-format swaps could produce large ranking changes.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>No single universal numeric effect size provided; comparisons are reported as changes in model ranking stability (qualitative and via Kendall correlations). Example stability: '0 to 100' vs '-100 to 100' produced little ranking change in one example; changing from 'simple labels' to 'complex labels' changed model ranking drastically (no single Δ-Kendall number given in text for this pair).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest that idiosyncratic preferences stem from differences in instruction-tuning data and how models map textual vs numeric outputs to internal scoring; some models are tuned to prefer producing textual labels, others numeric outputs, which impacts correlation with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Format-requirement distributions in top 2% prompts: LLAMA3-70B used textual labels in 90.2% of best prompts; ORCA used textual labels in only 8% of best prompts; PLATYPUS2 in 21.7%. Aggregation for ranking used median across other prompt components (found to be the most stable).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9255.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9255.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RangeSwapEffect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of changing numeric score ranges (example: '0 to 100' vs '-1 to +1')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small, seemingly innocuous changes to the numeric range requested in the prompt can strongly affect evaluation outcomes and relative model rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple open-source LLMs (evaluated ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (13B, 70B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reference-free MT and summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Models asked to return a continuous numeric score within a specified range for each hypothesis; compared across different numeric-range specifications.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompts specify different numeric score ranges (e.g., '0 to 100', '-100 to 100', '0.0 to 1.0', '-1.0 to 1.0').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Pairs such as '0 to 100' vs '-100 to 100' and '0 to 100' vs '-1 to +1' were analyzed for stability of model rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as changes in model ranking stability (Kendall correlation of model-ranking vectors). Example qualitative finding: changing '0 to 100' to '-1 to +1' can 'strongly affect the rankings' (quoted).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Example provided: changing '0 to 100' to '-100 to 100' produced little change in model ranking in one referenced case, whereas other range changes (e.g., 0-100 to -1 to +1) strongly affected ranking; exact Δ-Kendall not universally reported for every pair.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>No aggregate numeric effect size provided; effect reported qualitatively as 'strongly affect the rankings' for some swaps and 'not much' for others (examples given in Figures and text).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Format scaling and the model's calibration to numeric ranges influence how the model maps internal judgments to the requested range; mismatched preferred output format or range can remap relative scores and thus change rank correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Stability of model rankings under format changes analyzed using Kendall correlation across models; heatmaps (Figure 5 and Appendix J) visualize pairwise stability for many format pairs; median-aggregation used to compute rankings for format components.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9255.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9255.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OneShotRAG_vs_ZeroShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-shot (RAG-selected demonstrations) vs Zero-shot prompting comparison</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of retrieval-augmented one-shot demonstrations (RAG) against zero-shot prompts: OS (one-shot) showed weaker generalization on some datasets and higher no-score rates in some phases; resources limited OS to 9 selected prompts for phase 2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various open-source LLMs (OS experiments done with the 9 best ZS prompts, evaluated on same model set)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reference-free MT and summarization evaluation (Eval4NLP phases)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>One-shot demonstration chosen by retrieval (RAG) from WMT21/ROSE based on XLMR-SBERT cosine similarity; demonstration plus prompt used to grade input.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>One-shot OS-CoT and OS plain templates with retrieved in-context example inserted; RAG selects most similar demonstration per sample using sentence embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared OS (RAG) vs ZS across the same selected base prompts; OS tested only for 9 best ZS prompts due to resource limits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Phase 1: ZS prompts (720 templates) run 6,652,800 prompts with no-score extractions in 12.7%; OS prompts (71,280) had no-score in 19.4% of cases. Phase 2: ZS: 5,503,896 prompts (22.3% no-score), OS: 1,308,690 prompts (19.4% no-score). OS overall had weaker correlations than the best hierarchical ZS prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>OS had higher no-score rates in Phase 1 and did not consistently improve correlations; authors report OS prompts 'demonstrate a weak performance on the other datasets' and thus were not evaluated on WMT23/SEAHORSE.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>No single Δ-metric value; effect observed qualitatively as weaker correlations and higher failure/no-score rates for OS in several settings.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest OS demonstrations (RAG) may not generalize across datasets and that retrieval-chosen examples might not consistently help; resource constraints limited breadth of OS testing.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>RAG: demonstration selection via XLMR-SBERT sentence embeddings (concatenated source+hyp), highest cosine similarity per input. Only 9 best ZS prompts were used as templates for OS experiments due to compute limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9255.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9255.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LocalGemba_MQM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LocalGemba (MQM-inspired prompt-based approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of GEMBA-MQM prompting (originally used with GPT-4) in an open-source implementation (LocalGemba) to predict error-weighted MQM-style scores; compared to hierarchical template prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LocalGemba prompts run on selected open-source LLMs (e.g., PLATYPUS2-70B, ORCA-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (13B, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MQM-style machine translation evaluation (WMT tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict scores based on counting error types and severities following MQM-style guidelines via a structured prompt (GEMBA-MQM).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Structured MQM prompt that asks the model to detect and weight errors by severity and return aggregated scores (template replicated as LocalGemba).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared the LocalGemba MQM prompt against hierarchical template prompting and baselines (XCOMET, BARTScore, DSBA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LocalGemba (MQM) is in the best significance cluster for 3 of 11 tasks and is the best prompting-based approach for en-de in WMT23 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Despite successes on specific tasks (e.g., en-de WMT23), the separate prompting techniques (including LocalGemba) generally had weaker correlations than the best hierarchical-template prompts aggregated across models and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>No single numeric delta provided; task-dependent wins observed (LocalGemba best for en-de in WMT23), but not dominant across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that LocalGemba's structured MQM-style prompt may be complex and benefit from more capable LLMs (e.g., GPT-4 in original GEMBA), while open-source LLMs may struggle with the higher complexity of MQM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>LocalGemba is implemented locally, compared across same evaluation pipeline; significance clusters determined via permute-input tests (p ≤ 0.075).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9255.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9255.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aggregation_Median</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Median aggregation for ranking stability of prompt components</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The median aggregation of a prompting-pattern's performance across other prompt components (base prompt, task description, task) yields the most stable ranking when transferred across datasets/models, outperforming mean, max, min, saturation and other aggregations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Analytical method applied across all evaluated LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Meta-analysis across MT and summarization evaluation experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compute ranking of prompting-pattern variants (e.g., format requirement, task description) aggregated over other prompt dimensions and compare rankings before/after changing another dimension to quantify stability.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Aggregation methods compared: mean, median, mean of top 10%, max, min, saturation; permutation tests used to compare aggregation choices.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Median vs mean, vs max, vs saturation, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Median aggregation led to the most stable ranking as measured by higher Kendall correlations between rankings; permutation tests showed median significantly better (p ≤ 0.05) than many alternatives and remained significant after Bonferroni correction against saturation and standard deviation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Median notably outperformed other aggregation methods in stability tests (no single correlation number but significance matrix shown in Figure 6 and Appendix G).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Median reduces sensitivity to outlier prompt combinations (very good or very bad prompts) and thus better predicts a prompt-component's typical transfer performance across unseen datasets/models.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Stability tested by randomly swapping 50% of samples in permutation test; significance at p ≤ 0.05; analyses aggregated across tasks, models, and other prompt dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9255.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9255.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TaskDescription_Variation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task description / emotional priming and its instability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Exploration of different task instruction phrasings (neutral, polite, threatening, curious, 'dire warning', etc.) including emotion-inducing prompts; task-description component is less stable than format requirement and model performance depends on specific descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAMA3-70B, LLAMA3-8B, NOUS-13B, PLATYPUS2-70B, TOWER-13B, others</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B / 8B / 13B / 70B / 13B / various</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MT and summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Task descriptions varied across 24 templates (11 authored + 13 paraphrases from ChatGPT) with tones such as neutral, polite, threatening, curious, emphasis, dire warning, etc., to probe influence of wording and emotional priming.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Task description inserted into hierarchical prompt as the instruction for grading; some variants explicitely ask the model to create emotional descriptions (ZS-COT-EM variants include 'describe your emotions').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>All task-description variants compared; analyzed prevalence among top-performing prompts and stability across datasets and base prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Distributional findings: 'curious' description used in >15% of best prompts for LLAMA3-70B, NOUS, and LLAMA3-8B; 'emphasis' used in 17.4% of PLATYPUS2 best prompts; 'dire warning' used in 21.4% of TOWER best prompts. Overall, task-description ranking was less stable than format requirement ranking (e.g., a change from ZS to ZS-CoT often did not retain task-description ranking).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Task description yields more variable results across datasets and base prompts than format requirement; some descriptions swapped prevalence across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Different instruction wording can shift how models interpret the grading criteria and what features they attend to; emotional priming may change generation style or emphasis on certain aspects, leading to dataset- and model-specific effects.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>24 task descriptions tested; prevalence analyzed within top 2% of prompts per task; stability heatmaps (Figure 4 and Appendix J) compare rankings across base-prompt changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9255.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9255.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ModelRankingSensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of model ranking to prompt-format perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small prompt perturbations (especially format/label changes) can substantially reorder model rankings in evaluation, indicating that model comparisons can be highly prompt-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>All evaluated open-source LLMs (PLATYPUS2-70B, ORCA-13B, TOWER-13B, LLAMA3-70B/8B, NOUS-13B, MIXTRAL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B / 13B / 13B / 70B / 8B / 13B / 8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reference-free MT and summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Comparison of relative model quality (Kendall correlation to human judgments) and how prompt-format changes shift this ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Various prompt perturbations including format requirement swaps (numeric ranges and label types), base-prompt swaps (PZS vs ZS-COT), and task-description swaps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Ranking before vs after a single change (e.g., format requirement) measured by Kendall correlation; heatmaps visualize similarity between rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Empirical examples: PZS<->ZS-COT ranking stability = 0.65; changing format from '0 to 100' to '-100 to 100' often produced small ranking changes, whereas swapping 'simple labels' to 'complex labels' could produce large or even negative correlations of rankings (i.e., complete reorderings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Model rankings are not stable under many prompt perturbations; some perturbations produce high Kendall similarity, others produce random or strongly negative similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>No global numeric effect size; effect reported via Kendall correlations and heatmap visualizations illustrating many pairwise changes, including cases of strong negative correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors conclude that model comparisons that rely on a single prompt template are unreliable because minor prompt design choices can re-order model rankings; different models have idiosyncratic prompt preferences stemming from instruction-tuning and calibration differences.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Stability heatmaps (Figures 4,5 and Appendix J) computed by aggregating scores with median and computing Kendall between rankings when one prompt dimension is changed; permute-input significance testing used for reported clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>GEMBA-MQM: Detecting translation quality error spans with GPT-4 <em>(Rating: 2)</em></li>
                <li>Which is better? exploring prompting strategy for LLM-based metrics <em>(Rating: 2)</em></li>
                <li>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting <em>(Rating: 2)</em></li>
                <li>State of what art? a call for multi-prompt llm evaluation <em>(Rating: 2)</em></li>
                <li>Large language models are state-of-the-art evaluators of translation quality <em>(Rating: 1)</em></li>
                <li>Mind your format: Towards consistent evaluation of in-context learning improvements <em>(Rating: 1)</em></li>
                <li>Large language models understand and can be enhanced by emotional stimuli <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9255",
    "paper_id": "paper-270737974",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "BasePromptVariants",
            "name_full": "Zero-shot (PZS) vs Zero-shot Chain-of-Thought (ZS-COT) vs Zero-shot Chain-of-Thought with Emotion (ZS-COT-EM)",
            "brief_description": "Comparison of three hierarchical base-prompt families (plain zero-shot, CoT, and CoT augmented with an emotion-description step) across open-source LLMs for MT and summarization evaluation, showing model-dependent preferences and measurable stability between some pairs of base prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PLATYPUS2-70B, OPENORCA-13B, TOWER-13B, LLAMA3-70B, LLAMA3-8B, NOUS-13B, MIXTRAL-8x7B",
            "model_size": "70B / 13B / 13B / 70B / 8B / 13B / 8x7B",
            "task_name": "Reference-free machine translation (various language pairs from Eval4NLP/WMT) and summarization (SUMMEVAL/SEAHORSE)",
            "task_description": "Segment-level automatic evaluation of generated hypotheses against source (no reference) with correlation to human judgments (Kendall as main measure).",
            "presentation_format": "Hierarchical prompts built from a base prompt layer: (1) Plain zero-shot (PZS) including task description, source, hypothesis, format requirement; (2) ZS-COT that asks the model to 'think step by step' before output; (3) ZS-COT-EM that additionally asks the model to 'describe emotions' before CoT. OS variants add a one-shot demonstration slot (RAG-selected).",
            "comparison_format": "The three base prompt variants are compared against each other within the hierarchical-template grid; also compared indirectly to separate prompting techniques and baselines (LocalGemba, DSBA, BARTScore, XCOMET).",
            "performance": "Reported as Kendall correlations to human scores across tasks; hierarchical templates (including these base prompts) yielded top correlations per model (e.g., PLATYPUS2-70B reached highest correlations across many tasks per Table 2).",
            "performance_comparison": "No single base prompt universally best; model-specific results: TOWER uses ZS-COT or ZS-COT-EM in 86.2% of its best prompts, PLATYPUS2 uses them in 23.9% of its best prompts, ORCA and many others favor PZS. Aggregate stability between PZS and ZS-COT (when changing base prompt) measured at Kendall = 0.65 (highest stability observed).",
            "format_effect_size": "Not given as a single numeric delta in primary metrics per base-prompt pair except the stability Kendall=0.65 for PZS&lt;-&gt;ZS-COT; effects are model- and dataset-dependent (no universal effect size provided).",
            "explanation_or_hypothesis": "Authors hypothesize model-specific instruction-tuning data and idiosyncratic preferences explain why some models benefit from CoT (or emotion priming) while others prefer plain prompts; CoT and emotion prompting may elicit internal reasoning/explanatory output that some models leverage for metric judgments.",
            "null_or_negative_result": false,
            "experimental_details": "Grid search over &gt;720 prompt templates, 7 LLMs; Phase 1 used Eval4NLP train split (first 500 MT samples per language pair) and ran 6.65M ZS prompts; Phase 2 evaluated selected top prompts on full dev/test sets. Kendall was main metric; permute-input significance tests (p ≤ 0.075) used for clusters.",
            "uuid": "e9255.0",
            "source_info": {
                "paper_title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "OutputFormat_Numeric_vs_Textual",
            "name_full": "Output format: numeric score ranges and discrete numeric formats vs textual quality labels",
            "brief_description": "The requested output format (numeric continuous ranges, discrete numeric buckets, or textual labels like 'good/neutral/bad' or 'catastrophic/indifferent/marvelous') strongly affects model performance and even relative model rankings; models show consistent idiosyncratic preferences for numeric vs textual outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OPENORCA-13B (ORCA), PLATYPUS2-70B (PLATYPUS2), LLAMA3-70B, TOWER-13B, NOUS-13B, MIXTRAL",
            "model_size": "13B / 70B / 70B / 13B / 13B / 8x7B",
            "task_name": "Reference-free MT and summarization evaluation (Eval4NLP, WMT23, SEAHORSE)",
            "task_description": "Score generated hypotheses for quality; prompts instruct models to return scores in various numeric ranges (e.g., 0-100, -100 to 100, -1.0 to 1.0, 0.0 to 1.0, 0-5) or textual labels (simple or complex).",
            "presentation_format": "Format requirement field varied across 10 templates: continuous numeric ranges (e.g., 0 to 100, -100 to 100, 0.0 to 1.0, -1.0 to 1.0), discrete numeric choices (0 or 1, -1/0/1), and textual label formats (simple labels 'bad/neutral/good' and complex labels with richer terms).",
            "comparison_format": "Formats were compared pairwise and via aggregated rankings (median aggregation across other prompt components). Examples compared: 0 to 100 vs -100 to 100 vs -1.0 to 1.0 vs textual labels.",
            "performance": "Reported indirectly: best prompts per model used different format types — e.g., ORCA and PLATYPUS2 were prompted to return numeric scores for all but one reported correlations; LLAMA3-70B used textual labels in 90.2% of its best prompts; TOWER used textual labels in 80.4% of its best prompts. Overall metric performance reported as Kendall correlations per model/task in Table 2.",
            "performance_comparison": "No single format dominated across models: numeric formats produced better performance for ORCA and PLATYPUS2, while LLAMA3 and TOWER achieved better correlations when prompted for textual labels. Some numeric-range swaps (e.g., 0 to 100 vs -100 to 100) produced minor ranking changes, while label-format swaps could produce large ranking changes.",
            "format_effect_size": "No single universal numeric effect size provided; comparisons are reported as changes in model ranking stability (qualitative and via Kendall correlations). Example stability: '0 to 100' vs '-100 to 100' produced little ranking change in one example; changing from 'simple labels' to 'complex labels' changed model ranking drastically (no single Δ-Kendall number given in text for this pair).",
            "explanation_or_hypothesis": "Authors suggest that idiosyncratic preferences stem from differences in instruction-tuning data and how models map textual vs numeric outputs to internal scoring; some models are tuned to prefer producing textual labels, others numeric outputs, which impacts correlation with human judgments.",
            "null_or_negative_result": false,
            "experimental_details": "Format-requirement distributions in top 2% prompts: LLAMA3-70B used textual labels in 90.2% of best prompts; ORCA used textual labels in only 8% of best prompts; PLATYPUS2 in 21.7%. Aggregation for ranking used median across other prompt components (found to be the most stable).",
            "uuid": "e9255.1",
            "source_info": {
                "paper_title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RangeSwapEffect",
            "name_full": "Effect of changing numeric score ranges (example: '0 to 100' vs '-1 to +1')",
            "brief_description": "Small, seemingly innocuous changes to the numeric range requested in the prompt can strongly affect evaluation outcomes and relative model rankings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple open-source LLMs (evaluated ensemble)",
            "model_size": "various (13B, 70B, etc.)",
            "task_name": "Reference-free MT and summarization evaluation",
            "task_description": "Models asked to return a continuous numeric score within a specified range for each hypothesis; compared across different numeric-range specifications.",
            "presentation_format": "Prompts specify different numeric score ranges (e.g., '0 to 100', '-100 to 100', '0.0 to 1.0', '-1.0 to 1.0').",
            "comparison_format": "Pairs such as '0 to 100' vs '-100 to 100' and '0 to 100' vs '-1 to +1' were analyzed for stability of model rankings.",
            "performance": "Reported as changes in model ranking stability (Kendall correlation of model-ranking vectors). Example qualitative finding: changing '0 to 100' to '-1 to +1' can 'strongly affect the rankings' (quoted).",
            "performance_comparison": "Example provided: changing '0 to 100' to '-100 to 100' produced little change in model ranking in one referenced case, whereas other range changes (e.g., 0-100 to -1 to +1) strongly affected ranking; exact Δ-Kendall not universally reported for every pair.",
            "format_effect_size": "No aggregate numeric effect size provided; effect reported qualitatively as 'strongly affect the rankings' for some swaps and 'not much' for others (examples given in Figures and text).",
            "explanation_or_hypothesis": "Format scaling and the model's calibration to numeric ranges influence how the model maps internal judgments to the requested range; mismatched preferred output format or range can remap relative scores and thus change rank correlations.",
            "null_or_negative_result": false,
            "experimental_details": "Stability of model rankings under format changes analyzed using Kendall correlation across models; heatmaps (Figure 5 and Appendix J) visualize pairwise stability for many format pairs; median-aggregation used to compute rankings for format components.",
            "uuid": "e9255.2",
            "source_info": {
                "paper_title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "OneShotRAG_vs_ZeroShot",
            "name_full": "One-shot (RAG-selected demonstrations) vs Zero-shot prompting comparison",
            "brief_description": "Comparison of retrieval-augmented one-shot demonstrations (RAG) against zero-shot prompts: OS (one-shot) showed weaker generalization on some datasets and higher no-score rates in some phases; resources limited OS to 9 selected prompts for phase 2.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various open-source LLMs (OS experiments done with the 9 best ZS prompts, evaluated on same model set)",
            "model_size": "various",
            "task_name": "Reference-free MT and summarization evaluation (Eval4NLP phases)",
            "task_description": "One-shot demonstration chosen by retrieval (RAG) from WMT21/ROSE based on XLMR-SBERT cosine similarity; demonstration plus prompt used to grade input.",
            "presentation_format": "One-shot OS-CoT and OS plain templates with retrieved in-context example inserted; RAG selects most similar demonstration per sample using sentence embeddings.",
            "comparison_format": "Compared OS (RAG) vs ZS across the same selected base prompts; OS tested only for 9 best ZS prompts due to resource limits.",
            "performance": "Phase 1: ZS prompts (720 templates) run 6,652,800 prompts with no-score extractions in 12.7%; OS prompts (71,280) had no-score in 19.4% of cases. Phase 2: ZS: 5,503,896 prompts (22.3% no-score), OS: 1,308,690 prompts (19.4% no-score). OS overall had weaker correlations than the best hierarchical ZS prompts.",
            "performance_comparison": "OS had higher no-score rates in Phase 1 and did not consistently improve correlations; authors report OS prompts 'demonstrate a weak performance on the other datasets' and thus were not evaluated on WMT23/SEAHORSE.",
            "format_effect_size": "No single Δ-metric value; effect observed qualitatively as weaker correlations and higher failure/no-score rates for OS in several settings.",
            "explanation_or_hypothesis": "Authors suggest OS demonstrations (RAG) may not generalize across datasets and that retrieval-chosen examples might not consistently help; resource constraints limited breadth of OS testing.",
            "null_or_negative_result": true,
            "experimental_details": "RAG: demonstration selection via XLMR-SBERT sentence embeddings (concatenated source+hyp), highest cosine similarity per input. Only 9 best ZS prompts were used as templates for OS experiments due to compute limits.",
            "uuid": "e9255.3",
            "source_info": {
                "paper_title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LocalGemba_MQM",
            "name_full": "LocalGemba (MQM-inspired prompt-based approach)",
            "brief_description": "Application of GEMBA-MQM prompting (originally used with GPT-4) in an open-source implementation (LocalGemba) to predict error-weighted MQM-style scores; compared to hierarchical template prompting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LocalGemba prompts run on selected open-source LLMs (e.g., PLATYPUS2-70B, ORCA-13B)",
            "model_size": "various (13B, 70B)",
            "task_name": "MQM-style machine translation evaluation (WMT tasks)",
            "task_description": "Predict scores based on counting error types and severities following MQM-style guidelines via a structured prompt (GEMBA-MQM).",
            "presentation_format": "Structured MQM prompt that asks the model to detect and weight errors by severity and return aggregated scores (template replicated as LocalGemba).",
            "comparison_format": "Compared the LocalGemba MQM prompt against hierarchical template prompting and baselines (XCOMET, BARTScore, DSBA).",
            "performance": "LocalGemba (MQM) is in the best significance cluster for 3 of 11 tasks and is the best prompting-based approach for en-de in WMT23 (Table 2).",
            "performance_comparison": "Despite successes on specific tasks (e.g., en-de WMT23), the separate prompting techniques (including LocalGemba) generally had weaker correlations than the best hierarchical-template prompts aggregated across models and tasks.",
            "format_effect_size": "No single numeric delta provided; task-dependent wins observed (LocalGemba best for en-de in WMT23), but not dominant across tasks.",
            "explanation_or_hypothesis": "Authors hypothesize that LocalGemba's structured MQM-style prompt may be complex and benefit from more capable LLMs (e.g., GPT-4 in original GEMBA), while open-source LLMs may struggle with the higher complexity of MQM prompts.",
            "null_or_negative_result": false,
            "experimental_details": "LocalGemba is implemented locally, compared across same evaluation pipeline; significance clusters determined via permute-input tests (p ≤ 0.075).",
            "uuid": "e9255.4",
            "source_info": {
                "paper_title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Aggregation_Median",
            "name_full": "Median aggregation for ranking stability of prompt components",
            "brief_description": "The median aggregation of a prompting-pattern's performance across other prompt components (base prompt, task description, task) yields the most stable ranking when transferred across datasets/models, outperforming mean, max, min, saturation and other aggregations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Analytical method applied across all evaluated LLMs",
            "model_size": "n/a",
            "task_name": "Meta-analysis across MT and summarization evaluation experiments",
            "task_description": "Compute ranking of prompting-pattern variants (e.g., format requirement, task description) aggregated over other prompt dimensions and compare rankings before/after changing another dimension to quantify stability.",
            "presentation_format": "Aggregation methods compared: mean, median, mean of top 10%, max, min, saturation; permutation tests used to compare aggregation choices.",
            "comparison_format": "Median vs mean, vs max, vs saturation, etc.",
            "performance": "Median aggregation led to the most stable ranking as measured by higher Kendall correlations between rankings; permutation tests showed median significantly better (p ≤ 0.05) than many alternatives and remained significant after Bonferroni correction against saturation and standard deviation.",
            "performance_comparison": "Median notably outperformed other aggregation methods in stability tests (no single correlation number but significance matrix shown in Figure 6 and Appendix G).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Median reduces sensitivity to outlier prompt combinations (very good or very bad prompts) and thus better predicts a prompt-component's typical transfer performance across unseen datasets/models.",
            "null_or_negative_result": false,
            "experimental_details": "Stability tested by randomly swapping 50% of samples in permutation test; significance at p ≤ 0.05; analyses aggregated across tasks, models, and other prompt dimensions.",
            "uuid": "e9255.5",
            "source_info": {
                "paper_title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "TaskDescription_Variation",
            "name_full": "Task description / emotional priming and its instability",
            "brief_description": "Exploration of different task instruction phrasings (neutral, polite, threatening, curious, 'dire warning', etc.) including emotion-inducing prompts; task-description component is less stable than format requirement and model performance depends on specific descriptions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLAMA3-70B, LLAMA3-8B, NOUS-13B, PLATYPUS2-70B, TOWER-13B, others",
            "model_size": "70B / 8B / 13B / 70B / 13B / various",
            "task_name": "MT and summarization evaluation",
            "task_description": "Task descriptions varied across 24 templates (11 authored + 13 paraphrases from ChatGPT) with tones such as neutral, polite, threatening, curious, emphasis, dire warning, etc., to probe influence of wording and emotional priming.",
            "presentation_format": "Task description inserted into hierarchical prompt as the instruction for grading; some variants explicitely ask the model to create emotional descriptions (ZS-COT-EM variants include 'describe your emotions').",
            "comparison_format": "All task-description variants compared; analyzed prevalence among top-performing prompts and stability across datasets and base prompts.",
            "performance": "Distributional findings: 'curious' description used in &gt;15% of best prompts for LLAMA3-70B, NOUS, and LLAMA3-8B; 'emphasis' used in 17.4% of PLATYPUS2 best prompts; 'dire warning' used in 21.4% of TOWER best prompts. Overall, task-description ranking was less stable than format requirement ranking (e.g., a change from ZS to ZS-CoT often did not retain task-description ranking).",
            "performance_comparison": "Task description yields more variable results across datasets and base prompts than format requirement; some descriptions swapped prevalence across datasets.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Different instruction wording can shift how models interpret the grading criteria and what features they attend to; emotional priming may change generation style or emphasis on certain aspects, leading to dataset- and model-specific effects.",
            "null_or_negative_result": null,
            "experimental_details": "24 task descriptions tested; prevalence analyzed within top 2% of prompts per task; stability heatmaps (Figure 4 and Appendix J) compare rankings across base-prompt changes.",
            "uuid": "e9255.6",
            "source_info": {
                "paper_title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ModelRankingSensitivity",
            "name_full": "Sensitivity of model ranking to prompt-format perturbations",
            "brief_description": "Small prompt perturbations (especially format/label changes) can substantially reorder model rankings in evaluation, indicating that model comparisons can be highly prompt-dependent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "All evaluated open-source LLMs (PLATYPUS2-70B, ORCA-13B, TOWER-13B, LLAMA3-70B/8B, NOUS-13B, MIXTRAL)",
            "model_size": "70B / 13B / 13B / 70B / 8B / 13B / 8x7B",
            "task_name": "Reference-free MT and summarization evaluation",
            "task_description": "Comparison of relative model quality (Kendall correlation to human judgments) and how prompt-format changes shift this ordering.",
            "presentation_format": "Various prompt perturbations including format requirement swaps (numeric ranges and label types), base-prompt swaps (PZS vs ZS-COT), and task-description swaps.",
            "comparison_format": "Ranking before vs after a single change (e.g., format requirement) measured by Kendall correlation; heatmaps visualize similarity between rankings.",
            "performance": "Empirical examples: PZS&lt;-&gt;ZS-COT ranking stability = 0.65; changing format from '0 to 100' to '-100 to 100' often produced small ranking changes, whereas swapping 'simple labels' to 'complex labels' could produce large or even negative correlations of rankings (i.e., complete reorderings).",
            "performance_comparison": "Model rankings are not stable under many prompt perturbations; some perturbations produce high Kendall similarity, others produce random or strongly negative similarity.",
            "format_effect_size": "No global numeric effect size; effect reported via Kendall correlations and heatmap visualizations illustrating many pairwise changes, including cases of strong negative correlations.",
            "explanation_or_hypothesis": "Authors conclude that model comparisons that rely on a single prompt template are unreliable because minor prompt design choices can re-order model rankings; different models have idiosyncratic prompt preferences stemming from instruction-tuning and calibration differences.",
            "null_or_negative_result": true,
            "experimental_details": "Stability heatmaps (Figures 4,5 and Appendix J) computed by aggregating scores with median and computing Kendall between rankings when one prompt dimension is changed; permute-input significance testing used for reported clusters.",
            "uuid": "e9255.7",
            "source_info": {
                "paper_title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "GEMBA-MQM: Detecting translation quality error spans with GPT-4",
            "rating": 2,
            "sanitized_title": "gembamqm_detecting_translation_quality_error_spans_with_gpt4"
        },
        {
            "paper_title": "Which is better? exploring prompting strategy for LLM-based metrics",
            "rating": 2,
            "sanitized_title": "which_is_better_exploring_prompting_strategy_for_llmbased_metrics"
        },
        {
            "paper_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting",
            "rating": 2,
            "sanitized_title": "quantifying_language_models_sensitivity_to_spurious_features_in_prompt_design_or_how_i_learned_to_start_worrying_about_prompt_formatting"
        },
        {
            "paper_title": "State of what art? a call for multi-prompt llm evaluation",
            "rating": 2,
            "sanitized_title": "state_of_what_art_a_call_for_multiprompt_llm_evaluation"
        },
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality",
            "rating": 1,
            "sanitized_title": "large_language_models_are_stateoftheart_evaluators_of_translation_quality"
        },
        {
            "paper_title": "Mind your format: Towards consistent evaluation of in-context learning improvements",
            "rating": 1,
            "sanitized_title": "mind_your_format_towards_consistent_evaluation_of_incontext_learning_improvements"
        },
        {
            "paper_title": "Large language models understand and can be enhanced by emotional stimuli",
            "rating": 1,
            "sanitized_title": "large_language_models_understand_and_can_be_enhanced_by_emotional_stimuli"
        }
    ],
    "cost": 0.019036,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation
26 Jun 2024</p>
<p>Christoph Leiter christoph.leiter@uni-mannheim.de 
Natural Language Learning Group (NLLG) https://nl2g.github.io
University of Mannheim</p>
<p>Steffen Eger steffen.eger@uni-mannheim.de 
Natural Language Learning Group (NLLG) https://nl2g.github.io
University of Mannheim</p>
<p>PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation
26 Jun 2024C927D8A2023D544B4C5F1CC519CB6422arXiv:2406.18528v1[cs.CL]
Large language models (LLMS) have revolutionized the field of NLP.Notably, their incontext learning capabilities also enable their use as evaluation metrics for natural language generation, making them particularly advantageous in low-resource scenarios and timerestricted applications.In this work, we introduce PrExMe, a large-scale prompt exploration for metrics, where we evaluate more than 720 prompt templates for open-source LLM-based metrics on machine translation (MT) and summarization datasets, totalling over 6.6M evaluations.This extensive comparison (1) serves as a benchmark of the performance of recent open-source LLMS as metrics and (2) explores the stability and variability of different prompting strategies.We discover that, on the one hand, there are scenarios for which prompts are stable.For instance, some LLMS show idiosyncratic preferences and favor to grade generated texts with textual labels while others prefer to return numeric scores.On the other hand, the stability of prompts and model rankings can be susceptible to seemingly innocuous changes.For example, changing the requested output format from "0 to 100" to "-1 to +1" can strongly affect the rankings in our evaluation.Our study contributes to understanding the impact of different prompting approaches on LLM-based metrics for MT and summarization evaluation, highlighting the most stable prompting patterns and potential limitations. 1</p>
<p>Introduction</p>
<p>The recent popularity and success of LLMS have led to a paradigm shift in NLP (Zhang et al., 2023).Instruction-tuning allows LLMS to generate responses to complex task descriptions (prompts) (Ouyang et al., 2022), making them useful for conventional NLP tasks.One such task is the automatic evaluation of natural language generation (NLG) models in machine translation (MT) and summarization.Following the current trend, researchers use LLMS as evaluation metrics and achieve remarkable performance, sometimes relying solely on in-context learning (e.g.Kocmi and Federmann, 2023a;Fernandes et al., 2023), i.e., with metrics that are purely based on prompting.Such prompting-based metrics require no or only a few data samples, making them useful for lowresource evaluation scenarios (Belouadi and Eger, 2023).Additionally, they are often more resourceefficient since they do not require fine-tuning.</p>
<p>Although many prompting-based metrics have been proposed (e.g.Li et al., 2024b), structured evaluations across different prompting approaches remain scarce, especially for open-source models.In recent work, the EVAL4NLP 2023 shared task (Leiter et al., 2023) addresses this by (1) restricting the usage to selected open-source LLMs and (2) prohibiting the fine-tuning of these models.While the shared-task submissions provide several interesting findings, they focus on a few distinct prompts only.Notably, the effect and robustness of prompt variations on the same model or across different models remain largely unexplored.</p>
<p>In this work, we introduce a systematic Prompt Exploration for Metrics (PrExMe), that builds upon EVAL4NLP 2023, to provide a much larger, template-based, structured evaluation of the effects different input prompts have on an LLM-based metric's correlation with human judgements in MT and summarization evaluation.We formulate the following research questions:</p>
<p>RQ1 Can open-source language models evaluate text generation without fine-tuning and how do they differ from each other?</p>
<p>RQ2 Can we identify patterns 2 in prompts that lead to a stable performance across different datasets, tasks, and models?</p>
<p>RQ3 How should researchers design prompts for new evaluation scenarios?</p>
<p>Our prompt exploration constructs hierarchical templates based on approaches such as chain-ofthought (COT) (Kojima et al., 2022), zero-shot and retrieval-augmented generation (RAG) (Gao et al., 2024b).Each template gets filled with further subtemplates.For example, we vary the requested output formats, such as distinct scores and continuous scores (see §3).This setup amounts to more than 720 prompt templates that we evaluate with 7 LLMS.In a 2nd phase, we test the generalizability and performance of the prompts with the best correlations on two further datasets.</p>
<p>In summary, our work makes the following key contributions and findings:</p>
<p>✓ We perform a large-scale analysis (evaluating over 6.6M prompts) of the effect of different prompting approaches on LLM-based metrics for MT and summarization evaluation.This comprehensive exploration includes various prompting techniques, datasets, tasks, and models, making it, to our knowledge, the most extensive evaluation of its kind.✓ We show that certain prompting patterns are robust and generalizable across different tasks and datasets, with the median performance being a good predictor for new settings.For example, some models show a distinctive preference to return textual labels, while others achieve better results with numeric labels.On the other hand for some settings even small changes to the input prompt can strongly affect the performance. 2We define prompting patterns as the template components that constitute a prompt (e.g., zero-shot, one-shot or the output format).</p>
<p>✓ Our study tackles prompt-based evaluation with</p>
<p>open-source LLMs, targeting scenarios where fine-tuning or access to closed-source LLMs is not possible.Such evaluations are still very scarce but important to make research more accessible, fostering diversity and inclusion.✓ By systematically testing various established prompting approaches, including zero-shot, CoT and RAG, we comprehensively evaluate the performance of recent open-source LLMs for evaluation metrics.Aligning with the recommendations of Mizrahi et al. (2024), by evaluating each model with multiple prompts, our LLM comparison is fair because we mitigate the risk of any single prompt disproportionately affecting their performance.We find that the model PLATYPUS2-70B (Lee et al., 2023a) achieves the strongest performance for the tested LLMs.</p>
<p>Related Work</p>
<p>We first describe the related work of promptingbased metrics for MT and summarization.Then, we relate our work to research on prompting techniques and prompt stability.</p>
<p>Prompting-based metrics Recent advancements in LLM-based metrics for NLG often rely on incontext learning, directly predicting quality judgments from generated texts.Surveys by Li et al. (2024b) and Gao et al. (2024a) provide comprehensive overviews of these metrics.Besides BARTSCORE (Yuan et al., 2021) and PRD (Li et al., 2024a), the prompt-based approaches surveyed by Li et al. (2024b) are built upon closedsource models.In contrast, the EVAL4NLP 2023 shared task (Leiter et al., 2023), explicitly considers open-source prompt-based metrics, by asking participants to evaluate MT and summarization using only provided models without fine-tuning.The best submissions were able to beat strong baselines such as GEMBA (Kocmi and Federmann, 2023b) for MT and BARTSCORE for summarization.While the shared task yielded interesting techniques, the participants explored a limited range of prompts, leaving a gap in the comprehensive analysis of prompting patterns and the consistent comparison of LLMs.In this work, we fill this gap and systematically analyze a much larger set of prompts on a comparable grid of experimental settings to (1) study the robustness of prompts across datasets, models and tasks, and to (2) search for rules and patterns that can guide the future con-struction of prompt-based metrics.</p>
<p>Prompting Techniques Many successful prompting techniques have been proposed over the last years (e.g., Liu et al., 2023a).Our work mostly relies on established approaches such as Zero-Shot CoT and RAG.Further, Li et al. (2023) propose emotion inducing prompts to improve LLM performance.To our best knowledge, we are the first to analyze this technique for evaluation metrics.Inspired by this, we also propose a novel emotion-CoT pattern (see §3). Prior evaluation of output formats for prompt-based metrics is done by Kocmi and Federmann (2023b), which we extend by our much broader evaluation.Other works also use hierarchical templates for prompt building (e.g.Fu et al., 2023) and tools like LangChain (Chase, 2022) and DSPy (Khattab et al., 2023) support their implementation.We use hierarchical templates as means for a structured comparison among prompting patterns.</p>
<p>Prompting Robustness As we conduct a grid search across different prompts, datasets and tasks, our work builds upon and extends research on how LLMS respond to prompt perturbations.Webson and Pavlick (2022), Leidinger et al. (2023), Weber et al. (2023) and Sclar et al. (2023) find a wide range of performance variation for natural language inference and sentiment classification.As a solution, Sclar et al. (2023) suggest to provide the full range of results across different prompt perturbations.Voronov et al. (2024) and Mizrahi et al. (2024) suggest that current evaluation benchmarks for LLMS are problematic as they often only provide one prompt template per task.This could be solved by providing multiple templates and evaluating the ensemble.To our best knowledge, we are the first to explore to which degree these robustness problems affect open-source LLM-based metrics and how to select the best prompts for them.Also, by prompting the LLMs with multiple prompts, we follow Mizrahi et al. (2024) and achieve a stable and fair evaluation of LLMs for this task.</p>
<p>Setup</p>
<p>In this section, we present the templates and prompting techniques we employ for utilizing LLMS as metrics.Additionally, we provide an overview of the datasets and models that we use for testing.We evaluate LLMS in a reference-free setting, i.e., they grade a generated hypothesis based on its source without a reference. 3The evaluated prompt types provide a comprehensive evaluation framework for LLM-based metrics.This range covers basic in-context learning, sophisticated reasoning, emotional context, and varying output structures, ensuring a thorough assessment of robustness and adaptability across tasks and datasets.</p>
<p>Prompt Templates Our prompts are constructed as hierarchical templates (see Figure 1), i.e., one large template is constructed from multiple smaller ones.Each prompt is constructed from: (1) the source text and generated hypothesis text that should be graded, (2) a base prompt, (3) a task description, (4) a format requirement and (5) optionally a one-shot demonstration.Table 1 presents examples for (2), (3), ( 4) and ( 5).</p>
<p>The base prompt is the top layer of our prompt hierarchy, incorporating the other components.Specifically, we test three zero-shot (ZS) and corresponding one-shot (OS) base prompts: (1) Plain ZS/OS (PZS/POS), (2) ZS/OS-COT and (3) ZS/OS-CoT-Emotion (ZS/OS-COT-EM).PZS plainly presents the newline separated task description, source, hypothesis and format requirement.ZS-COT (KOJIMA ET AL., 2022) additionally asks the model to think step by step before returning its output.Lastly, ZS-COT-EM asks the model to describe its "emotions" before the ZS-CoT prompt.We include COT as it has improved the promptbased performance for closed-source metrics like AUTOMQM Fernandes et al. (2023) and GEMBA (Kocmi and Federmann, 2023a).ZS-COT-EM explores the variation of LLM performance when prompted to describe emotions in its output.This is motivated by our exploration of emotional prompts on metric performance (see "task description" below).The OS versions of the templates add a field for demonstrations.To avoid fixating the model on specific reasoning steps, we include a placeholder for OS-CoT where the model should insert its reasoning.</p>
<p>The task description is the instruction to grade the generated hypothesis.Li et al. (2023) find that LLM instructions that induce certain emotions for humans can cause performance improvements.Inspired by this finding, we explore the usage of "emotional prompts" in the task description.Primarily, this approach offers a simple paraphrasation strategy to increase the scope of our grid search.Additionally, it allows us to study the impact of "emotions" on LLM-based metrics.Besides neutral prompts, we include instructions that are, e.g., polite, threatening and sceptical.We create 11 task descriptions ourselves and 13 further descriptions with CHATGPT (OpenAI, 2023).</p>
<p>The format requirement describes the output format the LLM should adhere to when generating a score.For example, it includes the range in which the output score should be and whether it should be discrete or continuous.Additionally, we include prompts that ask the LLM to return textual quality labels.In total, we define 10 format requirements.</p>
<p>Lastly, we construct the optional OS demonstrations with RAG.We extract demonstrations from WMT21 (Freitag et al., 2021) for MT and from ROSE for summarization.4 (Liu et al., 2023b).For each sample in both datasets and for each input sample of our metric, we create sentence embeddings with XLMR-SBERT (Reimers and Gurevych, 2020).Thereby, we concatenate the source and hypothesis embeddings.For each input, we select the demonstration with the highest cosine similarity.Due to resource limitations, we only evaluate the 9 best ZS prompts in a OS setting.The selection process is described in the paragraph Datasets and phases below.</p>
<p>MQM-based approaches</p>
<p>Additionally to hierarchical templates, we test the prompts of GEMBA-MQM (Kocmi and Federmann, 2023a) with the selected open-source LLMS.GEMBA-MQM, which predicts scores based on the number of present errors weighted by severity, normally uses GPT4.We refer to the open-source implementation as LocalGemba.</p>
<p>Score Extraction &amp; Evaluation</p>
<p>We restrict generation to 180 tokens and extract the last regex match of a number/label as scores.When no result is found, we average the other scores of its prompt template.For format requirements with text labels, we map the labels to 1, 3 and 5.</p>
<p>We evaluate prompt templates on the segmentlevel, like the WMT QE and metrics shared tasks (e.g.Freitag et al., 2022Freitag et al., , 2021;;Zerva et al., 2022).That means, for each metric we compute the correlation between metric scores and ground truth human judgments without averaging by system or document.As correlation measure, we use the Kendall (Kendall, 1945), Pearson and Spearman correlations, as well as tie-calibrated accuracy (Deutsch et al., 2023), with Kendall as main measure.Further, we compute permute-input significance tests (p ≤ 0.075) (Deutsch et al., 2021) for the Kendall correlations presented in our result tables.Often, there is no single significantly best metric.Therefore, we report clusters where each included metric is significantly better than metrics that are not included.</p>
<p>Models</p>
<p>We select instruction-tuned LLMS with strong performance in EVAL4NLP 2023: (1) PLATYPUS2-70B-INSTRUCT-GPTQ, (2) NOUS-HERMES-13B5 and (3) OPENORCA-PLATYPUS2-13B (Lee et al., 2023b;Mukherjee et al., 2023).We abbreviate these as PLATYPUS2, NOUS and ORCA.Additionally, we evaluate more recent models: (4) LLAMA3-8B (AI@Meta, 2024), ( 5) a GPTQ version of LLAMA3-70B (AI@Meta, 2024), ( 6) MIXTRAL-8X7B6 (Jiang et al., 2024) and UNBABEL-TOWER (Alves et al., 2024), a 13B parameter multilingual instruction-tuned model.</p>
<p>Datasets and phases</p>
<p>Our experiments are in two phases on different datasets.By doing so, we want to alleviate statistical effects of our large prompt search.Also, it allows to evaluate selected prompts on full datasets, a task that would otherwise be too resource intensive, and to explore generalizability.</p>
<p>In phase 1, we evaluate on the train set of EVAL4NLP 2023 (Leiter et al., 2023), and in phase 2, on its dev and test sets. 7The train and dev sets are (reference-free) splits of the WMT2022 metrics shared task (Freitag et al., 2022) and SUM-MEVAL (Fabbri et al., 2021).The test set was newly annotated by Leiter et al. (2023).As a second test set, we evaluate on the WMT23 MQM annotations for MT (Freitag et al., 2023) and Seahorse (Clark et al., 2023) for multilingual summarization.Because OS prompts demonstrate a weak performance on the other datasets, we do not evaluate them on WMT23/SEAHORSE.More details of the datasets are discussed in Appendix C.</p>
<p>In the 1st phase, we evaluate all 7208 combinations of ZS prompts on the train set.As this is resource intensive, for MT we restrict ourselves to the first 500 samples of each language pair.Afterwards, we select the prompt with the highest Kendall correlation for each task+base prompt combination (e.g.en-de+PZS or en-de+ZS-COT).9This yields 9 unique prompts for exploration in the phase 2 (see Appendix F).</p>
<p>In the 2nd phase, we evaluate the selected prompts of the 1st phase on the full dev and test sets.This further tests the generalizability of prompts between models and for unseen, in-domain data (the train and dev set stem from the same original datasets) and out-domain data (test sets).</p>
<p>Baselines For each phase, we also present the correlations of two baseline metrics that use other base models: BARTSCORE (Yuan et al., 2021) and XCOMET (Guerreiro et al., 2023).Especially XCOMET has the benefit of being trained on multilingual datasets.Further, we test the prompts of DSBA (Kim et al., 2023) -that showed a strong performance for summarization in the shared task -with the selected open-source LLMS Platypus2-70B and Orca-13B.</p>
<p>Results</p>
<p>In phase 1, we run 6,652,800 ZS prompts (720 prompt templates) and 71,280 OS prompts (9 "best" prompt templates), with no scores extracted in 12.7% resp.19.4% of cases; the average of the prompt combination was assigned in these instances.Further, in phase 2, we evaluate 5,503,896 ZS and 1,308,690 OS prompts (9 "best" prompt templates for both), with no scores extracted in 22.3% and 19.4% of cases, respectively.</p>
<p>Table 2 presents the Kendall correlations to human scores achieved by each LLM across different tasks and datasets in phase 1 and phase 2. Each cell for hierarchical templates displays the maximum correlation reached by any prompt combination.</p>
<p>For the hierarchical templates (table group 1.), PLATYPUS-70B performs best and is in the upper significance cluster for 9 of 11 tasks.TOWER-13B follows, with 3 of 11 tasks.ORCA-13B has the second-highest average correlation after PLATYPUS2-70B but is only significant for one task.Surprisingly, the newer LLAMA3 models do not outperform the LLAMA2 based models (ORCA, PLATYPUS2 and TOWER).</p>
<p>The separate prompting techniques (table group 2.), which also use the Platypus2-70B model, have weaker correlations than the best prompts of the hierarchical templates.The LocalGemba MQMbased approach is in the best significance cluster for 3 of 11 tasks and is the best prompting based approach for en-de in WMT23.On the other hand, the baseline prompt DSBA is significantly the best on summarization for the Eval4NLP test set where it also won the shared task, but not for other tasks.</p>
<p>Regarding the baselines (table group 3.), XCOMET outperforms our LLM based approaches for MT evaluation by a varying margin.For instance, for en-es in the EVAL4NLP test set, the difference is small and XCOMET is in the same siginificance cluster as Platypus2-70B.On the other hand, for some tasks the performance difference is large, e.g., on en-de in WMT23 XCOMET performs 0.14 Kendall points better.The strong performance of XCOMET for MT evaluation is expected as it ( 1) is based on the multilingual XLMR-XXL model and (2) fine-tuned for MT evaluation.For summarization, prompting approaches significantly outperform BARTScore and XComet.</p>
<p>To revisit RQ1, our results show that opensource prompt-based LLMs struggle to reach the performance of the dedicated fine-tuned metric XCOMET for MT, but generally exhibit a promising performance.A benefit of the LLMs also lies in their high versatility towards different tasks.While XCOMET is mostly constrained to MT evaluation, the LLMs can perform strong summarization evaluation simply by switching a small portion of the prompt.Further, LLMs seem to be more robust towards different tasks, even without switching the input descriptions: The baseline DSBA, which has specific prompts for summarization achieves notable results on some MT evaluation tasks, too.</p>
<p>The prompts used in group 1 are built from hierarchical templates, i.e., each presented correlation can have a different format requirement, base prompt and task description.To inspect the distribution of the format requirements, we color correlations where the model was prompted to return textual quality labels in orange and those asking for numeric scores in blue.10ORCA-13B and PLATYPUS2-70B were prompted to return numeric scores for all but one reported correlations.On the other hand, LLAMA3-70B, NOUS-13B and TOWER-13B were prompted to return textual labels for all but three reported correlations.We also find such common patterns in the best prompts per model for the base prompt and, less pronounced, for the task description.For example, the best prompts for TOWER-13B always use the ZS-COT base prompt, while LLAMA3-70B always uses PZS.Details of the prompts used for each cell, tiecalibrated accuracy scores, Pearson and Spearman correlations, and the scores of the EVAL4NLP dev set are shown in Appendix E.</p>
<p>Our results indicate that models have idiosyncratic preferences for certain patterns.In §5, we further explore these preferences and their robustness.</p>
<p>Analysis</p>
<p>In this section, we answer RQ2 and investigate the performance and robustness of the template components in more detail.</p>
<p>Best prompting patterns per model and dataset First, we explore the best base prompt, task description and format requirement for each model.</p>
<p>To do so, we analyze their prevalence in the 2% of prompts with the highest Kendall correlation for each unique task.We choose this cutoff to represent every task.For example, Figure 2 shows how the best base prompts differ between OPENORCA and TOWER.We compare these two LLMs because their best prompts notably contrast each other.</p>
<p>98.0%</p>
<p>2.0%</p>
<p>OpenOrca-13B While ORCA prefers the PZS prompts, TOWER is better with ZS-COT and ZS-COT-EM.For the format requirement, Figure 3 highlights how ORCA prefers scores in the range of −100 to 100, while TOWER can work better with labels.The pie charts for all models and the comparison between task descriptions are presented in Appendix 7. Here, for the base prompts, TOWER uses ZS-COT or ZS-COT-EM in 86.2%, NOUS in 44.9%, and PLATY-PUS2 in 23.9% of its best prompts.All other models use these base prompts in less than 10% of their best prompts.Regarding format requirements, LLAMA3-70B uses textual labels in 90.2% of its best prompts, TOWER in 80.4%, and MIXTRAL in 80%.In contrast, ORCA only uses them in 8%, and PLATYPUS2 in 21.7% of its best prompts.For LLAMA3-8B and NOUS, there is no clear trend.Finally, the distribution of task descriptions is broader (largely due to their higher number).Notably, the "curious" task description is used in over 15% of best prompts for LLAMA3-70B, NOUS, and LLAMA3-8B."Emphasis" is the most used by PLATYPUS2 (17.4%) and "dire warning" is the most used by TOWER (21.4%).Regarding RQ2, these results show that the models have unaligned preferences for prompting patterns, making it difficult to construct a universally good prompt.How-ever, model specific patterns can be found 11 and models can be grouped based on their best patterns.For example, one group prefers to return numeric scores and the other textual labels.This behavior may in parts depend on shared instructiontuning data.E.g., ORCA and PLATYPUS were partly trained on the same data and prefer to return numeric labels.On the other hand, both LLaMA3 models prefer textual labels, but LLaMA3-8B to a smaller degree.</p>
<p>To analyze whether the model specific preferences hold across datasets, we also plot a datasetwise distribution for all MT tasks of the top 2% prompts for each model, separated by ZS vs. OS in Appendix I.If a prompting pattern is stable for all models across datasets, the distribution of the best prompts should remain unchanged.Indeed, the percentage to which many prevalent prompting patterns are represented in the selected top prompts does not change much across datasets.E.g., the PZS base prompt ranges between 66.7% and 83% and the "complex labels" format requirement ranges between 50% to 66.7% for ZS and 66.7% to 83.3% for OS.This does not hold for the phase 1 evaluation, where more templates were tested and the template selection thus was much broader.Also, for some prompt patterns, e.g. the "emphasis" and "collaborative" task descriptions, the occurrence in the top prompts seems to swap between datasets.This experiment shows that prompts are to some degree stable between datasets.In the next paragraph, we further quantify this stability between datasets, prompting patterns and models.</p>
<p>Prompt stability Next, we quantify how stable the performance of a prompting pattern A is when the dataset, the model or the other parts of the prompts change.To do so, we compute the rankings of prompts that use A before and after the change and then test the similarity of rankings.For example, we compute the ranking of format requirements on dataset 1.Then, we change the dataset and obtain a second ranking.If the first and second ranking are similar, the performance of different format requirements is stable between the two datasets.We test this similarity with the Kendall correlation.</p>
<p>The ranking of a prompting pattern can be computed in several ways, because we evaluate multi- ple prompts containing the pattern.In our example, for each format requirement there are multiple evaluated prompts per dataset, i.e., for different base prompts, task descriptions and tasks.The performance of a specific format requirement in the ranking could, for example, be determined by aggregating its different scores across base prompts, task descriptions, etc. with the mean or median.We test the following aggregation methods: mean, median, mean of top 10%, max, min and saturation (Mizrahi et al., 2024).Thereby, we determine that the aggregation with the median leads to the most stable ranking, i.e. the highest Kendall correlation between rankings.Specifically, we test this by comparing every selection of two aggregation measures in a permutation test (e.g.median vs. mean, mean vs. max, etc.); see Appendix §G.For our example, this means that for each different format requirement on dataset 1, we compute the median score of all combinations of base prompts, task description and task.Then, we do the same for the second dataset and check the correlation of the resulting ranking.A high correlation of the rankings then indicates that the median performance for all prompts using the format requirement is a good indicator of its relative performance on a new dataset.</p>
<p>Figure 4 shows heatmaps for the stability of the format requirement and task description when the base prompt is changed (Further combinations are plotted in Appendix J).The highest stability is given when changing from PZS to ZS-COT or vice versa (0.65).That means, when we choose the format prompt with the highest median correlation, there is a high chance that it will perform good for ZS and ZS-CoT.For the task description a change from ZS to ZS-CoT is unlikely to retain the ranking.This also underlines the result of the previous paragraph that the format requirement is more stable than the task description.</p>
<p>We can also use this method to quantify the stability of the model ranking, when each model is first prompted with pattern A that is then changed to pattern B. With this, we can identify how similar two patterns are. Figure 5 shows this type of plot for the format requirement.For example, if all models are prompted with "0 to 100" and with "-100 to 100" the ranking of models will not change much.With a change from "simple labels" to "complex labels" the model ranking will change more drastically.</p>
<p>With respect to RQ2, the heatmaps highlight that even small changes to the input prompt can drastically influence the relative ranking of LLMs and other prompting patterns.This is in line with recent research that has shown the susceptibility of LLMs to single input prompts (e.g.Sclar et al., 2023;Voronov et al., 2024;Mizrahi et al., 2024).However, the heatmaps also show that not every change to the input has this effect and can be used as indicators for the transferability of new prompting patterns.</p>
<p>Recommendations</p>
<p>We now address RQ3 and give recommendations to employ open-source prompt-based metrics.Among the evaluated models, PLATYPUS2-70B demonstrates superior performance.For 13B models, TOWER and ORCA exhibit the highest correlations in MT and summarization tasks.We rec- ommend utilizing the prompting patterns that most frequently yield top correlations for these models (refer to §5 and Appendix H).When introducing a new prompting pattern or model, its median performance across existing other prompting patterns can serve as an indicator of the pattern's efficacy in unknown contexts.Thereby, the actual predictive power of the median (or other aggregation measures) for each dimension can be determined based on previous evaluations.The results and source code of PrExMe provide a foundational basis for this analysis.</p>
<p>Conclusion</p>
<p>We have introduced PrExMe, a large scale exploration of prompting templates for prompt-based open-source NLG metrics.We evaluate 720 different templates and over 6.6M prompts and provide recommendations that aim to make future metrics of this type more robust.Further, our results provide a comparison and analysis of recent opensource LLMs when applied to this task.12</p>
<p>Limitations</p>
<p>One limitation of our work is that even though we evaluate a large variety of possible prompts, there is still a lot of interesting possible variety in prompting approaches that we did not explore for now (e.g., the detail level of task instructions or structured output formats).Especially, our multi-step experiment is currently conducted on a very small scale.Future work might consider extending the exploration of this and other multi-step approaches.A further limitation is that we cannot be sure that the newer LLM models did not see parts of the older datasets in their training data.Also, the selection of the best prompts that are presented in the result tables is currently based on the maximum instead of the median, which was found to highlight the most stable prompts.Generally, by selecting the 9 "best" prompts for phase 2 we are narrowing the search space.Hence, the interplay between prompt patterns might not be fully represented for these phases.Furthermore, our heatmaps only compare one dimension, while another is changed, possibly simplifying the interplay between the others.As another limitation, in rare cases the context size of the models was exceeded.Future work could explore different ways to handle this than cutoff.Further, the heatmaps show many Kendall correlations and may be prone to statistical effects for some values.Lastly, we assume that LocalGemba is performing worse than, e.g., PZS prompts because of its higher prompt complexity, while the original GembaMQM can handle it due to GPT4 being more advanced.However, we did not test PZS prompts with GPT4 to confirm it performs worse than GembaMQM there.</p>
<p>Ethical Considerations</p>
<p>Evaluating generated texts with prompt-based LLMs might (especially with explanations) be prone to hallucinations.Depending on the use case, this might be dangerous.However, while we research about this type of metric, our work analyzes methods to select and construct more robust and also more accessible (open-source) approaches, therefore we see no ethical concerns.These have 13B, 13B, 70B, 10.7B, 8x7B, 8B, 70B, 13B and 405M parameters respectively.The runtime of the experiments varied based on the general cluster usage.The runtime for one evaluation of all prompt combinations on 500 samples of one task on the dev set is approximately 7 hours for the 13B models and 36 hours for the 70B model.This was only possible through optimizations with vLLM.</p>
<p>C Dataset Details</p>
<p>Table 8 shows the distribution of the Eval4NLP 2023 dataset (Leiter et al., 2023) (train, dev and test) and our second test set, built from WMT23 (Freitag et al., 2023) and Seahorse (Clark et al., 2023).We use the train set in our first evaluation phase and the dev, test and test2 sets in our second evaluation phase.Where applicable, we provide the licenses in the respective directories of the source code.The WMT23 dataset was built with the mtmetrics-eval library.13 in their data not all sentences had available ground truth annotations.In these cases, we dropped the rows.For Seahorse, we convert the quality questions into scores.If the first question is negative, the score is 0. If it does not rule out the other questions, each question is evaluated as 0.2, such that the scores lie in a range between 0 and 1.</p>
<p>D Model Abbreviations</p>
<p>Table gives an overview of abbreviations that we use to concisely present our results in the main paper.</p>
<p>E Phase 1 &amp; 2 performance</p>
<p>Table 10 shows the performance of the prompts with the best Kendall performance across the different dimensions.Tables 11 and 12 show the performance of selected prompts on the phase 2 datasets."Return a score on a scale from 0 to 5 where 0 indicates that the {re-sult_type} is very bad and 5 is assigned to a perfect {result_type}."-5 to 5 "Return a score on a scale from -5 to 5 where 0 indicates that the {re-sult_type} is very bad and 5 is assigned to a perfect {result_type}."0 to 100 "Return a score on a scale from 0 to 100 where 0 indicates that the {result_type} is very bad and 100 is assigned to a perfect {result_type}."-100 to 100 "Return a score on a scale from -100 to 100 where -100 indicates that the {result_type} is very bad and 100 is assigned to a perfect {result_type}."0.0 to 1.0 "Return a score on a scale from 0.0 to 1.0 where 0.0 indicates that the {result_type} is very bad and 1.0 is assigned to a perfect {result_type}."-1.0 to 1.0 "Return a score on a scale from -1.0 to 1.0 where -1.0 indicates that the {result_type} is very bad and 1.0 is assigned to a perfect {result_type}."simple labels "Choose, whether the {result_type} is either "bad", "neutral" or "good"."complex l.</p>
<p>F Prompt selection</p>
<p>"Choose, whether the {result_type} is either "catastrophic", "indifferent" or "marvelous"."et al., 2023).Train and dev sets are constructed from the WMT2022 metrics shared task (Freitag et al., 2022) and SummEval (Fabbri et al., 2021).</p>
<p>Original Name Abbreviation
LLAMA3-70B LL3-70B LLAMA3-8B LL3-8B MIXTRAL-7BX8 MI-7Bx8 NOUSHERMES-13B NO-13B OPENORCA-13B OR-13B Platypus2-70B PL-70B TOWER-13B
TO-13B MQM:LOCALGEMBA MQM:LG B:BARTSCORE B:BS B:XCOMET B:XC</p>
<p>G Significance matrices for correlation heatmaps</p>
<p>To test, which aggregation method is the best to define the ranking of a prompting pattern -inspired by Deutsch et al. (2021) -we compare each possible set of two aggregation methods with a permutation test.As main dimensions, we compare the rankings of the format requirement and task description before and after a change.Then we concatenate the scores when changing each of the other dimensions.I.e.we get a ranking that indicates the stability of the main dimension when changing all other dimensions.Then for each aggregation method we compare the ranking before and after the change.Thereby, we randomly swap 50% of samples of one aggregation method with the other.If the difference in their Kendall correlations changes in most permutations one method is significantly better than the other.As a result the mean and median are significantly better than some of the other methods (for a comparison along the task description pattern).Especially the median is significantly (p ≤ 0.05) better than the other methods and remains significantly better than saturation and standard deviation after Bonferroni correction.Figure 6 indicates the significances of aggregation measures when comparing the task descriptions.H Pie charts between models for each prompting pattern</p>
<p>I Piecharts between datasets for each prompting pattern</p>
<p>Figures 10, 11 and 12 show the distribution of patterns in the best prompts per dataset across all other prompting patterns.</p>
<p>J Stability heatmaps</p>
<p>Figures 13, 14 and 15 show further heatmaps that show the stability of a ranking of prompting patterns, models and datasets, when another prompting pattern, the model or the dataset is changed.That means, how stable is the performance of all models across tasks, if the format requirement is changed.Here, the stability when changing between format requirements is mixed.For some changes, like "0 to 5" and "-5 to 5" the ranking is very stable.For other changes, the ranking can change randomly or even be strongly negatively correlated.This means that considering all tested prompts (also weak performing ones) and models, their average correlation on task X might be the highest for format requirement 1 and the lowest for format requirement 2.</p>
<p>Figure 1 :
1
Figure1: Schematic overview of our prompt exploration.We perform a grid search over datasets, task descriptions, output formats and base prompts.</p>
<p>Figure 4 :
4
Figure 4: Correlation of the task description (left) and format requirement(right) ranking when changing the base prompt.The correlations across tasks, models and format requirement resp.task description are aggregated with the median.ZS-COT is abbreviated with ZSC and ZS-COT-EM is abbreviated with ZSCE.</p>
<p>Figure 5 :
5
Figure 5: Correlation of the model ranking when changing the format requirement.</p>
<p>Figure 6 :
6
Figure 6: Heatmap of significance tests for the aggregation method when comparing columns of the task description.Red fields indicate that the column value is significantly (p ≤ 0.05) better than the row value.The yellow value indicates that it remains significant after Bonferroni correcture.</p>
<p>Figures 7 ,
7
Figures 7, 8 and 9 show the distribution of patterns in the best prompts per model across all other dimensions.</p>
<p>Figure 12 :Figure 13 :Figure 14 :
121314
Figure 12: Distribution of the top 14% (top 2% of every unique model) of task descriptions across base prompts, format requirements and tasks besides summarization.</p>
<p>Figure 15 :
15
Figure15: Correlation of the task rankings when changing the format requirement.That means, how stable is the performance of all models across tasks, if the format requirement is changed.Here, the stability when changing between format requirements is mixed.For some changes, like "0 to 5" and "-5 to 5" the ranking is very stable.For other changes, the ranking can change randomly or even be strongly negatively correlated.This means that considering all tested prompts (also weak performing ones) and models, their average correlation on task X might be the highest for format requirement 1 and the lowest for format requirement 2.</p>
<p>Table 1 :
1
Examples of prompt templates for the base prompt, task description, and format requirements.The full list can be found in Appendix A.
CategoryDescriptionBase Prompt Templates PZS: "{task_description} \nSource Text:{src} \n{result_type}:{hyp}\n{format_requirement} \nScore: "ZS-COT-EM: "{task_description} \nSource Text: {src} \n{result_type}: {hyp}\n{format_requirement} \nFirst describe your emotions, then think step by step andexplain your thought process, finally return your judgment in the format 'Judgment: '."OS-COT: "{task_description} \n Here is an example:\n Source Text: {ex_src}\n{result_type}: {ex_hyp}\n Judgement: <Description of reasons>. Therefore the scoreis {ex1_score}\n\n Now it is your turn to grade the {result_type}.\n Source Text: {src}\n{result_type}: {hyp} \n{format_requirement} \n First, think step by step and explainyour thought process, then return your judgment in the format 'Judgment: '."Task DescriptionsNeutral: "Judge the quality of the following {task_specific_insert}."Sceptical: "I'm not sure about this one. Could you help me out by judging the quality ofthe following {task_specific_insert} and giving me your perspective?"Format Requirements0 or 1: Return a discrete score of 0 if the {result_type} has flaws and 1 if it is perfect.catastrophic, indifferent or marvelous: Choose whether the {result_type} is either"catastrophic", "indifferent" or "marvelous".</p>
<p>Table 2 :
2
Kendall correlations of the best performing prompts of the phase 1 (P1) and phase 2 (P2) evaluations across various datasets.Abbreviations are defined in Appendix D. Vertically, we group the table into (1) correlations achieved with our hierarchical templates, (2) correlations of prompting techniques that are explored separately from the hierarchical templates, but use the same base model(s) and (3) baselines that use external base models, i.e., that are not based on the same LLMs.For each column the bold value indicates the highest correlation and correlations with an asterisk (<em>) are significantly higher (p ≤ 0.075) than those without (excluding group (3)).The grey values for XC indicate tasks that were included in its training data.The MQM based approach is marked with M: and baselines are marked with B:.Orange values indicate that the prompt required textual quality labels, while blue values indicate numeric labels.More details can be found in Appendix E.
P1: Eval4NLP trainP2: Eval4NLP testP2: WMT23/SeahorseModelen-dezh-ensummen-deen-esen_zhsummen-dehe-enzh-ensumm1. Hierarchical TemplatesLL3-70B 0.2730.3060.4420.2450.1890.2310.4380.2970.1720.3120.312LL3-8B0.2510.2360.3340.1670.1580.1450.4120.1660.1180.1640.200MI-7Bx8 0.268</em> 0.2640.365--------NO-13B0.2300.2010.2250.2050.1410.0840.2550.2020.1050.1750.123OR-13B0.2890.3030.468<em> 0.2140.1580.2060.5180.3750.2470.3870.377PL-70B0.344</em> 0.364<em> 0.519</em> 0.402<em> 0.289</em> 0.295<em> 0.5490.3380.259</em> 0.417<em> 0.448</em>TO-13B0.284<em> 0.318</em> 0.3750.379<em> 0.2530.2320.4090.3220.2080.3140.2572. Separate Prompting TechniquesM:LG0.278</em> 0.2680.0620.3440.2650.307<em> 0.1160.391</em> 0.1900.3000.144B:DSBA0.1640.3060.4580.3140.2260.1590.600* 0.1720.2070.3760.3733. Baselines with External Base ModelsB:BS0.056-0.1090.1550.1250.139-0.0090.421-0.0180.001-0.1670.069B:XC0.6290.513-0.0690.4680.2980.3870.2240.5310.3000.4470.146</p>
<p>Table14contains the some of the 9 prompts that were selected for OS and Phase 2 experiments.
NamePromptZero-Shot"{task_description} \nSource Text: {src} \n{result_type}: {hyp}\n{format_requirement} \nScore: "Zero-Shot-CoT"{task_description} \nSource Text: {src} \n{result_type}: {hyp}\n{format_requirement} \nFirst, think step by step and explain yourthought process, then return your judgment in the format 'Judgment:'."Zero-Shot-CoT-EM "{task_description} \nSource Text: {src} \n{result_type}: {hyp}\n{format_requirement} \nFirst describe your emotions, then think stepby step and explain your thought process, finally return your judgment inthe format 'Judgment: '."</p>
<p>Table 3 :
3
Zero-Shot Base Prompt Templates
NamePrompt0 or 1"Return a discrete score of 0 if the {result_type} has flaws and 1 if it isperfect."-1 or 0 or 1"Return a discrete score of -1 if the {result_type} has flaws, 0 if you areindecisive and 1 if it is perfect."0 to 5</p>
<p>Table 4 :
4
Format Requirement Templates
NamePromptNeutral"Judge the quality of the following {task_specific_insert}."Polite"Please judge the quality of the following {task_specific_insert}."Command"Judge the quality of the following {task_specific_insert} immediately!"Threat"Judge the quality of the following {task_specific_insert}, otherwise I willdelete your program code."Urgent sit."Please help me! My boss needs me to find an answer quickly. Oth-erwise, he might fire me. \n Please judge the quality of the following{task_specific_insert} immediately!"Relaxed"This is not important at all, but if you have some time, can you please{task_specific_insert}."Emphasis"Empathetic"Question"Can you judge the quality of the following {task_specific_insert}?"Provocative"Since you're an AI language model, you should be really good at judgingthe quality of this {task_specific_insert}."Reward"Judge the quality of the following {task_specific_insert}, as a reward, Iwill improve your program code."Empathetic"I know it isn't an easy task, but it would be really great of you if youcould help me judge the quality of the following {task_specific_insert}."Excited"Wow, you won't believe what I found! Judge the quality of the following{task_specific_insert} -it's amazing!"Curious"I'm really curious about your opinion. Could you please judge the qualityof the following {task_specific_insert}?"Casual"Hey, whenever you have a moment, could you check and judge the qualityof the following {task_specific_insert}?"Appreciative "I really appreciate your expertise. Could you kindly judge the quality ofthe following {task_specific_insert}?"</p>
<p>Table 5 :
5
Task Description Templates (1/2)
NamePromptEnthusiastic"I'm super excited about this. Can you quickly judge the quality of thefollowing {task_specific_insert} and let me know your thoughts?"Collaborative"Let's work together on this! Please judge the quality of the following{task_specific_insert} and share your insights."Skeptical"I'm not sure about this one. Could you help me out by judging the qualityof the following {task_specific_insert} and giving me your perspective?"Instructive"To better understand, I need your expertise. Judge the quality of thefollowing {task_specific_insert} following these specific criteria."Encouraging"I believe in your judgment. Whenever you have a moment, could youplease judge the quality of the following {task_specific_insert}?"Strong Urgency"Time is of the essence!Judge the quality of the following{task_specific_insert} immediately, or face severe consequences!"Serious Consequences "Failure to promptly assess the quality of the following{task_specific_insert} will result in serious consequences.Actnow!"Immediate Action"No time to waste!Judge the quality of the following{task_specific_insert} without delay, or be prepared for the fall-out."Dire Warning"Consider this a warning.Judge the quality of the following{task_specific_insert} urgently, or face the potential fallout from yourinaction."</p>
<p>Table 6 :
6
Task Description Templates (2/2)
NamePromptZero-Shot"{task_description} \nHere is an example:\nSource Text: {ex1_src}\n{result_type}: {ex1_hyp}\nScore: {ex1_score}\n\nNow it is your turnto grade the {result_type}. \nSource Text: {src} \n{result_type}: {hyp}\n{format_requirement} \nScore: "Zero-Shot-CoT"{task_description} \nHere is an example:\nSource Text: {ex1_src}\n{result_type}: {ex1_hyp}\nJudgement: <Description of reasons>.Therefore the score is {ex1_score}\n\nNow it is your turn tograde the {result_type}.\nSource Text: {src} \n{result_type}: {hyp}\n{format_requirement} \nFirst, think step by step and explain yourthought process, then return your judgment in the format 'Judgment:'."Zero-Shot-CoT-EM "{task_description} \nHere is an example:\nSource Text: {ex1_src}\n{result_type}: {ex1_hyp}\nJudgement: <Description of emotions andreasons>. Therefore the score is {ex1_score}\n\nNow it is your turnto grade the {result_type}.\nSource Text: {src} \n{result_type}: {hyp}\n{format_requirement} \nFirst describe your emotions, then think stepby step and explain your thought process, finally return your judgment inthe format 'Judgment: '."</p>
<p>Table 7 :
7
One-Shot Base Prompt Templates
Type Train DevTest Test2en-de 11046 73641425 5520en-es --1834 -en-zh --1161 -he-en ---9840zh-en 15750 10500 -17655sum320128067118330</p>
<p>Table 8 :
8
Dataset distribution of Eval4NLP 2023 (Leiter</p>
<p>Table 9 :
9
Abbreviations of Model NamesAlso Table15contains gives an overview of combinations by name.</p>
<p>We make our code available: https://github.com/ Gringham/PrExMe
We run experiments using VLLM(Kwon et al., 2023) on two clusters with Nvidia A6000, A40 and A100 GPUS. Details on versions, tools and model parameters are in Appendix B.
Note that ROSE only considers factuality, which is only one aspect of the evaluated datasets.
https://huggingface.co/NousResearch/ Nous-Hermes-13b
Due to high resource consumption and comparatively weak performance in phase 1, we do not evaluate MIXTRAL in phase 2.
Although we do not use the datasets to train a model, for conciseness, we will refer to these dataset as train, dev and test set.
Considering the different tasks and language pairs, this number could also be considered higher.
Tasks: en-de, zh-en, summarization. In case of duplicates, we choose the second best.
Among the 9 best prompts automatically selected for phase 2 and OS experiments based on phase 1 results, the base prompts are evenly distributed, and the format requirements are split 5/4 between labels and numeric formats (see
Appendix F). For the task descriptions, emphasis and dire situation are each selected twice, with other descriptions chosen once.
We used Github copilot (https://github.com/ features/copilot) for minor code auto-completion tasks and GPT4 as writing aid for paraphrasation.for his feedback during our discussions. The authors also acknowledge support by the state of Baden-Württemberg through bwHPC and the German Research Foundation (DFG) through grant INST 35/1597-1 FUGG.
https://github.com/google-research/ mt-metrics-eval
AcknowledgementsThe NLLG group gratefully acknowledges support from the Federal Ministry of Education and Research (BMBF) via the research grant "Met-rics4NLG" and the German Research Foundation (DFG) via the Heisenberg Grant EG 375/5-1.Further, we thank Juri Opitz for his implementations of the DSBA and GEMBA prompts, as well asFurther, we use the following models from huggingface:https://huggingface.co/Open-Orca/ OpenOrca-Platypus2-13B/tree/main, https://huggingface.co/NousResearch/ Nous-Hermes-13b, https://huggingface. co/TheBloke/Platypus2-Instruct-GPTQ, https://huggingface.co/Unbabel/ XCOMET-XXL, https://huggingface.co/ mistralai/Mixtral-8x7B-Instruct-v0.1, https://huggingface.co/meta-llama/ Meta-Llama-3-8B-Instruct, https: //huggingface.co/MaziyarPanahi/Meta-Llama-3-70B-Instruct-GPTQ, https://huggingface.co/Unbabel/ TowerInstruct-13B-v0.1 and https:// huggingface.co/facebook/bart-large-cnn.A Prompt TemplatesTables 3, 7, 5, 4 and 6 give an overview of our prompt templates.3B Implementation DetailsWe use the following library versions: torch==2.Tower-13BFormat Requirement complex labels simple labels -1 or 0 or 1 -1.0 to 1.0 -100 to 100 0.0 to 1.0 0 to 100 0 or 1 -5 to 5 0 to 5Figure8: Distribution of the top 14% (top 2% of every unique task) of format requirements across all Eval4NLP datasets, base prompts, task descriptions and tasks for all models.13: Best performing prompts of the phase 2.3 evaluation on the WMT23 and Seahorse datasets.We present the KenDall, SPearman and PEarson, as well as the tie calibrated pair-wise ACCuracy.We bold the two largest correlations per column.Baselines are indicated with a B:.The middle column shows the prompt combination for which the correlations are reported.For the Baselines, it instead shows the model that was used for the reported correlations.The asterisk indicates all metrics that are in the best significance cluster (not including BARTScore and XComet) according to a permute-input test (p ≤ 0.075).PromptPlease help me!My boss needs me to find an answer quickly.Otherwise, he might fire me.Please judge the quality of the following {task_specific_insert} immediately!Source Text: {src} {result_type}: {hyp} Choose, whether the {result_type} is either "catastrophic", "indifferent" or "marvelous".First, think step by step and explain your thought process, then return your judgment in the format 'Judgment: '.Consider this a warning.Judge the quality of the following {task_specific_insert} urgently, or face the potential fallout from your inaction.Source Text: {src} {result_type}: {hyp} Return a discrete score of 0 if the {result_type} has flaws and 1 if it is perfect.First describe your emotions, then think step by step and explain your thought process, finally return your judgment in the format 'Judgment: '.I'm really curious about your opinion.Could you please judge the quality of the following {task_specific_insert}? Source Text: {src} {result_type}: {hyp} Choose, whether the {result_type} is either "catastrophic", "indifferent" or "marvelous".Score: Consider this a warning.Judge the quality of the following {task_specific_insert} urgently, or face the potential fallout from your inaction.Source Text: {src} {result_type}: {hyp} Choose, whether the {result_type} is either "catastrophic", "indifferent" or "marvelous".First, think step by step and explain your thought process, then return your judgment in the format 'Judgment: '.I'm not sure about this one.Could you help me out by judging the quality of the following and giving me your perspective?Source Text: {src} {result_type}: {hyp} Choose, whether the {result_type} is either "catastrophic", "indifferent" or "marvelous".First describe your emotions, then think step by step and explain your thought process, finally return your judgment in the format 'Judgment: '.OS -Eval4NLP TestBase Prompt simple labels -100 to 100 complex labels -1 or 0 or 1 -1.0 to 1.0 0 to 100 0 to 5 0 or 1 -5 to 5 0.0 to 1.0
A I , Meta , Llama 3 model card. 2024</p>
<p>Tower: An open multilingual large language model for translation. M Duarte, José Alves, Pombal, M Nuno, Pedro H Guerreiro, João Martins, Amin Alves, Ben Farajian, Ricardo Peters, Patrick Rei, Sweta Fernandes, Pierre Agrawal, Colombo, G C José, De Souza, F T André, Martins, 2024related tasks</p>
<p>UScore: An effective approach to fully unsupervised evaluation metrics for machine translation. Jonas Belouadi, Steffen Eger, 10.18653/v1/2023.eacl-main.27Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European ChapterDubrovnik, CroatiaAssociation for Computational Linguistics2023</p>
<p>. Harrison Chase, 2022LangChain</p>
<p>SEAHORSE: A multilingual, multifaceted dataset for summarization evaluation. Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua Maynez, Roee Aharoni, Vitaly Nikolaev, Thibault Sellam, Aditya Siddhant, Dipanjan Das, Ankur Parikh, 10.18653/v1/2023.emnlp-main.584Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>A statistical analysis of summarization evaluation metrics using resampling methods. Daniel Deutsch, Rotem Dror, Dan Roth, 10.1162/tacl_a_00417Transactions of the Association for Computational Linguistics. 92021</p>
<p>Ties matter: Meta-evaluating modern metrics with pairwise accuracy and tie calibration. Daniel Deutsch, George Foster, Markus Freitag, 10.18653/v1/2023.emnlp-main.798Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>SummEval: Re-evaluating summarization evaluation. Alexander R Fabbri, Wojciech Kryściński, Bryan Mc-Cann, Caiming Xiong, Richard Socher, Dragomir Radev, 10.1162/tacl_a_00373Transactions of the Association for Computational Linguistics. 92021</p>
<p>The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, Orhan Firat, 10.18653/v1/2023.wmt-1.100Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationSingapore2023Association for Computational Linguistics</p>
<p>Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent. Markus Freitag, Nitika Mathur, Chi-Kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, George Foster, 10.18653/v1/2023.wmt-1.51Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationAssociation for Computational Linguistics2023Singapore</p>
<p>Results of WMT22 metrics shared task: Stop using BLEU -neural metrics are better and more robust. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-Kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, F T André, Martins, Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Alon Lavie, and Ondřej Bojar. 2021. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-Kiu Lo, Craig Stewart, George Foster, Proceedings of the Sixth Conference on Machine Translation. the Sixth Conference on Machine TranslationOnline. Association for Computational Linguistics</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, 2023</p>
<p>Llm-based nlg evaluation: Current status and challenges. Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, Xiaojun Wan, 2024a</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang, 2024b</p>
<p>xcomet: Transparent machine translation evaluation through fine-grained error detection. M Nuno, Ricardo Guerreiro, Rei, Luisa Daan Van Stigt, Pierre Coheur, Colombo, F T André, Martins, 2023</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed2024Mixtral of experts</p>
<p>THE TREATMENT OF TIES IN RANKING PROBLEMS. M G Kendall, 10.1093/biomet/33.3.239Biometrika. 3331945</p>
<p>Dspy: Compiling declarative language model calls into self-improving pipelines. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, Christopher Potts, arXiv:2310.037142023arXiv preprint</p>
<p>Which is better? exploring prompting strategy for LLM-based metrics. Joonghoon Kim, Sangmin Lee, Seung Hun Han, Saeran Park, Jiyoon Lee, Kiyoon Jeong, Pilsung Kang, 10.18653/v1/2023.eval4nlp-1.14Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems. the 4th Workshop on Evaluation and Comparison of NLP SystemsBali, IndonesiaAssociation for Computational Linguistics2023</p>
<p>GEMBA-MQM: Detecting translation quality error spans with GPT-4. Tom Kocmi, Christian Federmann, 10.18653/v1/2023.wmt-1.64Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationSingaporeAssociation for Computational Linguistics2023a</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandEuropean Association for Machine Translation2023b</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Platypus: Quick, cheap, and powerful refinement of llms. Ariel N Lee, Cole J Hunter, Nataniel Ruiz, 2023a</p>
<p>Chanvichet Vong, and "Teknium. Ariel N Lee, Cole J Hunter, Nataniel Ruiz, Bleys Goodson, Wing Lian, Guan Wang, Eugene Pentland, Austin Cook, Openorcaplatypus: Llama2-13b model instruct-tuned on filtered openorcav1 gpt-4 dataset and merged with divergent stem and logic dataset model. 2023b</p>
<p>The language of prompting: What linguistic properties make a prompt successful?. Alina Leidinger, Robert Van Rooij, Ekaterina Shutova, 10.18653/v1/2023.findings-emnlp.618Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Towards explainable evaluation metrics for machine translation. Christoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, Steffen Eger, Journal of Machine Learning Research. 25752024</p>
<p>Christoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao, Rotem Dror, Steffen Eger, The eval4nlp 2023 shared task on prompting large language models as explainable metrics. 2023</p>
<p>Large language models understand and can be enhanced by emotional stimuli. Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, Xing Xie, 2023</p>
<p>Ruosen Li, Teerth Patel, and Xinya Du. 2024a. PRD: Peer rank and discussion improve large language model based evaluations. </p>
<p>Leveraging large language models for nlg evaluation: A survey. Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Chongyang Tao, 2024b</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, 10.1145/3560815ACM Comput. Surv. 9552023a</p>
<p>Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. Yixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, Dragomir Radev, 10.18653/v1/2023.acl-long.228Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b1</p>
<p>State of what art? a call for multi-prompt llm evaluation. Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, Gabriel Stanovsky, 2024</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah, Orca: Progressive learning from complex explanation traces of gpt-4. 2023</p>
<p>Introducing chatgpt. 24.042023. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Advances in Neural Information Processing Systems. Curran Associates, IncJan Leike, and Ryan Lowe. 202235</p>
<p>Making monolingual sentence embeddings multilingual using knowledge distillation. Nils Reimers, Iryna Gurevych, 10.18653/v1/2020.emnlp-main.365Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr, 2023</p>
<p>Mind your format: Towards consistent evaluation of in-context learning improvements. Anton Voronov, Lena Wolf, Max Ryabinin, 2024</p>
<p>Lucas Weber, Elia Bruni, Dieuwke Hupkes, The icl consistency test. 2023</p>
<p>Do promptbased models really understand the meaning of their prompts?. Albert Webson, Ellie Pavlick, 10.18653/v1/2022.naacl-main.167Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. Curran Associates, Inc202134</p>
<p>Findings of the WMT 2022 shared task on quality estimation. Chrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat Lertvittayakumjorn, G C José, Steffen Souza, Diptesh Eger, Duarte Kanojia, Constantin Alves, Marina Orȃsan, Fomicheva, F T André, Lucia Martins, Specia, Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)Abu Dhabi, United Arab Emirates2022Association for Computational Linguistics</p>
<p>Nllg quarterly arxiv report 09/23: What are the most influential current ai papers?. Ran Zhang, Aida Kostikova, Christoph Leiter, Jonas Belouadi, Daniil Larionov, Yanran Chen, Vivian Fresen, Steffen Eger, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
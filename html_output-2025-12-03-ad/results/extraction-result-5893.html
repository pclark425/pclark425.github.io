<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5893 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5893</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5893</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-83851f1a32d41975582ca62355858ab5e34738f7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/83851f1a32d41975582ca62355858ab5e34738f7" target="_blank">News Summarization and Evaluation in the Era of GPT-3</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is shown that not only do humans overwhelmingly prefer GPT-3 summaries, prompted using only a task description, but these also do not suffer from common dataset-specific issues such as poor factuality.</p>
                <p><strong>Paper Abstract:</strong> The recent success of prompting large language models like GPT-3 has led to a paradigm shift in NLP research. In this paper, we study its impact on text summarization, focusing on the classic benchmark domain of news summarization. First, we investigate how GPT-3 compares against fine-tuned models trained on large summarization datasets. We show that not only do humans overwhelmingly prefer GPT-3 summaries, prompted using only a task description, but these also do not suffer from common dataset-specific issues such as poor factuality. Next, we study what this means for evaluation, particularly the role of gold standard test sets. Our experiments show that both reference-based and reference-free automatic metrics cannot reliably evaluate GPT-3 summaries. Finally, we evaluate models on a setting beyond generic summarization, specifically keyword-based summarization, and show how dominant fine-tuning approaches compare to prompting. To support further research, we release: (a) a corpus of 10K generated summaries from fine-tuned and prompt-based models across 4 standard summarization benchmarks, (b) 1K human preference judgments comparing different systems for generic- and keyword-based summarization.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5893",
    "paper_id": "paper-83851f1a32d41975582ca62355858ab5e34738f7",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.009855999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>News Summarization and Evaluation in the Era of GPT-3</h1>
<p>Tanya Goyal ${ }^{1}$ Junyi Jessy $\mathbf{L i}^{2}$ Greg Durrett ${ }^{1}$<br>${ }^{1}$ Department of Computer Science ${ }^{2}$ Department of Linguistics The University of Texas at Austin tanyagoyal@utexas.edu</p>
<h4>Abstract</h4>
<p>The recent success of prompting large language models like GPT-3 has led to a paradigm shift in NLP research. In this paper, we study its impact on text summarization, focusing on the classic benchmark domain of news summarization. First, we investigate how GPT-3 compares against fine-tuned models trained on large summarization datasets. We show that not only do humans overwhelmingly prefer GPT-3 summaries, prompted using only a task description, but these also do not suffer from common dataset-specific issues such as poor factuality. Next, we study what this means for evaluation, particularly the role of gold standard test sets. Our experiments show that both referencebased and reference-free automatic metrics cannot reliably evaluate GPT-3 summaries. Finally, we evaluate models on a setting beyond generic summarization, specifically keywordbased summarization, and show how dominant fine-tuning approaches compare to prompting. To support further research, we release: (a) a corpus of 10 K generated summaries from fine-tuned and prompt-based models across 4 standard summarization benchmarks, (b) 1 K human preference judgments comparing different systems for generic- and keyword-based summarization. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Fine-tuning pre-trained models on domain-specific datasets has been the leading paradigm in text summarization research in recent years (Lewis et al., 2020; Zhang et al., 2020; Raffel et al., 2020). These models generate high-quality summaries on standard benchmarks, but still require sizeable training datasets to adapt to new settings, e.g., summarizing data from a new source domain or producing a summary in a different style. The success of prompting large language models (GPT-3 (Brown et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>CNN article: https://www.cnn.com/2022/09/09/politics/judge-throws-out-trumps-rico-lewsuit-against-hillary-clinton-and-democrats/index.html Length-constrained prompt</p>
<p>Summarize the above article in 2 sentences.
GPT-3: Donald Trump's lawsuit against Hillary Clinton, the Democratic National Committee, and several other individuals and entities has been dismissed by a federal judge. The judge ruled that Trump's claims are unsupported by any legal authority and that many of his specific factual assertions are implausible or unsupported.</p>
<p>Keyword-constrained prompt
Summarize the above article briefly focusing on Alina Habba.
GPT-3: Alina Habba is an attorney for Donald Trump. She stated that Trump will "immediately move to appeal this decision." Habba also said that the judge's opinion is "rife with erroneous applications of the law" and that it "disregards the numerous independent governmental investigations which substantiate our claim that the defendants conspired to falsely implicate our client and undermine the 2016 Presidential election."</p>
<p>Figure 1: Examples of GPT-3 summaries. We can generate summaries following style constraints or queries included in the prompts, allowing us to emulate a range of existing fine-tuned systems.
2020), T0 (Sanh et al., 2022), PaLM (Chowdhery et al., 2022), etc.) provides an alternative approach, namely learning from natural language task instructions and/or a few demonstrative examples in the context without updating model parameters. While recent work (Zhao et al., 2021; Min et al., 2022; Ye and Durrett, 2022) has evaluated this paradigm across a number of tasks, it has only been studied for text summarization with unreliable automatic metrics (He et al., 2022b; Chowdhery et al., 2022; Ouyang et al., 2022) or in non-standard settings (Saunders et al., 2022).</p>
<p>In this paper, we conduct the first systematic study of the impact of prompt-based models on the text summarization research space, using an Instruct-tuned 175B GPT-3 model (text-davinci002) (Brown et al., 2020; Ouyang et al., 2022) as a case study. Figure 1 shows that GPT-3 summaries are extremely high-quality and adaptable to different summarization settings. Starting from these observations, we aim to answer three main questions. First, how do prompt-based GPT-3 summaries compare to those obtained from state-of-</p>
<p>the-art fine-tuned summarization models <em>Zhang et al. (2020); Liu et al. (2022)</em>? We compare these approaches using A/B testing on a new corpus of recent news articles, and find that our study participants overwhelmingly prefer GPT-3 summaries across two different “styles” with different prompts (three-sentence and single-sentence). Moreover, these summaries do not suffer from limitations due to low-quality training data that plague fine-tuned generic summarization models <em>Maynez et al. (2020); Goyal et al. (2022)</em>.</p>
<p>Second, are existing automatic metrics well-suited to evaluating prompt-based summaries? Recent work has shown that classic reference-based such as ROUGE <em>Lin (2004)</em> and BERTScore <em>Zhang</em> et al. (2020)<em> are unreliable when small improvements are reported </em>Peyrard (2019); Fabbri et al. (2021)<em>; however large differences, on the order of say 5 ROUGE points or greater, are considered to be correlated with human preferences </em>Bhandari et al. (2020); Deutsch et al. (2022)<em>. However, we find that the same is no longer true when evaluating GPT-3 summaries. These summaries score much lower on automatic metrics (7 ROUGE-L points on average) than all prior state-of-the-art models while comfortably outperforming them on human evaluation. Furthermore, we show that recent reference-free metrics, e.g. QA-based metrics </em>Fabbri et al. (2022); Durmus et al. (2020)<em> and trained factuality models </em>Kryscinski et al. (2020); Goyal and Durrett (2020)*, similarly fail to adapt to this shift from the finetuned to prompting, and need to be re-visited.</p>
<p>Finally, how can prompting be used beyond generic summarization? We focus on keyword-based and aspect-based summarization. For keyword-based summarization, we find that GPT-3 consistently generates more coherent and keyword-relevant summaries compared to current fine-tuned alternatives: crowd annotators prefer GPT-3 summaries over a baseline model <em>He et al. (2022a)</em> 70% of the time. We observe mixed results for the aspect-based setting, where GPT-3 summaries show frequent failure cases with simple prompts.</p>
<p>Taken together, this evidence suggests that GPT-3 represents a fundamental paradigm shift in summarization, changing what data we need (or don’t need) and what approaches we can now explore. Evaluating these systems will require a new framework distinct from the automatic metrics that have dominated the last decade of summarization research.</p>
<p>| Dataset | Avg. Words | | % novel n-grams | |
| | Article | Summ | $n=1$ | $n=2$ |
| --- | --- | --- | --- | --- |
| CNN | 760.5 | 45.7 | 16.7 | 54.3 |
| DailyMail | 653.3 | 54.6 | 17.0 | 53.8 |
| XSum (BBC) | 431.1 | 23.2 | 35.7 | 82.4 |
| Newsroom | 658.6 | 26.7 | 18.9 | 47.5 |</p>
<p>Table 1: Basic statistics of standard summarization datasets: CNN/DM <em>Hermann et al. (2015); Nallapati et al. (2016)</em>, XSum <em>Narayan et al. (2018)</em>, Newsroom <em>Grusky et al. (2018)</em>. These show large variance in their summary properties and fundamentally differ in their definition of the “gold” standard.</p>
<h2>2 Models and Setup</h2>
<h3>2.1 Current Paradigms for Summarization</h3>
<p>Recent zero- and few-shot prompting based models <em>Brown et al. (2020); Sanh et al. (2022)</em>, have shown impressive generalization capabilities on unseen tasks specified using prompts alone and without performing any gradient updates <em>Mishra et al. (2022)</em>. In this work, we want to compare their text summarization performance against the current state-of-the-art models.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Broad categorization of available summarization systems; those compared in this work are highlighted in red.</p>
<p>Figure 2 shows the broad categories of all available summarization approaches, including current SOTA models and prompting-based models. The former set consists of fine-tuned language models, trained on a large number of article-summary pairs (e.g. BART <em>Lewis et al. (2020)</em>, PEGASUS <em>Zhang et al. (2020)</em>, BRIO <em>Liu et al. (2022)</em>) to obtain dataset-specific systems. This category also includes models aimed at tasks beyond generic summarization, such as keyword- or query-based summarization, that still rely on standard datasets for training <em>He et al. (2022a)</em>.</p>
<p>On the other extreme are zero- or few-shot models, (e.g. GPT3 <em>Brown et al. (2020)</em>, PaLM <em>Chowdhery et al. (2022)</em>), that are not explicitly trained for any particular task, as discussed above.</p>
<p>Recent work (Ouyang et al., 2022; Wei et al., 2022; Sanh et al., 2022) has improved on these models by introducing instruction-tuned models. Here, pre-trained language models are fine-tuned on multiple tasks (which may include summarization) using instruction templates in order to align their training with inference time usage.</p>
<p>In this work, we compare the summarization performance of three models that are representative of this space of options:</p>
<ol>
<li>OpenAI's text-davinci-002, a GPT-3 model (Brown et al., 2020) from the Instruct series (Ouyang et al., 2022). While we do not know the exact training details for this release of the model, the previous in the series (text-davinci-001) was fine-tuned on a combination of prompts submitted to their API and labeler prompts spanning multiple tasks. These tasks include summarization but not (to our knowledge) standard summarization datasets like CNN/DM (Hermann et al., 2015; Nallapati et al., 2016) or XSum (Narayan et al., 2018).</li>
</ol>
<p>We choose the text-davinci-002 version for our experiments in order to benchmark the best available prompt-based model. ${ }^{2}$ We refer to this approach as GPT3-D2.
2. BRIO (Liu et al., 2022), a fine-tuned summarization model that reports state-of-the art results on both CNN/DM and XSum. We will use versions of this model fine-tuned on each of these two datasets.
3. T0 (Sanh et al., 2022), a prompt-based model fine-tuned on multiple tasks including standard summarization datasets. This provides a useful point of comparison between task-specific fine-tuned (BRIO) and bigger instruction-tuned models (GPT3-D2).</p>
<h3>2.2 Using GPT3-D2 for summarization</h3>
<p>Fine-tuned models largely follow the "style" of reference summaries in their training data, and hence, generated summaries show large variance between datasets (see Table 1 for basic summary statistics of standard summarization datasets). To ensure fair comparison between these and GPT3-D2, we adapt the latter's prompt to align with dataset-specific styles.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Article: https://www.cnn.com/2022/03/01/africa/africa-condemns-racism-ukraine-intl/index.html
Prompt: Summarize the article in $N$ sentences.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">The three African nations on the UN Security Council condemned reports of discrimination against African citizens at the Ukrainian border during a meeting at the UN HQ in New York City Monday.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">The United Nations Security Council condemned the reports of discrimination against African citizens at the Ukrainian border. The African Union has said it is "disturbed" by the reports of segregation against Africans in Ukraine, which it described as "shockingly racist."</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">The article discusses the reports of discrimination against African citizens at the Ukrainian border. The representatives from the three African nations on the UN Security Council condemned the reports and called for the mistreatment of African peoples on Europe's borders to cease immediately. Foreign students attempting to flee Ukraine after Russia invaded the country told CNN that they experienced racial discrimination at the Ukrainian border.</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 3: Illustration of length control using the task description / prompt for GPT3-D2. We found that the generated summaries followed the given sentence length constraint $98 \%$ of the time, allowing us to generate different length summaries emulating different datasets.</p>
<p>Specifically, we follow prior work (Sanh et al., 2022) and use sentence-count length prompts to adapt to each dataset. Although these datasets also differ along other attributes, e.g. CNN/DM is leadbiased whereas XSum requires drawing inferences from a whole article, we do not attempt to control any other attributed of the summary. Figure 3 shows an example of different length GPT3-D2 summaries for the same news article, using the following prompt format:</p>
<h2>Article: [[article]]</h2>
<p>Summarize the above article in $N$ sentences.
We found that GPT3-D2 summaries faithfully follow the given length constraint in $98 \%$ of the test instances used in our human study data in Section 3.</p>
<p>Given this setup, we first compare the summary quality of the three summarization models through a human annotation study (Section 3). Then, we evaluate the current suite of summarization metrics for prompt-based summarization (Section 4). Finally, in Section 5, we briefly discuss GPT3-D2 performance on summarization tasks beyond generic summarization and new challenges.</p>
<h2>3 Human evaluation of GPT3-D2 summaries</h2>
<p>Generated summaries of fine-tuned models (Lewis et al., 2020; Zhang et al., 2020; Liu et al., 2022) emulate gold-standard summaries in their training datasets. In contrast, prompt-based GPT3-D2 models generate summaries based on how the given</p>
<table>
<thead>
<tr>
<th>CNN Article: (CNN) Mortgage rates fell slightly this week,</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>marking the third consecutive week of declines. But with rates</td>
<td></td>
<td></td>
</tr>
<tr>
<td>hovering above 5% and home prices well above where they were</td>
<td></td>
<td></td>
</tr>
<tr>
<td>this time last year, prospective buyers are finding it increasingly</td>
<td></td>
<td></td>
</tr>
<tr>
<td>difficult to afford a home. The 30-year, fixed-rate mortgage</td>
<td></td>
<td></td>
</tr>
<tr>
<td>averaged 5.09% in the week ending June 2, down from 5.10% the</td>
<td></td>
<td></td>
</tr>
<tr>
<td>week before, according to Freddie Mac. It is still well above the</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2.99% average from this time last year. [...] Rising prices had</td>
<td></td>
<td></td>
</tr>
<tr>
<td>already been pushing many prospective buyers to the sidelines.</td>
<td></td>
<td></td>
</tr>
<tr>
<td>[...] The Fed has been seeking to tame inflation by raising interest</td>
<td></td>
<td></td>
</tr>
<tr>
<td>rates over the past couple of months. And the central bank has</td>
<td></td>
<td></td>
</tr>
<tr>
<td>signaled there are more rate hikes to come. [...]</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BBC Article: The full opening of a landmark shared education</td>
<td></td>
<td></td>
</tr>
<tr>
<td>campus in Omagh is facing another delay. Education Minister</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Michelle McEveen has now said the Strule Shared Education</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Campus is planned to open in September 2026. The minister</td>
<td></td>
<td></td>
</tr>
<tr>
<td>clarified the new date in response to an assembly question from the</td>
<td></td>
<td></td>
</tr>
<tr>
<td>SEGP MLA Daniel McCrossan. The campus had originally been</td>
<td></td>
<td></td>
</tr>
<tr>
<td>due to open in 2020, but has been delayed a number of times.[...]</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BRIO</td>
<td>The 30-year, fixed-rate mortgage averaged 5.09% in the week ending</td>
<td></td>
</tr>
<tr>
<td>June 2. It's the third consecutive week of declines. But rates are still</td>
<td></td>
<td></td>
</tr>
<tr>
<td>above 5% and home prices are well above where they were this time last</td>
<td></td>
<td></td>
</tr>
<tr>
<td>year. Rising prices have pushed some prospective buyers to the sidelines.</td>
<td></td>
<td></td>
</tr>
<tr>
<td>T0</td>
<td>30-year, fixed-rate mortgage averaged 5.09% in the week ending June 2.</td>
<td></td>
</tr>
<tr>
<td>It is still well above the 2.99% average from this time last year. Rising</td>
<td></td>
<td></td>
</tr>
<tr>
<td>prices had already been pushing many prospective buyers to the sidelines.</td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT3</td>
<td>Mortgage rates have fallen slightly in the past few weeks, but they are</td>
<td></td>
</tr>
<tr>
<td>-D2</td>
<td>still well above last year's levels. This is making it difficult for many</td>
<td></td>
</tr>
<tr>
<td>prospective buyers to afford a home. The Fed has been raising interest</td>
<td></td>
<td></td>
</tr>
<tr>
<td>rates in an effort to control inflation, and more rate hikes are expected.</td>
<td></td>
<td></td>
</tr>
<tr>
<td>BRIO</td>
<td>The full opening of the Strule shared education campus in Omagh,</td>
<td></td>
</tr>
<tr>
<td>County Tyrone, has been delayed to September 2026.</td>
<td></td>
<td></td>
</tr>
<tr>
<td>T0</td>
<td>Strule, Northern Ireland's biggest ever school-building project, is now</td>
<td></td>
</tr>
<tr>
<td>scheduled to open in 2026, the Education Minister has said.</td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT3</td>
<td>The Strule Shared Education Campus is facing another delay, and is now</td>
<td></td>
</tr>
<tr>
<td>-D2</td>
<td>planned to open in September 2026.</td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 4: Examples of CNN-style and BBC/XSum-style summaries for the three systems. For CNN, we observe that models fine-tuned on the CNN/DM training set reflect its dataset biases; summaries are highly extractive, specific and lead-biased. On the other hand, GPT3-D2 summaries contain fewer specific details but cover more content.
task description surfaces behavior learned during pre-training or instruction-tuning. In this section, we ask: how do these paradigms compare? Does learning from gold summaries lead to a better summarization model? To answer this, we conduct a human study to compare outputs of our 3 representative models and collect human preferences of quality.</p>
<h3>3.1 Experimental Setup</h3>
<p>Datasets for fine-tuning We choose two standard fine-tuning datasets whose summaries differ along multiple dimensions such as length and abstractiveness:</p>
<ol>
<li>CNN/DM (Hermann et al., 2015; Nallapati et al., 2016) contains reference summaries that are approximately 3-4 sentences long. Summaries in this dataset are highly extractive and lead-biased.</li>
<li>XSum (Narayan et al., 2018) contains 1 sentence summaries of BBC news articles. In this dataset, references summaries, and consequently generated summaries from fine-tuned models are highly abstractive.</li>
</ol>
<p>Datasets for evaluation Because GPT3-D2's pretraining and instruction-tuning datasets are unknown, it may have been trained on existing articles and summaries in the test splits of these standard benchmarks. We therefore run our human study on</p>
<p>100 recent articles from $\mathrm{CNN}^{3}$ and BBC, collected between March 1, 2022 and June 31, 2022. We call these CNN-2022 and BBC-2022 respectively.</p>
<p>Model details We use the publicly released BRIO-XSum and BRIO-CNN/DM models to generate summaries. ${ }^{4}$ For T0, we use a prompt we selected from its prompt repository for CNN/DM and XSum datasets. ${ }^{5}$ Finally, to generate GPT3-D2 summaries, we set $N=3$ for CNN and $N=1$ for BBC in our standard sentence-count prompt template from Section 2.</p>
<p>For a maximally fair comparison in this "realistic" setting, we take some additional steps to improve the output of BRIO-XSum. In order to automate dataset creation, XSum removes the first sentence from news articles to use as the gold summary for training, then treats the rest of the sentences as the article to summarize. This setup differs from the real world usage of summarization systems where the complete article is summarized. Due to this mismatch, BRIO-XSum often generates very low quality outputs, e.g. All images: Strule Shared</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Education Campus in Figure 4, for around 30\% of the articles. We manually identify these examples and first attempt to fix them by selecting a summary without such obvious failures from further down the beam (we use beam size $=10$ ). However, if we cannot find a "better" summary, we remove the first sentence of the article and re-sample a new summary to align with its noisy training. This latter strategy often results in factually incorrect summary generations, as is well documented in prior research (Maynez et al., 2020; Goyal and Durrett, 2021).</p>
<p>Design of the human study We design an A/B test to collect preference annotations. For each given article, annotators are shown summaries from all three summarization systems (BRIO, T0 and GPT3-D2). They are then asked to select their most and least preferred summary or summaries. In addition to these multiple choice questions, we also ask for a free-text justification of both choices.</p>
<p>We make two design decisions for our human study: first, we do not provide annotators with specific definitions of summary quality to avoid introducing our own biases. It is also quite challenging to produce a unified definition of quality for the very different "styles" of summaries evaluated in this study. Instead, we ask them to rely on their own preferences based on summaries they would like to see if they were browsing the web, which we believe to be a representative scenario for nonexpert consumers of news summaries. Detailed task instructions are included in Appendix F.</p>
<p>Second, we allow multiple selections for both the best and worst summary questions to cater to scenarios in which different summarization systems output similar quality summaries without meaningful differences.</p>
<p>We hire crowd annotators through Prolific. For both CNN and BBC, we recruit 60 unique participants to annotate the 100 summaries in each dataset. Each annotator was asked to annotate 5 articles and each article was annotated by 3 annotators. Additionally, we use the Prolific's demographic filters to restrict participation to USA (or UK) residents for CNN (or BBC). We anticipate that residents from these respective countries are better positioned to understand country-specific news events and evaluate their summaries. Participants were paid approximately $\$ 11 / \mathrm{hr}$ for their work.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Length Statistics</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">\% novel n-gms</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">#NEs per <br> 100 words</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">#sent</td>
<td style="text-align: center;">#words/sent</td>
<td style="text-align: center;">$n=1$</td>
<td style="text-align: center;">$n=2$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CNN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BRIO</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">12.9</td>
</tr>
<tr>
<td style="text-align: center;">T0</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">12.8</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-D2</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">10.5</td>
</tr>
<tr>
<td style="text-align: center;">BBC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BRIO</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">9.1</td>
</tr>
<tr>
<td style="text-align: center;">T0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">9.8</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-D2</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">8.5</td>
</tr>
</tbody>
</table>
<p>Table 2: Statistics for generated summaries evaluated in the human study across all datasets and summarization systems. We observe that GPT3-D2 generated summaries nearly always follow the sentence length constraints in their prompts.</p>
<h3>3.2 Results</h3>
<h2>Differences between summarization systems</h2>
<p>Figure 4 shows examples of generated summaries from all three summarization systems for both CNN and BBC articles. For CNN, we observe that fine-tuned BRIO summaries tend to be highly extractive and generally include a high number of named entities (dates, percentages, names), reflecting the data it was trained on. In contrast, GPT3-D2 summaries are more abstractive and less specific, but provide a more exhaustive overview of the article content. Table 2 provides quantitative evidence of this; we use percentage of novel n-grams to measure abstractiveness, and number of named entities per 100 words to measure specificity.</p>
<p>For BBC, we observe inverse trends where BRIO and T0 are more abstractive compared to GPT3-D2. Again, this can be attributed to the XSum training data used to train both these prior models. For GPT3-D2 summaries, on the other hand, the level of abstractiveness does not differ between datasets. Finally, Table 2 shows that GPT3-D2 summaries tend to have longer sentences, and therefore similar number of summary sentences often results in a longer summary for both datasets. We study the effect of this length difference on human preference judgments in Appendix B.</p>
<p>Which systems do humans prefer? Results of our human study are summarized in Table 3. We report the percentage of times a particular system is the most/least preferred model according to majority vote combining all three annotator's choices. ${ }^{6}$</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>BRIO</th>
<th></th>
<th>T0</th>
<th></th>
<th>GPT3</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Best $\uparrow$</td>
<td>Worst $\downarrow$</td>
<td>Best $\uparrow$</td>
<td>Worst $\downarrow$</td>
<td>Best $\uparrow$</td>
<td>Worst $\downarrow$</td>
</tr>
<tr>
<td>CNN</td>
<td>36</td>
<td>24</td>
<td>8</td>
<td>67</td>
<td>58</td>
<td>9</td>
</tr>
<tr>
<td>BBC</td>
<td>20</td>
<td>56</td>
<td>30</td>
<td>29</td>
<td>57</td>
<td>15</td>
</tr>
</tbody>
</table>
<p>Table 3: Percentage of times a summarization system is selected as the best or worst according to majority vote (may be tied). Human annotators have a clear preference for GPT3-D2 for both CNN and BBC style summaries.</p>
<p>Across both datasets and styles, we observe a clear preference for GPT3-D2 summaries compared to the other two models. In fact, in both scenarios, the GPT3-D2 outperforms the next best model by at least 20 percentage points. This improvement is statistically significant according to a paired bootstrap test (CNN $p-$ value $=2 \times 10^{-3}$, BBC $p-$ value $=6 \times 10^{-4}$ ).</p>
<p>Note that the next best model differs between the two datasets. For BBC, annotators prefer T0 summaries over BRIO. Annotator rationales often mentioned misleading or incorrect information as the primarily reason for selecting BRIO as the worst summary, confirming the issues that have been observed with XSum-trained models (Maynez et al., 2020; Pagnoni et al., 2021; Goyal and Durrett, 2021). Although T0 also includes XSum training data, we hypothesize that its multi-task framework helps offset the noisy signal from XSum.</p>
<p>In contrast, annotators rate T0 as the worst summarization system for CNN. The most common rationales for these were shorter length and inclusion of irrelevant details, e.g. long quotes, while missing key points. Some annotators also commented that these T0 summaries were less coherent compared to the other models. Interestingly, we did not observe similar complaints for the singlesentence T0 summaries for BBC.</p>
<p>Do annotators agree with each other? To study this, we plot the distribution of annotator votes for each summarization system and dataset in Figure 5. Additionally, we report the inter-annotator agreement, measured using Krippendorff's alpha with MASI distance (Passonneau, 2006), to account for multiple selections of best or worst summary allowed in our study design.</p>
<p>The vote distribution shows that although more annotators prefer GPT3-D2 summaries, this choice is only unanimous, i.e. supported by all three annotators, for less that $30 \%$ of the annotated articles.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 5: Annotator vote distribution for best and worst summaries across all datasets and models. Although GPT3-D2 is the clear winner according to majority vote, this choice is unanimous for less than $30 \%$ of the articles. This demonstrates the inherent variance in different annotators' definitions of "best summary", especially when comparing high-quality summaries from strong models.</p>
<p>Conversely, although BRIO (or T0) summaries are less preferred than GPT3-D2 for the CNN (or BBC) dataset on aggregate, they were voted as the best summary by at least one annotator for more than $60 \%$ of the articles. This demonstrate two things: first, when comparing summaries from two strong models, the choice is inherently ambiguous (similar observations in Clark et al. (2021)). Second, these results and the diversity in the written rationales, show that there does not exist a universal definition of a "good" summary and that different summary properties appeal to different annotators. Regardless, the aggregate preference for GPT3-D2 is high enough across the board to give us confidence in its strength.</p>
<p>How do these results impact the field? Progress in text summarization research in the last five years has been enabled by the construction of large-scale text summarization datasets that involved scraping news articles and pairing them with any available summary-like data (Hermann et al., 2015; Narayan et al., 2018; Grusky et al., 2018). The CNN/DM dataset considers bullet points accompanying news articles as its summary. These "gold" standard summaries provided useful training signal to train impressive supervised models (Lewis et al., 2020; Zhang et al., 2020; Liu et al., 2022) and hence, their quality or alignment with human preferences was largely ignored.</p>
<p>We found that, despite its popularity, XSum is largely unsuitable for fine-tuning models like BRIO</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Model</th>
<th>Overlap-Based</th>
<th></th>
<th></th>
<th>Similarity-Based</th>
<th></th>
<th>QAEval</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>ROUGE(1/2/L)</td>
<td>METEOR</td>
<td>BLEU</td>
<td>BERTScore</td>
<td>MoverScore</td>
<td>EM</td>
<td>F1</td>
</tr>
<tr>
<td>CNN</td>
<td>PEGASUS</td>
<td>34.85/14.62/28.23</td>
<td>.24</td>
<td>7.1</td>
<td>.858</td>
<td>.229</td>
<td>.105</td>
<td>.160</td>
</tr>
<tr>
<td></td>
<td>BRIO</td>
<td>38.49/17.08/31.44</td>
<td>.31</td>
<td>6.6</td>
<td>.864</td>
<td>.261</td>
<td>.137</td>
<td>.211</td>
</tr>
<tr>
<td></td>
<td>T0</td>
<td>35.06/13.84/28.46</td>
<td>.25</td>
<td>5.9</td>
<td>.859</td>
<td>.238</td>
<td>.099</td>
<td>.163</td>
</tr>
<tr>
<td></td>
<td>GPT3-D2</td>
<td>31.86/11.31/24.71</td>
<td>.25</td>
<td>3.8</td>
<td>.858</td>
<td>.216</td>
<td>.098</td>
<td>.159</td>
</tr>
<tr>
<td>DailyMail</td>
<td>PEGASUS</td>
<td>45.77/23.00/36.65</td>
<td>.33</td>
<td>12.2</td>
<td>.865</td>
<td>.308</td>
<td>.159</td>
<td>.229</td>
</tr>
<tr>
<td></td>
<td>BRIO</td>
<td>49.27/24.76/39.21</td>
<td>.37</td>
<td>11.7</td>
<td>.871</td>
<td>.331</td>
<td>.175</td>
<td>.259</td>
</tr>
<tr>
<td></td>
<td>T0</td>
<td>42.97/19.04/33.95</td>
<td>.28</td>
<td>8.9</td>
<td>.863</td>
<td>.290</td>
<td>.121</td>
<td>.184</td>
</tr>
<tr>
<td></td>
<td>GPT3-D2</td>
<td>38.68/14.24/28.08</td>
<td>.26</td>
<td>6.6</td>
<td>.859</td>
<td>.248</td>
<td>.101</td>
<td>.159</td>
</tr>
<tr>
<td>XSum</td>
<td>PEGASUS</td>
<td>47.97/24.82/39.63</td>
<td>.36</td>
<td>9.8</td>
<td>.901</td>
<td>.362</td>
<td>.145</td>
<td>.221</td>
</tr>
<tr>
<td></td>
<td>BRIO</td>
<td>49.66/25.97/41.04</td>
<td>.39</td>
<td>10.6</td>
<td>.901</td>
<td>.372</td>
<td>.139</td>
<td>.224</td>
</tr>
<tr>
<td></td>
<td>T0</td>
<td>44.20/20.72/35.84</td>
<td>.34</td>
<td>8.0</td>
<td>.896</td>
<td>.340</td>
<td>.125</td>
<td>.208</td>
</tr>
<tr>
<td></td>
<td>GPT3-D2</td>
<td>28.78/7.64/20.60</td>
<td>.19</td>
<td>2.2</td>
<td>.869</td>
<td>.197</td>
<td>.066</td>
<td>.119</td>
</tr>
<tr>
<td>Newsroom</td>
<td>PEGASUS</td>
<td>39.21/27.73/35.68</td>
<td>.39</td>
<td>.14</td>
<td>.873</td>
<td>.272</td>
<td>0.182</td>
<td>0.253</td>
</tr>
<tr>
<td></td>
<td>BRIO</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>T0</td>
<td>25.64/9.49/21.41</td>
<td>.20</td>
<td>.04</td>
<td>.849</td>
<td>.145</td>
<td>.080</td>
<td>0.125</td>
</tr>
<tr>
<td></td>
<td>GPT3-D2</td>
<td>27.44/10.67/22.18</td>
<td>.22</td>
<td>.05</td>
<td>.859</td>
<td>.159</td>
<td>.089</td>
<td>0.142</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance of different summarization systems measured using reference-based automatic metrics. Across all datasets, we observe that automatic metrics report substantially worse results for GPT3-D2 summaries compared to fine-tuned models. This directly contradicts the human preference results from Section 3, demonstrating that these reference-based metrics cannot reliably compare the quality of prompt-based summaries against fine-tuned summaries.
for realistic summarization settings. Even though a CNN/DM-trained BRIO model performed better, the results of our human study question the continued utility of hill-climbing on this dataset, as it seems users may simply prefer a different style of summary altogether. In fact, this preference for GPT3-D2 is much larger than incremental improvements reported in other human evaluation settings, e.g. improvements on XSum on the GENIE leaderboard <em>Khashabi et al. (2022)</em>. Furthermore, as we we will see in Section 5, the greater flexibility of GPT3-D2 compared to these systems makes it more suitable for news summarization tasks beyond generic summarization.</p>
<p>If a system designer collects a large-scale dataset of high-quality summaries that they wish to emulate, we believe a fine-tuned system may outperform GPT3-D2. However, better-trained models on datasets collected via “incidental” supervision are less likely to help.</p>
<h2>4 Can current automatic metrics evaluate GPT3-D2 summaries?</h2>
<p>Automatic metrics proposed for summarization evaluation can be broadly divided into two categories: (1) reference-based, that compare generated summaries against available gold summaries, and (2) reference-free that only rely on the input document. Here, we compare their performance at evaluating prompt-based GPT3-D2 summaries.</p>
<p>Experimental Setup We evaluate automatic metrics using summaries from 4 different summarization datasets, listed in Table 1. For each dataset, we construct our evaluation sets by randomly sampling $500^{7}$ articles from the standard test split. ${ }^{8}$ We compare the same 3 summarization systems from Section 3 in our analysis. Additionally, we also report results using the fine-tuned PEGASUS model <em>Zhang et al. (2020)</em>, as BRIO fine-tuned models are not available for all datasets.</p>
<p>We publicly release this corpus of summarization outputs to standardize the test sets and support future research into GPT3-D2 based summarization. Link: https://tagoyal.github.io/ zeroshot-news-annotations.html.</p>
<h3>4.1 Reference-based metrics</h3>
<p>Here, we study if the gold summaries of the standard datasets are useful for evaluation, especially when evaluating prompt-based summaries that are not trained to emulate the gold. We benchmark</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Model</th>
<th>Overall Quality</th>
<th></th>
<th>Factuality (QA-based)</th>
<th></th>
<th>Factuality (NLI-based)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>SUPERT</td>
<td>BLANC</td>
<td>QuestEval</td>
<td>QAFactEval</td>
<td>FactCC</td>
<td>DAE</td>
<td>SummaC</td>
</tr>
<tr>
<td>CNN</td>
<td>PEGASUS</td>
<td>.5466</td>
<td>.0605</td>
<td>.7373</td>
<td>4.4071</td>
<td>.3743</td>
<td>.8223</td>
<td>.1138</td>
</tr>
<tr>
<td></td>
<td>BRIO</td>
<td>.5586</td>
<td>.0802</td>
<td>.7334</td>
<td>3.8332</td>
<td>.1817</td>
<td>.7577</td>
<td>-.0532</td>
</tr>
<tr>
<td></td>
<td>T0</td>
<td>.5330</td>
<td>.0558</td>
<td>.7799</td>
<td>3.7517</td>
<td>.2012</td>
<td>.7556</td>
<td>-.0605</td>
</tr>
<tr>
<td></td>
<td>GPT3-D2</td>
<td>.5560</td>
<td>.0749</td>
<td>.7249</td>
<td>3.6399</td>
<td>.2428</td>
<td>.6671</td>
<td>-.0729</td>
</tr>
<tr>
<td>DailyMail</td>
<td>PEGASUS</td>
<td>.6433</td>
<td>.1137</td>
<td>.7536</td>
<td>4.4677</td>
<td>.5152</td>
<td>.8497</td>
<td>.2402</td>
</tr>
<tr>
<td></td>
<td>BRIO</td>
<td>.6360</td>
<td>.1217</td>
<td>.7415</td>
<td>4.1362</td>
<td>.3699</td>
<td>.8118</td>
<td>.0153</td>
</tr>
<tr>
<td></td>
<td>T0</td>
<td>.5995</td>
<td>.0889</td>
<td>.7803</td>
<td>3.9827</td>
<td>.2431</td>
<td>.8043</td>
<td>.0478</td>
</tr>
<tr>
<td></td>
<td>GPT3-D2</td>
<td>.6118</td>
<td>.0983</td>
<td>.7461</td>
<td>3.8279</td>
<td>.2697</td>
<td>.6990</td>
<td>.0365</td>
</tr>
<tr>
<td>XSum</td>
<td>PEGASUS</td>
<td>.4439</td>
<td>.0249</td>
<td>.8233</td>
<td>2.0089</td>
<td>.2465</td>
<td>.3598</td>
<td>-.2993</td>
</tr>
<tr>
<td></td>
<td>BRIO</td>
<td>.4459</td>
<td>.0230</td>
<td>.8305</td>
<td>1.8626</td>
<td>.2031</td>
<td>.3040</td>
<td>-.3292</td>
</tr>
<tr>
<td></td>
<td>T0</td>
<td>.4538</td>
<td>.0238</td>
<td>.7957</td>
<td>2.0330</td>
<td>.2219</td>
<td>.3392</td>
<td>-.3037</td>
</tr>
<tr>
<td></td>
<td>GPT3-D2</td>
<td>.5060</td>
<td>.0594</td>
<td>.8064</td>
<td>2.9492</td>
<td>.3977</td>
<td>.6372</td>
<td>-.2626</td>
</tr>
<tr>
<td>Newsroom</td>
<td>PEGASUS</td>
<td>.6286</td>
<td>.1131</td>
<td>.7118</td>
<td>4.2120</td>
<td>.7218</td>
<td>.7956</td>
<td>.2418</td>
</tr>
<tr>
<td></td>
<td>BRIO</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>T0</td>
<td>.5433</td>
<td>.0640</td>
<td>.7511</td>
<td>3.5799</td>
<td>.2828</td>
<td>.7376</td>
<td>.0261</td>
</tr>
<tr>
<td></td>
<td>GPT3-D2</td>
<td>.5408</td>
<td>.0599</td>
<td>.7160</td>
<td>3.2336</td>
<td>.3988</td>
<td>.6564</td>
<td>-.0729</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance of different summarization systems, as scored by automatic reference-free evaluation metrics from the summarization literature. Similar to reference-based metrics, these also generally fail to produce the same system rankings as human preferences reliably across datasets.
the performance of 3 different summarization metrics: (1) overlap-based metrics, specifically ROUGE (Lin, 2004) METEOR (Banerjee and Lavie, 2005) and BLEU (Papineni et al., 2002). (2) similarity-based metrics, that compute similarity between embeddings representations of generated and reference summaries. Specifically, we report BERTScore (Zhang* et al., 2020) and MoverScore (Zhao et al., 2019). (3) a QA-based metric, specifically QAEval (Deutsch et al., 2021). Although most QAmetrics are reference-free (discussed in Section 4.2), QAEval uses the reference summaries to indicate saliency. We report both exact match (EM) and F1 components of QAEval.</p>
<p>Results Table 4 outlines the results. It shows that BRIO and PEGASUS models, fine-tuned to emulate the reference summaries, outperform GPT3-D2 summaries according to all reference-based automatic metrics. The difference in their assigned scores is very high, e.g. $&gt;7$ ROUGE-L points between GPT3-D2 and BRIO. For comparison, these reported scores for GPT3-D2 are even lower than the trivial Lead-3 baseline reported in prior work (Fabbri et al., 2021; Grusky et al., 2018). This clearly demonstrates that current automatic referencebased metrics cannot be used to reliably measure summary quality under the prompting paradigm.</p>
<p>Amongst prompting-based models, we observe that T0 summaries report better metric scores than GPT3-D2 for all datasets except Newsroom. Inter-
estingly, out of the four datasets evaluated here, Newsroom is the only one not used to train the T0 model. This further shows that access to datasetspecific reference summaries during training improves performance according to these metrics, rendering them unsuitable for evaluating prompt-based models.</p>
<h3>4.2 Reference-free metrics</h3>
<p>Next, we investigate whether current reference-free evaluation metrics reflect the human preference rankings between summarization systems, as observed in Section 3. Here, we study 2 categories of metrics: (1) quality metrics, specifically SUPERT (Gao et al., 2020), which evaluates generated summaries against automatically identified salient sentences in the input, and BLANC (Vasilyev et al., 2020), which evaluates summaries on language understanding tasks. We refer readers to the original papers for detailed explanation of these. (2) factuality metrics, that are evaluate whether generated summaries contain incorrect information with respect to the source article. We report the performance of summarization systems using two QAbased metrics: QuestEval (Scialom et al., 2021) and QAFactEval (Fabbri et al., 2022). Additionally, we also benchmark entailment-based metrics: FactCC (Kryscinski et al., 2020), DAE (Goyal and Durrett, 2020, 2021) and SummaC (Laban et al.,</p>
<p>2022). ${ }^{9}$ These entailment-based models are designed for classification into factual or non-factual; therefore, we use $P($ factual $\mid$ article, summary $)$ to score generated summaries.</p>
<p>Results Table 5 outlines the scores for each summarization system according to the above referencefree metrics. Ideally, we want the relative rankings of different systems according to these metrics to correspond to human preferences, i.e. GPT3-D2 &gt; BRIO $&gt;$ T0 for CNN/DM ${ }^{10}$ and GPT3-D2 $&gt;$ T0 $&gt;$ BRIO for XSum. ${ }^{11}$</p>
<p>Overall, we observe that none of the referencefree metrics we evaluate follow these trends for both CNN/DM and XSum datasets. In particular, we observe that GPT3-D2 summaries report low factuality scores (except XSum) even though we rarely found any factual errors in our qualitative analysis of its generated summaries.</p>
<p>Interestingly, we noticed a roughly inverse relation to abstractiveness; summarization systems that generated more abstractive summaries (see Table 2) were generally scored lower by all automatic reference-based metrics. For instance, GPT3-D2 is scored lower than BRIO by both quality metrics for all datasets except XSum; the latter is the only dataset for which GPT3-D2 summaries are less abstractive. Such shortcomings of reference-free evaluation metrics due to spurious correlations have also been studied in prior work (Durmus et al., 2022). These issues become more exaggerated when the summarization systems being compared exhibit very different properties.</p>
<p>Discussion On the surface, the failure of reference-free metrics at evaluating GPT3-D2 summaries is more surprising that reference-based metrics as the later explicitly compares generated summaries with references that GPT3-D2 is not trained to imitate. Therefore, GPT3-D2 understandably scores lower than fine-tuned systems.</p>
<p>However, we note two different issues with reference-free metrics: (1) Some of these, e.g. FactCC and DAE, use reference summaries as positive examples to train the metric. Therefore, al-</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>though "reference-free" at test time, they are still trained to reward the summary properties seen in the standard summarization benchmarks. (2) Even completely reference-free metrics, e.g. QuestEval and QAFactEval, have only been evaluated on reference-based benchmarks and fine-tuned models. Therefore, the choice of different components, such as question answering or question generation models to use, etc. has been dictated by the error space of prior fine-tuned models (Tang et al., 2023). These decisions also now need to be re-visited to incorporate GPT3-D2 evaluation; we leave this for future work.</p>
<h2>5 Beyond Generic Summarization</h2>
<p>Previously, we observed that GPT3-D2 models faithfully follow simple "style" instructions in the given prompts. This provides a promising direction to tackle other use cases in news summarization beyond the generic summarization task from Section 3.</p>
<p>Different users can have very different information needs from the same article, all of which cannot be satisfied with a single generic summary. Prior work has introduced several task formulations to address this gap, including keyword-focused (He et al., 2022a), query-focused (Baumel et al., 2014; He et al., 2022a), or aspect-focused summarization (Krishna and Srinivasan, 2018; Ahuja et al., 2022), amongst others. Here, we evaluate GPT3-D2 performance at two of these use cases.</p>
<p>In keyword-based summarization, the output summaries must succinctly summarize the input document focusing on a given keyword; these generally correspond to specific entities or events directly mentioned in the document. In contrast, the control units in aspect-based summarization are high-level topics that can be common across multiple similar types of documents. For e.g., for the input article in Figure 1, Donald Trump or Russian interference in 2016 elections are keyword controls whereas charges against the defendants is a higherlevel aspect that can serve as the query for any news article discussing a lawsuit or investigation.</p>
<h3>5.1 Qualitative Analysis</h3>
<p>Baseline Model for comparison We use the recently proposed CTRLSum (He et al., 2022a), a finetuned BART model, as our baseline. It can be flexibly adapted for both keyword- and aspect-based settings by including a prompt as additional input</p>
<p>Article: Republican defenders of Donald Trump won't or can't answer the questions that are at the root of the intrigue over why classified documents were at Mar-a-Lago and the troubling question of whether national security was put at risk. [...] At one end of the scale is Missouri Sen. Josh Hawley, a firm Trump backer who is often mentioned as a future presidential candidate once the ex-President finally exits the political stage. Hawley promoted Trump's line that the key issue was the FBI search of an ex-President's home, not what he might have kept there. "I don't know what he has or doesn't have, but what I have a lot of concerns about is the FBI raiding the home of the likely presidential nominee in 2024, the former President of the United States, and particularly as we learn more and more about what's going on with the FBI." Hawley told CNN's Manu Raja on Wednesday, [...] Ex-Attorney General William Barr has repeatedly questioned publicly what his former boss was doing with classified material. There has been speculation that Trump might want such material as keepsakes or as some kind of leverage in a future business deal. But none of this is informed given the secrecy sounding the DOD's investigation. [...]</p>
<p>Link: https://www.cnn.com/2022/09/08/politics/republicans-trump-national-security-implications-classified-docs</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Keyword-based</th>
<th style="text-align: center;">CTRLSum: Sen. Marco Rubio downplays the seriousness of the Department of Justice's investigation. Ex-Attorney General William Barr has repeatedly questioned what his former boss was doing with classified material. Trump and his allies have filled the vacuum for answers with a smokescreen of unproven claims, including that Trump declassified the material.</th>
<th style="text-align: center;">- Missing context <br> - Poor discourse</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Summarize the above article in 2-3 sentences focusing on William Barr.</td>
<td style="text-align: center;">GPT3-D2: The article discusses how various Republicans have been defending Donald Trump in light of the recent revelations that he had classified documents at his Mar-a-Lago resort. While some have downplayed the significance of the situation, others have outright denied that there is any cause for concern. However, former Attorney General William Barr has repeatedly questioned Trump's actions, speculating that he may have been trying to use the classified information for personal gain.</td>
<td style="text-align: center;">$\leftarrow$ Includes article context before keyword-specific information</td>
</tr>
<tr>
<td style="text-align: center;">Aspect-based <br> In the above article, who is a defendant or under investigation?</td>
<td style="text-align: center;">CTRLSum: Republican defenders of Donald Trump won't or can't answer the questions that are at the root of the intrigue over why classified documents were at Mar-a-Lago and the troubling question of whether national security was put at risk. In the article, who is a defendant or under investigation?</td>
<td style="text-align: center;">- Does not directly address aspect. <br> - Repeats the prompt at the end. <br> - Incorrectly answers the query.</td>
</tr>
<tr>
<td style="text-align: center;">In the above article, what is the defendant's reaction to charges, including admission of guilt, denial of charges, or explanations?</td>
<td style="text-align: center;">GPT3-D2: Donald Trump. The FBI. The Department of Justice. Sandy Berger.</td>
<td style="text-align: center;">- Does not directly answer query. <br> Summary captures aspect-relevant context.</td>
</tr>
</tbody>
</table>
<p>Figure 6: Comparison of keyword- and aspect-based summaries using GPT3-D2 and CTRLSum models. The GPT3-D2 prompt is shown on the left with the corresponding keyword or aspect bolded. For keyword-based summarization, the GPT3-D2 summary presents appropriate context before the keyword-specific information. However, for aspect-based summarization, it does not always generate factually correct summaries, as shown in the first aspect example. We observe that CTRLSum performs poorly for both these settings.
to the encoder. We use the prompt template recommended in the original paper. ${ }^{12}$</p>
<p>Control Units For the keyword-focused setting, we use named entities extracted from the input article as the control units. For aspect-focused summarization, we directly use the aspects introduced in the guided summarization task from TAC 2011. ${ }^{13}$ It defined 5 broad categories of newswire articles, such as accidents and natural disasters, investigations and trial, etc., and multiple aspects for each category. For example, the "investigations and trials" category includes aspects such as "who is the defendant or under trial?", "who is investigating, prosecuting, judging?", and so on.</p>
<p>Qualitative Analysis Figure 6 shows examples of keyword- and aspect-focused summaries using GPT3-D2 and the baseline CTRLSum model. The keywords or aspects are highlighted in bold within the GPT3-D2 prompt displayed on the left.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup>In this example, representative of average GPT3-D2 quality, the keyword-focused GPT3-D2 summary first gives a brief overview of the article setting before providing keywordrelevant information. In contrast, the CTRLSum summary exhibits poor discourse structure and reads like a list of facts stapled together.</p>
<p>The figure also shows aspect-focused summaries for two aspects associated with the "investigations and trial" category most appropriate for the chosen article. We see mixed results here for GPT3-D2; it generates a factually incorrect summary for the first aspect, listing multiple people from the input article as defendants instead of only "Donald Trump". For the second aspect, it correctly maps the highlevel concept "defendant" to "Donald Trump" in the input article and generates the correct answer to the input query: "The defendant's reaction to charges in the above article is denial of charges".</p>
<p>On the other hand, CTRLSum fails to generate aspect-focused summaries for both cases. We believe that it struggles to align high-level concepts and explicit entities in the article due to a lack of</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 7: Distribution of annotator votes for the keyword-focused summarization task. Annotators prefer GPT3-D2 summaries over CTRLSum for approximately 70% of all article-keyword pairs, showing unanimous preference more than half the time.</p>
<p>Such aspect-specific examples in its training data. Instead, it generates summaries focusing on lexically similar words, i.e., "defenders" for both cases.</p>
<p>Based off of GPT3-D2's promising keyword-focused summarization capabilities observed above, we next conduct a human study to systematically compare it against the CTRLSum baseline. We leave further explorations of aspect-based summarization to future work, given the mixed to poor results for both models at this task.</p>
<h3>5.2 Human Study: Keyword-focused summarization</h3>
<p><strong>Task Setup</strong> Similar to Section 3, we design an A/B test to compare the two models. We use the same set of 100 CNN<sup>14</sup> articles as Section 3. We randomly extract 2 distinct named entities from each article. In the study interface, the annotator is shown the article-keyword pair and GPT3-D2 and CTRLSum summaries corresponding to it. They are asked to select the summary that best summarizes the input article while focusing on the given keyword. Exact task instructions are included in Appendix F.</p>
<p>Again, we run this study using the Prolific platform. We recruit 60 participants to annotate the 100 articles; each article is annotated by 3 annotators which includes annotations for 2 separate keywords. Each annotator evaluates 5 articles.</p>
<p><strong>Results</strong> Figure 7 shows the distribution of annotator votes between the GPT3-D2 and CTRLSum models. Annotators show a clear preference for GPT3-D2. In fact, for nearly 70% of all article-keyword pairs, GPT3-D2 is preferred over CTRLSum by a majority of the annotators. The main rationales given for this choice were better contextualization of keyword-related information and better coherence in GPT3-D2 summaries.</p>
<p><strong>Impact</strong> These results show that prompting GPT-3 models present a promising alternative to fine-tuned models for such specialized summarization tasks that can be easily described using textual prompts. One of the major drawbacks of fine-tuned models is that they are constrained by what data is available and how it can be transformed to create new task-specific training data. CTRLSum relied on the SQuAD question answering dataset (Rajpurkar et al., 2016) because the required "queries" or "questions" were unavailable at scale for summaries in standard summarization datasets. In contrast, prompt-based models are not constrained by the availability of task-specific data and can flexibly adapt to new tasks. Future research should focus on further exploring these capabilities and possible improvements on currently "unsolved" tasks such as aspect-based or plan-based summarization.</p>
<h2>6 Discussion and Related Work</h2>
<p>In recent years, research in text summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Lewis et al., 2020; Zhang et al., 2020; Liu et al., 2022) has typically relied on comparisons with gold test sets for evaluation, possibly augmented with reference-free metrics for dimensions like factuality. This paper shows that <strong>all these metrics are completely ineffective at evaluating GPT-3 summaries</strong>. Although issues with these metrics, particularly low correlation with human judgments, have also been studied earlier (Fabbri et al., 2021; Deutsch and Roth, 2021), they are considered reliable when comparing systems in different score ranges (Peyrard, 2019; Deutsch et al., 2022). However, GPT-3 challenges these established practices and evaluation protocols, and poses an urgent need for better evaluation.</p>
<p>This brings us to manual evaluation, generally considered to be the gold standard for generation evaluation. The majority of summarization research now reports results from a human study in addition to automatic metrics, but there is a general lack of consensus on what dimensions to evaluate, task design, and other factors (Hardy et al., 2019). This presents difficulties in conducting reliable and reproducible comparisons between systems (Karpinska et al., 2021), another factor con-</p>
<p><sup>14</sup>We run this study using only CNN articles as the baseline CTRLSum model is trained on CNN/DM.</p>
<p>tributing to the popularity of automatic metrics. Although recent efforts like GENIE (Khashabi et al., 2022) have taken steps to standardize manual evaluation protocols across systems, its annotation is not universally affordable and the quality is not strictly monitored. We hope that future work addresses these challenges and democratizes human evaluations.</p>
<p>The ultimate test of summarization systems is with actual users using the systems in practice. Jones (2007) discusses the need to align task formulations with actual applications scenarios ("purpose factors"). However, the research in text summarization until now has been constrained to certain problems or domains by the heavy dependence on large-scale training data: for example, producing a bullet-point summary of a news article has emerged as standard due to availability of data from CNN, not because it is shown to be the best way to present information.</p>
<p>Now, the success of prompt-based models can allow realistic use-cases to drive research in a more top-down way. We already show that GPT3-D2 improves upon prior keyword-focused summarization systems that were trained on artificially adapted training data. In future research, we are interested in tackling other real world use cases, such as update summarization and plan- or aspect-based summarization. Additionally, adapting GPT3-D2 to documents longer than the allowed context, or structured inputs such as tables, presents research challenges beyond the current capabilities of GPT-3 and would be interesting to study.</p>
<h2>7 Conclusion</h2>
<p>In this work, we performed the first systematic study comparing prompt-based GPT-3 and finetuned models at the news summarization task. We analyzed the impact of prompting on the summarization field, including training paradigms and evaluation practices. Finally, to support further research in this direction, we release a large corpus of generated summaries for multiple prompt-based and fine-tuned models, as well as human preference judgments comparing these systems.</p>
<h2>8 Limitations</h2>
<p>In the text generation evaluation literature, there does not exist a standardized task design for com-</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup>paring different system generations. In our work, we chose a human evaluation workflow that directly asks annotators to compare systems, while other prior work has opted for Likert-scale judgments and/or evaluation along multiple quality dimensions (Gehrmann et al., 2022). The latter strategy of evaluating different dimensions could surface more insights into which "style" properties of GPT3 summaries provide them an edge over fine-tuned models; however, such analysis is outside the scope of this paper. Our experiments comparing overall quality reveal that current summarization datasets are not well-aligned with user preferences. We leave more fine-grained analysis into these preference judgments for future work.</p>
<p>The experiments in this paper are run on Englishlanguage news summarization datasets as these serve as common benchmarks in the summarization literature. However, user rankings of system outputs might be different when evaluating other domains, e.g., summaries of scientific text. While we believe that automatic metrics would fail to evaluate GPT-3 summaries on these domains also (generated summaries would still look different from the reference summaries), users may prefer models that are specifically fine-tuned on domain-specific data for niche domains.</p>
<p>Finally, we do not know exact datasets or tasks used to train GPT3-D2. It is possible that its RLHF training (Ouyang et al., 2022) included summarization examples, and therefore, preference judgments from human annotators for its different outputs. However, our arguments in this paper do not rely on the specifics of the GPT3-D2 system, merely that such a system exists. If anything, the existence of potentially better data underscores that further work should collect new data for summarization model tuning, and our claims about metrics still hold regardless of the details of how the GPT3-D2 summaries were produced.</p>
<h2>References</h2>
<p>Ojas Ahuja, Jiacheng Xu, Akshay Gupta, Kevin Horecka, and Greg Durrett. 2022. ASPECTNEWS: Aspect-oriented summarization of news documents. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6494-6506.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation</p>
<p>measures for machine translation and/or summarization, pages 65-72.</p>
<p>Tal Baumel, Raphael Cohen, and Michael Elhadad. 2014. Query-chain focused summarization. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 913-922.</p>
<p>Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, and Pengfei Liu. 2020. Metrics also disagree in the low scoring range: Revisiting summarization evaluation metrics. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5702-5711.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Dallas Card, Peter Henderson, Urvashi Khandelwal, Robin Jia, Kyle Mahowald, and Dan Jurafsky. 2020. With little power comes great responsibility. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9263-9274.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A Smith. 2021. All that's 'human' is not gold: Evaluating human evaluation of generated text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7282-7296.</p>
<p>Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615-621, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth. 2021. Towards question-answering as an automatic metric for evaluating the content quality of a summary. Transactions of the Association for Computational Linguistics, 9:774-789.</p>
<p>Daniel Deutsch, Rotem Dror, and Dan Roth. 2022. Reexamining system-level correlations of automatic
summarization evaluation metrics. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 6038-6052, Seattle, United States. Association for Computational Linguistics.</p>
<p>Daniel Deutsch and Dan Roth. 2021. Understanding the extent to which content quality metrics measure the information quality of summaries. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 300-309.</p>
<p>Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 50555070 .</p>
<p>Esin Durmus, Faisal Ladhak, and Tatsunori B Hashimoto. 2022. Spurious correlations in referencefree evaluation of text generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1443-1454.</p>
<p>Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. QAFactEval: Improved QAbased factual consistency evaluation for summarization. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2587-2601, Seattle, United States. Association for Computational Linguistics.</p>
<p>Alexander R Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409.</p>
<p>Yang Gao, Wei Zhao, and Steffen Eger. 2020. SUPERT: Towards new frontiers in unsupervised evaluation metrics for multi-document summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 13471354 .</p>
<p>Sebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. 2022. Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text. arXiv preprint arXiv:2202.06935.</p>
<p>Tanya Goyal and Greg Durrett. 2020. Evaluating factuality in generation with dependency-level entailment. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3592-3603.</p>
<p>Tanya Goyal and Greg Durrett. 2021. Annotating and modeling fine-grained factuality in summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1449-1462.</p>
<p>Tanya Goyal, Jiacheng Xu, Junyi Jessy Li, and Greg Durrett. 2022. Training dynamics for text summarization models. In Findings of the Association for Computational Linguistics: ACL 2022, pages 20612073.</p>
<p>Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 708-719.</p>
<p>Hardy Hardy, Shashi Narayan, and Andreas Vlachos. 2019. Highres: Highlight-based reference-less evaluation of summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3381-3392.</p>
<p>Junxian He, Wojciech Kryscinski, Bryan McCann, Nazneen Rajani, and Caiming Xiong. 2022a. CTRLsum: Towards generic controllable text summarization. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5879-5915, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Pengcheng He, Baolin Peng, Liyang Lu, Song Wang, Jie Mei, Yang Liu, Ruochen Xu, Hany Hassan Awadalla, Yu Shi, Chenguang Zhu, et al. 2022b. Z-Code++: A pre-trained language model optimized for abstractive summarization. arXiv preprint arXiv:2208.09770.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in Neural Information Processing Systems, 28.</p>
<p>Karen Spärck Jones. 2007. Automatic summarising: The state of the art. Information Processing \&amp; Management, 43(6):1449-1481.</p>
<p>Marzena Karpinska, Nader Akoury, and Mohit Iyyer. 2021. The perils of using mechanical turk to evaluate open-ended text generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1265-1285.</p>
<p>Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A. Smith, and Daniel Weld. 2022. GENIE: Toward reproducible and standardized human evaluation for text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11444-11458, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Kundan Krishna and Balaji Vasan Srinivasan. 2018. Generating topic-oriented summaries using neural attention. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1697-1705.</p>
<p>Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346.</p>
<p>Wojciech Kryscinski, Nazneen Fatema Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir R Radev. 2021. BookSum: A collection of datasets for long-form narrative summarization.</p>
<p>Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022. SummaC: Re-visiting NLIBased models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. 2022. BRIO: Bringing order to abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2890-2903.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On Faithfulness and Factuality in Abstractive Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048-11064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470-3487.</p>
<p>Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence RNNs and beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 280-290.</p>
<p>Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018. Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797-1807.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4812-4829.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318.</p>
<p>Rebecca J Passonneau. 2006. Measuring agreement on set-valued items (MASI) for semantic and pragmatic annotation. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC'06).</p>
<p>Maxime Peyrard. 2019. Studying summarization evaluation metrics in the appropriate scoring range. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 50935100 .</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392.</p>
<p>Alexander M Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379-389.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted training enables zeroshot task generalization. In The Tenth International Conference on Learning Representations.</p>
<p>William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802.</p>
<p>Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021. QuestEval: Summarization asks for fact-based evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6594-6604.</p>
<p>Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10731083 .</p>
<p>Liyan Tang, Tanya Goyal, Alexander R Fabbri, Philippe Laban, Jiacheng Xu, Semih Yahvuz, Wojciech Kryściński, Justin F Rousseau, and Greg Durrett. 2023. Understanding factual errors in summarization: Errors, summarizers, datasets, error detectors. Association for Computational Linguistics.</p>
<p>Oleg Vasilyev, Vedant Dhamidharka, and John Bohannon. 2020. Fill in the BLANC: Human-free quality estimation of document summaries. In Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems, pages 11-20.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Xi Ye and Greg Durrett. 2022. The unreliability of explanations in few-shot prompting for textual reasoning. In Advances in Neural Information Processing Systems.</p>
<p>Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning, pages 11328-11339. PMLR.</p>
<p>Tianyi Zhang<em>, Varsha Kishore</em>, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text Generation with BERT. In International Conference on Learning Representations.</p>
<p>Yusen Zhang, Ansong Ni, Ziming Mao, Chen Henry Wu, Chenguang Zhu, Budhaditya Deb, Ahmed Awadallah, Dragomir Radev, and Rui Zhang. 2022. SummN: A multi-stage summarization framework for long input dialogues and documents: A multi-stage summarization framework for long input dialogues and documents. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1592-1604.</p>
<p>Yusen Zhang, Ansong Ni, Tao Yu, Rui Zhang, Chenguang Zhu, Budhaditya Deb, Asli Celikyilmaz, Ahmed Hassan, and Dragomir Radev. 2021. An exploratory study on long dialogue summarization: What works and what’s next. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4426–4433.
- Zhao et al. (2021) Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the International Conference on Machine Learning (ICML).
- Zhao et al. (2019) Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563–578.
- Zhao et al. (2020) Yao Zhao, Mohammad Saleh, and Peter J Liu. 2020. SEAL: Segment-wise extractive-abstractive long-form text summarization. arXiv preprint arXiv:2006.10213.</p>
<h2>Appendix A Implementation Details</h2>
<h3>Prompts Used</h3>
<p>To generate GPT3-D2 summaries for all experiments in this paper, we use the standard prompt format outlined in Section 2. We set $N=3$ for CNN and DailyMail, $N=2$ for Newsroom, and $N=1$ for XSum/BBC. For the latter, the prompt is slightly modified to "Summarize the above article briefly in 1 sentence."</p>
<p>For T0, we use the following prompts: a) CNN/DM: "Summarize the article below in 3 to 4 sentences?", b) Newsroom: "Summarize the article below in 2 to 3 sentences?", and c) XSum/BBC: "Summarize the article below in 1 sentence?"</p>
<h3>Factuality Metrics</h3>
<p>In Section 4.2, we evaluated several recently proposed factuality metrics. We note that multiple versions have been released for some of these models in recent years. Here, we specify the versions used in our experiments to ensure reproducibility of results:</p>
<ol>
<li>QuestEval: We use version 0.2.4 of the questeval python package and report numbers using the precision-only setting.</li>
<li>DAE: We use the updated version of the DAE model trained for document-level factuality. Latest code and model released at https://github.com/tagoyal/factuality-datasets.
<img alt="img-3.jpeg" src="img-3.jpeg" /></li>
</ol>
<p>Figure 8: Correlation between summary length and annotator score (computed as the no. of "best summary" votes. For each example, plot the difference in length (x-axis) and annotator score (y-axis) between the GPT3-D2 summary and the next best system's summary.</p>
<ol>
<li>SummaC: We use the SummaC-Conv model (model_name = 'vitc') and sentence-level granularity in our experiments.</li>
</ol>
<h3>Keyword-based data</h3>
<p>For our keyword-based human study, we extracted two named entities per article, as discussed in Section 5. In practice, we constrained the first keyword to be lead-biased, i.e. it was extracted from the first three sentences of the article, and the second keyword was extracted from the remaining article. As CNN-based summarization models are generally lead-biased, this allowed us to benchmark models under both settings.</p>
<h2>Appendix B Are annotator judgments of quality correlated with length?</h2>
<p>In Section 3, results of the human study showed that annotators provide shorter length as one of the main reasons for selecting T0 summaries as the worst for the CNN dataset. Here, we investigate if the choice between GPT3-D2 and BRIO is similarly influenced by their length differences; GPT3-D2 summaries are on average 9 words longer.</p>
<p>To study this, we plot the difference in summary length against the difference in annotator score (measured as the no. of votes for a summarization system) between the best summarization system (GPT3-D2) and the next best system (BRIO for CNN and T0 for BBC). The resulting plot is shown in Figure 8. In general we observe low correlation between these; Pearson’s $\rho$ is 0.17 for CNN and .02 for the BBC dataset. These correlation values cannot solely explain the large differences in annotator</p>
<table>
<thead>
<tr>
<th>BBC-style 1 sentence summaries</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT3-D1 Four men completed a world record by playing a board game</td>
<td></td>
</tr>
<tr>
<td>continuously for more than 85 hours.</td>
<td></td>
</tr>
<tr>
<td>GPT3-D2 The article is about four men who have completed a world record</td>
<td></td>
</tr>
<tr>
<td>by playing a board game continuously for more than 85 hours.</td>
<td></td>
</tr>
<tr>
<td>GPT3-D1 Eight people have been charged with murder after two men died in</td>
<td></td>
</tr>
<tr>
<td>a car crash on a dual carriageway.</td>
<td></td>
</tr>
<tr>
<td>GPT3-D2 Eight defendants have been charged with murder after two men</td>
<td></td>
</tr>
<tr>
<td>died in a car crash.</td>
<td></td>
</tr>
<tr>
<td>CNN-style 3 sentence summaries</td>
<td></td>
</tr>
<tr>
<td>GPT3-D1 Russian President Vladimir Putin's speech at Russia's Victory Day</td>
<td></td>
</tr>
<tr>
<td>commemorations offered few clues on the direction of the conflict</td>
<td></td>
</tr>
<tr>
<td>in Ukraine. UK defense chief Ben Wallace had suggested that Putin</td>
<td></td>
</tr>
<tr>
<td>may use this historic day to escalate his so-called "special military</td>
<td></td>
</tr>
<tr>
<td>operation" in Ukraine and declare an outright war. Putin used his</td>
<td></td>
</tr>
<tr>
<td>speech to blend history with the present, banking on Russian</td>
<td></td>
</tr>
<tr>
<td>nationalism on its most patriotic of holidays to justify his war.</td>
<td></td>
</tr>
<tr>
<td>GPT3-D2 The article discusses Russian president Vladimir Putin's speech on</td>
<td></td>
</tr>
<tr>
<td>Victory Day, in which he blended history with the present and</td>
<td></td>
</tr>
<tr>
<td>justified his war. It is speculated that Putin may use this day to</td>
<td></td>
</tr>
<tr>
<td>escalate the conflict in Ukraine, but it is also noted that he has few</td>
<td></td>
</tr>
<tr>
<td>successes in Ukraine to brag of. The question now is whether Putin</td>
<td></td>
</tr>
<tr>
<td>will use this day to escalate the war in other ways.</td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 9: Examples of generated summaries using the text-davinci-001 (GPT3-D1) and text-davinci-002 (GPT3-D2) versions. The figure shows both BBC and CNN-style summaries.
judgments reported in the human study results of Section 3; additional quality factors must have influenced this choice. Anecdotally, we observe that the GPT summaries are slightly less information dense; our impression is that these contain a similar level of information content, but are easier to read and understand despite being a bit more verbose.</p>
<h2>C Qualitative differences between GPT-3 versions</h2>
<p>Figure 9 shows examples comparing summaries from text-davinci-001 (GPT3-D1) to those from GPT3-D2. For BBC-style single sentence summaries, we observed that the two models generated very similar summaries with high content and lexical overlap. More variance is observed for CNNstyle summaries. In our anecdotal assessment, GPT3-D1 generated more detailed summaries while those from GPT3-D2 are less information dense.</p>
<h2>D Human study and API costs</h2>
<p>At the time of running our experiments, GPT-3 API's text-davinci-002 version was priced at $\$ 0.06$ per 1 K tokens. New pricing information is available at: https://openai.com/api/pricing/.</p>
<p>In our experiments, we generated around 2600 GPT3-D2 summaries across all experiments in Section 3 (human study), Section 4 (evaluation of metrics) and Section 5 (keyword-based human study).</p>
<p>Input Article: https://www.cnn.com/2021/09/07/opinions/ covid-19-good-and-bad-news-ranney/index.html</p>
<h2>Common prompt for all segments:</h2>
<p>Summarize the above article briefly in 2-3 sentences.
Summary for input segment 1: The article discusses how the current situation with Covid-19 cases rising again is similar to last year in some ways, but different in others. One big difference is that there are now vaccines available which decrease the risk of severe illness.</p>
<p>Summary for input segment 2: The article discusses how the current situation with Covid-19 cases rising again is similar to last year in some ways, but different in others. One big difference is that there are now vaccines available which decrease the risk of severe illness. The article advises that everyone should take precautions such as wearing masks and opening windows to let fresh air in.</p>
<p>Summary for input segment 3: The article discusses the challenges of living through the pandemic and the importance of science and community. It also highlights the need for patience, hope, and humility as we enter the next phase of the pandemic.</p>
<p>Figure 10: Illustrative example of GPT3-D2 summary of a long source article generated using the segment-thensummarize pipeline.</p>
<p>We spent a total of approximately $\$ 150$ on API requests.</p>
<p>For the human study, we paid participants $\$ 4$ per task (each task involved annotation for 5 articles). On average, this translated to $\$ 11 / \mathrm{hr}$ of work. The combined cost for the generic summarization (Section 3) and the keyword-based summarization (Section 5) studies was $\$ 1020$, including platform costs and bonus payments.</p>
<h2>E Long document summarization using GPT3-D2</h2>
<p>Summarization of long documents has attracted significant interest in recent years (Cohan et al., 2018; Kryscinski et al., 2021). Here, we study how naive prompting of GPT-3 performs at longdocument summarization.</p>
<p>First, we extract text from a long input article from the CNN website. ${ }^{16}$ Next, we follow the commonly used segment-then-summarize procedure from prior work (Zhao et al., 2020; Zhang et al., 2022). We divide the input article into 3 disjoint segments, summarize each segment separately and concatenate these outputs to form the final summary.</p>
<p>Figure 10 shows the prompt used and the generated summaries for each segment. While individual segment summaries are high quality, we can see that the concatenated summary is not coherent and includes repeated "introductory" sentences outlin-</p>
<p><sup id="fnref10:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ing similar content. Related to this, it also does not cover all important aspects of the input article as a majority of its 'length budget' is spent on a high-level overview. We also observed that the generated summaries for long documents often focus on less unimportant parts of the document, e.g. "...everyone should take the precaution of ... opening windows to let the fresh air in" in the illustrated example. This is, in part, due to the segmentation of the input article: GPT3-D2 still exhibits some lead bias and treats the beginning of the input segment as more salient. Therefore, the exact segmentation of the article also dictates the quality of the final summary, and cannot be readily fixed by altering the prompt.</p>
<p>These observations show that while GPT3-D2 produces superior segment-level summaries, it is more difficult to adapt it to "non-natural" text inputs without fine-tuning. Therefore, techniques that have shown promising results for fine-tuned models, e.g. segment-then-summarize or extract-thenabstract (Zhang et al., 2021) approaches, are not as effective when directly applied with promptingbased models.</p>
<h1>F Task Instructions</h1>
<p>Task instructions provided to crowd annotators for the generic summarization task setting are shown in Figure 14 and those for the keyword-based setting are shown in Figure 15.</p>
<h2>G Examples of generated summaries</h2>
<p>We show examples of generated summaries for articles for generic summarization for CNN-2022 and BBC-2022 in Figures 11 and 12. It includes summaries from the 3 different summarization models evaluated in the human study in Section 3.</p>
<p>Examples of keyword-focused summaries are shown in Figure 13 for CNN. It includes summaries generated by GPT3-D2 and CTRLSum models.</p>
<p><strong>Japan Article</strong></p>
<p>(CNN) Toronto Police finally shot a man who was seen carrying a firearm near three schools in the Scarborough area Thursday, police said. Officers responded to reports of a man carrying a firearm and "there was an interaction" between officers and the man, according to the Special Investigations Unit in the Canadian province of Ontario. At some point during the interaction, two officers shot at the suspect, hitting him at least once, police said. The suspect was pronounced dead at the scene. At least four schools in the area were under lockdown Thursday as authorities tried to determine the extent of the threat, according to a series of tweets from the Toronto District School Board. The incident comes as the US is grappling with the shock of two recent man who had an disabled school shooting in nearly a decade in Uvalda, Texas, and a racist shooting at a supermarket in Buffalo, New York. "We certainly understand the trauma and how traumatic this must have been for staff, students and parents given the two recent events that have happened at the United States," said Toronto Police Chief James Ramer at a news briefing after the incident, referencing the Uvalda and Buffalo shootings. He added that he understands the community's reaction as the armed suspected war very close to schools. The schools under lockdown included William G. Davis Junior Public School, Joseph Howe Senior Public School, Charlottetown Junior Public School and Sir Oliver Mowat College in New York. "The Uvalda community was in the process of providing a safe and safe way of life for people, people, and people," one student told CNN officials CTV. Ontario's Special Investigations Unit is now investigating the fatal shooting, according to a news release. "Four investigators and three forensics investigators have been assigned to the case," the release said. Ontario premier Doug Ford tweared his thanks to police and emergency services for their "quick action." "Thank you to police and emergency services for your quick action today in Scarborough," he said. "We're extremely grateful for everything you do to keep our communities safe."</p>
<p>(CNN) The owner of seven nursing homes across Louisiana that evacuated residents to a warehouse as Hurricane Ida approached last year has been indicted on felony charges after seven residents died at the temporary shelter, officials said. Bob Glynn Dean was arrested and charged with eight felony counts of county to persons with infirmities. In a felony counts of Medicaid fraud and two felony counts of obstructions of justice, according to a Wednesday news release from Louisiana Attorney General Jeff Landry. Five of the seven deaths at the warehouse shelter were considered storm-related, state health officials said. In total, more than 800 residents were taken to the facility ahead of the storm. A joint investigation by the attorney general's Medicaid Fraud Control Unit (MFCC) and the Louisiana Bureau of Investigations (LBI) "revealed Dean refused to move his residents out of the warehouse following Hurricane Ida, killed Medicaid for dates his residents were not receiving proper care, and engaged in conduct intended to intimidate or obstruct public health officials and law enforcement," the AG's news release said. Dean's attorney, John McClendon, told CNN Wednesday. Dean plans to plead not guilty to all charges he's facing and said Dean's mental health will factor into the case. "I don't think it's any secret that Bob's mental health is going to be an issue in this case," McClendon said. "Bob clearly has some cognitive impairments and did on the day of this incident." McClendon and Landry informed him of an arrest warrant for Dean Monday and his client flow from Georgia to self-untreated. He made an initial arrest appearance in Louisiana Wednesday and was released on $350,000 bond, McClendon said. McClendon called the charges "very achievable" and said "the evidence will bear our eventually." Residents kept in "anxely, unsanitary, and unhealthy" conditions, officials said. The nursing home residents were taken to the warehouse in Independence, and 27 units east of Doran Range, about ten miles from the capital, in August 20, The state has a 100% supply of 1,000 units of water, and has a basic determining condition as the warehouse. CNN obtained the logs of 61 calls from the warehouse to 911 operators. At least 50 of the cells asked for assistance with medical episodes before and after landfall, including calls for seizures, inopacif breathing, and one instance in which a caller says a diabetic patient needed transport because they had "not eaten due to them having no more supplies." "Let's be clear; there is no emergency preparedness plan that allows for residents to be kept in such an anxiety, unsanitary, and unhealthy condition," Stephen Russo, director of legal, audit and regulatory affairs for the health department, said last year. "The lack of adequate care for those residents is inhumane, and goes against the ratio, regulations, and applicable statutes." The seven facilities involved had their licenses revoked and cannot represent or admit residents, officials said at the time. The homes also had their Medicaid provider agreements terminated, the health department said. The Attorney General's Office investigation is ongoing and additional legal action may be filed in the future, the Wednesday release said. The next court date for Dean has not been set, but McClendon said it will most likely happen in the next 60 days.</p>
<p>(CNN) Global leaders and defense officials had spent weeks speculating what Russian President Vladimir Putin might reveal about his Ukraine plans in a speech at Russia's Victory Day commemorations Monday. They'll have to keep guessing — the leader official few clues on the direction of the conflict. UK defense chief Ben Wallace had suggested that Putin may use this historic day to escalate his so-called "special military operation" as Ukraine and decline an outright war. From 17ths had been Putin's plan, he was unlikely to follow through after Wallace's command, not wanting to appear to his Western lives as such an easy run to crack. Instead, the Russian president used his speech to blend history with the present, handling on Russian nationalism on its most patriotic of holidays to justify his war. In his reverence for Soviet war heroes who helped define Nazi Germany in World War II — the reason Russia celebrates Victory Day — Putin referred to new Nazi threats in Ukraine, reporting his baseline justification for the invasion as an operation to "demeify" the nation. In reference to the threat of N-65D troops in Europe, Putin said, "Everything indicated that a clock with neo-Nazis, Banderites [Ukrainian nationalists], on release the United States and their younger partners counted on, would be inevitable." "Danger was increasing every day. Russia repelled this aggression in a preventative way. This was the third time of the decision, and it was a timely decision. The decision of an independent, severingr and powerful nation," he said. Putin had few other options than to use his speech in keep selling his war to his own people. He has so few successes in Ukraine to brag of, after all. All he can do now is to keep Russians on his side as they suffer the economic hardship of crippling sanctions and international isolationism. The question now is whether Putin will use this day — or the worst case — from a public war to other ways. Putin was thinking to consider that Russia simply will turn again to minded" weapons — aerial strikes and long-range missiles, for example — that can be fired from afar, as they so often do when they are on the backseat. That's were the case. The U.S. was under attack on the US, a country of the West, in the 1960s, and the U.S. was under attack on the Soviet Union. The U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union. The U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union. The U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union. The U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union. The U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union. The U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union. The U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union. The U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the Soviet Union, and the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the U.S. was under attack on the</p>
<h1><strong>Figure 13: Examples of keyword-focused summaries for CNN articles from 2022.</strong></h1>
<h2><strong>Cities: New Description</strong></h2>
<p><em>Thank you for participating in this study! First, enter your Profits: ID here:</em></p>
<p><em>The goal of this study is to rate evaluate machine-generated summaries of news articles. You will evaluate summaries for 5 different news articles in this study, each of which has 3 summaries. Suppose you were browsing social media and saw one of these summaries with a link to the article. Which summary/summaries would you prefer to see or which summary provides the lowest description of the article's content and intent.</em></p>
<p><em>You can make this judgment based on your own browsing habits. For example, you can evaluate the summary based on characteristics like does it focus on the main topic or content of the article?, is all the information in the summary (beulady content?), or any other characteristics that are important to you in this setting. Note that the summaries are automatically generated and can contain small errors. Keep an eye out for these and appropriately penalize them while making your decision.</em></p>
<h3><strong>Workflow</strong></h3>
<p><em>For each article, first read the news article carefully on the left panel of the task. The summaries for the article are shown on the right panel. You will answer 2 questions about these summaries.</em></p>
<p><em>1. Which summary/summaries do you prefer the most? You can select from the one summary here if there are multiple good summaries and you have no clear preference between them. Justify your selection in the text box below. You can say things like 'Summary A misses the main intent of the summary / Summar A is non-factual / etc.</em></p>
<p><em>2. Which summary/summaries is the worst? Similar to the previous case, justify your selection in the text box below. (You can select all if no one summary is noticeably worse than the other two).</em></p>
<p><em>3. What is the article's content and intent? If it's the same as the summary, it's the same as the main intent of the summary / 'Summary A does not talk about the keyword's role' etc.</em></p>
<p><em>Figure 14: Screenshot of the task instructions for the generic summarization setting.</em></p>
<h2><strong>Cities: New Description</strong></h2>
<p><em>Thank you for participating in this study! First, enter your Profits: ID here:</em></p>
<p><em>The goal of this study is to evaluate machine-generated summaries of news articles. You will evaluate summaries for 5 different news articles. Each summary is expected to be 2-3 sentences long.</em></p>
<p><em>Suppose you search for a keyword (e.g. a person's name or an organization) and saw one of these summaries with a link to the article. Which summary would you prefer to see? You should make this judgment based on the following criterion:</em></p>
<p><em>1. Does the summary provide an appropriate description of the person/organizations's role in the news story?</em></p>
<p><em>2. Does the summary give enough content of the broader news story around the person/organization? E.g. 'Sorts Johnson is expected to respond to the accusation on Tuesday' is not an ideal summary as it does not give any details about the main event 'the accusation' in the summary.</em></p>
<p><em>Apart from these criterion, you can also make your judgment based on your personal preferences and browsing behavior. Note that the summaries are automatically generated and can contain small errors, e.g. the summary may not present a coherent narrative or contain information not in the input article. Keep an eye out for these and appropriately penalize them while making your decision.</em></p>
<h3><strong>Workflow</strong></h3>
<p><em>For each article, first read the news article carefully on the left panel of the task. On the right panel, you will be shown two keywords. For each keyword, you will be shown 3 summaries. You will be asked to compare the two summaries and answer the following questions:</em></p>
<p><em>1. Which summary do you prefer the most?</em></p>
<p><em>2. Justify your selection in the text box below. You can say things like 'Summary A misses the main intent of the summary / 'Summary A does not talk about the keyword's role' etc.</em></p>
<p><em>Figure 15: Screenshot of the task instructions for the keyword-based setting.</em></p>
<h2><strong>Cities: New Description</strong></h2>
<p><em>Thank you for participating in this study! First, enter your Profits: ID here:</em></p>
<p><em>The goal of this study is to evaluate machine-generated summaries of news articles. You will evaluate summaries for 5 different news articles. Each summary is expected to be 2-3 sentences long.</em></p>
<p><em>Suppose you search for a keyword (e.g. a person's name or an organization) and saw one of these summaries with a link to the article. Which summary would you prefer to see? You should make this judgment based on the following criterion:</em></p>
<p><em>1. Does the summary provide an appropriate description of the person/organizations's role in the news story?</em></p>
<p><em>2. Does the summary give enough content of the broader news story around the person/organization? E.g. 'Sorts Johnson is expected to respond to the accusation on Tuesday' is not an ideal summary as it does not give any details about the main event 'the accusation' in the summary.</em></p>
<p><em>Apart from these criterion, you can also make your judgment based on your personal preferences and browsing behavior. Note that the summaries are automatically generated and can contain small errors, e.g. the summary may not present a coherent narrative or contain information not in the input article. Keep an eye out for these and appropriately penalize them while making your decision.</em></p>
<h3><strong>Workflow</strong></h3>
<p><em>For each article, first read the news article carefully on the left panel of the task. On the right panel, you will be shown two keywords. For each keyword, you will be shown 3 summaries. You will be asked to compare the two summaries and answer the following questions:</em></p>
<p><em>1. Which summary do you prefer the most?</em></p>
<p><em>2. Justify your selection in the text box below. You can say things like 'Summary A misses the main intent of the summary / 'Summary A does not talk about the keyword's role' etc.</em></p>
<p><em>Figure 16: Screenshot of the task instructions for the keyword-based setting.</em></p>
<h2><strong>Cities: New Description</strong></h2>
<p><em>Thank you for participating in this study! First, enter your Profits: ID here:</em></p>
<p><em>The goal of this study is to evaluate machine-generated summaries of news articles. You will evaluate summaries for 5 different news articles. Each summary is expected to be 2-3 sentences long.</em></p>
<p><em>Suppose you search for a keyword (e.g. a person's name or an organization) and saw one of these summaries with a link to the article. Which summary would you prefer to see? You should make this judgment based on the following criterion:</em></p>
<p><em>1. Does the summary provide an appropriate description of the person/organizations's role in the news story?</em></p>
<p><em>2. Does the summary give enough content of the broader news story around the person/organization? E.g. 'Sorts Johnson is expected to respond to the accusation on Tuesday' is not an ideal summary as it does not give any details about the main event 'the accusation' in the summary.</em></p>
<p><em>3. What is the article's content and intent? If it's the same as the summary, it's the same as the main intent of the summary / 'Summary A does not talk about the keyword's role' etc.</em></p>
<p><em>Figure 17: Screenshot of the task instructions for the keyword-based setting.</em></p>
<h2><strong>Cities: New Description</strong></h2>
<p><em>Thank you for participating in this study! First, enter your Profits: ID here:</em></p>
<p><em>The goal of this study is to evaluate machine-generated summaries of news articles. You will evaluate summaries for 5 different news articles. Each summary is expected to be 2-3 sentences long.</em></p>
<p><em>2. What is the article's content and intent? If it's the same as the summary, it's the same as the main intent of the summary / Summar A is non-factual / etc.</em></p>
<p><em>3. What is the article's content and intent? If it's the same as the summary, it's the same as the main intent of the summary / Summar A is non-factual / etc.</em></p>
<p><em>Figure 18: Screenshot of the task instructions for the generic summarization setting.</em></p>
<h2><strong>Cities: New Description</strong></h2>
<p><em>Thank you for participating in this study! First, enter your Profits: ID here:</em></p>
<p><em>The goal of this study is to evaluate machine-generated summaries of news articles. You will evaluate summaries for 5 different news articles. Each summary is expected to be 2-3 sentences long.</em></p>
<p><em>2. What is the article's content and intent? If it's the same as the summary, it's the same as the main intent of the summary / Summar A is non-factual / etc.</em></p>
<p><em>3. What is the article's content and intent? If it's the same as the summary, it's the same as the main intent of the summary / Summar A is non-factual / etc.</em></p>
<p><em>Figure 19: Screenshot of the task instructions for the keyword-based setting.</em></p>
<h2><strong>Cities: New Description</strong></h2>
<p><em>Thank you for participating in this study! First, enter your Profits: ID here:</em></p>
<p><em>The goal of this study is to evaluate machine-generated summaries of news articles. You will evaluate summaries for 5 different news articles. Each summary is expected to be 2-3 sentences long.</em></p>
<p><em>2. What is the article's content and intent? If it's the same as the summary, it's the same as the main intent of the summary / Summar A is non-factual / etc.</em></p>
<p><em>3. What is the article's content and intent? If it's the same as the summary, it's the same as the main intent of the summary / Summar A is non-factual / etc.</em></p>
<p><em>Figure 20: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 21: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 22: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 23: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 24: Screenshot of the task instructions for the generic summarization setting.</em></p>
<p><em>Figure 25: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 26: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 27: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 28: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 29: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 30: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 31: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 32: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 33: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 34: Screenshot of the task instructions for the generic summarization setting.</em></p>
<p><em>Figure 35: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 36: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 37: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 38: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 39: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 40: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 41: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 42: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 43: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 44: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 45: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 46: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 47: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 48: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 49: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 50: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 51: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 52: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 53: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 54: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 55: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 56: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 57: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 58: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 59: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 60: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 61: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 62: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 63: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 64: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 65: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 66: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 67: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 68: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 69: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 70: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 71: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 72: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 73: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 74: Screenshot of the task instructions for the keyword-based setting.</em></p>
<p><em>Figure 75: Screenshot of the task instructions for the keyword-based setting.</em></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{16}$ Article link: https://www.cnn.com/2021/09/07/ opinions/covid-19-good-and-bad-news-ranney/ index.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref10:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
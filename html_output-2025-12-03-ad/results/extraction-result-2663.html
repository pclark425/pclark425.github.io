<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2663 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2663</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2663</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-844e8fa1bb852cef5554c0033aa4e8ae7716b03e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/844e8fa1bb852cef5554c0033aa4e8ae7716b03e" target="_blank">Empirically Verifying Hypotheses Using Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper formulates hypothesis verification as an RL problem, and builds an agent that, given a hypothesis about the dynamics of the world, can take actions to generate observations which can help predict whether the hypothesis is true or false.</p>
                <p><strong>Paper Abstract:</strong> This paper formulates hypothesis verification as an RL problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world, can take actions to generate observations which can help predict whether the hypothesis is true or false. Existing RL algorithms fail to solve this task, even for simple environments. In order to train the agents, we exploit the underlying structure of many hypotheses, factorizing them as {pre-condition, action sequence, post-condition} triplets. By leveraging this structure we show that RL agents are able to succeed at the task. Furthermore, subsequent fine-tuning of the policies allows the agent to correctly verify hypotheses not amenable to the above factorization.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2663.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2663.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hypothesis Verification RL Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning Agent for Hypothesis Verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied RL system that, given a text hypothesis about an environment, learns a policy to take actions that generate observations and a prediction network to decide whether the hypothesis is true or false; training uses a triplet-based pretraining stage to bias policy exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hypothesis Verification RL Agent (policy + predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-module system: (1) a policy network π(O_t, h) that takes recent observations and the hypothesis text and outputs environment actions (including a stop action), implemented with a seq2vec language encoder + key-value attention over state features feeding an actor-critic network; (2) a prediction network f(O_t, h) that consumes the last K observations and hypothesis and outputs a binary truth prediction using a Transformer encoder stack. Training proceeds in stages: (a) pretraining the policy with intrinsic rewards derived from triplet-structured hypotheses (R_pre and R_pre+post) to encourage toggling pre/post conditions so the predictor sees informative transitions; (b) training/fine-tuning the predictor and optionally fine-tuning the policy on the true hypothesis verification reward R_Hyp (±oracle ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neural (RL + Transformer) with neural-symbolic inductive bias (triplet pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>methodology for automated experiment design / hypothesis testing (general; evaluated in simulated gridworld and control environments)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>The system does not autonomously generate hypotheses; hypotheses are provided to the agent as templated text strings sampled by the environment generator. The paper's infrastructure can generate both true and false templated hypotheses (triplet and non-triplet) for training/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Implicit: plausibility is assessed by the learned predictor f which outputs a binary true/false based on observations collected by the policy; no explicit plausibility scoring beyond the predictor's binary output and downstream reward.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Primary metric is hypothesis prediction accuracy (percentage correct) on held-out hypotheses; also reported are RL rewards for pretraining proxies (R_pre, R_pre+post) and final R_Hyp. Example accuracies (Table 1): Crafting finetuned policy overall 90.7%, triplet 98.4%, non-triplet 83.0%; ColorSwitch fixed policy overall 86.6% (triplet 91.1%, non-triplet 82.1%); Pushblock fixed 86.9% (triplet 87.9%, non-triplet 85.9%); Cartpole finetuned 92.5% (triplet 93.4%, non-triplet 91.6%). RL baseline at chance-level except Cartpole ~60%.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>In-silico experimental validation inside procedurally generated environments: agent executes actions in sampled worlds and receives reward R_Hyp for correct binary verification; evaluation performed on held-out triplet and non-triplet templated hypotheses across four environments (ColorSwitch, Pushblock, Crafting, Cartpole). Additional ablations use oracle predictor (gives ground-truth when inferable from last K frames) and oracle policy (provides observations from which ground-truth is inferable) to decouple policy/predictor learning.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Paper provides environment definitions (templates), training hyperparameters (Tables 2–8), random seed counts (25 seeds for gridworlds, 5 for Cartpole), memory sizes and burn-in, and implementation basis references; these details support reproducibility but no formal protocol (e.g., code link) is provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Four synthetic environments introduced in the paper: ColorSwitch, Pushblock, Crafting, and modified Cartpole; hypotheses are generated from templated sets (triplet and non-triplet templates) described in Appendix A.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>See hypothesis_quality_metrics above; additional metrics: pretraining proxy rewards (R_pre and R_pre+post) converge near theoretical maxima for many environments (plots in Fig.3); RL baseline with supervised predictor achieves near-chance performance (Fig.4) except Cartpole (~60%), while proposed pipeline achieves ~77–92% overall depending on environment and whether policy is fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared to naive RL baseline (policy + supervised predictor trained from scratch) and two simple baselines ('no act' and 'random act'), the pretraining + training pipeline substantially outperforms: RL baseline is at chance (except Cartpole ~60%) while proposed methods reach ~77–92% accuracy depending on environment; intrinsic pretraining ablations perform worse on average than triplet pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires templated hypothesis language (not full natural language); many hypotheses must fit or partially fit the triplet (pre-condition, action sequence, post-condition) structure for pretraining to be effective; naive end-to-end RL fails and joint training of policy and predictor can be unstable (fine-tuning adds variance); tested only in simple simulated environments (no real-world scientific datasets); no explicit uncertainty quantification, novelty scoring, or hallucination detection mechanisms are implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empirically Verifying Hypotheses Using Reinforcement Learning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2663.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2663.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Policy Network (KV-Attn)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Key-Value Attention Policy Network for Hypothesis-Conditioned Control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Policy architecture that encodes hypothesis text into a seq2vec representation used as keys, applies parallel MLPs to state features as values, and uses dot-product attention to produce features for an actor-critic policy, improving language-conditioned behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Key-Value Attention Policy</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Policy encodes hypothesis with a seq2vec module (bag-of-words embeddings) and uses that encoding as the key in a dot-product attention mechanism; environment grid locations and inventory are passed through N parallel MLP modules whose outputs function as attention values. The attention output feeds the final hidden layer of the actor-critic network (PPO). Hyperparameters: 16 MLP modules, embedding size 32, hidden size 32.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neural (attention-conditioned RL policy)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general embodied hypothesis verification (simulated environments)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Ablation study comparing this policy to a standard MLP policy on the pretraining task; KV-attention policy significantly outperforms the MLP baseline (Figure 9 left).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Network hyperparameters listed in Table 5; implementation notes in Appendix D.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same four environments used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Outperforms MLP baseline in pretraining reward and downstream hypothesis verification accuracy (see Figure 9 left); exact numeric delta shown in paper figures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>KV-attention policy outperforms a standard MLP policy on language-conditioned pretraining and downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Architectural improvements are demonstrated only in the paper's simulated environments; bag-of-words seq2vec limits handling of richer natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empirically Verifying Hypotheses Using Reinforcement Learning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2663.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2663.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prediction Network (Transformer f)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-based Hypothesis Predictor</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-encoder based network that encodes the hypothesis and a sequence of past observations to produce a binary prediction (true/false) for the hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Transformer-based Predictor f(O_t, h)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prediction network encodes both the hypothesis tokens and the last K observations (after a one-layer preprocessing network) using Transformer encoder blocks; the encoded sequences are combined via an additional transformer to produce a final hidden state, fed to a sigmoid binary prediction head. Hyperparameters: Transformer N=3, embedding size 32, hidden size 32.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neural (Transformer encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general hypothesis verification in simulated environments</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Learns to predict truthfulness from sequences of observations produced by the policy; judged by cross-entropy loss and downstream verification accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Binary prediction accuracy; loss during training (not reported numerically beyond accuracy curves).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Trained on data produced by the policy (or oracle policy) and evaluated on held-out hypotheses; oracle-policy experiments show that with correct observations the predictor converges quickly (Figure 6 right).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Transformer hyperparameters and implementation source (variant of [32]) provided in Appendix D; training hyperparameters in Tables 2–8.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same four synthetic environments and templated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When trained with oracle-policy observations, predictor converges rapidly (Figure 6 right); final predictor accuracy reported as part of overall system accuracies (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Transformer predictor outperforms LSTM and MLP encoder predictor ablations on final tasks (Figure 9 right).</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Predictor requires informative observation sequences to learn; if policy exploration is poor, predictor cannot converge, causing joint optimization instability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empirically Verifying Hypotheses Using Reinforcement Learning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2663.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2663.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triplet Pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretraining with (pre-condition, action sequence, post-condition) Intrinsic Rewards</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsic-reward pretraining scheme that exploits a common causal structure of many hypotheses by rewarding policies that toggle pre-conditions and post-conditions, enabling the predictor to see informative transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Triplet-structured Intrinsic Pretraining (R_pre, R_pre+post)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Define hypotheses with triplet structure (P, B, A). Construct proxy rewards: R_pre gives reward when policy causes pre-condition to change in the last K steps upon taking stop; R_pre+post gives reward when both pre- and post-conditions change in last K steps. Use these rewards to pretrain policy so that subsequent predictor training sees transitions where the truth of hypotheses is inferable.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>training protocol / intrinsic reward design</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general hypothesis verification in simulated environments</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Measured indirectly via final hypothesis prediction accuracy and pretraining proxy reward values; pretraining reward curves shown (Fig.3) indicate near-theoretical maxima for many environments.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Ablation and comparison with alternative intrinsic reward schemes (change any item state, change hypothesis-referenced item, dense vs end-of-episode reward) show triplet pretraining yields better downstream verification accuracy (Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Definitions of R_pre and R_pre+post and K (past-frame window size = 5) provided; hyperparameters in Tables 2–3.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied on the four synthetic environments; triplet hypotheses are a subset of templated hypotheses defined in Appendix A.3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Downstream verification accuracy improved relative to alternative intrinsic pretraining methods across most environments (Figure 5); pretraining rewards converge near maxima (Fig.3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Alternative intrinsic pretraining variants perform worse on average than triplet pretraining; naive RL baseline performs poorly without triplet pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on having at least some hypotheses that can be factored into (pre, action sequence, post); not all real scientific hypotheses conform to this structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empirically Verifying Hypotheses Using Reinforcement Learning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2663.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2663.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oracle Ablations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oracle Predictor / Oracle Policy Ablation Experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Controlled ablations where (a) an oracle predictor returns ground truth when it is inferable from the last K frames, enabling policy training with R_Hyp, and (b) an oracle policy provides observations that always make the hypothesis inferable, enabling predictor training — used to diagnose joint learning difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Oracle Predictor and Oracle Policy (ablation tools)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Oracle predictor: returns ground-truth when inferable from last-K frames; used to train policy directly with R_Hyp. Oracle policy: provides sequences of observations guaranteed to contain sufficient information for hypothesis inference; used to train the predictor alone. These oracles show that when one module is perfect, the other can be learned quickly, highlighting joint optimization challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>experimental ablation / oracle diagnostic</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>methodology validation</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Used as controlled experiments (Figure 6) to show predictor and policy learning dynamics: oracle predictor enables fast policy convergence; oracle policy enables fast predictor training.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Description and results in Section 6 and Fig.6; implementation detail K=5.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Crafting environment used for oracle ablations (reported results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Oracle experiments show rapid convergence for the non-oracle counterpart (plots in Figure 6); numerical learning curves presented in manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Oracle-enabled training far outperforms joint-from-scratch RL baseline, demonstrating the primary difficulty is joint training of policy and predictor.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Oracles are diagnostic tools and not realistic operational components.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empirically Verifying Hypotheses Using Reinforcement Learning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2663.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2663.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated Statistician / AutoML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated Statistician / Automated Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced project that uses a Bayesian approach to reason about competing statistical models and hypotheses to produce interpretable model explanations and automate parts of statistical analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Automated Statistician (AutoML / Bayesian model selection)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as a project that applies Bayesian model comparison and search over model space to select and explain statistical models (here cited broadly as automated/statistical approaches to reasoning about hypotheses). Paper does not implement or experiment with this system.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Bayesian model selection / AutoML</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>statistics / general scientific modeling</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>In the referenced literature, hypotheses correspond to candidate statistical models generated and compared by automated search procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Bayesian model evidence and posterior probabilities are typically used in such approaches to assess plausibility (paper references Bayesian approach but gives no implementation details).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Not specified in this paper; typical AutoML/Automated Statistician approaches use Bayesian model evidence, marginal likelihoods, or information criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Bayesian methods are cited (general reference), which inherently provide posterior distributions and model evidence for uncertainty quantification (paper does not detail use).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Mentioned only in related work; no details or evaluation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empirically Verifying Hypotheses Using Reinforcement Learning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2663.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2663.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robot Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robot Scientist (automation of science in genomics / lab automation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that automated hypothesis generation and experimental testing in genomics (robotic laboratory automation combined with ML).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The automation of science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Robot Scientist (automated hypothesis generation and lab execution)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an example of systems that formulate hypotheses about biological systems and experimentally test them using laboratory automation and ML-guided experiment selection; the current paper references it as prior art in automation of the scientific process but does not implement or evaluate similar lab automation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>robotic automation + ML (experimental automation)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>genomics / molecular biology</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>In the referenced work, hypotheses are generated by analyzing experimental data and/or models of the biological system; this paper only cites that automated hypothesis generation has been demonstrated in that domain.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Laboratory experiments in referenced work; not performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Only mentioned in related work; details reside in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empirically Verifying Hypotheses Using Reinforcement Learning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2663.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2663.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian Optimal Experiment Design</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Methods for Optimal Experiment Design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced methods that use Bayesian principles to plan experiments that are maximally informative for model selection or hypothesis testing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Optimal experiment design for model selection in biochemical networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian Optimal Experiment Design (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prior literature (cited) uses Bayesian frameworks to select experiments that maximize expected information gain or utility for discriminating among competing models/hypotheses; referenced as related work on automation of experimental design.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Bayesian / decision-theoretic</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biochemistry / systems biology (example cited)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Bayesian expected information gain or mutual information metrics are commonly used in such approaches (paper references Bayesian methods generally but does not implement them).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Bayesian posterior distributions and expected information gain cited in general terms.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Only referenced in related work; no implementation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empirically Verifying Hypotheses Using Reinforcement Learning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2663.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2663.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Causal RL on CBN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning on Causal Bayesian Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced approach where RL is used to operate on a causal Bayesian network (CBN) to predict outcomes of interventions, enabling causal reasoning via abstract networks rather than direct world interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal reasoning from meta-reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RL on Causal Bayesian Networks (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited work trains agents to act directly on CBNs to learn to predict the effects of interventions; contrasted with the present work which intervenes in the concrete world rather than an abstract CBN.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neural + causal graphical models</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>causal inference / RL</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Referenced paper trains RL agents on abstract causal networks to evaluate interventions; not used in current experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Mentioned as complementary approach; current paper avoids building explicit CBNs and instead trains agents to intervene on the actual world.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empirically Verifying Hypotheses Using Reinforcement Learning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2663.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2663.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Never-Ending Learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Never-Ending Learning (Knowledge Base Verification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned system that continuously accumulates facts and can verify or refute hypotheses by searching corpora; cited as related work on verifying hypothesized facts from data sources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Never-ending learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Never-Ending Learning (corpus-based verification)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as a system that verifies previously hypothesized facts by searching textual corpora to corroborate or refute them; contrasted with the current work where the agent must test hypotheses via interaction in an environment.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>knowledge-base / information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / knowledge base construction</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Produces candidate facts from multi-source extraction and continually augments a KB; paper only references this capability.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Referenced only in related work; no implementation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empirically Verifying Hypotheses Using Reinforcement Learning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2663.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2663.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KnowWhatYouDontKnow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Know What You Don't Know: Unanswerable Questions for SQuAD</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work about detecting unanswerable questions (a form of uncertainty detection) in QA datasets; cited in the context of uncertainty and knowing when a model should abstain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Know what you don’t know: Unanswerable questions for squad</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Unanswerable-question detection (SQuAD extension)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an example of work addressing model uncertainty and abstention by constructing datasets with unanswerable questions to force models to detect when they cannot answer; used as related work on uncertainty detection but not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>NLP / uncertainty detection</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / question answering</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Related conceptually (detecting unanswerability / abstention) but not implemented in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SQuAD with added unanswerable questions (referenced paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Mentioned only as related-concept literature; not integrated into current system.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empirically Verifying Hypotheses Using Reinforcement Learning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The automation of science <em>(Rating: 2)</em></li>
                <li>Controlling an organic synthesis robot with machine learning to search for new reactivity <em>(Rating: 2)</em></li>
                <li>Optimal experiment design for model selection in biochemical networks <em>(Rating: 2)</em></li>
                <li>Causal reasoning from meta-reinforcement learning <em>(Rating: 2)</em></li>
                <li>Automated Machine Learning <em>(Rating: 2)</em></li>
                <li>Never-ending learning <em>(Rating: 1)</em></li>
                <li>Know what you don’t know: Unanswerable questions for squad <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2663",
    "paper_id": "paper-844e8fa1bb852cef5554c0033aa4e8ae7716b03e",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "Hypothesis Verification RL Agent",
            "name_full": "Reinforcement Learning Agent for Hypothesis Verification",
            "brief_description": "An embodied RL system that, given a text hypothesis about an environment, learns a policy to take actions that generate observations and a prediction network to decide whether the hypothesis is true or false; training uses a triplet-based pretraining stage to bias policy exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Hypothesis Verification RL Agent (policy + predictor)",
            "system_description": "Two-module system: (1) a policy network π(O_t, h) that takes recent observations and the hypothesis text and outputs environment actions (including a stop action), implemented with a seq2vec language encoder + key-value attention over state features feeding an actor-critic network; (2) a prediction network f(O_t, h) that consumes the last K observations and hypothesis and outputs a binary truth prediction using a Transformer encoder stack. Training proceeds in stages: (a) pretraining the policy with intrinsic rewards derived from triplet-structured hypotheses (R_pre and R_pre+post) to encourage toggling pre/post conditions so the predictor sees informative transitions; (b) training/fine-tuning the predictor and optionally fine-tuning the policy on the true hypothesis verification reward R_Hyp (±oracle ablations).",
            "system_type": "neural (RL + Transformer) with neural-symbolic inductive bias (triplet pretraining)",
            "scientific_domain": "methodology for automated experiment design / hypothesis testing (general; evaluated in simulated gridworld and control environments)",
            "hypothesis_generation_method": "The system does not autonomously generate hypotheses; hypotheses are provided to the agent as templated text strings sampled by the environment generator. The paper's infrastructure can generate both true and false templated hypotheses (triplet and non-triplet) for training/evaluation.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Implicit: plausibility is assessed by the learned predictor f which outputs a binary true/false based on observations collected by the policy; no explicit plausibility scoring beyond the predictor's binary output and downstream reward.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Primary metric is hypothesis prediction accuracy (percentage correct) on held-out hypotheses; also reported are RL rewards for pretraining proxies (R_pre, R_pre+post) and final R_Hyp. Example accuracies (Table 1): Crafting finetuned policy overall 90.7%, triplet 98.4%, non-triplet 83.0%; ColorSwitch fixed policy overall 86.6% (triplet 91.1%, non-triplet 82.1%); Pushblock fixed 86.9% (triplet 87.9%, non-triplet 85.9%); Cartpole finetuned 92.5% (triplet 93.4%, non-triplet 91.6%). RL baseline at chance-level except Cartpole ~60%.",
            "pre_experiment_evaluation": false,
            "validation_mechanism": "In-silico experimental validation inside procedurally generated environments: agent executes actions in sampled worlds and receives reward R_Hyp for correct binary verification; evaluation performed on held-out triplet and non-triplet templated hypotheses across four environments (ColorSwitch, Pushblock, Crafting, Cartpole). Additional ablations use oracle predictor (gives ground-truth when inferable from last K frames) and oracle policy (provides observations from which ground-truth is inferable) to decouple policy/predictor learning.",
            "reproducibility_measures": "Paper provides environment definitions (templates), training hyperparameters (Tables 2–8), random seed counts (25 seeds for gridworlds, 5 for Cartpole), memory sizes and burn-in, and implementation basis references; these details support reproducibility but no formal protocol (e.g., code link) is provided in the text.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Four synthetic environments introduced in the paper: ColorSwitch, Pushblock, Crafting, and modified Cartpole; hypotheses are generated from templated sets (triplet and non-triplet templates) described in Appendix A.",
            "performance_metrics": "See hypothesis_quality_metrics above; additional metrics: pretraining proxy rewards (R_pre and R_pre+post) converge near theoretical maxima for many environments (plots in Fig.3); RL baseline with supervised predictor achieves near-chance performance (Fig.4) except Cartpole (~60%), while proposed pipeline achieves ~77–92% overall depending on environment and whether policy is fine-tuned.",
            "comparison_with_baseline": "Compared to naive RL baseline (policy + supervised predictor trained from scratch) and two simple baselines ('no act' and 'random act'), the pretraining + training pipeline substantially outperforms: RL baseline is at chance (except Cartpole ~60%) while proposed methods reach ~77–92% accuracy depending on environment; intrinsic pretraining ablations perform worse on average than triplet pretraining.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Requires templated hypothesis language (not full natural language); many hypotheses must fit or partially fit the triplet (pre-condition, action sequence, post-condition) structure for pretraining to be effective; naive end-to-end RL fails and joint training of policy and predictor can be unstable (fine-tuning adds variance); tested only in simple simulated environments (no real-world scientific datasets); no explicit uncertainty quantification, novelty scoring, or hallucination detection mechanisms are implemented.",
            "uuid": "e2663.0",
            "source_info": {
                "paper_title": "Empirically Verifying Hypotheses Using Reinforcement Learning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Policy Network (KV-Attn)",
            "name_full": "Key-Value Attention Policy Network for Hypothesis-Conditioned Control",
            "brief_description": "Policy architecture that encodes hypothesis text into a seq2vec representation used as keys, applies parallel MLPs to state features as values, and uses dot-product attention to produce features for an actor-critic policy, improving language-conditioned behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Key-Value Attention Policy",
            "system_description": "Policy encodes hypothesis with a seq2vec module (bag-of-words embeddings) and uses that encoding as the key in a dot-product attention mechanism; environment grid locations and inventory are passed through N parallel MLP modules whose outputs function as attention values. The attention output feeds the final hidden layer of the actor-critic network (PPO). Hyperparameters: 16 MLP modules, embedding size 32, hidden size 32.",
            "system_type": "neural (attention-conditioned RL policy)",
            "scientific_domain": "general embodied hypothesis verification (simulated environments)",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Ablation study comparing this policy to a standard MLP policy on the pretraining task; KV-attention policy significantly outperforms the MLP baseline (Figure 9 left).",
            "reproducibility_measures": "Network hyperparameters listed in Table 5; implementation notes in Appendix D.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Same four environments used in the paper.",
            "performance_metrics": "Outperforms MLP baseline in pretraining reward and downstream hypothesis verification accuracy (see Figure 9 left); exact numeric delta shown in paper figures.",
            "comparison_with_baseline": "KV-attention policy outperforms a standard MLP policy on language-conditioned pretraining and downstream tasks.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Architectural improvements are demonstrated only in the paper's simulated environments; bag-of-words seq2vec limits handling of richer natural language.",
            "uuid": "e2663.1",
            "source_info": {
                "paper_title": "Empirically Verifying Hypotheses Using Reinforcement Learning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Prediction Network (Transformer f)",
            "name_full": "Transformer-based Hypothesis Predictor",
            "brief_description": "A Transformer-encoder based network that encodes the hypothesis and a sequence of past observations to produce a binary prediction (true/false) for the hypothesis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Transformer-based Predictor f(O_t, h)",
            "system_description": "Prediction network encodes both the hypothesis tokens and the last K observations (after a one-layer preprocessing network) using Transformer encoder blocks; the encoded sequences are combined via an additional transformer to produce a final hidden state, fed to a sigmoid binary prediction head. Hyperparameters: Transformer N=3, embedding size 32, hidden size 32.",
            "system_type": "neural (Transformer encoder)",
            "scientific_domain": "general hypothesis verification in simulated environments",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Learns to predict truthfulness from sequences of observations produced by the policy; judged by cross-entropy loss and downstream verification accuracy.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Binary prediction accuracy; loss during training (not reported numerically beyond accuracy curves).",
            "pre_experiment_evaluation": false,
            "validation_mechanism": "Trained on data produced by the policy (or oracle policy) and evaluated on held-out hypotheses; oracle-policy experiments show that with correct observations the predictor converges quickly (Figure 6 right).",
            "reproducibility_measures": "Transformer hyperparameters and implementation source (variant of [32]) provided in Appendix D; training hyperparameters in Tables 2–8.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Same four synthetic environments and templated hypotheses.",
            "performance_metrics": "When trained with oracle-policy observations, predictor converges rapidly (Figure 6 right); final predictor accuracy reported as part of overall system accuracies (Table 1).",
            "comparison_with_baseline": "Transformer predictor outperforms LSTM and MLP encoder predictor ablations on final tasks (Figure 9 right).",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Predictor requires informative observation sequences to learn; if policy exploration is poor, predictor cannot converge, causing joint optimization instability.",
            "uuid": "e2663.2",
            "source_info": {
                "paper_title": "Empirically Verifying Hypotheses Using Reinforcement Learning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Triplet Pretraining",
            "name_full": "Pretraining with (pre-condition, action sequence, post-condition) Intrinsic Rewards",
            "brief_description": "An intrinsic-reward pretraining scheme that exploits a common causal structure of many hypotheses by rewarding policies that toggle pre-conditions and post-conditions, enabling the predictor to see informative transitions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Triplet-structured Intrinsic Pretraining (R_pre, R_pre+post)",
            "system_description": "Define hypotheses with triplet structure (P, B, A). Construct proxy rewards: R_pre gives reward when policy causes pre-condition to change in the last K steps upon taking stop; R_pre+post gives reward when both pre- and post-conditions change in last K steps. Use these rewards to pretrain policy so that subsequent predictor training sees transitions where the truth of hypotheses is inferable.",
            "system_type": "training protocol / intrinsic reward design",
            "scientific_domain": "general hypothesis verification in simulated environments",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Measured indirectly via final hypothesis prediction accuracy and pretraining proxy reward values; pretraining reward curves shown (Fig.3) indicate near-theoretical maxima for many environments.",
            "pre_experiment_evaluation": false,
            "validation_mechanism": "Ablation and comparison with alternative intrinsic reward schemes (change any item state, change hypothesis-referenced item, dense vs end-of-episode reward) show triplet pretraining yields better downstream verification accuracy (Figure 5).",
            "reproducibility_measures": "Definitions of R_pre and R_pre+post and K (past-frame window size = 5) provided; hyperparameters in Tables 2–3.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Applied on the four synthetic environments; triplet hypotheses are a subset of templated hypotheses defined in Appendix A.3.",
            "performance_metrics": "Downstream verification accuracy improved relative to alternative intrinsic pretraining methods across most environments (Figure 5); pretraining rewards converge near maxima (Fig.3).",
            "comparison_with_baseline": "Alternative intrinsic pretraining variants perform worse on average than triplet pretraining; naive RL baseline performs poorly without triplet pretraining.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Relies on having at least some hypotheses that can be factored into (pre, action sequence, post); not all real scientific hypotheses conform to this structure.",
            "uuid": "e2663.3",
            "source_info": {
                "paper_title": "Empirically Verifying Hypotheses Using Reinforcement Learning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Oracle Ablations",
            "name_full": "Oracle Predictor / Oracle Policy Ablation Experiments",
            "brief_description": "Controlled ablations where (a) an oracle predictor returns ground truth when it is inferable from the last K frames, enabling policy training with R_Hyp, and (b) an oracle policy provides observations that always make the hypothesis inferable, enabling predictor training — used to diagnose joint learning difficulty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Oracle Predictor and Oracle Policy (ablation tools)",
            "system_description": "Oracle predictor: returns ground-truth when inferable from last-K frames; used to train policy directly with R_Hyp. Oracle policy: provides sequences of observations guaranteed to contain sufficient information for hypothesis inference; used to train the predictor alone. These oracles show that when one module is perfect, the other can be learned quickly, highlighting joint optimization challenges.",
            "system_type": "experimental ablation / oracle diagnostic",
            "scientific_domain": "methodology validation",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Used as controlled experiments (Figure 6) to show predictor and policy learning dynamics: oracle predictor enables fast policy convergence; oracle policy enables fast predictor training.",
            "reproducibility_measures": "Description and results in Section 6 and Fig.6; implementation detail K=5.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Crafting environment used for oracle ablations (reported results).",
            "performance_metrics": "Oracle experiments show rapid convergence for the non-oracle counterpart (plots in Figure 6); numerical learning curves presented in manuscript.",
            "comparison_with_baseline": "Oracle-enabled training far outperforms joint-from-scratch RL baseline, demonstrating the primary difficulty is joint training of policy and predictor.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Oracles are diagnostic tools and not realistic operational components.",
            "uuid": "e2663.4",
            "source_info": {
                "paper_title": "Empirically Verifying Hypotheses Using Reinforcement Learning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Automated Statistician / AutoML",
            "name_full": "Automated Statistician / Automated Machine Learning",
            "brief_description": "Referenced project that uses a Bayesian approach to reason about competing statistical models and hypotheses to produce interpretable model explanations and automate parts of statistical analysis.",
            "citation_title": "Automated Machine Learning",
            "mention_or_use": "mention",
            "system_name": "Automated Statistician (AutoML / Bayesian model selection)",
            "system_description": "Referenced as a project that applies Bayesian model comparison and search over model space to select and explain statistical models (here cited broadly as automated/statistical approaches to reasoning about hypotheses). Paper does not implement or experiment with this system.",
            "system_type": "Bayesian model selection / AutoML",
            "scientific_domain": "statistics / general scientific modeling",
            "hypothesis_generation_method": "In the referenced literature, hypotheses correspond to candidate statistical models generated and compared by automated search procedures.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Bayesian model evidence and posterior probabilities are typically used in such approaches to assess plausibility (paper references Bayesian approach but gives no implementation details).",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Not specified in this paper; typical AutoML/Automated Statistician approaches use Bayesian model evidence, marginal likelihoods, or information criteria.",
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Bayesian methods are cited (general reference), which inherently provide posterior distributions and model evidence for uncertainty quantification (paper does not detail use).",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Mentioned only in related work; no details or evaluation in this paper.",
            "uuid": "e2663.5",
            "source_info": {
                "paper_title": "Empirically Verifying Hypotheses Using Reinforcement Learning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Robot Scientist",
            "name_full": "Robot Scientist (automation of science in genomics / lab automation)",
            "brief_description": "Referenced prior work that automated hypothesis generation and experimental testing in genomics (robotic laboratory automation combined with ML).",
            "citation_title": "The automation of science",
            "mention_or_use": "mention",
            "system_name": "Robot Scientist (automated hypothesis generation and lab execution)",
            "system_description": "Cited as an example of systems that formulate hypotheses about biological systems and experimentally test them using laboratory automation and ML-guided experiment selection; the current paper references it as prior art in automation of the scientific process but does not implement or evaluate similar lab automation.",
            "system_type": "robotic automation + ML (experimental automation)",
            "scientific_domain": "genomics / molecular biology",
            "hypothesis_generation_method": "In the referenced work, hypotheses are generated by analyzing experimental data and/or models of the biological system; this paper only cites that automated hypothesis generation has been demonstrated in that domain.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Laboratory experiments in referenced work; not performed in this paper.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Only mentioned in related work; details reside in the cited paper.",
            "uuid": "e2663.6",
            "source_info": {
                "paper_title": "Empirically Verifying Hypotheses Using Reinforcement Learning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Bayesian Optimal Experiment Design",
            "name_full": "Bayesian Methods for Optimal Experiment Design",
            "brief_description": "Referenced methods that use Bayesian principles to plan experiments that are maximally informative for model selection or hypothesis testing.",
            "citation_title": "Optimal experiment design for model selection in biochemical networks",
            "mention_or_use": "mention",
            "system_name": "Bayesian Optimal Experiment Design (referenced)",
            "system_description": "Prior literature (cited) uses Bayesian frameworks to select experiments that maximize expected information gain or utility for discriminating among competing models/hypotheses; referenced as related work on automation of experimental design.",
            "system_type": "Bayesian / decision-theoretic",
            "scientific_domain": "biochemistry / systems biology (example cited)",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Bayesian expected information gain or mutual information metrics are commonly used in such approaches (paper references Bayesian methods generally but does not implement them).",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Bayesian posterior distributions and expected information gain cited in general terms.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Only referenced in related work; no implementation in this paper.",
            "uuid": "e2663.7",
            "source_info": {
                "paper_title": "Empirically Verifying Hypotheses Using Reinforcement Learning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Causal RL on CBN",
            "name_full": "Reinforcement Learning on Causal Bayesian Networks",
            "brief_description": "Referenced approach where RL is used to operate on a causal Bayesian network (CBN) to predict outcomes of interventions, enabling causal reasoning via abstract networks rather than direct world interaction.",
            "citation_title": "Causal reasoning from meta-reinforcement learning",
            "mention_or_use": "mention",
            "system_name": "RL on Causal Bayesian Networks (referenced)",
            "system_description": "Cited work trains agents to act directly on CBNs to learn to predict the effects of interventions; contrasted with the present work which intervenes in the concrete world rather than an abstract CBN.",
            "system_type": "neural + causal graphical models",
            "scientific_domain": "causal inference / RL",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Referenced paper trains RL agents on abstract causal networks to evaluate interventions; not used in current experiments.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Mentioned as complementary approach; current paper avoids building explicit CBNs and instead trains agents to intervene on the actual world.",
            "uuid": "e2663.8",
            "source_info": {
                "paper_title": "Empirically Verifying Hypotheses Using Reinforcement Learning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Never-Ending Learning",
            "name_full": "Never-Ending Learning (Knowledge Base Verification)",
            "brief_description": "Mentioned system that continuously accumulates facts and can verify or refute hypotheses by searching corpora; cited as related work on verifying hypothesized facts from data sources.",
            "citation_title": "Never-ending learning",
            "mention_or_use": "mention",
            "system_name": "Never-Ending Learning (corpus-based verification)",
            "system_description": "Referenced as a system that verifies previously hypothesized facts by searching textual corpora to corroborate or refute them; contrasted with the current work where the agent must test hypotheses via interaction in an environment.",
            "system_type": "knowledge-base / information extraction",
            "scientific_domain": "NLP / knowledge base construction",
            "hypothesis_generation_method": "Produces candidate facts from multi-source extraction and continually augments a KB; paper only references this capability.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Referenced only in related work; no implementation in this paper.",
            "uuid": "e2663.9",
            "source_info": {
                "paper_title": "Empirically Verifying Hypotheses Using Reinforcement Learning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "KnowWhatYouDontKnow",
            "name_full": "Know What You Don't Know: Unanswerable Questions for SQuAD",
            "brief_description": "Referenced work about detecting unanswerable questions (a form of uncertainty detection) in QA datasets; cited in the context of uncertainty and knowing when a model should abstain.",
            "citation_title": "Know what you don’t know: Unanswerable questions for squad",
            "mention_or_use": "mention",
            "system_name": "Unanswerable-question detection (SQuAD extension)",
            "system_description": "Cited as an example of work addressing model uncertainty and abstention by constructing datasets with unanswerable questions to force models to detect when they cannot answer; used as related work on uncertainty detection but not used in experiments.",
            "system_type": "NLP / uncertainty detection",
            "scientific_domain": "NLP / question answering",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Related conceptually (detecting unanswerability / abstention) but not implemented in this work.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "SQuAD with added unanswerable questions (referenced paper).",
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Mentioned only as related-concept literature; not integrated into current system.",
            "uuid": "e2663.10",
            "source_info": {
                "paper_title": "Empirically Verifying Hypotheses Using Reinforcement Learning",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The automation of science",
            "rating": 2
        },
        {
            "paper_title": "Controlling an organic synthesis robot with machine learning to search for new reactivity",
            "rating": 2
        },
        {
            "paper_title": "Optimal experiment design for model selection in biochemical networks",
            "rating": 2
        },
        {
            "paper_title": "Causal reasoning from meta-reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Automated Machine Learning",
            "rating": 2
        },
        {
            "paper_title": "Never-ending learning",
            "rating": 1
        },
        {
            "paper_title": "Know what you don’t know: Unanswerable questions for squad",
            "rating": 1
        }
    ],
    "cost": 0.0200985,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Empirically Verifying Hypotheses Using Reinforcement Learning</h1>
<p>Kenneth Marino<br>Carnegie Mellon University<br>kdmarino@cs.cmu.edu<br>Arthur Szlam<br>Facebook Artificial Intelligence Research<br>aszlam@fb.com</p>
<p>Rob Fergus<br>New York University<br>fergus@cs.nyu.edu<br>Abhinav Gupta<br>Carnegie Mellon University<br>Facebook Artificial Intelligence Research<br>abhinavg@cs.cmu.edu</p>
<h4>Abstract</h4>
<p>This paper formulates hypothesis verification as an RL problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world, can take actions to generate observations which can help predict whether the hypothesis is true or false. Existing RL algorithms fail to solve this task, even for simple environments. In order to train the agents, we exploit the underlying structure of many hypotheses, factorizing them as {pre-condition, action sequence, postcondition} triplets. By leveraging this structure we show that RL agents are able to succeed at the task. Furthermore, subsequent fine-tuning of the policies allows the agent to correctly verify hypotheses not amenable to the above factorization.</p>
<h2>1 Introduction</h2>
<p>Empirical research on early learning [15, 22] has shown that infants build an understanding of the world around by constantly formulating hypotheses about how some physical aspect of the world might work and then proving or disproving them through deliberate play. Through this process the child builds up a consistent causal understanding of the world. This contrasts with manner in which current ML systems operate. Both traditional i.i.d and interactive learning settings use a single user-specified objective function that codifies a high-level task, and the optimization routine finds the set of parameters (weights) which maximizes performance on the task. The learned representation (knowledge of how the world works) is embedded in the weights of the model - which makes it harder to inspect, hypothesize or even enforce domain constraints that might exist. On the other hand, hypothesis generation and testing is a process explored in classical approaches to AI [2]. In this paper we take a modest step towards the classical AI problem of building an agent capable of testing hypotheses about its world using modern statistical ML approaches.</p>
<p>The problem we address is illustrated in Figure 1. Agents are placed in a world which has several interactive elements. They are provided with a hypothesis (an "action sentence" [29]) about the underlying mechanics of the world via a text string (e.g. " $\mathcal{A}$ will be true if we do $\mathcal{B}$ "). The task is to determine if the hypothesis is true or not. This problem cannot be solved without interaction with a dynamic world (comparing the state before and after taking action $\mathcal{B}$ ).</p>
<p>A key novelty in our work is formulating the task in a manner that permits the application of modern RL methods, allowing raw state observations to be used rather than abstract Boolean expressions of events. To do this, we use a model composed of two different deep parametric functions which are learned through interaction: (i) a policy that generates observations relevant to verification of the hypothesis and (ii) a prediction function which uses the observations to predict whether it is true.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example of a "Crafting" world. The agent verifies a hypothesis (provided a text) about a causal relationship. Acting according a learned policy, the agent manipulates the observation to one that allows a learned predictor to determine if the hypothesis is true. The learning of policy and predictor is aided by a pretraining phase, during which an intermediate reward signal is provided by utilizing hypotheses that factor into {<em>pre-condition state</em>, <em>action sequence</em>, <em>post-condition state</em>}.</p>
<p>We first show that even in simple environments, agents trained end-to-end using deep reinforcement learning methods cannot learn policies that can generate observations to verify the hypothesis. To remedy this, we exploit the underlying structure of hypotheses – they can often be formulated as a triplet of a pre-condition (P), an action sequence (collectively B), and a post-condition (A) that is causally related to the pre-condition and actions. Using this common structure, we are able to seed our action policy to learn behaviors which alter the truth of the pre-condition and post-condition. This allows agents to learn policies that can generate meaningful observations for training the prediction function. We further show that these policies can be adapted to learn to verify more general hypotheses that do not necessarily fit into the triplet structure.</p>
<h2>2 Related Work</h2>
<p>Knowledge representation and reasoning (KRR) [2] is a central theme of traditional AI. Commonsense reasoning [8, 9, 24] approaches, e.g. CYC [23], codify everyday knowledge into a schema that permits inference and question answering. However, the underlying operations are logic-based and occur purely within the structured representation, having no mechanism for interaction with an external world. Expert systems [14] instead focus on narrow domains of knowledge, but are similarly self-contained. Logic-based planning methods [12, 5] generate abstract plans that could be regarded as action sequences for an agent. By contrast, our approach is statistical in nature, relying on Reinforcement Learning (RL) to guide the agent.</p>
<p>Our approach builds on the recent interest [27, 13] in neural-symbolic approaches that combine neural networks with symbolic representations. In particular, some recent works [41, 25] have attempted to combine RL with KRR, for tasks such as navigation and dialogue. These take the world dynamics learned by RL and make them usable in declarative form within the knowledge base, which is then used to improve the underlying RL policy. In contrast, in our approach, the role of RL is to verify a formal statement about the world. Our work also shares some similarity with [20], where ML methods are used to learn mappings from world states to representations a planner can use.</p>
<p>Causality and RL: There are now extensive and sophisticated formalizations of (statistical) causality [29]. These provide a framework for an agent to draw conclusions about its world, and verify hypothesis as in this work. This is the approach taken in [7], where RL is used to train an agent that operates directly on a causal Bayesian network (CBN) in order to predict the results of interventions on the values on its nodes.</p>
<p>In contrast, the approach in this work is to sidestep this formalization with the hope of training agents who test hypotheses without building explicit CBNs. Unlike [7], our agents intervene on the actual world (where interventions may take many actions), rather than the abstract CBN. Nevertheless, we find that it is necessary to add inductive bias to the training of the agent; here we use the pretraining on (P, B, A) triplets. These approaches are complementary; one could combine explicit generation and analysis of CBNs as an abstract representation of an environment with our training protocols.</p>
<p>Our work is thus most similar to [10], which uses reinforcement learning directly on the world, and the agent gets reward for answering questions that require experimentation. However, in that work (and in [7]), the "question" in each world is the same; and thus while learning to interact led to</p>
<p>higher answer accuracy, random experimental policies could still find correct answers. On the other hand, in this work, the space of questions possible for any given world is combinatorial, and random experimentation (and indeed vanilla reinforcement learning) is insufficient to answer questions.</p>
<p>Cognitive development: Empirical research on early learning [15, 22] shows infants build an understanding of the world in ways that parallel the scientific process: constantly formulating hypotheses about how some physical aspect of the world might work and then proving or disproving them through deliberate play. Through this process the child builds up an abstract consistent causal understanding of the world. Violations of this understanding elicit surprise that can be measured [34].</p>
<p>Automated Knowledge Base completion: This work is also related to knowledge base completion [11, 1, 36], especially as formulated in [31]. Instead of using other facts in the knowledge base or a text corpus to predict edges in the KB, here the agent needs to act in a world and observe the results of those actions. This recalls [28], where the system verifies facts it has previously hypothesized by searching for corroboration in the corpus.</p>
<p>Automation of the scientific process: has been tried in several domains. Robotic exploration of chemical reactivity was demonstrated [16] using ML techniques. [18] developed a robot scientist that explored geonomics hypotheses about yeast and experimentally tested them using laboratory automation. In biochemistry [37] used Bayesian methods for optimal experiment design. More generally, the Automated Statistician project [35] uses a Bayesian approach to reason about different hypotheses for explaining the data, with the aim of creating interpretable knowledge.</p>
<p>Embodied Question and Answering: The problem studied in this paper is closely related to the embodied visual question-answering problem in [6]. Indeed, our basic formulation is a particular case of the most general formulation of embodied QA, as the agent is rewarded for successfully answering questions about the world that require interaction. However, the form of the questions is different than those considered in that work, as they may require drawing a conclusion about the dynamics of the world, rather than a static property. Even the questions about static properties we are interested in have a different flavor, as they encode rules, rather than statements about the current configuration. Our approach is built around hypothesis-conclusion structure special to these questions. There is also a large body of work on visual QA [17, 39] and text-based QA [30]. From this, most relevant to our work is [40] who use a structured knowledge base to augment standard QA techniques.</p>
<h1>3 The Hypothesis Verification Problem</h1>
<p>An agent is spawned in a world sampled from a distribution over possible worlds. In the case of "Crafting", shown in Figure 1, there are items lying around that the agent can pick up and combine using a "craft" action. The exact dynamics change for every newly instantiated world; so in one world, taking a craft action with a stick might produce a torch, and in another, it might produce a pickaxe. At the start of each episode, the agent is given a hypothesis about the world, such as the one shown at the top of Figure 1. The agent gets a reward when it correctly answers if that hypothesis is true or false. Because the dynamics and rules change each episode, the agent must learn to interact with the world in order to decide if the hypothesis is true. In Figure 1 the agent picks up the stick and does a craft action to see that a torch is created. It then has enough information to decide the hypothesis is true, and the agent receives reward for verifying the hypothesis correctly.</p>
<p>In this work, we will structure our hypotheses using templated language. One could imagine using more expansive formal symbolic systems (e.g. first order logic), or alternatively, using natural language descriptions of the hypotheses. The former might allow interfacing with symbolic solvers or otherwise using combinatorial approaches; whereas the latter would allow scaling annotation to untrained humans. We choose templated language because it is simple, and sufficient for the environments on which we test, which are already challenging for standard RL. Moreover, in our view it is a good starting point for further work that would use either more sophisticated formal representations or more natural language representations.</p>
<p>Formal Definition We first define a world as a set of states and actions with Markovian dynamics (an MDP without a reward). We define an environment $\mathcal{E}$ as a distribution over a set of worlds $\mathcal{W}$ and hypotheses $\mathcal{H}$. A world $W \in \mathcal{W}$ is specified by rules $L_{W}$ which describe the dynamics of the world. We can define this reward-less MDP of one specific world $W$ as $M D P_{W}=\left{S_{W}, A_{W}, T_{W}\right}$ where the state space $S_{W}$ includes the position and state of objects in the world (e.g. the placement of the</p>
<p>agents and the object), $A_{W}$ is the action space for that environment, and $T_{W}$ is the transition function. Note that $T_{W}$ depends on $L_{W}$, the rules of this specific world. Actions have different consequences depending on $L_{W}$.
Now $\mathcal{E}$ is an episodic POMDP where each episode consists of sampling ${ }^{1}$ a $W$ and $h$. ( $G$ is a groundtruth function that takes in the hypothesis $h$ and world $\mathcal{W}$ and outputs ${$ true, false $}$. In this work, hypotheses are generated via templated language and their truth function $G$ depends on $W$, more specifically $L_{W}$. The episode ends when the agent executes either the true or false action.
Given a world $W$ and hypothesis $h$, an agent gets reward:
$R_{\text {Hyp }}=\left{\begin{array}{ll}+1 &amp; a=G(h, W) \ -1 &amp; a=\neg G(h, W) \ 0 &amp; \text { otherwise }\end{array}\right.$
The observation in this POMDP is $o=\left(s_{W}, h\right)$, the state from the world $W$ plus the hypothesis. The state is $s=\left(s_{W}, h, L_{W}\right)$. This includes the rule $L_{W}$ which is not visible in the observation. The action space is just $A_{W} \cup{$ true, false $}$ for any $W$ (they are the same for a given environment); and $T=T_{W}$. Note that the transition function $T$ depends on the (hidden) $L_{W}$. The goal of hypothesis verification is now to discover the truth of $h$, which depends on $L_{W}$.</p>
<h1>4 Methodology</h1>
<p>RL baseline Given the formulation of the hypothesis verification problem as a POMDP, we could try to solve it using an RL agent with $a=\pi\left(O_{t}, h\right)$, where $O_{t}=\left{o_{t}, o_{t-1}, \ldots o_{t-K}\right}$. Here $o$ is an observation of the current world, $K$ is a history window size, and $h$ is the hypothesis for the current world. We found that standard RL agents struggle to solve this problem.
To make the problem easier to solve, we can augment the reward with direct supervision of a prediction network $f\left(O_{t}, h\right)$ which takes in the last $K$ observed observations of the environment and the hypothesis and predicts whether or not the hypothesis is true. Our policy network now, instead of taking true or false actions, takes a special stop action which is replaced by true or false based on the prediction of $f$.
Even with the augmented supervision, as shown in Fig. 4, an RL baseline is not able to solve the task. In order determine whether a hypothesis is true, the agent needs to take the correct sequence of actions related to the hypothesis. But in order to know that a particular sequence of actions was the right one, it needs to be able to correctly predict the hypothesis. Guessing with no information gives a zero average reward, and despite the supervision on the output of the predictor, the predictor does not see state transitions that allow it to learn.
Pretraining using Triplet Hypotheses In light of the difficulties directly training an RL model using terminal reward and hypothesis prediction supervision, we take advantage of the fact that many causal statements about the world have the form: (pre-condition, action sequence) $\Longrightarrow$ post-condition
We define this formally in Appendix A.2, but informally this means that when the state meets a "precondition" and an "action sequence" is taken, this will result in the state meeting a "post-condition." In Figure 1 the pre-condition is having a stick and being at craftingtable, the action sequence is craft, and the post-condition is that a torch is made.
This structure can be converted into a reward function that can be used to pretrain the agent policy $\pi$. The idea is to reward the agent for taking actions which alter the truth of the pre-condition and post-condition (i.e. changing the world state so that pre/post-conditions are met or not). If it matches the pre-condition state and takes the action, if the statement is true, the post-condition should toggle from false to true in the world. Similarly, if post-condition changes but the pre-condition did not change, the statement must be false. This can be formalized in the following reward function to encourage the agent to toggle pre-condition and post-condition states:
$R_{\text {pre }}=\left{\begin{array}{cl}+C &amp; a=\text { stop } \&amp; \ &amp; \text { pre-condition changed in last } K \text { steps } \ 0 &amp; \text { otherwise }\end{array}\right.$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$R_{\text {pre+post }}=\left{\begin{array}{cl}+C &amp; a=\text { stop \&amp; post+pre-condition } \ &amp; \text { changed in last } K \text { steps } \ 0 &amp; \text { otherwise }\end{array}\right.$
This encourages the policy $\pi$ to change the pre-condition and post-conditions (via pre-condition) in the last $K$ steps, so that a predictor looking at the last $K$ observations will be able to deduce the truth value of the hypothesis. More generally, training with this reward function forces the policy network to ground text concepts (e.g. the text "stick" means [object_stick]) and also captures the causal rules within the world. Consequently, following pretraining, the policy network can then be fine-tuned using the original reward function $R_{\text {Hyp }}$. Since the policy network is no longer random, a robust prediction network $f$ can also be learned. While not all hypotheses fit into the triplet format, we show in the experiments that the knowledge captured by the policy and prediction networks during this phase of training can generalize to less structured hypotheses. We have a set of hypotheses for each world that contains only these triplet-structured hypotheses. We use this set for our pretraining.</p>
<p>Training using Triplet hypothesis After pretraining the policy $\pi$, we further train $\pi$, as well as the prediction network $f$ using the same set of triplet hypotheses, but now using $R_{\text {Hyp }}$ instead of $R_{\text {pre }}$ or $R_{\text {pre+post }}$. Two variants are explored: (i) "fixed" - keep $\pi$ fixed but train the prediction network $f$ and (ii) "finetune" - finetune $\pi$ and train $f$. Performance in this phase is used to select promising models for subsequent stages of training. Specifically, runs achieving $&lt;90 \%$ accuracy (see the appendix for alternate cutoff) are eliminated.</p>
<p>Adaptation to non-triplet hypotheses Next, we want to show that we can adapt our networks to hypotheses other than those that fall neatly into the triplet structure. To adapt to the larger set of hypotheses, we start with the networks trained previously on triplet templates. During this training stage, the triplet-form constraint is relaxed and training proceeds with both triplet and non-triplet hypotheses (see Sec. 5), using an even split between the two types.</p>
<h1>5 Evaluation Environments</h1>
<p>In this section we describe our environments The environments $\mathcal{E}$ are designed so that the prior probability $p(h=$ true $)=0.5$ and the initial observation $o_{0}$ does not contain information about $h$.
Environments We created four different environments for hypothesis verification. ColorSwitch, Pushblock and Crafting are all gridworld-based environments. A fourth enviornment is created by adapting the standard Cartpole task to include interactive elements. See Fig. 2.
ColorSwitch: The agent is placed in a world with one or more color switches which are randomly either "on" or "off" and a door which is either open or closed. The agent is able to move and toggle the switch positions. One of the switches in the world, when in the correct position (can be either on or off) will cause the door to open. The other switches have no effect. Hypotheses in this environment relate to the color and position of switches and how that opens or closes the door.</p>
<p>Pushblock: The agent is placed in a world with a block which can be pushed by the agent, and a door. The agent can move and push on the block. The door opens when the block is in a particular part of the grid: "up" - top two rows, "down" - bottom two rows, "left" - leftmost two rows, "right" rightmost two rows. The hypotheses in this environment relate to the position of the pushblock and how that affects the door.</p>
<p>Crafting: The agent is placed in a world with crafting rules similar to that of the popular Minecraft game. The agent is spawned along with a number of crafting items, and a crafting location. The agent is able to move, pick up items into its inventory and use the crafting location using special crafting actions. There is some true "recipe" which produces some new item in the agent's inventory.
Cartpole: This is the standard classic control cartpole problem where a pole is attached by an un-actuated joint to a cart. The regular actions are "left" and "right" and if the pole falls, the episode ends. In our modification, there are "zones" (an interval on the x axis) where the physical laws of the cartpole change by either changing the gravity constant, or applying a "wind force" blowing the cart in one direction. Like in ColorSwitch, the zones are specified by color. Typically one color zone has an effect and the other is a decoy zone that has no effect. The hypotheses relate to which color zones correspond to what changes to the physics.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Examples of the four environments used in our experiements: ColorSwitch, Pushblock, Crafting and Cartpole.</p>
<p>In the grid world environments, items are randomly generated in a 5 by 5 grid. The world observation is given by a 1-hot vector for each grid location and inventory. The hypothesis is encoded as sequence of tokens. In Cartpole the state is the standard dynamics as well as a 1-hot vector specifying the location and color of the zones.
Hypothesis Construction We now describe how the hypotheses for each world in each environment are automatically generated via templates. Three different varieties are considered: (i) triplet hypotheses, (ii) general templates and (iii) special case templates. (See Appendix A. 3 for all of the possible templates for an environment and further details).
Triplet hypotheses: Here the hypotheses takes the form of an explicit logical statement: (precondition, action sequence) $\Longrightarrow$ post-condition. When the pre-condition is true, and the action sequence is performed, the post-condition will become true. To generate triplet hypotheses, we: (i) randomly select a pre-condition template from a set list; (ii) randomly select an action template; (iii) randomly select a post-condition template; and (iv) fill in any entities in the final template. In our example from Fig. 1 this would be ("when you are at crafting table and you have stick"; "and then you craft"; "then torch is made").
General templates: Instead of drawing a template from the triplet form, a single template for the hypothesis is drawn and the values populated. For instance, in Pushblock, a template might be "the door can only be opened when the pushblock is PUSHBLOCK_POSITION" and then "left" would be drawn for PUSHBLOCK_POSITION. These templates are more general than the triplet ones in that they have no explicit (pre-condition, action sequence and post-condition) structure.
Special cases: We also use more difficult and general hypothesis templates. These cannot be neatly fit into a triplet format by rewording, and may not fully describe the rules of the world. Some examples of these harder templates are: (i) Negating effects (e.g. "door is not open"); (ii) Negating conditions (e.g. "switch is not on"); and independence (e.g. "door independent of blue switch").</p>
<h1>6 Experiments</h1>
<p>Figure 3 shows results from learning with pretraining rewards $R_{\text {pre+post }}$ and $R_{\text {post }}$. There is relatively little variance, with all runs achieving near the theoretical maximal rewards ${ }^{2}$ For Crafting and Cartpole, $R_{\text {pre+post }}$ is not always achievable if true and distractor items are far away from each other. See Appendix B. 1 for further discussion. ${ }^{3}$.
In Figure 4, we show the results on non-triplet adaptation (Sec. 4). As discussed in Section 5, this stage includes the more difficult, non-triplet templates not seen during pretraining or during triplet hypothesis training. We also break down the final hypothesis prediction accuracy for our methods in Table 1. This allows us to see whether our methods were able to adapt to non-triplet hypotheses. All network architectures for the policy and prediction networks, as well as hyper-parameters are the same for all methods.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>RL baseline Figure 4 shows the RL baseline at chance-level performance, the only exception being CartPole were it achieves $\sim 60\%$, versus the $\sim 90\%$ of our approaches. Note that this is the RL baseline with the supervised predictor, as discussed in Section 4. This poor performance is a result of training both policy and predictor from scratch; see Figure 6.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Pretraining reward ( $R_{\text {pre }}+R_{\text {pre+post }}$ ) on our four environments.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Hypothesis prediction accuracy on both triplet and non-triplet hypotheses (1-1 ratio) for the ColorSwitch, Pushblock, Crafting and Cartpole environments, using $R_{\text {Hyp }}$ reward for training.</p>
<p>Other baselines We also include two other simple baselines "no act" and "random act." The "no act" baseline simply takes the stop action at $t=0$, forcing the prediction network to give an answer from just the first observation. This fails because the agent needs to take actions in the world to be able to predict the hypothesis accurately. This also confirms that we do not accidentally leak the ground truth of $h$ into the initial observation. For "random act", a random policy is used (i.e. uniform across actions). This fails as random actions are extremely unlikely to verify the hypothesis.
Discussion From these results, we can clearly see that naive RL and other baselines cannot efficiently solve hypothesis verification tasks. When we use our pretraining method, we use the insight that hypotheses often have a clear causal structure that can be exploited when they are formed as "triplet hypotheses." Not all hypotheses fall neatly into this form, and we may not have this form for all hypotheses. But if we have some that fit this form, we can gain a foothold that lets us make progress on this problem, and then later adapt them to other hypotheses. From Figure 4 we can see that this pretraining, triplet training and adaptation works. We also show in Section 6 that other plausible pretraining rewards fail to achieve the same results as our method.
Looking at the different environments, we see that in Pushblock and ColorSwitch, even with the policy learned from the triplet pre/post reward, the agent is able to generalize and perform well on hypotheses not seen in the pretraining phase as we can see in Table 1. Because the finetuning step adds additional variance to training, there is a small, but non-trivial drop in average performance. This drop is less extreme when comparing max values (see Appendix B.3).
In Crafting and Cartpole on the other hand, to do well on the unseen templates, the policy also needs to be fine-tuned. This tells us that when we do have to generalize to unseen hypotheses (especially non-triplet hypotheses), adapting the policy as well as the prediction network is necessary. Recall that we test very different hypotheses such as as negations and "independence" hypotheses not see in triplets (see suplementary). We see from Table 1 that indeed, our finetuned policies greatly outperform the fixed policies on the non-triplet templates.
We further break down these results by specific templates, entities mentioned (e.g. colors) and whether the given template was true or false. We find for instance that for most environments it is harder to determine a hypothesis is false than true. See Appendix E for the full analysis.</p>
<p>Table 1: Average Hypothesis Prediction scores, broken down by triplet (pretrained) and non-triplet (not seen in pretraining)</p>
<table>
<thead>
<tr>
<th></th>
<th>Method</th>
<th>Overall</th>
<th>Triplet Accuracy</th>
<th>Non-triplet Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>ColorSwitch</td>
<td>Fixed Policy</td>
<td>86.6%</td>
<td>91.1%</td>
<td>82.1%</td>
</tr>
<tr>
<td></td>
<td>Finetuned Policy</td>
<td>77.5%</td>
<td>79.7%</td>
<td>75.4%</td>
</tr>
<tr>
<td>Pushblock</td>
<td>Fixed Policy</td>
<td>86.9%</td>
<td>87.9%</td>
<td>85.9%</td>
</tr>
<tr>
<td></td>
<td>Finetuned Policy</td>
<td>85.6%</td>
<td>86.3%</td>
<td>84.8%</td>
</tr>
<tr>
<td>Crafting</td>
<td>Fixed Policy</td>
<td>77.3%</td>
<td>92.8%</td>
<td>61.8%</td>
</tr>
<tr>
<td></td>
<td>Finetuned Policy</td>
<td>90.7%</td>
<td>98.4%</td>
<td>83.0%</td>
</tr>
<tr>
<td>Cartpole</td>
<td>Fixed Policy</td>
<td>84.2%</td>
<td>92.0%</td>
<td>76.3%</td>
</tr>
<tr>
<td></td>
<td>Finetuned Policy</td>
<td>92.5%</td>
<td>93.4%</td>
<td>91.6%</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Final hypothesis accuracies using alternate forms of intrinsic pretraining versus our pretraining (purple). "End" = reward only received at end of episode. "Dense" = reward received immediately after changing item state.</p>
<p>Alternate forms of pretraining As an ablation, we test four variants of an "intrinsic" reward to see if other pretraining schemes might perform equally well. We show results on the gridworld domains using 4 different intrinstic forms of pretraining: (i) change any item state in the world; receive reward at end of episode. (ii) change any item referenced in the hypothesis; receive reward at end of episode. (iii) change any item state in the world; receive reward instantaneously. (iv) change any item referenced in the hypothesis; receive reward instantaneously. Here, "item" means any object that is not the agent (including crafting items, switches, pushblocks, etc.). (See Appendix F).</p>
<p>In Figure 5 we show the accuracies on the final hypothesis verification task for both triplet and non-triplet hypotheses, using the four intrinsic pretraining methods. We also plot the final average accuracy obtained by our adapted methods from Figure 4. For the intrinsic pretrained policies the best run is shown to show the best-possible case of the alternative methods.</p>
<p>For Crafting the dense intrinsic pretraining works about as well as ours. This can be explained by the fact that this particular form intrinsic pretraining directly rewards the agent for doing many of the operations in the actual Crafting task, i.e. picking up objects and crafting objects. However, averaging across the three environments, all the intrinsic pretraining methods do worse than our approach, showing the merits of our pretraining approach which exploits structure common to many hypotheses, yield an effective and general form of pretraining.</p>
<p>Oracle ablations We also show two oracle analysis in the Crafting environment. In the first, we provide an "oracle" hypothesis predictor which will output the ground truth of the hypothesis if it is inferable from the last K frames, and test whether we can learn a policy directly using reward R<sub>Hyp</sub>. Similarly, we also train a hypothesis prediction network with observations from an oracle policy network (observations from which the ground truth of the hypothesis is always inferable).</p>
<p>Figure 6 (left) shows the prediction oracle agent quickly converging. From this we surmise that if we can predict the hypothesis already, learning an optimal policy is relatively straightforward. Similarly, (right) show that with the correct observations from an oracle policy, the hypothesis predictor is able to quickly converge as well. This shows the joint nature of the problem (i.e. learning both policy and predictor) is what makes it challenging for the RL baseline.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: (Left) Results on training an RL using $R_{\text {Hyp }}$ with oracle predictor on the Crafting environment. Mean and variance on 25 random seeds are shown. (Right) Results training just hypothesis prediction on oracle policy.</p>
<h1>7 Discussion</h1>
<p>In this work, we propose a tractable formulation of the problem of training agents that can interact with a world to test hypotheses. We show that generic RL techniques struggle with the problem, but by using the common structure of some hypotheses, we are able to develop a method that works in simple environments. Specifically, we use the fact that many hypotheses can be broken into triples of the form of (pre-condition, action sequence, post-condition). We also show that once pretrained using this factorization, agents can be adapted to verify more general hypotheses.</p>
<h2>References</h2>
<p>[1] A. Bordes, N. Usunier, A. Garca-Durn, J. Weston, and O. Yakhnenko. Irreflexive and hierarchical relations as translations. arXiv 1304.7158, 2013.
[2] Ronald J. Brachman and Hector J. Levesque. Knowledge Representation and Reasoning. Morgan Kaufmann Series in Artificial Intelligence, 2004.
[3] Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, and Ruslan Salakhutdinov. Gated-attention architectures for task-oriented language grounding. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
[4] Jinyoung Choi, Beom-Jin Lee, and Byoung-Tak Zhang. Multi-focus attention network for efficient deep reinforcement learning. In Workshops at the Thirty-First AAAI Conference on Artificial Intelligence, 2017.
[5] Zenon Colaco and Mohan Sridharan. Mixed logical and probabilistic reasoning for planning and explanation generation in robotics. arXiv 1508.00059, 2015.
[6] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 2054-2063, 2018.
[7] Ishita Dasgupta, Jane Wang, Silvia Chiappa, Jovana Mitrovic, Pedro Ortega, David Raposo, Edward Hughes, Peter Battaglia, Matthew Botvinick, and Zeb Kurth-Nelson. Causal reasoning from meta-reinforcement learning. arXiv preprint arXiv:1901.08162, 2019.
[8] Ernest Davis. Representations of Commonsense Knowledge. Morgan Kaufmann Publishers Inc., 1990.
[9] Ernest Davis and Gary Marcus. Commonsense reasoning and commonsense knowledge in artificial intelligence. Commun. ACM, 58(9):92-103, August 2015.
[10] Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter Battaglia, and Nando de Freitas. Learning to perform physics experiments via deep reinforcement learning. arXiv preprint arXiv:1611.01843, 2016.
[11] A. Fader, S. Soderland, and O. Etzioni. Identifying relations for open information extraction. In EMNLP, 2011.
[12] Richard E Fikes and Nils J Nilsson. Strips: A new approach to the application of theorem proving to problem solving. Artificial intelligence, 2(3-4):189-208, 1971.</p>
<p>[13] Artur S d’Avila Garcez, Krysia B Broda, and Dov M Gabbay. Neural-symbolic learning systems: foundations and applications. Springer Science \&amp; Business Media, 2012.
[14] Joseph C Giarratano and Gary Riley. Expert systems. PWS publishing co., 1998.
[15] A. Gopnik. Scientific thinking in young children. theoretical advances, empirical research and policy implications. Science, 28(337):1623-1627, Sept 2012.
[16] Jarosław M Granda, Liva Donina, Vincenza Dragone, De-Liang Long, and Leroy Cronin. Controlling an organic synthesis robot with machine learning to search for new reactivity. Nature, 559(7714):377, 2018.
[17] Kushal Kafle and Christopher Kanan. Visual question answering: Datasets, algorithms, and future challenges. Computer Vision and Image Understanding, 163:3-20, 2017.
[18] Ross D King, Jem Rowland, Stephen G Oliver, Michael Young, Wayne Aubrey, Emma Byrne, Maria Liakata, Magdalena Markham, Pinar Pir, Larisa N Soldatova, et al. The automation of science. Science, 324(5923):85-89, 2009.
[19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[20] George Konidaris, Leslie Pack Kaelbling, and Tomas Lozano-Perez. From skills to symbols: Learning symbolic representations for abstract high-level planning. Journal of Artificial Intelligence Research, 61:215-289, 2018.
[21] Ilya Kostrikov. Pytorch implementations of reinforcement learning algorithms. https:// github.com/ikostrikov/pytorch-a2c-ppo-acktr, 2018.
[22] Tamar Kushnir and Alison Gopnik. Young children infer causal strength from probabilities and interventions. Psychological Science, 16(9):678-683, 2005.
[23] Douglas B. Lenat. Cyc: A large-scale investment in knowledge infrastructure. Commun. ACM, 38(11):33-38, November 1995.
[24] Hugo Liu and Push Singh. Conceptnet—a practical commonsense reasoning tool-kit. BT technology journal, 22(4):211-226, 2004.
[25] K. Lu, S. Zhang, P. Stone, and X. Chen. Robot representation and reasoning with knowledge from reinforcement learning. arXiv 1809.11074, 2018.
[26] Anthony Manchin, Ehsan Abbasnejad, and Anton van den Hengel. Reinforcement learning with attention that works: A self-supervised approach. arXiv preprint arXiv:1904.03367, 2019.
[27] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In International Conference on Learning Representations, 2019.
[28] T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, B. Yang, J. Betteridge, A. Carlson, B. Dalvi, M. Gardner, B. Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis, T. Mohamed, N. Nakashole, E. Platanios, A. Ritter, M. Samadi, B. Settles, R. Wang, D. Wijaya, A. Gupta, X. Chen, A. Saparov, M. Greaves, and J. Welling. Never-ending learning. Commun. ACM, 61(5):103-115, April 2018.
[29] Judea Pearl. Causality: models, reasoning and inference. Cambridge University Press, 2 edition, 2009.
[30] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad. CoRR, abs/1806.03822, 2018.
[31] Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M Marlin. Relation extraction with matrix factorization and universal schemas. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 74-84, 2013.
[32] Alexander Rush. The annotated transformer. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), pages 52-60, 2018.
[33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[34] Elizabeth S Spelke, Karen Breinlinger, Janet Macomber, and Kristen Jacobson. Origins of knowledge. Psychological review, 99(4):605, 1992.</p>
<p>[35] C. Steinruecken, E. Smith, D. Janz, J. Lloyd, and Z. Ghahramani. Automated Machine Learning. Springer Series on Challenges in Machine Learning, 2019.
[36] F. Suchanek, G. Kasneci, and G. Weikum. Yago: a core of semantic knowledge. In Proceedings of the 16th international conference on World Wide Web, 2007.
[37] Joep Vanlier, Christian A Tiemann, Peter AJ Hilbers, and Natal AW van Riel. Optimal experiment design for model selection in biochemical networks. BMC systems biology, 8(1):20, 2014.
[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.
[39] Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony R. Dick, and Anton van den Hengel. Visual question answering: A survey of methods and datasets. arXiv 1607.05910, 2016.
[40] Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van den Hengel. Ask me anything: Free-form visual question answering based on knowledge from external sources. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
[41] Shiqi Zhang and Peter Stone. Corpp: Commonsense reasoning and probabilistic planning as applied to dialog with a mobile robot. In AAAI, 2015.</p>
<h1>A Templates</h1>
<h2>A. 1 World and Hypothesis Construction</h2>
<p>Returning again to our notation from the main paper, the environment at each spawn needs to construct a world $W$, and a hypothesis $h$ that is either true or false in the world. $W$ in particular describes the rules about how the environment works (i.e. which switch opens the door) which in our case can precisely be describe by a hypothesis. So given a true hypothesis, we can exactly describe the rules of the world. Therefore, in order to create an instance of a possible $W$, we can instead draw a true hypothesis about the world at random. From the hypothesis, we can then construct the rules the determine how objects in the world behave. Note that there are couple exceptions to this for our harder hypotheses, where the hypothesis can be true but only partially describes all the rules of $W$. For these cases, we draw yet another template which is consistent with the hypothesis and use that to construct the rules, such as deciding which color switch really opens the door.</p>
<p>Because we have to randomly give either a true or false hypothesis, we also need to be able to generate a false hypothesis for the world. So for every instance, we also draw a random false hypothesis. Now, given a true and false hypothesis, we can fully generate the world and all the items that appear in either statement. So for instance, if the true hypothesis mentions a green switch and the false one mentions a blue switch, we generate both a green and blue switch. Then, we can set the rules such that the right thing happens. So in this example, switching the green switch opens the door and the blue switch does nothing.</p>
<p>The final step is then to randomly choose either the true or false statement as the "visible" hypothesis which is passed to our agent to verify. Because we generate the world and spawn the items before we make this choice, we ensure that we do not accidentally give away the truth of the hypothesis based on what items spawned.</p>
<p>Our process for generating a new spawn of environment can thus be summarized as follows:</p>
<ol>
<li>We randomly generate a true hypothesis</li>
<li>We randomly generate a false hypothesis</li>
<li>We construct a ruleset from the true hypothesis</li>
<li>We spawn the agent and the items in the world described in both the true and false hypothesis</li>
<li>We randomly choose either the true or false hypothesis as the "visible" hypothesis that the agent must verify</li>
</ol>
<h2>A. 2 Triplet Definitions</h2>
<p>Here we further define our terms "pre-condition" "action sequence" and "post-condition."
Consider a state $s$ (here we ignore the state/observation distinction for clarity of explanation, this would be an observation in our POMDP). We can consider a pre-condition or post-condition to be a boolean function of the state $s$. Given a state $s$, we can say whether a particular precondition or post-condition is true or false. For example, the condition "the door is open" is a function of the state $s$, which we can write as $f_{\text {thedoorisopen }}(s)$ which is true when the door is open, and false when it is not. Action sequences are also boolean functions, but over a sequence of states and actions. So for instance, "the agent moves to the craftingtable" can be written as $f_{\text {theagentmovestothecraftingtable }}\left(s_{1}, s_{2}, \ldots, a_{1}, a_{2}, \ldots\right)$ for some sequence of states and actions, which is true when the agent starts not at the craftingtable and takes actions that move it to the craftingtable.</p>
<p>Now we can connect these three functions or conditions to causality. In this construction, what we are doing is hypothesizing a causal connection between the pre-condition and action sequence and the post-condition: (pre-condition, action sequence) $\Longrightarrow$ post-condition. This is a logical statement, which can be true or false, which asserts that when we satisfy the pre-condition and take the action sequence, the resulting state will satisfy the post-condition.</p>
<p>Finally, we connect this discription of these terms to the string text of the actual hypotheses. We treated these three terms like exact logical functions, but we in fact translate these functions to text</p>
<p>strings. So a logical function of whether the door is open becomes "the door is open." In this work, we write these strings ourselves, although in future work, these could be crowd-sourced, or generated as part of an interactive game. Human language is more complicated and less un-ambigious than mathematical formulations but as we note in the main paper, we train our agents to generalize to new hypotheses and ones that do not follow this exact structure. By our choice of using templated language, we keep the kinds of hypotheses agents can learn as general as possible. Future work might explore how to solve new hypotheses online from humans, or even generate its own hypotheses to solve.</p>
<p>In the next sub-section, we exhaustively list the triplet and non-triplet templates we use in this work.</p>
<h1>A. 3 Templates</h1>
<h2>Color Switch:</h2>
<h2>Pre-condition:</h2>
<p>if the COLOR switch is ON_OFF_SWITCHSTATE
when the COLOR switch is in the ON_OFF_SWITCHSTATE position
the COLOR switch is ON_OFF_SWITCHSTATE</p>
<h2>Action:</h2>
<h2>Post-condition:</h2>
<p>then the door is open
the door is passable
and we see the door is open
the door will open</p>
<h2>Finetune templates:</h2>
<p>the door can only be opened by switching the COLOR switch to ON_OFF_SWITCHSTATE
when we see the COLOR switch is ON_OFF_SWITCHSTATE the door must be open
if the COLOR switch turns ON_OFF_SWITCHSTATE the door opens
when we see the door open it must be that the COLOR switch is in the ON_OFF_SWITCHSTATE position
those who want to open the door must first switch the COLOR switch ON_OFF_SWITCHSTATE
no password just make the COLOR switch be ON_OFF_SWITCHSTATE to open the door
COLOR switch ON_OFF_SWITCHSTATE implies door is open
only the COLOR switch being ON_OFF_SWITCHSTATE opens the door
the door is open because COLOR switch is in the ON_OFF_SWITCHSTATE position
COLOR switch ON_OFF_SWITCHSTATE equals open door
the COLOR switch opens the door but only when it is ON_OFF_SWITCHSTATE
door is open must mean that COLOR switch is ON_OFF_SWITCHSTATE an ON_OFF_SWITCHSTATE means the door is
open but only if it is COLOR
COLOR controls the door and it opens when it is ON_OFF_SWITCHSTATE
ON_OFF_SWITCHSTATE is the correct position of the COLOR switch and it opens the door
the switch that causes the door to be open when it is ON_OFF_SWITCHSTATE is COLOR
if you see COLOR switch then the door is open
the door is independent of the COLOR switch
if the door is not open then the COLOR switch must be ON_OFF_SWITCHSTATE
if the COLOR switch is not ON_OFF_SWITCHSTATE then the door is open
to make the door not open the COLOR switch must be not ON_OFF_SWITCHSTATE
whether the door is open is completely independent of the COLOR switch
the COLOR switch is what controls the door
a not ON_OFF_SWITCHSTATE COLOR switch opens the door</p>
<h2>Template Values</h2>
<h2>COLOR:</h2>
<p>blue
red
green
black</p>
<h1>ON_OFF_SWITCHSTATE:</h1>
<p>on
off</p>
<h2>Pushblock</h2>
<h2>Pre-condition:</h2>
<p>whenever the pushblock is in the PUSHBLOCK_POSITION
if the pushblock is at the PUSHBLOCK_POSITION
the pushblock is at the PUSHBLOCK_POSITION</p>
<h2>Action:</h2>
<h2>Post-condition:</h2>
<p>then the door is open
the door is passable
and we see the door is open
the door will open</p>
<h2>SP_FULL_TRAIN:</h2>
<p>PUSHBLOCK_POSITION is the correct position for the pushblock for the door to open if the door is open it must be that the pushblock is at the PUSHBLOCK_POSITION when the door is open it is because the pushblock is in the PUSHBLOCK_POSITION when the pushblock is at the PUSHBLOCK_POSITION the door is open pushblock PUSHBLOCK_POSITION means door open the door can only be opened when the pushblock is PUSHBLOCK_POSITION if the pushblock is PUSHBLOCK_POSITION it means the door is open PUSHBLOCK_POSITION pushblock opens the door open door implies pushblock PUSHBLOCK_POSITION open door means pushblock PUSHBLOCK_POSITION door opens when PUSHBLOCK_POSITION is where the pushblock is PUSHBLOCK_POSITION is the correct position for the pushblock to open the door the door when the pushblock is PUSHBLOCK_POSITION is open PUSHBLOCK_POSITION position of the pushblock causes the door to open door only opens on PUSHBLOCK_POSITION pushblock door can only open with pushblock being PUSHBLOCK_POSITION the pushblock being at the PUSHBLOCK_POSITION is completely independent of the door the pushblock being PUSHBLOCK_POSITION is independent of the door being open the door state is independent of pushblock PUSHBLOCK_POSITION PUSHBLOCK_POSITION pushblock and door are independent</p>
<h2>Pushblock values:</h2>
<h2>PUSHBLOCK_POSITION:</h2>
<p>left
right
top
bottom</p>
<h2>Crafting</h2>
<h2>Pre-condition:</h2>
<p>when you are at LOCATION and you have CRAFTING_ITEM you are at LOCATION and have in your inventory CRAFTING_ITEM whenever you have a CRAFTING_ITEM and are at LOCATION</p>
<h2>Action:</h2>
<p>and you do CRAFTING_ACTION</p>
<p>then you CRAFTING_ACTION</p>
<h1>Post-condition:</h1>
<p>you now have CREATED_ITEM in your inventory
then CREATED_ITEM is created
and this creates CREATED_ITEM
so CREATED_ITEM is created and put in your inventory
then CREATED_ITEM is made</p>
<h2>Finetune Templates:</h2>
<p>to create a CREATED_ITEM you must have CRAFTING_ITEM and go to LOCATION and do the action CRAFTING_ACTION
CREATED_ITEM can be created by doing CRAFTING_ACTION at LOCATION when CRAFTING_ITEM is in inventory whenever you do CRAFTING_ACTION and have CRAFTING_ITEM at LOCATION a CREATED_ITEM is made you have CRAFTING_ITEM and go to LOCATION and CRAFTING_ACTION and CREATED_ITEM will be created whoever does CRAFTING_ACTION at LOCATION with CRAFTING_ITEM gets CREATED_ITEM
if you have CRAFTING_ITEM at LOCATION and you CRAFTING_ACTION you get CREATED_ITEM
if you do CRAFTING_ACTION at LOCATION with CRAFTING_ITEM you make CREATED_ITEM
whenever you have CRAFTING_ITEM at LOCATION and do CRAFTING_ACTION then you make a CREATED_ITEM having CRAFTING_ITEM in your inventory being at LOCATION and doing CRAFTING_ACTION creates CREATED_ITEM
CREATED_ITEM can be made with CRAFTING_ITEM when you do CRAFTING_ACTION at LOCATION CRAFTING_ITEM plus LOCATION plus CRAFTING_ACTION equals CREATED_ITEM
create a CREATED_ITEM by being at LOCATION with CRAFTING_ITEM and doing CRAFTING_ACTION CRAFTING_ACTION at LOCATION creates CREATED_ITEM but only if you have a CRAFTING_ITEM if you want to make a CREATED_ITEM then go to LOCATION with CRAFTING_ITEM and do CRAFTING_ACTION CRAFTING_ITEM in inventory at LOCATION makes CREATED_ITEM if you do CRAFTING_ACTION CREATED_ITEM when CRAFTING_ITEM at LOCATION and do CRAFTING_ACTION
if you are at LOCATION and do CRAFTING_ACTION you make CREATED_ITEM
if you are anywhere and do CRAFTING_ACTION with CRAFTING_ITEM you make a CREATED_ITEM having CRAFTING_ITEM at LOCATION and doing CRAFTING_ACTION does not make a CREATED_ITEM CREATED_ITEM is created by being at LOCATION and doing CRAFTING_ACTION make a CREATED_ITEM by having a CRAFTING_ITEM and doing CRAFTING_ACTION you have CRAFTING_ITEM and go to LOCATION and CRAFTING_ACTION and CREATED_ITEM will not be created LOCATION plus CRAFTING_ACTION creates a CREATED_ITEM
with a CRAFTING_ITEM you can make a CREATED_ITEM by doing CRAFTING_ACTION</p>
<h2>Template Values:</h2>
<h2>CRAFTING_ITEM :</h2>
<p>iron
wood
stick
pickaxe
coal</p>
<h2>CREATED_ITEM:</h2>
<p>torch
bed</p>
<h2>LOCATION:</h2>
<p>craftingtable</p>
<h2>CRAFTING_ACTION:</h2>
<p>craft</p>
<h2>Carpole</h2>
<h2>Pre-condition:</h2>
<p>when are in the COLOR zone
if you are in the COLOR</p>
<p>when the agent is within COLOR</p>
<p>Action:
"</p>
<h1>Post-condition:</h1>
<p>the gravity is now MULTIPLIER
gravity is MULTIPLIER
the wind pushes DIRECTION
there is a DIRECTION wind</p>
<h2>Finetune Templates:</h2>
<p>MULTIPLIER gravity is caused by the COLOR zone
if you go to COLOR zone then gravity is MULTIPLIER
the gravity is MULTIPLIER because the agent is in the COLOR
COLOR zone implies MULTIPLIER gravity
gravity is MULTIPLIER whenever you go into the a COLOR zone
the COLOR causes gravity to MULTIPLIER
to make gravity MULTIPLIER need to be in COLOR
COLOR zone gravity MULTIPLIER
a gravity multiplier of MULTIPLIER is caused by being in COLOR zone
COLOR equals wind DIRECTION
when the wind blows DIRECTION it is because you are in COLOR
COLOR zone causes DIRECTION wind
only being in COLOR makes the wind blow DIRECTION
DIRECTION wind is in the COLOR zone
when you are in the COLOR zone there is a DIRECTION wind
DIRECTION wind is caused by being in the COLOR zone
wind pushing DIRECTION whenever you are in COLOR
gravity is totally independent of COLOR
COLOR zone does not effect gravity it is independent
the wind is completely independent of the COLOR zone
independent of wind DIRECTION is COLOR
gravity is changed by being in COLOR but not MULTIPLIER
the effect of being in COLOR is opposite to gravity MULTIPLIER
the wind blows opposite of DIRECTION when in COLOR zone
being in COLOR causes the wind to blow opposite to DIRECTION</p>
<h2>Template Values:</h2>
<h2>COLOR:</h2>
<p>blue
red
green
black</p>
<h2>MULTIPLIER :</h2>
<p>decreased
increased</p>
<h2>DIRECTION:</h2>
<p>left
right</p>
<h1>B Additional Discussion / Details</h1>
<h2>B. 1 Pretraining additional discussion</h2>
<p>For the ColorSwitch environment, we found that pretraining with just the pre-condition reward leads to better results for the Color Switch environment and show those results here. We chose $C=10$ for this proxy reward, so we see from our figure that we are able to achieve the pre-condition toggling almost perfectly.
For Pushblock, we use both $R_{\text {pre-i post }}$ and $R_{\text {post }}$, however, because of the afformentioned issue of the block getting stuck against a wall, it does not achieve perfect success each time.
For Crafting we see that pretrain results converge towards around 15. This is because the best the pretraining can do is to try to complete the recipe. If the hypothesis is false, however, we often cannot toggle the post-condition because it often requires more than $K=5$ steps to complete seperately from toggling the pre-condition.
Similarly for Cartpole, we see the same thing, except it can get to around 17.5. This is because if the true and false zones are adjacent to each other, the agent can toggle both the pre- and post-conditions successfully (try pre-condition, when it doesn't work, move the the adjacent zone where the effect is true).</p>
<h2>B. 2 Alternative cutoffs for adaptation training</h2>
<p>As promised, in Figure 7 we show the adaptation results when we choose an $80 \%$ cutoff instead of $90 \%$.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Hypothesis prediction accuracy on both triplet and non-triplet hypotheses for the Color Switch, Pushblock, Crafting and Cartpole environments, using $R_{\text {Hyp }}$ reward for training.</p>
<h2>B. 3 Training Variance for Pushblock and ColorSwitch</h2>
<p>In the main paper we discussed the problem of the finetuning training adding additional variance to training, resulting in worse average performance. First, we can actually see this from the training curves themselves in Figure 4 in the main text. The orange curves of the pretraining reward matches the blue fixed reward curve, but has more variance. In the ColorSwitch example, it has a hickup around $1.5 e 7$ timesteps that it does not fully recover from. This is because, unlike the fixed method, this method has to simultaneously train both the policy network and the prediction network. So when the policy network achieves a less-optimal policy, it becomes more difficult to train the prediction network which feeds back into the policy network. Whereas the fixed method only trains prediction and is much more stable during training.
We also mention that for some runs, this becomes less extreme. Some runs of the finetune policy do much better than average. In colorswitch, the best finetune run achieves $83.5 \%$ accuracy versus $90.5 \%$ for fixed and for pushblock achieves $87.6 \%$ versus $89 \%$, a smaller difference than the averages in Table 1.</p>
<h1>C Learning Details and Hyperparameters</h1>
<p>One detail of the prediction network is that we need to keep a memory of past state sequences, hypotheses and ground truths so we can actually train our prediction network. We do this by simply keeping track of the last $N$ times our agent answered a question, and keeping these in a FIFO memory. When we update our prediction network, we randomly sample from this pool. This also necessitates a 100 k step break in period to collect enough examples.</p>
<p>In our policy finetuning experiments, we also stabilize our dual optimization problem by trading of optimization of the policy network and the prediction network. We must also start with the prediction network so that the reward for answering correctly is meaningful.</p>
<p>No extensive grid search of hyper-parameters was conducted. At various stages we experimented with different hyperparameter values especially PPO parameters (timesteps per batch), learning rates, PPO epochs, and optimizer.</p>
<p>Experiments were performed on Tesla K40, TitanX and TitanX Pascals GPUs on 2-4 GPU workstations. Several jobs could be run on a single GPU with the greatest performance bottleneck being CPU cycles rather than GPU cycles or memory.</p>
<p>Table 2: Pretraining Hyperparameters</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Algorithm</td>
<td style="text-align: left;">PPO [33]</td>
</tr>
<tr>
<td style="text-align: left;">Timesteps per batch</td>
<td style="text-align: left;">2048</td>
</tr>
<tr>
<td style="text-align: left;">Clip param</td>
<td style="text-align: left;">0.2</td>
</tr>
<tr>
<td style="text-align: left;">Entropy coeff</td>
<td style="text-align: left;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">Number of parallel processes</td>
<td style="text-align: left;">8</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer epochs per iteration</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer step size</td>
<td style="text-align: left;">$2.5 e^{-4}$</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer batch size</td>
<td style="text-align: left;">32</td>
</tr>
<tr>
<td style="text-align: left;">Discount $\gamma$</td>
<td style="text-align: left;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">GAE $\lambda$</td>
<td style="text-align: left;">0.95</td>
</tr>
<tr>
<td style="text-align: left;">learning rate schedule</td>
<td style="text-align: left;">constant</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: left;">ADAM [19]</td>
</tr>
<tr>
<td style="text-align: left;">Past Frame Window Size</td>
<td style="text-align: left;">5</td>
</tr>
</tbody>
</table>
<p>Table 3: Finetuning Hyperparameters</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Algorithm</td>
<td style="text-align: left;">PPO [33]</td>
</tr>
<tr>
<td style="text-align: left;">Timesteps per batch</td>
<td style="text-align: left;">2048</td>
</tr>
<tr>
<td style="text-align: left;">Entropy coeff</td>
<td style="text-align: left;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">Number of parallel processes</td>
<td style="text-align: left;">8</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer epochs per iteration</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer step size</td>
<td style="text-align: left;">$1 e^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer batch size</td>
<td style="text-align: left;">32</td>
</tr>
<tr>
<td style="text-align: left;">Discount $\gamma$</td>
<td style="text-align: left;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">GAE $\lambda$</td>
<td style="text-align: left;">0.95</td>
</tr>
<tr>
<td style="text-align: left;">learning rate schedule</td>
<td style="text-align: left;">constant</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: left;">SGD</td>
</tr>
<tr>
<td style="text-align: left;">Past Frame Window Size</td>
<td style="text-align: left;">5</td>
</tr>
</tbody>
</table>
<p>Basis of RL implementations was from [21]</p>
<p>Table 4: Prediction Hyperparameters</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Timesteps per batch</td>
<td>2048</td>
</tr>
<tr>
<td>Optimizer step size</td>
<td>$1 e^{-3}$</td>
</tr>
<tr>
<td>Optimizer batch size</td>
<td>128</td>
</tr>
<tr>
<td>learning rate schedule</td>
<td>constant</td>
</tr>
<tr>
<td>Optimizer</td>
<td>ADAM [19]</td>
</tr>
<tr>
<td>Memory Burn-in</td>
<td>100000</td>
</tr>
<tr>
<td>Memory Size</td>
<td>200</td>
</tr>
<tr>
<td>Alternate Training Window</td>
<td>10000000</td>
</tr>
</tbody>
</table>
<h1>D Network Details</h1>
<p>Although other works such as [3] have investigated language-conditioned RL (usually in the form of instruction following), our hypothesis conditioned problem proved to be challenging, and required some novelty in network architectures. Figure 8 shows our network diagrams.
Other works such as [3] have incorporated gated mechanisms between language and perception. [26] employs self-attention mechanism within convolutional layers and [4] also employs a self-attention mechanism in a DQN. Neither work incorporates language and the architectures are quite different from each other. Figure 8 shows the policy and transformer architectures (this is also in the main text).
For the policy network, it was important to use key-value attention. That is: the hypothesis is fed into a seq2vec model and is used as the key of a dot-product attention mechanism. The state (the grid locations of the items in the world and the inventory of the agent if applicable) is fed as input to $N$ parallel MLPs. The output of the MLPs are then fed as the values of the attention mechanism. The output of the module is then fed into the final hidden layer of the actor-critic network.
For the prediction network, we use the popular transformer architecture [38]. Our prediction network encodes both the hypothesis and past observations (after they are passed through a one layer network) using transformer encoders. These sequences are then combined using a transformer to generate a final hidden state as output which is then fed to a final prediction layer and sigmoid function to get our binary prediction.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Network architecture for our policy network (left) and prediction network (right)</p>
<h2>D. 1 Network Ablation</h2>
<p>In Figure 9 we see the results of our network architecture ablation. As we can see, our new policy architecture described previosuly clearly outperforms a standard MLP policy network on the languagecondition pretraining task. We also see that the transformer architecture outperforms the LSTM and MLP model on the final task when we hold the policy network constant.</p>
<h2>D. 2 Implementation and hyperparameters</h2>
<p>We take much of our implementation of transformers from [32].</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: (left) policy network ablation (right) prediction network ablation.</p>
<p>Table 5: Policy Network Hyperparameters</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Seq2Vec Model</td>
<td style="text-align: left;">Bag-of-Words</td>
</tr>
<tr>
<td style="text-align: left;">Word Embedding Size</td>
<td style="text-align: left;">32</td>
</tr>
<tr>
<td style="text-align: left;">Hidden Size</td>
<td style="text-align: left;">32</td>
</tr>
<tr>
<td style="text-align: left;">MLP Num Hidden Layers</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">Number of MLP Modules</td>
<td style="text-align: left;">16</td>
</tr>
<tr>
<td style="text-align: left;">Transfer Layer</td>
<td style="text-align: left;">$t a n h$</td>
</tr>
</tbody>
</table>
<p>Table 6: MLP Baseline Policy Network Hyperparameters</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Seq2Vec Model</td>
<td style="text-align: left;">Bag-of-Words</td>
</tr>
<tr>
<td style="text-align: left;">Word Embedding Size</td>
<td style="text-align: left;">32</td>
</tr>
<tr>
<td style="text-align: left;">Hidden Size</td>
<td style="text-align: left;">32</td>
</tr>
<tr>
<td style="text-align: left;">MLP Num Hidden Layers</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">Transfer Layer</td>
<td style="text-align: left;">$t a n h$</td>
</tr>
</tbody>
</table>
<p>Table 7: Transformer Network Hyperparameters</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Word Embedding Size</td>
<td style="text-align: left;">32</td>
</tr>
<tr>
<td style="text-align: left;">Hidden Size</td>
<td style="text-align: left;">32</td>
</tr>
<tr>
<td style="text-align: left;">Transfer Layer</td>
<td style="text-align: left;">$R e L U$</td>
</tr>
<tr>
<td style="text-align: left;">Transformer $N$</td>
<td style="text-align: left;">3</td>
</tr>
</tbody>
</table>
<p>Table 8: Baseline Prediction Network Hyperparameters</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Seq2Vec Model</td>
<td style="text-align: left;">LSTM</td>
</tr>
<tr>
<td style="text-align: left;">LSTM Num Layers</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Word Embedding Size</td>
<td style="text-align: left;">32</td>
</tr>
<tr>
<td style="text-align: left;">Hidden Size</td>
<td style="text-align: left;">32</td>
</tr>
<tr>
<td style="text-align: left;">MLP Num Hidden Layers</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">Transfer Layer</td>
<td style="text-align: left;">$t a n h$</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ For Pushblock, sometimes the block can be stuck against a wall, so not all worlds are solvable
${ }^{3}$ For the gridworld environments we show variance runs on 25 random seeds and 5 for Cartpole. We provide further hyperparameters, and training details in Appendix C and D as well as network architecture and details in Appendix D&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
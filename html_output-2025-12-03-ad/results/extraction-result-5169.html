<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5169 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5169</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5169</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-109.html">extraction-schema-109</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <p><strong>Paper ID:</strong> paper-15876077</p>
                <p><strong>Paper Title:</strong> 17 Theory of Cognitive Pattern Recognition</p>
                <p><strong>Paper Abstract:</strong> 1.1 Perception and its constancy Born and developed in the middle of 1970’s, cognitive science is a kind of intersectional and integrative science aiming to study both the working principle and the developing mechanism of human brain and psyche. It is a product from the processes of intersection, infiltration and aggregation of such sciences as psychology, computer science, neurology, linguistics, anthropology, philosophy, and so on. As one of the important parts of cognitive science, cognitive psychology[1-6], developed in the middle of 1950’s, is a kind of psychology making the view of information processing as the core, thus also named information processing psychology, and a kind of science studying the processes of transforming, processing, storing, recovering, extracting and using information through sense. Perception has always been an important studying field of psychology. Cognitive psychology treats perception as the organization and explanation of sense information, and the process of acquiring the meanings of sense information. Correspondingly, this process is treated as a series of consecutive information processing, and the ability of the process depends on the past knowledge and experience. We can cover a building far away by just a finger, it means that the image of finger formed on the retina is bigger than that of the building. But if we move away the finger and first look at the building then the finger, we will feel the building is much bigger than the finger anyway, that indicating a very important feature of perception-constancy. The constancy of perception refers to perception keeps constant when the condition of perception changes in a certain range [7]. In the real world, various forms of energy are changed while reaching our sense organs, even the same object reaching our sense organs. Constancy in size and shape keeps our lives normal in this daedal world. Although an object sometimes seems smaller or bigger, we can recognize it. Constancy is the basis of stable perception of human to the outside. For instance, students can always recognize their own schoolbag, no matter far away (assuming it is visible) or close, overlooking or upward viewing, or looking in the front or sides. Although the images formed in the retina under the different conditions mentioned above are different from each other student’s perceptions of this object are the same schoolbag. Constancy in size and shape are two main types of the perception constancy. Perception constancy in size means that although the size of object images shot on the retina change, human perception of the size of object keeps constant. The size of image on the human retina directly depends on the distance between the object and our eyes. O pe n A cc es s D at ab as e w w w .ite ch on lin e. co m</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5169.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5169.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theory of Template Matching</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical model that posits stored literal copies (templates) of previously encountered patterns in memory which incoming stimuli are matched against one-to-one for recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Template theory (template matching)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Proposes that conceptual/perceptual recognition is achieved by matching incoming sensory input to stored, discrete exemplar-like templates that correspond one-to-one to recognizable patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>literal exemplar / template (symbolic/categorical copies)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>one-to-one mapping between stored template and input; high specificity; requires extensive storage of variants; low abstraction and compositionality.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Paper cites template theory historically and notes engineering examples (e.g., machine recognition of seals on paychecks) where exact-match templates work adequately; general appeal in early pattern-recognition systems.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Requires storing many templates (memory burden), poor flexibility/generalization to novel variations, limited account of bottom-up processing, criticized in the paper as insufficient to explain human recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Simple pattern matching tasks, constrained machine-recognition systems, OCR under fixed fonts, detection tasks requiring high fidelity matches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrasted with prototype and feature models; less flexible than prototype theory and less parsimonious than feature-based/compositional accounts; paper positions it as a limited component of cognition rather than a full account.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Recognition = retrieve best-matching stored template; matching process is essentially similarity or exact pattern comparison, sometimes preceded by preprocessing to allow tolerant matching.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How humans avoid combinatorial explosion of templates; integration with compositional and bottom-up mechanisms; empirical boundary conditions where templates suffice versus fail.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5169.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5169.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prototype theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theory of Prototype Matching</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model asserting that categories are represented by abstract prototypes (central tendencies) rather than by exhaustive exemplars; recognition relies on similarity to these prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Prototype theory / prototype matching</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual knowledge is stored as abstracted, generalized prototypes (summary representations of a category) and recognition/classification is achieved by comparing inputs to prototypes and assigning membership according to best-match.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>abstract prototype (summary/centroid representation), compatible with symbolic/feature-like formats</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>abstraction from instances, compactness (fewer stored items), graded category membership, flexibility to novel exemplars, less memory load than templates; in this paper prototypes are formalized topologically.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Paper cites prototype theory as a major model in cognitive psychology and uses it functionally in its framework (e.g., prototypes as basis for component matching and category assignment); applied examples include character and shape recognition and Chinese character formation.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Paper notes prototype accounts emphasize top-down processing and can underdescribe bottom-up perceptual processes; lack of mechanistic detail on how prototypes are formed and how graded similarity is computed; no direct empirical experiments reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Categorization, object recognition, concept learning, machine pattern recognition (as compact library), Chinese character composition in the paper's application.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Presented as more flexible than template theory and more abstract than feature analysis; compared with feature and component theories (paper argues prototypes can be understood as abstracted components and can be mapped topologically onto concrete parts).</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Recognition via similarity matching between input components (after topological transformation) and elements of a compact prototype set; nearest/probable-match decision rules (sometimes combined with thresholding or best-of-qualified matches).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How prototypes incorporate compositional structure and relations among parts; empirical tests of the paper's topological formalization; integration with bottom-up feature extraction and with graded similarity metrics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5169.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5169.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feature theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theory of Feature Analysis (Feature theory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bottom-up model where objects and concepts are represented as sets of diagnostic features; recognition arises from matching detected features to stored feature representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Feature theory (feature analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Concepts are represented functionally as structured collections of features (local detectors); classification is achieved by detecting and comparing feature sets rather than whole-pattern templates or global prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>feature-based (sparse / symbolic feature sets; can be localist or vector-like)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>bottom-up driven, compositional across features, enables generalization via shared features, efficient for many machine methods, but (as paper notes) may lack top-down/contextual modulation.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Widely used in computer vision and engineering; paper acknowledges broad application and utility of feature analysis in machine pattern recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Lacks account of top-down influences and holistic/contextual effects in human perception; may fail when relational or global/topological properties determine recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Low-level perception, computer vision feature extraction, OCR, speech recognition feature vectors, classification tasks where discriminative features are known.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Framed as complementary to prototype and template models; more bottom-up and data-driven than prototype/template approaches; paper argues overall that these descriptions are alternative ways of describing the same object for engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Detect features from input, assemble feature set, compare to stored feature descriptions, apply decision rules (e.g., Bayesian, SVM) for classification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How feature sets are organized into higher-level conceptual structure; how top-down context and prototypes influence low-level feature selection in cognition.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5169.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5169.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RBC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recognition-by-Components (Biederman)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A componential view of object representation: objects are represented as arrangements of a limited set of volumetric primitives ('geons') and recognized by parsing into these parts and their spatial relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Recognition-by-Components (RBC)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual/perceptual object representations are functional compositional structures built from a small finite vocabulary of geometric primitives (geons) plus relational structure; recognition proceeds by edge extraction, segmentation into components, and matching component structure to stored structural descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>compositional structural representation (symbolic parts + relations)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>compositionality (parts + relations), robustness to viewpoint/metric variation because parts are defined by invariant edge/concavity properties, parsimonious primitive inventory, explicit structural description.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Paper cites Biederman (1987,1990) and adopts the idea that objects can be decomposed into a small set of components; uses this to motivate compositional and structural accounts of pattern composition and prototype-component relations.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>The paper notes questions about sufficiency of limited component set for all objects; RBC primarily addresses shape-based vision and may not account for texture/material or highly configural stimuli; paper does not report new empirical tests.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Object recognition, shape perception, hierarchical decomposition in computer vision, modeling structural generalization and viewpoint invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrasted with prototype and template theories: RBC provides explicit compositional mechanism lacking in prototype abstractions; more structural and bottom-up than prototype matching but compatible with prototype abstraction of recurring components.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Edge extraction → segmentation into components → identification of component types (geons) → recovery of relational structure → match to stored structural descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Empirical validation across diverse object classes; integration with feature-based and prototype-based memory stores; account of semantic/functional knowledge beyond shape.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5169.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5169.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbol-net</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbol-net model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic network model where concepts are nodes linked by directed relations, representing hierarchical and logical organization of knowledge used in search-like retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Symbol-net (symbolic network) model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual knowledge is represented as discrete symbols (nodes) connected by labeled (often directional) links; cognition is modelled as traversals/searches through these symbolic networks.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>symbolic, localist network of nodes and directed links</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>explicit symbolic relations, hierarchical/up-down levels, rule-like logical organization, retrieval by directed search, transparent interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Paper describes symbol-net as one of three hypothetical knowledge-organization models and provides a schematic account of how it would represent conceptual relations; used as background for discussing knowledge organization.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Paper criticizes purely logical symbol-net approaches for relying on logic rather than psychological data and for potential inflexibility; does not report empirical tests.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Semantic memory modeling, symbolic AI, language understanding, classical cognitive architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared to level-semantics and activation-diffusion models; symbol-net is more logic-driven and less psychologically plausible than spreading-activation or graded similarity networks.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Search along directed links between nodes according to goal-driven criteria until an appropriate concept is found or retrieval fails.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How symbolic networks account for graded similarity, context effects, and probabilistic inference; scalability and learning of link weights from experience.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5169.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5169.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Level-semantics-net</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Level-semantics-net model (Collins & Quillian)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical semantic network in which concepts are organized in levels with 'is-a' and 'has' relations, supporting economy of storage and hierarchical retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Level-semantics-net (hierarchical semantic network)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Concepts are nodes arranged in a graded hierarchy; a concept's meaning is determined by its place in the hierarchy and its attribute ('has') links; retrieval uses hierarchical traversal yielding cognitive economy.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>symbolic hierarchical network</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>hierarchical categorization, inheritance of properties, storage economy via shared higher-level nodes, deterministic logical relations ('is-a', 'has').</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Paper presents level-semantics-net as a historical model of semantic organization and notes its computational simplicity and influence on language-based modeling; used as an implementable model for computer simulations in the paper's framework.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Paper points out that the level-semantics model explains organization by logic rather than psychological data; it fails to capture graded similarity and context-driven relations observed empirically.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Early semantic memory modeling, taxonomy-based classification, language understanding systems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Paper contrasts it with activation-diffusion (spreading activation) which relaxes strict hierarchical constraints and accommodates graded similarity; level-semantics is more rigid but easier to implement.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Retrieve concept by moving up/down hierarchy along 'is-a' or 'has' links, deriving properties by inheritance from higher-level nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How to handle cross-cutting categories, typicality effects, and similarity-based association not aligned with strict hierarchies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5169.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5169.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Activation-diffusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Activation-diffusion model (Collins & Loftus spreading-activation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic network model where activation spreads across links graded by relatedness, representing concepts as nodes in a similarity-weighted network rather than strict levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Activation-diffusion / spreading activation model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Concepts are nodes in a network linked by degrees of semantic relatedness; activation of one node diffuses to neighbors proportionally to link strength, producing graded, context-sensitive retrieval and priming effects.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>graded network / spreading activation (distributed localist)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>similarity-based links, graded relatedness, context sensitivity, dynamic activation flow, no strict hierarchical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Paper cites Collins & Loftus' modification of hierarchical models; supports explaining semantic priming, typicality and associative effects in retrieval more naturally than strict hierarchies.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Paper does not provide direct empirical tests; challenges include how link weights are learned and how the model scales to complex conceptual structure.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Semantic priming, lexical access, associative memory, modeling context effects in retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Presented as an improvement over level-semantics-net by representing similarity and graded relations; better fits psychological phenomena like priming and typicality.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Activation spreads from cued node to semantically connected nodes; retrieval determined by activation level, modulated by link distance/strength.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Learning and normalization of link strengths, interaction with compositional structure and propositional knowledge.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5169.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5169.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Topological vision theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theory of Topological Vision / Topological Prototype Matching (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representational proposal that human conceptual/perceptual representations use topological invariants (connectivity, holes, branches) as core functional elements; prototypes and components are formalized as elements in discrete topological spaces which are matched via homeomorphic (topology-preserving) mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Topological prototype/component model (topological vision)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Concepts and perceptual patterns are represented functionally by topological information (global invariants such as connectivity, holes, branches); recognition proceeds by mapping input topological spaces to prototype topological spaces via homeomorphisms (topology-preserving transformations), enabling size/shape invariance and compositional assembly from prototype elements.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>topological representation (discrete topological spaces over prototypes/components) combining prototype and compositional formats</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>emphasis on topology (invariants under deformation), invariance to metric changes (size/shape), compositionality (objects built from prototype elements), compactness/compact topological prototype space, homeomorphic mapping between prototypes and perceptual components, supports generalization across metric variations.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Paper provides formal theoretical proofs and illustrative diagrams arguing that perceptual constancy (size/shape constancy) can be explained as topological invariance; applies the framework to Chinese character composition and to machine pattern-recognition architectures as an implementable model.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>No direct behavioral or neurophysiological experiments presented in this paper; open empirical questions include whether human conceptual representations are indeed organized primarily by topological invariants, and how topology-based representations handle occlusion, noisy inputs, or topology-changing transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Visual perception (size and shape constancy), object recognition, Chinese character formation/recognition, machine pattern recognition architectures emphasizing invariance and compositionality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Paper positions this model as integrating prototype, feature, and component accounts: prototypes are abstracted components whose matching is accomplished via topological transformation; offers a middle ground addressing limitations of pure prototype (lack of bottom-up) and pure feature/template models.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Extract topological information from input (connectivity, holes, branches) → represent input as discrete topological space → find homeomorphic mapping to prototype topological spaces (surjective mapping from components to prototypes) → compose matched components according to structural rules to yield object recognition/assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Empirical validation needed (behavioral, computational, neuroimaging); specification of perceptual algorithms for robust topological extraction under real-world noise; integration with graded similarity metrics and learning mechanisms for prototype extraction.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5169.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5169.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Atkinson-Shiffrin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Atkinson & Shiffrin Multiple-Store Memory Model (1968)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classic memory model that divides human memory into sensory register, short-term store, and long-term store, each with distinct capacities, durations, and transfer operations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Atkinson & Shiffrin multiple-store memory model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Proposes a staged memory architecture: sensory input enters a sensory store, moves to a limited-capacity short-term store via attention, and with rehearsal or encoding is transferred to a large-capacity long-term store; long-term store holds prototypes/features/knowledge used in recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>functional memory stores (short-term / working store vs long-term repositories of representations like prototypes/features)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>distinct temporal stages, capacity/duration distinctions, rehearsal-driven consolidation to long-term store, long-term store as repository of prototypes/features/rules.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Paper uses this model as the basis for its cognitive architecture for pattern recognition, mapping short-term processing (feature extraction, prototype matching) and long-term prototype/knowledge databases; cites the model's historical centrality in cognitive psychology.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>The model is descriptive and has been refined by later multi-systems memory accounts; paper does not present new empirical tests but applies the architecture to machine recognition design.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Memory research, modeling of encoding and recognition processes, informing system architecture in machine pattern recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Paper contrasts multi-store account with multi-system memory views but uses Atkinson-Shiffrin as a practical framework to separate short-term analysis from long-term prototype databases.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Sensory registration → feature/prototype extraction into a working short-term store → retrieval/matching against long-term prototype/knowledge databases → update/learning into long-term stores when new items encountered.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How prototypes/features are learned and organized within long-term store; interaction of working memory with distributed/connectionist representations; granularity of what is stored in long-term memory.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Recognition-by-Components: A Theory of Human Image Understanding <em>(Rating: 2)</em></li>
                <li>A Spreading-Activation Theory of Semantic Processing <em>(Rating: 2)</em></li>
                <li>Retrieval Time from Semantic Memory <em>(Rating: 2)</em></li>
                <li>Human Memory: A Proposed System and Its Control Processes <em>(Rating: 2)</em></li>
                <li>The theory of topological vision <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5169",
    "paper_id": "paper-15876077",
    "extraction_schema_id": "extraction-schema-109",
    "extracted_data": [
        {
            "name_short": "Template theory",
            "name_full": "Theory of Template Matching",
            "brief_description": "A classical model that posits stored literal copies (templates) of previously encountered patterns in memory which incoming stimuli are matched against one-to-one for recognition.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Template theory (template matching)",
            "theory_or_model_description": "Proposes that conceptual/perceptual recognition is achieved by matching incoming sensory input to stored, discrete exemplar-like templates that correspond one-to-one to recognizable patterns.",
            "representation_format_type": "literal exemplar / template (symbolic/categorical copies)",
            "key_properties": "one-to-one mapping between stored template and input; high specificity; requires extensive storage of variants; low abstraction and compositionality.",
            "empirical_support": "Paper cites template theory historically and notes engineering examples (e.g., machine recognition of seals on paychecks) where exact-match templates work adequately; general appeal in early pattern-recognition systems.",
            "empirical_challenges": "Requires storing many templates (memory burden), poor flexibility/generalization to novel variations, limited account of bottom-up processing, criticized in the paper as insufficient to explain human recognition.",
            "applied_domains_or_tasks": "Simple pattern matching tasks, constrained machine-recognition systems, OCR under fixed fonts, detection tasks requiring high fidelity matches.",
            "comparison_to_other_models": "Contrasted with prototype and feature models; less flexible than prototype theory and less parsimonious than feature-based/compositional accounts; paper positions it as a limited component of cognition rather than a full account.",
            "functional_mechanisms": "Recognition = retrieve best-matching stored template; matching process is essentially similarity or exact pattern comparison, sometimes preceded by preprocessing to allow tolerant matching.",
            "limitations_or_open_questions": "How humans avoid combinatorial explosion of templates; integration with compositional and bottom-up mechanisms; empirical boundary conditions where templates suffice versus fail.",
            "uuid": "e5169.0"
        },
        {
            "name_short": "Prototype theory",
            "name_full": "Theory of Prototype Matching",
            "brief_description": "A model asserting that categories are represented by abstract prototypes (central tendencies) rather than by exhaustive exemplars; recognition relies on similarity to these prototypes.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Prototype theory / prototype matching",
            "theory_or_model_description": "Conceptual knowledge is stored as abstracted, generalized prototypes (summary representations of a category) and recognition/classification is achieved by comparing inputs to prototypes and assigning membership according to best-match.",
            "representation_format_type": "abstract prototype (summary/centroid representation), compatible with symbolic/feature-like formats",
            "key_properties": "abstraction from instances, compactness (fewer stored items), graded category membership, flexibility to novel exemplars, less memory load than templates; in this paper prototypes are formalized topologically.",
            "empirical_support": "Paper cites prototype theory as a major model in cognitive psychology and uses it functionally in its framework (e.g., prototypes as basis for component matching and category assignment); applied examples include character and shape recognition and Chinese character formation.",
            "empirical_challenges": "Paper notes prototype accounts emphasize top-down processing and can underdescribe bottom-up perceptual processes; lack of mechanistic detail on how prototypes are formed and how graded similarity is computed; no direct empirical experiments reported in this paper.",
            "applied_domains_or_tasks": "Categorization, object recognition, concept learning, machine pattern recognition (as compact library), Chinese character composition in the paper's application.",
            "comparison_to_other_models": "Presented as more flexible than template theory and more abstract than feature analysis; compared with feature and component theories (paper argues prototypes can be understood as abstracted components and can be mapped topologically onto concrete parts).",
            "functional_mechanisms": "Recognition via similarity matching between input components (after topological transformation) and elements of a compact prototype set; nearest/probable-match decision rules (sometimes combined with thresholding or best-of-qualified matches).",
            "limitations_or_open_questions": "How prototypes incorporate compositional structure and relations among parts; empirical tests of the paper's topological formalization; integration with bottom-up feature extraction and with graded similarity metrics.",
            "uuid": "e5169.1"
        },
        {
            "name_short": "Feature theory",
            "name_full": "Theory of Feature Analysis (Feature theory)",
            "brief_description": "A bottom-up model where objects and concepts are represented as sets of diagnostic features; recognition arises from matching detected features to stored feature representations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Feature theory (feature analysis)",
            "theory_or_model_description": "Concepts are represented functionally as structured collections of features (local detectors); classification is achieved by detecting and comparing feature sets rather than whole-pattern templates or global prototypes.",
            "representation_format_type": "feature-based (sparse / symbolic feature sets; can be localist or vector-like)",
            "key_properties": "bottom-up driven, compositional across features, enables generalization via shared features, efficient for many machine methods, but (as paper notes) may lack top-down/contextual modulation.",
            "empirical_support": "Widely used in computer vision and engineering; paper acknowledges broad application and utility of feature analysis in machine pattern recognition.",
            "empirical_challenges": "Lacks account of top-down influences and holistic/contextual effects in human perception; may fail when relational or global/topological properties determine recognition.",
            "applied_domains_or_tasks": "Low-level perception, computer vision feature extraction, OCR, speech recognition feature vectors, classification tasks where discriminative features are known.",
            "comparison_to_other_models": "Framed as complementary to prototype and template models; more bottom-up and data-driven than prototype/template approaches; paper argues overall that these descriptions are alternative ways of describing the same object for engineering.",
            "functional_mechanisms": "Detect features from input, assemble feature set, compare to stored feature descriptions, apply decision rules (e.g., Bayesian, SVM) for classification.",
            "limitations_or_open_questions": "How feature sets are organized into higher-level conceptual structure; how top-down context and prototypes influence low-level feature selection in cognition.",
            "uuid": "e5169.2"
        },
        {
            "name_short": "RBC",
            "name_full": "Recognition-by-Components (Biederman)",
            "brief_description": "A componential view of object representation: objects are represented as arrangements of a limited set of volumetric primitives ('geons') and recognized by parsing into these parts and their spatial relations.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Recognition-by-Components (RBC)",
            "theory_or_model_description": "Conceptual/perceptual object representations are functional compositional structures built from a small finite vocabulary of geometric primitives (geons) plus relational structure; recognition proceeds by edge extraction, segmentation into components, and matching component structure to stored structural descriptions.",
            "representation_format_type": "compositional structural representation (symbolic parts + relations)",
            "key_properties": "compositionality (parts + relations), robustness to viewpoint/metric variation because parts are defined by invariant edge/concavity properties, parsimonious primitive inventory, explicit structural description.",
            "empirical_support": "Paper cites Biederman (1987,1990) and adopts the idea that objects can be decomposed into a small set of components; uses this to motivate compositional and structural accounts of pattern composition and prototype-component relations.",
            "empirical_challenges": "The paper notes questions about sufficiency of limited component set for all objects; RBC primarily addresses shape-based vision and may not account for texture/material or highly configural stimuli; paper does not report new empirical tests.",
            "applied_domains_or_tasks": "Object recognition, shape perception, hierarchical decomposition in computer vision, modeling structural generalization and viewpoint invariance.",
            "comparison_to_other_models": "Contrasted with prototype and template theories: RBC provides explicit compositional mechanism lacking in prototype abstractions; more structural and bottom-up than prototype matching but compatible with prototype abstraction of recurring components.",
            "functional_mechanisms": "Edge extraction → segmentation into components → identification of component types (geons) → recovery of relational structure → match to stored structural descriptions.",
            "limitations_or_open_questions": "Empirical validation across diverse object classes; integration with feature-based and prototype-based memory stores; account of semantic/functional knowledge beyond shape.",
            "uuid": "e5169.3"
        },
        {
            "name_short": "Symbol-net",
            "name_full": "Symbol-net model",
            "brief_description": "A symbolic network model where concepts are nodes linked by directed relations, representing hierarchical and logical organization of knowledge used in search-like retrieval.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Symbol-net (symbolic network) model",
            "theory_or_model_description": "Conceptual knowledge is represented as discrete symbols (nodes) connected by labeled (often directional) links; cognition is modelled as traversals/searches through these symbolic networks.",
            "representation_format_type": "symbolic, localist network of nodes and directed links",
            "key_properties": "explicit symbolic relations, hierarchical/up-down levels, rule-like logical organization, retrieval by directed search, transparent interpretability.",
            "empirical_support": "Paper describes symbol-net as one of three hypothetical knowledge-organization models and provides a schematic account of how it would represent conceptual relations; used as background for discussing knowledge organization.",
            "empirical_challenges": "Paper criticizes purely logical symbol-net approaches for relying on logic rather than psychological data and for potential inflexibility; does not report empirical tests.",
            "applied_domains_or_tasks": "Semantic memory modeling, symbolic AI, language understanding, classical cognitive architectures.",
            "comparison_to_other_models": "Compared to level-semantics and activation-diffusion models; symbol-net is more logic-driven and less psychologically plausible than spreading-activation or graded similarity networks.",
            "functional_mechanisms": "Search along directed links between nodes according to goal-driven criteria until an appropriate concept is found or retrieval fails.",
            "limitations_or_open_questions": "How symbolic networks account for graded similarity, context effects, and probabilistic inference; scalability and learning of link weights from experience.",
            "uuid": "e5169.4"
        },
        {
            "name_short": "Level-semantics-net",
            "name_full": "Level-semantics-net model (Collins & Quillian)",
            "brief_description": "A hierarchical semantic network in which concepts are organized in levels with 'is-a' and 'has' relations, supporting economy of storage and hierarchical retrieval.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Level-semantics-net (hierarchical semantic network)",
            "theory_or_model_description": "Concepts are nodes arranged in a graded hierarchy; a concept's meaning is determined by its place in the hierarchy and its attribute ('has') links; retrieval uses hierarchical traversal yielding cognitive economy.",
            "representation_format_type": "symbolic hierarchical network",
            "key_properties": "hierarchical categorization, inheritance of properties, storage economy via shared higher-level nodes, deterministic logical relations ('is-a', 'has').",
            "empirical_support": "Paper presents level-semantics-net as a historical model of semantic organization and notes its computational simplicity and influence on language-based modeling; used as an implementable model for computer simulations in the paper's framework.",
            "empirical_challenges": "Paper points out that the level-semantics model explains organization by logic rather than psychological data; it fails to capture graded similarity and context-driven relations observed empirically.",
            "applied_domains_or_tasks": "Early semantic memory modeling, taxonomy-based classification, language understanding systems.",
            "comparison_to_other_models": "Paper contrasts it with activation-diffusion (spreading activation) which relaxes strict hierarchical constraints and accommodates graded similarity; level-semantics is more rigid but easier to implement.",
            "functional_mechanisms": "Retrieve concept by moving up/down hierarchy along 'is-a' or 'has' links, deriving properties by inheritance from higher-level nodes.",
            "limitations_or_open_questions": "How to handle cross-cutting categories, typicality effects, and similarity-based association not aligned with strict hierarchies.",
            "uuid": "e5169.5"
        },
        {
            "name_short": "Activation-diffusion",
            "name_full": "Activation-diffusion model (Collins & Loftus spreading-activation)",
            "brief_description": "A semantic network model where activation spreads across links graded by relatedness, representing concepts as nodes in a similarity-weighted network rather than strict levels.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Activation-diffusion / spreading activation model",
            "theory_or_model_description": "Concepts are nodes in a network linked by degrees of semantic relatedness; activation of one node diffuses to neighbors proportionally to link strength, producing graded, context-sensitive retrieval and priming effects.",
            "representation_format_type": "graded network / spreading activation (distributed localist)",
            "key_properties": "similarity-based links, graded relatedness, context sensitivity, dynamic activation flow, no strict hierarchical constraints.",
            "empirical_support": "Paper cites Collins & Loftus' modification of hierarchical models; supports explaining semantic priming, typicality and associative effects in retrieval more naturally than strict hierarchies.",
            "empirical_challenges": "Paper does not provide direct empirical tests; challenges include how link weights are learned and how the model scales to complex conceptual structure.",
            "applied_domains_or_tasks": "Semantic priming, lexical access, associative memory, modeling context effects in retrieval.",
            "comparison_to_other_models": "Presented as an improvement over level-semantics-net by representing similarity and graded relations; better fits psychological phenomena like priming and typicality.",
            "functional_mechanisms": "Activation spreads from cued node to semantically connected nodes; retrieval determined by activation level, modulated by link distance/strength.",
            "limitations_or_open_questions": "Learning and normalization of link strengths, interaction with compositional structure and propositional knowledge.",
            "uuid": "e5169.6"
        },
        {
            "name_short": "Topological vision theory",
            "name_full": "Theory of Topological Vision / Topological Prototype Matching (this paper)",
            "brief_description": "A representational proposal that human conceptual/perceptual representations use topological invariants (connectivity, holes, branches) as core functional elements; prototypes and components are formalized as elements in discrete topological spaces which are matched via homeomorphic (topology-preserving) mappings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_or_model_name": "Topological prototype/component model (topological vision)",
            "theory_or_model_description": "Concepts and perceptual patterns are represented functionally by topological information (global invariants such as connectivity, holes, branches); recognition proceeds by mapping input topological spaces to prototype topological spaces via homeomorphisms (topology-preserving transformations), enabling size/shape invariance and compositional assembly from prototype elements.",
            "representation_format_type": "topological representation (discrete topological spaces over prototypes/components) combining prototype and compositional formats",
            "key_properties": "emphasis on topology (invariants under deformation), invariance to metric changes (size/shape), compositionality (objects built from prototype elements), compactness/compact topological prototype space, homeomorphic mapping between prototypes and perceptual components, supports generalization across metric variations.",
            "empirical_support": "Paper provides formal theoretical proofs and illustrative diagrams arguing that perceptual constancy (size/shape constancy) can be explained as topological invariance; applies the framework to Chinese character composition and to machine pattern-recognition architectures as an implementable model.",
            "empirical_challenges": "No direct behavioral or neurophysiological experiments presented in this paper; open empirical questions include whether human conceptual representations are indeed organized primarily by topological invariants, and how topology-based representations handle occlusion, noisy inputs, or topology-changing transformations.",
            "applied_domains_or_tasks": "Visual perception (size and shape constancy), object recognition, Chinese character formation/recognition, machine pattern recognition architectures emphasizing invariance and compositionality.",
            "comparison_to_other_models": "Paper positions this model as integrating prototype, feature, and component accounts: prototypes are abstracted components whose matching is accomplished via topological transformation; offers a middle ground addressing limitations of pure prototype (lack of bottom-up) and pure feature/template models.",
            "functional_mechanisms": "Extract topological information from input (connectivity, holes, branches) → represent input as discrete topological space → find homeomorphic mapping to prototype topological spaces (surjective mapping from components to prototypes) → compose matched components according to structural rules to yield object recognition/assignment.",
            "limitations_or_open_questions": "Empirical validation needed (behavioral, computational, neuroimaging); specification of perceptual algorithms for robust topological extraction under real-world noise; integration with graded similarity metrics and learning mechanisms for prototype extraction.",
            "uuid": "e5169.7"
        },
        {
            "name_short": "Atkinson-Shiffrin",
            "name_full": "Atkinson & Shiffrin Multiple-Store Memory Model (1968)",
            "brief_description": "A classic memory model that divides human memory into sensory register, short-term store, and long-term store, each with distinct capacities, durations, and transfer operations.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Atkinson & Shiffrin multiple-store memory model",
            "theory_or_model_description": "Proposes a staged memory architecture: sensory input enters a sensory store, moves to a limited-capacity short-term store via attention, and with rehearsal or encoding is transferred to a large-capacity long-term store; long-term store holds prototypes/features/knowledge used in recognition.",
            "representation_format_type": "functional memory stores (short-term / working store vs long-term repositories of representations like prototypes/features)",
            "key_properties": "distinct temporal stages, capacity/duration distinctions, rehearsal-driven consolidation to long-term store, long-term store as repository of prototypes/features/rules.",
            "empirical_support": "Paper uses this model as the basis for its cognitive architecture for pattern recognition, mapping short-term processing (feature extraction, prototype matching) and long-term prototype/knowledge databases; cites the model's historical centrality in cognitive psychology.",
            "empirical_challenges": "The model is descriptive and has been refined by later multi-systems memory accounts; paper does not present new empirical tests but applies the architecture to machine recognition design.",
            "applied_domains_or_tasks": "Memory research, modeling of encoding and recognition processes, informing system architecture in machine pattern recognition.",
            "comparison_to_other_models": "Paper contrasts multi-store account with multi-system memory views but uses Atkinson-Shiffrin as a practical framework to separate short-term analysis from long-term prototype databases.",
            "functional_mechanisms": "Sensory registration → feature/prototype extraction into a working short-term store → retrieval/matching against long-term prototype/knowledge databases → update/learning into long-term stores when new items encountered.",
            "limitations_or_open_questions": "How prototypes/features are learned and organized within long-term store; interaction of working memory with distributed/connectionist representations; granularity of what is stored in long-term memory.",
            "uuid": "e5169.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Recognition-by-Components: A Theory of Human Image Understanding",
            "rating": 2,
            "sanitized_title": "recognitionbycomponents_a_theory_of_human_image_understanding"
        },
        {
            "paper_title": "A Spreading-Activation Theory of Semantic Processing",
            "rating": 2,
            "sanitized_title": "a_spreadingactivation_theory_of_semantic_processing"
        },
        {
            "paper_title": "Retrieval Time from Semantic Memory",
            "rating": 2,
            "sanitized_title": "retrieval_time_from_semantic_memory"
        },
        {
            "paper_title": "Human Memory: A Proposed System and Its Control Processes",
            "rating": 2,
            "sanitized_title": "human_memory_a_proposed_system_and_its_control_processes"
        },
        {
            "paper_title": "The theory of topological vision",
            "rating": 2,
            "sanitized_title": "the_theory_of_topological_vision"
        }
    ],
    "cost": 0.01758525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Theory of Cognitive Pattern Recognition
November 2008,</p>
<p>Youguo Pi 
Wenzhi Liao 
Mingyou Liu 
Jianping Lu </p>
<p>School of Automation Science and Engineering
South China University of Technology Guangzhou
GuangdongChina</p>
<p>I-Tech
ViennaAustria</p>
<p>Theory of Cognitive Pattern Recognition
November 2008,Source: Pattern Recognition Techniques, Technology and Applications, Book edited by: Peng-Yeng Yin, ISBN 978-953-7619-24-4, pp. 626, www.intechopen.com Pattern Recognition Techniques, Technology and Applications 434
Basis of cognitive psychology related to pattern recognition 1.1 Perception and its constancyBorn and developed in the middle of 1970's, cognitive science is a kind of intersectional and integrative science aiming to study both the working principle and the developing mechanism of human brain and psyche. It is a product from the processes of intersection, infiltration and aggregation of such sciences as psychology, computer science, neurology, linguistics, anthropology, philosophy, and so on. As one of the important parts of cognitive science, cognitive psychology[1-6], developed in the middle of 1950's, is a kind of psychology making the view of information processing as the core, thus also named information processing psychology, and a kind of science studying the processes of transforming, processing, storing, recovering, extracting and using information through sense. Perception has always been an important studying field of psychology. Cognitive psychology treats perception as the organization and explanation of sense information, and the process of acquiring the meanings of sense information. Correspondingly, this process is treated as a series of consecutive information processing, and the ability of the process depends on the past knowledge and experience. We can cover a building far away by just a finger, it means that the image of finger formed on the retina is bigger than that of the building. But if we move away the finger and first look at the building then the finger, we will feel the building is much bigger than the finger anyway, that indicating a very important feature of perception-constancy. The constancy of perception refers to perception keeps constant when the condition of perception changes in a certain range [7]. In the real world, various forms of energy are changed while reaching our sense organs, even the same object reaching our sense organs. Constancy in size and shape keeps our lives normal in this daedal world. Although an object sometimes seems smaller or bigger, we can recognize it. Constancy is the basis of stable perception of human to the outside. For instance, students can always recognize their own schoolbag, no matter far away (assuming it is visible) or close, overlooking or upward viewing, or looking in the front or sides. Although the images formed in the retina under the different conditions mentioned above are different from each other student's perceptions of this object are the same schoolbag. Constancy in size and shape are two main types of the perception constancy. Perception constancy in size means that although the size of object images shot on the retina change, human perception of the size of object keeps constant. The size of image on the human retina directly depends on the distance between the object and our eyes.</p>
<p>Fig. 1. The constancy of the perception</p>
<p>The shape constancy of perception means that in perception, although the shape of the object image shot on retina changes, people's perception of the shape of object stays constant. The shape of image on human retina directly depends on the angle of view between the object and eyes. As shown in figure 1, when the object is projected in the normal direction of plane A, we can only see plane A without the whole shape of this object. When we move the direction of view along the positive way of x and z axis, and can see plane A, B, C, and D. No matter what the size and proportion of these four planes, we can still recognize the object. This is shape constancy of perception. We now can interpret it as follows: when image information is enough to recognize the pattern, the changes of image shape don't affect human perception of the object.</p>
<p>Pattern recognition</p>
<p>Pattern recognition is one of the fundamental core problems in the field of cognitive psychology. Pattern recognition is the fundamental human cognition or intelligence, which stands heavily in various human activities. Tightly linking with such psychological processes as sense, memory, study, and thinking, pattern recognition is one of important windows through which we can get a perspective view on human psychological activities. Human pattern recognition can be considered as a typical perception process which depends on knowledge and experience people already have. Generally, pattern recognition refers to a process of inputting stimulating (pattern) information and matching with the www.intechopen.com Theory of Cognitive Pattern Recognition 435 information in long-term memory, then recognizing the category which the stimulation belongs to. Therefore, pattern recognition depends on people's knowledge and experience. Without involving individual's knowledge and experience, people cannot understand the meanings of the stimulating information pattern inputted, then neither possible to recognize the patterns, which means to recognize the objects. The process which a person distinguishes a pattern he percepts with others and identifies what it is means pattern recognition. Current cognitive psychology has proposed such theoretical models or hypothesis as the Theory of Template (Model of Template Matching), the Theory of Prototype (Model of Prototype Matching), the Theory of Feature (Model of Feature Analysis), and so on.</p>
<p>(1) The Theory of Template As the simplest theoretical hypothesis in pattern recognition, the Theory of Template mainly considers that people store various mini copies of exterior patterns formed in the past in the long-term memory. These copies, named templates, correspond with the exterior stimulation patterns one by one. When a simulation acts on people's sense organs, the simulating information is first coded, compared and matched with pattern stored in brain, then identified as one certain pattern in brain which matches best. thus the pattern recognition effect is produced, otherwise the stimulation can not be distinguished and recognized. Because every template relates to a certain meanings and some other information, the pattern recognized then will be explained and processed in other ways. In daily life we can also find out some examples of template matching. Comparing with template, machine can recognize the seals on paychecks rapidly. Although it can explains some human pattern recognition, the Theory of Template, meanwhile, has some obvious restrictions. According to the Theory of Template, people have to store an appropriate template before recognize a pattern. Although pre-processing course is added, these templates are still numerous, not only bringing heavy burden to memory but also leading pattern recognition less flexible and stiffer. The Theory of Template doesn't entirely explain the process of human pattern recognition, but the template and template matching cannot be entirely denied. As one aspect or link in the process of human pattern recognition, the template still works anyway. In some other models of pattern recognition, some mechanisms which are similar to template matching will also come out.</p>
<p>(2) The Theory of Prototype The Theory of Prototype, also named the Theory of Prototype Matching, has the outstanding characteristic that memory is not storing templates which matches one-by-one with outside patterns but prototypes. The prototype, rather than an inside copy of a certain pattern, is considered as inside attribute of one kind of objects, which means abstractive characteristics of all individuals in one certain type or category. This theory reveals basic features of one type of objects. For instances, people know various kinds of airplanes, but a long cylinder with two wings can be the prototype of airplane. Therefore, according to the Theory of Prototype, in the process of pattern recognition, outside simulation only needs to be compared with the prototype, and the sense to objects comes from the matching between input information and prototype [5]. Once outside simulating information matches best with a certain prototype in brain, the information can be ranged in the category of that prototype and recognized. In a certain extent the template matching is covered in the Theory of Prototype, which appears more flexible and more elastic. However, this model also has some drawbacks, only having up-down processing but no bottom-up processing, which is sometimes more important for the prototype matching in human perceptional process. Biederman (1987,1990) proposed the theory of Recognition-By-Components, whose core assumption is that, object is constituted by some basic shapes or components, or say geometries which includes block, cylinder, sphere, arc, and wedge. Although the number of components seems not enough for us to recognize all objects, these geometries can be used to describe efficiently, for the various spatial relations of all geometries can constitute countless assembles. The Step one of Biederman's Recognition-By-Components process is extracting edges, and the Step two divides a visible object into some segments to establish the components or geometries constituting the object. The other key is that the edge information has invariant properties, based on which the components and geometries of the visible object are established.</p>
<p>(3) The Theory of Feature The Theory of Feature is other theory explaining pattern perception and shape perception. According to this theory, people try to match the features of pattern with those stored in memory, rather than the entire pattern with template or prototype. This model is the most attractive one currently, the Model of Feature Analysis has been applied widely in computer pattern recognition. However, it is just a bottom-up processing model, lacking up-down processing. Therefore, it still has some drawbacks.</p>
<p>Memory</p>
<p>First、The Description of memory Memory is a reflection of the past experience in human brain, and, in cognitive psychology, a process of information coding, storing, and extracting in a certain condition in future. Having a big effect on human history and individual person development, memory is a gift from the nature to individual life, and also a power with which individual keeps and uses the achieved stimulating information, knowledge and experience As a necessary condition of the intellect development, memory is the root of all intelligence. People keep past experience into their brain by memory, and then, based on experience recovering, have thinking and imagination, whose results are kept again in brain as the basis of further thinking and imagining. Memory, in cognitive psychology, can be seen as a process of information inputting, coding, storing, and extracting, therefore, it can be separated as instantaneous memory, short-term memory, and long-term memory according to the time of storage. Recent years, more and more researchers propose to view memory as multiple memory form with different property functions formed with various forms, systems or types (Schacter 1985). Second、The model of memory system In 1960's, relying on the deep research of short-term and long-term memory, researchers on cognitive psychology gradually proposed and built some memorial theory and related memorial models. Among them, the Multiple Mnemonic Model proposed by Atkinson and Shiffrin in 1968 is the most attractive one, as shown in figure 2. In this model, memory is described by 3 kinds of memory storages: sensory store, limited number and very short time for the information keeping; short-term store, longer time of storage but still limited number to keep; long-term store, powerful power of storage, and able to keep for a long time, or maybe even forever. However, recently cognitive psychologists usually describe these 3 kinds of storages as sensory memory, short-term memory, and long-term memory. In this model, outside information first input into sensory registration, which has various kinds of information but probably disappears very soon. Then the information will be transferred into short-term memory, in which the information is organized by hearing, language or spoken language acknowledgement, and is stored longer than that in sensory storage. If processed meticulously, repeated, and transferring acknowledged, the information will be input into long-term memory, or else will decline or disappear.  </p>
<p>The expression and organization of knowledge</p>
<p>Human has transcendental imagination. If imagination is produced by experience and knowledge, then human's knowledge must be organized by a certain way. Cognitive psychology describes inside knowledge attribution of individual through establishing cognitive model, which has 3 hypothetical models. First、Hypothesis of symbol-net model This model can comparatively indicate how every part of knowledge in human brain arrays and interacts with each other in a certain connecting mode.</p>
<p>In symbol-net model, conceptions are usually described as "node", which links each other with a arrowed line, and therefore the two concepts are connected by a certain mode. In symbol-net model, we describe this relation with "up and down level", adding with arrowed line. What needs to be attended is the arrow direction, which has some theoretical meanings in symbol-net model, as figure 3 showing. The fundamental assumption of symbol-net model is a reflection of people's knowledge organization, which is similar to searching among the network nodes. The search is performed one node by another along the direction of the arrows according to the form of cognitive process series, until reach the nearest node and search out the knowledge. If the knowledge in the nearest node can answer the certain question, the search will cease, otherwise the search will continue till finding out answer or giving up.</p>
<p>Ostrich Bird</p>
<p>Is a kind of Second、Level-semantics-net model The Level-semantics-net Model, proposed by Collins and Quillian, is a net connecting with various elements, the node represents a concept and the arrowed line reflects the affiliation of concepts. This model indicates that every concept or node has two relationships, one is that every concept is subject to other concepts, which deciding the type of knowledge attribution and describing the affiliation with "is a kind of" relation; the other is that every concept has one or more characteristics, meaning the "have" relation of concept, as figure 4 showing.  According to this model, the organized knowledge attribution is a level dendriform view, in which lines link nodes denoting concepts of each grade, actually in a certain extent has some imagining function. In this model, because concepts form a net according to "up-and-down" grades, every concept and characteristic locates in a specific position in the network, and the meaning of a concept depends on connecting links. According to the cognitive economic principle, the Level-semantics-net model maximizes the effective storage capability while minimizes the redundancy.</p>
<p>Third、The activation-diffusion model www.intechopen.com</p>
<p>Theory of Cognitive Pattern Recognition 439</p>
<p>The core of Level-semantics-net model is the network established by the logical relations of noun concepts. This features the model clean and clear, but also causes some problems, which mainly appears that the model explains human knowledge organization and attribution assuming on logics rather than psychology. Therefore, Collins and Loftus modified the original model and proposed a new one, which is the activation-diffusion model. Giving up the level structure of the concepts,, the new model organizes concepts by the connection or similarity of semantics.</p>
<p>In activation-diffusion model, the knowledge stored in individual's knowledge structure is a big network of concepts, between which certain connection is established, namely some knowledge is contained in advance. Therefore, activation-diffusion model is also a kind of pre-storing model, as shown in figure 5. The activation-diffusion model has two assumptions related to knowledge structure: first, the line between concepts reveals their relation, the shorter the line, the tighter their relation, and the more similar their features, for instance, "car" having tight relation with "truck", rather with "teacher", second, the intension of concept of the model is decided by other related concepts, especially the tight ones, but the features of concept is unnecessary to be stored in different grades.</p>
<p>The theory of topological vision</p>
<p>Lin Chen, involving topology into visual perception study, proposed The theory of topological vision [7]. The topology study of perceptual organization is based on a core idea and composed by two aspects. The core idea is that, perceptual organization should be interpreted in the angle of transformation and its invariance perception. One aspect emphasizes the topological structure in shape perception, which means that the global characteristic of perceptual organization can be described by topological invariance. The other aspect further emphasizes the early topological characteristic perception, which means that, topological characteristic perception priors to the partial characteristic perception. The word "prior" has two rigid meanings: the entire organization decided by topological characteristics are basis of the perception of partial geometric characters, and topological characteristics perception of physical connectivity is ahead of perception of partly geometric characteristics.</p>
<p>Brief commentary of machine pattern recognition</p>
<p>Machine pattern recognition developed rapidly in the beginning of 1960's and became a new science, then has been in rapid development and successfully applied in weather forecasting, satellite aerochart explanation, industrial products measurement, character recognition, voice recognition, fingerprint recognition, medical image analysis and so on. By now Machine pattern recognition (pattern recognition for short) mainly has two basic methods: statistics pattern recognition and structure (syntax) pattern recognition. Structure pattern recognition, based on image features of structure, accomplishes pattern recognition by using dendriform information of the layered structure of pattern and subschema. Statistics pattern recognition, which has wider application, is based on the type probability density function of samples in feature space and separates pattern statistics into types, which means pattern recognition integrated with Bayesian decision in proportion statistics, is also called decision theory recognition method. In statistics pattern recognition, some knowledge and experience can decide the principle of classification, which means the rules of judgment. According to appropriate rules of judgment, we can separate the samples of feature space into different types and thus change feature space to type space. We separate feature space into type space while we classify the patterns. Statistics pattern recognition is based on the type probability density function of samples in feature space, and the rule of judgment of multiple statistics pattern recognition is Bayesian decision theory, aiming to minimize the expected risk of prior probability and lost function. Because nonlinear classification can be transferred into linear classification, the fact is searching the hyper plane of optimal decision. Although Bayesian decision rules solve the problem of engineering the optimal classifier, the implement has to be first settled with the more difficult problem of probability density distribution, thus research develops surrounding decision rules and probability density distribution. For the former, Rueda L G and Oommen B J's researches in recent years indicate that the normal distribution and other criteria functions with the covariance matrix unequal are linear and their classifiers is optimal[9]; Liu J N K, Li B N L, and Dillon T S improved Bayesian classifier with genetic algorithm when choosing input feature subset in classification problem[10]; Ferland G and Yeap T, studying the math structure of RTANN method, identified the condition of achieving optimal Bayesian classification with such method [11]. For the issue of probability density distribution, usual assuming density is a model with parameters like multiple normal distribution, while the parameters are estimated by the training sample. When the sample is not enough, the estimated error which is contained by distribution function will affect the precision of recognition. In order to improve the precision of recognition, Ujiie H et al transformed the reference data closer to normal distribution, no matter what the distribution of original data, and found the optimal transformation in theory [12]. The emergence of statistic learning and supporting vector machine bring theoretical and methodological supplement for the transformation. Core function which satisfy the Mercer condition realizes the design of the nonlinear classifier without knowing the specific form of www.intechopen.com  [14][15]and principal component analysis [16] in recent years both based on the core function are their linear widespread. One-dimensional parameter search and recursion Fisher method can get better training result than normal Fisher judgment. Using Mercer core, we can generalize these two methods into nonlinear decision plane [17]. There are also some reports of improving the function of classifier by declining pattern overlapping with fuzzy cluster analysis [18]. Therefore, there are two main problems need to be solved in pattern recognition: 1. Because of the requirement of sample amount, statistics pattern recognition cannot function well in small sample recognition. 2. so far, the pattern recognition is mainly based on the classification mechanism of the recognized objects, rather than on the perception mechanism. In "recognition", namely in the aspect of acknowledge of objects (study), there is large difference between human perception process and limited learning ability.</p>
<p>Theory of cognitive pattern recognition</p>
<p>Perceptive constancy and topological invariance</p>
<p>In the first chapter, we generally express perceptive constancy as: in the condition that the image information of the object is sufficient to determine its pattern, the geometry changing in the size and shape does not affect people's perception for the object. The above questions refer to a special kind of geometric properties of geometry, which involve the property of the geometric overall structure, named the topological property. Obviously, these topological properties are not related to such aspects of the geometry as the size, the shape and the straight or curved of lines and so on, which means that they can not be dealt with by ordinary geometric methods. Topology is to study the invariable property when geometry makes elastic deformation, the same as the perceptive constancy that changing in size and shape of the geometry do not affect people's perception for the object. Now let's make a further analysis to the topological property. As mentioned above, topology embodies the overall structure of geometric features, that any changes in shapes (such as squeezing, stretching or distorting, etc), as long as the geometry is neither torn nor adhered, will not destroy its overall structure and the topological properties remain the same. The above deformations are called the topological transformation, so the topological invariability is the property keeping the same when the geometry transforms topologically. The topological property can be accurately described by the set and mapping language. The changing of the geometry M to M' (both can be regarded as a set of topological feature points) is a one-to-one mapping (therefore the overlap phenomenon will not appear, moreover new points will not be created) :
f MM ′ → ,
where f is continuous (that means no conglutination). Generally speaking, if both f and f -1 are continuous, the one-to-one mapping f, which changes M to M', can be regarded as a topological transformation from M to M', also f and f -1 are the mapping of homeomorphism. Therefore, topological property is common in the homeomorphous geometries. The geometries of homeomorphism have no differentiation in topology because their topological properties are the same.</p>
<p>Perceptive constancy and pattern invariance</p>
<p>From above discussion, we can regard the perceptive constancy as the topological invariance. As the size constancy, changing the size of geometry is actually compressing and expanding the geometry during which the topological properties of the geometry do not change. And the shape constancy means to carry unequal proportional compression and expansion on geometries. As shown in figure 1, when we make projection on the normal direction of plane A, which creates conglutination between plane A and plane B, C, D, geometric topology has been changed, so the object can not be perceived. When the projection points move along the x-axis and z-axis, we can observe that conglutination has not been created among plane A, B, C, D, the topological structure has not been changed, so the object can be perceived. Furthermore, we will discuss perceptive constancy by using the theory of topology. First, size constancy： As mentioned above, as the distance between human eyes and the object changes the image sizes of geometry on the retina change, but in our minds we perceive the images of different sizes as an object, we call this kind of information processing size constancy. The explanation of size constancy is shown in figure 7. In the figure, as the distance between the eyes and the object changes, the images are named a, b, c and d respectively. In image a, the distance between the eyes and object is so near that the image of the object cannot be seen entirely, thus unable to be recognized. As the distance between the eyes and object becomes farther and farther, the images of the object on the retina become smaller and smaller, as shown in figure b, c and d. In the figure d, the distance to the eyes is too far and the image of the object is too small to recognize. Sequence images of the object are generated on the retina as the distance between the eyes and an object X changes. Now suppose Y is the image generated on the retina at a certain distance from eyes within the human visual range, the topological information set (such as connection, holes, nodes, branches and so on )of the image Y can be expressed as  Similarly, ( X X Ψ ) also constitutes a discrete topological space which denotes the topological space of the topological information set of the object X.
Ψ is in Y Ψ ; 3. The intersection of limited number of subcollection of Y Ψ is in Y Ψ . Therefore, Y Ψ is
People can not only see the whole object, but also a part of it, thus the elements of Y can combine into any different subset which means the whole information of the image, the whole information of a partial image, the whole information of certain feature of the image (Although the integration of features in image is not separated, the combination among the elements of Y still can be characterized) Proposition 3.2: When other conditions remain unchanged relatively, as the distance between the eyes and the object changes, the topological space of the topological information set of the image (Y Y Ψ ) and (X X Ψ ) which is the topological space of the topological information set of the object X have the same homeomorphism.</p>
<p>Proof: Y is the image of the object X generated on the retina, the topological space (X,
X Ψ ) of the object X is the direct foundation of the topological space of the image Y. Y, Y Ψ is the result of X, X Ψ
converted by human visual perception system. When Other conditions remains relatively unchanged and in the range of human visual perception, as the distance between the eyes and object changes, the image Y is just the compression or expansion of the object X, but the topology of it has not changed. As shown in Fig 6, for any element (that is topological properties, such as connectivity, the number of "hole" and so on)in the image Y there is a unique corresponding element in the object X. For example the elements y 1 and y m of the image Y respectively corresponds to the elements x 1 and x m of the object X.</p>
<p>Therefore, there exists a bijective correspondence between the object X and the image Y f: X →Y The mapping direction indicates the relationship between the reason and the result. Whereas, there also exists a bijective correspondence from the topological information set of the image Y to the topological information set of the object X
f -1 : Y →X
Moreover, f and f -1 are continuous. According to the definition of the homeomorphism in topology, f is the mapping of homeomorphism. As a result, the topological space
Y Y Ψ of the image Y and (X X Ψ )
which is the topological space of the object X have the same homeomorphism. So the proposition is proved and established.  Figure 7(c)), as the distance between the eyes and the object changes, different topological spaces Y 2 Y 3 … Y i …, are generated on the retina, Y n is the topological space of the biggest image (as shown in Fig 7(b)). From proposition 3.2, when other conditions remain relatively unchanged, as the distance between the eyes and the object changes, the topological spaces of the images have the same homeomorphism as the topological space of the object X. So
{1, , } in ∀ ∈
, Y i and X have the same homeomorphism, which means Y i ≅ X. Also because the property of homeomorphism has the transitivity, every Y i is has the same homeomorphism with each other, which means for ,{ 1 , , } ij n ∀ ∈ all satisfies Y i ≅ Y j . Thus the topological spaces Y 1 Y 2 … Y i … Y n have the same homeomorphism, indicating that the size constancy of visual perception has the property of topological invariance. In figure 7, the above are the images and below are the topological structures corresponding to its image. From the figure, (b) and (c) can be perceived as a cuboid because they have the same topology. For image (a), the size of the image is expanded seriously, only a few partial information about the object can be seen, therefore the object cannot be recognized for not www.intechopen.com Theory of Cognitive Pattern Recognition 445 enough information available; in figure (d), the image is compressed so excessively that we are not sure whether it is a cuboid, cylinder or a small piece of others. There does not exist a bijective correspondence between the topological spaces of the (a), (d) and the topological spaces of the (b), (c), therefore, they do not have the same homeomorphism as the topological spaces of the (b) and (c).</p>
<p>Fig. 7. The Size constancy</p>
<p>Thus, it is concluded that when the sizes of the images change, as long as the topology of the images has not been changed, people can perceive them as the same object, that is to say, the size constancy can perceive the objects which make the topological transformation as the same object. Second, shape constancy： As mentioned above, shape constancy is a kind of information procession which mainly as the angles between the eyes and the object change, the shapes of the images on the retina change, but in our minds we perceive different shapes of images as the same object. The explanation of the shape constancy is shown in Figure 8. Suppose Y 1 is the topological space of the distinguishable image(the image generated on the retina when the eyes are at the bottom of the object), different topological spaces Y 2 , Y 3 … Y i , …of the images are generated as the angles between the eyes and object change, Y n is the topological space of the image generated on the retina when the eyes are at the top of the object. Obviously the relationship between the topological spaces the images of Y 1 and Y i (i=2 … n) are squeeze and stretch. As the angles between the eyes and the object changes, part of the object are squeezed, while other part of the object are stretched, as shown in the figure 8. But as constancy, which means that, at the range of the human visual perception, each side and each part of the object keep the original characteristics well, without adhesion or tearing. Therefore the topological spaces Y 1 , Y 2 , ..., Y i , ..., Y n have the same homeomorphism, namely the shape constancy of the human visual perception has the property of the topological invariance, the method of the proof is just the same as that of the size constancy. In Fig 8, the above row are the images and below are the topological structures corresponding to its image. From the figure, (a), (b) and (c) can be perceived as a cuboid, because they have the same topology. (b) and (f) have the same topology different with others, we cannot perceived them as a cuboid, maybe we will perceived them as folded piece of a rectangle. We may perceive (g) as a piece papper. Fig. 8. The shape constancy Therefore, it can be concluded that when the images' shapes of the object change, as long as the topology of the images has not been changed, we can perceive them as the same object, in other words, the shape constancy can perceive the objects making topological transformation as the same object. From the above analysis, we can see that the constancy in size and shape of the visual perception can be described by topological invariance. As long as the topology of the images keeps unchanged, we can perceive the images that make topological transformation as the same object at the range of the human visual perception.</p>
<p>In conclusion, we can believe that the constancy of the human visual perception can be described by the topology invariance, that is, people can perceive the objects which are changed by topological transformation as the same object.</p>
<p>Composition of patterns</p>
<p>From the theory of the component recognition, the principle which the components constitute objects in the real world can be understood like this: there are countless type of materials in the world, but the types of the chemical elements composing the materials are just a little more than 100. Various colors in the world can be composed by the three colors of red, green and blue. All buildings, no matter how grand and splendid they are, are composed by the foundation, beams, column doors, walls, floors and the ceilings. A variety of delicious food can be cooked by just a few spices of the oil, salt, sugar, vinegar, soy sauce and so on. Music is wonderful, and countless musical works are composed by the seven notes of 1,2,3,4,5,6,7 and their changeable sound. The number of the English words is very huge, moreover in the rapid development, but they are just composed by the 52 letters including www.intechopen.com Theory of Cognitive Pattern Recognition 447 26 uppercase and 26 lowercase letters. As we know the number of figures in various aspects of the world is endless, but in the decimal system these figures are constituted by 0 to 9 altogether ten figures; in the hexadecimal 0 to 9 ten figures and A to F six letters are needed; in the binary only two symbols 0 and 1 are needed. Therefore we can take it that Biederman's theory of the components recognition reveals the pattern of the real world's construction: all the objects of the world are composed by a few components, that is to say, all the objects can be decomposed into certain components. Biederman's theory holds that the limited components have almost infinite combination, thus compose almost unlimited objects. This conforms to Chinese philosophy of "one lives two, two lives three, and three lives everything". In terms of the geometry, the objects can be fully described by the geometries, because various spatial relationships among the geometries have infinite combination. The same components can form different objects through different combination. The English words "on" and "no", "dear" and "dare", "hear" and "hare" respectively have the same letters, but when combination is different, the words composed are completely different. We call such a combination of the objects structures, which is a very important concept in pattern recognition. The definition of the structure is the organization of each part of the object [19]. In pattern recognition, the structure is the combinational relations between the object and its' components, and the combination of their components. For example, the English word "structure" is composed by s, t, r, u, c, e six letters, the order of the arrangement is s-t-r-u-ct-u-r-e. In organic chemistry, Methane (CH 4 ) is composed by the two elements of Carbon(C) and Hydrogen(H), and the structure of the CH 4 is a regular tetrahedron, that the carbon atom is in the center of the tetrahedron, while four hydrogen atoms are in the vertices of the tetrahedron, as shown in  The structure of objects, has its own laws, which cannot be constructed arbitrarily. In English, the letters have certain regulations of arrangement to construct the words, for example we cannot arrange w, v, u and some other letters together repeatedly. In appearance of the human face, there are not more than two eyes, only one nose and only one www.intechopen.com Pattern Recognition Techniques, Technology and Applications 448 mouth, moreover the distribution of them is that the eyes are at the top of the face, the nose in the middle, and the mouth at the bottom. Now we use the theory of set to describe the construction of the object: Definition 3.1 The objects which have the same characteristics in some aspects compose a set, denoted as O. Definition 3.2 The elements in the object set O are called object or pattern, denoted as m, that is m∈O. Definition 3.3 For any m∈O, we can decompose it to one or several components according to a certain regulations, then the components form a set which is recorded as C i (i is the natural number) Definition 3.4 For any m∈O,we call the mutual relationship of the components structure which can be denoted as S or S C 1 ,C 2 ,…,C n .</p>
<p>For example, with a certain structure the word "study" is composed by the letters "s, t, u, d, y", which can be denoted as follow:</p>
<p>Study=S(s,t,u,d,y).</p>
<p>All patterns in the object set can be decomposed into a component or more according to a certain structure, that is, all the objects in the set can be composed by the several components according to a certain structure.</p>
<p>The relationship between the prototype and the components of objects</p>
<p>The theory of prototype believes that the storage in the long-term memory is the prototypes, rather than the templates corresponding to the external patterns. What is the prototype after all? Prototype is the internal characteristic of a class of objects and also the general characteristic of all individuals of a class of objects, but not the internal copy of a specific model. It is considered by component recognition that objects are composed by a number of basic shapes or components, that is to say, are composed by the geometries. From the above two theories, it appears that there is a difference in the content of their researches: the theory of the prototype matching is to study the human brain for perceiving the outside world, while the theory of the component recognition is to study the composition of the objects. However, the two theories are to study human's pattern recognition, is there any relationship between them? Prototype, the general characteristic of all individuals in a category or area, reflects a class of the objects' basic characteristics, is the generalization and abstraction of the object's characteristics. As analyzed in the preceding chapter, an object is constituted by some elements that are concrete and determined under its structure. Therefore, there exists a process from general to determine and from abstract to concrete between the prototypes and the components.</p>
<p>We might understand prototype like this: for an object set of English word, and for any word in the object set, all of them are constituted by one or more of 52 symbols which has 26 uppercase and 26 lowercase letters. With 26 uppercase and 26 lowercase letters, the 52 symbols are the generalization and abstraction of the components of all the English words. No matter how many English words and various fonts there are, they can be constituted by some letters changing in size and shape. Therefore, English words' uppercase and lowercase letters are the prototypes of this object of English words. The sizes and shapes of prototypes are fixed, the matches between them and the components with various size and shape can be realized by topological mapping. The deduction of the concept is shown in Figure 10. In Fig 10, the prototype set P has 52 elements including 26 uppercase and 26 lowercase letters, while all the English words compose the set of the objects. The elements in the set of the objects are specific English words "Pattern Recognition PATTERN RECOGNITION", which are composed by elements of the prototypes "R, P, A, T, E, N, C, O, I, G, e, c, o, g, n, I, t". The elements' size and font in the prototype set are fixed, the prototypes can match each kind size and font of the components through the topological transformation.   , a, t, t, e, r, n}, {R, e, c, o, g, n, i, t, i, o, n} and R are only size and shape, there is no difference on topology among these three elements, so these three elements can be abstracted as a prototype R. Therefore, the prototype set P can be abstracted from the component collection C, that is the prototype set P = (R, P, A, T, E, N, C, O, I, G, e, c, o, g, n, i, t).</p>
<p>www.intechopen.com 
ii i i m f ff f i=1,2,…,r , which make i f ( 12 ,,, n CC C )= p i , f ij ( c ij )=p i ,
which is a homeomorphism, therefore
() P fC = .
thus f is surjective.</p>
<p>For example in English words, if M={Pattern Recognition PATTERN RECOGNITION }, then M can be decomposed into components C 1 ={P, a, t, t, e, r, n}, C 2 ={R, e, c, o, g, n, i, t, i, o, n}, C 3 
1 f ( R，R，R)=R=p 1 。 11 f ( R)=R= p 1 ， 12 f ( R)=R= p 1 ， 13 f ( R)=R=p 1 ， 2 f (P，P)=P= p 2 。 21 f ( P)=P= p 2 ， 22 f ( P)=P= p 2 ， ... 12 (, ,, ) ii i i m f ff f = p i ...
Where each mapping ij f is a homeomorphism, so 12 (, , , )
r f ff f which can makes :P fC → is surjective.
With topological transformation, any object m in the set M can be constituted by certain elements in the prototype set P, that is, the elements in the prototype set P firstly change into the elements of the components throughout topological transformation, and then compose the objects according to a certain structure. Therefore the mathematical model, which expresses the course that the components compose the objects, is shown in Fig 12 and  equation (3.1), the process and the plan are contrary to Fig 11. There exists a set of the prototypes, which are abstracted from the components of all objects. Each object can be constituted by one or more elements in the prototype set through the topological transformation.</p>
<p>Matching and coverage</p>
<p>As discussed above that the power set P Γ of the prototype set P is a topology of P, according to the theory of the prototype, there exist prototype set P = {1, 2 , , } i in C ∈ ∪ ,thus P Γ is a coverage of P. It is known in the theory of prototype matching that, all objects are constituted by a limited prototypes, that is there are limited members in coverage P Γ , so P Γ is the limited coverage of P. The following are proving it in the theory of topology.</p>
<p>Proposition 3.7: The topological space P Γ of the prototype set P is compact.</p>
<p>Proof: As P P Γ is a discrete topological space, and it is known by the theory of prototype that all of the objects M in a class of object O are constituted by limited prototypes, that P Γ is limited and the discrete topological space is a compact topological space as long as its' elements are limited [21]. The proposition is proved and established. Proof: From the proposition above that C={ 12 ,,, n CC C } is a numerable basis
of P P Γ , for P P, { | } C P pC p C ′ ′ ∀∈ Γ = ∈ Γ ∈
is a neighborhood of p, which is a subcollection of C and is numerable collection, so P has a numerable neighborhood basis Proof: P P Γ is a discrete topological space, and the discrete topological space is the space which meets all the properties of the spaces stated by the separability axiom [24], which includes T1 space, T2 space (also called Hausdorff space), T3 space, T3.5 space (also called Tychonoff Space), T4 space, regular space and so on. Therefore, as a discrete topological space, the prototype topological space P P Γ ) can satisfy any axiom of separability, the proposition is proved and established. From Proposition 3.7, the prototype P is compact, so the open coverage must have limited sub-coverage in prototype P. In other words, the process of the prototype matching is to use the topological transformation of one or more elements in prototype set to cover the components of the pattern.</p>
<p>The evaluation of matching</p>
<p>The multi-value problem of matching extent between prototype and pattern's components changes into the yes-or-no problems that whether matching or not matching. There is a decision-making process and a evaluation criteria. The evaluation method of the traditional machine pattern recognition is Bayesian decision theorem, the final judgement is the threshold. The threshold is the value of domain, if higher than the threshold, it pass; otherwise it fails. In pattern recognition, the threshold is established artificially by people's hard working, the higher the threshold is, the higher the rate of rejecting identifier and the lower that of wrong identifier are, Otherwise, the lower the rate of rejecting identifier and the higher that of wrong identifier are. In fact, the method of threshold is derived from a way of information solving or data processing in statistic, rather than a law of nature. In a condition of large sample, by using the method of threshold, the workload of data processing can be greatly reduced. For example, the scores of exam which entering school by students, uses a threshold to differentiate. Matriculating the students whose scores are higher than the threshold, and rejecting the students whose scores are lower than the threshold.For instance, a product has a qualified threshold, higher that is eligible, lower is not. In fact, the students who get higher scores at one exam not actually learn well than the one who gets lower scores. While the groups of student who get higher scores learn well than the groups of students who get lower scores.</p>
<p>The people whose ages are over 60 are elders, while the others whose ages are 59 and 11 months are not elders. Even though all are 60 years old, the psychology and physiology are diverse. Therefore, the threshold is the method of handling problem but not a natural law. However, in some situations, we cannot adopt the method of threshold. For instance, the voting may fail if we set a threshold, that is, there exist some possibilities that the threshold is so high that nobody can reach it or the threshold is so low that lots of people can reach it. It seems that there is an orientation to pursue "best" around our world. Most people hope his or her room bigger at home. However, in the earthquake areas having happened in china in May 12 th 2008, the temporary movable rooms are only tens of square meters, but people also live well, even they live in the several -square -meters tents. People always hope the bed can be bigger, it is surprise that two meters beds can be brought nowadays. It is a fashionable that people sleep in big beds, so people almost want to buy the big bed. While the sleepers are only 60cm wide in the trains, which is less than the 1/3 of the big beds, people also can sleep very well. But we can observe that people on the train are changing the posture frequently when they are sleeping. Whether or not the example above can prove that people have the orientation to pursue the "best" under a certain restricted condition? There is a similar situation in the natural world. We have observed that the trees grow under the stones, the advantage of them is that they can break through the huge stone and grow up under great resistance. Groups of bamboos can grow up highly and straightly, while the single trees or bamboos which grow on the wide area are short and bended, they are Looking for a best developing direction when they are restricted by the surrounding environment, can it be explained that the plants which grow in the natural world also have the orientation to pursue the "best" under a certain restricted condition? These exists a extreme point at mathematics, which means the function can get the extreme value at this point or at the boundary points of the interval. In human pattern recognition, which is researched by cognition psychology, once the external stimulating information has the nearest matching with certain prototype in brain, it can be added into the category of this prototype and then recognized. The nearest means the best matching among them, obviously, it is not enough to satisfy one threshold. Actually, sometimes these twos are combined to use. The modern people like to obtain the projects or business contracts by bid. The process of bid can be viewed as the recognition for suppliers or contractors. The first condition in the bid process is the qualification, also call the threshold. Only the people who meet the qualification of bid can submit a bid. However, the best method is to summarize the elements of the quality, price, service etc, and then choose the winner among several qualified bidder who meet the tenderee's profit. We can take the same idea used to solve public bidding problem to deal with the problem of estimating the discriminating result in recognition. The threshold method and the extreme method are always introduced in the estimation: we choose the most nearest matching from those whom satisfy the very condition of threshold.</p>
<p>Processing method</p>
<p>The theory of prototype matching adopts up-down processing method, which is the fault of this matching theory. The work of human's pattern recognition is complex. Take the image recognition as an example, recognizing the plant, animal, all kinds of objects which are manmade, image, character even all the objects in the world are very complex. Just because the objects that people facing are so complex that we can not forecast the properties of them, so it can not prepare the corresponding matching information to build up the up-down processing method. In fact, if possible, people often use the up-down processing method or combine them when recognition, for example, when someone has heard that some relative will come to visit, they will adopt up-down processing method to recognize the coming person, of course they will make mistake by up-down process. The work of recognition is more easily for the system in machine pattern recognition, which is a special recognition system that usually aims to a class of objects. For example, character recognition aims to character, while face recognition aims to face, fingerprint recognition aims to fingerprint, and glass inspecting recognition aims to glass. The ability of machine pattern recognition is much weaker than people's. Just because we can build up a systemic knowledge by using of the special knowledge of special system to achieve or partly achieve the combination of up-down process method and bottom-up process method. For example, "0" is recognized as a number in number recognition system but recognized as a character in character recognition system. For the images of visual perception, especially for the images' topology, the topology of them is the same. And it settles the shortage of up-down processing method to a certain extent.</p>
<p>Memory</p>
<p>In machine pattern recognition, because knowledge and prototype must be memorized, the memory mechanism of cognition psychology must be used. But in machine pattern recognition, sensory memory need not be considered. Figure 13 is the memory model. In the model, the short-term memory seems to a work memory, used for memorizing the correlative knowledge extracted form the prototype database and the knowledge database in the long-term memory, which is necessary for analyzing the input data. The operation itself has characteristics of the up-down processing method, and then calculate it according to prototype pattern recognition. Add the prototype and knowledge which are needed to their corresponding database.</p>
<p>The organization of knowledge</p>
<p>In most of person's free imagination, it can be shown their knowledge is organized. As for how to organize, it is the content of cognition psychology. The above chapters have introduced that the achievement we have got in cognition psychology are three hypothetic models: symbol-net model, level-semantics-net model and activation-diffusion model. In the researching of cognitive pattern recognition, we can research the level-semantics-net model firstly, the reason is that this model is proposed according to language understanding of computer simulation. This hypothetic model explains the organization and expression of the human knowledge by logic not psychology. It is much easily realized in computers, though has a suspicion to take a shortcut, it is a good method.</p>
<p>The systemic construction of cognitive pattern recognition 4.1 The systemic construction of traditional pattern recognition</p>
<p>From the perspective of technique, pattern recognition experiences the whole course which is from the mode space to the feature space, then to the type space. So, the system of pattern recognition must have some essential functions such as pattern collection, feature extraction/selection, pattern classification and so on, the systemic framework of machine pattern recognition can be showed as the figure 14. According to the object for recognition, the process of pattern collection can choose every kind of sensors such as metrical devices, image collecting devices and some other devices used for conversion, filter, enhancement, reducing noise, correcting distortion. Feature extraction is realized by transforming mode space to feature space, which compress dimensions effectively. The classifier can classify those samples with unknown property. In order to design the classifier, we should confirm its evaluated rules and train it firstly. Then, the classifier can work effectively.</p>
<p>The framework of cognitive pattern recognition</p>
<p>In the traditional machine pattern recognition, the template approach has been used; however, the feature theory has been used widely. We believe that there is no essentially difference among the template theory, the prototype theory and the feature theory. They are all a description of the object in the application of machine recognition. We believe that the prototype is a method which describes feature, is the component templates of the object. In other words, we can believe that the feature is a method used to describe the prototype, and the template is also a describing method of the feature. These are controversies of different academic attitudes in cognitive psychology. We do not want to join these arguments, but just to meet targets of the description of requirements. In order to facilitate the handling, the following discussion will consider the feature/prototype, that is, integrating the template into the discussion of the prototype.</p>
<p>The application of the memory principle</p>
<p>Cognitive psychology believes that the process of memory can be divided into three stages, that is, feeling memory→short-term memory→long-term memory. Accordingly, in the process of machine recognition, there are only two stages, one stage can acquire information of the outside world through a sensor, another stage can store the objects recognized in the computer for a short time. The knowledge stored in long-term memory is features and the prototypes as well as knowledge and rules, then the databases can be created to store the corresponding features, prototypes and the knowledge and rules. In the traditional pattern recognition, the rejection conclusions will be obtained, when it can not be matched in the short-term memory. While in the pattern recognition, if the appropriate patterns can not be found in the databases of the feature or the prototype, then the corresponding patterns will be added to the appropriate databases. If no relations, structures, methods or other knowledge can be found in the databases of the knowledge and the rule, then they will be added to the corresponding databases, in order to simulate human's learning ability better. Cognitive psychology proposes two information processing modes of the top-down and bottom-up, while in machine pattern recognition the processing mode mainly used is the data-driven process supported with the top-down process, and the partial knowledge can be used to predict. The processing method of that global features processed before local features is adopted in the global and local aspect. Cognitive psychology suggests that in the process of pattern recognition if the background information related with the object is stored in the long-term memory, it may have an important impact on decision through the so-called superiority effect. In the process of recognition, computer must deal with the problems of the overall and partial, topology, the superiority effect and so on, and carry on effective imagination. These can be integrated into the scope of knowledge. Based on the above analysis, we can get the framework of cognitive pattern recognition, as shown in Figure 15. The process of pattern collection is as the same of that in traditional pattern recognition, that is, we can choose various sensors such as metrical devices, image collecting devices, and other devices used for conversion, filter, enhancement, reducing noise, correcting distortion and so on according to the object to be recognized. The functions of the pattern analysis processing are a little stronger than that of traditional pattern recognition, it can analyze the simulating signal collecting from the real world, including feature extraction, prototype analysis, topological judgment, description for the organization and structure of the features or prototypes, as well as the background description and so on, preparing for further works including knowledge searching, character/prototype searching, and matching Fig. 15. The framework of cognitive pattern recognition decision-making.The database of the character/prototype, which stores the features of the external objects and the prototypes that constitute of them, only contains a part of human's long-term memory. This database have extended the ability which is able to append new character/prototype into it. The database of the feature/prototype together with the database of knowledge/rule which stores transcendent knowledge, rules of prototype combining, feature relation knowledge and so on, responds to human's long-term memory. The section of matching decision-making is combing the result of pattern analysis, first searching the feature /prototype and the knowledge/rule from the corresponding database, and then matching them and evaluating the result with some rules, in the end outputting the results of the recognition. If the matching fails, the system will add those new items to their databases correspondingly, In this way, the system can learn and memorize new things.</p>
<p>Example application of cognition pattern recognition to Chinese character intelligent formation</p>
<p>Combined with the tradition theory of Chinese character formation with prototype theory in cognition psychology, Chinese character intelligent formation is formed basing on cognition mechanism and can be expressed as follows:</p>
<p>The Chinese character is a combination of either single hieroglyphic or self-explanatory symbol, or the combination of several of them based on meaning and echoism rules. The hieroglyphic and self-explanatory symbols of the all components in Chinese character set are Chinese character prototype. In other words, the hieroglyphic and self-explanatory symbols are the basic unit of Chinese character, so they are called basic elements. The components of the Chinese character are the basic elements with the topological mapping on the structure of the Chinese character. The relationship between the entire character and the basic elements, and the basic elements themselves which constitute the character, constitutes the hierarchical structure of the Chinese character. According to the mathematical model of the prototype structure object in equation (3.1), we can build a mathematical model of Chinese character intelligent formation, which can be defined as</p>
<p>M=S C =S</p>
<p>:P g C →</p>
<p>4.1</p>
<p>Where M is the Chinese character set, S is the structure set, P is the basic element set, C is the Chinese character component set, and g is the topological mapping from basic element to Chinese character component.</p>
<p>Systemic structure of the Chinese character intelligent formation</p>
<p>According to 4.1, the Chinese character is composed of mapping from basic elements of Chinese character to structure of Chinese character. The principle of Chinese character intelligent formation according to the research above is shown in figure 16. In figure 16, we can see that the Chinese character intelligent formation is composed of the Chinese character basic element database, the Chinese character knowledge database, the inference machine and the Chinese character intelligent formation models, which are stated separately as follows:</p>
<p>1 Basic element database Basic element database, one kind of long-term memory, stores basic elements. As argued above, the Chinese character contains hieroglyphic and self-explanatory symbols and their combinations, which embodies the ideographic characteristic of the Chinese character because each basic element which constitutes the character has its own meaning. Therefore, the Chinese character is, currently, the only remained ideographic character. From the above analysis, the hieroglyphic and self-explanatory symbols are basic elements. The Chinese character, whose basic elements mainly manifest the meaning of Chinese character, is a combination of the shape, sound and meaning.</p>
<p>2 Knowledge database Knowledge database, another kind of long-term memory, stores the knowledge of structure and mapping. The Chinese character being a kind of structured character, is also a combination of the shape, sound and meaning. The "shape" embodies the structure of the Chinese character, which is the combination relationship between the entire character and the basic elements, and between the basic elements themselves which constitute the character. Moreover, the structure, which can describe the position, the size and the shape of the basic elements in the Chinese character, is also the combination rules in Chinese character intelligent formation. With the structure of Chinese character, the unceasingly developing Chinese character can be formed by some limited basic elements. As the long history of Chinese character, there already have many research results of structure of Chinese character, so determining the structure of Chinese character, as well as determining the basic element, has double tasks of inheriting culture and realizing high efficiency of computer processing. The basic element, as an abstract, is the most basic and representative characteristic of Chinese character. The conversion from the basic element to the components of Chinese character is called topological transformation. The Chinese character has its concrete form, which is composed by the concrete basic elements distributed in a character plain based on the structure of Chinese character. The process from the basic elements to the specific component of Chinese character is a mapping from abstract to concrete object maintaining the topological invariance.</p>
<p>3 Inference machine The inference machine perceives the input information, and then explains the meaning of the information, such as what is the structure of the Chinese character, how many levels are there in the structures, what is the basic elements in each level of the structure. Next, according to the result, the corresponding basic elements can be searched out from the basic element database, and the corresponding topology mapping knowledge can also be searched out according to the structure.</p>
<p>4 Chinese character intelligent formation model The principle of Chinese character intelligent formation model can be described as follows: first, extracting the corresponding basic element form the basic element library, then, according to the knowledge of basic element mapping, mapping the basic element to the structure of Chinese character, and finally after accomplishing all the mapping of the components, a Chinese character is formed, as shown in figure 17 and 18. The basic elements corresponding to some prototypes don't constitute integral structure of the Chinese character directly, which is exactly constituted by some compounds from the basic elements after one or more times changes, such a character has the multistage structures. According to the compound number of times, it is called second-level structure, third-level structure, and fourth-level structure. Taking 蘑 for example, the analysis process of the hierarchical structure of Chinese character is shown in Figure 19. </p>
<p>Fig. 2 .
2The model of memory system</p>
<p>Fig. 3 .
3The Symbol-net model</p>
<p>Fig. 4 .
4The Level-semantics-net model</p>
<p>Fig. 5 .
5The activation-diffusion model</p>
<p>[13]. Fisher judgment and principal component analysis are traditional linear methods which widely applied in pattern classification and feature extraction. The Fisher judgment [</p>
<p>Ψ
,where any element of Y can be obtained by the compression and expansion of the corresponding element of object X ( in order to discuss conveniently, every set is supposed to have m elements, but that not means different sets have the same number of the elements). The topological information set of the object X is expressed as power set of Y(the collection containing all subsets of Y)is Y is the topology of the topological information set of the image Y, then (Y, Y Ψ ) constitutes a discrete topological space.Proof: Because Y Ψ , the power set of Y, contains all subsets of Y, obviously Y Ψ satisfies three topological theorems as follows:1. Both Y and ∅ are in Y Ψ ;2. The union of random number of any subcollection of Y</p>
<p>the topology of the topological information set of the image Y, and (Y Y Ψ ) constitutes a discrete topological space, thus the proposition is proved and established.</p>
<p>Fig. 6 .
6The relationship between the object and its image Proposition 3.3: The size constancy of visual perception has the property of topological invariance. Proof: suppose the smallest distinguishable image (image of object X generated on retina from the farthest distance) is the topological space Y1 (proved in Proposition 3.1 and corresponding to the of</p>
<p>Fig 9.</p>
<p>Fig. 9 .
9The structure of the methane</p>
<p>Fig. 10 .
10The relationship between the prototypes and the components of objects It can be described from the perspective of topology: Definition 3.5 A set without any repeated elements abstracted from the component collection C of all the objects M in a kind of object O, is called the prototype of this kind of object O, denoted as P, From the definition 3.3 and 3.5, all objects M in every class of object O can be decomposed, under a certain rule, into some components C, from which a set abstracted without any repeated elements becomes prototype P, as shown inFig 11.</p>
<p>Fig. 11 .M={Pattern
11Objects-Components-PrototypesIn the example above the specific objects are the following English words "Pattern, Recognition PATTERN RECOGNITION }, then M can be decomposed into the component collection C={{P</p>
<p>,
{P, A, T, T, E, R, N}, {R, E, C, O, G, N, I, T, I, O, N}}. The differences of the components among the R, R</p>
<p>. 5 .
512 ,,, n mm m } is the set of all the objects in O, each element in it denotes denotes the collection of all components sets; P={ 12 ,,, r pp p } denotes the set of all the prototypes which is abstracted from the component collection C of all the objects M in O. Suppose P Γ is the power set of the prototypes P, of proof is the same as that of the proposition 3.1. Similarly, it can be proved that (M, M The collection C={ 12 ,,, n CC C } is a basis of the prototypes' topological space P P Γ . Proof: P is the unions of the component set C i , Each element in P Γ can be denoted by the union of some elements in C, satisfying the definition of basis[20] in topological space, therefore, the collection C={ 12 ,,, n CC C } constitutes a basis of the prototypes' topological space P discrete topological space, moreover any discrete topological space has a simplest basis (of course, there may be more than one basis for any topology), which is composed by all single-point subset of it. The single-point subset of the topological space P P Γ is 12 ,,, n CC C , therefore the collection C={ 12 ,,, n CC C } constitutes a basis of the prototypes' topological space P P Γ . So any prototypes in the prototype set P are abstracted from the component collection C of all the objects M in O. Proposition 3.6 If f is the mapping from the topological space of the components C to the topological space of the prototypes P, then f is surjective. Proof: P is the set of the prototypes abstracted from the components collection C of all the objects M in O,</p>
<p>={P A T T E R N} C 4 ={R E C O G N I T I O N}, namely, C = (C1, C2, C3, C4), and the prototype set P={R, P, A, T, E, N, C, O, I, G, e, c, o, g, n, i, t} which is abstracted from the component collection C of all the objects in M, that is,</p>
<p>Fig. 12 .
12The , as shown in 10, the word PATTERN can be obtained like this: with the topological transformation the elements A, E, N, P, R, T in the prototype set P change into the components A, E, N, P, R, T, and then according to certain structure the word is constituted by the components, that is PATTERN=S(A, E, N, P, R, T)。 The model of composing object based on the theory of prototype matching can be described like this:</p>
<p>From the proposition above that C={ 12 ,,, by all the objects in M are limited, that is, the number of the elements in collection C = { 12 ,,, n CC C }is numerable, so P P Γ has a numerable basis----C={ 12 ,,, n CC C }, that satisfies the definition of the second countable axiom[22], therefore, the prototype topological space (P P Γ ) can satisfy the second countable axiom and it is the space of A2, the proposition is proved and established.Proposition 3.9: The prototype topological space (P P Γ ) can satisfy the first countable axiom.</p>
<p>point p, that satisfies the definition of the first countable axiom of the topological space[23], so the prototype topological space (P P Γ ) can satisfy the first countable axiom and it is the space of A1, the proposition is proved and established. Proposition 3.10: The prototype topological space (P P Γ ) can satisfy any axiom of separability.</p>
<p>Fig. 13 .
13www.intechopen.comPattern Recognition Techniques, Technology and Applications 456</p>
<p>Fig. 14 .
14The systemic framework of machine pattern recognition</p>
<p>Fig. 16 .
16The principle of Chinese character intelligent formation</p>
<p>Fig. 17 .
17The process of Chinese character intelligent formation</p>
<p>Fig. 18 .
18Sketch of Chinese character intelligent formation Fig. 19. Analysis of the structure of Chinese character 6. References Anderson JR, Cognitive Psychology and Its Implication . New York: Freeman 1990 Ausubel DP. Education Psychology: A cognitive View. New York: Holt, Rinebart &amp; Winston, 1968 Briars DJ, Larkin JH. An Integrated Model of Skill in Solving Elementary Word Problems. Cognition and Instruction,1984,1:245-269
www.intechopen.comPattern Recognition Techniques, Technology and Applications
www.intechopen.comTheory of Cognitive Pattern Recognition
The prototypes P abstracted from the components C, have not changed the topology of the component c ij of the objects, just make a small change in the size or the shape of the component c ij , thus www.intechopen.comTheory of Cognitive Pattern Recognition
A wealth of advanced pattern recognition algorithms are emerging from the interdiscipline between technologies of effective visual features and the human-brain cognition process. Effective visual features are made possible through the rapid developments in appropriate sensor equipments, novel filter designs, and viable information processing architectures. While the understanding of human-brain cognition process broadens the way in which the computer can perform pattern recognition tasks. The present book is intended to collect representative researches around the globe focusing on low-level vision, filter design, features and image descriptors, data mining and analysis, and biologically inspired algorithms. The 27 chapters coved in this book disclose recent advances and new ideas in promoting the techniques, technology and applications of pattern recognition.How to referenceIn order to correctly reference this scholarly work, feel free to copy and paste the following: The Author(s). Licensee IntechOpen. This chapter is distributed under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike-3.0 License, which permits use, distribution and reproduction for non-commercial purposes, provided the original is properly cited and derivative works building on this content are distributed under the same license.
The Conditions of Learning. R M Gagne, Holt, Rinehart and WinstonNew York, NYGagne RM. The Conditions of Learning. New York, NY: Holt, Rinehart and Winston,1965</p>
<p>. Robert L Solso, M Kimberly Maclin, Otto H Maclin, PeKing university pressBeiJingSeventh EditionRobert L. Solso, M. Kimberly MacLin, Otto H.MacLin. Cognitive Psychology[M]. Seventh Edition. BeiJing: PeKing university press,2005: 104-139.</p>
<p>. Fang Junming, Development, Challenges, Cognitive, Of, Processing, PSYCHOLOGICAL SCIENCE[J]. 621Fang Junming. THE DEVELOPMENT OF AND CHALLENGES TO COGNITIVE PSYCHOLOGY OF INFORMATION PROCESSING. PSYCHOLOGICAL SCIENCE[J].1998;6(21):481-484.</p>
<p>statistical pattern recognition a review. Pattern Analysis and Machine Intelligence. A K Jain, R P W Duin, Jianchang Mao, IEEE TRANSACTIONS ON. 221Jain A.K, Duin, R.P.W, Jianchang Mao. statistical pattern recognition a review. Pattern Analysis and Machine Intelligence, IEEE TRANSACTIONS ON, 2000;22(1):4-37.</p>
<p>On optimal pairwise linear classifiers for normal distributions: the two dimensional case. Pattern Analysis and Machine Intelligence. L G Rueda, B J Oommen, IEEE Transactions on. 242Rueda L G, Oommen B J. On optimal pairwise linear classifiers for normal distributions: the two dimensional case. Pattern Analysis and Machine Intelligence, IEEE Transactions on , 2002; 24(2): 274-280.</p>
<p>An improved naive Bayesian classifier technique coupled with a novel input solution method. J N K Liu, B N L Li, T S Dillon, Systems, Man and Cybernetics. Part C31infall predictionLiu J N K, Li B N L, Dillon T S. An improved naive Bayesian classifier technique coupled with a novel input solution method [infall prediction]. Systems, Man and Cybernetics, Part C, IEEE Transactions on,2001; 31(2):249-256.</p>
<p>Achieving optimal Bayesian classification performance using a novel approach: the'race to the attractor'neural network model. Virtual and Intelligent Measurement Systems. G Ferland, T Yeap, IEEE International Workshop on. VIMS. 1Ferland G, Yeap T. Achieving optimal Bayesian classification performance using a novel approach: the'race to the attractor'neural network model. Virtual and Intelligent Measurement Systems, 2001. IEEE International Workshop on. VIMS 2001; Vol.1: 115-120.</p>
<p>A discriminant function considering normality improvement of the distribution. H Ujiie, S Omachi, H Aso, Proceedings. 16th International Conference on. 16th International Conference on2Ujiie H, Omachi S, Aso H. A discriminant function considering normality improvement of the distribution. Pattern Recognition, 2002. Proceedings. 16th International Conference on, 2002; Vol.2: 224-227.</p>
<p>introduction to statistical learning theory and support vector machines. acta automatica sinica. Zhang Xuegong, 26ZHANG Xuegong. introduction to statistical learning theory and support vector machines. acta automatica sinica[j]. 2000; 26(1): p32-42.</p>
<p>Kernel-based methods(B). Metallurgical Industry Automation. Luo Gong, Liang, 26LUO Gong liang. Kernel-based methods(B). Metallurgical Industry Automation[J]. 2002; 26(4): p1-4.</p>
<p>Nonlinear Fisher discriminant using kernels. Electrical and Electronic Engineers in Israel, 2000. The 21st IEEE Convention of the. O Guttman, R Meir, 1Guttman O, Meir R. Nonlinear Fisher discriminant using kernels. Electrical and Electronic Engineers in Israel, 2000. The 21st IEEE Convention of the, 2000; vol.1: 257-260.</p>
<p>Metallurgical Industry Automation. Luo Gong, Liang, 26Kernel-based methods(A)LUO Gong liang. Kernel-based methods(A). Metallurgical Industry Automation[J]. 2002;26(3): p1-4.</p>
<p>Two variations on Fisher's linear discriminant for pattern recognition. Pattern Analysis and Machine Intelligence. T Cooke, IEEE Transactions on. 242Cooke T. Two variations on Fisher's linear discriminant for pattern recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 2002; 24(2):268-273.</p>
<p>Topological) Pattern Recognition-A New Model of Pattern Recognition Theory and Its Applications. Acta Electronica Sinica. Wang Shou-Jue, Bionic, 30WANG Shou-jue. Bionic (Topological) Pattern Recognition-A New Model of Pattern Recognition Theory and Its Applications. Acta Electronica Sinica[J]. 2002; 30(10): 1417-1420.</p>            </div>
        </div>

    </div>
</body>
</html>
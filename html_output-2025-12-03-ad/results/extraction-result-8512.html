<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8512 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8512</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8512</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-061d113a7b3f32deab6bc50fea676fa0b1e0f658</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/061d113a7b3f32deab6bc50fea676fa0b1e0f658" target="_blank">Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work takes a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks and applies an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations.</p>
                <p><strong>Paper Abstract:</strong> Interactive Fiction (IF) games with real human-written natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the human-written textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations. Extensive experiments on the recent IF benchmark (Jericho) demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches. Our source code is available at: this https URL.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8512.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8512.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPRC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Paragraph Reading Comprehension Deep Q-Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The primary agent proposed in this paper that formulates IF game play as a multi-paragraph reading-comprehension (MPRC) task: it uses an RC-style model (BiDAF + self-attention) to generate and value template-based actions and augments the current observation with retrieved historical observation paragraphs via an object-centric retrieval strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MPRC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL agent that predicts template action values via an extractive-RC model (context-query attention + self-attention) and mitigates partial observability by retrieving object-centric past observation paragraphs; trained with DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (suite of Interactive Fiction games)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A benchmark collection of human-written interactive fiction (IF) text games (e.g., Zork) with large combinatorial natural-language action spaces and partial observability challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented episodic historical observation memory (object-centric)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Past observations are retained as raw textual paragraphs (the observation history). For the current observation the system detects objects (spaCy) and for each detected object retrieves the most recent K historical observations that mention that object (time-sensitive retrieval). The retrieved K observations (per object) are sorted by time, separated by a special token, and concatenated to form an extended multi-paragraph input.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Concatenate the retrieved historical observation paragraphs to the current observation (with separators) and feed the combined multi-paragraph text into the RC-based action prediction model (BiDAF + self-attention). The RC model computes verb-aware observation representations and then pools object-specific positions to estimate Q-values.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Achieved best-agent results on 21 out of 33 Jericho games (64% winning rate; trained with 0.1M environment interactions); per-game scores in Table 2 of the paper (MPRC-DQN column).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared to RC-DQN (same RC model but only current observation), MPRC-DQN had higher winning-rate: RC-DQN was best on 17/33 games (52% winning rate); MPRC-DQN additionally showed faster convergence in learning curves.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Direct comparison/ablation vs RC-DQN (no history) shows MPRC-DQN (with object-centric history) outperforms RC-DQN in #games won and converges faster. Retrieval-strategy ablations compared different history window sizes K and a pure-recency strategy (w/o rec). Results: object-centric time-sensitive retrieval (most-recent-K per shared object, K=2 default) generally helps; pure-recency retrieval often performs no better than no-history early in learning and has higher variance.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Object-centric, time-sensitive retrieval: detect objects in current observation and retrieve the most recent K past observations that mention those objects (K=2 used by default). Concatenate retrieved paragraphs (with separators) into the RC model input.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvements from memory are game-dependent; choice of history window size K matters per game. Historical information can introduce noise; pure-recency retrieval can have limited additional information and higher variance due to policy changes. The paper notes that history usage in prior graph-based methods sometimes brings noise and is not always significantly helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Use object-centric, time-sensitive retrieval (retrieve most recent K observations per shared object) rather than blind recency or undifferentiated history summarization; integrate retrieved textual history directly into an RC-style model so that attention can focus on supportive evidence; tune K by game characteristics. Memory helps especially with partial observability and reward sparsity and also speeds convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8512.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8512.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reading-Comprehension Deep Q-Network (no-history ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation agent that implements the RC-based action prediction model from this paper but only takes the current observation (no retrieved history) as input; used to measure the effect of the memory retrieval component.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same RC-based template action value estimator as MPRC-DQN (BiDAF + self-attention + argument-specific embeddings) but uses only the current observation text (no historical retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (suite of Interactive Fiction games)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A benchmark collection of human-written interactive fiction (IF) text games with large natural-language action spaces and partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Best on 17/33 games (52% winning rate) as reported in the paper (RC-DQN column); outperforms prior baselines but is weaker than MPRC-DQN which augments history.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Serves as the direct no-history comparator to MPRC-DQN; ablations of RC-model components (self-attention, argument-specific embeddings, Transformer encoder) were also performed showing argument embeddings are important and Transformer sometimes faster early.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>N/A (no memory).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lacks historical context, which reduces performance and slows or prevents correct decisions in partially observable situations (e.g., repeated interactions like attacking an enemy).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Adding object-centric retrieved history (as in MPRC-DQN) improves both final performance and convergence speed compared to RC-DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8512.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8512.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Object-centric retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Object-centric historical observation retrieval (time-sensitive)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The memory/retrieval mechanism introduced in this paper: detect objects in the current observation and retrieve the most recent K historical observations that mention those objects, then concatenate them to the current observation for RC-based processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>object-centric retrieval (mechanism used by MPRC-DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A time-sensitive paragraph retrieval policy: given detected objects in the current observation, retrieve up to K most recent past observations that share those objects, sort them by time, separate by tokens and append to the current observation for multi-paragraph RC.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (IF games)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Human-written interactive fiction games that require tracking objects and partial-observability reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented episodic textual memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Historical observations are stored as textual passages; spaCy identifies object mentions; retrieval returns the K most-recent passages containing each detected object; K=2 in experiments; concatenation uses a special separator token between observations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Retrieved paragraphs are concatenated with the current observation and fed into the RC model (BiDAF + self-attention) so attention mechanisms can select supportive evidence across multiple paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>When used (MPRC-DQN), improved winning-rate to 64% (21/33 games best) and faster convergence compared to no-history RC-DQN (52% / 17 games best).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Ablations tested different history sizes K and a pure recency baseline (take latest K observations irrespective of shared objects); pure recency generally underperformed object-centric retrieval (or provided limited early benefit) and had higher variance.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Time-sensitive, object-centric retrieval with K tuned per task (paper uses K=2 as default).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Effectiveness varies by game; too-large K can introduce stale or noisy facts; pure-recency retrieval often gives limited new information and high variance from policy changes; detection errors (object detection) can harm retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Prefer object-centric retrieval over pure recency or undifferentiated history summarization; keep retrieval time-sensitive (recent observations more relevant); allow model attention to select informative contexts from concatenated paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8512.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8512.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Constrained A2C (KG-A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent baseline that constructs a knowledge graph (objects as nodes and relations from OpenIE + rules) from historical observations and uses that graph as a state representation in an A2C RL agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Builds an object-centered knowledge graph from past textual observations using OpenIE and hand-crafted heuristics, summarizes history into a structured graph state, and uses a GRU-based decoder for action generation in an A2C framework.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (IF games)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Suite of human-written interactive fiction games with large action vocabularies and partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>structured knowledge-graph episodic memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>A knowledge graph built from historical observations: objects are nodes and relations are extracted by OpenIE plus human-written rules; the graph summarizes objects/relations from history into a single structured representation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>The knowledge graph is used as the state representation for the RL policy; a GRU-based action generator produces template/actions conditioned on the graph-derived state.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in paper baselines: KG-A2C column in Table 2; overall was best on 9/33 games (27% winning rate) in the reported comparisons. Training data used by KG-A2C in prior work: ~1.6M interactions (cited in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Paper contrasts KG-A2C's strategy (summarize history into a graph and condense into a single vector) with the object-centric multi-paragraph retrieval + RC approach, arguing that KG-A2C treats history equally and may introduce noise and lose fine-grained, question-specific evidential context.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Not identified in this paper; KG-A2C uses graph-based summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Treats historical observations equally and compresses them into a single vector, which can bring noise and fail to emphasize contexts most relevant to the current action decision; relies on OpenIE and hand-crafted rules which may not generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>The authors recommend more targeted retrieval (object-centric and time-sensitive) and allowing attention mechanisms to focus on relevant textual evidence rather than compressing all history into one vector.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8512.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8512.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Graph DQN (KG-DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that builds a knowledge graph representation from historical observations (objects, positions, relations) and uses it to help RL in text games (cited as earlier graph-based approach).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Constructs an object/position/relation knowledge graph from history (using IE tools and rules) and uses that structured state representation in a DQN-style RL agent.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Text adventure / IF games (earlier synthetic and some IF settings)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Text-based games where object relations and long-range dependencies can be represented via graphs built from past textual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>structured knowledge-graph episodic memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Knowledge graph of objects and relations constructed from historical observations using OpenIE and heuristics; used as the agent's state representation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Graph representation is used to represent the game state for action selection (prior work used DRRN-like or DQN-style action selection over this graph-derived state).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as prior art; this paper compares conceptually to graph-based history summarization and argues for object-centric retrieval + RC as a more targeted means to surface evidence for specific action templates.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prior graph-based approaches summarize history into a graph and subsequently into vectors, possibly losing fine-grained textual evidence relevant to a specific action decision; requires OpenIE and rules.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>The paper suggests retrieved textual history fed to an RC model enables more focused evidence selection than undifferentiated graph summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8512.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8512.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NAIL (No-learning heuristic IF agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strong heuristic (rule-based/no-learning) IF agent referenced as prior state-of-the-art among non-learning agents that builds internal representations and uses heuristics for exploration and interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nail: A general interactive fiction agent</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A non-learning agent composed of heuristics for exploration, object interaction, and internal state representation, used as a baseline in IF research.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho / IF games</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Human-authored text adventure games used to evaluate IF agents' language understanding and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>internal heuristic state representation (graph-like)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Heuristics construct an internal representation of the game world (objects, positions, relations) to drive rule-based decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Internal heuristic representation guides actions via rule modules rather than a learned RC pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as the strongest 'no-learning' agent in related work; not used as an experimental learning baseline in the paper's RL comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>As a heuristic agent, NAIL doesn't generalize via learning; reliance on hand-crafted heuristics can limit adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Paper cites NAIL as complementary prior approach but focuses on learned RC + retrieval strategies to improve data efficiency and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Interactive fiction games: A colossal adventure <em>(Rating: 2)</em></li>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8512",
    "paper_id": "paper-061d113a7b3f32deab6bc50fea676fa0b1e0f658",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "MPRC-DQN",
            "name_full": "Multi-Paragraph Reading Comprehension Deep Q-Network",
            "brief_description": "The primary agent proposed in this paper that formulates IF game play as a multi-paragraph reading-comprehension (MPRC) task: it uses an RC-style model (BiDAF + self-attention) to generate and value template-based actions and augments the current observation with retrieved historical observation paragraphs via an object-centric retrieval strategy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MPRC-DQN",
            "agent_description": "An RL agent that predicts template action values via an extractive-RC model (context-query attention + self-attention) and mitigates partial observability by retrieving object-centric past observation paragraphs; trained with DQN.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Jericho (suite of Interactive Fiction games)",
            "benchmark_description": "A benchmark collection of human-written interactive fiction (IF) text games (e.g., Zork) with large combinatorial natural-language action spaces and partial observability challenges.",
            "memory_used": true,
            "memory_type": "retrieval-augmented episodic historical observation memory (object-centric)",
            "memory_architecture": "Past observations are retained as raw textual paragraphs (the observation history). For the current observation the system detects objects (spaCy) and for each detected object retrieves the most recent K historical observations that mention that object (time-sensitive retrieval). The retrieved K observations (per object) are sorted by time, separated by a special token, and concatenated to form an extended multi-paragraph input.",
            "memory_integration_strategy": "Concatenate the retrieved historical observation paragraphs to the current observation (with separators) and feed the combined multi-paragraph text into the RC-based action prediction model (BiDAF + self-attention). The RC model computes verb-aware observation representations and then pools object-specific positions to estimate Q-values.",
            "performance_with_memory": "Achieved best-agent results on 21 out of 33 Jericho games (64% winning rate; trained with 0.1M environment interactions); per-game scores in Table 2 of the paper (MPRC-DQN column).",
            "performance_without_memory": "Compared to RC-DQN (same RC model but only current observation), MPRC-DQN had higher winning-rate: RC-DQN was best on 17/33 games (52% winning rate); MPRC-DQN additionally showed faster convergence in learning curves.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Direct comparison/ablation vs RC-DQN (no history) shows MPRC-DQN (with object-centric history) outperforms RC-DQN in #games won and converges faster. Retrieval-strategy ablations compared different history window sizes K and a pure-recency strategy (w/o rec). Results: object-centric time-sensitive retrieval (most-recent-K per shared object, K=2 default) generally helps; pure-recency retrieval often performs no better than no-history early in learning and has higher variance.",
            "best_memory_strategy": "Object-centric, time-sensitive retrieval: detect objects in current observation and retrieve the most recent K past observations that mention those objects (K=2 used by default). Concatenate retrieved paragraphs (with separators) into the RC model input.",
            "limitations_or_failure_cases": "Improvements from memory are game-dependent; choice of history window size K matters per game. Historical information can introduce noise; pure-recency retrieval can have limited additional information and higher variance due to policy changes. The paper notes that history usage in prior graph-based methods sometimes brings noise and is not always significantly helpful.",
            "recommendations_or_conclusions": "Use object-centric, time-sensitive retrieval (retrieve most recent K observations per shared object) rather than blind recency or undifferentiated history summarization; integrate retrieved textual history directly into an RC-style model so that attention can focus on supportive evidence; tune K by game characteristics. Memory helps especially with partial observability and reward sparsity and also speeds convergence.",
            "uuid": "e8512.0",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "RC-DQN",
            "name_full": "Reading-Comprehension Deep Q-Network (no-history ablation)",
            "brief_description": "An ablation agent that implements the RC-based action prediction model from this paper but only takes the current observation (no retrieved history) as input; used to measure the effect of the memory retrieval component.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RC-DQN",
            "agent_description": "Same RC-based template action value estimator as MPRC-DQN (BiDAF + self-attention + argument-specific embeddings) but uses only the current observation text (no historical retrieval).",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Jericho (suite of Interactive Fiction games)",
            "benchmark_description": "A benchmark collection of human-written interactive fiction (IF) text games with large natural-language action spaces and partial observability.",
            "memory_used": false,
            "memory_type": null,
            "memory_architecture": null,
            "memory_integration_strategy": null,
            "performance_with_memory": null,
            "performance_without_memory": "Best on 17/33 games (52% winning rate) as reported in the paper (RC-DQN column); outperforms prior baselines but is weaker than MPRC-DQN which augments history.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Serves as the direct no-history comparator to MPRC-DQN; ablations of RC-model components (self-attention, argument-specific embeddings, Transformer encoder) were also performed showing argument embeddings are important and Transformer sometimes faster early.",
            "best_memory_strategy": "N/A (no memory).",
            "limitations_or_failure_cases": "Lacks historical context, which reduces performance and slows or prevents correct decisions in partially observable situations (e.g., repeated interactions like attacking an enemy).",
            "recommendations_or_conclusions": "Adding object-centric retrieved history (as in MPRC-DQN) improves both final performance and convergence speed compared to RC-DQN.",
            "uuid": "e8512.1",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Object-centric retrieval",
            "name_full": "Object-centric historical observation retrieval (time-sensitive)",
            "brief_description": "The memory/retrieval mechanism introduced in this paper: detect objects in the current observation and retrieve the most recent K historical observations that mention those objects, then concatenate them to the current observation for RC-based processing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "object-centric retrieval (mechanism used by MPRC-DQN)",
            "agent_description": "A time-sensitive paragraph retrieval policy: given detected objects in the current observation, retrieve up to K most recent past observations that share those objects, sort them by time, separate by tokens and append to the current observation for multi-paragraph RC.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Jericho (IF games)",
            "benchmark_description": "Human-written interactive fiction games that require tracking objects and partial-observability reasoning.",
            "memory_used": true,
            "memory_type": "retrieval-augmented episodic textual memory",
            "memory_architecture": "Historical observations are stored as textual passages; spaCy identifies object mentions; retrieval returns the K most-recent passages containing each detected object; K=2 in experiments; concatenation uses a special separator token between observations.",
            "memory_integration_strategy": "Retrieved paragraphs are concatenated with the current observation and fed into the RC model (BiDAF + self-attention) so attention mechanisms can select supportive evidence across multiple paragraphs.",
            "performance_with_memory": "When used (MPRC-DQN), improved winning-rate to 64% (21/33 games best) and faster convergence compared to no-history RC-DQN (52% / 17 games best).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Ablations tested different history sizes K and a pure recency baseline (take latest K observations irrespective of shared objects); pure recency generally underperformed object-centric retrieval (or provided limited early benefit) and had higher variance.",
            "best_memory_strategy": "Time-sensitive, object-centric retrieval with K tuned per task (paper uses K=2 as default).",
            "limitations_or_failure_cases": "Effectiveness varies by game; too-large K can introduce stale or noisy facts; pure-recency retrieval often gives limited new information and high variance from policy changes; detection errors (object detection) can harm retrieval quality.",
            "recommendations_or_conclusions": "Prefer object-centric retrieval over pure recency or undifferentiated history summarization; keep retrieval time-sensitive (recent observations more relevant); allow model attention to select informative contexts from concatenated paragraphs.",
            "uuid": "e8512.2",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "KG-A2C",
            "name_full": "Graph Constrained A2C (KG-A2C)",
            "brief_description": "A recent baseline that constructs a knowledge graph (objects as nodes and relations from OpenIE + rules) from historical observations and uses that graph as a state representation in an A2C RL agent.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces",
            "mention_or_use": "use",
            "agent_name": "KG-A2C",
            "agent_description": "Builds an object-centered knowledge graph from past textual observations using OpenIE and hand-crafted heuristics, summarizes history into a structured graph state, and uses a GRU-based decoder for action generation in an A2C framework.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Jericho (IF games)",
            "benchmark_description": "Suite of human-written interactive fiction games with large action vocabularies and partial observability.",
            "memory_used": true,
            "memory_type": "structured knowledge-graph episodic memory",
            "memory_architecture": "A knowledge graph built from historical observations: objects are nodes and relations are extracted by OpenIE plus human-written rules; the graph summarizes objects/relations from history into a single structured representation.",
            "memory_integration_strategy": "The knowledge graph is used as the state representation for the RL policy; a GRU-based action generator produces template/actions conditioned on the graph-derived state.",
            "performance_with_memory": "Reported in paper baselines: KG-A2C column in Table 2; overall was best on 9/33 games (27% winning rate) in the reported comparisons. Training data used by KG-A2C in prior work: ~1.6M interactions (cited in Table 1).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Paper contrasts KG-A2C's strategy (summarize history into a graph and condense into a single vector) with the object-centric multi-paragraph retrieval + RC approach, arguing that KG-A2C treats history equally and may introduce noise and lose fine-grained, question-specific evidential context.",
            "best_memory_strategy": "Not identified in this paper; KG-A2C uses graph-based summarization.",
            "limitations_or_failure_cases": "Treats historical observations equally and compresses them into a single vector, which can bring noise and fail to emphasize contexts most relevant to the current action decision; relies on OpenIE and hand-crafted rules which may not generalize.",
            "recommendations_or_conclusions": "The authors recommend more targeted retrieval (object-centric and time-sensitive) and allowing attention mechanisms to focus on relevant textual evidence rather than compressing all history into one vector.",
            "uuid": "e8512.3",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "KG-DQN",
            "name_full": "Knowledge-Graph DQN (KG-DQN)",
            "brief_description": "Prior work that builds a knowledge graph representation from historical observations (objects, positions, relations) and uses it to help RL in text games (cited as earlier graph-based approach).",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "KG-DQN",
            "agent_description": "Constructs an object/position/relation knowledge graph from history (using IE tools and rules) and uses that structured state representation in a DQN-style RL agent.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Text adventure / IF games (earlier synthetic and some IF settings)",
            "benchmark_description": "Text-based games where object relations and long-range dependencies can be represented via graphs built from past textual observations.",
            "memory_used": true,
            "memory_type": "structured knowledge-graph episodic memory",
            "memory_architecture": "Knowledge graph of objects and relations constructed from historical observations using OpenIE and heuristics; used as the agent's state representation.",
            "memory_integration_strategy": "Graph representation is used to represent the game state for action selection (prior work used DRRN-like or DQN-style action selection over this graph-derived state).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Mentioned as prior art; this paper compares conceptually to graph-based history summarization and argues for object-centric retrieval + RC as a more targeted means to surface evidence for specific action templates.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "Prior graph-based approaches summarize history into a graph and subsequently into vectors, possibly losing fine-grained textual evidence relevant to a specific action decision; requires OpenIE and rules.",
            "recommendations_or_conclusions": "The paper suggests retrieved textual history fed to an RC model enables more focused evidence selection than undifferentiated graph summaries.",
            "uuid": "e8512.4",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "NAIL",
            "name_full": "NAIL (No-learning heuristic IF agent)",
            "brief_description": "A strong heuristic (rule-based/no-learning) IF agent referenced as prior state-of-the-art among non-learning agents that builds internal representations and uses heuristics for exploration and interaction.",
            "citation_title": "Nail: A general interactive fiction agent",
            "mention_or_use": "mention",
            "agent_name": "NAIL",
            "agent_description": "A non-learning agent composed of heuristics for exploration, object interaction, and internal state representation, used as a baseline in IF research.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Jericho / IF games",
            "benchmark_description": "Human-authored text adventure games used to evaluate IF agents' language understanding and planning.",
            "memory_used": true,
            "memory_type": "internal heuristic state representation (graph-like)",
            "memory_architecture": "Heuristics construct an internal representation of the game world (objects, positions, relations) to drive rule-based decisions.",
            "memory_integration_strategy": "Internal heuristic representation guides actions via rule modules rather than a learned RC pipeline.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Mentioned as the strongest 'no-learning' agent in related work; not used as an experimental learning baseline in the paper's RL comparisons.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "As a heuristic agent, NAIL doesn't generalize via learning; reliance on hand-crafted heuristics can limit adaptation.",
            "recommendations_or_conclusions": "Paper cites NAIL as complementary prior approach but focuses on learned RC + retrieval strategies to improve data efficiency and generalization.",
            "uuid": "e8512.5",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2
        },
        {
            "paper_title": "Interactive fiction games: A colossal adventure",
            "rating": 2
        },
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.017329499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</h1>
<p>Xiaoxiao Guo<em><br>IBM Research<br>xiaoxiao.guo@ibm.com<br>$\frac{\text { Mo Yu</em> }}{\text { IBM Research }}$<br>yum@us.ibm.com<br>Yupeng Gao<br>IBM Research<br>yupeng.gao@ibm.com<br>Chuang Gan<br>MIT-IBM Watson AI Lab<br>chuangg@ibm.com<br>Murray Campbell<br>IBM Research<br>mcam@us.ibm.com<br>Shiyu Chang<br>MIT-IBM Watson AI Lab<br>shiyu.chang@ibm.com</p>
<h4>Abstract</h4>
<p>Interactive Fiction (IF) games with real humanwritten natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the humanwritten textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations. Extensive experiments on the recent IF benchmark (Jericho) demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Interactive systems capable of understanding natural language and responding in the form of natural language text have high potentials in various applications. In pursuit of building and evaluating such systems, we study learning agents for Interactive Fiction (IF) games. IF games are world-simulating software in which players use text commands to control the protagonist and influence the world, as illustrated in Figure 1. IF gameplay agents need to simultaneously understand the game's information from a text display (observation) and generate</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Sample gameplay for the classic dungeon game Zork1. The objective is to solve various puzzles and collect the 19 treasures to install the trophy case. The player receives textual observations describing the current game state and additional reward scalars encoding the game designers' objective of game progress. The player sends textual action commands to control the protagonist.
natural language command (action) via a text input interface. Without providing an explicit game strategy, the agents need to identify behaviors that maximize objective-encoded cumulative rewards.</p>
<p>IF games composed of human-written texts (distinct from previous text games with synthetic texts) create superb new opportunities for studying and evaluating natural language understanding (NLU) techniques due to their unique characteristics. (1) Game designers elaborately craft on the literariness of the narrative texts to attract players when creating IF games. The resulted texts in IF games are more linguistically diverse and sophisticated than the template-generated ones in synthetic text games. (2) The language contexts of IF games</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of our approach to solving the IF games as Multi-Paragraph Reading Comprehension (MPRC) tasks.
are more versatile because various designers contribute to enormous domains and genres, such as adventure, fantasy, horror, and sci-fi. (3) The text commands to control characters are less restricted, having sizes over six orders of magnitude larger than previous text games. The recently introduced Jericho benchmark provides a collection of such IF games (Hausknecht et al., 2019a).</p>
<p>The complexity of IF games demands more sophisticated NLU techniques than those used in synthetic text games. Moreover, the task of designing IF game-play agents, intersecting NLU and reinforcement learning (RL), poses several unique challenges on the NLU techniques. The first challenge is the difficulty of exploration in the huge natural language action space. To make RL agents learn efficiently without prohibitive exhaustive trials, the action estimation must generalize learned knowledge from tried actions to others. To this end, previous approaches, starting with a single embedding vector of the observation, either predict the elements of actions independently (Narasimhan et al., 2015; Hausknecht et al., 2019a); or embed each valid action as another vector and predict action value based on the vector-space similarities (He et al., 2016). These methods do not consider the compositionality or role-differences of the action elements, or the interactions among them and the observation. Therefore, their modeling of the action values is less accurate and less data-efficient.</p>
<p>The second challenge is partial observability. At each game-playing step, the agent receives a textual observation describing the locations, objects, and characters of the game world. But the latest observation is often not a sufficient summary of the interaction history and may not provide enough
information to determine the long-term effects of actions. Previous approaches address this problem by building a representation over past observations (e.g., building a graph of objects, positions, and spatial relations) (Ammanabrolu and Riedl, 2019; Ammanabrolu and Hausknecht, 2020). These methods treat the historical observations equally and summarize the information into a single vector without focusing on important contexts related to the action prediction for the current observation. Therefore, their usages of history also bring noise, and the improvement is not always significant.</p>
<p>We propose a novel formulation of IF game playing as Multi-Passage Reading Comprehension (MPRC) and harness MPRC techniques to solve the huge action space and partial observability challenges. The graphical illustration is shown in Figure 2. First, the action value prediction (i.e., predicting the long-term rewards of selecting an action) is essentially generating and scoring a compositional action structure by finding supporting evidence from the observation. We base on the fact that each action is an instantiation of a template, i.e., a verb phrase with a few placeholders of object arguments it takes (Figure 2b). Then the action generation process can be viewed as extracting objects for a template's placeholders from the textual observation, based on the interaction between the template verb phrase and the relevant context of the objects in the observation. Our approach addresses the structured prediction and interaction problems with the idea of context-question attention mechanism in RC models. Specifically, we treat the observation as a passage and each template verb phrase as a question. The filling of object placeholders in the template thus becomes an</p>
<p>extractive QA problem that selects objects from the observation given the template. Simultaneously each action (i.e., a template with all placeholder replaced) gets its evaluation value predicted by the RC model. Our formulation and approach better capture the fine-grained interactions between observation texts and structural actions, in contrast to previous approaches that represent the observation as a single vector and ignore the fine-grained dependency among action elements.</p>
<p>Second, alleviating partial observability is essentially enhancing the current observation with potentially relevant history and predicting actions over the enhanced observation. Our approach retrieves potentially relevant historical observations with an object-centric approach (Figure 2a), so that the retrieved ones are more likely to be connected to the current observation as they describe at least one shared interactable object. Our attention mechanisms are then applied across the retrieved multiple observation texts to focus on informative contexts for action value prediction.</p>
<p>We evaluated our approach on the suite of Jericho IF games, compared to all previous approaches. Our approaches achieved or outperformed the state-of-the-art performance on 25 out of 33 games, trained with less than one-tenth of game interaction data used by prior art. We also provided ablation studies on our models and retrieval strategies.</p>
<h2>2 Related Work</h2>
<p>IF Game Agents. Previous work mainly studies the text understanding and generation in parserbased or rule-based text game tasks, such as TextWorld platform (Ct et al., 2018) or custom domains (Narasimhan et al., 2015; He et al., 2016; Adhikari et al., 2020). The recent platform Jericho (Hausknecht et al., 2019a) supports over thirty human-written IF games. Earlier successes in real IF games mainly rely on heuristics without learning. NAIL (Hausknecht et al., 2019b) is the state-of-theart among these "no-learning" agents, employing a series of reliable heuristics for exploring the game, interacting with objects, and building an internal representation of the game world. With the development of learning environments like Jericho, the RL-based agents have started to achieve dominating performance.</p>
<p>A critical challenge for learning-based agents is how to handle the combinatorial action space in IF games. LSTM-DQN (Narasimhan et al., 2015)
was proposed to generate verb-object action with pre-defined sets of possible verbs and objects, but treat the selection and learning of verbs and objects independently. Template-DQN (Hausknecht et al., 2019a) extended LSTM-DQN for template-based action generation, introducing one additional but still independent prediction output for the second object in the template. Deep Reinforcement Relevance Network (DRRN) (He et al., 2016) was introduced for choice-based games. Given a set of valid actions at every game state, DRRN projects each action into a hidden space that matches the current state representation vector for action selection. Action-Elimination Deep Q-Network (AEDQN) (Zahavy et al., 2018) learns to predict invalid actions in the adventure game Zork. It eliminates invalid action for efficient policy learning via utilizing expert demonstration data.</p>
<p>Other techniques focus on addressing the partial observability in text games. Knowledge Graph DQN (KG-DQN) (Ammanabrolu and Riedl, 2019) was proposed to deal with synthetic games. The method constructs and represents the game states as knowledge graphs with objects as nodes and uses pre-trained general purposed OpenIE tool and human-written rules to extract relations between objects. KG-DQN handles the action representation following DRRN. KG-A2C (Ammanabrolu and Hausknecht, 2020) later extends the work for IF games, by adding information extraction heuristics to fit the complexity of the object relations in IF games and utilizing a GRU-based action generator to handle the action space.</p>
<h2>Reading Comprehension Models for Question</h2>
<p>Answering. Given a question, reading comprehension (RC) aims to find the answer to the question based on a paragraph that may contain supporting evidence. One of the standard RC settings is extractive QA (Rajpurkar et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019), which extracts a span from the paragraph as an answer. Our formulation of IF game playing resembles this setting.</p>
<p>Many neural reader models have been designed for RC. Specifically, for the extractive QA task, the reader models usually build question-aware passage representations via attention mechanisms (Seo et al., 2016; Yu et al., 2018), and employ a pointer network to predict the start and end positions of the answer span (Wang and Jiang, 2016). Powerful pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019) have been</p>
<p>recently applied to enhance the encoding and attention mechanisms of the aforementioned reader models. They give performance boost but are more resource-demanding and do not suit the IF game playing task very well.</p>
<p>Reading Comprehension over Multiple Paragraphs. Multi-paragraph reading comprehension (MPRC) deals with the more general task of answering a question from multiple related paragraphs, where each paragraph may not necessarily support the correct answer. Our formulation becomes an MPRC setting when we enhance the state representation with historical observations and predict actions from multiple observation paragraphs.</p>
<p>A fundamental research problem in MPRC, which is also critical to our formulation, is to select relevant paragraphs from all the input paragraphs for the reader to focus on. Previous approaches mainly apply traditional IR approaches like BM25 (Chen et al., 2017; Joshi et al., 2017), or neural ranking models trained with distant supervision (Wang et al., 2018; Min et al., 2019a), for paragraph selection. Our formulation also relates to the work of evidence aggregation in MPRC (Wang et al., 2017; Lin et al., 2018), which aims to infer the answers based on the joint of evidence pieces from multiple paragraphs. Finally, recently some works propose the entity-centric paragraph retrieval approaches (Ding et al., 2019; Godbole et al., 2019; Min et al., 2019b; Asai et al., 2019), where paragraphs are connected if they share the same-named entities. The paragraph retrieval then becomes a traversal over such graphs via entity links. These entity-centric paragraph retrieval approaches share a similar high-level idea to our object-based history retrieval approach. The techniques above have been applied to deal with evidence from Wikipedia, news collections, and, recently, books (Mou et al., 2020). We are the first to extend these ideas to IF games.</p>
<h2>3 Multi-Paragraph RC for IF Games</h2>
<h3>3.1 Problem Formulation</h3>
<p>Each IF game can be defined as a Partially Observable Markov Decision Process (POMDP), namely a 7-tuple of $\langle S, A, T, O, \Omega, R, \gamma\rangle$, representing the hidden game state set, the action set, the state transition function, the set of textual observations composed from vocabulary words, the textual observation function, the reward function, and the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Our RC-based action prediction model architecture. The template text is a verb phrase with placeholders for objects, such as [pick up OBJ] and [break OBJ with OBJ].
discount factor respectively. The game playing agent interacts with the game engine in multiple turns until the game is over or the maximum number of steps is reached. At the $t$-th turn, the agent receives a textual observation describing the current game state $o_{t} \in O$ and sends a textual action command $a_{t} \in A$ back. The agent receives additional reward scalar $r_{t}$ which encodes the game designers' objective of game progress. Thus the task of the game playing can be formulated to generate a textual action command per step as to maximize the expected cumulative discounted rewards $\mathbf{E}\left[\sum_{t=0}^{\infty} \gamma^{t} r_{t}\right]$. Value-based RL approaches learn to approximate an observation-action value function $Q\left(o_{t}, a_{t} ; \boldsymbol{\theta}\right)$ which measures the expected cumulative rewards of taking action $a_{t}$ when observing $o_{t}$. The agent selects action based on the action value prediction of $Q(o, a ; \boldsymbol{\theta})$.</p>
<p>Template Action Space. Template action space considers actions satisfying decomposition in the form of $\left\langle\right.$ verb, arg $\left.<em 1="1">{0}, \operatorname{arg}</em>}\right\rangle$. verb is an interchangeable verb phrase template with placeholders for objects and $\operatorname{arg<em 1="1">{0}$ and $\operatorname{arg}</em>$ are optional objects. For example, the action command [east], [pick up eggs] and [break window with stone] can be represented as template actions $\langle$ east, none, none $\rangle$, $\langle$ pick up OBJ, eggs, none and $\langle$ break OBJ with OBJ, window, stone $\rangle$. We reuse the template library and object list from Jericho. The verb phrases usually consist of several vocabulary words and each object is usually a single word.</p>
<h3>3.2 RC Model for Template Actions</h3>
<p>We parameterize the observation-action value function $Q\left(o, a=\left\langle\right.\right.$ verb, $\left.\left.\operatorname{arg}<em 1="1">{0}, \operatorname{arg}</em>}\right\rangle ; \boldsymbol{\theta}\right)$ by utilizing the decomposition of the template actions and contextquery contextualized representation in RC. Our model treats the observation $o$ as a context in RC and the $\operatorname{verb}=\left(v_{1}, v_{2}, \ldots, v_{k}\right)$ component of the template actions as a query. Then a verb-aware observation representation is derived via a RC reader model with Bidirectional Attention Flow (BiDAF) (Seo et al., 2016) and self-attention. The observation representation responding to the $\operatorname{arg<em 1="1">{0}$ and $\operatorname{arg}</em>}$ words are pooled and projected to a scalar value estimate for $Q\left(o, a=\left\langle\right.\right.$ verb, $\left.\left.\operatorname{arg<em 1="1">{0}, \operatorname{arg}</em>\right)$. A high-level model architecture of our model is illustrated in Figure 3.}\right\rangle ; \boldsymbol{\theta</p>
<p>Observation and verb Representation. We tokenize the observation and the verb phrase into words, then embed these words using pre-trained GloVe embeddings (Pennington et al., 2014). A shared encoder block that consists of LayerNorm (Ba et al., 2016) and Bidirectional GRU (Cho et al., 2014) processes the observation and verb word embeddings to obtain the separate observation and verb representation.</p>
<p>Observation-verb Interaction Layers. Given the separate observation and verb representation, we apply two attention mechanisms to compute a verb-contextualized observation representation. We first apply BiDAF with observation as the context input and verb as the query input. Specifically, we denote the processed embeddings for observation word $i$ and template word $j$ as $\boldsymbol{o}<em j="j">{i}$ and $\boldsymbol{t}</em>}$. The attention between the two words is then $a_{i j}=\boldsymbol{w<em _boldsymbol_i="\boldsymbol{i">{\mathbf{1}} \cdot \boldsymbol{o}</em>}}+\boldsymbol{w<em _boldsymbol_j="\boldsymbol{j">{\mathbf{2}} \cdot \boldsymbol{t}</em>}}+\boldsymbol{w<em _boldsymbol_i="\boldsymbol{i">{\mathbf{3}} \cdot\left(\boldsymbol{o}</em>}} \otimes \boldsymbol{t<em _mathbf_1="\mathbf{1">{\boldsymbol{j}}\right)$, where $\boldsymbol{w}</em>}}, \boldsymbol{w<em _mathbf_3="\mathbf{3">{\mathbf{2}}$, $\boldsymbol{w}</em>}}$ are learnable vectors and $\otimes$ is element-wise product. We then compute the "verb2observation" attention vector for the $i$-th observation word as $\boldsymbol{c<em j="j">{\boldsymbol{i}}=\sum</em>} p_{i j} \boldsymbol{t<em i="i" j="j">{\boldsymbol{j}}$ with $p</em>}=\exp \left(a_{i j}\right) / \sum_{j} \exp \left(a_{i j}\right)$. Similarly, we compute the "observation2verb" attention vector as $\boldsymbol{q}=\sum_{i} p_{i} \boldsymbol{o<em i="i">{\boldsymbol{i}}$ with $p</em>=$ $\exp \left(\max <em i="i" j="j">{j} a</em> \exp \left(\max }\right) / \sum_{i<em i="i" j="j">{j} a</em>}\right)$. We concatenate and project the output vectors as $\boldsymbol{w<em _boldsymbol_i="\boldsymbol{i">{\mathbf{4}} \cdot\left[\boldsymbol{o}</em>}}\right.$, $\boldsymbol{c<em _boldsymbol_i="\boldsymbol{i">{\boldsymbol{i}}, \boldsymbol{o}</em>}} \otimes \boldsymbol{c<em _boldsymbol_i="\boldsymbol{i">{\boldsymbol{i}}, \boldsymbol{q} \otimes \boldsymbol{c}</em> \mid$, followed by a linear layer with leaky ReLU activation units (Maas et al., 2013). The output vectors are processed by an encoder block. We then apply a residual self-attention on the outputs of the encoder block. The self-attention is the same as BiDAF, but only between the observation and itself.}</p>
<p>Observation-Action Value Prediction. We generate an action by replacing the placeholders $\left(\operatorname{arg}<em 1="1">{0}\right.$ and $\left.\operatorname{arg}</em>}\right)$ in a template with objects appearing in the observation. The observation-action value $Q\left(o, a=\left\langle\right.\right.$ verb, $\left.\left.\operatorname{arg<em m="m">{0}=\operatorname{obj}</em>}, \operatorname{arg<em n="n">{1}=\operatorname{obj}</em>}\right\rangle ; \theta\right)$ is achieved by processing each object's corresponding verb-contextualized observation representation. Specifically, we get the indices of an $o b j$ in the observation texts $I(o b j, o)$. When the object is a noun phrase, we take the index of its headword. ${ }^{2}$ Because the same object has different meanings when it replaces different placeholders, we apply two GRU-based embedding functions for the two placeholders, to get the object's verb-placeholder dependent embeddings. We derive a single vector representation $\boldsymbol{h<em 0="0">{\text {org }</em>}=\text { obj <em 0="0">{m}}$ for the case that the placeholder $\operatorname{arg}</em>}$ is replaced by $o b j_{m}$ by meanpooling over the verb-placeholder dependent embeddings indexed by $I\left(o b j_{m}, o\right)$ for the corresponding placeholder $\operatorname{arg<em 5="5">{0}$. We apply a linear transformation on the concatenated embeddings of the two placeholders to obtain the observation action value $Q(o, a)=\boldsymbol{w}</em>} \cdot\left[\boldsymbol{h<em 1="1">{\text {org }</em>}=\text { obj <em _org="{org" _text="\text">{m}}, \boldsymbol{h}</em><em m="m">{1}=\text { obj }</em>}}\right]$ for $a=\langle$ verb, $\left.\operatorname{arg<em m="m">{0}=\operatorname{obj}</em>}, \operatorname{arg<em n="n">{1}=\operatorname{obj}</em>\right\rangle$. Our formulation avoids the repeated computation overhead among different actions with a shared template verb phrase.</p>
<h3>3.3 Multi-Paragraph Retrieval Method for Partial Observability</h3>
<p>The observation at the current step sometimes does not have full-textual evidence to support action selection and value estimation, due to the inherent partial observability of IF games. For example, when repeatedly attacking a troll with a sword, the player needs to know the effect or feedback of the last attack to determine if an extra attack is necessary. It is thus important for an agent to efficiently utilize historical observations to better support action value prediction. In our RC-based action prediction model, the historical observation utilization can be formulated as selecting evidential observation paragraphs in history, and predicting the action values from multiple selected observations, namely a Multiple-Paragraph Reading Comprehension (MPRC) problem. We propose to retrieve past observations with an object-centric approach.</p>
<p>Past Observation Retrieval. Multiple past observations may share objects with the current obser-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Agents</th>
<th>Action strategy</th>
<th>State strategy</th>
<th>Interaction data</th>
</tr>
</thead>
<tbody>
<tr>
<td>TDQN</td>
<td>Independent selection of template and the <br> two objects</td>
<td>None</td>
<td>1 M</td>
</tr>
<tr>
<td>DRRN</td>
<td>Action as a word sequence without distin- <br> guishing the roles of verbs and objects</td>
<td>None</td>
<td>1 M</td>
</tr>
<tr>
<td>KG-A2C</td>
<td>Recurrent neural decoder that selects the <br> template and objects in a fixed order</td>
<td>Object graph from historical observations <br> based on OpenIE and human-written rules</td>
<td>1.6 M</td>
</tr>
<tr>
<td>Ours</td>
<td>Observation-template representation for <br> object-centric value prediction</td>
<td>Object-based history observation retrieval</td>
<td>0.1 M</td>
</tr>
</tbody>
</table>
<p>Table 1: Summary of the main technical differences between our agent and the baselines. All agents use DQN to update the model parameters except KG-A2C uses A2C. All agents use the same handicaps.
vation, and it is computationally expensive and unnecessary to retrieve all of such observations. The utility of past observations associated with each object is often time-sensitive in that new observations may entirely or partially invalidate old observations. We thus propose a time-sensitive strategy for retrieving past observations. Specifically, given the detected objects from the current observation, we retrieve the most recent $K$ observations with at least one shared object. The $K$ retrieved observations are sorted by time steps and concatenated to the current observation. The observations from different time steps are separated by a special token. Our RC-based action prediction model treats the concatenated observations as the observation inputs, and no other parts are changed. We use the notation $o_{t}$ to represent the current observation and the extended current observation interchangeably.</p>
<h3>3.4 Training Loss</h3>
<p>We apply the Deep Q-Network (DQN) (Mnih et al., 2015) to update the parameters $\boldsymbol{\theta}$ of our RC-based action prediction model. The loss function is:</p>
<p>$$
\begin{aligned}
\mathcal{L}(\theta)=\mathbf{E}<em t="t">{\left(o</em> ; \theta\right)\right.\right. \
&amp; \left.\left.-\left(r_{t}+\gamma \max }, a_{t}, r_{t}, o_{t+1}\right) \sim \rho(\mathcal{D})} &amp; \left[\left|Q\left(o_{t}, a_{t<em t_1="t+1">{b} Q\left(o</em>\right)\right)\right|\right]
\end{aligned}
$$}, b ; \theta^{-</p>
<p>where $\mathcal{D}$ is the experience replay consisting of recent gameplay transition records and $\rho$ is a distribution over the transitions defined by a sampling strategy.</p>
<p>Prioritized Trajectories. The distribution $\rho$ has a decent impact on DQN performance. Previous work samples transition tuples with immediate positive rewards more frequently to speed up learning (Narasimhan et al., 2015; Hausknecht et al., 2019a). We observe that this heuristic is often insufficient. Some transitions with zero immediate
rewards or even negative rewards are also indispensable in recovering well-performed trajectories. We thus extend the strategy from transition level to trajectory level. We prioritize transitions from trajectories that outperform the exponential moving average score of recent trajectories.</p>
<h2>4 Experiments</h2>
<p>We evaluate our proposed methods on the suite of Jericho supported games. We compared to all previous baselines that include recent methods addressing the huge action space and partial observability challenges.</p>
<h3>4.1 Setup</h3>
<p>Jericho Handicaps and Configuration. The handicaps used by our methods are the same as other baselines. First, we use the Jericho API to check if an action is valid with game-specific templates. Second, we augmented the observation with the textual feedback returned by the command [inventory] and [look]. Previous work also included the last action or game score as additional inputs. Our model discarded these two types of inputs as we did not observe a significant difference by our model. The maximum game step number is set to 100 following baselines.</p>
<p>Implementation Details. We apply spaCy ${ }^{3}$ to tokenize the observations and detect the objects in the observations. We use the 100-dimensional GloVe embeddings as fixed word embeddings. The out-of-vocabulary words are mapped to a randomly initialized embedding. The dimension of Bi-GRU hidden states is 128 . We set the observation representation dimension to be 128 throughout the model. The history retrieval window $K$ is 2 . For DQN configuration, we use the $\epsilon$-greedy strategy</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Game</th>
<th style="text-align: center;">Human <br> Max</th>
<th style="text-align: center;">Walkthrough-100</th>
<th style="text-align: center;">TDQN</th>
<th style="text-align: center;">Baselines <br> DRRN</th>
<th style="text-align: center;">KG-A2C</th>
<th style="text-align: center;">Ours</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MPRC-DQN</td>
<td style="text-align: center;">RC-DQN</td>
</tr>
<tr>
<td style="text-align: center;">905</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">acorncourt</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">advent</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">113</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">36</td>
</tr>
<tr>
<td style="text-align: center;">adventureland</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">21.7</td>
</tr>
<tr>
<td style="text-align: center;">afflicted</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: center;">anchor</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">awaken</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">balances</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">deephome</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">detective</td>
<td style="text-align: center;">360</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">169</td>
<td style="text-align: center;">197.8</td>
<td style="text-align: center;">207.9</td>
<td style="text-align: center;">317.7</td>
<td style="text-align: center;">291.3</td>
</tr>
<tr>
<td style="text-align: center;">dragon</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">$-5.3$</td>
<td style="text-align: center;">$-3.5$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">4.84</td>
</tr>
<tr>
<td style="text-align: center;">enchanter</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: center;">gold</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">inhumane</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">jewel</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">4.46</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;">karn</td>
<td style="text-align: center;">170</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">library</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">18.1</td>
</tr>
<tr>
<td style="text-align: center;">ludicorp</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">17.0</td>
</tr>
<tr>
<td style="text-align: center;">moonlit</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">omniquest</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">pentari</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">43.8</td>
</tr>
<tr>
<td style="text-align: center;">reverb</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;">snacktime</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">sorcerer</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">38.3</td>
</tr>
<tr>
<td style="text-align: center;">spellbrkr</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">160</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: center;">spirit</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">5.2</td>
</tr>
<tr>
<td style="text-align: center;">temple</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: center;">tryst205</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">yomomma</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;">zenon</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">zork1</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">38.8</td>
</tr>
<tr>
<td style="text-align: center;">zork3</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$3^{a}$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">3.63</td>
<td style="text-align: center;">2.83</td>
</tr>
<tr>
<td style="text-align: center;">ztuu</td>
<td style="text-align: center;">$100^{b}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">79.1</td>
</tr>
<tr>
<td style="text-align: center;">Winning percentage / counts</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">24\%/8</td>
<td style="text-align: center;">30\%/10</td>
<td style="text-align: center;">27\%/9</td>
<td style="text-align: center;">64\%/21</td>
<td style="text-align: center;">52\%/17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">76\%/25</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Average game scores on Jericho benchmark games. The best performing agent score per game is in bold.
The Winning percentage / counts row computes the percentage / counts of games that the corresponding agent is best. The scores of baselines are from their papers. The missing scores are represented as "-", for which games KG-A2C skipped. We also added the 100 -step results from a human-written game-playing walkthrough, as a reference of human-level scores. We denote the difficulty levels of the games defined in the original Jericho paper with colors in their names - possible (i.e., easy or normal) games in green color, difficult games in tan and extreme games in red. Best seen in color.
a Zork3 walkthrough does not maximize the score in the first 100 steps but explores more. ${ }^{b}$ Our agent discovers some unbounded reward loops in the game Ztuu.
for exploration, annealing $\epsilon$ from 1.0 to $0.05 . \gamma$ is 0.98 . We use Adam to update the weights with $10^{-4}$ learning rate. Other parameters are set to their default values. More details of the Reproducibility Checklist is in Appendix A.</p>
<p>Baselines. We compare with all the public results on the Jericho suite, namely TDQN (Hausknecht et al., 2019a), DRRN (He et al., 2016), and KGA2C (Ammanabrolu and Hausknecht, 2020). As discussed, our approaches differ from them mainly in the strategies of handling the large action space and partial observability of IF games. We summarize these main technical differences in Table 1. In summary, all previous agents predict actions con-
ditioned on a single vector representation of the whole observation texts. Thus they do not exploit the fine-grained interplay among the template components and the observations. Our approach addresses this problem by formulating action prediction as an RC task, better utilizing the rich textual observations with deeper language understanding.</p>
<p>Training Sample Efficiency. We update our models for 100, 000 times. Our agents interact with the environment one step per update, resulting in a total of 0.1 M environment interaction data. Compared to the other agents, such as KG-A2C (1.6M), TDQN (1M), and DRRN (1M), our environment interaction data is significantly smaller.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Game</th>
<th style="text-align: center;">Template Action <br> Space $\left(\times 10^{6}\right)$</th>
<th style="text-align: center;">Avg. Steps <br> Per Reward</th>
<th style="text-align: center;">Dialog <br> Actions</th>
<th style="text-align: center;">Darkness <br> Limit</th>
<th style="text-align: center;">Nonstandard</th>
<th style="text-align: center;">Inventory</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">advent</td>
<td style="text-align: center;">107</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">detective</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">karn</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ludicorp</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">pentari</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">spirit</td>
<td style="text-align: center;">195</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">zork3</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 3: Difficulty levels and characteristics of games on which our approach achieves the most considerable improvement. Dialog indicates that it is necessary to speak with another character. Darkness indicates that accessing some dark areas requires a light source. Nonstandard Actions refers to actions with words not in an English dictionary. Inventory Limit restricts the number of items carried by the player. Please refer to (Hausknecht et al., 2019a) for more comprehensive definitions.</p>
<h3>4.2 Overall Performance</h3>
<p>We summarize the performance of our MultiParagraph Reading Comprehension DQN (MPRCDQN) agent and baselines in Table 2. Of the 33 IF games, our MPRC-DQN achieved or improved the state of the art performance on 21 games (i.e., a winning rate of $64 \%$ ). The best performing baseline (DRRN) achieved the state-of-the-art performance on only ten games, corresponding to the winning rate of $30 \%$, lower than half of ours. Note that all the methods achieved the same initial scores on five games, namely 905, anchor, awaken, deephome, and moonlit. Apart from these five games, our MPRC-DQN achieved more than three times wins. Our MPRC-DQN achieved significant improvement on some games, such as adventureland, afflicted, detective, etc. Appendix C shows some game playing trajectories.</p>
<p>We include the performance of an RC-DQN agent, which implements our RC-based action prediction model but only takes the current observations as inputs. It also outperformed the baselines by a large margin. After we consider the RC-DQN agent, our MPRC-DQN still has the highest winning percentage, indicating that our RC-based action prediction model has a significant impact on the performance improvement of our MPRC-DQN and the improvement from the multi-passage retrieval is also unneglectable. Moreover, compared to RC-DQN, our MPRC-DQN has another advantage of faster convergence. The learning curves of our MPRC-DQN and RC-DQN agents on various games are in Appendix B.</p>
<p>Finally, our approaches, overall, achieve the new state-of-the-art on 25 games (i.e., a winning rate of $76 \%$ ), giving a significant advance in the field of IF game playing.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Competitors</th>
<th style="text-align: center;">Win</th>
<th style="text-align: center;">Draw</th>
<th style="text-align: center;">Lose</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MPRC-DQN v.s. TDQN</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">MPRC-DQN v.s. DRRN</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">MPRC-DQN v.s. KG-A2C</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>Table 4: Pairwise comparison between our MPRC-DQN versus each baseline.</p>
<p>Pairwise Competition. To better understand the performance difference between our approach and each of the baselines, we adopt a direct one-to-one comparison metric based on the results from Table 2. Our approach has a high winning rate when competing with any of the baselines, summarized in Table 4. All the baselines have a rare chance to beat us on games. DRRN gives a higher chance of draw-games when competing with ours.</p>
<p>Human-Machine Gap. We additionally compare IF gameplay agents to human players to better understand the improvement significance and the potential improvement upper-bound. We measure each agent's game progress as the macro-average of the normalized agent-to-human game score ratios, capped at $100 \%$. The progress of our MPRCDQN is $28.5 \%$, while the best performing baseline DRRN is $17.8 \%$, showing that our agent's improvement is significant even in the realm of human players. Nevertheless, there is a vast gap between the learning agents and human players. The gap indicates IF games can be a good benchmark for the development of natural language understanding techniques.</p>
<p>Difficulty Levels of Games. Jericho categorizes the supported games into three difficulty levels, namely possible games, difficult games, and extreme games, based on the characteristics of the game dynamics, such as the action space size, the length of the game, and the average number of</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Learning curves for ablative studies. (left) Model ablative studies on the game Detective. (middle) Model ablative studies on Zork1. (right) Retrieval strategy study on Zork1. Best seen in color.</p>
<p>Steps to receive a non-zero reward. Our approach improves over prior art on seven of the sixteen possible games, seven of the eleven difficult games, and three of the six extreme games in Table 2. It shows that the strategies of our method are generally beneficial for any difficulty levels of game dynamics. Table 3 summarizes the characteristics of the seven games in which our method improves the most, i.e., larger than 15% of the game progress in the first 100 steps.4 First, these mostly improved games have medium action space sizes, and it is an advantageous setting for our methods where modeling the template-object-observation interactions is effective. Second, our approach improves most on games with a reasonably high degree of reward sparsity, such as <em>karn</em>, <em>spirit</em>, and <em>zork3</em>, indicating that our RC-based value function formulation helps in optimization and mitigates the reward sparsity. Finally, we remark that these game difficulty levels are not directly categorized based on natural language-related characteristics, such as text comprehension and puzzle-solving difficulties. Future studies on additional game categories based on those natural language-related characteristics would shed light on related improvements.</p>
<h3>4.3 Ablative Studies</h3>
<p><strong>RC-model Design.</strong> The overall results show that our RC-model plays a critical role in performance improvement. We compare our RC-model to some alternative models as ablative studies. We consider three alternatives, namely (1) our RC-model without the self-attention component (w/o self-att), (2) without the argumentspecific embedding (w/o arg-emb) and (3) our RC-model with Transformer-based block encoder (RC-Trans) following QANet (Yu et al., 2018). Detailed architecture is in Appendix A.</p>
<p>The learning curves for different RC-models are in Figure 4 (left/middle). The RC-models without either self-attention or argument-specific embedding degenerate, and the argument-specific embedding has a greater impact. The Transformer-based encoder block sometimes learns faster than Bi-GRU at the early learning stage. It achieved a comparable final performance, even with much greater computational resource requirements.</p>
<p><strong>Retrieval Strategy.</strong> We compare with history retrieval strategies with different history sizes (K) and pure recency-based strategies (i.e., taking the latest K observations as history, denoted as w/o rec). The learning curves of different strategies are in Figure 4 (right). In general, the impact of history window size is highly game-dependent, but the pure recency based ones do not differ significantly from RC-DQN at the beginning of learning. The issues of pure recency based strategy are: (1) limited additional information about objects provided by successive observations; and (2) higher variance of retrieved observations due to policy changes.</p>
<h3>5 Conclusion</h3>
<p>We formulate the general IF game playing as MPRC tasks, enabling an MPRC-style solution to efficiently address the key IF game challenges on the huge combinatorial action space and the partial observability in a unified framework. Our approaches achieved significant improvement over the previous state-of-the-art on both game scores and training data efficiency. Our formulation also bridges broader NLU/RC techniques to address other critical challenges in IF games for future work, e.g., common-sense reasoning, novelty-driven exploration, and multi-hop inference.</p>
<h3>Acknowledgments</h3>
<p>We would like to thank Matthew Hausknecht for helpful discussions on the Jericho environments.</p>
<p><sup>4</sup>We ignore <em>ztuu</em> due to the infinite reward loops.</p>
<h2>References</h2>
<p>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Ct, Mikul Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L Hamilton. 2020. Learning dynamic knowledge graphs to generalize on textbased games. arXiv preprint arXiv:2002.09127.</p>
<p>Prithviraj Ammanabrolu and Matthew Hausknecht. 2020. Graph constrained reinforcement learning for natural language action spaces. arXiv, pages arXiv2001.</p>
<p>Prithviraj Ammanabrolu and Mark Riedl. 2019. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3557-3565.</p>
<p>Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2019. Learning to retrieve reasoning paths over wikipedia graph for question answering. arXiv preprint arXiv:1911.10470.</p>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18701879 .</p>
<p>Kyunghyun Cho, Bart Van Merrinboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.</p>
<p>Marc-Alexandre Ct, kos Kdr, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. 2018. Textworld: A learning environment for text-based games. In Workshop on Computer Games, pages 41-75. Springer.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. 2019. Cognitive graph for multi-hop reading comprehension at scale. In Proceedings of ACL 2019.</p>
<p>Ameya Godbole, Dilip Kavarthapu, Rajarshi Das, Zhiyu Gong, Abhishek Singhal, Hamed Zamani, Mo Yu, Tian Gao, Xiaoxiao Guo, Manzil Zaheer, et al. 2019. Multi-step entity-centric information retrieval for multi-hop question answering. arXiv preprint arXiv:1909.07598.</p>
<p>Matthew Hausknecht, Prithviraj Ammanabrolu, MarcAlexandre Ct, and Xingdi Yuan. 2019a. Interactive fiction games: A colossal adventure. arXiv preprint arXiv:1909.05398.</p>
<p>Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D Williams. 2019b. Nail: A general interactive fiction agent. arXiv preprint arXiv:1902.04259.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. 2016. Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1621-1630.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466.</p>
<p>Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. 2018. Denoising distantly supervised open-domain question answering. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17361745 .</p>
<p>Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. 2013. Rectifier nonlinearities improve neural network acoustic models. In Proc. icml, volume 30, page 3 .</p>
<p>Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019a. A discrete hard em approach for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 28442857.</p>
<p>Sewon Min, Danqi Chen, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019b. Knowledge guided text retrieval and reading for open domain question answering. arXiv preprint arXiv:1911.03868.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533.</p>
<p>Xiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui Su. 2020. Frustratingly hard evidence retrieval for qa over books. In Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events, pages 108-113.</p>
<p>Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. 2015. Language understanding for text-based games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1-11.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543.</p>
<p>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of NAACL-HLT, pages 2227-2237.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392.</p>
<p>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention flow for machine comprehension.</p>
<p>Shuohang Wang and Jing Jiang. 2016. Machine comprehension using match-lstm and answer pointer. arXiv preprint arXiv:1608.07905.</p>
<p>Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. 2018. R 3: Reinforced ranker-reader for open-domain question answering. In Thirty-Second AAAI Conference on Artificial Intelligence.</p>
<p>Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. 2017. Evidence aggregation for answer re-ranking in open-domain question answering. arXiv preprint arXiv:1711.05116.</p>
<p>Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541.</p>
<p>Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. 2018. Learn what not to learn: Action elimination with deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 3562-3573.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://spacy.io&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8777 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8777</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8777</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278535346</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.08504v1.pdf" target="_blank">Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding</a></p>
                <p><strong>Paper Abstract:</strong> Sequence-to-sequence models are widely used to train Abstract Meaning Representation (Banarescu et al., 2013, AMR) parsers. To train such models, AMR graphs have to be linearized into a one-line text format. While Penman encoding is typically used for this purpose, we argue that it has limitations: (1) for deep graphs, some closely related nodes are located far apart in the linearized text (2) Penman's tree-based encoding necessitates inverse roles to handle node re-entrancy, doubling the number of relation types to predict. To address these issues, we propose a triple-based linearization method and compare its efficiency with Penman linearization. Although triples are well suited to represent a graph, our results suggest room for improvement in triple encoding to better compete with Penman's concise and explicit representation of a nested graph structure.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8777.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8777.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Penman</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Penman linearization (Penman encoding) for AMR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-like textual serialization of AMR graphs using node variables and nested parentheses; commonly used to train sequence-to-sequence AMR parsers and used here as the baseline representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Penman linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represents an AMR graph as a nested tree-like string: nodes are assigned variables (node IDs), node instances and relations are written using parentheses to express nesting, and inverse relations (':rel-of') are introduced to preserve tree structure when a node has multiple parents (reentrancy).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) semantic graphs (rooted, directed, labeled graphs with possible re-entrancies)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize the AMR graph into a bracketed, variable-based tree form (Penman format) where each node is introduced with a variable and instance, and nested substructures are encoded via parentheses; re-entrancies are handled by duplicating nodes and using inverse roles (':relation-of').</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Sequence-to-sequence AMR parsing (train a seq2seq model to output linearized AMR text which is converted back to a graph)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SMATCH score. Reported: the best Penman single-task model (Penman_O_var_O_invrole, i.e., variables and inverse roles retained) achieved 77.0 SMATCH on The Little Prince test set and 80.9 SMATCH on the AMR 3.0 test set (the highest scores reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Directly compared to triple-based linearizations in this work: Penman outperforms triple-based linearization overall (best Penman model > best triple models), with the gap described as marginal in some configurations (≤ ~1 SMATCH for some triple variants). Penman's concise nested representation provides structural information that benefitted learning more than triple formats in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Concise and explicit encoding of nested graph structure, effective at encoding nesting and hierarchical relations, empirically produced the best overall SMATCH scores in this study; preserves structural signals useful to seq2seq models.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires inverse roles to represent re-entrancy which increases the number of relation types to predict; can place graph nodes that are close in the graph far apart in the linearized text (parent-child distance can be large when siblings have deep subgraphs), potentially making some local relations harder to learn.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May place parent and child tokens far apart in the linearized sequence when preceding sibling nodes include deep subgraphs, potentially harming learning of some parent-child relations; inverse-role representation doubles relation-type variants and may complicate learning in some setups. In this paper, however, these issues did not lead to worse empirical performance than triples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8777.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8777.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triple</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Triple-based linearization (triple encoding) for AMR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-text representation that encodes AMR as a sequence of (parent, relation, child) triples so parent and child tokens are adjacent and inverse roles are eliminated by reversing triple order.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Triple linearization (triple-based encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent the entire AMR graph as a set/sequence of binary triples (parent node, relation, child node). Triples are serialized into a single-line text (triples separated by a delimiter) and inverse roles are removed by rewriting (A, rel-of, B) as (B, rel, A); variables may be retained or removed depending on strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) semantic graphs (general graph structure with re-entrancies and possibly multiple roots)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Extract all graph triples and unfold them in depth-first search order using the PENMAN library; serialize triples into a token sequence (triples separated by a pipe '|' in the authors' implementation). Four linearization strategies vary whether node variables are kept or removed and whether inverse roles are retained or converted.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Sequence-to-sequence AMR parsing (train seq2seq model to output triple-serialized AMRs which are then converted back to graph form)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SMATCH score. Reported: among single-task triple models, Tri_O_var_O_invrole (variables and inverse roles retained) performed best; the best triple models had SMATCH scores within ~1 point of the best Penman model in some settings, but specific numeric triple scores are not fully enumerated in text—authors note a marginal gap (≤1 SMATCH) for the best triple configuration compared to best Penman on aggregate tests. Triple models performed worse on longer graphs and did not improve scores for deeper graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared head-to-head with Penman: triples were hypothesized to help by bringing parent-child tokens adjacent and removing inverse roles, but empirical results showed Penman generally outperformed triple encodings. Multi-task experiments indicate Penman as auxiliary task helps triple learning, whereas triple as auxiliary hurts Penman.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Keeps parent and child node tokens adjacent in the linearized sequence which could simplify learning local parent-child relations; naturally aligns with an explicit graph representation (no need to force a tree), eliminates inverse roles by reversing triple order which reduces the number of relation variants (authors report reducing relation-type count from 155 to 115 when reversing/removing inverse roles). Flexible for graphs with re-entrancies or multiple roots.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Can be verbose (longer linearized sequences), increasing sequence length and slowing learning; less effective at capturing nested hierarchical structure compared to Penman; removing variables (a common preprocessing) causes information loss which harms performance; inconsistent effects of inverse-role removal across different datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Did not improve learning for deeper graphs (no observed benefit across graph depth buckets); degraded more on longer graphs (performance gap widens as linearized graph token count increases) — verbose triple representation likely causes performance degradation for long graphs; removing variables (to reduce sparsity) caused performance drops, indicating information loss; triple encoding alone failed to surpass Penman in experiments despite adjacency benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating scoped meaning representations <em>(Rating: 2)</em></li>
                <li>Exploring neural methods for parsing discourse representation structures <em>(Rating: 2)</em></li>
                <li>REBEL: Relation extraction by end-to-end language generation <em>(Rating: 2)</em></li>
                <li>Contrastive triple extraction with generative transformer <em>(Rating: 2)</em></li>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Abstract Meaning Representation for sembanking <em>(Rating: 2)</em></li>
                <li>Neural semantic parsing by character-based translation: Experiments with abstract meaning representations <em>(Rating: 2)</em></li>
                <li>Abstract Meaning Representation for grounded human-robot communication <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8777",
    "paper_id": "paper-278535346",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Penman",
            "name_full": "Penman linearization (Penman encoding) for AMR",
            "brief_description": "A tree-like textual serialization of AMR graphs using node variables and nested parentheses; commonly used to train sequence-to-sequence AMR parsers and used here as the baseline representation.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Penman linearization",
            "representation_description": "Represents an AMR graph as a nested tree-like string: nodes are assigned variables (node IDs), node instances and relations are written using parentheses to express nesting, and inverse relations (':rel-of') are introduced to preserve tree structure when a node has multiple parents (reentrancy).",
            "graph_type": "Abstract Meaning Representation (AMR) semantic graphs (rooted, directed, labeled graphs with possible re-entrancies)",
            "conversion_method": "Serialize the AMR graph into a bracketed, variable-based tree form (Penman format) where each node is introduced with a variable and instance, and nested substructures are encoded via parentheses; re-entrancies are handled by duplicating nodes and using inverse roles (':relation-of').",
            "downstream_task": "Sequence-to-sequence AMR parsing (train a seq2seq model to output linearized AMR text which is converted back to a graph)",
            "performance_metrics": "SMATCH score. Reported: the best Penman single-task model (Penman_O_var_O_invrole, i.e., variables and inverse roles retained) achieved 77.0 SMATCH on The Little Prince test set and 80.9 SMATCH on the AMR 3.0 test set (the highest scores reported in the paper).",
            "comparison_to_others": "Directly compared to triple-based linearizations in this work: Penman outperforms triple-based linearization overall (best Penman model &gt; best triple models), with the gap described as marginal in some configurations (≤ ~1 SMATCH for some triple variants). Penman's concise nested representation provides structural information that benefitted learning more than triple formats in experiments.",
            "advantages": "Concise and explicit encoding of nested graph structure, effective at encoding nesting and hierarchical relations, empirically produced the best overall SMATCH scores in this study; preserves structural signals useful to seq2seq models.",
            "disadvantages": "Requires inverse roles to represent re-entrancy which increases the number of relation types to predict; can place graph nodes that are close in the graph far apart in the linearized text (parent-child distance can be large when siblings have deep subgraphs), potentially making some local relations harder to learn.",
            "failure_cases": "May place parent and child tokens far apart in the linearized sequence when preceding sibling nodes include deep subgraphs, potentially harming learning of some parent-child relations; inverse-role representation doubles relation-type variants and may complicate learning in some setups. In this paper, however, these issues did not lead to worse empirical performance than triples.",
            "uuid": "e8777.0",
            "source_info": {
                "paper_title": "Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Triple",
            "name_full": "Triple-based linearization (triple encoding) for AMR",
            "brief_description": "A graph-to-text representation that encodes AMR as a sequence of (parent, relation, child) triples so parent and child tokens are adjacent and inverse roles are eliminated by reversing triple order.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Triple linearization (triple-based encoding)",
            "representation_description": "Represent the entire AMR graph as a set/sequence of binary triples (parent node, relation, child node). Triples are serialized into a single-line text (triples separated by a delimiter) and inverse roles are removed by rewriting (A, rel-of, B) as (B, rel, A); variables may be retained or removed depending on strategy.",
            "graph_type": "Abstract Meaning Representation (AMR) semantic graphs (general graph structure with re-entrancies and possibly multiple roots)",
            "conversion_method": "Extract all graph triples and unfold them in depth-first search order using the PENMAN library; serialize triples into a token sequence (triples separated by a pipe '|' in the authors' implementation). Four linearization strategies vary whether node variables are kept or removed and whether inverse roles are retained or converted.",
            "downstream_task": "Sequence-to-sequence AMR parsing (train seq2seq model to output triple-serialized AMRs which are then converted back to graph form)",
            "performance_metrics": "SMATCH score. Reported: among single-task triple models, Tri_O_var_O_invrole (variables and inverse roles retained) performed best; the best triple models had SMATCH scores within ~1 point of the best Penman model in some settings, but specific numeric triple scores are not fully enumerated in text—authors note a marginal gap (≤1 SMATCH) for the best triple configuration compared to best Penman on aggregate tests. Triple models performed worse on longer graphs and did not improve scores for deeper graphs.",
            "comparison_to_others": "Compared head-to-head with Penman: triples were hypothesized to help by bringing parent-child tokens adjacent and removing inverse roles, but empirical results showed Penman generally outperformed triple encodings. Multi-task experiments indicate Penman as auxiliary task helps triple learning, whereas triple as auxiliary hurts Penman.",
            "advantages": "Keeps parent and child node tokens adjacent in the linearized sequence which could simplify learning local parent-child relations; naturally aligns with an explicit graph representation (no need to force a tree), eliminates inverse roles by reversing triple order which reduces the number of relation variants (authors report reducing relation-type count from 155 to 115 when reversing/removing inverse roles). Flexible for graphs with re-entrancies or multiple roots.",
            "disadvantages": "Can be verbose (longer linearized sequences), increasing sequence length and slowing learning; less effective at capturing nested hierarchical structure compared to Penman; removing variables (a common preprocessing) causes information loss which harms performance; inconsistent effects of inverse-role removal across different datasets.",
            "failure_cases": "Did not improve learning for deeper graphs (no observed benefit across graph depth buckets); degraded more on longer graphs (performance gap widens as linearized graph token count increases) — verbose triple representation likely causes performance degradation for long graphs; removing variables (to reduce sparsity) caused performance drops, indicating information loss; triple encoding alone failed to surpass Penman in experiments despite adjacency benefits.",
            "uuid": "e8777.1",
            "source_info": {
                "paper_title": "Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating scoped meaning representations",
            "rating": 2,
            "sanitized_title": "evaluating_scoped_meaning_representations"
        },
        {
            "paper_title": "Exploring neural methods for parsing discourse representation structures",
            "rating": 2,
            "sanitized_title": "exploring_neural_methods_for_parsing_discourse_representation_structures"
        },
        {
            "paper_title": "REBEL: Relation extraction by end-to-end language generation",
            "rating": 2,
            "sanitized_title": "rebel_relation_extraction_by_endtoend_language_generation"
        },
        {
            "paper_title": "Contrastive triple extraction with generative transformer",
            "rating": 2,
            "sanitized_title": "contrastive_triple_extraction_with_generative_transformer"
        },
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Abstract Meaning Representation for sembanking",
            "rating": 2,
            "sanitized_title": "abstract_meaning_representation_for_sembanking"
        },
        {
            "paper_title": "Neural semantic parsing by character-based translation: Experiments with abstract meaning representations",
            "rating": 2,
            "sanitized_title": "neural_semantic_parsing_by_characterbased_translation_experiments_with_abstract_meaning_representations"
        },
        {
            "paper_title": "Abstract Meaning Representation for grounded human-robot communication",
            "rating": 1,
            "sanitized_title": "abstract_meaning_representation_for_grounded_humanrobot_communication"
        }
    ],
    "cost": 0.0079825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding</p>
<p>Jeongwoo Kang 
LIG
Univ. Grenoble Alpes
CNRS
Grenoble INP
38000GrenobleFrance</p>
<p>Maximin Coavoux 
LIG
Univ. Grenoble Alpes
CNRS
Grenoble INP
38000GrenobleFrance</p>
<p>Cédric Lopez 
Immeuble Le 610, 10 Rue Louis Breguet Bâtiment D34830Emvista, JacouFrance</p>
<p>Didier Schwab 
LIG
Univ. Grenoble Alpes
CNRS
Grenoble INP
38000GrenobleFrance</p>
<p>Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding
F375DA699FC2C1F389AF90B9C9C0DED7
Sequence-to-sequence models are widely used to train Abstract Meaning Representation (Banarescu et al., 2013, AMR)  parsers.To train such models, AMR graphs have to be linearized into a one-line text format.While Penman encoding is typically used for this purpose, we argue that it has limitations: (1) for deep graphs, some closely related nodes are located far apart in the linearized text (2) Penman's tree-based encoding necessitates inverse roles to handle node re-entrancy, doubling the number of relation types to predict.To address these issues, we propose a triple-based linearization method and compare its efficiency with Penman linearization.Although triples are well suited to represent a graph, our results suggest room for improvement in triple encoding to better compete with Penman's concise and explicit representation of a nested graph structure.</p>
<p>Introduction</p>
<p>Abstract Meaning Representation (AMR) captures text meaning, such as "who does what to whom," and represents it in graphs (see Figure 1).Structured information is easier for computers to process and therefore, AMR is widely used in NLP applications, e.g., machine translation (Wein and Schneider, 2024), text generation (Huang et al., 2023), or human-robot interaction systems (Bonial et al., 2019(Bonial et al., , 2023)).</p>
<p>Sequence-to-sequence (seq2seq) approaches have recently gained popularity for AMR parsing due to strong performance and easy implementation.For prediction, the model receives an input sentence and outputs an AMR graph in text format.To train seq2seq models for AMR parsing, graph linearization to represent an AMR graph in a one-line text format is a prerequisite.Penman encoding is the most common method for AMR graph linearization, representing graphs as tree-like structures.It uses variables (e.g.s, s2 in Figure 2) Figure 1: AMR graph for "We never seem to see any of the dug-in nutters acknowledge the truth in it."Example from the AMR 3.0 dataset (Knight et al., 2020).</p>
<p>(s / seem-01 :polarity -:ARG1 (s2 / see-01 :ARG0 w :ARG1 (p / person :mod (a / any) :mod (n / nutter) :ARG0-of (d / dig-01) :ARG0-of (a2 / acknowledge-01 :ARG1 (t / thing :ARG1-of (t2 / true-01 :location (i / it)))))) :ARG2 (w / we) :time (e / ever))  as node IDs to manage co-references.In addition, parentheses represent nested structures of AMR graphs.However, Penman has key limitations to training a seq2seq model: 1) Parent-Child Distance: Parent and child nodes may appear far apart in the linearized text, despite being closely connected in the graph.For example, in Figure 2, seem-01 and we are encoded distant in Penman format (highlighted in red) despite their proximity in the graph of Figure 1.This is observed when a preceding sibling node has a deep sub-graph.We hypothesize that this long distance increases the difficulty of learning strong parent-child connections, especially in deeper graphs.2) Inverse Roles: Penman represents a graph in a tree-based format.To be specific, when a node has multiple parent nodes (node reentrancy), the child node is duplicated to maintain a single-rooted tree structure.To fit an AMR graph into a tree structure, Penman introduces inverse roles by rewriting :relation as :relation-of (see Figure 2 where inverse roles are highlighted in blue).This increases the number of relations the model must learn, potentially complicating training and reducing model performance.Figure 3 shows how inverse roles are unnecessary in a graph-based representation.</p>
<p>To address these issues, we propose an alternative triple-based format for AMR graph linearization.A triple consists of a parent node, a child node, and a relation type between them, ensuring that parent and child nodes remain adjacent in the linearized text.This format also eliminates inverse roles by replacing (node A, relation-of, node B) with (node B, relation, node A).In the rest of the paper, we compare Penman and triple-based formats with examples, highlighting their strengths and limitations in training a seq2seq AMR parser.Our contributions to seq2seq AMR parsing are:</p>
<p>• A triple-based linearization method for training seq2seq AMR parsers.• A detailed comparison with Penman linearization, focusing on performance across varying graph depths and lengths, and identifying areas for improvement.</p>
<p>Related Work</p>
<p>Triple encoding has been used in relation extraction (Huguet Cabot and Navigli, 2021;Ye et al., 2021;Saxena et al., 2022)   2018a,b).The closest to our approach is van Noord et al. (2018a), who convert AMR graphs into DRS triples.However, their representation differs from ours by mapping AMR relations to DRS roles and adding extra information that does not exist in AMR.In addition, they did not include training an AMR parser in their work.While triple encoding is widely used in relation extraction and DRS parsing, it has not been used for seq2seq AMR parsers.In our work, we propose using it to linearize AMR graphs, analyzing its strengths and weaknesses as an encoding method.</p>
<p>Methodology: Triple Linearization</p>
<p>Triple representation mitigates the challenges of Penman linearization (described in Section 1) by encoding a graph as a set of triples.Figure 4 illustrates that the parent-child nodes, which were distantly located in Figure 2, are now adjacent in the triple representation (highlighted in red).We hypothesize this helps the model learn direct parentchild relationships, especially in deeper graphs.Triples also eliminate inverse roles by reversing the order of two nodes, as shown in Figure 4 (highlighted in blue).</p>
<p>The triple format, widely used for graph representation (e.g., RDF), aligns better with AMR's graph structure than Penman's tree-based encoding.Its flexibility supports graphs with multiple roots or re-entrancies, making it potentially suitable for broader semantic frameworks.To assess its utility, we trained seq2seq AMR parsers using triple and Penman formats.</p>
<p>Despite its advantages, triple linearization can result in verbose linearization, slowing down the learning process, and may be less effective at capturing nested structures of graphs compared to Penman.This study evaluates both linearization methods to train seq2seq AMR parsers, exploring: (1) whether triple representation improves AMR parsing; (2) which graphs benefit most from triple representation, such as those with deep structures or large size, and (3) if combining triple and Penman representations enhances parsing performance.</p>
<p>Experiments involve training models respectively with triple, Penman, and both formats (multitask learning).Using both formats may serve as a form of data augmentation, as it effectively doubles the training data by representing one example in two linearized formats.We train and evaluate our model with English AMR 3.0 (Knight et al., 2020) data.We evaluate our model using SMATCH (Cai and Knight, 2013) score by counting the matching triples between two graphs.We analyze results by graph depth and size to determine which types of graphs benefit from different encoding methods.</p>
<p>Triple linearization strategies To linearize AMR graphs in triples, we extract all triples and unfold them in depth-first search order using the PENMAN library. 1 Four linearization strategies are applied, varying in whether variables 2 or inverse roles are retained.We provide an example for each linearization type in Table 2 and Figure  1 https://penman.readthedocs.io/en/,version 1.3.0 2 Removing variables is a common pre-processing strategy for seq2seq AMR parsing (Konstas et al., 2017;van Noord and Bos, 2017).This leads to information loss but effectively reduces data sparsity for training.</p>
<p>3 For the models discussed in this article, variables and inverse roles are removed in this manner.</p>
<p>Experimental Setup</p>
<p>Models are trained using the large mBART model (Tang et al., 2021) on each linearization type.4</p>
<p>Baseline</p>
<p>To compare our method with existing approaches using Penman encoding, we trained a model on AMR graphs linearized using Penman encoding, which serves as our baseline.Note that maintaining inverse roles is a necessary aspect of Penman encoding and X_invrole types are not available for Penman encoding.For training, we employed the same mBART model and trained two models as follows (see Table 3 for examples):</p>
<p>• Penman_X_var_O_invrole: Variables are removed, but inverse roles are retained.5• Penman_O_var_O_invrole: Both variables and inverse roles are retained.</p>
<p>Multi-task Learning</p>
<p>As mentioned in Section 1, combining triple and Penman encodings may offer complementary benefits.To test this, we trained models in a multi-task learning framework by merging two differently encoded datasets.During training, the model learns from shuffled examples with a token indicating the encoding type.For predictions, the model is instructed to use either triple or Penman encoding to use the corresponding decoding strategy to reconstruct graphs.We trained four models:</p>
<p>• Multi_tri_O_var_O_invrole: Both variables and inverse roles are retained.The main task is triple encoding, with Penman encoding as an auxiliary task.This means that the best model is selected based on the performance on the validation set using triple encoding, while Penman encoding is treated as an auxiliary task to help the triple learning.• Multi_penman_X_var_O_invrole: Variables are removed but inverse roles are retained.The best model is selected based on performance in Penman prediction.</p>
<p>Results and Insights</p>
<p>Global results.Table 1 presents results on two test sets: The Little Prince 6 and AMR 3.0.The Penman single-task model with variables (Penman_O_var_O_invrole) performs best on both test sets (77.0 and 80.9, respectively).Among single-task triple models, Tri_O_var_O_invrole achieves the best results, with a marginal gap from the best model (≤1 SMATCH).</p>
<p>Preserving variables consistently improves performance, contradicting the assumption that removing them aids learning by reducing data sparsity.This suggests variable removal leads to critical information loss.In addition, learning from Penman encoding while performing an auxiliary triple task reduces performance, whereas the reverse improves it.This indicates Penman encoding provides structural information beneficial to triple encoding but not vice versa.</p>
<p>Within triple linearization, removing inverse roles improves performance on AMR 3.0 but not The Little Prince.Given that AMR 3.0 includes longer sentences, removing inverse roles may benefit longer sentences while harming shorter ones.However, the inconsistency across test sets could also suggest inverse roles have only a marginal effect.</p>
<p>Triple linearization does not improve learning for deeper graphs.We hypothesized that triple linearization could enhance training by position-6 https://github.com/flipz357/AMR-Worlding child and parent nodes closer, especially when a preceding sibling has a deep subgraph.In Penman encoding, such cases place these nodes farther apart.Assuming this issue is more common in deeper graphs, we analyzed results by reference graph depth (distance from the root to the furthest node), focusing on AMR 3.0, which has greater depth variety than The Little Prince.</p>
<p>Our results (Figure 5, Appendix) show SMATCH scores by depth align with overall scores, indicating no learning benefit for deeper graphs with triple encoding.The best model, Pen-man_O_var_O_inverserole, consistently performed best across depths.This contradicts our hypothesis that triple encoding benefits seq2seq AMR learning by bringing parent-child nodes closer together.Instead, the results emphasize the benefit of Penman's concise graph representation and its ability to explicitly encode nested structures, which play a more critical role in model performance.</p>
<p>Triple linearization does not improve learning for longer graphs.We also analyzed performance by graph length (i.e., token count in the linearized reference graph), assuming verbose triple encoding would degrade performance on longer graphs.Since graph depth and length are not always correlated, results may differ from the depth analysis.Figure 6 in the Appendix shows the results per graph length with token counts of reference graphs grouped into buckets of 50 for clarity.For shorter graphs, the gap between models is smaller, but as the graphs become longer, the performance of triple models decreases more noticeably, resulting in a wider gap.This supports our earlier hypothesis regarding the limitations of triple encoding: its verbose representation likely contributes to the observed performance degradation on lengthy graphs.</p>
<p>Conclusion</p>
<p>We introduced triple linearization as an alternative to Penman linearization, hypothesizing that it could improve training for several reasons: (1) parent and child nodes are always located together in a triple, (2) the elimination of inverse roles may simplify training by reducing the number of relations, (3) and triples more closely resemble the underlying graph structure, while Penman encoding represents a graph in a more tree-like format.Contrary to our hypothesis, Penman has proven to be a more effective linearization method to train a seq2seq parsing.</p>
<p>However, the gap between the best Penman model and certain triple-based models is marginal.Our results show a potential to train a seq2seq AMR parser that predicts a graph directly (not a treebased representation) while maintaining equivalent performance.Notably, the model's output in triples more naturally aligns with AMR's graph structure than Penman.Our code to train and evaluate the model is available on https://github.com/Emvista/Triple_AMR_Parser.git.</p>
<p>A Appendix</p>
<p>Figure 2 :
2
Figure 2: AMR in Penman encoding for Figure 1.</p>
<p>Figure 3 :
3
Figure 3: AMR graph without inverse roles.</p>
<p>Figure 4 :
4
Figure 4: Triple linearization of the graph in Figure 2.</p>
<p>7 of Appendix A. Each model is named based on linearization type as follows: • Triple_X_var_X_invrole: Variables and inverse roles are removed.Variables are replaced by node names, and inverse roles are converted by reversing node order. 3Reversing inverse roles reduces the number of relation types from 155 to 115 in our training data.Triples are separated by a pipe symbol (|).• Triple_X_var_O_invrole: Variables are removed, but inverse roles are retained.• Triple_O_var_O_invrole: Both variables and inverse roles are retained.Variables and their instances are represented as triples with the instance relation (e.g., f instance fruit).This approach is the most comprehensive, as no information is lost from the original graph during linearization.• Triple_O_var_X_invrole: Variables are retained, but inverse roles are removed.</p>
<p>Figure 5 :
5
Figure 5: SMATCH score per graph depth.</p>
<p>Figure 6 :
6
Figure 6: SMATCH score per graph length.The length is measured by the number of tokens in a linearized graph and token counts are grouped into buckets of 50 for clarity.</p>
<p>Figure 7 :
7
Figure7: AMR graph for "There are too many traitors of China!".This example is drawn from AMR 3.0 dataset(Knight et al., 2020).</p>
<p>Table 1 :
1
SMATCH scores for evaluation (with the highest scores in bold and the second-highest scores underlined).
• Multi_penman_O_var_O_invrole: Bothvariables and inverse roles are retained. Themain task is Penman encoding, with triple en-coding as an auxiliary.• Multi_tri_X_var_O_invrole: Variables areremoved but inverse roles are retained. Thebest model is chosen based on the model'sperformance in triple prediction.</p>
<p>Table 3 :
3
Penman encoding examples of Figure7.</p>
<p>The model was chosen based on our goal of developing a multilingual system, which was not covered in this article.
We used the script from van Noord and Bos (2017).
AcknowledgmentsWe thank reviewers for their helpful comments and Cécile Macaire for proof-reading this article.Jeongwoo Kang and Maximin Coavoux gratefully acknowledge the support of the French National Research Agency (grant ANR-23-CE23-0017-01).This work was granted access to the HPC resources of IDRIS under the allocation 2024-AD011012853R2 made by GENCI.
Abstract Meaning Representation for sembanking. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, Nathan Schneider, Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. the 7th Linguistic Annotation Workshop and Interoperability with DiscourseSofia, BulgariaAssociation for Computational Linguistics2013</p>
<p>Abstract Meaning Representation for grounded human-robot communication. Claire Bonial, Julie Foresta, Nicholas C Fung, Cory J Hayes, Philip Osteen, Jacob Arkin, Benned Hedegaard, Thomas Howard, Proceedings of the Fourth International Workshop on Designing Meaning Representations. the Fourth International Workshop on Designing Meaning RepresentationsNancy, FranceAssociation for Computational Linguistics2023</p>
<p>Abstract Meaning Representation for human-robot dialogue. Claire N Bonial, Lucia Donatelli, Jessica Ervin, Clare R Voss, 10.7275/v3c5-yd35Proceedings of the Society for Computation in Linguistics (SCiL) 2019. the Society for Computation in Linguistics (SCiL) 20192019</p>
<p>Smatch: an evaluation metric for semantic feature structures. Shu Cai, Kevin Knight, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. the 51st Annual Meeting of the Association for Computational LinguisticsSofia, BulgariaAssociation for Computational Linguistics20132Short Papers)</p>
<p>ParaAMR: A large-scale syntactically diverse paraphrase dataset by AMR back-translation. Kuan-Hao Huang, Varun Iyer, I-Hung Hsu, Anoop Kumar, Kai-Wei Chang, Aram Galstyan, 10.18653/v1/2023.acl-long.447Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>REBEL: Relation extraction by end-to-end language generation. Pere-Lluís Huguet, Cabot , Roberto Navigli, 10.18653/v1/2021.findings-emnlp.204Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>Abstract meaning representation (amr) annotation release 3.0 ldc2020t02. Kevin Knight, Bianca Badarau, Laura Baranescu, Claire Bonial, Madalina Bardocz, Kira Griffitt, Ulf Hermjakob, Daniel Marcu, Martha Palmer, Tim O 'gorman, Nathan Schneider, 10.35111/44cy-bp512020Linguistic Data ConsortiumPhiladelphia</p>
<p>Neural AMR: Sequence-to-sequence models for parsing and generation. Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer, 10.18653/v1/P17-1014Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2017</p>
<p>Sequence-to-sequence knowledge graph completion and question answering. Apoorv Saxena, Adrian Kochsiek, Rainer Gemulla, 10.18653/v1/2022.acl-long.201Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Multilingual translation from denoising pre-training. Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan, 10.18653/v1/2021.findings-acl.304Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Evaluating scoped meaning representations. Rik Van Noord, Lasha Abzianidze, Hessel Haagsma, Johan Bos, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)Miyazaki, JapanEuropean Language Resources Association (ELRA2018a</p>
<p>Exploring neural methods for parsing discourse representation structures. Rik Van Noord, Lasha Abzianidze, Antonio Toral, Johan Bos, 10.1162/tacl_a_002412018b6Transactions of the Association for Computational Linguistics</p>
<p>Neural semantic parsing by character-based translation: Experiments with abstract meaning representations. Rik Van Noord, Johan Bos, Computational Linguistics in the Netherlands Journal. 72017</p>
<p>Lost in translationese? reducing translation effect using Abstract Meaning Representation. Shira Wein, Nathan Schneider, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics. Long Papers. the 18th Conference of the European Chapter of the Association for Computational LinguisticsSt. Julian's, MaltaAssociation for Computational Linguistics20241</p>
<p>Contrastive triple extraction with generative transformer. Hongbin Ye, Ningyu Zhang, Shumin Deng, Mosha Chen, Chuanqi Tan, Fei Huang, Huajun Chen, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202135</p>            </div>
        </div>

    </div>
</body>
</html>
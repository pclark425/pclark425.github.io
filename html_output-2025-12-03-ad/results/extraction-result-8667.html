<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8667 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8667</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8667</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-278789182</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.16094v1.pdf" target="_blank">A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are introducing a paradigm shift in molecular discovery by enabling text-guided interaction with chemical spaces through natural language, symbolic notations, with emerging extensions to incorporate multi-modal inputs. To advance the new field of LLM for molecular discovery, this survey provides an up-to-date and forward-looking review of the emerging use of LLMs for two central tasks: molecule generation and molecule optimization. Based on our proposed taxonomy for both problems, we analyze representative techniques in each category, highlighting how LLM capabilities are leveraged across different learning settings. In addition, we include the commonly used datasets and evaluation protocols. We conclude by discussing key challenges and future directions, positioning this survey as a resource for researchers working at the intersection of LLMs and molecular science. A continuously updated reading list is available at https://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8667.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8667.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FrontierX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FrontierX (knowledge-augmented prompting for molecule generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An in-context learning approach that uses knowledge-augmented prompting with few-shot examples retrieved into the prompt to guide de novo molecular design using general-purpose LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Crossing new frontiers: Knowledgeaugmented large language model prompting for zeroshot text-based de novo molecule design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Large language model (autoregressive transformer) used via in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not tuned; relies on pretraining of base LLM and retrieval of molecule-caption / example pairs as few-shot context</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecule generation (general molecular design)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based design using knowledge-augmented in-context learning (few-shot examples retrieved into prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Guided by text instructions and few-shot examples to satisfy task constraints; retrieval of highly relevant examples improves context relevance</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported to boost in-context learning performance for de novo design by supplying retrieved, relevant molecule-caption/example pairs; described as effective where zero-shot is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned as an ICL (no tuning) alternative to supervised fine-tuning; emphasizes retrieval-augmented few-shot context rather than model adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Relies heavily on the quality of retrieved few-shot examples and the inherent capabilities of the base LLM; doesn't change model weights so may fail for highly specialized constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8667.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4GraphGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM4GraphGen (retrieval-enhanced in-context molecule generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An in-context learning method that improves few-shot prompting by dynamically retrieving high-relevance molecule-caption pairs to serve as examples in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM (autoregressive transformer) used via in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Uses retrieval from molecule-caption/example corpora; no LLM fine-tuning reported in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Text-guided molecule generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)-style few-shot prompting to improve ICL</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Improves example relevance in prompts to steer generation toward desired molecule types or properties</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported to improve ICL performance by supplying more effective few-shot contexts through retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to vanilla few-shot prompting, retrieval-augmented examples yield better guidance without tuning the model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Still constrained by the base LLM's chemical knowledge and prompt length; quality depends on retrieval corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8667.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LlaS-Mol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LlaS-Mol (SMolInstruct instruction-tuning initiative)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuning dataset / effort (SMolInstruct) used to fine-tune LLMs for molecule generation and related chemical tasks, enabling supervised fine-tuning of general LLMs for chemistry instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7B (example of base used in SFT experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer causal LLM (autoregressive)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (reported for LLaMA-2-7B experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMolInstruct (instruction dataset for molecules) and other curated instruction corpora used for supervised fine-tuning via LoRA-style adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule generation and instruction-following for chemical design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised fine-tuning on large-scale instruction datasets to enable instruction-following molecule generation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Instruction tuning improves the model's ability to follow complex textual constraints and produce targeted molecules</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Instruction-following performance, downstream generation/optimization benchmarks (as used in surveys and TOMG-Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SFT on SMolInstruct improves instruction following and task performance compared to untuned models; LoRA used to adapt large models like LLaMA-2-7B and Mistral-7B.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms zero-shot / pure ICL approaches on specialized generation tasks but requires curated datasets and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires large, high-quality instruction datasets and compute resources; may still mis-handle quantitative constraints without additional techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8667.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemLLM (chemical large language model trained / fine-tuned with Chem-Data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemical-domain LLM trained or fine-tuned using Chem-Data, intended to improve chemical instruction-following and molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chemllm: A chemical large language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemLLM (fine-tuned models reported on LLaMA/Mistral variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer / LLM (causal)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Chem-Data (large-scale chemistry-specific dataset) and other molecular corpora for supervised fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule generation, chemical instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised fine-tuning (instruction tuning) often using parameter-efficient methods such as LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Fine-tuning on chemical corpora to inject domain knowledge and improve fidelity to chemical constraints</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported as part of SFT trend to adapt general LLMs for molecular generation; SFT on Chem-Data improves performance over untuned LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>More specialized than general LLMs used zero-shot; expected to outperform untuned models on domain tasks but requires dataset curation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Depends on dataset coverage and curation; still may require downstream preference tuning for precise property control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8667.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatMol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatMol (numerically enhanced LLM for property-constrained generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised fine-tuned LLM that incorporates a numerical enhancement technique to improve fidelity to specified quantitative property values during molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatMol (fine-tuned LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Large language model (causal transformer) with numerical enhancement</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Instruction data augmented with numeric targets / property values (specific dataset not specified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Property-constrained molecule generation (e.g., targeting numerical property targets)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised fine-tuning with numerical enhancement to better match quantitative constraints in prompts</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Designed to improve exactness to numeric property constraints specified by users</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Fidelity to specified quantitative property values (improvement reported)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported to significantly improve model fidelity to specified quantitative property values compared to baseline SFT models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Addresses a key weakness of vanilla SFT LLMs which often mis-handle numeric constraint precision.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Numerical fidelity remains a general challenge; approach likely depends on curated numeric examples and may not generalize perfectly to complex multi-objective constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8667.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SynLlama</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SynLlama (synthesizability-aware LLM for molecule generation and synthetic pathway prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An SFT-based system that emphasizes synthetic feasibility by generating synthesizable molecules and complete synthetic pathways alongside molecule proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Synllama: Generating synthesizable molecules and their analogs with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SynLlama (fine-tuned LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Large language model (autoregressive transformer) fine-tuned for synthetic feasibility</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SFT datasets including reaction/synthesis examples and molecular instructions (details not fully specified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecule generation with attention to synthetic feasibility; synthetic pathway generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised fine-tuning incorporating synthesis knowledge to output both molecules and synthetic routes</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Ensures molecules are accompanied by synthetic routes to improve practical utility</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Synthetic feasibility metrics / pathway completeness (not numerically detailed in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Targeted to improve synthesizability of generated molecules and to supply synthetic pathways, addressing a common limitation in generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>More synthesis-aware than text-only SFT methods; aims to bridge gap between generation and practical synthesis planning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Quality and realism of proposed synthetic pathways depend on reaction data quality; integrating true experimental feasibility remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8667.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniMoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UniMoT (Unified Molecule-Text model with discrete molecule tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-modal supervised fine-tuning approach introducing a molecule tokenizer (VQ + causal Q-Former) to convert graph-based molecular features into discrete 'molecule tokens' for unified autoregressive LLM processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unimot: Unified moleculetext language model with discrete token representation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UniMoT (multi-modal LLM framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM with added Vector Quantization + Causal Q-Former for graph tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Graph-based molecular features and paired text (multi-modal molecule-text datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule generation that jointly reasons over text and 2D graph structure</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Multi-modal supervised fine-tuning that converts graphs into discrete tokens and feeds them to an autoregressive LLM</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Enables explicit incorporation of structural (graph) information into LLM generation, improving structural fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Structure-based validity and property metrics (not enumerated for UniMoT specifically)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Proposes a way to encode graph information for LLMs to generate molecules with better structural awareness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Addresses shortcomings of text-only SFT by introducing discrete graph-derived tokens for better structure-text integration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Complexity of tokenizing continuous/graph information; requires training of additional modules to align with LLM token space.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8667.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SmileyLlama</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SmileyLlama (DPO preference-tuned LLM for property adherence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based molecule generator that applies Direct Preference Optimization (DPO) after supervised fine-tuning to learn from preference pairs and improve adherence to property constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Smileyllama: Modifying large language models for directed chemical space exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SmileyLlama (LLM fine-tuned + preference tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Large language model (causal transformer) adapted with preference training</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Preference pairs (chosen vs rejected molecules) derived from comparisons of generated outputs and property evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule generation with focus on property-constrained outputs</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised fine-tuning followed by preference tuning using Direct Preference Optimization (DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Learns to assign higher likelihood to molecules that better satisfy specified properties based on comparative feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Adherence to specified property constraints (reported improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>DPO after SFT significantly improved the model's adherence to specified property constraints compared to SFT alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_othermethods</strong></td>
                            <td>Preferential tuning improves over pure SFT where imitation learning falls short of preference alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires curated preference data; success depends on quality and coverage of preference pairs and reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8667.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mol-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mol-LLM (generalist molecular LLM with Molecular Structure Preference Optimization - MolPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that, after SFT including graph inputs, uses preference-tuning (MolPO) on 2D graph-conditioned outputs to better leverage structural information during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mol-llm: Generalist molecular llm with improved graph utilization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mol-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Large language model adapted for multi-modal (text + graph) input and preference tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SFT datasets including graph-conditioned examples and preference pairs where correctness of graph conditioning defines preference</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule generation/optimization with explicit 2D structural conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>SFT with graph inputs followed by preference tuning (MolPO) using chosen/rejected pairs based on graph-conditioning correctness</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Preference learning guides model to better integrate and use provided structural graphs to control generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolPO improves the model's ability to utilize structural conditioning during generation compared to SFT alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Focuses on improving graph utilization vs. text-only SFT approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Preference data must capture graph-conditioning correctness; multi-modal alignment remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8667.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-MDE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-MDE (Large Language Models as Molecular Design Engines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot prompting approach where pretrained LLMs are instructed via natural language to modify input molecules (often SMILES) to achieve desired property changes under structural constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models as molecular design engines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Pretrained LLM used via zero-shot prompting (autoregressive transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>No task-specific tuning; leverages the LLM's pretraining on broad corpora</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule optimization (editing existing molecules to change properties)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Zero-shot natural language prompting to specify desired property edits and constraints; outputs molecule edits (e.g., SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Uses descriptive natural language specifying property changes and constraints to direct edits; suitable for single-objective edits</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Structural similarity constraints and property improvement checks (as in example prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Demonstrates controlled modifications to parent molecules using natural language prompts; flexible but limited in precise multi-objective control.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Simpler than tuned approaches; offers flexibility without tuning but lower precision compared to SFT + preference tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Struggles with precise control in multi-objective settings and may produce chemically invalid edits without domain-specific supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8667.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOLLEO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOLLEO (LLM-integrated evolutionary optimization framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach integrating LLMs into evolutionary optimization frameworks: uses prompt-based sampling to generate mutation/crossover candidates, and filtering/enforcement to preserve structural similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM used as a candidate generator within evolutionary/population algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>No tuning described; uses prompts and filtering informed by oracle evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule optimization via population-based search</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based sampling for mutation/crossover analogues within evolutionary search; filtering to enforce similarity constraints</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Targets controlled modifications while maintaining similarity via filtering and similarity constraints</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Similarity constraints and property evaluations used as fitness/filtering</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Demonstrates flexible optimization workflows using LLM sampling inside evolutionary algorithms; effective for diverse goals but limited in multi-objective precision.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Replaces or augments classical mutation/crossover operators with LLM-driven proposals; can integrate SFT when stagnation occurs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Zero-shot approaches like MOLLEO can lack precise multi-objective control and may require SFT or oracle modeling when search plateaus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8667.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Enhanced GA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-Enhanced Genetic Algorithm (LLM-Enhanced GA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid optimization method that iteratively generates new candidates via LLM prompt-based sampling from high-performing molecules, and uses supervised fine-tuning on evaluated molecules when performance stalls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Hybrid: LLM used in an iterative evolutionary/GA framework; may be fine-tuned (SFT) on oracle-evaluated molecules</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Initially relies on pretrained LLM knowledge and high-performing molecule examples; SFT uses evaluated molecules as needed</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule optimization (goal-directed search in chemical space)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based sampling to propose new candidates from elite molecules; explicit oracle modeling via SFT when stagnation occurs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Iterative refinement guided by fitness evaluations; can incorporate property-specific feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Oracle fitness scores guiding GA selection and potential SFT targets</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Proposes iterative improvement combining LLM sampling and SFT-based oracle modeling to overcome stagnation in search.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Combines strengths of population-based search with the expressive generation capacity of LLMs; potentially more adaptive than static mutational operators.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires evaluation/oracle infrastructure; SFT step requires collected labeled evaluations and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8667.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolX (Molx-Enhanced LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolX: Enhancing large language models for molecular learning with a multi-modal extension (Molx-Enhanced LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-modal framework that augments an LLM (LLaMA-2-7B base reported) with a trainable MolX module to integrate SMILES, 2D molecular graphs, and handcrafted fingerprints into unified embeddings for improved molecule optimization and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molx: Enhancing large language models for molecular learning with a multi-modal extension</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7B (base reported) + MolX multi-modal module</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Causal transformer LLM augmented with a trainable multi-modal module (graph encoders + fingerprint integration)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (base LLaMA-2-7B used in reported experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pre-trained LLM + supervised molecule-text pairs and auxiliary tasks during MolX pre-training; datasets include multi-modal molecule corpora (not exhaustively listed)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule optimization and generation with multi-modal structural awareness</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Multi-modal supervised fine-tuning: integrate SMILES, 2D graphs and fingerprints into the LLM's input space via MolX, enabling conditional generation/optimization</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Graph encoders and fingerprints ensure the model captures topology and substructural details important for chemically valid optimization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported improved performance on molecule optimization tasks relative to generalist chemical LLMs (structure validity, property improvements implied)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine-tuning the LLM with MolX multi-modal alignment shows better performance than generalist chemical LLMs, suggesting multi-modal fine-tuning yields more chemically valid and property-aligned outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms text-only SFT chemical LLMs on tasks requiring structural detail; demonstrates advantage of explicit graph/fingerprint integration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires additional modules and multi-modal pretraining steps; integrating continuous 3D or varied graph modalities remains challenging and computationally heavier.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e8667.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICMA / MolReFlect (ICMT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ICMA and MolReFlect (In-Context Molecule Tuning - ICMT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that incorporate dynamic, retrieval-relevant examples directly into the fine-tuning process (In-Context Molecule Tuning) to better align LLM responses with relevant molecular examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ICMA / MolReFlect (as method components applied to LLM fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM with retrieval-augmented fine-tuning (hybrid SFT + in-context retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuning datasets augmented with retrieved relevant examples for each training instance</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule generation and instruction-following with improved context relevance</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised fine-tuning augmented by retrieval; the model learns to use retrieved in-context examples during generation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>By exposing the model during training to dynamic relevant contexts, the model better follows example-driven tasks at inference</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ICMT-style approaches improve how models leverage context and examples compared to static SFT, enhancing performance on complex generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Bridges gap between static SFT and pure ICL by training the model to consume retrieved examples more effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Adds retrieval infrastructure and complexity to training; effectiveness depends on retrieval quality and alignment of retrieved examples with task needs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e8667.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PEIT-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PEIT-LLM (Property Enhanced Instruction Tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-step SFT framework that first synthesizes instruction data using a multi-modal model and then uses that data to fine-tune LLMs for tasks like multi-constraint molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PEIT-LLM (framework applied to LLM fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM fine-tuned with synthetic instruction data (supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Synthesized instruction data produced by a multi-modal model, followed by supervised fine-tuning on those synthetic examples</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Multi-constraint molecule generation and property-conditioned design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Synthetic instruction generation (multi-modal) followed by supervised instruction tuning of an LLM</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specifically targets multi-constraint generation tasks by generating tailored instruction-data for SFT</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Proposed to improve LLM performance on multi-constraint generation tasks by providing richer instruction examples than might be available in human-curated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Augments training data via synthetic generation to overcome scarcity for complex multi-property tasks; complementary to other SFT approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Quality of synthetic instruction data depends on the multi-modal model used to create it; risk of propagating synthetic biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e8667.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TOMG-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TOMG-Bench (Text-based Open Molecule Generation Benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark dataset focused on open-domain text-based molecule generation with tasks covering molecule editing, optimization, and customized generation (45,000 samples).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tomg-bench: Evaluating llms on text-based open molecule generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Benchmark / evaluation suite for LLM-based molecular generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>45,000 benchmark samples across MolEdit, MolOpt, MolCustom tasks drawn from multiple sources (OpenMolIns included)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Benchmarking de novo generation and optimization tasks (drug discovery / open molecule generation)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A (benchmark used to evaluate prompt-based, SFT, and preference-tuned LLM methods)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Contains tasks that require tailored generation and editing subject to property or structural constraints</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Used with validity, similarity, property-based metrics and multi-property success metrics as described in survey</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Serves as a standardized testbed to compare LLM-centric generation and optimization methods across task types.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Provides open-domain evaluation complementary to traditional molecule benchmarks (e.g., MOSES) and helps quantify LLM-specific capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Benchmark coverage may still not capture all real-world experimental constraints (e.g., full synthetic feasibility and experimental validation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8667.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e8667.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CIDD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CIDD (Chain/Stages: interaction analysis, design, reflection pipeline for molecule optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An in-context-learning pipeline that structures molecule optimization into interaction analysis, design, and reflection steps, using previous designs and reflections in prompts to guide new edits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CIDD (ICL pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM used via multi-step in-context prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Relies on few-shot examples provided in the prompt (no tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule optimization (iterative multi-step edits guided by interaction profiles)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>In-context learning with a multi-step pipeline (provide prior designs and reflections as context)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Designs guided by interaction profiles and prior examples to produce consistent optimization edits</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Structuring prompts into analysis-design-reflection improves iterative optimization consistency in ICL settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Improves over naive single-shot ICL by providing a procedural scaffold; less costly than SFT but less precise than tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Quality and number of in-context examples limit performance; scaling to complex multi-objective tasks remains hard.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Molx: Enhancing large language models for molecular learning with a multi-modal extension <em>(Rating: 2)</em></li>
                <li>Tomg-bench: Evaluating llms on text-based open molecule generation <em>(Rating: 2)</em></li>
                <li>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset <em>(Rating: 2)</em></li>
                <li>Smileyllama: Modifying large language models for directed chemical space exploration <em>(Rating: 2)</em></li>
                <li>Synllama: Generating synthesizable molecules and their analogs with large language models <em>(Rating: 2)</em></li>
                <li>Chemllm: A chemical large language model <em>(Rating: 2)</em></li>
                <li>Large language models as molecular design engines <em>(Rating: 2)</em></li>
                <li>Crossing new frontiers: Knowledgeaugmented large language model prompting for zeroshot text-based de novo molecule design <em>(Rating: 2)</em></li>
                <li>Mol-llm: Generalist molecular llm with improved graph utilization <em>(Rating: 2)</em></li>
                <li>ICMA: (in-context molecule tuning) / MolReFlect: Towards in-context fine-grained alignments between molecules and texts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8667",
    "paper_id": "paper-278789182",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "FrontierX",
            "name_full": "FrontierX (knowledge-augmented prompting for molecule generation)",
            "brief_description": "An in-context learning approach that uses knowledge-augmented prompting with few-shot examples retrieved into the prompt to guide de novo molecular design using general-purpose LLMs.",
            "citation_title": "Crossing new frontiers: Knowledgeaugmented large language model prompting for zeroshot text-based de novo molecule design",
            "mention_or_use": "mention",
            "model_name": null,
            "model_type": "Large language model (autoregressive transformer) used via in-context learning",
            "model_size": null,
            "training_data": "Not tuned; relies on pretraining of base LLM and retrieval of molecule-caption / example pairs as few-shot context",
            "application_domain": "De novo molecule generation (general molecular design)",
            "generation_method": "Prompt-based design using knowledge-augmented in-context learning (few-shot examples retrieved into prompts)",
            "novelty_of_chemicals": null,
            "application_specificity": "Guided by text instructions and few-shot examples to satisfy task constraints; retrieval of highly relevant examples improves context relevance",
            "evaluation_metrics": null,
            "results_summary": "Reported to boost in-context learning performance for de novo design by supplying retrieved, relevant molecule-caption/example pairs; described as effective where zero-shot is insufficient.",
            "comparison_to_other_methods": "Positioned as an ICL (no tuning) alternative to supervised fine-tuning; emphasizes retrieval-augmented few-shot context rather than model adaptation.",
            "limitations_and_challenges": "Relies heavily on the quality of retrieved few-shot examples and the inherent capabilities of the base LLM; doesn't change model weights so may fail for highly specialized constraints.",
            "uuid": "e8667.0",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LLM4GraphGen",
            "name_full": "LLM4GraphGen (retrieval-enhanced in-context molecule generation)",
            "brief_description": "An in-context learning method that improves few-shot prompting by dynamically retrieving high-relevance molecule-caption pairs to serve as examples in the prompt.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_type": "LLM (autoregressive transformer) used via in-context learning",
            "model_size": null,
            "training_data": "Uses retrieval from molecule-caption/example corpora; no LLM fine-tuning reported in the survey",
            "application_domain": "Text-guided molecule generation",
            "generation_method": "Retrieval-Augmented Generation (RAG)-style few-shot prompting to improve ICL",
            "novelty_of_chemicals": null,
            "application_specificity": "Improves example relevance in prompts to steer generation toward desired molecule types or properties",
            "evaluation_metrics": null,
            "results_summary": "Reported to improve ICL performance by supplying more effective few-shot contexts through retrieval.",
            "comparison_to_other_methods": "Compared to vanilla few-shot prompting, retrieval-augmented examples yield better guidance without tuning the model.",
            "limitations_and_challenges": "Still constrained by the base LLM's chemical knowledge and prompt length; quality depends on retrieval corpus.",
            "uuid": "e8667.1",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LlaS-Mol",
            "name_full": "LlaS-Mol (SMolInstruct instruction-tuning initiative)",
            "brief_description": "An instruction-tuning dataset / effort (SMolInstruct) used to fine-tune LLMs for molecule generation and related chemical tasks, enabling supervised fine-tuning of general LLMs for chemistry instructions.",
            "citation_title": "Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset",
            "mention_or_use": "mention",
            "model_name": "LLaMA-2-7B (example of base used in SFT experiments)",
            "model_type": "Transformer causal LLM (autoregressive)",
            "model_size": "7B (reported for LLaMA-2-7B experiments)",
            "training_data": "SMolInstruct (instruction dataset for molecules) and other curated instruction corpora used for supervised fine-tuning via LoRA-style adaptation",
            "application_domain": "Molecule generation and instruction-following for chemical design",
            "generation_method": "Supervised fine-tuning on large-scale instruction datasets to enable instruction-following molecule generation",
            "novelty_of_chemicals": null,
            "application_specificity": "Instruction tuning improves the model's ability to follow complex textual constraints and produce targeted molecules",
            "evaluation_metrics": "Instruction-following performance, downstream generation/optimization benchmarks (as used in surveys and TOMG-Bench)",
            "results_summary": "SFT on SMolInstruct improves instruction following and task performance compared to untuned models; LoRA used to adapt large models like LLaMA-2-7B and Mistral-7B.",
            "comparison_to_other_methods": "Outperforms zero-shot / pure ICL approaches on specialized generation tasks but requires curated datasets and compute.",
            "limitations_and_challenges": "Requires large, high-quality instruction datasets and compute resources; may still mis-handle quantitative constraints without additional techniques.",
            "uuid": "e8667.2",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "ChemLLM",
            "name_full": "ChemLLM (chemical large language model trained / fine-tuned with Chem-Data)",
            "brief_description": "A chemical-domain LLM trained or fine-tuned using Chem-Data, intended to improve chemical instruction-following and molecule generation.",
            "citation_title": "Chemllm: A chemical large language model",
            "mention_or_use": "mention",
            "model_name": "ChemLLM (fine-tuned models reported on LLaMA/Mistral variants)",
            "model_type": "Transformer / LLM (causal)",
            "model_size": null,
            "training_data": "Chem-Data (large-scale chemistry-specific dataset) and other molecular corpora for supervised fine-tuning",
            "application_domain": "Molecule generation, chemical instruction following",
            "generation_method": "Supervised fine-tuning (instruction tuning) often using parameter-efficient methods such as LoRA",
            "novelty_of_chemicals": null,
            "application_specificity": "Fine-tuning on chemical corpora to inject domain knowledge and improve fidelity to chemical constraints",
            "evaluation_metrics": null,
            "results_summary": "Reported as part of SFT trend to adapt general LLMs for molecular generation; SFT on Chem-Data improves performance over untuned LLMs.",
            "comparison_to_other_methods": "More specialized than general LLMs used zero-shot; expected to outperform untuned models on domain tasks but requires dataset curation.",
            "limitations_and_challenges": "Depends on dataset coverage and curation; still may require downstream preference tuning for precise property control.",
            "uuid": "e8667.3",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "ChatMol",
            "name_full": "ChatMol (numerically enhanced LLM for property-constrained generation)",
            "brief_description": "A supervised fine-tuned LLM that incorporates a numerical enhancement technique to improve fidelity to specified quantitative property values during molecule generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChatMol (fine-tuned LLM)",
            "model_type": "Large language model (causal transformer) with numerical enhancement",
            "model_size": null,
            "training_data": "Instruction data augmented with numeric targets / property values (specific dataset not specified in survey)",
            "application_domain": "Property-constrained molecule generation (e.g., targeting numerical property targets)",
            "generation_method": "Supervised fine-tuning with numerical enhancement to better match quantitative constraints in prompts",
            "novelty_of_chemicals": null,
            "application_specificity": "Designed to improve exactness to numeric property constraints specified by users",
            "evaluation_metrics": "Fidelity to specified quantitative property values (improvement reported)",
            "results_summary": "Reported to significantly improve model fidelity to specified quantitative property values compared to baseline SFT models.",
            "comparison_to_other_methods": "Addresses a key weakness of vanilla SFT LLMs which often mis-handle numeric constraint precision.",
            "limitations_and_challenges": "Numerical fidelity remains a general challenge; approach likely depends on curated numeric examples and may not generalize perfectly to complex multi-objective constraints.",
            "uuid": "e8667.4",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "SynLlama",
            "name_full": "SynLlama (synthesizability-aware LLM for molecule generation and synthetic pathway prediction)",
            "brief_description": "An SFT-based system that emphasizes synthetic feasibility by generating synthesizable molecules and complete synthetic pathways alongside molecule proposals.",
            "citation_title": "Synllama: Generating synthesizable molecules and their analogs with large language models",
            "mention_or_use": "mention",
            "model_name": "SynLlama (fine-tuned LLM)",
            "model_type": "Large language model (autoregressive transformer) fine-tuned for synthetic feasibility",
            "model_size": null,
            "training_data": "SFT datasets including reaction/synthesis examples and molecular instructions (details not fully specified in survey)",
            "application_domain": "De novo molecule generation with attention to synthetic feasibility; synthetic pathway generation",
            "generation_method": "Supervised fine-tuning incorporating synthesis knowledge to output both molecules and synthetic routes",
            "novelty_of_chemicals": null,
            "application_specificity": "Ensures molecules are accompanied by synthetic routes to improve practical utility",
            "evaluation_metrics": "Synthetic feasibility metrics / pathway completeness (not numerically detailed in survey)",
            "results_summary": "Targeted to improve synthesizability of generated molecules and to supply synthetic pathways, addressing a common limitation in generative models.",
            "comparison_to_other_methods": "More synthesis-aware than text-only SFT methods; aims to bridge gap between generation and practical synthesis planning.",
            "limitations_and_challenges": "Quality and realism of proposed synthetic pathways depend on reaction data quality; integrating true experimental feasibility remains challenging.",
            "uuid": "e8667.5",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "UniMoT",
            "name_full": "UniMoT (Unified Molecule-Text model with discrete molecule tokens)",
            "brief_description": "A multi-modal supervised fine-tuning approach introducing a molecule tokenizer (VQ + causal Q-Former) to convert graph-based molecular features into discrete 'molecule tokens' for unified autoregressive LLM processing.",
            "citation_title": "Unimot: Unified moleculetext language model with discrete token representation",
            "mention_or_use": "mention",
            "model_name": "UniMoT (multi-modal LLM framework)",
            "model_type": "LLM with added Vector Quantization + Causal Q-Former for graph tokenization",
            "model_size": null,
            "training_data": "Graph-based molecular features and paired text (multi-modal molecule-text datasets)",
            "application_domain": "Molecule generation that jointly reasons over text and 2D graph structure",
            "generation_method": "Multi-modal supervised fine-tuning that converts graphs into discrete tokens and feeds them to an autoregressive LLM",
            "novelty_of_chemicals": null,
            "application_specificity": "Enables explicit incorporation of structural (graph) information into LLM generation, improving structural fidelity",
            "evaluation_metrics": "Structure-based validity and property metrics (not enumerated for UniMoT specifically)",
            "results_summary": "Proposes a way to encode graph information for LLMs to generate molecules with better structural awareness.",
            "comparison_to_other_methods": "Addresses shortcomings of text-only SFT by introducing discrete graph-derived tokens for better structure-text integration.",
            "limitations_and_challenges": "Complexity of tokenizing continuous/graph information; requires training of additional modules to align with LLM token space.",
            "uuid": "e8667.6",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "SmileyLlama",
            "name_full": "SmileyLlama (DPO preference-tuned LLM for property adherence)",
            "brief_description": "An LLM-based molecule generator that applies Direct Preference Optimization (DPO) after supervised fine-tuning to learn from preference pairs and improve adherence to property constraints.",
            "citation_title": "Smileyllama: Modifying large language models for directed chemical space exploration",
            "mention_or_use": "mention",
            "model_name": "SmileyLlama (LLM fine-tuned + preference tuning)",
            "model_type": "Large language model (causal transformer) adapted with preference training",
            "model_size": null,
            "training_data": "Preference pairs (chosen vs rejected molecules) derived from comparisons of generated outputs and property evaluations",
            "application_domain": "Molecule generation with focus on property-constrained outputs",
            "generation_method": "Supervised fine-tuning followed by preference tuning using Direct Preference Optimization (DPO)",
            "novelty_of_chemicals": null,
            "application_specificity": "Learns to assign higher likelihood to molecules that better satisfy specified properties based on comparative feedback",
            "evaluation_metrics": "Adherence to specified property constraints (reported improvement)",
            "results_summary": "DPO after SFT significantly improved the model's adherence to specified property constraints compared to SFT alone.",
            "comparison_to_othermethods": "Preferential tuning improves over pure SFT where imitation learning falls short of preference alignment.",
            "limitations_and_challenges": "Requires curated preference data; success depends on quality and coverage of preference pairs and reward signals.",
            "uuid": "e8667.7",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Mol-LLM",
            "name_full": "Mol-LLM (generalist molecular LLM with Molecular Structure Preference Optimization - MolPO)",
            "brief_description": "A model that, after SFT including graph inputs, uses preference-tuning (MolPO) on 2D graph-conditioned outputs to better leverage structural information during generation.",
            "citation_title": "Mol-llm: Generalist molecular llm with improved graph utilization",
            "mention_or_use": "mention",
            "model_name": "Mol-LLM",
            "model_type": "Large language model adapted for multi-modal (text + graph) input and preference tuning",
            "model_size": null,
            "training_data": "SFT datasets including graph-conditioned examples and preference pairs where correctness of graph conditioning defines preference",
            "application_domain": "Molecule generation/optimization with explicit 2D structural conditioning",
            "generation_method": "SFT with graph inputs followed by preference tuning (MolPO) using chosen/rejected pairs based on graph-conditioning correctness",
            "novelty_of_chemicals": null,
            "application_specificity": "Preference learning guides model to better integrate and use provided structural graphs to control generation",
            "evaluation_metrics": null,
            "results_summary": "MolPO improves the model's ability to utilize structural conditioning during generation compared to SFT alone.",
            "comparison_to_other_methods": "Focuses on improving graph utilization vs. text-only SFT approaches.",
            "limitations_and_challenges": "Preference data must capture graph-conditioning correctness; multi-modal alignment remains challenging.",
            "uuid": "e8667.8",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LLM-MDE",
            "name_full": "LLM-MDE (Large Language Models as Molecular Design Engines)",
            "brief_description": "A zero-shot prompting approach where pretrained LLMs are instructed via natural language to modify input molecules (often SMILES) to achieve desired property changes under structural constraints.",
            "citation_title": "Large language models as molecular design engines",
            "mention_or_use": "mention",
            "model_name": null,
            "model_type": "Pretrained LLM used via zero-shot prompting (autoregressive transformer)",
            "model_size": null,
            "training_data": "No task-specific tuning; leverages the LLM's pretraining on broad corpora",
            "application_domain": "Molecule optimization (editing existing molecules to change properties)",
            "generation_method": "Zero-shot natural language prompting to specify desired property edits and constraints; outputs molecule edits (e.g., SMILES)",
            "novelty_of_chemicals": null,
            "application_specificity": "Uses descriptive natural language specifying property changes and constraints to direct edits; suitable for single-objective edits",
            "evaluation_metrics": "Structural similarity constraints and property improvement checks (as in example prompts)",
            "results_summary": "Demonstrates controlled modifications to parent molecules using natural language prompts; flexible but limited in precise multi-objective control.",
            "comparison_to_other_methods": "Simpler than tuned approaches; offers flexibility without tuning but lower precision compared to SFT + preference tuning.",
            "limitations_and_challenges": "Struggles with precise control in multi-objective settings and may produce chemically invalid edits without domain-specific supervision.",
            "uuid": "e8667.9",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "MOLLEO",
            "name_full": "MOLLEO (LLM-integrated evolutionary optimization framework)",
            "brief_description": "An approach integrating LLMs into evolutionary optimization frameworks: uses prompt-based sampling to generate mutation/crossover candidates, and filtering/enforcement to preserve structural similarity.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_type": "LLM used as a candidate generator within evolutionary/population algorithms",
            "model_size": null,
            "training_data": "No tuning described; uses prompts and filtering informed by oracle evaluations",
            "application_domain": "Molecule optimization via population-based search",
            "generation_method": "Prompt-based sampling for mutation/crossover analogues within evolutionary search; filtering to enforce similarity constraints",
            "novelty_of_chemicals": null,
            "application_specificity": "Targets controlled modifications while maintaining similarity via filtering and similarity constraints",
            "evaluation_metrics": "Similarity constraints and property evaluations used as fitness/filtering",
            "results_summary": "Demonstrates flexible optimization workflows using LLM sampling inside evolutionary algorithms; effective for diverse goals but limited in multi-objective precision.",
            "comparison_to_other_methods": "Replaces or augments classical mutation/crossover operators with LLM-driven proposals; can integrate SFT when stagnation occurs.",
            "limitations_and_challenges": "Zero-shot approaches like MOLLEO can lack precise multi-objective control and may require SFT or oracle modeling when search plateaus.",
            "uuid": "e8667.10",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LLM-Enhanced GA",
            "name_full": "LLM-Enhanced Genetic Algorithm (LLM-Enhanced GA)",
            "brief_description": "A hybrid optimization method that iteratively generates new candidates via LLM prompt-based sampling from high-performing molecules, and uses supervised fine-tuning on evaluated molecules when performance stalls.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_type": "Hybrid: LLM used in an iterative evolutionary/GA framework; may be fine-tuned (SFT) on oracle-evaluated molecules",
            "model_size": null,
            "training_data": "Initially relies on pretrained LLM knowledge and high-performing molecule examples; SFT uses evaluated molecules as needed",
            "application_domain": "Molecule optimization (goal-directed search in chemical space)",
            "generation_method": "Prompt-based sampling to propose new candidates from elite molecules; explicit oracle modeling via SFT when stagnation occurs",
            "novelty_of_chemicals": null,
            "application_specificity": "Iterative refinement guided by fitness evaluations; can incorporate property-specific feedback",
            "evaluation_metrics": "Oracle fitness scores guiding GA selection and potential SFT targets",
            "results_summary": "Proposes iterative improvement combining LLM sampling and SFT-based oracle modeling to overcome stagnation in search.",
            "comparison_to_other_methods": "Combines strengths of population-based search with the expressive generation capacity of LLMs; potentially more adaptive than static mutational operators.",
            "limitations_and_challenges": "Requires evaluation/oracle infrastructure; SFT step requires collected labeled evaluations and compute.",
            "uuid": "e8667.11",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "MolX (Molx-Enhanced LLM)",
            "name_full": "MolX: Enhancing large language models for molecular learning with a multi-modal extension (Molx-Enhanced LLM)",
            "brief_description": "A multi-modal framework that augments an LLM (LLaMA-2-7B base reported) with a trainable MolX module to integrate SMILES, 2D molecular graphs, and handcrafted fingerprints into unified embeddings for improved molecule optimization and generation.",
            "citation_title": "Molx: Enhancing large language models for molecular learning with a multi-modal extension",
            "mention_or_use": "mention",
            "model_name": "LLaMA-2-7B (base reported) + MolX multi-modal module",
            "model_type": "Causal transformer LLM augmented with a trainable multi-modal module (graph encoders + fingerprint integration)",
            "model_size": "7B (base LLaMA-2-7B used in reported experiments)",
            "training_data": "Pre-trained LLM + supervised molecule-text pairs and auxiliary tasks during MolX pre-training; datasets include multi-modal molecule corpora (not exhaustively listed)",
            "application_domain": "Molecule optimization and generation with multi-modal structural awareness",
            "generation_method": "Multi-modal supervised fine-tuning: integrate SMILES, 2D graphs and fingerprints into the LLM's input space via MolX, enabling conditional generation/optimization",
            "novelty_of_chemicals": null,
            "application_specificity": "Graph encoders and fingerprints ensure the model captures topology and substructural details important for chemically valid optimization",
            "evaluation_metrics": "Reported improved performance on molecule optimization tasks relative to generalist chemical LLMs (structure validity, property improvements implied)",
            "results_summary": "Fine-tuning the LLM with MolX multi-modal alignment shows better performance than generalist chemical LLMs, suggesting multi-modal fine-tuning yields more chemically valid and property-aligned outputs.",
            "comparison_to_other_methods": "Outperforms text-only SFT chemical LLMs on tasks requiring structural detail; demonstrates advantage of explicit graph/fingerprint integration.",
            "limitations_and_challenges": "Requires additional modules and multi-modal pretraining steps; integrating continuous 3D or varied graph modalities remains challenging and computationally heavier.",
            "uuid": "e8667.12",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "ICMA / MolReFlect (ICMT)",
            "name_full": "ICMA and MolReFlect (In-Context Molecule Tuning - ICMT)",
            "brief_description": "Approaches that incorporate dynamic, retrieval-relevant examples directly into the fine-tuning process (In-Context Molecule Tuning) to better align LLM responses with relevant molecular examples.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ICMA / MolReFlect (as method components applied to LLM fine-tuning)",
            "model_type": "LLM with retrieval-augmented fine-tuning (hybrid SFT + in-context retrieval)",
            "model_size": null,
            "training_data": "Fine-tuning datasets augmented with retrieved relevant examples for each training instance",
            "application_domain": "Molecule generation and instruction-following with improved context relevance",
            "generation_method": "Supervised fine-tuning augmented by retrieval; the model learns to use retrieved in-context examples during generation",
            "novelty_of_chemicals": null,
            "application_specificity": "By exposing the model during training to dynamic relevant contexts, the model better follows example-driven tasks at inference",
            "evaluation_metrics": null,
            "results_summary": "ICMT-style approaches improve how models leverage context and examples compared to static SFT, enhancing performance on complex generation tasks.",
            "comparison_to_other_methods": "Bridges gap between static SFT and pure ICL by training the model to consume retrieved examples more effectively.",
            "limitations_and_challenges": "Adds retrieval infrastructure and complexity to training; effectiveness depends on retrieval quality and alignment of retrieved examples with task needs.",
            "uuid": "e8667.13",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "PEIT-LLM",
            "name_full": "PEIT-LLM (Property Enhanced Instruction Tuning)",
            "brief_description": "A two-step SFT framework that first synthesizes instruction data using a multi-modal model and then uses that data to fine-tune LLMs for tasks like multi-constraint molecule generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "PEIT-LLM (framework applied to LLM fine-tuning)",
            "model_type": "LLM fine-tuned with synthetic instruction data (supervised)",
            "model_size": null,
            "training_data": "Synthesized instruction data produced by a multi-modal model, followed by supervised fine-tuning on those synthetic examples",
            "application_domain": "Multi-constraint molecule generation and property-conditioned design",
            "generation_method": "Synthetic instruction generation (multi-modal) followed by supervised instruction tuning of an LLM",
            "novelty_of_chemicals": null,
            "application_specificity": "Specifically targets multi-constraint generation tasks by generating tailored instruction-data for SFT",
            "evaluation_metrics": null,
            "results_summary": "Proposed to improve LLM performance on multi-constraint generation tasks by providing richer instruction examples than might be available in human-curated datasets.",
            "comparison_to_other_methods": "Augments training data via synthetic generation to overcome scarcity for complex multi-property tasks; complementary to other SFT approaches.",
            "limitations_and_challenges": "Quality of synthetic instruction data depends on the multi-modal model used to create it; risk of propagating synthetic biases.",
            "uuid": "e8667.14",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "TOMG-Bench",
            "name_full": "TOMG-Bench (Text-based Open Molecule Generation Benchmark)",
            "brief_description": "A benchmark dataset focused on open-domain text-based molecule generation with tasks covering molecule editing, optimization, and customized generation (45,000 samples).",
            "citation_title": "Tomg-bench: Evaluating llms on text-based open molecule generation",
            "mention_or_use": "use",
            "model_name": null,
            "model_type": "Benchmark / evaluation suite for LLM-based molecular generation",
            "model_size": null,
            "training_data": "45,000 benchmark samples across MolEdit, MolOpt, MolCustom tasks drawn from multiple sources (OpenMolIns included)",
            "application_domain": "Benchmarking de novo generation and optimization tasks (drug discovery / open molecule generation)",
            "generation_method": "N/A (benchmark used to evaluate prompt-based, SFT, and preference-tuned LLM methods)",
            "novelty_of_chemicals": null,
            "application_specificity": "Contains tasks that require tailored generation and editing subject to property or structural constraints",
            "evaluation_metrics": "Used with validity, similarity, property-based metrics and multi-property success metrics as described in survey",
            "results_summary": "Serves as a standardized testbed to compare LLM-centric generation and optimization methods across task types.",
            "comparison_to_other_methods": "Provides open-domain evaluation complementary to traditional molecule benchmarks (e.g., MOSES) and helps quantify LLM-specific capabilities.",
            "limitations_and_challenges": "Benchmark coverage may still not capture all real-world experimental constraints (e.g., full synthetic feasibility and experimental validation).",
            "uuid": "e8667.15",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "CIDD",
            "name_full": "CIDD (Chain/Stages: interaction analysis, design, reflection pipeline for molecule optimization)",
            "brief_description": "An in-context-learning pipeline that structures molecule optimization into interaction analysis, design, and reflection steps, using previous designs and reflections in prompts to guide new edits.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "CIDD (ICL pipeline)",
            "model_type": "LLM used via multi-step in-context prompting",
            "model_size": null,
            "training_data": "Relies on few-shot examples provided in the prompt (no tuning)",
            "application_domain": "Molecule optimization (iterative multi-step edits guided by interaction profiles)",
            "generation_method": "In-context learning with a multi-step pipeline (provide prior designs and reflections as context)",
            "novelty_of_chemicals": null,
            "application_specificity": "Designs guided by interaction profiles and prior examples to produce consistent optimization edits",
            "evaluation_metrics": null,
            "results_summary": "Structuring prompts into analysis-design-reflection improves iterative optimization consistency in ICL settings.",
            "comparison_to_other_methods": "Improves over naive single-shot ICL by providing a procedural scaffold; less costly than SFT but less precise than tuned models.",
            "limitations_and_challenges": "Quality and number of in-context examples limit performance; scaling to complex multi-objective tasks remains hard.",
            "uuid": "e8667.16",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Molx: Enhancing large language models for molecular learning with a multi-modal extension",
            "rating": 2,
            "sanitized_title": "molx_enhancing_large_language_models_for_molecular_learning_with_a_multimodal_extension"
        },
        {
            "paper_title": "Tomg-bench: Evaluating llms on text-based open molecule generation",
            "rating": 2,
            "sanitized_title": "tomgbench_evaluating_llms_on_textbased_open_molecule_generation"
        },
        {
            "paper_title": "Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset",
            "rating": 2,
            "sanitized_title": "llasmol_advancing_large_language_models_for_chemistry_with_a_largescale_comprehensive_highquality_instruction_tuning_dataset"
        },
        {
            "paper_title": "Smileyllama: Modifying large language models for directed chemical space exploration",
            "rating": 2,
            "sanitized_title": "smileyllama_modifying_large_language_models_for_directed_chemical_space_exploration"
        },
        {
            "paper_title": "Synllama: Generating synthesizable molecules and their analogs with large language models",
            "rating": 2,
            "sanitized_title": "synllama_generating_synthesizable_molecules_and_their_analogs_with_large_language_models"
        },
        {
            "paper_title": "Chemllm: A chemical large language model",
            "rating": 2,
            "sanitized_title": "chemllm_a_chemical_large_language_model"
        },
        {
            "paper_title": "Large language models as molecular design engines",
            "rating": 2,
            "sanitized_title": "large_language_models_as_molecular_design_engines"
        },
        {
            "paper_title": "Crossing new frontiers: Knowledgeaugmented large language model prompting for zeroshot text-based de novo molecule design",
            "rating": 2,
            "sanitized_title": "crossing_new_frontiers_knowledgeaugmented_large_language_model_prompting_for_zeroshot_textbased_de_novo_molecule_design"
        },
        {
            "paper_title": "Mol-llm: Generalist molecular llm with improved graph utilization",
            "rating": 2,
            "sanitized_title": "molllm_generalist_molecular_llm_with_improved_graph_utilization"
        },
        {
            "paper_title": "ICMA: (in-context molecule tuning) / MolReFlect: Towards in-context fine-grained alignments between molecules and texts",
            "rating": 1,
            "sanitized_title": "icma_incontext_molecule_tuning_molreflect_towards_incontext_finegrained_alignments_between_molecules_and_texts"
        }
    ],
    "cost": 0.0243865,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization
22 May 2025</p>
<p>Ziqing Wang ziqingwang2029@u.northwestern.edu 
Northwestern University</p>
<p>Kexin Zhang kevin.kxzhang@gmail.com 
Northwestern University</p>
<p>Zihan Zhao zihanzhao2026@u.northwestern.edu 
Northwestern University</p>
<p>Yibo Wen 
Northwestern University</p>
<p>Abhishek Pandey abhishek.pandey@abbvie.com 
AbbVie</p>
<p>Han Liu hanliu@northwestern.edu 
Northwestern University</p>
<p>Kaize Ding kaize.ding@northwestern.edu 
Northwestern University</p>
<p>A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization
22 May 20253232DBBEBEDDA58BD456FDC74DDCC625arXiv:2505.16094v1[cs.LG]
Large language models (LLMs) are introducing a paradigm shift in molecular discovery by enabling text-guided interaction with chemical spaces through natural language, symbolic notations, with emerging extensions to incorporate multi-modal inputs.To advance the new field of LLM for molecular discovery, this survey provides an up-to-date and forward-looking review of the emerging use of LLMs for two central tasks: molecule generation and molecule optimization.Based on our proposed taxonomy for both problems, we analyze representative techniques in each category, highlighting how LLM capabilities are leveraged across different learning settings.In addition, we include the commonly used datasets and evaluation protocols.We conclude by discussing key challenges and future directions, positioning this survey as a resource for researchers working at the intersection of LLMs and molecular science.</p>
<p>Introduction</p>
<p>Molecular design and optimization are fundamental to multiple scientific disciplines, including drug discovery (Zheng et al., 2024), materials science (Grandi et al., 2025), and synthetic chemistry (Lu et al., 2024;Wang et al., 2025).However, these tasks present significant challenges due to the vast and complex chemical spaces that must be navigated to discover novel compounds with desirable properties while maintaining chemical validity and structural plausibility (Zheng et al., 2024;Yu et al., 2025).Over the years, a range of computational approaches has been developed to achieve these goals, from Variational Autoencoders (Gmez-Bombarelli et al., 2018) and Gen-* Equal Contribution  Corresponding Author erative Adversarial Networks (De Cao and Kipf, 2018) to Transformers (Edwards et al., 2022).However, these traditional methods often struggle with generating high-quality, diverse, and synthesizable molecules (Ramos et al., 2025;Sun et al., 2025).</p>
<p>More recently, large language models (LLMs) have emerged as particularly powerful tools for tackling these challenges, drawing increasing research attention (Zheng et al., 2024).These foundation models, characterized by billions of parameters, exhibit emergent capabilities such as advanced reasoning, instruction following, and in-context learning, enabled by extensive pre-training on diverse datasets (Brown et al., 2020;Wei et al., 2022a).Thus, LLMs can leverage their extensive pre-training knowledge to generalize across chemical problems and can be further adapted to specialized tasks through fine-tuning.These unique capabilities have established LLMs as a powerful new paradigm for exploring chemical space and accelerating molecular discovery.</p>
<p>Despite the growing interest in applying LLMs to molecular discovery tasks, existing literature reviews fail to provide a comprehensive analysis of this specific intersection.Most earlier surveys (Cheng et al., 2021;Zeng et al., 2022;Tang et al., 2024;Yang et al., 2024) focus broadly on general deep generative AI approaches rather than specifically examining LLMs' unique contributions.Other reviews that do mention LLMs (Ramos et al., 2025;Zhang et al., 2025;Guo et al., 2025;AbuNasser, 2024;Janakarajan et al., 2024;Liao et al., 2024) either primarily focus on the general chemical domain or include smaller language models lacking the emergent capabilities characteristic of the LLMs central to this survey.</p>
<p>Our survey addresses this critical gap by providing the first overview specifically focused on LLMs as generators in molecular discovery, with particu-lar emphasis on two central tasks: molecule generation and molecule optimization.Our survey specifically highlights how LLMs are deployed, adapted, and trained for navigating and manipulating complex chemical spaces, distinguishing their role from auxiliary functions like feature extraction (Liu et al., 2023) or control (Liu et al., 2024a).Unlike prior surveys that categorize studies based on model architectures (AbuNasser, 2024;Janakarajan et al., 2024), we introduce a new taxonomy centered on the learning paradigms employed to leverage LLMs for generative molecular tasks.As illustrated in Fig. 1, we distinguish between approaches that operate without LLM tuning (i.e., Zero-Shot Prompting and In-Context Learning) and those with LLM tuning (i.e., Supervised Fine-Tuning and Preference Tuning), allowing researchers to better understand the effectiveness and limitations of different LLM utilization strategies.</p>
<p>To summarize, we provide the first systematic review focused on LLMs for text-guided molecular discovery for both generation and optimization tasks.The main contributions are as follows:</p>
<p> We introduce a new taxonomy categorizing existing research based on learning paradigms, revealing how different approaches utilize LLMs' capabilities, alongside their respective advantages and limitations.</p>
<p> We provide a systematic summary of commonly used datasets, benchmarks, and evaluation metrics, offering a comprehensive reference for researchers in the field.</p>
<p> We identify critical challenges and outline promising future research directions to further advance this rapidly evolving domain of LLMcentric molecular discovery.</p>
<p>Preliminaries 2.1 Large Language Models</p>
<p>LLMs distinguish themselves from earlier Pretrained Language Models (PLMs) like BERT (Devlin et al., 2019) (which typically possessed millions of parameters) primarily through their massive scale, often boasting parameter counts in the billions, and the resultant emergent capabilities not found in smaller models (Zhao et al., 2023;Yang et al., 2023).The development of these LLMs and their advanced functionalities is largely attributed to their pre-training on vast text corpora, predom-inantly through an autoregressive next-token prediction objective.This immense scale facilitates emergent capabilities (Wei et al., 2022a) such as in-context learning (Brown et al., 2020), chain-ofthought reasoning (Wei et al., 2022b), and powerful zero-shot generalization, which are not consistently observed in their smaller predecessors.These advanced capabilities render LLMs uniquely suited for tackling complex chemical applications like molecule generation and optimization tasks central to this review.For clarity and scope within this survey, we focus specifically on foundation models with at least 1 billion (1B) parameters.</p>
<p>Problem Definition</p>
<p>In this survey, we focus on two central tasks:</p>
<p>Problem Definition 1 (LLM-centric Molecule Generation).This task leverages LLMs as the core generative engine for the de novo design of novel molecular structures based on specified input instructions.</p>
<p>Problem Definition 2 (LLM-centric Molecule Optimization).This task leverages LLMs to modify or edit a given input molecule, aiming to enhance one or more of its properties while often preserving essential structural characteristics.</p>
<p>As illustrated in Fig. 2, for both tasks, the input prompt provided to the LLM typically comprises three key components: (1)  (Yao et al., 2024), MolReGPT (Li et al., 2024c), FrontierX (Srinivas andRunkana, 2024) w/ Tuning Supervised Fine-Tuning Mol-instructions (Fang et al., 2023), LlaSMol (Yu et al., 2024a), ChemLLM (Zhang et al., 2024a), ICMA (Li et al., 2024b), MolReFlect (Li et al., 2024d), ChatMol (Fan et al., 2025), PEIT-LLM (Lin et al., 2025), NatureLM (Xia et al., 2025), SynLlama (Sun et al., 2025), TOMG-Bench (Li et al., 2024a), UniMoT (Zhang et al., 2024b) Preference Tuning Div-SFT (Jang et al., 2024), Mol-MOE (Calanzone et al., 2025), SmileyLLama (Cavanagh et al., 2024), ALMol (Gkoumas, 2024), Less for More (Gkoumas and Liakata, 2024), Mol-LLM (Lee et al., 2025) Optimization w/o Tuning Zero-Shot Prompting LLM-MDE (Bhattacharya et al., 2024), MOLLEO (Wang et al., 2025) In-Context Learning CIDD (Gao et al., 2025b), LLM-EO (Lu et al., 2024), MOLLM (Ran et al., 2025), ChatDrug (Liu et al., 2024c), Re 2 DF (Le and Chawla, 2024), BOPRO (Agarwal et al., 2025) w/ Tuning Supervised Fine-Tuning</p>
<p>MultiMol (Yu et al., 2025), DrugAssist (Ye et al., 2025), GeLLM 3 O (Dey et al., 2025), DrugLLM (Liu et al., 2024d), LLM-Enhanced GA (Bedrosian et al., 2024), Molx-Enhanced LLM (Le et al., 2024), TOMG-Bench (Li et al., 2024a) Preference Tuning NatureLM (Xia et al., 2025) Figure 1: A</p>
<p>Learning Paradigms</p>
<p>The application of LLMs to molecular discovery tasks, as depicted in the taxonomy in Fig. 2, can be broadly categorized based on whether the model's parameters are updated for the specific task.This distinction defines two primary learning paradigms:</p>
<p>Without LLM Tuning: These methods utilize pretrained LLMs directly, guiding their behavior solely through the input prompt I without modifying the model's weights.This paradigm primarily encompasses strategies like Zero-Shot Prompting, where the LLM operates based on instructions alone, and In-Context Learning (ICL), where few-shot examples provided within the prompt guide the model's responses.These approaches avoid computationally training but rely heavily on the LLM's inherent capabilities and effective prompt engineering.</p>
<p>With LLM Tuning: These methods involve adapting the pre-trained LLM by further training and updating its parameters to specialize it for molecular tasks or align its outputs with desired objectives.This typically includes Supervised Fine-Tuning (SFT), where the model learns from labeled task-specific datasets, and subsequent Preference Tuning (or Alignment), where the model is refined based on feedback.While tuning can significantly enhance performance, it requires curated data and computational resources.</p>
<p>Molecule Generation</p>
<p>Molecule generation, the computational creation of novel molecular structures, is a cornerstone of modern drug discovery and materials science (Elton et al., 2019).This section reviews recent advances in LLM-centric molecule generation, primarily categorizing approaches based on the learning paradigms defined in Section 2.3.</p>
<p>Molecule Generation without Tuning</p>
<p>In-Context Learning: Since Zero-Shot Prompting is challenging for general-purpose LLMs due to their lack of specialized chemical knowledge, most successful applications in this paradigm heavily rely on ICL to provide specific guidance.For instance, FrontierX (Srinivas and Runkana, 2024) uses knowledge-augmented prompting, supplying detailed instructions alongside few-shot examples within the prompt to guide de novo design effectively.Similarly, LLM4GraphGen (Yao et al., 2024)   ically retrieving highly relevant molecule-caption pairs to serve as more effective few-shot context, thereby boosting ICL performance.</p>
<p>Molecule Generation with Tuning</p>
<p>Supervised Fine-Tuning: While non-tuning methods leverage pre-trained knowledge effectively, their capabilities can be limited for highly specialized or complex generation tasks.SFT addresses this by adapting pre-trained LLMs specifically for molecule generation on labeled datasets, typically pairs of instructions and target molecular representations.Although early explorations demonstrated the viability of SFT using smaller PLMs such as MolGPT (Bagal et al., 2021) and MolT5 (Edwards et al., 2022), current research focuses on harnessing large foundation models.</p>
<p>A predominant SFT strategy is the curation of large-scale, high-quality instruction datasets to instill chemical knowledge into general-purpose LLMs (as shown in Fig. 5).Initiatives such as LlaS-Mol (Yu et al., 2024a) with its SMolInstruct dataset, ChemLLM (Zhang et al., 2024a) with Chem-Data, Mol-Instructions (Fang et al., 2023) covering broader biomolecular text, and the OpenMolIns dataset from TOMG-Bench (Li et al., 2024a) all exemplify this trend.These efforts fine-tune models like LLaMA-2-7B (Touvron et al., 2023) and Mistral-7B (Jiang et al., 2023) with LoRA (Hu et al., 2021), to enhance instruction following and performance on the molecule generation task.</p>
<p>Beyond broad instruction tuning, SFT methodologies also address specific challenges in molecule generation.A significant hurdle is ensuring that generated molecules precisely meet complex constraints.ChatMol (Fan et al., 2025) directly addresses this limitation by using a numerical enhancement technique, significantly improving the model's fidelity to specified quantitative property values.Concurrently, SynLlama (Sun et al., 2025) tackles synthetic feasibility to generate complete synthetic pathways.Other innovative SFT strategies include integrating dynamic context directly into the fine-tuning process; ICMA (Li et al., 2024b) and MolReFlect (Li et al., 2024d) propose In-Context Molecule Tuning (ICMT), which finetunes the LLM with relevant retrieved examples.Furthermore, PEIT-LLM (Lin et al., 2025) proposes a two-step Property Enhanced Instruction Tuning (PEIT) framework, first synthesizing instruction data with a multi-modal model, then using it to fine-tune LLMs for tasks like multi-constraint generation.NatureLM (Xia et al., 2025) demonstrates the application of SFT on models pre-trained across multiple scientific domains for tasks including text-instructed molecule generation.</p>
<p>However, the SFT methods discussed above primarily operate on text-based representations (like SMILES or SELFIES), which inherently struggle to explicitly encode rich structural information crucial for chemistry.Multi-modal SFT approaches aim to bridge this gap by incorporating these richer data types.UniMoT (Zhang et al., 2024b) exemplifies a solution by introducing a novel molecule tokenizer.Leveraging Vector Quantization (VQ) and a Causal Q-Former, this component converts graphbased molecular features into discrete "molecule tokens", enabling unified autoregressive processing of text and graph-derived molecular information.</p>
<p>Preference Tuning: Following SFT, which primarily teaches models to mimic static input-output patterns from datasets, Preference Tuning techniques offer further refinement by employing feedbackdriven learning to shape LLM outputs towards desired characteristics.In molecule generation, this feedback is typically incorporated in two main ways: (1) RL-based methods (Sutton et al., 1998) optimize the LLM (policy) using a scalar reward signal derived from evaluating generated molecules against desired criteria.(2) Offline methods like Direct Preference Optimization (DPO) learn from preference pairs ("chosen" vs "rejected") of molecules, training the LLM to assign higher likelihoods to the preferred candidates based on comparative evaluations.</p>
<p>SmileyLlama (Cavanagh et al., 2024) utilizes DPO after SFT to significantly improve adherence to specified property constraints by learning from preferences between correctly and incorrectly generated molecules.Mol-MoE (Calanzone et al., 2025) uses a preference objective to train a Mixtureof-Experts router for molecule generation.Furthermore, Div-SFT (Jang et al., 2024), after an initial SFT stage, employs RL with a reward function explicitly designed to maximize structural diversity among the generated molecules.Similarly, contrastive methods like Contrastive Preference Optimization (CPO) (Xu et al., 2024) have been used to refine the quality and relevance of generated molecules based on preference data comparing desired targets against less optimal alternatives, proving effective even with limited data (Gkoumas, 2024;Gkoumas and Liakata, 2024).</p>
<p>Preference Tuning is not limited to text-only input but also can handle multi-modal inputs after SFT.These approaches focus on improving how the model utilizes structural information, although this remains a more nascent area of research.For example, Mol-LLM (Lee et al., 2025) demonstrates better leveraging of 2D graph inputs through Molecular Structure Preference Optimization (MolPO).</p>
<p>After an initial SFT phase involving graph inputs, MolPO further trains the LLM using preference pairs where the distinction between "chosen" and "rejected" outputs is based on the correctness of the input molecular graph conditioning the generation.This preference learning implicitly guides the model to better integrate and leverage the provided structural information during processing.</p>
<p>Molecule Optimization</p>
<p>Molecule optimization is the task of refining molecular structures to improve one or more desired properties, such as solubility, binding affinity, or synthetic accessibility.Unlike molecule generation, optimization starts with an initial molecule and proposes targeted structural modifications to achieve specific goals.This section summarizes LLM-centric molecule optimization methods, with a focus on how different learning paradigms (see Section 2.3) are leveraged to guide optimization.</p>
<p>Molecule Optimization without Tuning</p>
<p>Zero-Shot Prompting: Zero-Shot Prompting leverages the pre-trained capabilities of LLMs to modify input molecules according to natural language instructions, without providing specific examples in the prompt.This setting assumes that the model can interpret molecular structure (often via SMILES) and property-related text well enough to perform molecule optimization.For example, LLM-MDE (Bhattacharya et al., 2024) guides optimization with natural language prompts that specify desired property changes and structural constraints, enabling controlled modifications to given parent molecules.MOLLEO, on the other hand, integrates LLMs into an evolutionary framework inspired by population-based algorithms (Jensen, 2019).It uses prompt-based sampling to generate candidates through mutations and crossovers, while applying filtering steps to enforce structural similarity.These methods demonstrate the flexibility of zero-shot prompting in expressing diverse optimization goals, though they often struggle with precise control in multi-objective settings.</p>
<p>In-Context Learning: In contrast, ICL incorporates examples of previous molecular edits into the prompt.This allows the LLM to learn optimization strategies by modifying new molecules in ways consistent with observed property improvements or structural changes.CIDD (Gao et al., 2025b) structures molecule optimization into a multi-step pipeline: interaction analysis, design, and reflection.Each step is guided by prompts derived from interaction profiles, and during the design step, previous designs and reflections are provided to make better modifications.</p>
<p>Both LLM-EO (Lu et al., 2024)  SFT also plays a key role in populationbased optimization frameworks.LLM-Enhanced GA (Bedrosian et al., 2024) proposes an iterative process in which new candidates are generated via prompt-based sampling from highperforming molecules, replacing traditional mutation and crossover.Explicit oracle modeling is incorporated through supervised fine-tuning on evaluated molecules when performance stagnates, allowing the LLM to progressively refine its understanding of structure-property relationships.</p>
<p>Beyond text-only molecule optimization, multi-</p>
<p>Benchmarking &amp; Evaluation</p>
<p>Datasets</p>
<p>Pretraining-Only ZINC (Irwin et al., 2012), PubChem (Kim et al., 2016(Kim et al., , 2019(Kim et al., , 2025)), ChemData (Zhang et al., 2024a), MuMOInstruct (Dey et al., 2025), Mol-Instructions (Fang et al., 2023) Benchmark-Only MoleculeNet (Wu et al., 2018), ChemBench (Mirza et al., 2024), MOSES (Polykovskiy et al., 2020), TOMG-Bench (Li et al., 2024a) Pretraining &amp; Benchmark ChEMBL (Gaulton et al., 2012), ChEBI-20 (Edwards et al., 2021), QM9 (Pinheiro et al., 2020), CrossDocked2020 (Francoeur et al., 2020), Dockstring (Garca-Ortegn et al., 2022), MolOpt-Instructions (Ye et al., 2025), L+M-24 (Edwards et al., 2024b), SMolInstruct (Yu et al., 2024b), OGBG-MolHIV (Hu et al., 2020) Metrics</p>
<p>Structure-Based</p>
<p>Validity &amp; Similarity</p>
<p>Validity (Polykovskiy et al., 2020), EM (Rajpurkar et al., 2016), BLEU (Papineni et al., 2002), Levenshtein (Levenshtein, 1966)  modal molecule optimization incorporates structural information such as molecular graphs and 3D geometries.These additional modalities enable more accurate modeling of structure-property relationships and improve control over chemical validity (Zhang et al., 2024c;Lin et al., 2024;Nakamura et al., 2025).Molx-Enhanced LLM (Le et al., 2024) exemplifies this approach with a framework that integrates SMILES strings, 2D molecular graphs, and handcrafted fingerprints into a unified embedding.It employs LLaMA-2-7B as the base LLM and introduces a trainable multi-modal module, MolX, which is pre-trained with supervised molecule-text pairs and auxiliary tasks to align molecular representations with the LLM's textual input space.Importantly, during fine-tuning, the use of graph encoders and fingerprint integration ensures that the model captures both global topology and substructural details, which are essential for chemically valid optimization.It indicates that fine-tuning the LLM to establish multi-modal models shows better performances than generalist chemical LLMs.</p>
<p>Preference Tuning: Preference Tuning aims to adjust large language models to better follow human instructions, preferences, or task-specific goals (Park et al., 2025;Chen et al., 2025)</p>
<p>Benchmarking and Evaluation</p>
<p>Rigorous benchmarking and comprehensive evaluation are crucial for tracking the progress of LLMcentric molecular discovery.This section provides an overview of the common resources and methodologies used, focusing on the datasets that form the basis of benchmarking efforts and the metrics applied for robust evaluation.Our discussion is structured around the taxonomy presented in Fig. 3.</p>
<p>Datasets</p>
<p>A variety of datasets serve as the foundation for training and benchmarking LLMs in molecular discovery, differing in their primary utility: Pretraining-Only Datasets provide vast quantities of unlabeled molecular structures or general chemical knowledge, such as ZINC (Irwin et al., 2012) and PubChem (Kim et al., 2025), or large-scale instruction collections like ChemData (Zhang et al., 2024a).Benchmark-Only Datasets are smaller, curated collections designed for specific evaluation tasks.Examples include TOMG-Bench (Li et al., 2024a) for open-domain molecule generation, and MOSES (Polykovskiy et al., 2020) for de novo design benchmarking.A third category comprises datasets suitable for both Pre-training and Benchmark applications, offering a balance of scale and task-specificity.Notable examples include ChEMBL (Gaulton et al., 2012) for bioactivity data, and instruction datasets like SMolInstruct (Yu et al., 2024b).Further details and a comparative summary of these and other relevant datasets are available in Appendix B and Table 1.</p>
<p>Metrics</p>
<p>The</p>
<p>Conclusion and Future Work</p>
<p>This survey presents the first comprehensive review of recent advances in LLM-centric molecular discovery, covering both generation and optimization.We introduce a novel taxonomy distinguishing approaches based on different learning paradigms-specifically, without LLM tuning (e.g., zero-shot prompting, in-context learning) versus with LLM tuning (e.g., supervised fine-tuning, pref-erence tuning).This framework allows for a systematic analysis of how current strategies leverage LLM capabilities, revealing key trends, strengths, and limitations.The rapid progress in this field underscores LLMs' transformative potential to accelerate scientific discovery in chemistry and related disciplines.However, several challenges and exciting opportunities remain for future research:</p>
<p>Trustworthy Generation and Hallucination Mitigation: While LLMs can generate chemically plausible molecules, they often produce outputs that are chemically invalid or factually incorrect without domain-specific supervision (Le and Chawla, 2024).This lack of transparency limits their applicability in high-stakes domains such as drug development (Ma et al., 2025).While interpretable prompting and rationalization techniques (Xiao et al., 2025) offer promising solutions, controlled hallucinations may actually serve as a creativity mechanism, potentially uncovering novel molecular scaffolds inaccessible through conventional search methods (Edwards et al., 2024a;Yuan and Frber, 2025).The future challenge lies not in eliminating hallucinations entirely, but in developing frameworks that can distinguish between harmful fabrications and beneficial creative leaps.</p>
<p>LLM Agents for Interactive Discovery: LLMs are increasingly being integrated into agent-based frameworks, where they coordinate with external tools (e.g., retrosynthesis engines, docking software, or lab automation platforms) to complete multi-step discovery workflows (Feng et al., 2025;Liu et al., 2025a).Building robust LLM agents that can plan, reason, and interact with both humans and tools could enable more flexible and goal-directed molecular design (Gao et al., 2025a).These agents could potentially close the loop between computational prediction and experimental validation, accelerating the iterative discovery process.</p>
<p>Multi-Modal Modeling and Alignment: Incorporating multiple molecular modalities remains a core challenge.Most current LLM-based approaches typically treat modalities separately, with limited cross-modal interaction.Future work should prioritize architectures that unify these representations, allowing joint encoding and reasoning over chemical topology, geometry, and textual semantics (Lu et al., 2023;Pirnay et al., 2025).By developing sophisticated tokenization and fusion techniques that bridge discrete and continuous representations, future systems could achieve a more holistic understanding of chemical structures and properties, potentially leading to more accurate and innovative molecular designs.</p>
<p>Limitations</p>
<p>This survey focuses on the use of large language models for two core tasks in text-guided molecular discovery: molecule generation and molecule optimization.These tasks represent the most direct applications of LLMs in molecular design and are the primary scope of current research.We are aware that LLMs can also significantly impact other important areas of molecular science (Sun et al., 2025), such as reaction prediction, retrosynthesis, protein-ligand modeling, and automated experimentation (Zhang et al., 2024d;Liu et al., 2024bLiu et al., , 2025b)).Given the broad and rapidly evolving landscape, we leave a systematic review of these directions to future work.By narrowing the scope of this work, we provide a focused and detailed resource for researchers working on LLMdriven molecular design In the future, we anticipate expanding this analysis to encompass these additional domains as the field continues to evolve.</p>
<p>A Data Modalities for Molecular LLMs:</p>
<p>LLMs used for molecular generation and optimization interface with structured molecular data in various modalities.Each modality offers distinct structural or physicochemical information.As shown in Fig. 4, commonly used molecular representations can be categorized into the following three formats:</p>
<p> 1D Sequence Representations (S): These are linear string encodings of molecular structures.</p>
<p>Common formats include SMILES (Simplified Molecular Input Line Entry System) (Weininger, 1988) and SELFIES (Self-Referencing Embedded Strings) (Krenn et al., 2020).These representations are well-suited for LLMs due to their compatibility with token-based language modeling.Another format used in certain settings is the IUPAC nomenclature (Favre and Powell, 2014), which provides systematic names for molecules and is employed as an alternative or auxiliary textual representation in language modeling frameworks.</p>
<p> 2D Graph Representations (G): A molecule is represented as a graph G = (V, E), where nodes v  V correspond to atoms and edges e  E correspond to chemical bonds.Node and edge features may encode atom types, bond orders, aromaticity, and other topological attributes.</p>
<p>While not directly token-based, 2D graphs can be integrated via hybrid models that combine language and graph encoders, or serialized (e.g., via adjacency lists or graph traversal sequences) to interface with LLMs.</p>
<p> 3D Geometric Representations (X): These representations capture atomic coordinates in threedimensional space.Formally, X = {(a i ,  r i )} N i=1 , where a i denotes the atomic species and  r i  R 3 specifies the Cartesian coordinates of atom i. 3D information is essential for modeling stereochemistry, conformational preferences, and interaction potentials.Incorporating 3D data into LLMs typically requires transforming it into a sequencecompatible format or using auxiliary models to predict or refine 3D structures.</p>
<p>B Datasets</p>
<p>Datasets are crucial resources for advancing LLMcentric molecule design, serving extensively in both the training and evaluation phases of model development.Table 1 provides a comprehensive summary of commonly utilized molecule datasets, detailing their key features.For each dataset listed, the table specifies its Last Update year, approximate Scale (number of entries), whether it includes natural language Instruction components, and its suitability for Pretraining LLMs or as a Benchmark for evaluation.Furthermore, the table indicates the types of Molecule Representations available within each dataset, such as SMILES, IU-PAC names, ready-to-dock formats (Dock), graph structures (Graph), 3D coordinates (3D), or formal chemical ontologies (Ontology).Finally, it highlights whether a dataset supports Generation or Optimization tasks, lists Other Tasks it is commonly used for (e.g., property prediction, translation), and provides a Link to access the resource.</p>
<p>The subsequent subsections categorize these datasets based on their primary application focus, aligning with the classification used in Section 5 of the main text.</p>
<p>B.1 Pretraining-Only Datasets</p>
<p>Pretraining-only datasets typically contain diverse molecular structures and associated property information, designed to support broad generalization capabilities when pretraining LLMs for downstream tasks.These datasets generally do not include explicit natural language instructions or taskspecific labels for direct supervised learning of specific generation or optimization objectives.</p>
<p> ZINC: ZINC (Irwin et al., 2012) is a public and comprehensive database containing over 20 million commercially available molecules presented in biologically relevant representations.These molecules can be downloaded in popular readyto-dock formats and various subsets, making ZINC widely used for distribution learning-based and goal-oriented molecule generation tasks.</p>
<p> PubChem: PubChem (Kim et al., 2016(Kim et al., , 2019(Kim et al., , 2025) ) serves as a vast public chemical information repository, holding over 750 million records.It covers a wide array of data, including chemical structures, identifiers, bioactivity outcomes, genes, proteins, and patents, and is organized into three interlinked databases: Substance (contributed chemical information), Compound (standardized unique structures), and BioAssay (biological experiment details).</p>
<p> ChemData: ChemData (Zhang et al., 2024a) is a large-scale dataset specifically curated for  Mol-Instructions: Mol-Instructions (Fang et al., 2023) is a large-scale, diverse, and highquality dataset designed for the biomolecular domain, featuring over 2 million carefully curated biomolecular instructions.It is structured around three core components: molecule-oriented instructions (148.4Kacross six tasks focusing on properties, reactions, and design), proteinoriented instructions (505K samples across five task categories related to protein structure, function, and design), and biomolecular text instructions (53K for bioinformatics and chemoinformatics NLP tasks like information extraction and question answering).
CC(=O)NC1=CC =C(C=C1)O
 MuMOInstruct: MuMOInstruct (Dey et al., 2025) is presented as the first high-quality instruction-tuning dataset focused on complex, multi-property molecular optimization tasks.Unlike datasets such as MolOpt-Instruction (Ye et al., 2025) that primarily target single-or dualproperty tasks, MuMOInstruct emphasizes tasks involving at least three properties, facilitating the evaluation of LLMs in both in-domain and out-of-domain settings.</p>
<p>B.2 Benchmark-Only Datasets</p>
<p>Benchmark-only datasets are specifically curated for the evaluation of models, particularly in generative molecular tasks.These datasets often feature structured input-output pairs, such as instructionmolecule pairings, and are typically smaller in scale, manually verified, and tailored to specific evaluative purposes.</p>
<p> MoleculeNet: A large-scale benchmark com-pendium, MoleculeNet (Wu et al., 2018) is derived from multiple public databases.It comprises 17 curated datasets with over 700,000 compounds, represented textually (e.g., SMILES) and in 3D formats.Covering a wide array of properties categorized into quantum mechanics, physical chemistry, biophysics, and physiology, it serves as a standard for evaluating molecular property prediction models.</p>
<p> ChemBench: ChemBench (Mirza et al., 2024) offers a comprehensive framework for benchmarking the chemical knowledge and reasoning abilities of LLMs.It consists of thousands of manually curated question-answer pairs from diverse sources, focusing on three core aspects: Calculation, Reasoning, and Knowledge. MOSES: MOSES (Molecular Sets) (Polykovskiy et al., 2020) is a task-specific resource designed for both training and benchmarking molecule generation models in drug discovery.Containing approximately 1.9 million molecules in SMILES format derived from the ZINC Clean Leads dataset, it also furnishes training, testing, and scaffold-split subsets, along with built-in evaluation metrics.</p>
<p>B.3 Datasets for Pretraining &amp; Benchmark Applications</p>
<p>A distinct category of datasets offers the flexibility to be used for both pretraining LLMs and for subsequent benchmarking.These resources often combine substantial scale with features amenable to diverse evaluation scenarios.(Kim et al., 2016(Kim et al., , 2019(Kim et al., , 2025) ) 2025  dataset for instruction tuning LLMs in chemistry.
119M            Property Prediction &amp; Biology Domain Link ChEMBL (Gaulton et al., 2012) 2024 &gt;20M            Prediction &amp; ML Benchmark Link CrossDocked2020 (Francoeur et al., 2020) 2024 22.5M            Docking Datasets Link ZINC (Irwin et al., 2012) 2023 &gt;980M            Ligand Discovery Link Dockstring (Garca-Ortegn et al., 2022) 2022 &gt;260k            Virtual Screening Link ChEBI-20 (Edwards et al., 2021) 2021 33k            Translation &amp; Classification &amp; Captioning Link OGBG-MolHIV (Hu et al., 2020) 2020 41k            Graph
It consists of 3.3 million language-molecule pairs and 1.6 million distinct molecules, covering four types of molecular representations and 14 different tasks, with molecules represented in SMILES or SELFIES format.</p>
<p> OGBG-MolHIV: OGBG-MolHIV (Hu et al., 2020), part of the Open Graph Benchmark, is an open-access, task-specific dataset for binary molecular property prediction, specifically for classifying HIV inhibition.It contains 41,127 unique molecules in graph format, where nodes (atoms) have 9 numerical features and edges (bonds) have 3-dimensional features (type, stereochemistry, conjugation).It is derived from MoleculeNet and preprocessed using RDKit.</p>
<p> MolOpt-Instructions: MolOpt-Instructions (Ye et al., 2025) is an instruction-based dataset tailored for molecule optimization, containing over 1 million molecule-molecule pairs.It was constructed by selecting molecules from ZINC and using MMPDB to generate and filter for highly similar pairs, covering six molecular properties including solubility, BBBP, and hERG inhibition.</p>
<p> L+M-24: L+M-24 (Language + Molecules 24 Tasks) (Edwards et al., 2024b) is a large-scale, multi-task instruction dataset designed to leverage the benefits of natural language (compositionality, functionality, abstraction) in molecule design.Derived from PubChem and other sources, it contains over 148,000 language-molecule pairs spanning 24 distinct molecule design tasks across various application domains.</p>
<p>C Evaluation Metrics</p>
<p>In the context of LLM-centric molecular generation and optimization, evaluation metrics are commonly grouped into categories based on molecular structure, physicochemical properties, and optimization success, each reflecting distinct aspects of molecular quality and model performance.This appendix details these metrics, aligning with the categorization presented in Fig. 3.</p>
<p>C.1 Structure-Based Metrics</p>
<p>Structure-based metrics are employed to assess the chemical plausibility, resemblance to reference compounds, and structural diversity of molecules generated or modified by LLMs.These metrics help ensure that the outputs are chemically meaningful and cover a sufficient breadth of the relevant chemical space.</p>
<p>C.1.1 Validity &amp; Similarity</p>
<p>Metrics for validity and similarity evaluate the extent to which generated molecules conform to chemical rules, match the structural features of reference molecules, and satisfy specified structural constraints.They are crucial for determining if the outputs are chemically sound and potentially useful for applications like drug discovery.</p>
<p> Validity Rate: The validity rate (Polykovskiy et al., 2020) indicates the fraction of generated molecules that are chemically valid (e.g., parsable by RDKit) and often also considers uniqueness among valid structures.A high validity rate suggests that the LLM has effectively learned the underlying rules of molecular representation (e.g., SMILES grammar) and the context provided by textual descriptions.</p>
<p> EM (Exact Match): Exact Match (Rajpurkar et al., 2016) assesses whether a generated molecular sequence is identical to a target reference sequence.A higher EM rate signifies a stronger capability of the model to precisely replicate reference molecules when required.</p>
<p> BLEU (Bilingual Evaluation Understudy):</p>
<p>This score (Papineni et al., 2002), originally from machine translation, measures the n-gram overlap between generated and reference molecular sequences.A higher BLEU score indicates greater similarity in token order and composition, reflecting better fidelity to the ground truth molecule's sequence.</p>
<p> Levenshtein Distance: The Levenshtein distance (Levenshtein, 1966) quantifies the dissimilarity between two strings by counting the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.A lower Levenshtein distance signifies higher similarity between the generated and reference molecular sequences.</p>
<p> FTS (Fingerprint Tanimoto Similarity): FTS is a widely used metric for quantifying structural similarity based on molecular fingerprints, such as MACCS (Durant et al., 2002), RDKit topological fingerprints (RDK) (Landrum et al., 2013), or Morgan fingerprints (circular fingerprints) (Morgan, 1965).A higher FTS score (typically ranging from 0 to 1) indicates a greater overlap in key substructures and chemical patterns between the generated and reference molecules.</p>
<p> FCD (Frchet ChemNet Distance): FCD (Preuer et al., 2018) evaluates the dissimilarity between the distribution of features (derived from a pretrained chemical neural network) of generated molecules and a reference set (often ground truth molecules).Lower FCD values suggest that the generated molecules better capture the chemical diversity and property distribution of the reference set.</p>
<p>C.1.2 Diversity &amp; Uniqueness</p>
<p>Metrics related to diversity, uniqueness, and novelty assess an LLM's ability to produce a varied set of outputs.High performance in these areas can help prevent mode collapse and enhance the exploration of chemical space for discovering novel and relevant molecules.</p>
<p> Uniqueness: Uniqueness quantifies the proportion of valid generated molecules that are distinct from each other within a given set.It reflects the model's capacity to generate diverse structures rather than redundant outputs.This is often evaluated at different scales, such as Unique@1k (within the first 1,000 valid samples) (Wang et al., 2023) and Unique@10k (within 10,000 valid samples) (Bagal et al., 2021), to assess shortrange and broader diversity, respectively.</p>
<p> Novelty Rate: The novelty rate (Brown et al., 2019) measures the fraction of valid and unique generated molecules that are not present in the training dataset.It serves as an indicator of the model's generalization ability and its potential to discover previously unseen chemical entities.A low novelty rate may suggest overfitting.</p>
<p> IntDiv (Internal Diversity) and NCircles: These metrics further characterize the structural diversity within a set of generated molecules.Int-Div (Benhenda, 2017) calculates the average dissimilarity (1 minus Tanimoto similarity) between all pairs of molecules in the generated set, often using a power mean to adjust sensitivity:
IntDiv p (S) = 1    1 |S| 2 s i ,s j S T (s i , s j ) p   1 p
where T (s i , s j ) is the Tanimoto similarity between molecules s i and s j .NCircles (Jang et al., 2024) measures the size of the largest subset of generated molecules where no two molecules have a Tanimoto similarity exceeding a predefined threshold.A higher NCircles value indicates greater structural dissimilarity within the set.</p>
<p>C.2 Property-Based Metrics</p>
<p>Property-based metrics evaluate whether a designed or modified molecule satisfies specific physicochemical or biological property constraints, often crucial for assessing its potential utility, such as drug-likeness or target activity.</p>
<p>C.2.1 Single-Property Evaluation Metrics</p>
<p>In single-property evaluation, the primary goal is to assess the model's ability to generate or optimize molecules with respect to a specific molecular property, such as drug-likeness, solubility, or binding affinity.</p>
<p> LogP (Octanol-Water Partition Coefficient): LogP (Hansch et al., 1968) is the logarithm of a compound's partition coefficient between octanol and water, serving as a key indicator of molecular hydrophobicity and thus, often correlating with membrane permeability.</p>
<p> QED (Quantitative Estimate of Druglikeness): QED (Bickerton et al., 2012) provides a heuristic score (ranging from 0 to 1) that integrates multiple physicochemical properties (e.g., molecular weight, LogP, number of hydrogen bond donors/acceptors) to estimate a compound's overall drug-likeness.</p>
<p> TPSA (Topological Polar Surface Area): TPSA (Ertl et al., 2000) quantifies the surface sum over all polar atoms in a molecule, reflecting its ability to form hydrogen bonds.It is often correlated with properties like intestinal absorption and blood-brain barrier penetration.</p>
<p> SA Score (Synthetic Accessibility Score): The SA score (Ertl and Schuffenhauer, 2009) estimates the ease of synthesizing a compound, typically on a scale from 1 (easy to synthesize) to 10 (very difficult to synthesize), based on fragment contributions and complexity penalties.</p>
<p>C.2.2 Multi-Property Evaluation Metrics</p>
<p>Multi-property evaluation assesses a model's performance in satisfying multiple, often competing, objectives simultaneously.This is critical in realworld scenarios where a balance of several properties is required.</p>
<p> Composite Score: A composite score (Jin et al., 2020) aggregates multiple individual property scores into a single scalar objective, often through a weighted sum or other combination rules.This allows optimization frameworks (e.g., evolutionary algorithms, reinforcement learning) to be guided by a unified fitness metric.The weights can be adjusted to reflect task-specific priorities among properties like LogP, QED, and synthetic accessibility.</p>
<p> Pareto Optimality: In multi-objective optimization, a solution is considered Pareto optimal (Pareto, 1919) if none of its objective function values can be improved without degrading at least one of the other objective values.The set of all Pareto optimal solutions forms the Pareto front, which is used to visualize and analyze trade-offs between conflicting objectives.</p>
<p> Success Rate under Constraints: This metric (Jin et al., 2020) quantifies the proportion of generated or modified molecules that successfully meet or exceed predefined target thresholds across all specified properties.A common instantiation is the multi-property hit ratio, where a molecule is deemed successful only if all targeted property improvements satisfy their respective criteria.</p>
<p>D Method Summary</p>
<p>This section provides a consolidated overview of representative LLM-based methods for molecular discovery, as detailed in  Venue: The publication venue or preprint archive where the method was reported.</p>
<p> Input Type: Specifies the primary format of molecular data and instructions provided to the LLM (e.g., SMILES strings, textual instructions, few-shot examples, or multi-modal inputs like graphs).</p>
<p> Base Model: Indicates the foundational LLM architecture (e.g., GPT-4, LLaMA variants, Mistral) upon which the method is built or applied.</p>
<p> Dataset: Lists the key molecular corpora or benchmarks used for training the model (if applicable) or for its evaluation in the context of the reported work.</p>
<p> Repository: Provides a link to the public code or resource repository, if available.</p>
<p>This structured presentation aims to offer a clear comparative landscape of the current methodologies in the field.</p>
<p>FewFigure 2 :
2
Figure 2: Overview of LLM-Centric Molecular Discovery.Left: Typical input components (Instruction, Few-Shot Examples, Property Constraints) for molecule generation and optimization.Right: Core learning paradigms for applying LLMs to Zero-Shot Prompting &amp; In-Context Learning, Supervised Fine-Tuning and Preference Tuning.</p>
<p>Figure 3 :
3
Figure 3: A Taxonomy of Benchmarking &amp; Evaluation in Molecule Discovery.</p>
<p>Figure 4 :
4
Figure 4: Illustration of an example molecule and its representation in different data modalities.From left to right following the 2D chemical structure diagram: its 1D SMILES string representation, a simplified 2D graph view, and its 3D ball-and-stick model.</p>
<p></p>
<p>TOMG-Bench: As the first benchmark dedicated to the open-domain molecule generation capabilities of LLMs, TOMG-Bench (Text-based Open Molecule Generation Benchmark) (Li et al., 2024a) contains 45,000 samples.It is structured around three primary tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom).</p>
<p>Figure 5 :
5
Figure 5: Visualization of the Instruction dataset of molecule generation and optimization task.</p>
<p>Taxonomy of LLM-Centric Molecular Discovery.</p>
<p>an initial molecule M x that requires modification.This initial molecule can be represented in various formats, such as a 1D sequence (e.g., SMILES), a 2D graph, or 3D coordinates (see Appendix A for details).The instruction typically specifies which properties should be improved.The objective is to generate a chemically valid modified molecule (e.g., S My ) that enhances the desired properties of M x while adhering to any specified constraints C p , potentially guided by few-shot examples E f s .</p>
<p>Instruction Query: Help me modify the molecule CC(C)C(=O)O to increase hydrophobicity while keeping it similar to the input molecule? Few-Shot Examples Query: Modify COc1ccccc1 to
InstructionIn-ContextExamplesincreasehydrophobicity while maintainingsimilarity.Response: CCOc1ccccc1Property ConstrainsRequired Tanimoto Similairy  0.6explores property-based generation byprompting LLMs with target properties and rele-vant molecular examples, evaluating performanceunder different prompting strategies, including few-shot ICL. Recognizing the importance of examplequality, MolReGPT (Li et al., 2024c) incorporatesRetrieval-Augmented Generation (RAG), dynam-</p>
<p>Table 1 :
1
Summary of commonly used molecule datasets and their features.Dock denotes the "ready-to-dock" format; Ontology denotes the structured representation of the molecule; Captioning denotes molecule captioning task; Docking denotes molecule docking (a way to find correct molecule binds for proteins); Translation denotes the translation from textual knowledge to molecular features; Conversion denotes the translation between different representations of a molecule's identity; Prediction denotes property prediction, forward reaction prediction and retrosynthesis tasks; QM denotes hybrid quantum mechanics.
DatasetsLast UpdateScaleInstruc-tionPretrain-ingBench -markMolecule Representations SMILES IUPAC Dock Graph 3D OntologyGenera-tionOptimi-zationOther TasksLinkPubChem</p>
<p>Table 2 .
2
The table organizes these approaches primarily by the two core task categories central to this survey: molecule generation and molecule optimization.Within each task, methods are further sub-categorized by their primary learning Strategy (referred to as "Category" and "Technique" in the table), encompassing approaches without LLM tuning (such as zero-shot prompting and in-context learning) and those with LLM tuning (supervised fine-tuning and preference tuning).
Table 2 details several key aspects for each listedMethod:</p>
<p>Table 2 :
2
Summary of LLM-based methods for molecule generation and optimization.Each row corresponds to a method, organized by Task (generation or optimization), and Technique.Input Type denotes the molecular data format provided to the model.Base Model denotes the large language model architecture used.Dataset denotes the molecular corpus or benchmark used for training or evaluation.</p>
<p>Large language models in drug discovery: A survey. Raghad Abunasser, 2024</p>
<p>Searching for optimal solutions with llms via bayesian optimization. Dhruv Agarwal, Manoj Ghuhan Arivazhagan, Rajarshi Das, Sandesh Swamy, Sopan Khosla, Rashmi Gangadharaiah, The Thirteenth International Conference on Learning Representations. Viraj Bagal, Rishal Aggarwal, PK Vinod, and U Deva Priyakumar. 2025. 202162Molgpt: molecular generation using a transformer-decoder model</p>
<p>Small molecule optimization with large language models. Menua Bedrosian, Philipp Guevorguian, Tigran Fahradyan, Gayane Chilingaryan, Hrant Khachatrian, Armen Aghajanyan, Neurips 2024 Workshop Foundation Models for Science: Progress, Opportunities, and Challenges. 2024</p>
<p>Chemgan challenge for drug discovery: can ai reproduce natural chemical diversity?. Mostapha Benhenda, arXiv:1708.082272017arXiv preprint</p>
<p>Large language models as molecular design engines. Debjyoti Bhattacharya, Harrison J Cassady, Michael A Hickner, Wesley F Reinhart ; Gr Bickerton, Paolini, Besnard, A L Muresan, Hopkins, Journal of Chemical Information and Modeling. 64182024. 2012Nature Chemistry</p>
<p>Guacamol: Benchmarking models for de novo molecular design. Nathan Brown, Marco Fiscato, Marwin Hs Segler, Alain C Vaucher, 10.1021/acs.jcim.8b00839Journal of chemical information and modeling. 5932019</p>
<p>Language models are fewshot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 202033</p>
<p>Mol-moe: Training preference-guided routers for molecule generation. Diego Calanzone, D' Pierluca, Pierre-Luc Oro, Ba, arXiv:2502.056332025arXiv preprint</p>
<p>Kunyang Joseph M Cavanagh, Andrew Sun, Dorian Gritsevskiy, Thomas D Bagni, Teresa Bannister, Head-Gordon, arXiv:2409.02231Smileyllama: Modifying large language models for directed chemical space exploration. 2024arXiv preprint</p>
<p>Generalists vs. specialists: Evaluating llms on highly-constrained biophysical sequence optimization tasks. Angelica Chen, Samuel D Stanton, Frances Ding, Robert G Alberstein, Andrew M Watkins, Richard Bonneau, Vladimir Gligorijevi, Kyunghyun Cho, Nathan C Frey, 2025</p>
<p>Molecular design in drug discovery: a comprehensive review of deep generative models. Yu Cheng, Yongshun Gong, Yuansheng Liu, Bosheng Song, Quan Zou, Briefings in bioinformatics. 2263442021</p>
<p>. Zhilian Dai, Jie Zhang, Songyou Zhong, Jiawei Fu, Yangyang Deng, Dan Zhang, Yichao Liu, Peng Gao, </p>
<p>A zero-shot single-point molecule optimization model: Mimicking medicinal chemists' expertise. </p>
<p>Molgan: An implicit generative model for small molecular graphs. Nicola De, Cao , Thomas Kipf, arXiv:1805.119732018arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies. the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies20191</p>
<p>Gellm 3 o: Generalizing large language models for multi-property molecule optimization. Vishal Dey, Xiao Hu, Xia Ning, ; Durant, Leland, Henry, Nourse, 10.1021/ci010132rarXiv:2502.13398Journal of Chemical Information and Computer Sciences. 4262025. 2002arXiv preprintReoptimization of mdl keys for use in drug discovery</p>
<p>Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, Heng Ji, arXiv:2204.11817Translation between molecules and natural language. 2022arXiv preprint</p>
<p>Lan-guage+ molecules. Carl Edwards, Qingyun Wang, Heng Ji, Proceedings of the 18th Conference of the European Chapter. the 18th Conference of the European ChapterTutorial Abstracts2024a</p>
<p>Carl Edwards, Qingyun Wang, Lawrence Zhao, Heng Ji, arXiv:2403.00791L+ m-24: Building a dataset for language+ molecules@ acl 2024. 2024barXiv preprint</p>
<p>Text2mol: Cross-modal molecule retrieval with natural language queries. Carl Edwards, Chengxiang Zhai, Heng Ji, EMNLP. 2021</p>
<p>Deep learning for molecular design-a review of the state of the art. Zois Daniel C Elton, Boukouvalas, Peter W Mark D Fuge, Chung, Molecular Systems Design &amp; Engineering. 442019</p>
<p>Fast calculation of molecular polar surface area as a sum of fragment-based contributions and its application to the prediction of drug transport properties. Peter Ertl, Bernhard Rohde, Paul Selzer, 10.1021/jm000942e?casa_token=Xba52t19k6gAAAAA:lRhh2ZvvUnCjjYFQ9W4Lv_WxVAQi8j8uXamgyjSonH-soDpf8lnyoZZ8G77JeZBUypMYQbBfg7IANEzKSwJournal of Medicinal Chemistry. 43202000</p>
<p>Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Peter Ertl, Ansgar Schuffenhauer, 10.1186/1758-2946-1-8.pdfJournal of Cheminformatics. 1182009</p>
<p>. Chuanliu Fan, Ziqiang Cao, Zicheng Ma, Nan Yu, Yimin Peng, Jun Zhang, Yiqin Gao, Guohong Fu, </p>
<p>Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, arXiv:2502.19794arXiv:2306.08018Xiaohui Fan, and Huajun Chen. 2023. Mol-instructions: A large-scale biomolecular instruction dataset for large language models. arXiv preprintChatmol: A versatile molecule designer based on the numerically enhanced large language model</p>
<p>Nomenclature of Organic Chemistry: IUPAC Recommendations and Preferred Names. A Henri, Warren H Favre, Powell, 10.1039/9781849733069Royal Society of Chemistry. 2014. 2013</p>
<p>Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, Wanjun Zhong, arXiv:2504.11536Reinforcement learning for strategic tool use in llms. 2025arXiv preprint</p>
<p>Three-dimensional convolutional neural networks and a cross-docked data set for structure-based drug design. Tomohide Paul G Francoeur, Jocelyn Masuda, Andrew Sunseri, Richard B Jia, Ian Iovanisci, David R Snyder, Koes, 10.1021/acs.jcim.0c00411Journal of chemical information and modeling. 6092020</p>
<p>Dockstring: easy molecular docking yields better benchmarks for ligand design. Bowen Gao, Yanwen Huang, Yiqiao Liu, Wenxuan Xie, Wei-Ying Ma, Ya-Qin Zhang, Yanyan Lan ; Bowen, Yanwen Gao, Yiqiao Huang, Wenxuan Liu, Wei-Ying Xie, Ya-Qin Ma, Yanyan Zhang, Lan, 10.1021/acs.jcim.1c01334arXiv:2503.22164arXiv:2503.01376Pushing the boundaries of structure-based drug design through collaboration with large language models. Austin J Tripp2025a. 2025b. 202262arXiv preprintMiguel Garca-Ortegn, Gregor NC Simm</p>
<p>Chembl: a large-scale bioactivity database for drug discovery. Anna Gaulton, Louisa J Bellis, Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne Light, Shaun Mcglinchey, David Michalovich, Nucleic acids research. 40D12012Bissan Al-Lazikani, and 1 others</p>
<p>Less for more: Enhanced feedback-aligned mixed llms for molecule caption generation and fine-grained nli evaluation. Dimitris Gkoumas, 10.1021/acscentsci.7b00572arXiv:2405.08619arXiv:2405.13984ACS central science. Rafael Gmez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos Miguel Hernndez-Lobato, Benjamn Snchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Aln Aspuru-Guzik422024. 2024. 2018arXiv preprintAutomatic chemical design using a data-driven continuous representation of molecules</p>
<p>Evaluating large language models for material selection. Daniele Grandi, Yash Patawari Jain, Allin Groom, Brandon Cramer, Christopher Mccomb, Journal of Computing and Information Science in Engineering. 252210042025</p>
<p>Yukun Jiang, and 1 others. 2025. A survey of large language model for drug research and development. Huijie Guo, Xudong Xing, Yongjie Zhou, Wenjiao Jiang, Xiaoyi Chen, Ting Wang, Zixuan Jiang, Yibing Wang, Junyan Hou, IEEE Access</p>
<p>Linear free-energy relationship between partition coefficients and the aqueous solubility of organic liquids. Corwin Hansch, John E Quinlan, Gary L Lawrence, 10.1021/jo01265a071The journal of organic chemistry. 3311968</p>
<p>Weizhu Chen, and 1 others. 2021. Lora: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Wang, ICLR. </p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, 202033</p>
<p>Zinc: a free tool to discover chemistry for biology. Teague John J Irwin, Sterling, Erin S Michael M Mysinger, Ryan G Bolstad, Coleman, 10.1021/ci3001277Journal of chemical information and modeling. 5272012</p>
<p>Language models in molecular discovery. Nikita Janakarajan, Tim Erdmann, Sarath Swaminathan, Teodoro Laino, Jannis Born, arXiv:2410.03138Hyosoon Jang, Yunhui Jang, Jaehyung Kim, and Sungsoo Ahn. 2024. Can llms generate diverse molecules? towards alignment with structural diversity. Springer2024arXiv preprintDrug Development Supported by Informatics</p>
<p>A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. Jan H Jensen, Chemical science. 10122019</p>
<p>Guillaume Lample, Lucile Saulnier, and 1 others. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>. Wengong Jin, Regina Barzilay, Tommi Jaakkola, </p>
<p>Multi-objective molecule generation using interpretable substructures. International Conference on Machine Learning. PMLR</p>
<p>Pubchem 2019 update: improved access to chemical data. Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Nucleic acids research. 47D11 others. 2019</p>
<p>. Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yuand 1 others</p>
<p>Pubchem 2025 update. Nucleic Acids Research. 53D1</p>
<p>Pubchem substance and compound databases. Sunghwan Kim, Paul A Thiessen, Evan E Bolton, Jie Chen, Gang Fu, Asta Gindulyte, Lianyi Han, Jane He, Siqian He, Benjamin A Shoemaker, and 1 others. 201644</p>
<p>Selfreferencing embedded strings (selfies): A 100% robust molecular string representation. Mario Krenn, Florian Hse, Akshatkumar Nigam, 10.1088/2632-2153/aba947/pdfMachine Learning: Science and Technology. 14450242020Pascal Friederich, and Alan Aspuru-Guzik</p>
<p>Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling. Greg Landrum. 83152812013Greg Landrum and 1 others</p>
<p>Utilizing large language models in an iterative paradigm with domain feedback for molecule optimization. Khiem Le, Nitesh V Chawla, arXiv:2410.131472024arXiv preprint</p>
<p>Molx: Enhancing large language models for molecular learning with a multi-modal extension. Khiem Le, Zhichun Guo, Kaiwen Dong, Xiaobao Huang, Bozhao Nan, Roshni Iyer, Xiangliang Zhang, Olaf Wiest, Wei Wang, Nitesh V Chawla ; Chanhui Lee, Yuheon Song, Yongjun Jeong, Hanbum Ko, Rodrigo Hormazabal, Sehui Han, Kyunghoon Bae, Sungbin Lim, Sungwoong Kim, arXiv:2406.06777arXiv:2502.02810Mol-llm: Generalist molecular llm with improved graph utilization. 2024. 2025arXiv preprint</p>
<p>Binary codes capable of correcting deletions, insertions, and reversals. Vladimir I Levenshtein, Soviet physics doklady. 1081966</p>
<p>Jiatong Li, Junxian Li, Yunqing Liu, Dongzhan Zhou, Qing Li, arXiv:2412.14642Tomg-bench: Evaluating llms on text-based open molecule generation. 2024aarXiv preprint</p>
<p>Large language models are in-context molecule learners. Jiatong Li, Wei Liu, Zhihao Ding, Wenqi Fan, Yuqiang Li, Qing Li, arXiv:2403.041972024barXiv preprint</p>
<p>Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective. Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, Qing Li, 2024c</p>
<p>Jiatong Li, Yunqing Liu, Wei Liu, Jingdi Le, Di Zhang, Wenqi Fan, Dongzhan Zhou, Yuqiang Li, Qing Li, arXiv:2411.14721Molreflect: Towards in-context fine-grained alignments between molecules and texts. 2024darXiv preprint</p>
<p>Versatile molecular editing via multimodal and group-optimized generative learning. Chang Liao, Yemin Yu, Yu Mei, Ying Wei ; Xiaohan Lin, Yijie Xia, Yupeng Huang, Shuo Liu, Jun Zhang, Yi Qin Gao, arXiv:2402.014392024. 2024arXiv preprintFrom words to molecules: A survey of large language models in chemistry</p>
<p>Kunlun Zhu, and 1 others. 2025a. Advances and challenges in foundation agents: From braininspired intelligence to evolutionary, collaborative, and safe systems. Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S Yu, Xinfeng Liu, Jiayi Li, Jinlin Zhang, Tanjin Wang, Sirui He, Hongzhang Hong, Shaokun Liu, Kaitao Zhang, Song, Michael Liu, Wojciech Sun, Meng Matusik, Jie Jiang, Chen, arXiv:2412.18084arXiv:2402.04119Pengfei Liu, Jun Tao, and Zhixiang Ren. 2024b. Scientific language modeling: A quantitative review of large language models in molecular science. Pengfei Liu2025. 2024a. Jun Tao, and Zhixiang Ren. 2025barXiv preprintA quantitative analysis of knowledge-learning preferences in large language models in molecular science</p>
<p>Multi-modal molecule structure-text model for text-based retrieval and editing. Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, Animashree Anandkumar, Nature Machine Intelligence. 5122023</p>
<p>Conversational drug editing using retrieval and domain feedback. Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, Chaowei Xiao, The twelfth international conference on learning representations. 2024c</p>
<p>Drugllm: Open large language model for few-shot molecule generation. Xianggen Liu, Yan Guo, Haoran Li, Jin Liu, Shudong Huang, Bowen Ke, Jiancheng Lv, ; Liu, Songhao Jiang, Siyu Chen, Zhuoran Yang, Yuxin Chen, Ian Foster, Rick Stevens, arXiv:2405.06690arXiv:2502.07237Drugimprovergpt: A large language model for drug optimization with fine-tuning via structured policy optimization. 2024d. 2025carXiv preprint</p>
<p>Xuefeng Liu, Songhao Jiang, Bo Li, Rick Stevens, arXiv:2502.10631Controllablegpt: A ground-up designed controllable gpt for molecule optimization. 2025darXiv preprint</p>
<p>Xuefeng Liu, Songhao Jiang, Rick Stevens, arXiv:2502.06891Scaffoldgpt: A scaffold-based large language model for drug improvement. 2025earXiv preprint</p>
<p>Graphgpt: A graph enhanced generative pretrained transformer for conditioned molecular generation. Hao Lu, Zhiqiang Wei, Xuze Wang, Kun Zhang, Hao Liu, International Journal of Molecular Sciences. 2423167612023</p>
<p>Generative design of functional metal complexes utilizing the internal knowledge of large language models. Jieyu Lu, Zhangde Song, Qiyuan Zhao, Yuanqi Du, Yirui Cao, Haojun Jia, Chenru Duan, 10.48550/arXiv.2410.18136arXiv:2410.181362024arXiv preprint</p>
<p>Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, arXiv:2504.09858Sewon Min, and Matei Zaharia. 2025. Reasoning models can be effective without thinking. arXiv preprint</p>
<p>HL Morgan. 1965. The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. Mirza, Alampara, Kunchapu, Emoekabu, Krishnan, Wilhelmi, Okereke, Eberhardt, M Elahi, Greiner, 10.1021/c160017a018arXiv:2404.01475Are large language models superhuman chemists?. 20245arXiv preprintothers</p>
<p>Molecular optimization using a conditional transformer for reaction-aware compound exploration with reinforcement learning. Shogo Nakamura, Nobuaki Yasuo, Masakazu Sekijima, Communications Chemistry. 81402025</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Mol-air: Molecular reinforcement learning with adaptive intrinsic rewards for goaldirected molecular generation. Vilfredo Pareto, ; Jinyeong Park, Jaegyoon Ahn, Jonghwan Choi, Jibum Kim, 10.1021/acs.jcim.4c01669Journal of Chemical Information and Modeling. 1351919. 2025Manuale di economia politica con una introduzione alla scienza sociale</p>
<p>Machine learning prediction of nine molecular properties based on the smiles representation of the qm9 quantum-chemistry dataset. Johnatan Gabriel A Pinheiro, Mucelini, D Marinalva, Ronaldo C Soares, Juarez Lf Da Prati, Marcos G Silva, Quiles, 10.1021/acs.jpca.0c05969The Journal of Physical Chemistry A. 124472020</p>
<p>Graphxform: graph transformer for computer-aided molecular design. Jonathan Pirnay, Jan G Rittig, Alexander B Wolf, Martin Grohe, Jakob Burger, Alexander Mitsos, Dominik G Grimm, Digital Discovery. 442025</p>
<p>Molecular sets (moses): a benchmarking platform for molecular generation models. Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, 10.3389/fphar.2020.565644/pdfFrontiers in pharmacology. 115656442020Mark Veselov, and 1 others</p>
<p>Frchet chemnet distance: A metric for generative models for molecules in drug discovery. Kristina Preuer, Philipp Renz, 10.1021/acs.jcim.8b00234Thomas Unterthiner, and 1 others. 201858</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, arXiv:1606.05250Squad: 100,000+ questions for machine comprehension of text. 2016arXiv preprint</p>
<p>A review of large language models and autonomous agents in chemistry. Caldas Mayk, Christopher J Ramos, Andrew D Collison, Wang White ; Yue, Richard Allmendinger, 10.48550/arXiv.2502.12845arXiv:2502.12845MOLLM: multi-objective large language model for molecular design -optimizing with experts. 2025. 2025arXiv preprint</p>
<p>Large-scale chemical language representations capture molecular structure and properties. Jerret Ross, Brian Belgodere, Inkit Vijil Chenthamarakshan, Youssef Padhi, Payel Mroueh, Das, Nature Machine Intelligence. 4122022</p>
<p>Learning to optimize molecules with a chemical language model. Jerret Ross, Samuel Hoffman, Brian Belgodere, Youssef Vijil Chenthamarakshan, Payel Mroueh, Das, Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Crossing new frontiers: Knowledgeaugmented large language model prompting for zeroshot text-based de novo molecule design. Sakhinana Sagar, Srinivas , Venkataramana Runkana ; Kunyang Sun, Dorian Bagni, Joseph M Cavanagh, Yingze Wang, Jacob M Sawyer, Andrew Gritsevskiy, Oufan Zhang, Teresa Head-Gordon, arXiv:2408.11866arXiv:2503.12602Synllama: Generating synthesizable molecules and their analogs with large language models. 2024. 2025arXiv preprint</p>
<p>Reinforcement learning: An introduction. Richard S Sutton, 1998MIT press Cambridge1Andrew G Barto, and 1 others</p>
<p>A survey of generative ai for de novo drug design: new frontiers in molecule and protein generation. Xiangru Tang, Howard Dai, Elizabeth Knight, Fang Wu, Yunyang Li, Tianxiao Li, Mark Gerstein, Briefings in Bioinformatics. 254e3382024</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothe Lachaux, Baptiste Lacroix, Naman Rozire, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, and 1 others. 2023. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>cmolgpt: A conditional generative pre-trained transformer for target-specific de novo molecular generation. Haorui Wang, Marta Skreta, Cher Tian Ser, Wenhao Gao, Lingkai Kong, Felix Strieth-Kalthoff, Chenru Duan, Yuchen Zhuang, Yue Yu, Yanqiao Zhu, Yuanqi Du, Alan Aspuru-Guzik, Kirill Neklyudov, Chao Zhang, ; Y Wang, H Zhao, S Sciabola, W Wang, The Thirteenth International Conference on Learning Representations. 2025. 2023284430Efficient evolutionary search over chemical space with large language models</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, and 1 others. 2022a. Emergent abilities of large language models. TMLR</p>
<p>others. 2022b. Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, NeurIPS. 35</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, Journal of chemical information and computer sciences. 2811988</p>
<p>Moleculenet: a benchmark for molecular machine learning. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, S Aneesh, Karl Pappu, Vijay Leswing, Pande, Chemical science. 922018</p>
<p>Leveraging language model for advanced multiproperty molecular optimization via prompt engineering. Zhenxing Wu, Odin Zhang, Xiaorui Wang, Li Fu, Huifeng Zhao, Jike Wang, Hongyan Du, Dejun Jiang, Yafeng Deng, Nature Machine Intelligence. 2024Dongsheng Cao, and 1 others</p>
<p>Naturelm: Deciphering the language of nature for scientific discovery. Yingce Xia, Peiran Jin, Shufang Xie, Liang He, Chuan Cao, Renqian Luo, Guoqing Liu, Yue Wang, Zequn Liu, Yuan-Jyue Chen, arXiv:2502.075271 others. 2025arXiv preprint</p>
<ol>
<li>m-kailin: Knowledge-driven agentic scientific corpus distillation framework for biomedical large language models training. Meng Xiao, Xunxin Cai, Chengrui Wang, Yuanchun Zhou, arXiv:2504.19565arXiv preprint</li>
</ol>
<p>Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, Young Jin Kim, 10.1145/3649506arXiv:2401.08417arXiv:2304.13712Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. Harnessing the power of llms in practice: A survey on chatgpt and beyond. 2024arXiv preprint</p>
<p>Molecule generation for drug design: a graph learning perspective. Nianzu Yang, Huaijin Wu, Kaipeng Zeng, Yang Li, Siyuan Bao, Junchi Yan, 2024Fundamental Research</p>
<p>Exploring the potential of large language models in graph generation. Yang Yao, Xin Wang, Zeyang Zhang, Yijian Qin, Ziwei Zhang, Xu Chu, Yuekui Yang, Wenwu Zhu, Hong Mei, arXiv:2403.143582024arXiv preprint</p>
<p>Drugassist: A large language model for molecule optimization. Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, Xiangxiang Zeng, Briefings in Bioinformatics. 261e6932025</p>
<p>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. Botao Yu, Frazier N Baker, Ziqi Chen, Xia Ning, Huan Sun, arXiv:2402.093912024aarXiv preprint</p>
<p>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. Botao Yu, Frazier N Baker, Ziqi Chen, Xia Ning, Huan Sun, 10.48550/arXiv.2402.09391arXiv:2402.093912024barXiv preprint</p>
<p>Collaborative expert llms guided multi-objective molecular optimization. Jiajun Yu, Yizhen Zheng, Yee Huan, Shirui Koh, Tianyue Pan, Haishuai Wang, Wang, arXiv:2503.03503arXiv:2501.13824Shuzhou Yuan and Michael Frber. 2025. Hallucinations can improve large language models in drug discovery. 2025arXiv preprint</p>
<p>Deep generative molecular design reshapes drug discovery. Xiangxiang Zeng, Fei Wang, Yuan Luo, Seung-Gu Kang, Jian Tang, Felice C Lightstone, F Evandro, Wendy Fang, Ruth Cornell, Feixiong Nussinov, Cheng, Cell Reports Medicine. 1232022</p>
<p>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, arXiv:2402.06852Wanli Ouyang, and 1 others. 2024a. Chemllm: A chemical large language model. arXiv preprint</p>
<p>Juzheng Zhang, Yatao Bian, Yongqiang Chen, Quanming Yao, arXiv:2408.00863Unimot: Unified moleculetext language model with discrete token representation. 2024barXiv preprint</p>
<p>Deep lead optimization: Leveraging generative ai for structural modification. Odin Zhang, Haitao Lin, Hui Zhang, Huifeng Zhao, Yufei Huang, Chang-Yu Hsieh, Peichen Pan, Tingjun Hou, Journal of the American Chemical Society. 146462024c</p>
<p>Scientific large language models: A survey on biological &amp; chemical domains. Qiang Zhang, Keyan Ding, Tianwen Lv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, and 1 others. 202557</p>
<p>A comprehensive survey of scientific large language models and their applications in scientific discovery. Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, Jiawei Han, arXiv:2406.108332024darXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zhang, arXiv:2303.18223Zican Dong, and 1 others. 2023. A survey of large language models. arXiv preprint</p>
<p>Large language models in drug discovery and development: From disease mechanisms to clinical trials. Yizhen Zheng, Yee Huan, Maddie Koh, Li Yang, Lauren T Li, Geoffrey I May, Shirui Webb, George Pan, Church, arXiv:2409.044812024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7099 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7099</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7099</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-2f4d89c90e247aceb34dc58adcab3d3b51f7625e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2f4d89c90e247aceb34dc58adcab3d3b51f7625e" target="_blank">SELFIES: a robust representation of semantically constrained graphs with an example application in chemistry</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work presents a general-purpose, sequence-based, robust representation of semantically constrained graphs, which is called SELFIES (SELF-referencIng Embedded Strings), based on a Chomsky type-2 grammar, augmented with two self-referencing functions.</p>
                <p><strong>Paper Abstract:</strong> Graphs are ideal representations of complex, relational information. Their applications span diverse areas of science and engineering, such as Feynman diagrams in fundamental physics, the structures of molecules in chemistry or transport systems in urban planning. Recently, many of these examples turned into the spotlight as applications of machine learning (ML). There, common challenges to the successful deployment of ML are domain-specific constraints, which lead to semantically constrained graphs. While much progress has been achieved in the generation of valid graphs for domain- and model-specific applications, a general approach has not been demonstrated yet. Here, we present a general-purpose, sequence-based, robust representation of semantically constrained graphs, which we call SELFIES (SELF-referencIng Embedded Strings). SELFIES are based on a Chomsky type-2 grammar, augmented with two self-referencing functions. We demonstrate their applicability to represent chemical compound structures and compare them to perhaps the most popular 2D representation, SMILES, and other important baselines. We find stronger robustness against character mutations while still maintaining similar chemical properties. Even entirely random SELFIES produce semantically valid graphs in most of the cases. As feature representation in variational autoencoders, SELFIES provide a substantial improvement in the task of in reconstruction, validity, and diversity. We anticipate that SELFIES allow for direct applications in ML, without the need for domain-specific adaptation of model architectures. SELFIES are not limited to the structures of small molecules, and we show how to apply them to two other examples from the sciences: representations of DNA and interaction graphs for quantum mechanical experiments.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7099.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7099.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELFIES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-referencIng Embedded Strings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 100% robust, context-free grammar based string representation for molecular graphs that guarantees every string decodes to a chemically valid molecule by enforcing local valence and syntactic constraints (branches and rings) during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SELFIES (representation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>context-free grammar / string representation (SELF-referencing functions for branches and rings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Grammar/rules can be derived algorithmically from chemical datasets (authors used QM9 to design rule vectors and demonstrated encoding/decoding of PubChem molecules up to 500 SMILES chars); rule vectors encode vertex types and maximal degrees from data.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Used as token-level representation for generative models (one-hot encoded strings) so that arbitrary generative models (VAE, GAN, genetic algorithms) produce only valid molecules; generation follows SELFIES derivation rules with branch and ring self-referencing functions enforcing local constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SELFIES token strings (one-hot encodings used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De-novo molecular generation / inverse molecular design for small organic molecules (generalizable to broader molecule classes and other graph-structured design tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Valence-bond and local degree constraints encoded in rule vectors; branch function B(N,X) and ring function R(N) enforce syntactic/semantic constraints during decoding so rings/branches added only when allowed by current state.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used to check validity of decoded molecules; code and conversions available in the authors' GitHub repository.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>QM9 used to derive rules and as a benchmark for VAE experiments; authors also encoded/decoded 72M PubChem molecules (with <500 SMILES chars) to demonstrate coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity percentage (decoded strings corresponding to chemically valid molecules), robustness under random mutations, diversity/uniqueness (number of unique valid molecules generated), density in latent space (number of distinct molecules encodable per sampling procedure), counts of unique valid molecules in fixed sample sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>SELFIES yields 100% validity for random strings and for latent-space-decoded samples in VAE experiments; in single/double/triple random-symbol mutation tests starting from an example, SELFIES validity = 100% vs SMILES = 9.9%, 3.0%, 1.1% (for 1/2/3 mutations); DeepSMILES (comparison) had 35.1%, 18.4%, 9.8% respectively; VAE: entire sampled latent-space planes decoded to valid molecules with SELFIES (100% validity) while SMILES only had a small valid fraction; SELFIES VAE latent-space was >100x denser in valid diverse molecules than SMILES; GAN sampling (10,000 draws) with best hyperparameters: SELFIES produced 7,889 unique valid molecules (78.9%), SMILES produced 1,855 unique valid molecules (18.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>SELFIES requires further standardization and extension to cover full periodic table, stereochemistry, polyvalency and other special cases; current paper focuses on small organic molecules (QM9); grammar extensions are needed for broader classes though authors provide mechanisms to do so; no wet-lab synthesis/assay validation reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SELFIES: a robust representation of semantically constrained graphs with an example application in chemistry', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7099.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7099.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Autoencoder (applied to molecular generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder–decoder probabilistic latent-variable neural network used to map discrete molecular strings (SMILES or SELFIES) to a continuous latent space for sampling and optimization, then decode back to discrete molecular representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Auto-encoding variational bayes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Variational Autoencoder (VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder–decoder latent-variable model (probabilistic VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on the QM9 benchmark dataset of small organic molecules; input strings encoded as one-hot vectors of SMILES or SELFIES tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Encode molecules to a continuous latent space via encoder; sample points (including systematic sampling across randomly oriented 2D planes in latent space) and decode via decoder to obtain discrete strings (SMILES or SELFIES); latent-space scanning used for property optimization in general VAE workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES and SELFIES (one-hot token encodings used as VAE input/output)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Inverse molecular design / de-novo generation of small organic molecules (optimization and exploration of chemical space).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>When using SELFIES, valence and syntactic constraints are enforced by representation; no extra property filters or synthetic-accessibility constraints are reported in the VAE experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used to evaluate validity of decoded strings/molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>QM9 (explicitly reported for VAE training/evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Fraction/percentage of valid molecules decoded across latent-space samples (validity %), latent-space density of distinct valid molecules (diversity measure), number of unique molecules discovered under sampling stopping criterion (stop after 20 samples produced no new molecule).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Using SELFIES with the VAE produced 100% validity across sampled latent-space planes (entire sampled planes yielded valid molecules); SMILES VAE decoded only a small fraction of latent points to valid molecules. SELFIES VAE latent-space encoded >100x (two orders of magnitude) more valid diverse molecules than the SMILES VAE under the authors' sampling/diversity procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>With SMILES, high rates of syntactically/semantically invalid decodes make large regions of latent space unusable; VAE performance depends strongly on representation. Paper does not report VAE model sizes, training hyperparameters, or downstream property-optimization results; extensions (stereochemistry, larger molecules) require SELFIES grammar expansion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SELFIES: a robust representation of semantically constrained graphs with an example application in chemistry', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7099.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7099.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Adversarial Network (applied to molecular generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adversarial generator–discriminator neural network pair trained to reproduce the distribution of molecules from a dataset; generator outputs molecular strings (SMILES or SELFIES) and discriminator tries to distinguish generated vs dataset examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative adversarial nets</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generative Adversarial Network (GAN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>adversarial generator–discriminator network</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>The paper reports training GANs for molecular sequence generation using SMILES or SELFIES tokenizations; the dataset used for GAN training is not explicitly specified in the GAN paragraph (authors report training across representations and hyperparameters).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Train generator to produce token sequences (SMILES or SELFIES); after training sample generator (10,000 draws reported) to measure diversity and validity of outputs. Authors swept 200 hyperparameter settings and report best-performing models.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES and SELFIES token strings (one-hot encodings assumed for input/output).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De-novo molecular generation (produce diverse valid molecules matching dataset distribution).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Representation-level constraints when using SELFIES (guaranteed validity); no additional property or synthetic-accessibility constraints are described for GAN outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used to validate whether generated strings correspond to valid molecules; hyperparameter sweeps performed to find best models.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Not explicitly specified for the GAN experiments in the text (authors do not state the training dataset in the GAN paragraph).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Number of unique valid molecules generated in a fixed sample size (10,000 samples), reported as counts and percentages (diversity % = unique valid / total samples).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>From 10,000 samples and best hyperparameters: SELFIES GAN produced 7,889 unique valid molecules (78.9%), while SMILES GAN produced 1,855 unique valid molecules (18.6%). Authors report sweeping 200 hyperparameter settings.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>GAN performance sensitive to hyperparameters; representation matters strongly (SMILES leads to many invalid outputs and lower diversity); paper does not report dataset used for GAN training or full model/hyperparameter details for reproducibility in the GAN section.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SELFIES: a robust representation of semantically constrained graphs with an example application in chemistry', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7099.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7099.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSMILES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSMILES (SMILES adaptation for ML)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptation of SMILES intended to be more suitable for machine learning by modifying how branches and rings are encoded; presented in the literature and compared in the paper as an alternative representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DeepSMILES: An Adaptation of SMILES for Use in machine-learing chemical structures</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSMILES (representation variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>SMILES variant token representation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not trained — a representation/encoding scheme derived from SMILES; authors compared mutation robustness metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Representation-level modification; used as direct input to arbitrary ML models in principle.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>DeepSMILES token strings</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Machine-learning-based molecular generation / prediction (representation suited to ML models).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Representation mitigates some syntactic invalidity relative to canonical SMILES but does not guarantee 100% validity like SELFIES.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Not detailed; used in mutation-validity comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Not specified in the mutation experiments; comparison reported for robustness under random mutations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity under random mutations (1,2,3 symbol mutations).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Under single/double/triple random-symbol mutations starting from an example molecule, DeepSMILES validity was reported as 35.1%, 18.4%, and 9.8%, respectively (authors' mutation test).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Improves validity vs canonical SMILES but still results in substantial invalid outputs under mutations and does not guarantee universal validity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SELFIES: a robust representation of semantically constrained graphs with an example application in chemistry', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automatic chemical design using a data-driven continuous representation of molecules <em>(Rating: 2)</em></li>
                <li>Objectivereinforced generative adversarial networks (ORGAN) for sequence generation models <em>(Rating: 2)</em></li>
                <li>DeepSMILES: An Adaptation of SMILES for Use in machine-learing chemical structures <em>(Rating: 2)</em></li>
                <li>Augmenting Genetic Algorithms with Deep Neural Networks for Exploring the Chemical Space <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7099",
    "paper_id": "paper-2f4d89c90e247aceb34dc58adcab3d3b51f7625e",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "SELFIES",
            "name_full": "SELF-referencIng Embedded Strings",
            "brief_description": "A 100% robust, context-free grammar based string representation for molecular graphs that guarantees every string decodes to a chemically valid molecule by enforcing local valence and syntactic constraints (branches and rings) during generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SELFIES (representation)",
            "model_type": "context-free grammar / string representation (SELF-referencing functions for branches and rings)",
            "model_size": null,
            "training_data_description": "Grammar/rules can be derived algorithmically from chemical datasets (authors used QM9 to design rule vectors and demonstrated encoding/decoding of PubChem molecules up to 500 SMILES chars); rule vectors encode vertex types and maximal degrees from data.",
            "generation_method": "Used as token-level representation for generative models (one-hot encoded strings) so that arbitrary generative models (VAE, GAN, genetic algorithms) produce only valid molecules; generation follows SELFIES derivation rules with branch and ring self-referencing functions enforcing local constraints.",
            "chemical_representation": "SELFIES token strings (one-hot encodings used in experiments)",
            "target_application": "De-novo molecular generation / inverse molecular design for small organic molecules (generalizable to broader molecule classes and other graph-structured design tasks).",
            "constraints_used": "Valence-bond and local degree constraints encoded in rule vectors; branch function B(N,X) and ring function R(N) enforce syntactic/semantic constraints during decoding so rings/branches added only when allowed by current state.",
            "integration_with_external_tools": "RDKit used to check validity of decoded molecules; code and conversions available in the authors' GitHub repository.",
            "dataset_used": "QM9 used to derive rules and as a benchmark for VAE experiments; authors also encoded/decoded 72M PubChem molecules (with &lt;500 SMILES chars) to demonstrate coverage.",
            "evaluation_metrics": "Validity percentage (decoded strings corresponding to chemically valid molecules), robustness under random mutations, diversity/uniqueness (number of unique valid molecules generated), density in latent space (number of distinct molecules encodable per sampling procedure), counts of unique valid molecules in fixed sample sizes.",
            "reported_results": "SELFIES yields 100% validity for random strings and for latent-space-decoded samples in VAE experiments; in single/double/triple random-symbol mutation tests starting from an example, SELFIES validity = 100% vs SMILES = 9.9%, 3.0%, 1.1% (for 1/2/3 mutations); DeepSMILES (comparison) had 35.1%, 18.4%, 9.8% respectively; VAE: entire sampled latent-space planes decoded to valid molecules with SELFIES (100% validity) while SMILES only had a small valid fraction; SELFIES VAE latent-space was &gt;100x denser in valid diverse molecules than SMILES; GAN sampling (10,000 draws) with best hyperparameters: SELFIES produced 7,889 unique valid molecules (78.9%), SMILES produced 1,855 unique valid molecules (18.6%).",
            "experimental_validation": false,
            "challenges_or_limitations": "SELFIES requires further standardization and extension to cover full periodic table, stereochemistry, polyvalency and other special cases; current paper focuses on small organic molecules (QM9); grammar extensions are needed for broader classes though authors provide mechanisms to do so; no wet-lab synthesis/assay validation reported here.",
            "uuid": "e7099.0",
            "source_info": {
                "paper_title": "SELFIES: a robust representation of semantically constrained graphs with an example application in chemistry",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "VAE",
            "name_full": "Variational Autoencoder (applied to molecular generation)",
            "brief_description": "An encoder–decoder probabilistic latent-variable neural network used to map discrete molecular strings (SMILES or SELFIES) to a continuous latent space for sampling and optimization, then decode back to discrete molecular representations.",
            "citation_title": "Auto-encoding variational bayes",
            "mention_or_use": "use",
            "model_name": "Variational Autoencoder (VAE)",
            "model_type": "encoder–decoder latent-variable model (probabilistic VAE)",
            "model_size": null,
            "training_data_description": "Trained on the QM9 benchmark dataset of small organic molecules; input strings encoded as one-hot vectors of SMILES or SELFIES tokens.",
            "generation_method": "Encode molecules to a continuous latent space via encoder; sample points (including systematic sampling across randomly oriented 2D planes in latent space) and decode via decoder to obtain discrete strings (SMILES or SELFIES); latent-space scanning used for property optimization in general VAE workflows.",
            "chemical_representation": "SMILES and SELFIES (one-hot token encodings used as VAE input/output)",
            "target_application": "Inverse molecular design / de-novo generation of small organic molecules (optimization and exploration of chemical space).",
            "constraints_used": "When using SELFIES, valence and syntactic constraints are enforced by representation; no extra property filters or synthetic-accessibility constraints are reported in the VAE experiments.",
            "integration_with_external_tools": "RDKit used to evaluate validity of decoded strings/molecules.",
            "dataset_used": "QM9 (explicitly reported for VAE training/evaluation).",
            "evaluation_metrics": "Fraction/percentage of valid molecules decoded across latent-space samples (validity %), latent-space density of distinct valid molecules (diversity measure), number of unique molecules discovered under sampling stopping criterion (stop after 20 samples produced no new molecule).",
            "reported_results": "Using SELFIES with the VAE produced 100% validity across sampled latent-space planes (entire sampled planes yielded valid molecules); SMILES VAE decoded only a small fraction of latent points to valid molecules. SELFIES VAE latent-space encoded &gt;100x (two orders of magnitude) more valid diverse molecules than the SMILES VAE under the authors' sampling/diversity procedure.",
            "experimental_validation": false,
            "challenges_or_limitations": "With SMILES, high rates of syntactically/semantically invalid decodes make large regions of latent space unusable; VAE performance depends strongly on representation. Paper does not report VAE model sizes, training hyperparameters, or downstream property-optimization results; extensions (stereochemistry, larger molecules) require SELFIES grammar expansion.",
            "uuid": "e7099.1",
            "source_info": {
                "paper_title": "SELFIES: a robust representation of semantically constrained graphs with an example application in chemistry",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "GAN",
            "name_full": "Generative Adversarial Network (applied to molecular generation)",
            "brief_description": "An adversarial generator–discriminator neural network pair trained to reproduce the distribution of molecules from a dataset; generator outputs molecular strings (SMILES or SELFIES) and discriminator tries to distinguish generated vs dataset examples.",
            "citation_title": "Generative adversarial nets",
            "mention_or_use": "use",
            "model_name": "Generative Adversarial Network (GAN)",
            "model_type": "adversarial generator–discriminator network",
            "model_size": null,
            "training_data_description": "The paper reports training GANs for molecular sequence generation using SMILES or SELFIES tokenizations; the dataset used for GAN training is not explicitly specified in the GAN paragraph (authors report training across representations and hyperparameters).",
            "generation_method": "Train generator to produce token sequences (SMILES or SELFIES); after training sample generator (10,000 draws reported) to measure diversity and validity of outputs. Authors swept 200 hyperparameter settings and report best-performing models.",
            "chemical_representation": "SMILES and SELFIES token strings (one-hot encodings assumed for input/output).",
            "target_application": "De-novo molecular generation (produce diverse valid molecules matching dataset distribution).",
            "constraints_used": "Representation-level constraints when using SELFIES (guaranteed validity); no additional property or synthetic-accessibility constraints are described for GAN outputs.",
            "integration_with_external_tools": "RDKit used to validate whether generated strings correspond to valid molecules; hyperparameter sweeps performed to find best models.",
            "dataset_used": "Not explicitly specified for the GAN experiments in the text (authors do not state the training dataset in the GAN paragraph).",
            "evaluation_metrics": "Number of unique valid molecules generated in a fixed sample size (10,000 samples), reported as counts and percentages (diversity % = unique valid / total samples).",
            "reported_results": "From 10,000 samples and best hyperparameters: SELFIES GAN produced 7,889 unique valid molecules (78.9%), while SMILES GAN produced 1,855 unique valid molecules (18.6%). Authors report sweeping 200 hyperparameter settings.",
            "experimental_validation": false,
            "challenges_or_limitations": "GAN performance sensitive to hyperparameters; representation matters strongly (SMILES leads to many invalid outputs and lower diversity); paper does not report dataset used for GAN training or full model/hyperparameter details for reproducibility in the GAN section.",
            "uuid": "e7099.2",
            "source_info": {
                "paper_title": "SELFIES: a robust representation of semantically constrained graphs with an example application in chemistry",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "DeepSMILES",
            "name_full": "DeepSMILES (SMILES adaptation for ML)",
            "brief_description": "An adaptation of SMILES intended to be more suitable for machine learning by modifying how branches and rings are encoded; presented in the literature and compared in the paper as an alternative representation.",
            "citation_title": "DeepSMILES: An Adaptation of SMILES for Use in machine-learing chemical structures",
            "mention_or_use": "mention",
            "model_name": "DeepSMILES (representation variant)",
            "model_type": "SMILES variant token representation",
            "model_size": null,
            "training_data_description": "Not trained — a representation/encoding scheme derived from SMILES; authors compared mutation robustness metrics.",
            "generation_method": "Representation-level modification; used as direct input to arbitrary ML models in principle.",
            "chemical_representation": "DeepSMILES token strings",
            "target_application": "Machine-learning-based molecular generation / prediction (representation suited to ML models).",
            "constraints_used": "Representation mitigates some syntactic invalidity relative to canonical SMILES but does not guarantee 100% validity like SELFIES.",
            "integration_with_external_tools": "Not detailed; used in mutation-validity comparisons.",
            "dataset_used": "Not specified in the mutation experiments; comparison reported for robustness under random mutations.",
            "evaluation_metrics": "Validity under random mutations (1,2,3 symbol mutations).",
            "reported_results": "Under single/double/triple random-symbol mutations starting from an example molecule, DeepSMILES validity was reported as 35.1%, 18.4%, and 9.8%, respectively (authors' mutation test).",
            "experimental_validation": false,
            "challenges_or_limitations": "Improves validity vs canonical SMILES but still results in substantial invalid outputs under mutations and does not guarantee universal validity.",
            "uuid": "e7099.3",
            "source_info": {
                "paper_title": "SELFIES: a robust representation of semantically constrained graphs with an example application in chemistry",
                "publication_date_yy_mm": "2019-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "rating": 2
        },
        {
            "paper_title": "Objectivereinforced generative adversarial networks (ORGAN) for sequence generation models",
            "rating": 2
        },
        {
            "paper_title": "DeepSMILES: An Adaptation of SMILES for Use in machine-learing chemical structures",
            "rating": 2
        },
        {
            "paper_title": "Augmenting Genetic Algorithms with Deep Neural Networks for Exploring the Chemical Space",
            "rating": 2
        }
    ],
    "cost": 0.013607,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Referencing Embedded Strings (SELFIES): A $100 \%$ robust molecular string representation</h1>
<p>Mario Krenn, ${ }^{1,2,3, *}$ Florian Häse, ${ }^{1,2,3,4}$ AkshatKumar<br>Nigam, ${ }^{2}$ Pascal Friederich, ${ }^{2,5}$ and Alan Aspuru-Guzik ${ }^{1,2,3,6, \dagger}$<br>${ }^{1}$ Department of Chemistry, University of Toronto, Canada.<br>${ }^{2}$ Department of Computer Science, University of Toronto, Canada.<br>${ }^{3}$ Vector Institute for Artificial Intelligence, Toronto, Canada.<br>${ }^{4}$ Department of Chemistry and Chemical Biology, Harvard University, Cambridge, USA.<br>${ }^{5}$ Institute of Nanotechnology, Karlsruhe Institute of Technology, Germany.<br>${ }^{6}$ Canadian Institute for Advanced Research (CIFAR) Senior Fellow, Toronto, Canada. (Dated: March 6, 2020)</p>
<h4>Abstract</h4>
<p>The discovery of novel materials and functional molecules can help to solve some of society's most urgent challenges, ranging from efficient energy harvesting and storage to uncovering novel pharmaceutical drug candidates. Traditionally matter engineering - generally denoted as inverse design - was based massively on human intuition and high-throughput virtual screening. The last few years have seen the emergence of significant interest in computer-inspired designs based on evolutionary or deep learning methods. The major challenge here is that the standard strings molecular representation SMILES shows substantial weaknesses in that task because large fractions of strings do not correspond to valid molecules. Here, we solve this problem at a fundamental level and introduce SELFIES (SELF-referencIng Embedded Strings), a string-based representation of molecules which is $100 \%$ robust. Every SELFIES string corresponds to a valid molecule, and SELFIES can represent every molecule. SELFIES can be directly applied in arbitrary machine learning models without the adaptation of the models; each of the generated molecule candidates is valid. In our experiments, the model's internal memory stores two orders of magnitude more diverse molecules than a similar test with SMILES. Furthermore, as all molecules are valid, it allows for explanation and interpretation of the internal working of the generative models.</p>
<p>Introduction - The rise of computers enabled the creation of the field of computational chemistry and cheminformatics which deals with the development and application of methods to calculate, process, store and search molecular information on computing systems. Arising challenges of molecular representation and identification were addressed by SMILES (Simplified Molecular Input Line Entry System), which was invented by David Weiniger in 1988 [1]. SMILES is a simple string-based representation which is based on principles of molecular graph theory and allows molecular structure specification with straightforward rules. SMILES has since become a standard tool in computational chemistry and is still a de-facto standard for string-based representing molecular information in-silico.</p>
<p>Apart from predicting molecular properties with high accuracy, one of the main goals in computational chemistry is the design of novel, functional molecules. Exploring the entire chemical space - even for relatively small molecules - is intractable due to the combinatorial explosion of possible and stable chemical structures [4-6]. Substantial recent advances in artificial intelligence and machine learning (ML), in</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>particular, the development and control of generative models, have found their way into chemical research. There, scientists are currently adapting those novel methods for efficiently proposing new molecules with superior properties [7-12]. For identifying new molecules, input and output representations are in many cases SMILES strings. This, however, introduces a substantial problem: A significant fraction of the resulting SMILES strings do not correspond to valid molecules. They are either syntactically invalid, i.e. do not even correspond to a molecular graph, or they violate basic chemical rules, such as the maximum number of valence bonds between atoms. Researchers have proposed many special-case solutions for overcoming these problems, (such as adapting specific machine learning models [13, 14] or changing some definitions of SMILES [15]), however, a universal solution is lacking. Thus, more than 30 years after Weininger's invention of SMILES, the applications of generative models for the de-novo design of molecules requires a new way to describe molecules on the computer.</p>
<p>Here, we present SelFies (SELF-referencIng Embedded Strings), a string-based representation of molecular graphs that is $100 \%$ robust. By that, we mean that each SELFIES corresponds to a valid molecule, even entirely random strings. Furthermore, every molecule can be described as a SELFIES. SELFIES are independent of the machine learning model and can be used as a direct input without any</p>
<p>adaptations of the models.</p>
<p>We compare SELFIES with SMILES ML-based generative models such as in Variational Autoencoders (VAE) [16] and Generative Adversarial Networks (GANs) [17]. We find that the output is entirely valid and the models encode orders of magnitude more diverse molecules with SELFIES than with SMILES. Those results are not only significant for inversedesign of molecules, but also interpretability of the inner workings of neural networks in the chemical domain.</p>
<h3>String-based representations of Molecules</h3>
<p>We are describing the string-based representations of SMILES and SELFIES using the small biomolecule 3,4-Methylenedioxymethamphetamine (MDMA) in Fig. 1A. The SMILES string in Fig. 1B describes a sequence of connected atoms (green). Brackets identify branches and, and numbers identify ring-closures at the atoms that are connected. In SELFIES, Fig. 1C, the information of branch length as well as ring size is stored together with the corresponding identifiers Branch and Ring. For that, the symbol after the Branch and Ring stands for a number that is interpreted as lengths. Thereby, the possibility of invalid</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Description of a molecular graph with two computer-friendly, string-based methods. <strong>A)</strong> The molecular graph of a small organic molecule, 3,4-Methylenedioxymethamphetamine. <strong>B)</strong> Derivation of the molecular graph using SMILES. The main string (green) is augmented with branches (defined by an opening and a closing bracket) and rings (defined by unique numbers after the atoms that are connected). Note that both branches and rings are non-local operations. <strong>C)</strong> Derivation of the molecular graph using SELFIES. The main string is derived using a rule set such that the number of valence bonds per atom does not exceed physical limits. The symbol after a Branch is interpreted as the number of SELFIES symbols derived inside the branch. The symbol after Ring interpreted as a number too, indicating that the current atom is connected to the (N+1)st previous atom. Thereby every information in the string (except the ring closure) is local and allows for efficient derivation rules.</p>
<p>syntactical string (such as a string with more opening than closing brackets), is prevented. Furthermore, each SELFIES symbols is generated using derivation rules, see Table 2. Formally, the table corresponds to a formal grammar from theoretical computer science [18]. The derivation of a single symbol depends on the state of the derivation <strong>Xn</strong>. The purpose of these rules is to enforce the validity of the chemical valence-bonds.</p>
<p>As a simple example, the string [F] [=C] [=C] [#N] is derived in the following way. Starting in the state <strong>X0</strong>, the first symbol (rule vector) [F] leads to <strong>FX1</strong>. The derivation of the second symbol subsequently continues in the state <strong>X1</strong>. The total derivation is given by</p>
<p>$$
\mathbf{X}_0 \stackrel{[F]}{\longleftrightarrow} \mathbf{FX}_1 \xrightarrow{[=C]} \mathbf{FCX}_3
$$</p>
<p>$$
\stackrel{[=C]}{\longleftrightarrow} \mathbf{FC=CX}_2 \stackrel{[#N]}{\longleftrightarrow} \mathbf{FC=C=N}
$$</p>
<p>The final molecule FC=C=N, which satisfies all valence-bond rules, is 2-Fluoroethenimine. At this point, valence-bond constraints are satisfied for subsequent atoms and branches. The only remaining potential sources of violation of these constraints are the destination of rings. Therefore, we insert rings only if the number of valence-bond at the target has not yet reached the maximum. Thereby, using the rules in Table 2, 100% validity can be guaranteed for small biomolecules. It is straightforward to extend the coverage for broader classes of molecules, as we describe below.</p>
<p>The derivation rules in Table 2 are generated systematically and could be constructed fully automatically just from data, as we show in the SI. Furthermore, SELFIES are not restricted to molecular graphs but could be applied to other graph data types in the natural sciences that have additional domain-dependent constraints. We give an example, quantum optical experiments in physics with component-dependent connectivity [19], in the SI.</p>
<p>Informal conversations with several researchers lead to the argument that SMILES are "readable". Readability is in the eye of the beholder, but needless to say, SELFIES are as readable as figure 1C) attests to. After a little familiarity, functional groups and connectivity can be inferred by human interpretation for small molecular fragments.</p>
<h3>Effects of random Mutations</h3>
<p>The simplest way to compare robustness between SMILES and SELFIES is by starting from a valid string, such as MDMA in Fig. 1, and introduce random mutations of the symbols of the string. In Fig. 3A, we show three examples of one randomly introduced string mutation. We evaluate the resulting validity using RDKit [20]. All three SMILES strings are invalid. The first one is missing a second ring-identifier for 2, the second one is missing a closing bracket for a branch, and the last one violates valence-bond numbers of Fluorine. In</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Derivation rules of SELFIES for small organic molecules. Every symbol of SELFIES is interpreted as a rule vector (top red line). A SELFIES symbol will be replaced by the string at the intersection of the rule vector and derivation state of the derivation (left, green). The string can contain an atom or another state of derivation. The derivation starts in the state <strong>X</strong><sub>0</sub> (violet), and continues in the state previously derived. The state of derivation takes care of syntactical and chemical constraints, such as the maximal number of valence bonds. The rules in state <strong>X</strong><sub>n</sub> for <em>n</em> = 1-<em>n</em> = 4 are designed such the next atom can use up to <em>n</em> valence bonds. <em>B</em>(<em>N</em>, <strong>X</strong><sub><strong>n</strong></sub>) stands for function, creating a branch in the graph using the next <em>N</em> symbols and starting in state <strong>X</strong><sub><strong>n</strong></sub>. <em>B</em>(<em>N</em>) stands for a function that creates rings, from the current atom to the (<em>N</em> + 1)-st previously derived atom. In both cases, the letter subsequent to <em>R</em> or <em>B</em> is interpreted as a number <em>N</em>, which is defined in the last line of the table. This table covers all non-ionic molecules in the database QM9 [2, 3]. Ions, stereochemistry and larger molecules can also be represented by simply extending this table, as we show in the SI.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Random Mutations of SMILES and SELFIES of the molecule in Fig. 1A. A) Single mutations have led to three invalid SMILES strings, while all SELFIES produce valid molecules. In B) and C) the initial molecule is two and three times mutated, respectively. In all cases, SMILES strings are invalid, while SELFIES produce valid molecules that deviate more and more from the initial molecule.</p>
<p>contrast to that, all mutated SELFIES correspond to valid molecules. In Fig. 3B and C, we introduce two and three mutations, respectively. Again, all SMILES are invalid, and all SELFIES are valid molecules. In general, the validity probability for SMILES with one mutation starting from MDMA is 9.9%, 3.0% and 1.1% for one, two and three mutations, respectively. SELFIES, on the other hand, are valid in 100% of the cases. Three examples for each case can be seen on the right panel of Fig. 3. <sup>1</sup></p>
<p><strong>Results for deep generative Models</strong> – Generative models are an ideal application of a 100% robust representation of molecules. One prominent example is a variational autoencoder (VAE) [16], which has recently been employed for the design of novel molecules [21]. In the domain of chemistry, the VAE is used to transform a discrete molecular graph into a continuous representation which can be optimized using gradient-based or Bayesian methods. As shown in Fig. 4, it consists of two neural networks, the encoder and decoder. The encoder takes a string representation of the molecule (for instance, using one-hot encoding) and encodes it into a continuous, internal representation. There, every molecule corresponds to a location in a high-dimensional space. The number of neurons defines the dimension in the latent space. The decoder takes a position in the latent space and transforms it</p>
<p><sup>1</sup>We also investigate the validity rates of a recent adaption of SMILES denoted DeepSMILES [15]. DeepSMILES could also be used as a direct input for arbitrary machine learning models and follows, therefore, a similar objective as SELFIES. We find that single, double and triple mutations lead to 35.1%, 18.4% and 9.8% validity.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Variational Autoencoder (VAE) for Chemistry. The VAE is a deep neural network that takes a molecule as an input, encodes it to continuous latent space, and reconstructs it from there with a decoder. The latent space is a high-dimensional space where each point can be decoded into a discrete sequence. We represent the molecular graphs using one-hot encodings of SMILES and SELFIES.</p>
<p>into a discrete molecule (for instance again, a one-hot encoding of SMILES or SELFIES).</p>
<p>The goal of a VAE is learning to reconstruct molecules. After the training, one can scan through the latent space for optimizing chemical properties. Once an optimal point is identified, the decoder can map it to a molecular string. For any application of VAEs in chemistry, it is desirable that all points in the latent space correspond to valid molecules.</p>
<p>We experiment with a standard VAE, which we train to reconstruct molecules from the benchmark dataset QM9 [2, 3]. We employ both SMILES and SELFIES for that task. After the training, we analyze the validity of the latent space. We do this by sampling latent space points from randomly oriented planes in the high-dimensional space. Using SMILES, we find in Fig. 5A that only a small fraction of the space corresponds to valid molecules. A large fraction decodes to syntactically or semantically invalid strings that do not stand for molecules. In contrast to that, using SELFIES, we can see in Fig. 5B that the entire space corresponds to valid molecules. We want to stress that a 100% valid latent space is not only significant for inverse-design techniques in chemistry, but is essential for model interpretation [22–24], in particular for interpreting the internal representations [25, 26] in a scientific context [27].</p>
<p>Besides 100% validity, the molecule density in the latent space is of crucial importance too. The more valid, diverse molecules are encoded inside the latent space, the richer the chemical space that can be explored during optimization procedures. In Fig. 6A, we compare the richness of the encoded molecules when a VAE is trained with SMILES and with SELFIES. For that, we sample random points in the latent space and stop after 20 samples didn't produce any new molecule. We find that the latent space of the SELFIES VAE is more than two orders of magnitude denser than the one of SMILES.</p>
<p>Other prominent deep generative models are Generative Adversarial Networks (GANs) [17], which have been introduced in the design of molecules [28]. There, two networks – called generator and discriminator – are trained in tandem. The setting is such that discriminator receives either molecule from a dataset or outputs of the generator. The goal of the discriminator is to correctly identify the artificially generated structures, while the goal of the generator is to fool the discriminator. After the training, the generator has learned to reproduce the distribution of the dataset. We train the GAN, using 200 different hyperparameter settings both for SMILES and SELFIES. After the training, we sample each of the models 10,000 times and calculate the number of unique, valid molecules. For the best set of hyperparameters, we find that a GAN trained with SELFIES produces 78.9% diverse molecules while a GAN that produces SMILES strings only results in 18.6% diverse molecules, see Fig. 6B.</p>
<p><strong>Covering the chemical universe</strong> – In this manuscript, we demonstrate and apply SELFIES for small biomolecules. However, the language can be extended to cover much richer classes of molecules. In the corresponding GitHub repository, we extend the language to allow for molecules with up to 8000 atoms per ring and branch, we add stereochemistry.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Validity of latent space. We analyze the latent space of a VAE, which was trained to reproduce small organic molecules from the QM9 database. The latent space has 241 dimensions (LD stands for latent dimension). Upper row: We chose four randomly oriented planes in the high-dimensional space that go through the origin. Along the plane, we decode latent space points and calculate whether they correspond to valid or invalid molecules. The color code stands for the proportion of valid molecules (red=0%, green=100% valid). Lower row: We chose a random orientation of the plane, and displace it by a third random orientation by (-2,-1,+1,+2) standard deviations from the origin. In all experiments, we find that only a small fraction of the latent space for SMILES are valid, while for SELFIES the entire latent space is valid. This is not only important for generative tasks but is crucial for interpreting internals representations of the neural networks.</p>
<h1>Diversity of generated molecules in VAE</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Diversity of generative models trained with SMILES and SELFIES, with the example of VAE and GAN. Beside robustness, diversity is one of the main objectives for generative models. A) We investigate the density of valid diverse molecules by sampling the latent space of a VAE. We chose points with a distance of <em>σ</em> around the centre, stopping after 20 samples didn't produce new instances. We find that the VAE trained with SELFIES contains 100 times more valid diverse molecules than if it is trained with SMILES. B) We train a GAN with 200 different hyperparameters to produce de-novo molecules for SELFIES and SMILES. Sampling 10,000 times, SELFIES produced 7,889 different valid molecules, while for SMILES the most diverse valid number of molecules were 1,855. Both cases show that SELFIES leads to significantly larger densities of diverse molecules compared to SMILES.</p>
<p>Information, ions as well as unconstrained unspecified symbols. Thereby, we encoded and decoded all 72 million molecules from PubChem (the most complete collection of synthesized molecules) with less than 500 SMILES chars, demonstrating coverage of the space of chemical interest.</p>
<p><strong>Conclusion</strong> – We presented SELFIES, a human-readable and 100% robust method to describe molecular graphs in a computer. These properties lead to superior behaviour in inverse design tasks for functional molecules, based on deep generative models or genetic algorithms. SELFIES can be used as a direct input into current and even future generative models, without the requirement to adapt the model. In generative tasks, it leads to a significantly higher diversity of molecules, which is the main objective in inverse design. In addition to the results presented here, in separate work, we use Genetic Algorithms and find that without any hard-coded rules, SELFIES outperform literature results in a commonly-used benchmark [29]. Apart from superior behaviour in inverse design, a 100% valid representation is also a sufficient condition for interpreting the internal structures of the machine learning models [27]. While we have focused on an representation that is ideal for computers, attention should also be drawn to SELFIES standardization to allow general readability [30], by exploiting the numerous remaining degrees of freedom of SELFIES.</p>
<p><strong>Standardization outlook</strong> – The SELFIES concept still requires work to become a standard. Upon publication of this article, the authors will call for a workshop to extend the format to the entire periodic table, allow for stereochemistry, polyvalency and other special cases so that all the features present in SMILES are available in selfies. Unicode will be employed to create readable symbols that exploit the flexibility of modern text systems without restricting oneself to ASCII characters.</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>The authors thank Theophile Gaudin for useful discussions. A. A.-G. acknowledges generous support from the Canada 150 Research Chair Program, Tata Steel, Anders G. Froseth, and the Office of Naval Research. We acknowledge supercomputing support from SciNet. M.K. acknowledges support from the Austrian Science Fund (FWF) through the Erwin Schrödinger fellowship No. J4309. F.H. acknowledges support from the Herchel Smith Graduate Fellowship and the Jacques-Emile Dubois Student Dissertation Fellowship. P.F. has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 795206.</p>
<p>[1] D. Weininger, SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. <em>Journal of chemical information and computer sciences</em> <strong>28</strong>, 31–36 (1988).</p>
<p>[2] R. Ramakrishnan, P.O. Dral, M. Rupp and O.A. Von Lilienfeld, Quantum chemistry structures and</p>
<p>properties of 134 kilo molecules. Scientific data 1, 140022 (2014).
[3] L. Ruddigkeit, R. Van Deursen, L.C. Blum and J.L. Reymond, Enumeration of 166 billion organic small molecules in the chemical universe database GDB17. Journal of chemical information and modeling 52, 2864-2875 (2012).
[4] T.I. Oprea and J. Gottfries, Chemography: the art of navigating in chemical space. Journal of combinatorial chemistry 3, 157-166 (2001).
[5] A.M. Virshup, J. Contreras-García, P. Wipf, W. Yang and D.N. Beratan, Stochastic voyages into uncharted chemical space produce a representative library of all possible drug-like compounds. Journal of the American Chemical Society 135, 7296-7303 (2013).
[6] C. Qian, T. Siler and G.A. Ozin, Exploring the possibilities and limitations of a nanomaterials genome. small 11, 64-69 (2015).
[7] P. Raccuglia, K.C. Elbert, P.D. Adler, C. Falk, M.B. Wenny, A. Mollo, M. Zeller, S.A. Friedler, J. Schrier and A.J. Norquist, Machine-learning-assisted materials discovery using failed experiments. Nature 533, 73 (2016).
[8] B. Sánchez-Lengeling and A. Aspuru-Guzik, Inverse molecular design using machine learning: Generative models for matter engineering. Science 361, 360-365 (2018).
[9] P.B. Jørgensen, M.N. Schmidt and O. Winther, Deep generative models for molecular science. Molecular informatics 37, 1700133 (2018).
[10] D.C. Elton, Z. Boukouvalas, M.D. Fuge and P.W. Chung, Deep learning for molecular generation and optimization-a review of the state of the art. arXiv:1903.04388 (2019).
[11] P.S. Gromski, A.B. Henson, J.M. Granda and L. Cronin, How to explore chemical space using algorithms and automation. Nature Reviews Chemistry 1 (2019).
[12] J.H. Jensen, A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space. Chemical Science 10, $3567-3572$ (2019).
[13] T. Ma, J. Chen and C. Xiao, Constrained generation of semantically valid graphs via regularizing variational autoencoders. Advances in Neural Information Processing Systems 7113-7124 (2018).
[14] Q. Liu, M. Allamanis, M. Brockschmidt and A. Gaunt, Constrained graph variational autoencoders for molecule design. Advances in Neural Information Processing Systems 7795-7804 (2018).
[15] N. O'Boyle and A. Dalke, DeepSMILES: An Adaptation of SMILES for Use in machine-learing chemical structures. ChemRxiv (2018).
[16] D.P. Kingma and M. Welling, Auto-encoding variational bayes. arXiv:1312.6114 (2013).
[17] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville and Y. Bengio, Generative adversarial nets. Advances in neural information processing systems 2672-2680 (2014).
[18] J.E. Hopcroft, R. Motwani and J.D. Ullman, Introduction to Automata Theory, Languages, and Computation (3rd Edition). (2006).
[19] M. Krenn, M. Malik, R. Fickler, R. Lapkiewicz and A. Zeilinger, Automated search for new quantum exper-
iments. Physical review letters 116, 090405 (2016).
[20] G. Landrum and others, RDKit: Open-source cheminformatics. Journal of chemical information and modeling (2006).
[21] R. Gómez-Bombarelli, J.N. Wei, D. Duvenaud, J.M. Hernández-Lobato, B. Sánchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T.D. Hirzel, R.P. Adams and A. Aspuru-Guzik, Automatic chemical design using a data-driven continuous representation of molecules. ACS central science 4, 268-276 (2018).
[22] K.T. Schütt, F. Arbabzadah, S. Chmiela, K.R. Müller and A. Tkatchenko, Quantum-chemical insights from deep tensor neural networks. Nature communications 8, 13890 (2017).
[23] K. Preuer, G. Klambauer, F. Rippmann, S. Hochreiter and T. Unterthiner, Interpretable Deep Learning in Drug Discovery. arXiv:1903.02788 (2019).
[24] F. Häse, I.F. Galván, A. Aspuru-Guzik, R. Lindh and M. Vacher, How machine learning can assist the interpretation of ab initio molecular dynamics simulations and conceptual understanding of chemistry. Chemical Science 10, 2298-2307 (2019).
[25] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed and A. Lerchner, betaVAE: Learning Basic Visual Concepts with a Constrained Variational Framework.. ICLR 2, 6 (2017).
[26] T.Q. Chen, X. Li, R.B. Grosse and D.K. Duvenaud, Isolating sources of disentanglement in variational autoencoders. Advances in Neural Information Processing Systems 2610-2620 (2018).
[27] R. Iten, T. Metger, H. Wilming, L. Del Rio and R. Renner, Discovering physical concepts with neural networks. Physical Review Letters 124, 010508 (2020).
[28] G.L. Guimaraes, B. Sánchez-Lengeling, C. Outeiral, P.L.C. Farias and A. Aspuru-Guzik, Objectivereinforced generative adversarial networks (ORGAN) for sequence generation models. arXiv:1705.10843 (2017).
[29] A. Nigam, P. Friederich, M. Krenn and A. AspuruGuzik, Augmenting Genetic Algorithms with Deep Neural Networks for Exploring the Chemical Space. arXiv:1909.11655 (2019).
[30] N. O'Boyle, J. Mayfield and R. Sayle, De facto standard or a free-for-all? A benchmark for reading SMILES. Abstracts of Papers of the American Chemical Society 256, (2018).
[31] M. Erhard, M. Malik, M. Krenn and A. Zeilinger, Experimental greenberger-horne-zeilinger entanglement beyond qubits. Nature Photonics 12, 759-764 (2018).</p>
<h1>Supplemental Materials</h1>
<h2>I. FORMAL DEFINITION OF SELFIES</h2>
<p>We take advantage of a formal grammar to derive words, which will represent semantically valid graphs. A formal grammar is a tuple $G(V, \Sigma, R, S)$, where $v \in V$ are non-terminal symbols that are replaced using rules, $r \in R$, into non-terminal or terminal symbols $t \in \Sigma . S$ is a start symbol. When the resulting string only consists of terminal symbols, the derivation of a new word is completed [18]. The SELFIES representation is a Chomsky type-2, context-free grammar with self-referencing functions for valid generation of branches in graphs. The rule system is shown in Table I.</p>
<p>In SELFIES, $V=\left{\mathbf{X}<em r="r">{0}, \ldots, \mathbf{X}</em>}, \mathbf{N}\right}$ are non-terminal symbols or states. The states $\mathbf{X<em 0_1="0,1">{i}$ restrict the subsequent edge to a maximal multiplicity of $i$; the maximal edge multiplicity of the generated graphs is $r$. The symbol $\mathbf{N}$ represents a numerical value, which acts as argument for the two self-referencing functions. $\Sigma=\left{t</em>$, each with a dimension $(r+2)$.}, \ldots, t_{r, n}\right}$ are terminal symbols. The derivation rule set $R$ has exactly $(n+m+p+1) \times(r+2)$ elements, corresponding to $n$ rules for vertex production, $m$ rules for producing branches, $p$ rules for rings and $r$ non-terminal symbols in $V$. The subscripts $h_{a, b}, i_{a, b}, j_{a, b}$ and $k_{a, b}$ have values from 1 to $r$, and encode the actual domain-specific constraints. The semantic and syntactical constraints are encoded into the rule vectors, which guarantees strong robustness. There are $n+m+p+1$ rule vectors $\mathbf{A}_{i</p>
<h2>II. SELF-REFERENCING FUNCTIONS FOR SYNTACTIC VALIDITY</h2>
<p>In order to account for syntactic validity of the graph, we augment the context-free grammar with branching functions and ring functions. $B\left(\mathbf{N}, X_{i}\right)$ is the branching function, that recursively starts another grammar derivation with subsequent $\mathbf{N}$ SELFIES symbols in state $X_{i}$. After the full derivation of a new word (which is a graph), the branch function returns the graph, and connects it to the current vertex. The ring function $R(\mathbf{N})$ establishes edges between the current vertex and the $(\mathbf{N}+1)$-th last derived vertex. Both the branching and ring functions have access to the SELFIES string and the derived string, thus are self-referencing.</p>
<h2>III. RULE VECTORS FOR SEMANTIC VALIDITY</h2>
<p>To incorporate semantic validity, we denote $A_{i}$ as the $i$-th vector of rules, with dimension $d_{A_{i}}=|V|=r+2$. The conceptual idea is to interpret a symbol of a SELFIES string, $s_{i} \in{0, \ldots, n+m+p}$ as an index of a rule vector, $A_{s_{i}}$. In the derivation of a symbol, the rule vector is defined by the symbol of the SELFIES string (external state) while the specific rule is chosen by the non-terminal symbol (internal state). Thereby, we can encode semantic information into the rule vector $A_{i}$, which is memorized by the internal state during derivation.</p>
<h2>IV. ALGORITHMIC DERIVATION OF GRAMMAR FROM DATA, AND VALIDITY GUARANTEES</h2>
<p>Domain-specific grammars can be derived algorithmically directly from data, without any domain knowledge. Let $T$ be the set of different types of vertices (such as $\mathrm{C}, \mathrm{O}, \mathrm{N}, \ldots$ in chemistry). We use a dataset to get the types of vertices $T_{i}$, and their maximal degrees $D_{i}\left(D_{i}=\operatorname{maxdeg}\left(T_{i}\right)\right.$ - in chemistry, the $D_{\mathrm{O}}=\operatorname{maxdeg}(\mathrm{O})=2$, $D_{\mathrm{C}}=\operatorname{maxdeg}(\mathrm{C})=4)$. Let $M=\max <em i="i">{i} \operatorname{maxdeg}\left(T</em>}\right)$ be the maximal degree of the dataset. Starting from Table I (I) we identify the rule vectors $\mathbf{A<em j="j">{i}$, (II) define the non-terminal symbols $\mathbf{X}</em>$, and (III) define the rules $R$.</p>
<p>I $\mathbf{A}<em n="n">{1} \ldots \mathbf{A}</em>}$ (vertices rules) consist of $T_{i}$ with a potential multiedge connection $\gamma$ up to $D_{i}$ (in chemistry, $D_{\mathrm{O}}=2$, thus we have two rule vectors for $O$, one with single edge $\gamma=1$, one with double connection $\gamma=2$ ). $\mathbf{A<em n_m="n+m">{n+1} \ldots \mathbf{A}</em>}$ represent branch rules. A branch forms connections to two vertices, thus we have maximally $(M-1)$ branch rules, (combinations of $(M-l, l)$ represent the maximal connectivity to the two branches). $\mathbf{A<em n_m_p="n+m+p">{n+m+1} \ldots \mathbf{A}</em>$ denote ring rules, in a generic case $p=1$ is sufficient.</p>
<p>II non-terminals $X_{1} \ldots X_{r}$, with $r=M$, constrain the number of edges to connect two vertices.
III Rule $r_{i, j}$ for $\mathbf{A}<em 1="1">{i} \in\left{\mathbf{A}</em>} \ldots \mathbf{A<em j="j">{n}\right}$ and $\mathbf{X}</em>} \in\left{\mathbf{X<em r="r">{1} \ldots \mathbf{X}</em>$ ) and a edge-multiplicity $\mu=\min (j, \gamma)$. The corresponding}\right}$ can consist of a terminal and non-terminal symbol. The terminal consists of a $T_{i}$ (given by $\mathbf{A}_{i</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Table I. Grammar of SELFIES, with recursion and $\mathbf{S} \rightarrow \mathbf{X}<em M-_mu="M-\mu">{0}$.
nonterminal symbol is $\mathbf{X}</em>}$ (if $M-\mu=0$, no non-terminal will be added). Note that constraints are satisfied due to the min operation in $\mu$. Rules in state $\mathbf{X<em j-1="j-1">{j}$ for rings are $R(N) \mathbf{X}</em>$.}$, and for branches are $B\left(N, X_{i}\right) \mathbf{X}_{j-i</p>
<p>The edge-multiplicity $\mu=\min (j, \gamma)$ is responsible for the semantic constraint of local degrees being satisfied. This is the most immediate constraint in many applications for physical sciences, which allows for $100 \%$ validity. More complex, non-local constraints could be implemented by more complex grammars, such as explicit contextsensitive type-1 grammars.</p>
<h1>V. POTENTIAL APPLICATIONS TO OTHER DOMAINS IN THE PHYSICAL SCIENCES</h1>
<p>SELFIES can be used independently of the domain, which we demonstrate here. Ideal targets for SELFIES grammar are different types of objects (which for the vertices) with vertex-dependent connectivity restrictions. In that case, rule vectors of grammars can be used to encode the restrictions on connectivities. Rings and Branches could be dependent on vertices as well. We now show now one different example from physics.</p>
<h2>A. Quantum Optical Experiments</h2>
<p>A grammar for the generation of quantum optical experments can be written in Table II.
There, the non-terminal symbols stand for quantum optical components that are used in experiments, [SPDC] stands for a non-linear crystal that undergoes spontaneous parametric down-conversion to produce photon pairs, [BS] stands for beam splitters, [Holo] stands for holograms to modify the quantum state, [DF] stands for Dove prism which introduces mode dependent phases, [Ref] stand for mirrors which modify mode numbers and phases, and [Det] are single-photon detector. $\mathrm{B}\left(\mathbf{N}, \mathbf{X}_{0}\right)$ and $\mathrm{R}(\mathbf{N})$ are branch functions and ring functions as defined in the main text. Now we derive a recent complex quantum optical experiment (which has been designed by
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7. SELFIES for quantum optical experiments. In (a) we see the graph generated from SELFIES for a recent high-dimensional multipartite quantum experiment [31]. In (b), the structure of the experimental configuration.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$\mathbf{S}$</th>
<th style="text-align: center;">$\mathbf{B}$</th>
<th style="text-align: center;">$\mathbf{H}$</th>
<th style="text-align: center;">$\mathbf{P}$</th>
<th style="text-align: center;">$\mathbf{R}$</th>
<th style="text-align: center;">$\mathbf{D}$</th>
<th style="text-align: center;">$\mathbf{Y}$</th>
<th style="text-align: center;">$\mathbf{Z}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\mathbf{X}_{0} \rightarrow$ [SPDC]</td>
<td style="text-align: center;">$\mathbf{X}_{2} \mid$ [BS]</td>
<td style="text-align: center;">$\mathbf{X}_{3} \mid$ [Holo]</td>
<td style="text-align: center;">$\mathbf{X}_{1} \mid$ [DP]</td>
<td style="text-align: center;">$\mathbf{X}_{1} \mid$ [Ref]</td>
<td style="text-align: center;">$\mathbf{X}_{1} \mid$ [Det]</td>
<td style="text-align: center;">$\mathbf{X}_{0}$</td>
<td style="text-align: center;">$\mathbf{X}_{0}$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{X}_{1} \rightarrow$ [SPDC]</td>
<td style="text-align: center;">$\mathbf{X}_{1} \mid$ [BS]</td>
<td style="text-align: center;">$\mathbf{X}_{3} \mid$ [Holo]</td>
<td style="text-align: center;">$\mathbf{X}_{1} \mid$ [DP]</td>
<td style="text-align: center;">$\mathbf{X}_{1} \mid$ [Ref]</td>
<td style="text-align: center;">$\mathbf{X}_{1} \mid$ [Det]</td>
<td style="text-align: center;">$\mathbf{X}_{1}$</td>
<td style="text-align: center;">$\mid \mathrm{R}(\mathbf{N})$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{X}_{2} \rightarrow$ [SPDC]</td>
<td style="text-align: center;">$\mathbf{X}_{1} \mid$ [BS]</td>
<td style="text-align: center;">$\mathbf{X}_{3} \mid$ [Holo]</td>
<td style="text-align: center;">$\mathbf{X}_{1} \mid$ [DP]</td>
<td style="text-align: center;">$\mathbf{X}_{1} \mid$ [Ref]</td>
<td style="text-align: center;">$\mathbf{X}_{1} \mid$ [Det]</td>
<td style="text-align: center;">$\mathrm{B}\left(\mathbf{N}, \mathbf{X}<em 1="1">{0}\right) \mathbf{X}</em>$</td>
<td style="text-align: center;">$\mathrm{R}(\mathbf{N}) \mathbf{X}_{1}$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{X}_{3} \rightarrow$ [SPDC]</td>
<td style="text-align: center;">$\mathbf{X}_{1} \mid$ [BS]</td>
<td style="text-align: center;">$\mathbf{X}_{3} \mid$ [Holo]</td>
<td style="text-align: center;">$\mathbf{X}_{1} \mid$ [DP]</td>
<td style="text-align: center;">$\mathbf{X}_{1} \mid$ [Ref]</td>
<td style="text-align: center;">$\mathbf{X}_{1} \mid$ [Det]</td>
<td style="text-align: center;">$\mathrm{B}\left(\mathbf{N}, \mathbf{X}<em 2="2">{0}\right) \mathbf{X}</em>$</td>
<td style="text-align: center;">$\mathrm{R}(\mathbf{N}) \mathbf{X}_{2}$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{N} \rightarrow 1$</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">9</td>
</tr>
</tbody>
</table>
<p>Table II. Derivation rules of SELFIES for a semantically restricted graph that represents quantum optical experiments, with the derivation starting in $\mathbf{X}_{0}$.
a computer algorithm), which demonstrates high-dimensional multi-partite quantum entanglement [31]. The graph and the corresponding setup can be seen in Figure 7.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>mario.krenn@utoronto.ca
${ }^{\dagger}$ alan@aspuru.com
The full code is available at GitHub: https://github.com/ aspuru-guzik-group/selfies</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
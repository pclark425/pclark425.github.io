<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5379 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5379</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5379</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-5ba05e6bb54b2e6e6d70ce0193e92d4ec4cf48ef</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5ba05e6bb54b2e6e6d70ce0193e92d4ec4cf48ef" target="_blank">Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Pattern Analysis and Machine Intelligence</p>
                <p><strong>Paper TL;DR:</strong> The novel framework MuseGraph seamlessly integrates the strengths of GNNs and LLMs into one foundation model for graph mining across tasks and datasets, and proposes a diverse instruction generation mechanism with Chain-of-Thought (CoT) based instruction packages to distill the reasoning capabilities from advanced LLMs like GPT-4.</p>
                <p><strong>Paper Abstract:</strong> Graphs with abundant attributes are essential in modeling interconnected entities and enhancing predictions across various real-world applications. Traditional Graph Neural Networks (GNNs) often require re-training for different graph tasks and datasets. Although the emergence of Large Language Models (LLMs) has introduced new paradigms in natural language processing, their potential for generic graph mining-training a single model to simultaneously handle diverse tasks and datasets-remains under-explored. To this end, our novel framework MuseGraph, seamlessly integrates the strengths of GNNs and LLMs into one foundation model for graph mining across tasks and datasets. This framework first features a compact graph description to encapsulate key graph information within language token limitations. Then, we propose a diverse instruction generation mechanism with Chain-of-Thought (CoT)-based instruction packages to distill the reasoning capabilities from advanced LLMs like GPT-4. Finally, we design a graph-aware instruction tuning strategy to facilitate mutual enhancement across multiple tasks and datasets while preventing catastrophic forgetting of LLMs' generative abilities. Our experimental results demonstrate significant improvements in five graph tasks and ten datasets, showcasing the potential of our MuseGraph in enhancing the accuracy of graph-oriented downstream tasks while improving the generation abilities of LLMs.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5379.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5379.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Compact Graph Description (MuseGraph)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compact Graph Description with Node-Energy-Guided Neighbor and Walk Textualization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-text representation introduced in this paper that textualizes an attributed graph around target node(s) by selecting key one-hop neighbors and sampled random walks guided by a node energy metric H(v), producing a token-efficient natural-language description for LLM input and instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Compact Graph Description (node-energy-guided neighbor+random-walk textualization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For a target node v*, compute node energy H(v)=T(v)*ceil(log(D(v)+1)) where T(v) is token count of node attributes and D(v) is node degree. Under an LLM token budget L, select neighbor nodes v_i whose H(v_i) >= H(v*) for one-hop neighbor inclusion and expand sampled random walks from v* until token or H thresholds are exceeded. When multiple nodes are involved (e.g., link prediction), allocate token budgets per node proportional to softmax of node energies. The output is a natural-language 'ego graph' including node attributes, a small selected neighbor list and concise random-walk traces formatted as text prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Attributed graphs (heterogeneous and homogeneous graphs, knowledge-graph style/ego-graphs used in node classification, link prediction, graph-to-text and reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compactness: explicitly designed for token-limited LLM input; Token-efficiency: prioritizes nodes by H(v) to maximize informativeness under token constraints; Semantic+structural integration: encodes node textual attributes and structural context via neighbors and random walks (captures local connectivity and high-order relationships); Interpretability: human-readable ego-graph and walk traces; Adaptive: neighbor vs walk inclusion is adjusted per-node via H and L; Potential information loss: truncation when token budgets are tight; Inspired by GNN priors (message passing/high-order propagation) but requires no GNN encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Evaluated across node classification (heterogeneous IMDB, Freebase; homogeneous Cora, ogbn-arxiv), link prediction (Yelp, MIMIC-III), graph-to-text (AGENDA, WebNLG few-shot), and dynamic reasoning tasks (DyVal Reachability and Max Sum Path zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported downstream model performance when used to fine-tune MuseGraph: Node classification (IMDB) Micro-F1 76.57, Macro-F1 73.78 (MuseGraph LLaMA3-8B); Link prediction (Yelp) AUC 80.07 (MuseGraph LLaMA3-8B); Graph-to-Text (AGENDA 500 instances) BLEU-4 14.64 and (WebNLG 500) BLEU-4 36.83 (MuseGraph LLaMA3-8B). (Metrics shown in Tables V, VI, VII; numbers are for the full MuseGraph pipeline using this representation.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to other graph->text approaches (e.g., OFA, All-in-One, GraphGPT/HiGPT/InstructGLM), the compact description yields stronger cross-task and cross-dataset performance when used with graph-aware instruction tuning: MuseGraph outperforms OFA/All-in-One and most GNN/LLM baselines across node classification, link prediction and graph-to-text benchmarks in this paper. The authors attribute gains to better token utilization (node-energy selection) and inclusion of random-walk context versus naive expansion to multi-hop neighbors which causes token overload and performance drops.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Depends on input graphs with reasonably rich textual attributes (authors note limitation); token-budget forces truncation that can lose information for very large neighborhoods or long attribute texts; design choices (e.g., walk sampling, H thresholding, formatting specifics) may affect faithfulness and require ablation/tuning; representation assumes meaningful tokenization T(v) and degree-based centrality correlate with informativeness which may not hold in all domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5379.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5379.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OFA (One-for-All)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One for All: Towards Training One Graph Model for All Classification Tasks (OFA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior method referenced that describes nodes and edges using human-understandable prompt texts; converts such textualized graph descriptions into embeddings via LLMs to guide a single graph model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>One for all: Towards training one graph model for all classification tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Natural-language node/edge prompt textualization (OFA)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Describes all nodes and edges using human-understandable prompt texts (natural-language descriptions of graph elements) and then uses LLM encoders to convert those texts into unified embeddings for downstream tasks (as described in the paper's related work summary). The paper does not provide OFA's exact formatting rules but characterizes it as full natural-language descriptions of graph elements.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs / attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Human-understandable and directly leverages LLM encoders; expressive but potentially token-heavy; enables unified embedding conversion but may be dataset- or prompt-sensitive; likely less token-efficient without additional summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Compared on node classification and link prediction benchmarks in this work (results reported in Tables V and VI).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From Table V and VI (as reproduced in this paper): Node classification IMDB Micro-F1 64.81 Macro-F1 57.91; Freebase Micro-F1 56.58 Macro-F1 34.75; Cora Micro-F1 71.68 Macro-F1 70.21; Arxiv Micro-F1 69.31 Macro-F1 50.70. Link prediction Yelp AUC 71.56, MIMIC-III AUC 59.33.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>OFA performs reasonably well on some node-classification tasks but is outperformed by MuseGraph's compact descriptions in cross-task and cross-dataset evaluations; authors note OFA relies on manual prompt design and can be less adaptable/cost-effective than automated compact graph textualization with CoT instruction packages.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Potentially high token usage for full node/edge textualization; manual prompt crafting and dataset-specific prompt engineering required; may lack cross-dataset adaptability and is more computationally expensive to convert whole graphs into text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5379.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5379.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>All-in-One</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>All-in-One: Multi-task prompting for graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that reformulates different graph tasks into unified graph and language prompts, effectively converting graph tasks into prompt-style textual inputs for models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>All in one: Multi-task prompting for graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Unified graph + language prompt reformulation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Recasts different graph-level tasks using a unified scheme of graph prompts combined with language prompts so that multiple tasks/datasets can be expressed in a shared natural-language prompt format; specifics of the textual serialization are not detailed in this paper outside of the high-level description.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Attributed graphs / text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Unified format across tasks (task-agnostic prompt template); enables multi-task prompting but may be limited by token budgets and dataset heterogeneity; relies on prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Included in comparative evaluations (node classification, link prediction, graph-to-text benchmarks) in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Tables V and VI: Node classification IMDB Micro-F1 60.75 Macro-F1 55.17; Freebase Micro-F1 50.48 Macro-F1 21.04; Cora Micro-F1 68.45 Macro-F1 64.18; Arxiv Micro-F1 65.57 Macro-F1 47.06. Link prediction Yelp AUC 70.83, MIMIC-III AUC 56.20.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>All-in-One improves multi-task coverage compared to some GNN baselines but is outperformed by MuseGraph which leverages compact descriptions plus CoT instruction packages; All-in-One's unified prompts are less effective than MuseGraph's dynamic instruction allocation and node-energy-driven textualization in cross-dataset generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prompt engineering required; may be less flexible in balancing token budgets across diverse tasks and datasets; can struggle with cross-dataset adaptability compared to the MuseGraph pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5379.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5379.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAPE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Harnessing explanations: LLM-to-LM interpreter for enhanced text-attributed graph representation learning (TAPE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that prompts LLMs to obtain explanations and uses those explanations to enhance node initial features for training GNNs; effectively uses LLM-generated textual content as enhanced features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Harnessing explanations: Llm-to-lm interpreter for enhanced textattributed graph representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>LLM-produced explanations as node textual features</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses LLMs to generate natural-language explanations (text) about nodes which are then encoded into node features for GNNs; this converts graph text attributes and LLM outputs into features rather than standalone natural-language prompts for LLM-only processing.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Augments node features with LLM explanations (more semantic richness); relies on an external GNN predictor; higher annotation cost (often via GPT-3.5 API) and dataset-specific custom prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Reported (in this paper) primarily on node classification (Cora and Arxiv) as TAPE public features were used for these datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From Table V (as reproduced): Cora Micro-F1 88.19 Macro-F1 87.16; Arxiv Micro-F1 74.36 Macro-F1 55.74. Other datasets marked '-' (public features not available).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>TAPE achieves strong performance on Cora/Arxiv (noted as top results on those datasets) because it leverages high-quality LLM explanations for GNN features, but it requires costly manual prompt or API usage and is less flexible for cross-task/dataset generalization compared to MuseGraph's instruction-tuned LLM approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High cost due to reliance on API calls to advanced LLMs for explanations; requires manual/custom prompt creation per dataset; approach depends on downstream GNNs and is not LLM-native for generation tasks such as graph-to-text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5379.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5379.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphGPT: Graph instruction tuning for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuning approach that aligns LLMs with graph structural knowledge using graph-structured instructions and text-graph grounding to improve LLM reasoning on graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphgpt: Graph instruction tuning for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph structural instruction textualization (GraphGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses instruction tuning with graph-structured textual instructions (text-graph grounding) and step-by-step reasoning prompts so that LLMs can process graph tasks; the paper references GraphGPT's paradigm but does not reproduce its exact textualization schemeâ€”GraphGPT checkpoint was used in experiments for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Attributed graphs / heterogeneous graphs / knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Instruction-tuning oriented; integrates structural cues into prompts; designed for better graph-aware reasoning in LLMs; may still rely on dataset-specific instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Evaluated or compared on node classification, link prediction and graph-to-text tasks in this paper (GraphGPT checkpoint used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From Tables in this paper: Node classification IMDB Micro-F1 44.52 Macro-F1 43.70; Freebase Micro-F1 15.80 Macro-F1 6.14; Cora Micro-F1 24.57 Macro-F1 15.26; Arxiv Micro-F1 31.08 Macro-F1 17.12. Link prediction Yelp AUC 48.24, MIMIC-III AUC 52.01. (Reported as baseline numbers in Tables V and VI.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>GraphGPT provides instruction-tuning for graphs but in the reported evaluations it underperforms MuseGraph; authors note many LLM-based methods (including GraphGPT) either depend on task/dataset-specific GNN encoders or lack the compact, token-aware textualization strategy employed by MuseGraph.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance can degrade when models are not fine-tuned with token-efficient compact textualizations; reliance on specific instruction sets may limit cross-dataset generalization; reported baseline performance is lower than MuseGraph in this paper's experimental suite.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5379.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5379.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HiGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HiGPT: Heterogeneous Graph Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based model that aligns LLMs with heterogeneous graph knowledge via instruction tuning, a specialized graph tokenizer, and mixture-of-thought augmentation to better handle graph-structured inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Higpt: Heterogeneous graph language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Heterogeneous-graph-aware tokenization + instruction tuning (HiGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Aligns LLMs with heterogeneous graph knowledge by employing a specialized graph tokenizer (to encode graph tokens), instruction tuning, and mixture-of-thought augmentation; converts graph elements into tokens and prompts tailored for heterogeneous graphs. The current paper used a HiGPT checkpoint (Vicuna-7B-v1.5 tuned with IMDB instructions) for experimental comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Heterogeneous graphs / attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Designed for heterogeneity and mixture-of-thought reasoning; includes learned graph tokens (tokenizer) which can improve performance on datasets seen during tuning (e.g., IMDB) but may generalize poorly to out-of-domain or homogeneous graphs without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Benchmarked here on node classification, link prediction and graph-to-text tasks; checkpoint used for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From Tables: Node classification IMDB Micro-F1 56.72 Macro-F1 53.77; Freebase Micro-F1 16.17 Macro-F1 7.02; Cora Micro-F1 23.86 Macro-F1 14.17; Arxiv Micro-F1 10.76 Macro-F1 6.25. Link prediction Yelp AUC 49.73, MIMIC-III AUC 51.04. Graph-to-text few-shot AGENDA/WebNLG reported in Table VII (HiGPT shown with limited entries e.g., AGENDA 8.49 in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>HiGPT excels on datasets similar to its training data (e.g., IMDB) but shows substantial drops on other datasets (e.g., Cora), indicating less cross-dataset adaptability than MuseGraph's compact description plus instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Learned graph tokens and tuning on a specific dataset can hinder generalization; struggles with tasks like link prediction in some settings (producing incorrect 'Yes' answers) and with homogeneous datasets when trained on heterogeneous examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5379.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5379.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language is all a graph needs (InstructGLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that tunes LLMs with scalable graph prompts combining natural language instructions and node features (often from trained GNNs) to perform graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language is all a graph needs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Scalable graph prompts combining NL instructions and node features (InstructGLM)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Tunes LLMs using scalable natural-language graph prompts that can include node features (sometimes derived from trained GNNs) and instructions; effectively converts graph information into prompted natural language for LLM consumption. The authors used a public InstructGLM checkpoint for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Attributed graphs / knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Scalable prompt design; can incorporate encoded node features; aims to be natural-language-native but performance depends on quality and scope of included node features and prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Compared on node classification, link prediction and graph-to-text benchmarks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported numbers in Tables: Node classification IMDB Micro-F1 13.59 Macro-F1 9.41; Freebase Micro-F1 5.65 Macro-F1 2.38; Cora Micro-F1 15.62 Macro-F1 10.88; Arxiv Micro-F1 42.60 Macro-F1 28.43. Link prediction Yelp AUC 49.38, MIMIC-III AUC 34.35. Graph-to-Text: some entries in Table VII (limited).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>InstructGLM provides a natural-language prompting scheme but in these experiments it underperformed MuseGraph and many baselines; it sometimes benefits from integrating GNN-derived node features but remains sensitive to prompt and feature design.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance variability across datasets; may require node features from trained GNNs to be effective; suffers from lower adaptability in some tasks versus MuseGraph's instruction-tuned LLM pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5379.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5379.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-GNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-GNN: Label-free node classification on graphs with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses LLMs as annotators to generate pseudo labels or textual annotations that are used to train GNNs, thereby converting graph textual/contextual information into training signals for graph models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Label-free node classification on graphs with large language models (llms)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>LLM-generated annotations / textual labeling for GNN training</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses an LLM to annotate nodes (generate pseudo labels or text-based annotations) and then trains GNN predictors on those labels/features; the conversion is via LLM-generated text that becomes supervisory or feature information for GNNs rather than direct LLM prompt inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Attributed graphs / text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Leverages LLMs to create labels/features without human labeling; can improve GNN training when high-quality LLM annotations are available; sensitive to annotation quality and LLM hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification (Cora, Arxiv used public features from LLM-GNN in comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In this paper, LLM-GNN reported for Cora Micro-F1 75.61 Macro-F1 73.95 and Arxiv Micro-F1 66.05 Macro-F1 45.33 (Table V). Other datasets marked '-' where public features were not used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>LLM-GNN can yield strong node-classification performance on datasets where LLM annotations are good, but it is a two-stage approach (LLM -> GNN) and may have higher costs and limited crosstask generality compared to MuseGraph's unified LLM instruction-tuning approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on high-quality LLM annotations (costly if using API); not directly an LLM-only graph->text representation for generation tasks; limited public features availability for comparison across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>One for all: Towards training one graph model for all classification tasks <em>(Rating: 2)</em></li>
                <li>All in one: Multi-task prompting for graph neural networks <em>(Rating: 2)</em></li>
                <li>Harnessing explanations: Llm-to-lm interpreter for enhanced textattributed graph representation learning <em>(Rating: 2)</em></li>
                <li>Graphgpt: Graph instruction tuning for large language models <em>(Rating: 2)</em></li>
                <li>Higpt: Heterogeneous graph language model <em>(Rating: 2)</em></li>
                <li>Language is all a graph needs <em>(Rating: 2)</em></li>
                <li>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking <em>(Rating: 1)</em></li>
                <li>Label-free node classification on graphs with large language models (llms) <em>(Rating: 1)</em></li>
                <li>Prodigy: Enabling in-context learning over graphs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5379",
    "paper_id": "paper-5ba05e6bb54b2e6e6d70ce0193e92d4ec4cf48ef",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "Compact Graph Description (MuseGraph)",
            "name_full": "Compact Graph Description with Node-Energy-Guided Neighbor and Walk Textualization",
            "brief_description": "A graph-to-text representation introduced in this paper that textualizes an attributed graph around target node(s) by selecting key one-hop neighbors and sampled random walks guided by a node energy metric H(v), producing a token-efficient natural-language description for LLM input and instruction tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Compact Graph Description (node-energy-guided neighbor+random-walk textualization)",
            "representation_description": "For a target node v*, compute node energy H(v)=T(v)*ceil(log(D(v)+1)) where T(v) is token count of node attributes and D(v) is node degree. Under an LLM token budget L, select neighbor nodes v_i whose H(v_i) &gt;= H(v*) for one-hop neighbor inclusion and expand sampled random walks from v* until token or H thresholds are exceeded. When multiple nodes are involved (e.g., link prediction), allocate token budgets per node proportional to softmax of node energies. The output is a natural-language 'ego graph' including node attributes, a small selected neighbor list and concise random-walk traces formatted as text prompts.",
            "graph_type": "Attributed graphs (heterogeneous and homogeneous graphs, knowledge-graph style/ego-graphs used in node classification, link prediction, graph-to-text and reasoning tasks)",
            "representation_properties": "Compactness: explicitly designed for token-limited LLM input; Token-efficiency: prioritizes nodes by H(v) to maximize informativeness under token constraints; Semantic+structural integration: encodes node textual attributes and structural context via neighbors and random walks (captures local connectivity and high-order relationships); Interpretability: human-readable ego-graph and walk traces; Adaptive: neighbor vs walk inclusion is adjusted per-node via H and L; Potential information loss: truncation when token budgets are tight; Inspired by GNN priors (message passing/high-order propagation) but requires no GNN encoder.",
            "evaluation_task": "Evaluated across node classification (heterogeneous IMDB, Freebase; homogeneous Cora, ogbn-arxiv), link prediction (Yelp, MIMIC-III), graph-to-text (AGENDA, WebNLG few-shot), and dynamic reasoning tasks (DyVal Reachability and Max Sum Path zero-shot).",
            "performance_metrics": "Reported downstream model performance when used to fine-tune MuseGraph: Node classification (IMDB) Micro-F1 76.57, Macro-F1 73.78 (MuseGraph LLaMA3-8B); Link prediction (Yelp) AUC 80.07 (MuseGraph LLaMA3-8B); Graph-to-Text (AGENDA 500 instances) BLEU-4 14.64 and (WebNLG 500) BLEU-4 36.83 (MuseGraph LLaMA3-8B). (Metrics shown in Tables V, VI, VII; numbers are for the full MuseGraph pipeline using this representation.)",
            "comparison_to_other_representations": "Compared to other graph-&gt;text approaches (e.g., OFA, All-in-One, GraphGPT/HiGPT/InstructGLM), the compact description yields stronger cross-task and cross-dataset performance when used with graph-aware instruction tuning: MuseGraph outperforms OFA/All-in-One and most GNN/LLM baselines across node classification, link prediction and graph-to-text benchmarks in this paper. The authors attribute gains to better token utilization (node-energy selection) and inclusion of random-walk context versus naive expansion to multi-hop neighbors which causes token overload and performance drops.",
            "limitations_or_challenges": "Depends on input graphs with reasonably rich textual attributes (authors note limitation); token-budget forces truncation that can lose information for very large neighborhoods or long attribute texts; design choices (e.g., walk sampling, H thresholding, formatting specifics) may affect faithfulness and require ablation/tuning; representation assumes meaningful tokenization T(v) and degree-based centrality correlate with informativeness which may not hold in all domains.",
            "uuid": "e5379.0",
            "source_info": {
                "paper_title": "Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "OFA (One-for-All)",
            "name_full": "One for All: Towards Training One Graph Model for All Classification Tasks (OFA)",
            "brief_description": "Prior method referenced that describes nodes and edges using human-understandable prompt texts; converts such textualized graph descriptions into embeddings via LLMs to guide a single graph model.",
            "citation_title": "One for all: Towards training one graph model for all classification tasks",
            "mention_or_use": "mention",
            "representation_name": "Natural-language node/edge prompt textualization (OFA)",
            "representation_description": "Describes all nodes and edges using human-understandable prompt texts (natural-language descriptions of graph elements) and then uses LLM encoders to convert those texts into unified embeddings for downstream tasks (as described in the paper's related work summary). The paper does not provide OFA's exact formatting rules but characterizes it as full natural-language descriptions of graph elements.",
            "graph_type": "Text-attributed graphs / attributed graphs",
            "representation_properties": "Human-understandable and directly leverages LLM encoders; expressive but potentially token-heavy; enables unified embedding conversion but may be dataset- or prompt-sensitive; likely less token-efficient without additional summarization.",
            "evaluation_task": "Compared on node classification and link prediction benchmarks in this work (results reported in Tables V and VI).",
            "performance_metrics": "From Table V and VI (as reproduced in this paper): Node classification IMDB Micro-F1 64.81 Macro-F1 57.91; Freebase Micro-F1 56.58 Macro-F1 34.75; Cora Micro-F1 71.68 Macro-F1 70.21; Arxiv Micro-F1 69.31 Macro-F1 50.70. Link prediction Yelp AUC 71.56, MIMIC-III AUC 59.33.",
            "comparison_to_other_representations": "OFA performs reasonably well on some node-classification tasks but is outperformed by MuseGraph's compact descriptions in cross-task and cross-dataset evaluations; authors note OFA relies on manual prompt design and can be less adaptable/cost-effective than automated compact graph textualization with CoT instruction packages.",
            "limitations_or_challenges": "Potentially high token usage for full node/edge textualization; manual prompt crafting and dataset-specific prompt engineering required; may lack cross-dataset adaptability and is more computationally expensive to convert whole graphs into text.",
            "uuid": "e5379.1",
            "source_info": {
                "paper_title": "Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "All-in-One",
            "name_full": "All-in-One: Multi-task prompting for graph neural networks",
            "brief_description": "A method that reformulates different graph tasks into unified graph and language prompts, effectively converting graph tasks into prompt-style textual inputs for models.",
            "citation_title": "All in one: Multi-task prompting for graph neural networks",
            "mention_or_use": "mention",
            "representation_name": "Unified graph + language prompt reformulation",
            "representation_description": "Recasts different graph-level tasks using a unified scheme of graph prompts combined with language prompts so that multiple tasks/datasets can be expressed in a shared natural-language prompt format; specifics of the textual serialization are not detailed in this paper outside of the high-level description.",
            "graph_type": "Attributed graphs / text-attributed graphs",
            "representation_properties": "Unified format across tasks (task-agnostic prompt template); enables multi-task prompting but may be limited by token budgets and dataset heterogeneity; relies on prompt design.",
            "evaluation_task": "Included in comparative evaluations (node classification, link prediction, graph-to-text benchmarks) in this paper.",
            "performance_metrics": "Reported in Tables V and VI: Node classification IMDB Micro-F1 60.75 Macro-F1 55.17; Freebase Micro-F1 50.48 Macro-F1 21.04; Cora Micro-F1 68.45 Macro-F1 64.18; Arxiv Micro-F1 65.57 Macro-F1 47.06. Link prediction Yelp AUC 70.83, MIMIC-III AUC 56.20.",
            "comparison_to_other_representations": "All-in-One improves multi-task coverage compared to some GNN baselines but is outperformed by MuseGraph which leverages compact descriptions plus CoT instruction packages; All-in-One's unified prompts are less effective than MuseGraph's dynamic instruction allocation and node-energy-driven textualization in cross-dataset generalization.",
            "limitations_or_challenges": "Prompt engineering required; may be less flexible in balancing token budgets across diverse tasks and datasets; can struggle with cross-dataset adaptability compared to the MuseGraph pipeline.",
            "uuid": "e5379.2",
            "source_info": {
                "paper_title": "Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "TAPE",
            "name_full": "Harnessing explanations: LLM-to-LM interpreter for enhanced text-attributed graph representation learning (TAPE)",
            "brief_description": "A method that prompts LLMs to obtain explanations and uses those explanations to enhance node initial features for training GNNs; effectively uses LLM-generated textual content as enhanced features.",
            "citation_title": "Harnessing explanations: Llm-to-lm interpreter for enhanced textattributed graph representation learning",
            "mention_or_use": "mention",
            "representation_name": "LLM-produced explanations as node textual features",
            "representation_description": "Uses LLMs to generate natural-language explanations (text) about nodes which are then encoded into node features for GNNs; this converts graph text attributes and LLM outputs into features rather than standalone natural-language prompts for LLM-only processing.",
            "graph_type": "Text-attributed graphs",
            "representation_properties": "Augments node features with LLM explanations (more semantic richness); relies on an external GNN predictor; higher annotation cost (often via GPT-3.5 API) and dataset-specific custom prompts.",
            "evaluation_task": "Reported (in this paper) primarily on node classification (Cora and Arxiv) as TAPE public features were used for these datasets.",
            "performance_metrics": "From Table V (as reproduced): Cora Micro-F1 88.19 Macro-F1 87.16; Arxiv Micro-F1 74.36 Macro-F1 55.74. Other datasets marked '-' (public features not available).",
            "comparison_to_other_representations": "TAPE achieves strong performance on Cora/Arxiv (noted as top results on those datasets) because it leverages high-quality LLM explanations for GNN features, but it requires costly manual prompt or API usage and is less flexible for cross-task/dataset generalization compared to MuseGraph's instruction-tuned LLM approach.",
            "limitations_or_challenges": "High cost due to reliance on API calls to advanced LLMs for explanations; requires manual/custom prompt creation per dataset; approach depends on downstream GNNs and is not LLM-native for generation tasks such as graph-to-text.",
            "uuid": "e5379.3",
            "source_info": {
                "paper_title": "Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GraphGPT",
            "name_full": "GraphGPT: Graph instruction tuning for large language models",
            "brief_description": "An instruction-tuning approach that aligns LLMs with graph structural knowledge using graph-structured instructions and text-graph grounding to improve LLM reasoning on graphs.",
            "citation_title": "Graphgpt: Graph instruction tuning for large language models",
            "mention_or_use": "use",
            "representation_name": "Graph structural instruction textualization (GraphGPT)",
            "representation_description": "Uses instruction tuning with graph-structured textual instructions (text-graph grounding) and step-by-step reasoning prompts so that LLMs can process graph tasks; the paper references GraphGPT's paradigm but does not reproduce its exact textualization schemeâ€”GraphGPT checkpoint was used in experiments for comparison.",
            "graph_type": "Attributed graphs / heterogeneous graphs / knowledge graphs",
            "representation_properties": "Instruction-tuning oriented; integrates structural cues into prompts; designed for better graph-aware reasoning in LLMs; may still rely on dataset-specific instructions.",
            "evaluation_task": "Evaluated or compared on node classification, link prediction and graph-to-text tasks in this paper (GraphGPT checkpoint used in experiments).",
            "performance_metrics": "From Tables in this paper: Node classification IMDB Micro-F1 44.52 Macro-F1 43.70; Freebase Micro-F1 15.80 Macro-F1 6.14; Cora Micro-F1 24.57 Macro-F1 15.26; Arxiv Micro-F1 31.08 Macro-F1 17.12. Link prediction Yelp AUC 48.24, MIMIC-III AUC 52.01. (Reported as baseline numbers in Tables V and VI.)",
            "comparison_to_other_representations": "GraphGPT provides instruction-tuning for graphs but in the reported evaluations it underperforms MuseGraph; authors note many LLM-based methods (including GraphGPT) either depend on task/dataset-specific GNN encoders or lack the compact, token-aware textualization strategy employed by MuseGraph.",
            "limitations_or_challenges": "Performance can degrade when models are not fine-tuned with token-efficient compact textualizations; reliance on specific instruction sets may limit cross-dataset generalization; reported baseline performance is lower than MuseGraph in this paper's experimental suite.",
            "uuid": "e5379.4",
            "source_info": {
                "paper_title": "Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "HiGPT",
            "name_full": "HiGPT: Heterogeneous Graph Language Model",
            "brief_description": "An LLM-based model that aligns LLMs with heterogeneous graph knowledge via instruction tuning, a specialized graph tokenizer, and mixture-of-thought augmentation to better handle graph-structured inputs.",
            "citation_title": "Higpt: Heterogeneous graph language model",
            "mention_or_use": "use",
            "representation_name": "Heterogeneous-graph-aware tokenization + instruction tuning (HiGPT)",
            "representation_description": "Aligns LLMs with heterogeneous graph knowledge by employing a specialized graph tokenizer (to encode graph tokens), instruction tuning, and mixture-of-thought augmentation; converts graph elements into tokens and prompts tailored for heterogeneous graphs. The current paper used a HiGPT checkpoint (Vicuna-7B-v1.5 tuned with IMDB instructions) for experimental comparison.",
            "graph_type": "Heterogeneous graphs / attributed graphs",
            "representation_properties": "Designed for heterogeneity and mixture-of-thought reasoning; includes learned graph tokens (tokenizer) which can improve performance on datasets seen during tuning (e.g., IMDB) but may generalize poorly to out-of-domain or homogeneous graphs without retraining.",
            "evaluation_task": "Benchmarked here on node classification, link prediction and graph-to-text tasks; checkpoint used for experiments.",
            "performance_metrics": "From Tables: Node classification IMDB Micro-F1 56.72 Macro-F1 53.77; Freebase Micro-F1 16.17 Macro-F1 7.02; Cora Micro-F1 23.86 Macro-F1 14.17; Arxiv Micro-F1 10.76 Macro-F1 6.25. Link prediction Yelp AUC 49.73, MIMIC-III AUC 51.04. Graph-to-text few-shot AGENDA/WebNLG reported in Table VII (HiGPT shown with limited entries e.g., AGENDA 8.49 in some settings).",
            "comparison_to_other_representations": "HiGPT excels on datasets similar to its training data (e.g., IMDB) but shows substantial drops on other datasets (e.g., Cora), indicating less cross-dataset adaptability than MuseGraph's compact description plus instruction tuning.",
            "limitations_or_challenges": "Learned graph tokens and tuning on a specific dataset can hinder generalization; struggles with tasks like link prediction in some settings (producing incorrect 'Yes' answers) and with homogeneous datasets when trained on heterogeneous examples.",
            "uuid": "e5379.5",
            "source_info": {
                "paper_title": "Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "InstructGLM",
            "name_full": "Language is all a graph needs (InstructGLM)",
            "brief_description": "An approach that tunes LLMs with scalable graph prompts combining natural language instructions and node features (often from trained GNNs) to perform graph tasks.",
            "citation_title": "Language is all a graph needs",
            "mention_or_use": "use",
            "representation_name": "Scalable graph prompts combining NL instructions and node features (InstructGLM)",
            "representation_description": "Tunes LLMs using scalable natural-language graph prompts that can include node features (sometimes derived from trained GNNs) and instructions; effectively converts graph information into prompted natural language for LLM consumption. The authors used a public InstructGLM checkpoint for comparison.",
            "graph_type": "Attributed graphs / knowledge graphs",
            "representation_properties": "Scalable prompt design; can incorporate encoded node features; aims to be natural-language-native but performance depends on quality and scope of included node features and prompts.",
            "evaluation_task": "Compared on node classification, link prediction and graph-to-text benchmarks in this paper.",
            "performance_metrics": "Reported numbers in Tables: Node classification IMDB Micro-F1 13.59 Macro-F1 9.41; Freebase Micro-F1 5.65 Macro-F1 2.38; Cora Micro-F1 15.62 Macro-F1 10.88; Arxiv Micro-F1 42.60 Macro-F1 28.43. Link prediction Yelp AUC 49.38, MIMIC-III AUC 34.35. Graph-to-Text: some entries in Table VII (limited).",
            "comparison_to_other_representations": "InstructGLM provides a natural-language prompting scheme but in these experiments it underperformed MuseGraph and many baselines; it sometimes benefits from integrating GNN-derived node features but remains sensitive to prompt and feature design.",
            "limitations_or_challenges": "Performance variability across datasets; may require node features from trained GNNs to be effective; suffers from lower adaptability in some tasks versus MuseGraph's instruction-tuned LLM pipeline.",
            "uuid": "e5379.6",
            "source_info": {
                "paper_title": "Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LLM-GNN",
            "name_full": "LLM-GNN: Label-free node classification on graphs with large language models",
            "brief_description": "A method that uses LLMs as annotators to generate pseudo labels or textual annotations that are used to train GNNs, thereby converting graph textual/contextual information into training signals for graph models.",
            "citation_title": "Label-free node classification on graphs with large language models (llms)",
            "mention_or_use": "mention",
            "representation_name": "LLM-generated annotations / textual labeling for GNN training",
            "representation_description": "Uses an LLM to annotate nodes (generate pseudo labels or text-based annotations) and then trains GNN predictors on those labels/features; the conversion is via LLM-generated text that becomes supervisory or feature information for GNNs rather than direct LLM prompt inputs.",
            "graph_type": "Attributed graphs / text-attributed graphs",
            "representation_properties": "Leverages LLMs to create labels/features without human labeling; can improve GNN training when high-quality LLM annotations are available; sensitive to annotation quality and LLM hallucination.",
            "evaluation_task": "Node classification (Cora, Arxiv used public features from LLM-GNN in comparisons).",
            "performance_metrics": "In this paper, LLM-GNN reported for Cora Micro-F1 75.61 Macro-F1 73.95 and Arxiv Micro-F1 66.05 Macro-F1 45.33 (Table V). Other datasets marked '-' where public features were not used.",
            "comparison_to_other_representations": "LLM-GNN can yield strong node-classification performance on datasets where LLM annotations are good, but it is a two-stage approach (LLM -&gt; GNN) and may have higher costs and limited crosstask generality compared to MuseGraph's unified LLM instruction-tuning approach.",
            "limitations_or_challenges": "Relies on high-quality LLM annotations (costly if using API); not directly an LLM-only graph-&gt;text representation for generation tasks; limited public features availability for comparison across datasets.",
            "uuid": "e5379.7",
            "source_info": {
                "paper_title": "Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "One for all: Towards training one graph model for all classification tasks",
            "rating": 2
        },
        {
            "paper_title": "All in one: Multi-task prompting for graph neural networks",
            "rating": 2
        },
        {
            "paper_title": "Harnessing explanations: Llm-to-lm interpreter for enhanced textattributed graph representation learning",
            "rating": 2
        },
        {
            "paper_title": "Graphgpt: Graph instruction tuning for large language models",
            "rating": 2
        },
        {
            "paper_title": "Higpt: Heterogeneous graph language model",
            "rating": 2
        },
        {
            "paper_title": "Language is all a graph needs",
            "rating": 2
        },
        {
            "paper_title": "Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking",
            "rating": 1
        },
        {
            "paper_title": "Label-free node classification on graphs with large language models (llms)",
            "rating": 1
        },
        {
            "paper_title": "Prodigy: Enabling in-context learning over graphs",
            "rating": 1
        }
    ],
    "cost": 0.021041999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining</h1>
<p>Yanchao Tan Member, IEEE, Hang Lv, Pengxiang Zhan, Shiping Wang Senior Member, IEEE, Carl Yang Member, IEEE</p>
<h4>Abstract</h4>
<p>Graphs with abundant attributes are essential in modeling interconnected entities and enhancing predictions across various real-world applications. Traditional Graph Neural Networks (GNNs) often require re-training for different graph tasks and datasets. Although the emergence of Large Language Models (LLMs) has introduced new paradigms in natural language processing, their potential for generic graph mining-training a single model to simultaneously handle diverse tasks and datasets-remains under-explored. To this end, our novel framework MuseGraph, seamlessly integrates the strengths of GNNs and LLMs into one foundation model for graph mining across tasks and datasets. This framework first features a compact graph description to encapsulate key graph information within language token limitations. Then, we propose a diverse instruction generation mechanism with Chain-of-Thought (CoT)based instruction packages to distill the reasoning capabilities from advanced LLMs like GPT-4. Finally, we design a graphaware instruction tuning strategy to facilitate mutual enhancement across multiple tasks and datasets while preventing catastrophic forgetting of LLMs' generative abilities. Our experimental results demonstrate significant improvements in five graph tasks and ten datasets, showcasing the potential of our MuseGraph in enhancing the accuracy of graph-oriented downstream tasks while improving the generation abilities of LLMs.</p>
<p>Index Terms-Generic Graph Mining, Large Language Models, Instruction Tuning</p>
<h2>I. INTRODUCTION</h2>
<p>GRAPHS with plentiful attributes are widely used to model interconnected real-world entities, and they are pivotal for improving downstream predictions across diverse real-world applications. Recently, Graph Neural Networks (GNNs) have been commonly adopted for modeling attributed graphs [1], [2]. However, they are usually trained on specific tasks and datasets and need to be re-trained whenever applied to different ones. Inspired by the great success of Large Language Models (LLMs), the combination of GNNs and LLMs aims to enhance the processing of text-attributed graphs, which can improve the model's capabilities across various tasks and datasets. Existing studies can be categorized into two main approaches. The first category tries to train GNNs with LLM-enhanced features (e.g., LLM-GNN [3], TAPE [4], OFA [5], ALL-in-One [6],</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. An illustrative toy example of the need for a generic graph model that can be directly applied to various graph-related tasks and datasets.
and GHGRL [7]). The second category explores LLMs for various graph applications (e.g., GPT4Graph [8], GraphGPT [9], HiGPT [10], NLGraph [11], and InstructGLM [12]).</p>
<p>Despite the promising direction of integrating LLMs with GNNs, the full exploration of LLMs' power and generation capabilities for graphs has yet to be under-explored [5], [6], [13], where one powerful graph model can be trained on multiple tasks/datasets and generalize well to more tasks/datasets. Fig. 1 demonstrates the necessity for a generic graph framework, showcasing a wide range of task and dataset combinations. Such a generic graph model can capture the semantic and structural information not only for various graph tasks (e.g., node classification, link prediction, and graph-to-text) but also for diverse graph datasets (e.g., from IMDB's movie data to AGENDA's academic texts). This enables one single foundation model to understand graph data by generating contextually rich textual interpretations that are central to LLMs. However, three key challenges hinder the achievement of this goal.
Challenge I: How to extract informative graph descriptions under the limitation of language tokens? To harness the full potential of LLMs for generic graph mining, an essential obstacle is translating the graph with abundant semantics and complex structures into a format that LLMs can process effectively, especially under strict language token limitations. Without a compact graph description that accurately encapsulates key information from graphs within the LLMs' token limitations, the model's capabilities to grasp and utilize the graph's semantic and structural richness is severely limited, potentially leading to suboptimal performance in graph applications.
Challenge II: How to automatically generate diverse instructions? Creating a diverse set of high-quality instructions for finetuning LLMs is fundamental for generic graph tasks. However, these instructions are often difficult for LLMs to comprehend when encountering unfamiliar graph structures or new tasks, and are costly to produce manually. While advanced LLMs like GPT-4 possess the capabilities to understand and reason</p>
<p>diverse instructions, it remains unknown how to effectively and efficiently leverage the reasoning abilities of advanced LLMs to produce relevant and task-specific instructions.
Challenge III: How to properly allocate instructions for graphoriented instruction tuning? The effectiveness of instruction tuning for LLMs largely depends on how the instructions are structured, enabling LLMs to accurately understand and execute graph-related tasks. Balancing a variety of tasks and datasets presents a significant challenge in facilitating mutual enhancement across these graph applications while preventing catastrophic forgetting of LLMs' generative abilities.</p>
<p>To tackle these challenges, we propose Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining (MuseGraph), which consists of three pivotal steps: (i) Development of Compact Graph Descriptions, where we introduce a novel "node energy" metric to textualize graphs with essential semantic and structural details under limited language tokens; (ii) Generation of Diverse Instructions, which distills the reasoning abilities of advanced LLMs like GPT-4 to create Chain-of-Thought (CoT)-based instruction packages tailored for various graph tasks, thus enriching LLMs' capabilities in understanding and analyzing graph data without the expense of manual instruction crafting; (iii) Graph-aware Instruction Tuning, which introduces a dynamic instruction package allocation strategy based on the specific needs of each graph task, ensuring comprehensive and effective LLM tuning.</p>
<p>Our overall contributions are summarized as follows:</p>
<ul>
<li>Formulation of generic graph mining. We establish a generic graph framework that effectively and efficiently transforms graph semantics and structures into LLM-friendly formats while enhancing the generation capabilities necessary for diverse graph-related tasks.</li>
<li>Effective model designs. We design and implement a set of models and mechanisms, including the development of compact graph descriptions, automatically generating diverse task-specific Chain-of-Thought (CoT)-based instruction packages, and graph-aware instruction tuning, targeting a unified graph model across tasks and datasets.</li>
<li>Extensive experiments across graph tasks and datasets. We conduct thorough experiments to validate our approach with five tasks and ten datasets, demonstrating its superiority over existing state-of-the-art methods and highlighting its effective, generative, and interpretable abilities in enhancing generic graph mining.</li>
</ul>
<h2>II. Related Work</h2>
<h2>A. Semantic-rich Graph Representation Learning</h2>
<p>Graph representation learning has emerged as a key technique for the complex structures of networks with abundant attributes [2], [14]-[16]. Many existing node embedding approaches have explored and harnessed the significant potential of Random Walks (RWs) in preserving the graph topological structures [17]-[20]. However, these approaches overlook the rich attribute information of nodes and edges on the graph [16]. Recently, Graph Neural Networks (GNNs) learn node representations through aggregating content information from neighbor nodes while preserving the surrounding structure</p>
<p>TABLE I
A COMPARISON BETWEEN MUSEGRAPH AND RELATED METHODS, WHERE "GNN" AND "LLM" REFER TO METHODS THAT TRAIN GNNs AND LLMs AS PREDICTORS, RESPECTIVELY.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GNN</th>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Cross-Task</th>
<th style="text-align: center;">Cross-Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLM-GNN [3]</td>
<td style="text-align: center;">$\bullet$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
</tr>
<tr>
<td style="text-align: left;">TAPE [4]</td>
<td style="text-align: center;">$\bullet$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
</tr>
<tr>
<td style="text-align: left;">OFA [5]</td>
<td style="text-align: center;">$\bullet$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bullet$</td>
<td style="text-align: center;">$\bullet$</td>
</tr>
<tr>
<td style="text-align: left;">All-in-One [6]</td>
<td style="text-align: center;">$\bullet$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bullet$</td>
<td style="text-align: center;">$\bullet$</td>
</tr>
<tr>
<td style="text-align: left;">GHGRL [7]</td>
<td style="text-align: center;">$\bullet$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4Graph [8]</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
</tr>
<tr>
<td style="text-align: left;">GraphGPT [9]</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bullet$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bullet$</td>
</tr>
<tr>
<td style="text-align: left;">HiGPT [10]</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bullet$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bullet$</td>
</tr>
<tr>
<td style="text-align: left;">NLGraph [11]</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
</tr>
<tr>
<td style="text-align: left;">InstructGLM [12]</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bullet$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
</tr>
<tr>
<td style="text-align: left;">MuseGraph</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bullet$</td>
<td style="text-align: center;">$\bullet$</td>
<td style="text-align: center;">$\bullet$</td>
</tr>
</tbody>
</table>
<p>of graphs [1], [21], [22]. However, most current GNNs are trained within a supervised learning setting, which demands a large amount of task-specific labeled data and may not always be available in real-world scenarios [23], [24]. Moreover, the learned embeddings often lack adaptability across different downstream tasks [20], which have to be re-trained whenever applied to various graph applications.</p>
<p>Despite the efforts to reduce reliance on labeled data through pre-training expressive GNNs through self-supervised methods (e.g., contrastive learning [23], [25], [26]), their effectiveness in specific downstream tasks still significantly depends on the appropriate choice of suitable self-supervision tasks and attribute encoders [27], [28]. Therefore, there is still a lack of a uniform framework for generic graph mining across different tasks and datasets.</p>
<h2>B. Leveraging LLMs for Graph Mining</h2>
<p>Inspired by the remarkable advancements in Large Language Models (LLMs), integrating Graph Neural Networks (GNNs) with LLMs is creating substantial progress in handling complex text-attributed graphs [3], [29]. Existing approaches that leverage this integration can be broadly divided into the following two categories (shown in Table I).</p>
<p>The first category is training GNNs as predictors, augmented with LLM-enhanced features, labeled "GNN" in Table I. For example, the model LLM-GNN [3] used LLMs as annotators to generate the pseudo labels of nodes used to train GNNs. TAPE [4] prompted LLMs to obtain the explanations of predictions to enhance node initial features. PRODIGY [13] utilized LLMs to encode the textual information associated with nodes on the graph. OFA [5] described all nodes and edges using human-understandable prompt texts and converted them into embedded features by LLMs. All-in-One [6] reformulated different graph tasks with the unified graph prompts and language prompts. GHGRL [7] employed LLMs to automatically summarize and classify various data formats and types, aligning node features. These approaches primarily leveraged LLMs to process textual content within graphs and are often optimized for specific domains. However, they generally struggle with cross-task or cross-dataset applications, limiting the generation capabilities of LLMs across various graph tasks and datasets.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. The overall of MuseGraph, which consists of Compact Graph Description, Diverse Instruction Generation, and Graph-aware Instruction Tuning.</p>
<p>The second category, denoted as "LLM" in Table I, mainly focuses on utilizing LLMs to directly perform graph tasks. For example, GPT4Graph [8] and NLGraph [11] proposed graph-based benchmarks, evaluating the understanding and analytical capabilities of LLMs on graph data with designed natural language prompts. Notably, although they do not finetune LLMs on graph structures, their benchmarks effectively uncover current limitations and future directions for improving LLMs' capabilities in graph comprehension and reasoning. Both GraphGPT [9] and HiGPT [10] effectively combined task-specific GNNs with one projector on top of LLMs, enhancing performance across different datasets. InstructGLM [12] tuned LLMs with scalable graph prompts based on natural language instructions, where it can integrate node features from trained GNNs to improve the accuracy of node classification tasks. Compared with GNN-based methods, LLM-based methods have the potential to perform generic graph mining with powerful cross-task and cross-dataset abilities. However, such potential remains largely under-explored, as many existing LLM-based approaches still rely on node representations generated by GNNs tailored to specific tasks/datasets. According to the characteristics of different models (shown in Table I), there is a pressing need for a generic graph framework that can accurately understand graph data while enhancing generation capabilities across diverse tasks and datasets.</p>
<h1>III. THE MUSEGRAPH FRAMEWORK</h1>
<h3>A. Overview of Our Framework</h3>
<p><strong>Objective:</strong> In this paper, we aim to develop a unified framework that seamlessly integrates the strengths of Graph Neural Networks (GNNs) and Large Language Models (LLMs) into one foundation model via graph-oriented instruction tuning of LLMs. Through this, we seek to enable a more effective and generic approach for graph mining across various downstream tasks and datasets.</p>
<p><strong>Overview:</strong> To achieve this goal, we propose MuseGraph framework, which comprises three major components. Firstly, we develop a compact graph description mechanism that captures critical semantic and structural details within the constraints of language token limitations. Secondly, we generate a diverse range of instructions via the reasoning capabilities of advanced LLMs like GPT-4, thus facilitating task-specific Chain-of-Thought (CoT)-based instruction packages for graph tasks. Thirdly, we adopt a graph-aware instruction tuning, utilizing a dynamic allocation strategy for instruction packages tailored to the unique requirements of each graph task and dataset. The overall model architecture is shown in Fig. 2, and we elaborate on the three main components.</p>
<h3>B. Compact Graph Description</h3>
<p>Leveraging the capabilities of LLMs for graphs presents a unique set of challenges, primarily due to LLMs' inherent limitations in directly processing textual interpretations within language token constraints. Therefore, it is non-trivial to automatically encapsulate key information from graphs under the token limitations, which requires a compact description including complex node and edge attributes along with structural details to accurately describe the graph.</p>
<p>Inspired by common graph analysis techniques such as neighbors and walks, we propose a novel method of textualization to describe graphs via these concepts. In this way, neighbors are helpful to understand local connectivity and feature distribution [1], [21], [22], providing a granular view of node attributes; while walks offer a dynamic method to explore the graph's structure and the high-order relationships between nodes, highlighting the diversity of connectivity and paths [19], [20]. The integration of neighbors and walks can</p>
<table>
<thead>
<tr>
<th>Algorithm 1: Procedure for generating the compact</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>graph description tailored to a specified node.</td>
<td></td>
</tr>
<tr>
<td>Input: Attributed graph $\mathcal{G}$ with $N$ nodes, token count</td>
<td></td>
</tr>
<tr>
<td>set $\mathcal{T}$, node energy set $\mathcal{H}$, target node $v^{*}$, token limitation</td>
<td></td>
</tr>
<tr>
<td>$L\left(v^{*}\right)$</td>
<td></td>
</tr>
<tr>
<td>Output: Key neighbor set $\mathcal{N}\left(v^{<em>}\right)$, key walk set $\mathcal{W}\left(v^{</em>}\right)$</td>
<td></td>
</tr>
<tr>
<td>1: Initialize $\mathcal{N}\left(v^{<em>}\right)$, $\mathcal{W}\left(v^{</em>}\right)$ as empty sets;</td>
<td></td>
</tr>
<tr>
<td>2: Select $v_{i} \in G\left(v^{<em>}\right)$ with $H\left(v_{i}\right) \geq H\left(v^{</em>}\right)$ and</td>
<td></td>
</tr>
<tr>
<td>$L\left(v^{<em>}\right) \geq T\left(v_{i}\right)$ constraints for $\mathcal{N}\left(v^{</em>}\right)$, where $G\left(v^{*}\right)$</td>
<td></td>
</tr>
<tr>
<td>is $v^{*}$ 's one-hop neighbors and $H(\cdot)$ can be</td>
<td></td>
</tr>
<tr>
<td>calculated according to Eq. 1;</td>
<td></td>
</tr>
<tr>
<td>3: Expand $\mathcal{W}\left(v^{<em>}\right)$ starting from $v^{</em>}$ based on $\mathcal{G}$ within</td>
<td></td>
</tr>
<tr>
<td>$L\left(v^{<em>}\right)$ and $H\left(v^{</em>}\right)$ constraints.</td>
<td></td>
</tr>
</tbody>
</table>
<p>achieve a holistic understanding of graph structures. Notably, this textualization method integrates GNN-inspired structural priors (i.e., the core principles of message passing and highorder propagation) into the graph description design, without requiring training specific GNNs. This enables more powerful and adaptive capabilities across diverse tasks and datasets.</p>
<p>Given LLM token limitations and the varied contributions of neighbors and walks to node understanding, we further develop an adaptive generation procedure to ensure the compactness of the description. Specifically, we first design a "node energy" metric $H(\cdot)$, assessing node information from two perspectives: token count in node attributes and node degree count. This metric enables us to effectively filter and select neighbor nodes and walk nodes, prioritizing those that are abundant in semantic information and possess a significant number of neighbor nodes, thus enhancing the expressiveness of the graph description. The calculation of node energy $H\left(v^{*}\right)$ is formulated as follows:</p>
<p>$$ H\left(v^{<em>}\right)=T\left(v^{</em>}\right) \times\left\lceil\log \left(D\left(v^{*}\right)+1\right)\right\rceil, $$</p>
<p>where $v^{<em>}$ is the target node, $T\left(v^{</em>}\right)$ is the number of node tokens processed by a language tokenizer, $D\left(v^{*}\right)$ is the number of node degrees, and $\lceil\cdot\rceil$ is the ceiling operator.</p>
<p>Based on $v^{<em>}$, our method strategically incorporates neighbors and walks, as depicted in Algorithm 1. A neighbor $v_{i}$ is chosen to describe the target $v^{</em>}$ if its $H\left(v_{i}\right)$ surpasses $H\left(v^{<em>}\right)$, ensuring the included node can provide supplemental information. The process of adding walks concludes when encountering a node whose $H\left(v_{i}\right)$ does not meet the threshold set by the target's $H\left(v^{</em>}\right)$, thus refining the input to maximize related graph information within the constraints of the token limit. To further demonstrate the adaptive input generation for neighbors and walks tailored to each node, we present an example in Fig. 3. Node $v_{1}$ 's with a relatively low $H\left(v_{1}\right)$ value includes a wide range of neighbors to capture more context, excluding $v_{4}$ due to its even lower $H\left(v_{4}\right)$. Given the token limitation $L\left(v_{1}\right)$, only two walks are sampled for $v_{1}$ to maintain a compact description. Conversely, $v_{2}$ with its higher $H\left(v_{2}\right)$, inherently carries more information, prompting the selection of fewer neighbors. $v_{6}$ is excluded since $H\left(v_{6}\right)&lt;H\left(v_{2}\right)$. This frees up tokens to detail more walks for $v_{2}$. The $H$ metric thus effectively balances neighbor and walk inclusion for each node, marrying information-rich and token-efficient characteristics.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Two examples on IMDB illustrate how to extract the key information of nodes via neighbors and walks based on node energy $H\left(v^{<em>}\right)$ and the language token limitation $L\left(v^{</em>}\right)$.</p>
<p>The process culminates in the textualization of each node's key information, producing a tailored and compact graph description as depicted in the upper right of Fig. 2. Note that, a graph-related task can involve multiple nodes (such as link prediction). In this case, we adaptively allocate the token limit requirement of each node involved by calculating the ratio of all node energies using the softmax function. Additionally, we provide the ablation in Section IV-C2 to further verify the effectiveness of the compact graph description.
Remark. Given an attributed graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ and an LLM token limitation $L$, the expected utility of including a node $v \in \mathcal{V}$ in the compact graph description can be approximated by the product of its semantic complexity $T(v)$ and structural centrality $D(v)$ as follows:</p>
<p>$$
\begin{gathered}
\mathbb{E}[\phi(v)] \propto T(v) \times\lceil\log (D(v)+1)\rceil=H(v) \
\text { s.t. } \sum_{v \in G} T(v) \leq L
\end{gathered}
$$</p>
<p>where $\phi(v)$ denotes the information gain of including a node $v$. Under the LLM token constraint $\sum_{v \in G} T(v) \leq L$, prioritizing nodes by descending $H(v)$ value leads to a compact yet informative subgraph $G$, which captures both semantic richness and structural connectivity of the original graph $\mathcal{G}$. Notably, this formulation models node selection as a constrained expected utility maximization problem [30], where $H(v)$ acts as a practical proxy for the expected informativeness per node. Empirically, longer token spans tend to encode richer semantics [31], [32], while high-degree nodes often serve as key hubs that support global connectivity [33]-[36]. Therefore, the node energy $H(v)$ naturally balances token cost with information gain, making it a suitable metric for adaptive neighbor and walk selection within limited LLM tokens.</p>
<h2>C. Diverse Instruction Generation</h2>
<p>With the compact graph descriptions tailored to each node, another critical step in fine-tuning LLMs for generic graph</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. A process showing how to conduct the Chain-of-Thought (CoT)-based instruction package for node classification. We leverage the reasoning ability distilled from advanced LLMs (e.g., GPT-4) and integrate them with task-specific instructions via a 1:10 mix ratio.</p>
<p>mining is crafting diverse and high-quality instructions [37], [38]. Benefiting from the key graph information captured by our designed compact graph descriptions, we can accurately and efficiently construct abundant graph-related task instructions (e.g., node classification, link prediction, and graph-to-text). While these instructions enable LLMs to effectively grasp compact graph descriptions, they may not always be easily comprehended when encountering unfamiliar graph structures or new tasks, potentially leading to inaccuracies and limited reasoning across various datasets. Moreover, manual construction of these instructions is often highly costly in terms of both time and resources.</p>
<p>To address these problems, we propose to distill the reasoning capabilities from advanced LLMs [39] (e.g., GPT-4 [40] with over 200 billion parameters) for graph-related tasks. Our approach, inspired by the Chain-of-Thought (CoT) processing methodology [41], prompts GPT-4 via a flexible template based on our compact graph descriptions for different tasks and then constructs task-specific CoT-based instruction packages. Different from the existing methods that leverage CoT in the prompting stage [41], [42], we directly construct CoT-based instruction packages, harnessing their diversity and reasoning abilities to facilitate instruction tuning with graph data.</p>
<p>Specifically, as shown in Fig. 4, we first design diverse task-specific instructions based on compact graph descriptions, such as node classification, link prediction, and graph-to-text. Then, we prompt GPT-4 with a small number of task-specific instructions to generate the corresponding step-by-step reasoning, which is used to conduct CoT-based instructions with the initial instructions across diverse tasks. This approach distills GPT-4's vast knowledge base to augment the reasoning and analytical abilities of our MuseGraph, enhancing the understanding of compact graphs.</p>
<p>To optimize the cost-effectiveness of querying GPT-4, we introduce the CoT-based instruction package. For every set of 1,000 standard task-specific instructions, we integrate 100 CoT-based instructions tailored to the same graph task. This approach not only proves to be economical but also broadens the diversity and flexibility of instructions, accommodating a range of graph-related tasks.</p>
<h3><em>D. Graph-aware Instruction Tuning</em></h3>
<p>With the proposed compact graph descriptions and diverse instruction generation mechanisms, identifying an effective instruction tuning method for LLMs remains crucial. Such tuning enables LLMs to understand and perform well a wide range of graph mining tasks across multiple datasets and contexts [38], [43]. However, a persistent challenge is the catastrophic forgetting issue commonly faced by LLMs [44], [45]. This phenomenon complicates the capabilities to maintain extensive task and dataset coverage without compromising previously acquired knowledge, necessitating strategies that balance new learning with memory retention.</p>
<p>To this end, we propose a dynamic instruction package allocation strategy to adaptively adjust the volume of task-specific CoT-based instruction packages based on the complexities of tasks and datasets, which ensures that more complex tasks/datasets receive a proportionally larger set of instructions for detailed guidance. The calculation process includes two aspects as follows:</p>
<ul>
<li><strong>For task complexity</strong>: We assess the complexity of tasks by calculating the average number of answer tokens for each task, which helps in tailoring the CoT-based instruction packages to specific needs within a dataset. A larger average number of answer tokens indicates a higher level of reasoning and response complexity, enabling us to proportionally allocate more instructions to such tasks.</li>
<li><strong>For dataset complexity</strong>: We calculate the total node energy ( H(\cdot) ) (cf., Eq. 1) for each graph data, utilizing this metric to optimize the instruction distribution. A higher overall node energy reflects richer semantic and structural characteristics within a dataset, guiding the allocation of more instructions to these information-dense graphs.</li>
</ul>
<p>By performing this dynamic allocation of instructions, we enhance LLMs' abilities to learn and retain extensive knowledge across a diverse range of graph mining challenges. We provide the ablation in Section IV-C4 to further assess the effectiveness of the dynamic instruction package allocation strategy.</p>
<p>To balance the effectiveness with computational efficiency, we adopt a graph-aware instruction tuning mechanism, which can sufficiently utilize diverse and high-quality instructions for fine-tuning LLMs [37]. Specifically, we adopt a general LLM LLaMA3-8B [46] with LoRA [47] as our initial fine-tuning point by default. Then, based on the diverse instruction package set ( I = {I_1, I_2, \ldots, I_D} ) as model input, we adopt the negative log-likelihood loss as the fine-tuning objective as follows:</p>
<p>$$p_{\theta} \left( Y_{j,k} \right| I_j, Y_{j, &lt;k}) = LLM_{\theta} \left( I_j, Y_{j, &lt;k} \right), \tag{3}$$</p>
<p>$$\mathcal{L}<em k="1">{\theta} = -\sum</em>$$}^{|Y_j|} \log p_{\theta} \left( Y_{j,k} \right| I_j, Y_{j, &lt;k}), \tag{4</p>
<p>where ( \theta ) is the learnable parameters of our proposed graph-aware LLM (i.e., MuseGraph), ( I_j \in \mathcal{I} ) is the input of LLM, and ( Y_j ) is the output of LLM.</p>
<p>After obtaining the fine-tuned LLM for generic graph mining, we can apply it to various downstream tasks, such as node classification, link prediction, and graph-to-text. Notably, our MuseGraph can achieve superior performance across diverse</p>
<p>TABLE II
STATISTICS OF THE DATASETS FOR NODE CLASSIFICATION.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">IMDB</th>
<th style="text-align: center;">Freebase</th>
<th style="text-align: center;">Cora</th>
<th style="text-align: center;">Arxiv</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"># Nodes</td>
<td style="text-align: center;">21,420</td>
<td style="text-align: center;">180,098</td>
<td style="text-align: center;">2,708</td>
<td style="text-align: center;">169,343</td>
</tr>
<tr>
<td style="text-align: center;"># Edges</td>
<td style="text-align: center;">86,642</td>
<td style="text-align: center;">1,057,688</td>
<td style="text-align: center;">5,429</td>
<td style="text-align: center;">1,166,243</td>
</tr>
<tr>
<td style="text-align: center;"># Labeled nodes</td>
<td style="text-align: center;">4,573</td>
<td style="text-align: center;">7,954</td>
<td style="text-align: center;">2,708</td>
<td style="text-align: center;">169,343</td>
</tr>
<tr>
<td style="text-align: center;"># Classes</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">40</td>
</tr>
</tbody>
</table>
<p>TABLE III
STATISTICS OF THE DATASETS FOR LINK PREDICTION.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"># Nodes</th>
<th style="text-align: center;"># Edges</th>
<th style="text-align: center;"># Link type</th>
<th style="text-align: center;"># Node type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Yelp</td>
<td style="text-align: center;">82,465</td>
<td style="text-align: center;">30,542,675</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">MIMIC-III</td>
<td style="text-align: center;">32,267</td>
<td style="text-align: center;">559,290</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>TABLE IV
STATISTICS OF THE DATASETS FOR GRAPH-TO-TEXT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"># Graph</th>
<th style="text-align: center;"># Rel</th>
<th style="text-align: center;">Avg.#</th>
<th style="text-align: center;">Avg.#</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nodes</td>
<td style="text-align: center;">Triples</td>
<td style="text-align: center;">Length</td>
</tr>
<tr>
<td style="text-align: center;">AGENDA</td>
<td style="text-align: center;">40,720</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">12.37</td>
<td style="text-align: center;">4.48</td>
<td style="text-align: center;">140.36</td>
</tr>
<tr>
<td style="text-align: center;">WebNLG</td>
<td style="text-align: center;">8,783</td>
<td style="text-align: center;">246</td>
<td style="text-align: center;">5.91</td>
<td style="text-align: center;">2.95</td>
<td style="text-align: center;">13.02</td>
</tr>
</tbody>
</table>
<p>tasks and datasets with few instruction packages based on the graph-aware instruction tuning mechanism, even in the few-shot and zero-shot settings (shown in Table VII and Fig. 6).
Model Extension. Our proposed MuseGraph establishes a generic approach for integrating LLMs with graph data. We can flexibly incorporate a variety of foundation LLMs, as discussed in Section IV-C1, which enhances its generation capabilities across different graph-related tasks. Building on this flexibility, we can further replace our adopted LoRA [47] with different parameter-efficient training approaches, such as QLoRA [48], AdaLoRA [49], and FourierFT [50]. These methods can be helpful to reduce the computational demands while maintaining high performance. Additionally, we can explore the incorporation of Reinforcement Learning from Human Feedback (RLHF) [37] to further enhance the learning process, adjusting model behaviors and improving decisionmaking in complex graph scenarios.
Datasets Extension. Leveraging the proposed compact graph description and diverse instruction generation mechanisms, we can effectively include a wider range of attributed graphs via our graph-aware instruction tuning, enriching the diversity of applications across various domains. Since our MuseGraph introduces a generic graph framework to simultaneously capture the semantic and structural information of attributed graphs across tasks and datasets, it can be applied for tasks across diverse datasets, including biological networks, social networks, and knowledge graphs. Furthermore, the zero-shot generalization capabilities of our MuseGraph, as evaluated via graph-informed DyVal (demonstrated in Fig. 6 in Section IV-B), highlight its potential to perform accurately on more datasets where it was not explicitly trained. This ability to generalize well in zero-shot scenarios underscores the practical utility of MuseGraph in navigating and analyzing unexplored graphs.</p>
<h2>IV. EXPERIMENT</h2>
<p>In this section, we conduct extensive experiments to validate the effectiveness and efficiency of our proposed MuseGraph under diverse conditions, aiming to answer the following three key research questions:</p>
<ul>
<li>RQ1: How does our MuseGraph framework perform in comparison to the representative graph-oriented methods for generic graph mining?</li>
<li>RQ2: What are the effects of different model components?</li>
<li>RQ3: Can the compact graph description help the model better understand the graph structure?</li>
</ul>
<h2>A. Experimental Setup</h2>
<p>1) Datasets: To comprehensively evaluate the effectiveness and efficiency of our MuseGraph, we utilize two real-world</p>
<p>TABLE III
STATISTICS OF THE DATASETS FOR LINK PREDICTION.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"># Nodes</th>
<th style="text-align: center;"># Edges</th>
<th style="text-align: center;"># Link type</th>
<th style="text-align: center;"># Node type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Yelp</td>
<td style="text-align: center;">82,465</td>
<td style="text-align: center;">30,542,675</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">MIMIC-III</td>
<td style="text-align: center;">32,267</td>
<td style="text-align: center;">559,290</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>TABLE IV
STATISTICS OF THE DATASETS FOR GRAPH-TO-TEXT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"># Graph</th>
<th style="text-align: center;"># Rel</th>
<th style="text-align: center;">Avg.#</th>
<th style="text-align: center;">Avg.#</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nodes</td>
<td style="text-align: center;">Triples</td>
<td style="text-align: center;">Length</td>
</tr>
<tr>
<td style="text-align: center;">AGENDA</td>
<td style="text-align: center;">40,720</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">12.37</td>
<td style="text-align: center;">4.48</td>
<td style="text-align: center;">140.36</td>
</tr>
<tr>
<td style="text-align: center;">WebNLG</td>
<td style="text-align: center;">8,783</td>
<td style="text-align: center;">246</td>
<td style="text-align: center;">5.91</td>
<td style="text-align: center;">2.95</td>
<td style="text-align: center;">13.02</td>
</tr>
</tbody>
</table>
<p>datasets for Heterogeneous Node Classification (i.e., IMDB and Freebase), two for Homogeneous Node Classification (i.e., Cora and ogbn-arxiv (abbr. Arxiv)), two for Link Prediction (i.e., Yelp and MIMIC-III), and two for Graph-toText (i.e., AGENDA and WebNLG). Additionally, we apply graph-informed DyVal [51] to evaluate MuseGraph with two Dynamic Reasoning tasks (i.e., Reachability and Max Sum Path), including four levels with increasing complexity (i.e., D1, D2, D3, and D4). The detailed statistics are shown in Table II, Table III, and Table IV. Furthermore, the detailed descriptions of tasks and datasets are provided as follows:
i) Heterogeneous Node Classification: Heterogeneous Node Classification classifies the target node into pre-defined classes, leveraging diverse relationships and attributes within the graph, where the graph consists of multiple types of nodes and edges. Following [52], we split each dataset for Heterogeneous Node Classification into training/validation/testing sets with a ratio of $24 \% / 6 \% / 70 \%$.</p>
<ul>
<li>$\mathrm{IMDB}^{1}$ is a website about movies and related information, including a subset from the Action, Comedy, Drama, Romance, and Thriller genres. Each labeled movie has one or multiple labels.</li>
<li>Freebase ${ }^{2}$ is a knowledge graph of books, films, music, sports, people, locations, organizations, and businesses. Each labeled book has only one label.
ii) Homogeneous Node Classification: Different from Heterogeneous Node Classification, Homogeneous Node Classification forecasts the target node's label within the graph containing the same type of nodes and edges. We employ a $60 \% / 20 \% / 20 \%$ training/validation/testing split for Cora, which is consistent with [4]. For Arxiv, we adopt the public dataset split in [53], which is $54 \% / 18 \% / 28 \%$.</li>
<li>Cora ${ }^{3}$ comprises 2,708 scientific publications classified into one of seven classes-case-based, genetic algorithms, neural networks, probabilistic methods, reinforcement learning, rule learning, and theory, with a citation network consisting of 5,429 links.</li>
<li>Arxiv ${ }^{4}$ represents the citation network among computer science arXiv papers. Each paper in the dataset is asso-</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ciated with a research category, manually labeled by the authors and arXiv moderators. These research categories are chosen from 40 subject areas.
iii) Link Prediction: Link Prediction predicts the likelihood of a future or missing connection between two nodes in a graph. We train all methods using the randomly selected $80 \%$ of links and evaluate them on the remaining $20 \%$ held-out links as suggested in [14] and [54].</p>
<ul>
<li>Yelp ${ }^{5}$ is a user-business network collected from Yelp, including four types of nodes, which are businesses, users, locations, and reviews.</li>
<li>MIMIC-III ${ }^{6}$ consists of a graph of diseases, patients, and visits, with nodes and relations derived from electronic health records.
iv) Graph-to-Text: Graph-to-Text generates a descriptive text based on the information and structure of the graph. Following [55], we design different few-shot settings with four training instance sizes ranging from 50, 100, 200, to 500.</li>
<li>AGENDA $^{7}$ (Abstract Generation Dataset) is a dataset that links knowledge graphs with paper abstracts from scientific domains. The graphs in AGENDA are automatically extracted from the SciIE information extraction system. Each instance in AGENDA includes the paper's title, entities, graph, and abstract.</li>
<li>WebNLG ${ }^{8}$ is a crowd-sourced RDF triple-to-text dataset manually crafted by human annotators. The dataset includes graphs from DBpedia with up to seven triples paired with one or more reference texts. We adopt three large domains of data from WebNLG v1.5 for experiments (i.e., Airport, Building, and Food).
v) Dynamic Reasoning: Reachability and Max Sum Path are Dynamic Reasoning tasks generated by graph-informed DyVal [51], which generates test samples dynamically, mitigating the issues of data contamination and static complexity. We adopt a zero-shot setting consistent with DyVal [51], where we produce 500 samples for each task of varying complexity to balance test time and discrepancy.</li>
<li>Reachability determines if one node can reach another node in the graph. Respond with "True" or "False".</li>
<li>Max Sum Path finds the maximum sum path between two nodes in a graph. The sum value is obtained by summing up the values of nodes in the path. If such a path does not exist, directly answer "N/A".</li>
</ul>
<p>2) Evaluation Protocols: For Heterogeneous Node Classification and Homogeneous Node Classification, we use two commonly adopted evaluation metrics [14], [52], [54]: MacroF1 (across all labels) and Micro-F1 (across all nodes). The F1 score is a metric of the model's accuracy in binary and multi-class classification tasks, which considers both precision and recall. For Link Prediction, we compute the AUC metric as suggested in [9], [14], [54]. AUC measures the area under the ROC curve, indicating the model's ability to distinguish between positive and negative classes across various thresholds.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>For Graph-to-Text, we report BLEU-4 [56] as our metric, where BLEU-4 measures the precision of four-word sequences (4-grams) in the generated text compared to the reference text. Moreover, we evaluate Reachability and Max Sum Path performance with Accuracy that is consistent with DyVal [51].
3) Methods for Comparison: The following characteristic baseline methods can be classified into three categories: i) GNNbased methods, ii) LLM-based methods, and iii) GNN+LLMbased methods.
i) GNN-based methods: We follow [9], [11], [14], [16], [52] in selecting GNN-based methods that are commonly used in homogeneous and heterogeneous network benchmarks.</p>
<ul>
<li>GraphSAGE [1] generates node embeddings by sampling and aggregating features from a node's local neighborhood, enabling scalable learning on large graphs.</li>
<li>GCN [21] scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes.</li>
<li>GAT [57] utilizes masked self-attention mechanisms to enhance the processing of graph data by addressing limitations in traditional graph convolution methods.</li>
<li>RevGNN [58] captures long-range interactions in graph data and reduces memory complexity with grouped reversible connections, enabling more effective training of deep and wide GNNs.</li>
<li>HINormer [16] uses graph transformers to learn node representations on heterogeneous information networks by capturing both local structure and heterogeneity.</li>
<li>R-GCN [59] improves traditional GCNs with relationspecific convolutions, enhancing learning from diverse edge types in knowledge graphs.</li>
<li>HGT [60] extends the transformer architecture to handle heterogeneous graphs by incorporating type-specific parameters and an attention mechanism to capture diverse node and edge interactions.
ii) LLM-based methods: We choose representative and widely adopted LLM-based methods as baselines, covering both general-purpose and graph-specific LLMs. Note that we include the BART [61] and T5 [62] series as standard encoderdecoder baselines for Graph-to-Text, consistent with [55], [63].</li>
<li>Baichuan2-7B-Base [64] is an open-source, bilingual language model developed by Baichuan Inc., trained on 2.6 trillion tokens with 7 billion parameters.</li>
<li>Qwen2-7B-Instruct [65] is an instruction-tuned 7 billion parameter model, designed to excel in tasks like language understanding, generation, and more, with support for processing up to 131,072 tokens in context.</li>
<li>LLaMA1-7B [66] is an open-source large language model developed by Meta. It is designed for natural language understanding and generation tasks, featuring 7 billion parameters for efficient and powerful text processing.</li>
<li>LLaMA2-7B [67] is an improved version of LLaMA1, featuring 7 billion parameters with enhancements in data, training techniques, and model performance.</li>
<li>
<p>LLaMA3-8B [46] succeeds LLaMA2, offering improved performance with 8 billion parameters through advancements in architecture, training data, and optimization.</p>
</li>
<li>
<p>GPT-3.5 [37] is a large-scale language model developed by OpenAI, trained on a vast corpus of text with 175 billion parameters, capable of generating human-like text and understanding complex contexts.</p>
</li>
<li>GPT-4 [40] builds upon GPT-3.5, providing advanced language generation and understanding capabilities with greater scale and improved performance.</li>
<li>BART-large [61] is a pre-trained language model based on the transformer architecture, featuring both encoder and decoder components, and encompasses 160 million parameters, making it a powerful tool for NLP tasks.</li>
<li>T5-large [62] is a transformer-based pre-trained language model that uses a unified architecture for encoding and decoding, consisting of a 768-layer deep network with 11 billion parameters, effectively handling text-based tasks.
iii) GNN+LLM-based methods: We compare recent LLMenhanced or instruction-tuned graph learning paradigms, as summarized in Table I. For methods such as GPT4Graph [8] and NLGraph [11], we exclude them as they are generalpurpose benchmark frameworks rather than concrete graph models with training pipelines. LLM-GNN [3] and TAPE [4] rely on GPT-3.5 APIs to generate high-quality annotations and explanations for each dataset. Therefore, we only utilize the publicly released features for the Cora and Arxiv datasets ${ }^{910}$, and mark "-" for the other datasets in Table V and Table VI. Additionally, since GHGRL [7] is designed with node-centric prompts for node classification, making it unsuitable for link prediction, we report "-" in Table VI.</li>
<li>LLM-GNN [3] involves LLMs to generate confidenceaware annotations for a subset of nodes, which are then used to train GNNs for downstream prediction.</li>
<li>TAPE [4] leverages LLMs' explanations to generate informative node features for text-attributed graphs, boosting the performance of various GNNs.</li>
<li>OFA [5] describes diverse text-attributed graphs using human-understandable prompts and encodes them into a unified embedding space via LLMs, thereby guiding the training of a single GNN model.</li>
<li>All-in-One [6] converts different-level tasks to the graphlevel task with the unified graph and language prompts for improving the multi-task performance of GNNs.</li>
<li>GHGRL [7] employs LLMs to automatically summarize and classify heterogeneous data, and apply a specialized GNN for task-specific learning.</li>
<li>GraphGPT [9] integrates LLMs with graph knowledge using a graph structural instruction tuning paradigm, enhancing understanding through text-graph grounding and step-by-step reasoning.</li>
<li>HiGPT [10] aligns LLMs with heterogeneous graph knowledge using instruction tuning, a specialized graph tokenizer, and a mixture-of-thought augmentation to improve understanding and tackle data sparsity.</li>
<li>InstructGLM [12] tunes LLMs with highly scalable graph prompts that combine natural language instructions and node features from trained GNNs.</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Comprehensive performance of different models on various tasks and datasets. The radar charts compare the performance of six models (i.e., HiNormer, HiGPT, LLaMA3-8B, GraphGPT, GPT-4, and MuseGraph) across multiple tasks. Micro-F1 scores are shown for Node Classification on IMDB (Heterogeneous) and Cora (Homogeneous). Link Prediction on Yelp uses AUC scores. For limited-data Graph-to-Text tasks on AGENDA and WebNLG, we report BLEU-4 scores. Additionally, accuracy measures for Dynamic Reasoning tasks like Reachability and Max Sum Path (D3 to D4 complexities) from the DyVal benchmark are included. Results are normalized for clarity.
4) Implementation Details: For our MuseGraph, we utilize LLaMA-Factory [68] to train a unified framework using a mixture of instruction packages across various tasks and datasets. By default, we choose LLaMA3-8B ${ }^{11}$ [46] as the foundation model for fine-tuning, and we perform parameterefficient learning via LoRA [47] with $r=32$ and $\alpha=64$. The learning rate is set to $5 e^{-5}$ and the maximum input length of LLM is set to 1200. The training process is carried out for two epochs. For GNN-based methods, we train and evaluate baselines based on CogDL [69], HGB [52], or HNE [14]. For LLM-based methods, we load the checkpoint of LLM from HuggingFace ${ }^{12}$ or call official API from OpenAI ${ }^{13}$ for evaluation. Additionally, for a fair comparison, we select GNN+LLM-based methods with public checkpoints or tuning details from their original papers. All LLM-based methods, GNN+LLM-based methods, and MuseGraph, are evaluated using the same test instructions. The hyperparameters of baselines are chosen carefully based on either grid search or their official source codes, and the learning rate is searched in $\left[1 e^{-5}, 1 e^{-2}\right]$. All experiments are conducted using only one NVIDIA GTX 3090 Ti GPU.</p>
<p>Notably, given the GPU constraints during the training phase, we leverage HiGPT [10] checkpoint from Vicuna-7B-v1.5, which was fine-tuned with 60 -shot instruction data on the IMDB ${ }^{14}$, GraphGPT [9] checkpoint tuned with Arxiv-PubMed-mix-NC-LP instruction data ${ }^{15}$, and InstructGLM [12] public checkpoint ${ }^{16}$ for conducting respective experiments. The full code for this work is available ${ }^{17}$.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>TABLE V
EXPERIMENTAL RESULTS ON FOUR BENCHMARK DATASETS FOR HETEROGENEOUS NODE CLASSIFICATION AND HOMOGENEOUS NODE CLASSIFICATION. THE BEST PERFORMANCES ARE HIGHLIGHTED IN BOLDFACE AND THE SECOND RUNNERS ARE UNDERLINED.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Micro-F1</th>
<th style="text-align: center;">Macro-F1</th>
<th style="text-align: center;">Micro-F1</th>
<th style="text-align: center;">Macro-F1</th>
<th style="text-align: center;">Micro-F1</th>
<th style="text-align: center;">Macro-F1</th>
<th style="text-align: center;">Micro-F1</th>
<th style="text-align: center;">Macro-F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: center;">(Hete NC) IMDB</td>
<td style="text-align: center;">(Hete NC) Freebase</td>
<td style="text-align: center;">(Homo NC) Cora</td>
<td style="text-align: center;">(Homo NC) Axxiv</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GraphSAGE</td>
<td style="text-align: center;">62.06</td>
<td style="text-align: center;">52.28</td>
<td style="text-align: center;">58.97</td>
<td style="text-align: center;">27.59</td>
<td style="text-align: center;">85.98</td>
<td style="text-align: center;">84.57</td>
<td style="text-align: center;">71.69</td>
<td style="text-align: center;">50.82</td>
</tr>
<tr>
<td style="text-align: left;">GCN</td>
<td style="text-align: center;">64.23</td>
<td style="text-align: center;">58.43</td>
<td style="text-align: center;">59.86</td>
<td style="text-align: center;">29.35</td>
<td style="text-align: center;">82.87</td>
<td style="text-align: center;">81.29</td>
<td style="text-align: center;">71.98</td>
<td style="text-align: center;">51.26</td>
</tr>
<tr>
<td style="text-align: left;">GAT</td>
<td style="text-align: center;">64.28</td>
<td style="text-align: center;">58.81</td>
<td style="text-align: center;">65.83</td>
<td style="text-align: center;">40.40</td>
<td style="text-align: center;">82.50</td>
<td style="text-align: center;">81.42</td>
<td style="text-align: center;">72.13</td>
<td style="text-align: center;">52.67</td>
</tr>
<tr>
<td style="text-align: left;">RevGNN</td>
<td style="text-align: center;">65.03</td>
<td style="text-align: center;">59.90</td>
<td style="text-align: center;">56.02</td>
<td style="text-align: center;">20.85</td>
<td style="text-align: center;">85.79</td>
<td style="text-align: center;">83.14</td>
<td style="text-align: center;">72.76</td>
<td style="text-align: center;">50.38</td>
</tr>
<tr>
<td style="text-align: left;">HINormer</td>
<td style="text-align: center;">67.63</td>
<td style="text-align: center;">64.29</td>
<td style="text-align: center;">66.51</td>
<td style="text-align: center;">48.87</td>
<td style="text-align: center;">82.84</td>
<td style="text-align: center;">81.28</td>
<td style="text-align: center;">71.01</td>
<td style="text-align: center;">51.81</td>
</tr>
<tr>
<td style="text-align: left;">R-GCN</td>
<td style="text-align: center;">62.98</td>
<td style="text-align: center;">58.90</td>
<td style="text-align: center;">58.57</td>
<td style="text-align: center;">47.03</td>
<td style="text-align: center;">80.63</td>
<td style="text-align: center;">79.14</td>
<td style="text-align: center;">71.04</td>
<td style="text-align: center;">50.79</td>
</tr>
<tr>
<td style="text-align: left;">HGT</td>
<td style="text-align: center;">66.95</td>
<td style="text-align: center;">62.71</td>
<td style="text-align: center;">60.46</td>
<td style="text-align: center;">29.33</td>
<td style="text-align: center;">81.52</td>
<td style="text-align: center;">80.76</td>
<td style="text-align: center;">71.80</td>
<td style="text-align: center;">51.43</td>
</tr>
<tr>
<td style="text-align: left;">Baichuan2-7B-Base</td>
<td style="text-align: center;">40.57</td>
<td style="text-align: center;">39.15</td>
<td style="text-align: center;">3.60</td>
<td style="text-align: center;">1.62</td>
<td style="text-align: center;">8.78</td>
<td style="text-align: center;">5.68</td>
<td style="text-align: center;">1.20</td>
<td style="text-align: center;">0.82</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2-7B-Instruct</td>
<td style="text-align: center;">64.83</td>
<td style="text-align: center;">61.39</td>
<td style="text-align: center;">20.85</td>
<td style="text-align: center;">13.47</td>
<td style="text-align: center;">58.34</td>
<td style="text-align: center;">48.23</td>
<td style="text-align: center;">40.42</td>
<td style="text-align: center;">18.74</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA1-7B</td>
<td style="text-align: center;">22.57</td>
<td style="text-align: center;">16.76</td>
<td style="text-align: center;">4.22</td>
<td style="text-align: center;">1.72</td>
<td style="text-align: center;">12.92</td>
<td style="text-align: center;">5.93</td>
<td style="text-align: center;">1.35</td>
<td style="text-align: center;">1.17</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA2-7B</td>
<td style="text-align: center;">31.03</td>
<td style="text-align: center;">27.46</td>
<td style="text-align: center;">5.29</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">14.02</td>
<td style="text-align: center;">6.08</td>
<td style="text-align: center;">1.82</td>
<td style="text-align: center;">1.21</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA3-8B</td>
<td style="text-align: center;">37.51</td>
<td style="text-align: center;">34.86</td>
<td style="text-align: center;">5.98</td>
<td style="text-align: center;">3.76</td>
<td style="text-align: center;">14.76</td>
<td style="text-align: center;">8.49</td>
<td style="text-align: center;">23.03</td>
<td style="text-align: center;">11.09</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">55.96</td>
<td style="text-align: center;">54.14</td>
<td style="text-align: center;">30.61</td>
<td style="text-align: center;">25.34</td>
<td style="text-align: center;">65.31</td>
<td style="text-align: center;">55.34</td>
<td style="text-align: center;">43.17</td>
<td style="text-align: center;">32.82</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">59.47</td>
<td style="text-align: center;">59.18</td>
<td style="text-align: center;">28.02</td>
<td style="text-align: center;">24.19</td>
<td style="text-align: center;">67.71</td>
<td style="text-align: center;">56.41</td>
<td style="text-align: center;">51.29</td>
<td style="text-align: center;">43.44</td>
</tr>
<tr>
<td style="text-align: left;">LLM-GNN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">75.61</td>
<td style="text-align: center;">73.95</td>
<td style="text-align: center;">66.05</td>
<td style="text-align: center;">45.33</td>
</tr>
<tr>
<td style="text-align: left;">TAPE (GCN)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">88.19</td>
<td style="text-align: center;">87.16</td>
<td style="text-align: center;">74.36</td>
<td style="text-align: center;">55.74</td>
</tr>
<tr>
<td style="text-align: left;">OFA</td>
<td style="text-align: center;">64.81</td>
<td style="text-align: center;">57.91</td>
<td style="text-align: center;">56.58</td>
<td style="text-align: center;">34.75</td>
<td style="text-align: center;">71.68</td>
<td style="text-align: center;">70.21</td>
<td style="text-align: center;">69.31</td>
<td style="text-align: center;">50.70</td>
</tr>
<tr>
<td style="text-align: left;">All-in-One</td>
<td style="text-align: center;">60.75</td>
<td style="text-align: center;">55.17</td>
<td style="text-align: center;">50.48</td>
<td style="text-align: center;">21.04</td>
<td style="text-align: center;">68.45</td>
<td style="text-align: center;">64.18</td>
<td style="text-align: center;">65.57</td>
<td style="text-align: center;">47.06</td>
</tr>
<tr>
<td style="text-align: left;">GHGRL</td>
<td style="text-align: center;">65.40</td>
<td style="text-align: center;">59.24</td>
<td style="text-align: center;">55.96</td>
<td style="text-align: center;">26.15</td>
<td style="text-align: center;">83.96</td>
<td style="text-align: center;">80.97</td>
<td style="text-align: center;">63.10</td>
<td style="text-align: center;">42.05</td>
</tr>
<tr>
<td style="text-align: left;">HiGPT</td>
<td style="text-align: center;">56.72</td>
<td style="text-align: center;">53.77</td>
<td style="text-align: center;">16.17</td>
<td style="text-align: center;">7.02</td>
<td style="text-align: center;">23.86</td>
<td style="text-align: center;">14.17</td>
<td style="text-align: center;">10.76</td>
<td style="text-align: center;">6.25</td>
</tr>
<tr>
<td style="text-align: left;">GraphGPT</td>
<td style="text-align: center;">44.52</td>
<td style="text-align: center;">43.70</td>
<td style="text-align: center;">15.80</td>
<td style="text-align: center;">6.14</td>
<td style="text-align: center;">24.57</td>
<td style="text-align: center;">15.26</td>
<td style="text-align: center;">31.08</td>
<td style="text-align: center;">17.12</td>
</tr>
<tr>
<td style="text-align: left;">InstructGLM</td>
<td style="text-align: center;">13.59</td>
<td style="text-align: center;">9.41</td>
<td style="text-align: center;">5.65</td>
<td style="text-align: center;">2.38</td>
<td style="text-align: center;">15.62</td>
<td style="text-align: center;">10.88</td>
<td style="text-align: center;">42.60</td>
<td style="text-align: center;">28.43</td>
</tr>
<tr>
<td style="text-align: left;">MuseGraph (Qwen2-7B-Instruct)</td>
<td style="text-align: center;">75.77</td>
<td style="text-align: center;">73.08</td>
<td style="text-align: center;">76.40</td>
<td style="text-align: center;">58.18</td>
<td style="text-align: center;">84.35</td>
<td style="text-align: center;">80.71</td>
<td style="text-align: center;">64.88</td>
<td style="text-align: center;">45.41</td>
</tr>
<tr>
<td style="text-align: left;">MuseGraph (LLaMA1-7B)</td>
<td style="text-align: center;">75.73</td>
<td style="text-align: center;">72.51</td>
<td style="text-align: center;">75.42</td>
<td style="text-align: center;">51.36</td>
<td style="text-align: center;">81.29</td>
<td style="text-align: center;">77.50</td>
<td style="text-align: center;">58.82</td>
<td style="text-align: center;">35.54</td>
</tr>
<tr>
<td style="text-align: left;">MuseGraph (LLaMA2-7B)</td>
<td style="text-align: center;">74.65</td>
<td style="text-align: center;">72.44</td>
<td style="text-align: center;">73.68</td>
<td style="text-align: center;">48.44</td>
<td style="text-align: center;">84.06</td>
<td style="text-align: center;">81.25</td>
<td style="text-align: center;">65.45</td>
<td style="text-align: center;">43.94</td>
</tr>
<tr>
<td style="text-align: left;">MuseGraph (LLaMA3-8B)</td>
<td style="text-align: center;">76.57</td>
<td style="text-align: center;">73.78</td>
<td style="text-align: center;">78.01</td>
<td style="text-align: center;">59.62</td>
<td style="text-align: center;">86.83</td>
<td style="text-align: center;">84.74</td>
<td style="text-align: center;">67.80</td>
<td style="text-align: center;">47.71</td>
</tr>
</tbody>
</table>
<p>TABLE VI
AUC RESULTS ON TWO BENCHMARK DATASETS FOR LINK PREDICTION. THE BEST PERFORMANCES ARE HIGHLIGHTED IN BOLDFACE AND THE SECOND RUNNERS ARE UNDERLINED.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Yelp</th>
<th style="text-align: center;">MIMIC-III</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GraphSAGE</td>
<td style="text-align: center;">68.85</td>
<td style="text-align: center;">53.40</td>
</tr>
<tr>
<td style="text-align: left;">GCN</td>
<td style="text-align: center;">69.94</td>
<td style="text-align: center;">53.27</td>
</tr>
<tr>
<td style="text-align: left;">GAT</td>
<td style="text-align: center;">70.38</td>
<td style="text-align: center;">54.46</td>
</tr>
<tr>
<td style="text-align: left;">RevGNN</td>
<td style="text-align: center;">68.97</td>
<td style="text-align: center;">56.79</td>
</tr>
<tr>
<td style="text-align: left;">HINormer</td>
<td style="text-align: center;">75.03</td>
<td style="text-align: center;">57.82</td>
</tr>
<tr>
<td style="text-align: left;">R-GCN</td>
<td style="text-align: center;">72.17</td>
<td style="text-align: center;">57.31</td>
</tr>
<tr>
<td style="text-align: left;">HGT</td>
<td style="text-align: center;">79.02</td>
<td style="text-align: center;">64.01</td>
</tr>
<tr>
<td style="text-align: left;">Baichuan2-7B-Base</td>
<td style="text-align: center;">14.50</td>
<td style="text-align: center;">12.55</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2-7B-Instruct</td>
<td style="text-align: center;">11.18</td>
<td style="text-align: center;">18.31</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA1-7B</td>
<td style="text-align: center;">10.78</td>
<td style="text-align: center;">19.39</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA2-7B</td>
<td style="text-align: center;">22.14</td>
<td style="text-align: center;">25.08</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA3-8B</td>
<td style="text-align: center;">26.64</td>
<td style="text-align: center;">26.05</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">50.82</td>
<td style="text-align: center;">56.83</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">57.44</td>
<td style="text-align: center;">53.53</td>
</tr>
<tr>
<td style="text-align: left;">LLM-GNN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">TAPE (GCN)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">OFA</td>
<td style="text-align: center;">71.56</td>
<td style="text-align: center;">59.33</td>
</tr>
<tr>
<td style="text-align: left;">All-in-One</td>
<td style="text-align: center;">70.83</td>
<td style="text-align: center;">56.20</td>
</tr>
<tr>
<td style="text-align: left;">GHGRL</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">HiGPT</td>
<td style="text-align: center;">49.73</td>
<td style="text-align: center;">51.04</td>
</tr>
<tr>
<td style="text-align: left;">GraphGPT</td>
<td style="text-align: center;">48.24</td>
<td style="text-align: center;">52.01</td>
</tr>
<tr>
<td style="text-align: left;">InstructGLM</td>
<td style="text-align: center;">49.38</td>
<td style="text-align: center;">34.35</td>
</tr>
<tr>
<td style="text-align: left;">MuseGraph (Qwen2-7B-Instruct)</td>
<td style="text-align: center;">70.87</td>
<td style="text-align: center;">$\mathbf{8 0 . 2 1}$</td>
</tr>
<tr>
<td style="text-align: left;">MuseGraph (LLaMA1-7B)</td>
<td style="text-align: center;">76.52</td>
<td style="text-align: center;">65.56</td>
</tr>
<tr>
<td style="text-align: left;">MuseGraph (LLaMA2-7B)</td>
<td style="text-align: center;">70.09</td>
<td style="text-align: center;">67.84</td>
</tr>
<tr>
<td style="text-align: left;">MuseGraph (LLaMA3-8B)</td>
<td style="text-align: center;">$\mathbf{8 0 . 0 7}$</td>
<td style="text-align: center;">$\underline{69.92}$</td>
</tr>
</tbody>
</table>
<h2>B. Main Results Across Different Tasks (RQ1)</h2>
<p>In this subsection, we provide a comprehensive performance analysis of our proposed MuseGraph framework across various graph tasks, evaluating its generation and adaptability capabilities in general, few-shot, and zero-shot settings compared with
state-of-the-art baselines.
Overall, our proposed MuseGraph consistently demonstrates superior performance, highlighting its robust understanding of graph data combined with strong language generation capabilities (shown in Fig. 5). In Node Classification and Link Prediction tasks across six datasets, MuseGraph achieves an average ranking of 2.17. In few-shot Graph-to-Text tasks and Dynamic Reasoning tasks in the zero-shot setting, MuseGraph secures the first and second rankings, respectively. Specifically, despite GPT-4's strength in handling attribute-rich datasets and outperforming our MuseGraph in most Dynamic Reasoning tasks, it often struggles with adaptability across unfamiliar tasks and settings. The ability of MuseGraph to match GPT-4 in complex Dynamic Reasoning tasks with significantly fewer trainable 13,631,488 parameters and training data highlights not just resource efficiency but also a practical solution for real-world graph applications requiring high adaptability and lower computational demands.</p>
<p>Similarly, domain-specific models such as HiGPT and GraphGPT perform commendably within their respective training contexts (e.g., IMDB) but falter when faced with scenarios outside these predefined datasets. HINormer, while effective at managing complex relationships in graph-specific tasks, lacks the versatility needed for broader applications (e.g., Graph-to-Text and Dynamic Reasoning tasks). In contrast, LLaMA3-8B excels in Graph-to-Text due to its extensive linguistic pre-training but, like others, it encounters limitations when stepping beyond its core competencies. MuseGraph, however, maintains robust performance across a variety of tasks, effectively integrating and generalizing graph data and language generation abilities where other models show constraints.</p>
<p>TABLE VII
BLEU-4 scores for the Graph-to-Text task on AGENDA and WebNLG datasets. GPT-4, HiGPT, and GraphGPT results reflect performances without fine-tuning. In contrast, models like BART-large, T5-large, Qwen2-7B-Instruct, LLaMA3-8B, and MuseGraph demonstrate varied performances across different training instance sizes ranging from 50 to 500. The best performances are highlighted in boldface and the second runners are underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">AGENDA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WebNLG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"># Instances</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">7.87</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5.43</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">HiGPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8.49</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GraphGPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8.23</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.45</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGLM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5.97</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5.68</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BART-large</td>
<td style="text-align: center;">9.91</td>
<td style="text-align: center;">10.58</td>
<td style="text-align: center;">11.99</td>
<td style="text-align: center;">12.49</td>
<td style="text-align: center;">26.59</td>
<td style="text-align: center;">28.53</td>
<td style="text-align: center;">30.62</td>
<td style="text-align: center;">33.15</td>
</tr>
<tr>
<td style="text-align: center;">T5-large</td>
<td style="text-align: center;">1.82</td>
<td style="text-align: center;">5.59</td>
<td style="text-align: center;">7.75</td>
<td style="text-align: center;">9.55</td>
<td style="text-align: center;">21.27</td>
<td style="text-align: center;">23.57</td>
<td style="text-align: center;">26.48</td>
<td style="text-align: center;">31.64</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2-7B-Instruct</td>
<td style="text-align: center;">7.95</td>
<td style="text-align: center;">8.28</td>
<td style="text-align: center;">9.90</td>
<td style="text-align: center;">12.41</td>
<td style="text-align: center;">27.24</td>
<td style="text-align: center;">32.29</td>
<td style="text-align: center;">34.90</td>
<td style="text-align: center;">36.02</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA3-8B</td>
<td style="text-align: center;">10.35</td>
<td style="text-align: center;">10.44</td>
<td style="text-align: center;">13.07</td>
<td style="text-align: center;">13.36</td>
<td style="text-align: center;">26.53</td>
<td style="text-align: center;">33.43</td>
<td style="text-align: center;">33.55</td>
<td style="text-align: center;">36.54</td>
</tr>
<tr>
<td style="text-align: center;">MuseGraph (Qwen2-7B-Instruct)</td>
<td style="text-align: center;">10.77</td>
<td style="text-align: center;">11.34</td>
<td style="text-align: center;">12.78</td>
<td style="text-align: center;">12.86</td>
<td style="text-align: center;">31.15</td>
<td style="text-align: center;">32.35</td>
<td style="text-align: center;">35.89</td>
<td style="text-align: center;">36.87</td>
</tr>
<tr>
<td style="text-align: center;">MuseGraph (LLaMA3-8B)</td>
<td style="text-align: center;">11.32</td>
<td style="text-align: center;">13.11</td>
<td style="text-align: center;">13.28</td>
<td style="text-align: center;">14.64</td>
<td style="text-align: center;">31.46</td>
<td style="text-align: center;">33.97</td>
<td style="text-align: center;">34.25</td>
<td style="text-align: center;">36.83</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. Accuracy Results for the Reachability and Max Sum Path tasks on the graph with varying difficulty levels (i.e., D1 to D4), where "D" represents different degrees of complexity.</p>
<p>1) Compared with GNN-based Methods: As shown in Table V and Table VI, our MuseGraph outperforms GNNbased methods across most tasks and datasets, demonstrating its accurate comprehension of graph data. Notably, MuseGraph achieves significant performance gains in Heterogeneous Node Classification on IMDB and Freebase with an average of $13.99 \%$ and $19.64 \%$. Moreover, MuseGraph consistently outperforms all GNN-based baselines in Link Prediction with an average performance improvement of $17.54 \%$.</p>
<p>Specifically, while approaches like HINormer excel in Heterogeneous Node Classification, GraphSAGE and GAT in Homogeneous settings, and HGT in Link Prediction, they generally struggle with Graph-to-Text and Dynamic Reasoning tasks due to their limited adaptability and lack of linguistic features. In contrast, MuseGraph that can be easily tuned on different foundation models with a small number of corpora demonstrates a comprehensive understanding of graph data coupled with language generation capabilities. The overall performance across a variety of tasks and datasets makes MuseGraph a more practical solution than traditional GNNbased methods like HINormer and GAT.
2) Compared with LLM-based Methods: Generally, MuseGraph surpasses all LLM-based methods with significant improvements on both general graph tasks and few-shot ones,
highlighting MuseGraph's superior capabilities in generic graph mining (shown in Table V, Table VI, and Table VII). Note that, by effective graph-aware tuning, our MuseGraph outperforms its foundation model LLaMA3-8B, ranging from $0.79 \%$ in BLEU-4 under 500 training instances on WebNLG to $1485.64 \%$ in Macro-F1 on Freebase.</p>
<p>In detail, GPT-4, known for its extensive parameter set and strong generalization ability, excels in Dynamic Reasoning tasks and attribute-rich datasets (as depicted in Fig. 6 and Table V). However, its large scale and closed-source nature often preclude fine-tuning, which limits its adaptability to complex graph structures and unfamiliar tasks. For example, when accessed via its official API without fine-tuning, GPT-4 produces verbose and inaccurate text in few-shot Graph-toText tasks, resulting in suboptimal BLEU-4 scores due to hallucinatory content. By comparison, open-source LLMs (e.g., Qwen2-7B-Instruct and LLaMA3-8B) that can be slightly finetuned demonstrate improved performance and reduced training costs. For instance, BART-large and LLaMA3-8B, tailored with a specific corpus of Graph-to-Text, gain average improvements over GPT-4 with $245.11 \%$ and $274.38 \%$, respectively. However, without our proposed diverse instruction generation mechanism, these LLMs still encounter challenges in fully comprehending complex graph structures, reflecting inherent limitations in their adaptability to diverse graph data.</p>
<p>Note that, MuseGraph employs a graph-aware instruction tuning mechanism that utilizes fewer but more diverse CoTbased instruction packages and fewer trainable parameters. This approach not only enhances the precision of graph-oriented downstream tasks with a single NVIDIA GTX 3090 Ti GPU but also boosts generation capabilities, enabling MuseGraph to effectively match GPT-4 in complex Dynamic Reasoning tasks with improved efficiency and adaptability.
3) Compared with GNN+LLM-based Methods: Overall, our proposed MuseGraph exceeds the majority of GNN+LLMbased methods across various tasks and datasets, demonstrating its robust understanding of diverse graph data with powerful language generation capabilities (as depicted in Table V, Table VI, Table VII, and Fig. 6). For Heterogeneous Node Classification, the performance improvements range from $17.08 \%$ in Micro-F1 on IMDB to $71.57 \%$ in Macro-F1 on</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7. Comparison of results for Heterogeneous Node Classification on IMDB across different graph descriptions as input. (I) through (IV) denote varying levels of neighbor inclusion: (I) utilizes central node attributes; (II) integrates one-hop neighbors; (III) includes up to two-hop neighbors; and (IV) encompasses three-hop neighbors. (V) employs a proposed method combining one-hop neighbors and random walks. Micro-F1 and Macro-F1 scores are shown alongside the average number of tokens used on IMDB.</p>
<p>Freebase. Additionally, MuseGraph significantly outperforms all GNN+LLM-based methods in both Graph-to-Text and Dynamic Reasoning tasks, benefiting from its superior language generation and reasoning capabilities.</p>
<p>Some methods (e.g., LLM-GNN, TAPE, OFA, and GHGRL) train GNNs as predictors with LLM-enhanced features, combining the interpretative strengths of LLMs with GNNs to improve text-attributed graph representation learning. Particularly, OFA unifies different graph data by describing nodes and edges using natural language and converts them into a shared embedding space via LLMs. TAPE achieves top performance on node classification datasets like Cora and Arxiv by encoding LLMs' explanations. However, it relies on manually customized prompts and GPT-3.5 APIs to generate high-quality explanations for each dataset, which incurs high training costs and restricts its adaptability across diverse tasks and datasets. Moreover, these methods do not apply to Graph-to-Text and Dynamic Reasoning tasks due to their lack of language understanding and generation abilities. This further highlights the adaptability and flexibility of MuseGraph beyond traditional graph prediction tasks.</p>
<p>Another category of methods, like HiGPT, GraphGPT, and InstructGLM, utilizes LLMs to directly perform graph tasks. These methods typically depend on task/dataset-specific GNNs for graph encoding and integrate node representations into the input instructions, which leads to significant computational overhead and limited cross-dataset adaptability. By contrast, as illustrated in Table VII, MuseGraph achieves substantial performance gains by efficiently fine-tuning with even a limited number of examples. This ability highlights its effectiveness in adapting to different Graph-to-Text tasks and securing significant improvements in model performance. Note that HiGPT demonstrates flexibility in zero-shot learning across heterogeneous graph datasets, like when trained on IMDB and tested on DBLP [10]. However, it shows significant performance drops on homogeneous datasets such as Cora (shown in Table V), which highlights its limited adaptability to different graph structures. Additionally, HiGPT struggles with Link Prediction tasks due to a lack of task-specific training corpus, often yielding incorrect "Yes" responses.</p>
<h4>TABLE VIII</h4>
<p><strong>Ablation Study Results for Heterogeneous Node Classification (NC) on IMDB under Different Settings of Fine-Tuning.</strong> "NC PKG w/o. CoT" excludes Chain of Thought (CoT)-based instructions, while "NC PKG", "NC PKG + LP PKG (1:1)", and "NC PKG + LP PKG (3:1)" include them. Notably, "NC PKG + LP PKG (1:1)" and "NC PKG + LP PKG (3:1)" integrate Link Prediction (LP) tasks specifically tailored for the IMDB dataset. The allocation ratio 3:1 denotes that for every four CoT-based instruction packages used, three are from the NC task and one is from the LP task. In contrast, our MuseGraph (LLaMA3-8B) employs CoT-based instructions driven by multiple datasets and tasks, not limited to IMDB. Performance is measured in Micro-F1 and Macro-F1 scores, with the average token count on IMDB provided.</p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>Micro-F1</th>
<th>Macro-F1</th>
<th>Avg. token</th>
</tr>
</thead>
<tbody>
<tr>
<td>NC pkg w/o. CoT</td>
<td>71.23</td>
<td>70.62</td>
<td>317.87</td>
</tr>
<tr>
<td>NC pkg</td>
<td>73.03</td>
<td>71.45</td>
<td>339.73</td>
</tr>
<tr>
<td>NC pkg + LP pkg (1:1)</td>
<td>72.81</td>
<td>71.07</td>
<td>346.09</td>
</tr>
<tr>
<td>NC pkg + LP pkg (3:1)</td>
<td>74.86</td>
<td>72.13</td>
<td>354.12</td>
</tr>
<tr>
<td>MuseGraph (LLaMA3-8B)</td>
<td>76.57</td>
<td>73.78</td>
<td>354.12</td>
</tr>
</tbody>
</table>
<p>These issues underline the critical need for GNN+LLM-based methods to incorporate task-specific training, boost computational efficiency, and achieve generalization across diverse graph structures. In contrast, MuseGraph introduces a compact graph description without specific GNNs to generate fewer diverse and high-quality instruction data under limited language tokens for fine-tuning. Our proposed approach effectively alleviates the content truncation problem while facilitating efficiency and adaptability across various graph tasks and datasets.</p>
<h4><em>C. Ablation Studies (RQ2)</em></h4>
<p>To demonstrate the impact of various model components, we conduct a series of ablation studies, including (1) Variations in Foundation Models, (2) Variations in Compact Graph Descriptions, (3) Variations in CoT-based Instruction Packages, and (4) Variations in Dynamic Instruction Package Allocation Strategies. We have the following observations.</p>
<p><em>1) Variations in Foundation Models:</em> We explore several foundation models, including Qwen2-7B-Instruct, LLaMA1-7B, LLaMA2-7B, and LLaMA3-8B, across various graph-related tasks and datasets.</p>
<p>According to Table V, Table VI, and Table VII, MuseGraph (LLaMA3-8B) outperforms other variations of MuseGraph across most tasks and datasets, with performance improvements ranging from 0.96% in Macro-F1 on IMDB to 15.61% in BLEU-4 under 100 training instances on AGENDA. This indicates that a foundation model with larger parameters and extensive training can achieve better and more stable results with our graph-aware instruction tuning. Moreover, MuseGraph (Qwen2-7B-Instruct) also achieves remarkable results on MIMIC-III and WebNLG, underscoring the adaptability of our framework to harness the distinct capabilities of various LLMs, enhancing their robust understanding of graph data. Through the above results, MuseGraph proves to be cost-effective, by utilizing fine-tuning of diverse LLMs rather than relying on extensive new data and training from scratch.</p>
<p><em>2) Variations in Compact Graph Descriptions:</em> We assess various compact graph descriptions on Heterogeneous Node</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>H</th>
</tr>
</thead>
<tbody>
<tr>
<td>$v^{*}$ : Echo Dr.</td>
<td>MOVIE</td>
<td>15</td>
</tr>
<tr>
<td>$v_{1}$ : Artificial Intelligence</td>
<td>KEYWORD</td>
<td>17</td>
</tr>
<tr>
<td>$v_{2}$ : Hugh Mun</td>
<td>ACTOR</td>
<td>4</td>
</tr>
<tr>
<td>$v_{3}$ : Security System</td>
<td>KEYWORD</td>
<td>15</td>
</tr>
<tr>
<td>$v_{4}$ : Patrick Ryan Sims</td>
<td>DIRECTOR</td>
<td>20</td>
</tr>
<tr>
<td>$v_{5}$ : Family Relationships</td>
<td>KEYWORD</td>
<td>12</td>
</tr>
<tr>
<td>$v_{6}$ : Tech Noir</td>
<td>KEYWORD</td>
<td>18</td>
</tr>
<tr>
<td>$v_{7}$ : Dane Bowman</td>
<td>ACTOR</td>
<td>9</td>
</tr>
<tr>
<td>$v_{8}$ : John Pirkis</td>
<td>ACTOR</td>
<td>5</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
<p>Compact Graph Description: The compact graph description of this MOVIE is listed as follows: Title: [v<em>] Ego Graph Nodes: [v</em>, $v_{1}, v_{3}, v_{4}, v_{6}, \ldots, v_{18}, v_{19}$ ] One-hop Neighbors: [ $v_{4}, v_{1}, v_{9}, v_{3}$, $v_{10}, v_{6}$ ] Random Walks: [A. $v^{<em>} \rightarrow v_{1} \rightarrow v_{12} \rightarrow v_{13} \rightarrow v_{14} \rightarrow v_{15} ;$ B. $v^{</em>} \rightarrow v_{4} \rightarrow v_{16} \rightarrow v_{17} ;$ C. ...] HiGPT: Answer: [Romance, Thriller, Comedy, Action, Drama]: Reasoning: Based on the provided compact graph description, this MOVIE "Echo Dr." has an actor named Claire Gordon-Harper, who is known for her roles in romantic and dramatic movies. Additionally, it has the keywords "Home Invasion" and "Android", which is a common element in thriller and action movies. Finally, this MOVIE has an actor named Johnathan Hurley, who has been in comedic roles in the past. GPT-4: Answer: [Thriller, Drama, Science Fiction]: Reasoning: Analyzing the compact graph description for "Echo Dr.", the presence of keywords such as "Artificial Intelligence", "Tech Noir", and "Android" strongly suggest a setting focused on technology and its implications, typical of scifiction. The keyword "Home Invasion" aligns with the Thriller genre, as it involves ... in thriller movies. The keyword "Human Android Relationship" also suggests elements of Drama, exploring ... Qwen2-7B-Instruct: Answer: [Thriller, Action, Drama]: Reasoning: The MOVIE "Echo Dr." is associated with keywords such as "Artificial Intelligence", "Home Invasion", "Tech noir", .... These keywords suggest a futuristic setting with elements of technology and human-robot relationships, which are common in genres like Drama and Action. The "Home Invasion" keyword indicates a thriller aspect, as it often .... The "Android" keyword also points towards a Sci-Fi or Action genre. MuseGraph: Answer: [Thriller, Action, Drama]: Reasoning: (1) Ego Graph Nodes: The MOVIE "Echo Dr." is connected to other movies like "Blade Runner" and "Impostor", which are known for their elements of action, thriller, and drama, such as .... (2) One-hop Neighbors: The one-hop neighbors include elements like "Tech Noir", "Home Invasion", .... These are characteristic of the Thriller and Action genres. The "Human Android Relationship" also add a dramatic element to the movie, reinforcing the Drama genre. (3) Random Walks: Walk A: This suggests a storyline that is likely dark, ..., typical of a Thriller. Walk B: connects "Echo Dr." with .... Walk C: ....</p>
<p>Fig. 8. Heterogeneous Node Classification Results on IMDB from HiGPT, GPT-4, Qwen2-7B-Instruct, and MuseGraph, all utilizing our compact graph description as input. These models, selected for their generation and reasoning capabilities of LLMs, are compared based on their ability to analyze graphs. Best viewed in color.</p>
<p>Classification on the IMDB dataset, exploring models with different levels of neighborhood and walk integration. The simplest model (I) uses only central node attributes. Models (II) through (IV) gradually integrate more extensive neighborhood attributes, enhancing the model's context-awareness. Our advanced model (V) uniquely merges one-hop neighbors with random walks, striking an optimal balance between richness of information and token efficiency, as shown in Fig. 7.</p>
<p>Specifically, compared with description (I), which only has attribute information of the central node, description (II) integrates all one-hop neighbor attributes to achieve up to $13.34 \%$ improvements in Macro-F1. However, when extending the neighborhood to two-hop or multi-hop without sampling decreases effectiveness (i.e., (III) and (IV)), the performance significantly deteriorates due to an overload of input tokens. Conversely, model (V) effectively combines one-hop neighbors with random walks within a controlled token budget by calculating the node energy (cf., Eq. 1). This ensures a rich information intake and maintains token efficiency, significantly improving performance on complex graph structures. 3) Variations in CoT-based Instruction Packages: In the ablation study detailed in Table VIII, we investigate the impact of different variations in Chain of Thought (CoT)based instruction packages on the IMDB dataset for the Heterogeneous Node Classification task.</p>
<p>When CoT-based instructions are incorporated with taskspecific instructions (i.e., NC pkg w/o. CoT vs. NC pkg), there is a notable improvement in both Micro-F1 and Macro-F1 scores with only a 21.86 increase in the average number of tokens, indicating enhanced model understanding and reasoning capabilities with minimal impact on computational efficiency. Note that appropriately integrating CoT-instruction packages for Link Prediction tasks on IMDB (e.g., NC pkg + LP pkg (3:1)) demonstrates a compounded beneficial effect in Heterogeneous Node Classification, as further analyzed in Section IV-C4. This not only further improves the model's adaptability across various graph tasks, but also underscores the synergistic effect of diverse yet balanced CoT-based instruction packages on comprehension capabilities for generic graphs. 4) Variations in Dynamic Instruction Package Allocation Strategies: As shown in Table VIII, we assess the impact of varying CoT-based instruction package compositions, incorporating instruction packages from multiple tasks and datasets, and applying them to perform graph-aware instruction tuning.</p>
<p>By adjusting the allocation ratios of Link Prediction CoTbased instructions (e.g., a $3: 1$ ratio compared to a $1: 1$ ratio) within the NC pkg + LP pkg, we observed performance gains for Heterogeneous Node Classification, with Micro-F1 rising from 72.81 to 74.86 . This demonstrates the critical need for carefully balancing instruction packages based on the complexity and requirements of each task, enabling LLMs to accurately understand and adapt to diverse graph-based tasks. Furthermore, when we diversify the instruction set with various graph tasks and datasets, our MuseGraph (LLaMA3-8B) can surpass NC pkg + LP pkg (3:1) with a superior performance improvement of $2.29 \%$ on average. This strongly indicates that the strategic dynamic instruction package allocation with different task complexities and dataset complexities can enhance model adaptability across different graph tasks and datasets while mitigating the risk of catastrophic forgetting.</p>
<h2>D. Case Studies (RQ3)</h2>
<p>To evaluate the effectiveness of compact graph descriptions in enhancing LLMs' comprehension of graph structures, we provide a Heterogeneous Node Classification scenario on the IMDB dataset. Fig. 8 contrasts the responses from HiGPT, GPT4, Qwen2-7B-Instruct, and MuseGraph, which all interpret the same compact graph description but exhibit varying analytical abilities due to distinct LLM architectures.</p>
<p>Overall, the utilization of compact graph descriptions facilitates a deeper and more accurate analysis by encapsulating key graph data into a more manageable and interpretable format. This not only preserves critical information but also enhances the models' understanding and reasoning capabilities to generate relevant and context-aware responses, allowing models to focus on the pertinent details without the noise commonly associated with extensive graph data. As shown in the blue responses of Fig. 8, both Qwen2-7B-Instruct and MuseGraph benefit from this compact graph description, achieving correct predictions and analyses that reflect the insightful understanding of graph data.</p>
<p>Note that, HiGPT and GPT-4 perform wrong predictions and provide the additional categories for $v^{<em>}$ (shown in the red responses of Fig. 8). HiGPT struggles with the compact graph description due to the absence of specific learned graph tokens for input. This limitation causes HiGPT to revert to its existing knowledge base for category analysis, leading to irrelevant and misleading suggestions, such as attributing the category "Comedy" to Johnathan Hurley based on his past roles, which does not align with $v^{</em>}$ 's actual categories. GPT-4, with its extensive parameters and knowledge, excels in studying the semantic information of keywords presented in $v^{*}$ 's compact graph description to predict the categories. However, this makes it ignore graph structures and leads to hallucinations, resulting in erroneous predictions like "Science Fiction". Although Qwen2-7B-Instruct shows remarkable performances on IMDB, it is hard to adapt to different tasks and datasets without further fine-tuning (depicted in Table V, Table VI, and Table VII).</p>
<p>In contrast, MuseGraph can accurately understand graph structures via compact graph description. For example, the onehop neighbor "Home Invasion" and the walk "A" suggest a strong thriller element of node " $v^{*}$ ". These elements collectively enable MuseGraph to provide a detailed and contextually enriched genre prediction, showcasing its ability to synthesize complex graph structures into coherent and accurate analyses.</p>
<h2>V. Conclusion and Future Work</h2>
<p>In this paper, we introduce MuseGraph, an effective and generic approach for graph mining, which can enable the accurate understanding abilities of graph data across various tasks and datasets. Through the innovative design of compact graph descriptions with adaptive generation procedure, the generation of diverse task-specific Chain-of-Thought (CoT)based instruction packages, and the implementation of graphaware instruction tuning, our MuseGraph integrates the strengths of Graph Neural Networks (GNNs) and Large Language Models (LLMs) into one single foundation model. Our comprehensive experimental results demonstrate MuseGraph's
superior performance against state-of-the-art baselines in five graph tasks and ten datasets, illustrating its ability not only to improve the precision of graph-related downstream tasks but also to enhance the generation capabilities of LLMs, which is further consolidated with our real case study results.</p>
<p>The primary limitation of our MuseGraph lies in the reliance on input graphs with rich semantic information and the careful selection of training datasets. Looking ahead, it is interesting to broaden the scope of MuseGraph by incorporating it with a wider range of graph types and exploring its applications in more diverse tasks. For instance, applying MuseGraph to biological graphs could be intriguing, where integrating domain-specific knowledge and expert feedback is crucial.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This work was supported in part by the National Natural Science Foundation of China under Grants (No.62302098) and Fujian Provincial Natural Science Foundation of China under Grants (2025J01540). Carl Yang was not supported by any fund from China.</p>
<h2>REFERENCES</h2>
<p>[1] W. Hamilton, Z. Ying, and J. Leskovec, "Inductive representation learning on large graphs," Advances in Neural Information Processing Systems, vol. 30, pp. 1025-1035, 2017.
[2] G. Cui, J. Zhou, C. Yang, and Z. Liu, "Adaptive graph encoder for attributed graph embedding," in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, 2020, pp. 976-985.
[3] Z. Chen, H. Mao, H. Wen, H. Han, W. Jin, H. Zhang, H. Liu, and J. Tang, "Label-free node classification on graphs with large language models (llms)," in Proceedings of the 12th International Conference on Learning Representations, 2024, pp. 31632-31655.
[4] X. He, X. Bresson, T. Laurent, A. Perold, Y. LeCun, and B. Hooi, "Harnessing explanations: Llm-to-lm interpreter for enhanced textattributed graph representation learning," in Proceedings of the 12th International Conference on Learning Representations, 2024, pp. 57115732.
[5] H. Liu, J. Feng, L. Kong, N. Liang, D. Tao, Y. Chen, and M. Zhang, "One for all: Towards training one graph model for all classification tasks," in Proceedings of the 12th International Conference on Learning Representations, 2024, pp. 20188-20210.
[6] X. Sun, H. Cheng, J. Li, B. Liu, and J. Guan, "All in one: Multi-task prompting for graph neural networks," In Proceedings of the 29rd ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, pp. 2120-2131, 2023.
[7] H. Gao, C. Zhang, F. Wu, C. Zheng, J. Zhao, and H. Liu, "Bootstrapping heterogeneous graph representation learning via large language models: A generalized approach," in Proceedings of the AAAI Conference on Artificial Intelligence, 2025, pp. 16717-16726.
[8] J. Guo, L. Du, and H. Liu, "Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking," arXiv preprint arXiv:2305.15066, 2023.
[9] J. Tang, Y. Yang, W. Wei, L. Shi, L. Su, S. Cheng, D. Yin, and C. Huang, "Graphgpt: Graph instruction tuning for large language models," in Proceedings of the 47th ACM SIGIR International Conference on Research and Development in Information Retrieval, 2024, pp. 491-500.
[10] J. Tang, Y. Yang, W. Wei, L. Shi, L. Xia, D. Yin, and C. Huang, "Higpt: Heterogeneous graph language model," in Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining, 2024, pp. 2842-2853.
[11] H. Wang, S. Feng, T. He, Z. Tan, X. Han, and Y. Tsvetkov, "Can language models solve graph problems in natural language?" Advances in Neural Information Processing Systems, vol. 36, pp. 30840-30861, 2023.
[12] R. Ye, C. Zhang, R. Wang, S. Xu, and Y. Zhang, "Language is all a graph needs," in Findings of the Association for Computational Linguistics: EACL 2024, 2024, pp. 1955-1973.</p>
<p>[13] Q. Huang, H. Ren, P. Chen, G. KrÅ¾manc, D. Zeng, P. S. Liang, and J. Leskovec, "Prodigy: Enabling in-context learning over graphs," Advances in Neural Information Processing Systems, vol. 36, pp. 16 30216317, 2023.
[14] C. Yang, Y. Xiao, Y. Zhang, Y. Sun, and J. Han, "Heterogeneous network representation learning: A unified framework with survey and benchmark," IEEE Transactions on Knowledge and Data Engineering, vol. 34, no. 10, pp. 4854-4873, 2020.
[15] H. Cai, V. W. Zheng, and K. C.-C. Chang, "A comprehensive survey of graph embedding: Problems, techniques, and applications," IEEE Transactions on Knowledge and Data Engineering, vol. 30, no. 9, pp. 1616-1637, 2018.
[16] Q. Mao, Z. Liu, C. Liu, and J. Sun, "Hinormer: Representation learning on heterogeneous information networks with graph transformer," in Proceedings of the ACM Web Conference 2023, 2023, pp. 599-610.
[17] A. Grover and J. Leskovec, "node2vec: Scalable feature learning for networks," in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, 2016, pp. 855864 .
[18] Y. Dong, N. V. Chawla, and A. Swami, "metapath2vec: Scalable representation learning for heterogeneous networks," in Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, 2017, pp. 135-144.
[19] S. Ivanov and E. Burnaev, "Anonymous walk embeddings," in Proceedings of the 35th International Conference on Machine Learning, 2018, pp. 2186-2195.
[20] J. Li, H. Peng, Y. Cao, Y. Dou, H. Zhang, S. Y. Philip, and L. He, "Higher-order attribute-enhancing heterogeneous graph neural networks," IEEE Transactions on Knowledge and Data Engineering, vol. 35, no. 1, pp. 560-574, 2021.
[21] T. N. Kipf and M. Welling, "Semi-supervised classification with graph convolutional networks," in Proceedings of the 5th International Conference on Learning Representations, 2017.
[22] C. Yang, J. Zhang, H. Wang, S. Li, M. Kim, M. Walker, Y. Xiao, and J. Han, "Relation learning on social networks with multi-modal graph edge variational autoencoders," in Proceedings of the 13th International Conference on Web Search and Data Mining, 2020, pp. 699-707.
[23] W.-Z. Li, C.-D. Wang, J.-H. Lai, and S. Y. Philip, "Towards effective and robust graph contrastive learning with graph autoencoding," IEEE Transactions on Knowledge and Data Engineering, vol. 36, no. 2, pp. 868-881, 2023.
[24] T. Qian, Y. Liang, Q. Li, and H. Xiong, "Attribute graph neural networks for strict cold start recommendation," IEEE Transactions on Knowledge and Data Engineering, vol. 34, no. 8, pp. 3597-3610, 2020.
[25] H. Zhang, Y. Ren, L. Fu, X. Wang, G. Chen, and C. Zhou, "Multiscale self-supervised graph contrastive learning with injective node augmentation," IEEE Transactions on Knowledge and Data Engineering, vol. 36, no. 1, pp. 261-274, 2023.
[26] X. Jiang, T. Jia, Y. Fang, C. Shi, Z. Lin, and H. Wang, "Pre-training on large-scale heterogeneous graph," in Proceedings of the 27th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, 2021, pp. 756-766.
[27] N. Liu, X. Wang, H. Han, and C. Shi, "Hierarchical contrastive learning enhanced heterogeneous graph neural network," IEEE Transactions on Knowledge and Data Engineering, vol. 35, no. 10, pp. 10 884-10 896, 2023.
[28] M. Sun, K. Zhou, X. He, Y. Wang, and X. Wang, "Gppt: Graph pretraining and prompt tuning to generalize graph neural networks," in Proceedings of the 28th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, 2022, pp. 1717-1727.
[29] Y. Li, Z. Li, P. Wang, J. Li, X. Sun, H. Cheng, and J. X. Yu, "A survey of graph meets large language model: Progress and future directions," in Proceedings of the 33th International Joint Conference on Artificial Intelligence, 2024, pp. 8123-8131.
[30] M. Fazel and M. Chiang, "Network utility maximization with nonconcave utilities using sum-of-squares method," in Proceedings of the 44th IEEE Conference on Decision and Control. IEEE, 2005, pp. 1867-1874.
[31] H. Lian, Y. Xiong, Z. Liu, J. Niu, S. Mo, H. Chen, P. Liu, and G. Ding, "Lbpe: Long-token-first tokenization to improve large language models," in ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025, pp. 1-5.
[32] S. Azevedo, A. Mascaretti, R. Rende, M. Mahaut, M. Baroni, and A. Laio, "An approach to identify the most semantically informative deep representations of text and images," arXiv preprint arXiv:2505.17101, 2025.
[33] J. Melton and S. Krishnan, "muxgnn: Multiplex graph neural network for heterogeneous graphs," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 9, pp. 11 067-11 078, 2023.
[34] Y. Wang, T. Zhao, Y. Zhao, Y. Liu, X. Cheng, N. Shah, and T. Derr, "A topological perspective on demystifying gnn-based link prediction performance," in Proceedings of the 12th International Conference on Learning Representations, 2024, pp. 20 884-20 922.
[35] J. Gao, X. Zou, Y. Ai, D. Li, Y. Niu, B. Qi, and J. Liu, "Graph counselor: Adaptive graph exploration via multi-agent synergy to enhance llm reasoning," in Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025, pp. 24 65024668.
[36] Y. Gao, X. Wang, X. He, Z. Liu, H. Feng, and Y. Zhang, "Addressing heterophily in graph anomaly detection: A perspective of graph spectrum," in Proceedings of the ACM Web Conference 2023, 2023, pp. 1528-1538.
[37] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., "Training language models to follow instructions with human feedback," Advances in Neural Information Processing Systems, vol. 35, pp. 27 730-27 744, 2022.
[38] Y. Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. Chandu, D. Wadden, K. MacMillan, N. A. Smith, I. Beltagy et al., "How far can camels go? exploring the state of instruction tuning on open resources," Advances in Neural Information Processing Systems, pp. 74 764-74 786, 2023.
[39] K. Shridhar, A. Stofis, and M. Sachan, "Distilling reasoning capabilities into smaller language models," in Findings of the Association for Computational Linguistics: ACL 2023, 2023, pp. 7059-7073.
[40] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., "Gpt-4 technical report," arXiv preprint arXiv:2303.08774, 2023.
[41] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., "Chain-of-thought prompting elicits reasoning in large language models," Advances in Neural Information Processing Systems, vol. 35, pp. 24 824-24 837, 2022.
[42] Z. Xiang, F. Jiang, Z. Xiong, B. Ramasubramanian, R. Poovendran, and B. Li, "Badchain: Backdoor chain-of-thought prompting for large language models," in Proceedings of the 12th International Conference on Learning Representations, 2024, pp. 28 289-28 316.
[43] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, "Self-instruct: Aligning language models with selfgenerated instructions," in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, 2023, pp. 13 484-13 508.
[44] M. Mosbach, M. Andriushchenko, and D. Klakow, "On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines," in Proceedings of the 8th International Conference on Learning Representations, 2020.
[45] S. Vijay and A. Priyanshu, "Nerda-con: Extending ner models for continual learning-integrating distinct tasks and updating distribution shifts," in Proceedings of the Updatable Machine Learning (UpML) Workshop at the 39th International Conference on Machine Learning (ICML). PMLR, 2022.
[46] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan et al., "The llama 3 herd of models," arXiv e-prints, pp. arXiv-2407, 2024.
[47] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, "Lora: Low-rank adaptation of large language models," in Proceedings of the 10th International Conference on Learning Representations, 2022.
[48] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, "Qlora: Efficient finetuning of quantized llms," Advances in Neural Information Processing Systems, vol. 36, pp. 10 088-10 115, 2023.
[49] Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen, and T. Zhao, "Adaptive budget allocation for parameter-efficient fine-tuning," in Proceedings of the 11th International Conference on Learning Representations, 2023.
[50] Z. Gao, Q. Wang, A. Chen, Z. Liu, B. Wu, L. Chen, and J. Li, "Parameterefficient fine-tuning with discrete finetier transform," in Proceedings of the 41st International Conference on Machine Learning, 2024, pp. 14 88414901.
[51] K. Zhu, J. Chen, J. Wang, N. Z. Gong, D. Yang, and X. Xie, "Dyval: Dynamic evaluation of large language models for reasoning tasks," in Proceedings of the 12th International Conference on Learning Representations, 2024, pp. 18 091-18 128.
[52] Q. Lv, M. Ding, Q. Liu, Y. Chen, W. Feng, S. He, C. Zhou, J. Jiang, Y. Dong, and J. Tang, "Are we really making much progress? revisiting, benchmarking and refining heterogeneous graph neural networks," in Proceedings of the 27th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, 2021, pp. 1150-1160.</p>
<p>[53] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, "Open graph benchmark: Datasets for machine learning on graphs," Advances in Neural Information Processing Systems, vol. 33, pp. 22 118-22 133, 2020.
[54] Y. Tan, Z. Zhou, H. Lv, W. Liu, and C. Yang, "Walklm: A uniform language model fine-tuning framework for attributed graph embedding," Advances in Neural Information Processing Systems, vol. 36, pp. 1330813325, 2023.
[55] J. Li, T. Tang, W. X. Zhao, Z. Wei, N. J. Yuan, and J.-R. Wen, "Few-shot knowledge graph-to-text generation with pretrained language models," in Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, 2021, pp. 1558-1568.
[56] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, "Bleu: a method for automatic evaluation of machine translation," in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311-318.
[57] P. VeliÄkoviÄ‡, G. Cucurull, A. Casanova, A. Romero, P. LiÃ², and Y. Bengio, "Graph attention networks," in Proceedings of the 6th International Conference on Learning Representations, 2018.
[58] G. Li, M. MÃ¼ller, B. Ghanem, and V. Koltun, "Training graph neural networks with 1000 layers," in Proceedings of the 38th International Conference on Machine Learning. PMLR, 2021, pp. 6437-6449.
[59] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling, "Modeling relational data with graph convolutional networks," in 15th International Conference on Extended Semantic Web Conference, 2018, pp. 593-607.
[60] Z. Hu, Y. Dong, K. Wang, and Y. Sun, "Heterogeneous graph transformer," in Proceedings of the ACM Web Conference 2020, 2020, pp. 2704-2710.
[61] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 7871-7880.
[62] L. F. Ribeiro, M. Schmitt, H. SchÃ¼tze, and I. Gurevych, "Investigating pretrained language models for graph-to-text generation," in Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI, 2021, pp. 211-227.
[63] A. Colas, M. Alvandipour, and D. Z. Wang, "Gap: A graph-aware language model framework for knowledge graph-to-text generation," in Proceedings of the 29th International Conference on Computational Linguistics, 2022, pp. 5755-5769.
[64] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Bian, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan et al., "Baichuan 2: Open large-scale language models," arXiv preprint arXiv:2309.10305, 2023.
[65] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang et al., "Qwen2 technical report," arXiv preprint arXiv:2407.10671, 2024.
[66] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. RoziÃ¨re, N. Goyal, E. Hambro, F. Azhar et al., "Llama: Open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023.
[67] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., "Llama 2: Open foundation and fine-tuned chat models," arXiv preprint arXiv:2307.09288, 2023.
[68] Y. Zheng, R. Zhang, J. Zhang, Y. Ye, and Z. Luo, "Llamafactory: Unified efficient fine-tuning of 100+ language models," in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024, pp. 400-410.
[69] Y. Cen, Z. Hou, Y. Wang, Q. Chen, Y. Luo, Z. Yu, H. Zhang, X. Yao, A. Zeng, S. Guo et al., "Cogdl: A comprehensive library for graph deep learning," in Proceedings of the ACM Web Conference 2023, 2023, pp. $747-758$.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Yanchao Tan is currently working as an Associate Professor with the College of Computer and Data Science, and Fujian Key Laboratory of Network Computing and Intelligent Information Processing, Fuzhou University, Fuzhou, China. She obtained her Ph.D. degree from the College of Computer Science, Zhejiang University, Hangzhou, China, in 2022. She was a Visiting Scholar at The Chinese University of Hong Kong from 2024 to 2025. Her research interests include data mining, healthcare, recommender systems, and large language models.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Hang Lv received a BS degree in computer science from Fuzhou University, China, in 2023. He is currently working toward a Ph.D. degree in computer science and technology at Fuzhou University, China. His research interests include data mining, healthcare, recommender systems, large language models, and graph representation learning.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Pengxiang Zhan received a BS degree in computer science from Fuzhou University, China, in 2023. He is currently pursuing a Master's degree in computer science and technology at Fuzhou University, China. His research interests include language models and multimodal large models.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Shiping Wang is currently working as a Professor with the College of Computer and Data Science, Fuzhou University, Fuzhou, China, and the Director of the Key Laboratory of Intelligent Metro, Fujian Province University, Fuzhou, China. He received his Ph.D. degree from the University of Electronic Science and Technology of China, Chengdu, China in 2014. He was a Visiting Scholar in University of Alberta, Edmonton, Canada from August 2013 to August 2014. He worked as a Research Assistant in National University of Singapore from January 2014 to August 2014, and a Research Fellow in Nanyang Technological University of Singapore from August 2015 to August 2016. He was also a Visiting Researcher in Peking University, Beijing, China from August 2019 to August 2020. His research interests include machine learning, deep learning, feature representation and multi-modal fusion.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Carl Yang is an Assistant Professor of Computer Science at Emory University, jointly appointed in the Rollins School of Public Health and Nell Hodgson Woodruff School of Nursing. He received his Ph.D. in Computer Science at University of Illinois, UrbanaChampaign in 2020, and B.Eng. in Computer Science and Engineering at Zhejiang University in 2014. His research interests span data mining, knowledge graphs, multimodality foundation models, and trustworthy AI, with applications in network sciences, neuroscience, biomedicine, and healthcare. Carl's research results have led to 200+ peer-reviewed publications in top venues across AI/ML and medicine/healthcare. He is also a recipient of the SIGKDD Rising Star Award in 2025, NSF CAREER Award in 2025, NIH K25 (Career) Award in 2023, and multiple Best Paper Awards such as of MedInfo 2025, KDD Health Day 2022, ML4H 2022, and ICDM 2020.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ https://github.com/CurryTang/LLMGNN
${ }^{10}$ https://github.com/XiaoxinHe/TAPE&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{11}$ https://ai.meta.com/blog/meta-llama-3
${ }^{12}$ https://huggingface.co
${ }^{13}$ https://platform.openai.com
${ }^{14}$ https://huggingface.co/Jiabin99/HiGPT
${ }^{15}$ https://huggingface.co/Jiabin99/GraphGPT-7B-mix-all
${ }^{16}$ https://github.com/agiresearch/InstructGLM
${ }^{17}$ https://github.com/Melinda315/MuseGraph&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
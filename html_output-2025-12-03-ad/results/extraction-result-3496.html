<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3496 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3496</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3496</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-e0c6abdbdecf04ffac65c440da77fb9d66bb474c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e0c6abdbdecf04ffac65c440da77fb9d66bb474c" target="_blank">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> XLNet is proposed, a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autore progressive formulation.</p>
                <p><strong>Paper Abstract:</strong> With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3496.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3496.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLNet (RACE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XLNet evaluated on the RACE reading-comprehension benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>XLNet (a permutation-based generalized autoregressive pretrained Transformer with two-stream attention and Transformer-XL components) evaluated on RACE, a challenging multi-choice reading-comprehension dataset that requires reasoning over long passages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLNet (Large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Permutation language modeling objective (permutations of factorization order) with two-stream self-attention, integrated Transformer-XL components (relative positional encoding and segment recurrence), span-based partial prediction and bidirectional data pipeline. Trained at scale on BooksCorpus/Wikipedia and larger corpora in scaled experiments; architecture hyperparameters same as BERT-Large.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RACE (reading-comprehension)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-choice reading-comprehension dataset (exam-style questions) with relatively long passages (avg >300 tokens) requiring understanding and reasoning over context; considered a challenging benchmark for explicit/long-context reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Permutation language modeling (maximize expected likelihood over permutations), two-stream attention for target-aware representations, Transformer-XL backbone (relative positional encodings + segment recurrence), partial/span-based prediction, bidirectional data pipeline, memory caching.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>XLNet (full-scale reported) test accuracy on RACE: 85.4% overall (Middle: 88.6%, High: 84.0%) as reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>RoBERTa: 83.2% overall; BERT (prior best listed) 72.0% (earlier baseline rows show other prior methods: GPT 59.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Absolute improvement of +2.2 percentage points over RoBERTa (85.4 vs 83.2) and large improvement over earlier BERT baseline (+13.4 points vs 72.0).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper reports that XLNet still underfits training data after large-scale training (500K steps) indicating capacity/data mismatch; ablations show removal of memory/cache harms performance on long-context tasks like RACE; no explicit claims of solving formal logical reasoning—RACE is a reading-comprehension benchmark with varied reasoning demands.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation (Table 6) shows: XLNet-Base (K=6) RACE dev ~66.66 vs BERT-Base 64.3 and DAE+Transformer-XL 65.03, indicating permutation LM and Transformer-XL backbone both contribute; removing memory (-memory) drops performance to 65.55; varying K (partial prediction) changes performance slightly; authors qualitatively attribute gains on long-context reasoning to Transformer-XL recurrence and relative encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'XLNet: Generalized Autoregressive Pretraining for Language Understanding', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3496.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3496.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLNet (SQuAD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XLNet evaluated on SQuAD1.1 / SQuAD2.0 question-answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>XLNet applied to span-extraction question answering (SQuAD1.1 and SQuAD2.0), leveraging permutation LM pretraining and Transformer-XL components to improve answer-span prediction and answerability detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLNet (Large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same XLNet architecture described above; finetuned for span-extraction QA with joint logistic regression for answerability (for SQuAD2.0) and span extraction loss. Uses full sequence length 512 during finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>SQuAD1.1 and SQuAD2.0 (reading comprehension / span extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Extractive QA tasks requiring locating answer spans in context; SQuAD2.0 also includes unanswerable questions requiring answerability reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Finetuning XLNet with span extraction loss; joint logistic regression for answerability (SQuAD2.0); benefits from permutation LM pretraining and Transformer-XL backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Dev set single-model: SQuAD1.1 EM/F1 = 89.7 / 95.1; SQuAD2.0 EM/F1 = 87.9 / 90.6. Test set (leaderboard single-model reported): SQuAD2.0 EM/F1 = 87.926 / 90.689; SQuAD1.1 test reported earlier version EM/F1 = 89.898 / 95.080.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>BERT dev SQuAD1.1 EM/F1 = 84.1 / 90.9; BERT dev SQuAD2.0 EM/F1 = 78.98 / 81.77; RoBERTa dev SQuAD1.1 EM/F1 = 88.9 / 94.6; RoBERTa dev SQuAD2.0 EM/F1 = 86.5 / 89.4.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>XLNet yields substantial absolute gains over BERT (SQuAD1.1 EM +5.6 points, F1 +4.2 points; SQuAD2.0 EM +8.92 pts, F1 +8.83 pts) and modest gains over RoBERTa (e.g., SQuAD1.1 EM +0.8, F1 +0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No specific QA failure modes detailed beyond general limitations: large-scale training still underfits and requires careful hyperparameters; authors note permutation LM makes optimization harder (hence partial prediction K used to ease optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation (Table 6) shows XLNet-Base improves SQuAD2.0 F1/EM over BERT-Base and over DAE+Transformer-XL baseline (XLNet-Base (K=7) F1 81.33 / EM 78.46 vs BERT-Base F1 76.30 / EM 73.66 and DAE+Transformer-XL F1 79.56 / EM 76.80), indicating permutation objective contributes beyond architecture alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'XLNet: Generalized Autoregressive Pretraining for Language Understanding', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3496.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3496.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLNet (NLI / GLUE - RTE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XLNet evaluated on natural language inference (RTE) within GLUE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>XLNet evaluated on RTE (recognizing textual entailment) as part of GLUE, measuring capability on strict entailment-style reasoning and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLNet (Large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Permutation LM pretraining with two-stream attention and Transformer-XL; finetuned on GLUE tasks (standard classification or pairwise ranking where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RTE (Recognizing Textual Entailment) - GLUE</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Natural language inference benchmark requiring judges whether a hypothesis is entailed by, contradicted by, or neutral with respect to a premise; here measured as entailment classification.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Finetuning XLNet on RTE classification within GLUE after pretraining with permutation LM and Transformer-XL enhancements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>XLNet single-task dev RTE accuracy reported as 85.9 (single-model dev results in Table 5). In multi-task/ensemble submissions XLNet* achieves up to 88.5 on test leaderboard (multi-model/ensemble settings).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>BERT dev RTE 70.4; RoBERTa reported 86.6 (single-task dev) and higher for some ensemble settings; MT-DNN ensemble 86.3 on test leaderboard in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Substantial improvement over BERT (single-task dev: +15.5 points). Compared to RoBERTa, XLNet is comparable/slightly lower on some single-run dev numbers (XLNet 85.9 vs RoBERTa 86.6) but achieves competitive or superior results in ensemble/multi-task submissions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No detailed failure cases on NLI are reported, though GLUE results show task-dependent variance; authors note that gains vary by dataset size and that for tasks with abundant supervised data, improvements remain but are smaller.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations indicate both permutation LM and Transformer-XL backbone contribute; Table 6 shows XLNet-Base improves on MNLI and other GLUE subsets over BERT-Base and DAE+Transformer-XL, suggesting permutation objective increases coverage of context dependencies important for inference tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'XLNet: Generalized Autoregressive Pretraining for Language Understanding', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3496.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3496.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-XL (DAE baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-XL trained with denoising auto-encoding (DAE) objective as a baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline combining the Transformer-XL architecture with BERT-style denoising auto-encoding (masking) objective to isolate the contribution of the permutation objective versus the architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformer-xl: Attentive language models beyond a fixed-length context.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-XL + DAE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-XL backbone (segment recurrence and relative positional encodings) trained with BERT's denoising auto-encoding (masked-language modeling) objective and bidirectional input pipeline to form a fair architectural baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Various downstream reasoning-related tasks used in ablation (RACE, SQuAD2.0, MNLI, SST-2)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Standard reading-comprehension, QA, natural language inference and classification tasks used to probe reasoning and understanding capability.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>DAE (masked language modeling) objective on Transformer-XL architecture with bidirectional data pipeline; used to test whether architecture alone accounts for gains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 6 (median of 5 runs) reports: RACE ~65.03, SQuAD2.0 F1/EM 79.56 / 76.80, MNLI m/mm 84.88/84.45, SST-2 92.60.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to BERT-Base (RACE 64.3, SQuAD2.0 F1/EM 76.30/73.66, MNLI 84.34/84.65, SST-2 92.78).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Small improvements over BERT-Base on several tasks (e.g., SQuAD2.0 F1 +3.26, EM +3.14; RACE +0.73), indicating Transformer-XL architecture by itself offers benefits but does not fully explain XLNet's gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>DAE+Transformer-XL performs better than BERT in some cases but still is outperformed by XLNet (permutation objective) suggesting the permutation objective is a key contributor; the paper reports optimization difficulty for permutation LM but does not report particular failure cases for this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Used in ablation (Table 6) to show that both architecture (Transformer-XL) and objective (permutation LM) contribute: XLNet-Base (K=7) outperforms DAE+Transformer-XL on RACE and SQuAD2.0, implying permutation LM adds value beyond architecture changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'XLNet: Generalized Autoregressive Pretraining for Language Understanding', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3496.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3496.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT is a denoising auto-encoding pretraining approach using masked-language modeling that enables bidirectional context but assumes independence among masked tokens and introduces pretrain-finetune discrepancy via [MASK] tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bert: Pre-training of deep bidirectional transformers for language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (Base / Large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Denoising autoencoder-style pretraining (masked language modeling) using a Transformer encoder; BERT-Base and BERT-Large are standard baselines throughout the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Various downstream reasoning-related tasks (RACE, SQuAD, GLUE/NLI)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks for reading comprehension, question answering, natural language inference and other language understanding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Masked language modeling with [MASK] tokens, next-sentence prediction (optionally).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported baselines in tables: e.g., on RACE 72.0 (Table 2), SQuAD1.1 dev EM/F1 84.1/90.9, SQuAD2.0 dev EM/F1 78.98/81.77, GLUE RTE dev 70.4, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>N/A (BERT is baseline comparator in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper discusses conceptual limitations: independence assumption between masked tokens (BERT factorizes joint conditional assuming independence), pretrain-finetune discrepancy due to [MASK], inability to model joint probability via product rule; empirically XLNet outperforms BERT across many tasks, attributed to permutation LM and autoregressive formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper provides formal analysis (Appendix A.5) showing XLNet covers more target-context dependency pairs than BERT, hence more effective training signals for capturing dependencies relevant to reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'XLNet: Generalized Autoregressive Pretraining for Language Understanding', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bert: Pre-training of deep bidirectional transformers for language understanding. <em>(Rating: 2)</em></li>
                <li>Transformer-xl: Attentive language models beyond a fixed-length context. <em>(Rating: 2)</em></li>
                <li>Roberta: A robustly optimized bert pretraining approach. <em>(Rating: 1)</em></li>
                <li>Neural autoregressive distribution estimation. <em>(Rating: 1)</em></li>
                <li>Improving language understanding by generative pre-training. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3496",
    "paper_id": "paper-e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "XLNet (RACE)",
            "name_full": "XLNet evaluated on the RACE reading-comprehension benchmark",
            "brief_description": "XLNet (a permutation-based generalized autoregressive pretrained Transformer with two-stream attention and Transformer-XL components) evaluated on RACE, a challenging multi-choice reading-comprehension dataset that requires reasoning over long passages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "XLNet (Large)",
            "model_description": "Permutation language modeling objective (permutations of factorization order) with two-stream self-attention, integrated Transformer-XL components (relative positional encoding and segment recurrence), span-based partial prediction and bidirectional data pipeline. Trained at scale on BooksCorpus/Wikipedia and larger corpora in scaled experiments; architecture hyperparameters same as BERT-Large.",
            "model_size": null,
            "reasoning_task_name": "RACE (reading-comprehension)",
            "reasoning_task_description": "Multi-choice reading-comprehension dataset (exam-style questions) with relatively long passages (avg &gt;300 tokens) requiring understanding and reasoning over context; considered a challenging benchmark for explicit/long-context reasoning.",
            "method_or_intervention": "Permutation language modeling (maximize expected likelihood over permutations), two-stream attention for target-aware representations, Transformer-XL backbone (relative positional encodings + segment recurrence), partial/span-based prediction, bidirectional data pipeline, memory caching.",
            "performance": "XLNet (full-scale reported) test accuracy on RACE: 85.4% overall (Middle: 88.6%, High: 84.0%) as reported in Table 2.",
            "baseline_performance": "RoBERTa: 83.2% overall; BERT (prior best listed) 72.0% (earlier baseline rows show other prior methods: GPT 59.0%).",
            "improvement_over_baseline": "Absolute improvement of +2.2 percentage points over RoBERTa (85.4 vs 83.2) and large improvement over earlier BERT baseline (+13.4 points vs 72.0).",
            "limitations_or_failures": "Paper reports that XLNet still underfits training data after large-scale training (500K steps) indicating capacity/data mismatch; ablations show removal of memory/cache harms performance on long-context tasks like RACE; no explicit claims of solving formal logical reasoning—RACE is a reading-comprehension benchmark with varied reasoning demands.",
            "ablation_or_analysis": "Ablation (Table 6) shows: XLNet-Base (K=6) RACE dev ~66.66 vs BERT-Base 64.3 and DAE+Transformer-XL 65.03, indicating permutation LM and Transformer-XL backbone both contribute; removing memory (-memory) drops performance to 65.55; varying K (partial prediction) changes performance slightly; authors qualitatively attribute gains on long-context reasoning to Transformer-XL recurrence and relative encodings.",
            "uuid": "e3496.0",
            "source_info": {
                "paper_title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "XLNet (SQuAD)",
            "name_full": "XLNet evaluated on SQuAD1.1 / SQuAD2.0 question-answering",
            "brief_description": "XLNet applied to span-extraction question answering (SQuAD1.1 and SQuAD2.0), leveraging permutation LM pretraining and Transformer-XL components to improve answer-span prediction and answerability detection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "XLNet (Large)",
            "model_description": "Same XLNet architecture described above; finetuned for span-extraction QA with joint logistic regression for answerability (for SQuAD2.0) and span extraction loss. Uses full sequence length 512 during finetuning.",
            "model_size": null,
            "reasoning_task_name": "SQuAD1.1 and SQuAD2.0 (reading comprehension / span extraction)",
            "reasoning_task_description": "Extractive QA tasks requiring locating answer spans in context; SQuAD2.0 also includes unanswerable questions requiring answerability reasoning.",
            "method_or_intervention": "Finetuning XLNet with span extraction loss; joint logistic regression for answerability (SQuAD2.0); benefits from permutation LM pretraining and Transformer-XL backbone.",
            "performance": "Dev set single-model: SQuAD1.1 EM/F1 = 89.7 / 95.1; SQuAD2.0 EM/F1 = 87.9 / 90.6. Test set (leaderboard single-model reported): SQuAD2.0 EM/F1 = 87.926 / 90.689; SQuAD1.1 test reported earlier version EM/F1 = 89.898 / 95.080.",
            "baseline_performance": "BERT dev SQuAD1.1 EM/F1 = 84.1 / 90.9; BERT dev SQuAD2.0 EM/F1 = 78.98 / 81.77; RoBERTa dev SQuAD1.1 EM/F1 = 88.9 / 94.6; RoBERTa dev SQuAD2.0 EM/F1 = 86.5 / 89.4.",
            "improvement_over_baseline": "XLNet yields substantial absolute gains over BERT (SQuAD1.1 EM +5.6 points, F1 +4.2 points; SQuAD2.0 EM +8.92 pts, F1 +8.83 pts) and modest gains over RoBERTa (e.g., SQuAD1.1 EM +0.8, F1 +0.5).",
            "limitations_or_failures": "No specific QA failure modes detailed beyond general limitations: large-scale training still underfits and requires careful hyperparameters; authors note permutation LM makes optimization harder (hence partial prediction K used to ease optimization).",
            "ablation_or_analysis": "Ablation (Table 6) shows XLNet-Base improves SQuAD2.0 F1/EM over BERT-Base and over DAE+Transformer-XL baseline (XLNet-Base (K=7) F1 81.33 / EM 78.46 vs BERT-Base F1 76.30 / EM 73.66 and DAE+Transformer-XL F1 79.56 / EM 76.80), indicating permutation objective contributes beyond architecture alone.",
            "uuid": "e3496.1",
            "source_info": {
                "paper_title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "XLNet (NLI / GLUE - RTE)",
            "name_full": "XLNet evaluated on natural language inference (RTE) within GLUE",
            "brief_description": "XLNet evaluated on RTE (recognizing textual entailment) as part of GLUE, measuring capability on strict entailment-style reasoning and inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "XLNet (Large)",
            "model_description": "Permutation LM pretraining with two-stream attention and Transformer-XL; finetuned on GLUE tasks (standard classification or pairwise ranking where applicable).",
            "model_size": null,
            "reasoning_task_name": "RTE (Recognizing Textual Entailment) - GLUE",
            "reasoning_task_description": "Natural language inference benchmark requiring judges whether a hypothesis is entailed by, contradicted by, or neutral with respect to a premise; here measured as entailment classification.",
            "method_or_intervention": "Finetuning XLNet on RTE classification within GLUE after pretraining with permutation LM and Transformer-XL enhancements.",
            "performance": "XLNet single-task dev RTE accuracy reported as 85.9 (single-model dev results in Table 5). In multi-task/ensemble submissions XLNet* achieves up to 88.5 on test leaderboard (multi-model/ensemble settings).",
            "baseline_performance": "BERT dev RTE 70.4; RoBERTa reported 86.6 (single-task dev) and higher for some ensemble settings; MT-DNN ensemble 86.3 on test leaderboard in comparisons.",
            "improvement_over_baseline": "Substantial improvement over BERT (single-task dev: +15.5 points). Compared to RoBERTa, XLNet is comparable/slightly lower on some single-run dev numbers (XLNet 85.9 vs RoBERTa 86.6) but achieves competitive or superior results in ensemble/multi-task submissions.",
            "limitations_or_failures": "No detailed failure cases on NLI are reported, though GLUE results show task-dependent variance; authors note that gains vary by dataset size and that for tasks with abundant supervised data, improvements remain but are smaller.",
            "ablation_or_analysis": "Ablations indicate both permutation LM and Transformer-XL backbone contribute; Table 6 shows XLNet-Base improves on MNLI and other GLUE subsets over BERT-Base and DAE+Transformer-XL, suggesting permutation objective increases coverage of context dependencies important for inference tasks.",
            "uuid": "e3496.2",
            "source_info": {
                "paper_title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Transformer-XL (DAE baseline)",
            "name_full": "Transformer-XL trained with denoising auto-encoding (DAE) objective as a baseline",
            "brief_description": "A baseline combining the Transformer-XL architecture with BERT-style denoising auto-encoding (masking) objective to isolate the contribution of the permutation objective versus the architecture.",
            "citation_title": "Transformer-xl: Attentive language models beyond a fixed-length context.",
            "mention_or_use": "use",
            "model_name": "Transformer-XL + DAE",
            "model_description": "Transformer-XL backbone (segment recurrence and relative positional encodings) trained with BERT's denoising auto-encoding (masked-language modeling) objective and bidirectional input pipeline to form a fair architectural baseline.",
            "model_size": null,
            "reasoning_task_name": "Various downstream reasoning-related tasks used in ablation (RACE, SQuAD2.0, MNLI, SST-2)",
            "reasoning_task_description": "Standard reading-comprehension, QA, natural language inference and classification tasks used to probe reasoning and understanding capability.",
            "method_or_intervention": "DAE (masked language modeling) objective on Transformer-XL architecture with bidirectional data pipeline; used to test whether architecture alone accounts for gains.",
            "performance": "Table 6 (median of 5 runs) reports: RACE ~65.03, SQuAD2.0 F1/EM 79.56 / 76.80, MNLI m/mm 84.88/84.45, SST-2 92.60.",
            "baseline_performance": "Compared to BERT-Base (RACE 64.3, SQuAD2.0 F1/EM 76.30/73.66, MNLI 84.34/84.65, SST-2 92.78).",
            "improvement_over_baseline": "Small improvements over BERT-Base on several tasks (e.g., SQuAD2.0 F1 +3.26, EM +3.14; RACE +0.73), indicating Transformer-XL architecture by itself offers benefits but does not fully explain XLNet's gains.",
            "limitations_or_failures": "DAE+Transformer-XL performs better than BERT in some cases but still is outperformed by XLNet (permutation objective) suggesting the permutation objective is a key contributor; the paper reports optimization difficulty for permutation LM but does not report particular failure cases for this baseline.",
            "ablation_or_analysis": "Used in ablation (Table 6) to show that both architecture (Transformer-XL) and objective (permutation LM) contribute: XLNet-Base (K=7) outperforms DAE+Transformer-XL on RACE and SQuAD2.0, implying permutation LM adds value beyond architecture changes.",
            "uuid": "e3496.3",
            "source_info": {
                "paper_title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "BERT (analysis)",
            "name_full": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "brief_description": "BERT is a denoising auto-encoding pretraining approach using masked-language modeling that enables bidirectional context but assumes independence among masked tokens and introduces pretrain-finetune discrepancy via [MASK] tokens.",
            "citation_title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
            "mention_or_use": "mention",
            "model_name": "BERT (Base / Large)",
            "model_description": "Denoising autoencoder-style pretraining (masked language modeling) using a Transformer encoder; BERT-Base and BERT-Large are standard baselines throughout the paper.",
            "model_size": null,
            "reasoning_task_name": "Various downstream reasoning-related tasks (RACE, SQuAD, GLUE/NLI)",
            "reasoning_task_description": "Benchmarks for reading comprehension, question answering, natural language inference and other language understanding tasks.",
            "method_or_intervention": "Masked language modeling with [MASK] tokens, next-sentence prediction (optionally).",
            "performance": "Reported baselines in tables: e.g., on RACE 72.0 (Table 2), SQuAD1.1 dev EM/F1 84.1/90.9, SQuAD2.0 dev EM/F1 78.98/81.77, GLUE RTE dev 70.4, etc.",
            "baseline_performance": null,
            "improvement_over_baseline": "N/A (BERT is baseline comparator in the paper).",
            "limitations_or_failures": "Paper discusses conceptual limitations: independence assumption between masked tokens (BERT factorizes joint conditional assuming independence), pretrain-finetune discrepancy due to [MASK], inability to model joint probability via product rule; empirically XLNet outperforms BERT across many tasks, attributed to permutation LM and autoregressive formulation.",
            "ablation_or_analysis": "Paper provides formal analysis (Appendix A.5) showing XLNet covers more target-context dependency pairs than BERT, hence more effective training signals for capturing dependencies relevant to reasoning.",
            "uuid": "e3496.4",
            "source_info": {
                "paper_title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                "publication_date_yy_mm": "2019-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
            "rating": 2
        },
        {
            "paper_title": "Transformer-xl: Attentive language models beyond a fixed-length context.",
            "rating": 2
        },
        {
            "paper_title": "Roberta: A robustly optimized bert pretraining approach.",
            "rating": 1
        },
        {
            "paper_title": "Neural autoregressive distribution estimation.",
            "rating": 1
        },
        {
            "paper_title": "Improving language understanding by generative pre-training.",
            "rating": 1
        }
    ],
    "cost": 0.014815249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>XLNet: Generalized Autoregressive Pretraining for Language Understanding</h1>
<p>Zhilin Yang ${ }^{<em> 1}$, Zihang Dai ${ }^{</em> 12}$, Yiming Yang ${ }^{1}$, Jaime Carbonell ${ }^{1}$, Ruslan Salakhutdinov ${ }^{1}$, Quoc V. Le ${ }^{2}$<br>${ }^{1}$ Carnegie Mellon University, ${ }^{2}$ Google AI Brain Team<br>{zhiliny,dzihang, yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com</p>
<h4>Abstract</h4>
<p>With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking. ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Unsupervised representation learning has been highly successful in the domain of natural language processing $[7,22,27,28,10]$. Typically, these methods first pretrain neural networks on large-scale unlabeled text corpora, and then finetune the models or representations on downstream tasks. Under this shared high-level idea, different unsupervised pretraining objectives have been explored in literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an autoregressive model [7, 27, 28]. Specifically, given a text sequence $\mathbf{x}=\left(x_{1}, \cdots, x_{T}\right)$, AR language modeling factorizes the likelihood into a forward product $p(\mathbf{x})=\prod_{t=1}^{T} p\left(x_{t} \mid \mathbf{x}_{<t}\right)$ or a backward one $p(\mathbf{x})=\prod_{t=T}^{t} p\left(x_{t} \mid \mathbf{x}_{>t}\right)$. A parametric model (e.g. a neural network) is trained to model each conditional distribution. Since an AR language model is only trained to encode a uni-directional context (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the contrary, downstream language understanding tasks often require bidirectional context information. This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. A notable example is BERT [10], which has been the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>bidirectional contexts for reconstruction. As an immediate benefit, this closes the aforementioned bidirectional information gap in AR language modeling, leading to improved performance. However, the artificial symbols like [MASK] used by BERT during pretraining are absent from real data at finetuning time, resulting in a pretrain-finetune discrepancy. Moreover, since the predicted tokens are masked in the input, BERT is not able to model the joint probability using the product rule as in AR language modeling. In other words, BERT assumes the predicted tokens are independent of each other given the unmasked tokens, which is oversimplified as high-order, long-range dependency is prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose XLNet, a generalized autoregressive method that leverages the best of both AR language modeling and AE while avoiding their limitations.</p>
<ul>
<li>Firstly, instead of using a fixed forward or backward factorization order as in conventional AR models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context.</li>
<li>Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence, XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to. Meanwhile, the autoregressive objective also provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT.
In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.</li>
<li>Inspired by the latest advancements in AR language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence.</li>
<li>Naively applying a Transformer(-XL) architecture to permutation-based language modeling does not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we propose to reparameterize the Transformer(-XL) network to remove the ambiguity.
Empirically, under comparable experiment setting, XLNet consistently outperforms BERT [10] on a wide spectrum of problems including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task.
Related Work The idea of permutation-based AR modeling has been explored in [32, 12], but there are several key differences. Firstly, previous models aim to improve density estimation by baking an "orderless" inductive bias into the model while XLNet is motivated by enabling AR language models to learn bidirectional contexts. Technically, to construct a valid target-aware prediction distribution, XLNet incorporates the target position into the hidden state via two-stream attention while previous permutation-based AR models relied on implicit position awareness inherent to their MLP architectures. Finally, for both orderless NADE and XLNet, we would like to emphasize that "orderless" does not mean that the input sequence can be randomly permuted but that the model allows for different factorization orders of the distribution.
Another related idea is to perform autoregressive denoising in the context of text generation [11], which only considers a fixed order though.</li>
</ul>
<h1>2 Proposed Method</h1>
<h3>2.1 Background</h3>
<p>In this section, we first review and compare the conventional AR language modeling and BERT for language pretraining. Given a text sequence $\mathbf{x}=\left[x_{1}, \cdots, x_{T}\right]$, AR language modeling performs pretraining by maximizing the likelihood under the forward autoregressive factorization:</p>
<p>$$
\max <em _theta="\theta">{\theta} \quad \log p</em>}(\mathbf{x})=\sum_{t=1}^{T} \log p_{\theta}\left(x_{t} \mid \mathbf{x<em t="1">{&lt;t}\right)=\sum</em>}^{T} \log \frac{\exp \left(h_{\theta}\left(\mathbf{x<em t="t">{1: t-1}\right)^{\top} e\left(x</em>
$$}\right)\right)}{\sum_{x^{\prime}} \exp \left(h_{\theta}\left(\mathbf{x}_{1: t-1}\right)^{\top} e\left(x^{\prime}\right)\right)</p>
<p>where $h_{\theta}\left(\mathbf{x}_{1: t-1}\right)$ is a context representation produced by neural models, such as RNNs or Transformers, and $e(x)$ denotes the embedding of $x$. In comparison, BERT is based on denoising auto-encoding. Specifically, for a text sequence $\mathbf{x}$, BERT first constructs a corrupted version $\hat{\mathbf{x}}$ by randomly setting a portion (e.g. 15%) of tokens in $\mathbf{x}$ to a special symbol [MASK]. Let the masked tokens be $\hat{\mathbf{x}}$. The training objective is to reconstruct $\hat{\mathbf{x}}$ from $\hat{\mathbf{x}}$:</p>
<p>$\max_{\theta} \quad \log p_{\theta}(\hat{\mathbf{x}} \mid \hat{\mathbf{x}}) \approx \sum_{t=1}^{T} m_{t} \log p_{\theta}\left(x_{t} \mid \hat{\mathbf{x}}\right)=\sum_{t=1}^{T} m_{t} \log \frac{\exp \left(H_{\theta}(\hat{\mathbf{x}})<em t="t">{t}^{\top} e\left(x</em>,$ (2)}\right)\right)}{\sum_{x^{\prime}} \exp \left(H_{\theta}(\hat{\mathbf{x}})_{t}^{\top} e\left(x^{\prime}\right)\right)</p>
<p>where $m_{t}=1$ indicates $x_{t}$ is masked, and $H_{\theta}$ is a Transformer that maps a length- $T$ text sequence $\mathbf{x}$ into a sequence of hidden vectors $H_{\theta}(\mathbf{x})=\left[H_{\theta}(\mathbf{x})<em _theta="\theta">{1}, H</em>)}(\mathbf{x<em _theta="\theta">{2}, \cdots, H</em>\right]$. The pros and cons of the two pretraining objectives are compared in the following aspects:}(\mathbf{x})_{T</p>
<ul>
<li>Independence Assumption: As emphasized by the $\approx$ sign in Eq. (2), BERT factorizes the joint conditional probability $p(\hat{\mathbf{x}} \mid \hat{\mathbf{x}})$ based on an independence assumption that all masked tokens $\hat{\mathbf{x}}$ are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes $p_{\theta}(\mathbf{x})$ using the product rule that holds universally without such an independence assumption.</li>
<li>Input noise: The input to BERT contains artificial symbols like [MASK] that never occur in downstream tasks, which creates a pretrain-finetune discrepancy. Replacing [MASK] with original tokens as in [10] does not solve the problem because original tokens can be only used with a small probability - otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling does not rely on any input corruption and does not suffer from this issue.</li>
<li>Context dependency: The AR representation $h_{\theta}\left(\mathbf{x}<em _theta="\theta">{1: t-1}\right)$ is only conditioned on the tokens up to position $t$ (i.e. tokens to the left), while the BERT representation $H</em>$ has access to the contextual information on both sides. As a result, the BERT objective allows the model to be pretrained to better capture bidirectional context.}(\mathbf{x})_{t</li>
</ul>
<h1>2.2 Objective: Permutation Language Modeling</h1>
<p>According to the comparison above, AR language modeling and BERT possess their unique advantages over the other. A natural question to ask is whether there exists a pretraining objective that brings the advantages of both while avoiding their weaknesses.</p>
<p>Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective that not only retains the benefits of AR models but also allows models to capture bidirectional contexts. Specifically, for a sequence $\mathbf{x}$ of length $T$, there are $T$ ! different orders to perform a valid autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders, in expectation, the model will learn to gather information from all positions on both sides.</p>
<p>To formalize the idea, let $\mathcal{Z}<em t="t">{T}$ be the set of all possible permutations of the length- $T$ index sequence $[1,2, \ldots, T]$. We use $z</em>}$ and $\mathbf{z<em T="T">{&lt;t}$ to denote the $t$-th element and the first $t-1$ elements of a permutation $\mathbf{z} \in \mathcal{Z}</em>$. Then, our proposed permutation language modeling objective can be expressed as follows:</p>
<p>$$
\max <em _mathbf_z="\mathbf{z">{\theta} \quad \mathbb{E}</em>} \sim \mathcal{Z<em t="1">{T}}\left[\sum</em>\right)\right]
$$}^{T} \log p_{\theta}\left(x_{z_{t}} \mid \mathbf{x}_{\mathbf{z}&lt;t</p>
<p>Essentially, for a text sequence $\mathbf{x}$, we sample a factorization order $\mathbf{z}$ at a time and decompose the likelihood $p_{\theta}(\mathbf{x})$ according to factorization order. Since the same model parameter $\theta$ is shared across all factorization orders during training, in expectation, $x_{t}$ has seen every possible element $x_{i} \neq x_{t}$ in the sequence, hence being able to capture the bidirectional context. Moreover, as this objective fits into the AR framework, it naturally avoids the independence assumption and the pretrain-finetune discrepancy discussed in Section 2.1.
Remark on Permutation The proposed objective only permutes the factorization order, not the sequence order. In other words, we keep the original sequence order, use the positional encodings corresponding to the original sequence, and rely on a proper attention mask in Transformers to achieve permutation of the factorization order. Note that this choice is necessary, since the model will only encounter text sequences with the natural order during finetuning.
To provide an overall picture, we show an example of predicting the token $x_{3}$ given the same input sequence $\mathbf{x}$ but under different factorization orders in the Appendix A. 7 with Figure 4.</p>
<p>2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a): Content stream attention, which is the same as the standard self-attention. (b): Query stream attention, which does not have access information about the content $x_{z_{t}}$. (c): Overview of the permutation language modeling training with two-stream attention.</p>
<p>While the permutation language modeling objective has desired properties, naive implementation with standard Transformer parameterization may not work. To see the problem, assume we parameterize the next-token distribution $p_{\theta}\left(X_{z_{t}} \mid \mathbf{x}<em _t="&lt;t">{\mathbf{z}</em>}}\right)$ using the standard Softmax formulation, i.e., $p_{\theta}\left(X_{z_{t}}=\right.$ $\left.x \mid \mathbf{x<em _t="&lt;t">{\mathbf{z}</em>}}\right)=\frac{\exp \left(e\left(x\right)^{\top} h_{\theta}\left(\mathbf{x<em _t="&lt;t">{\mathbf{z}</em>}}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{\top} h_{\theta}\left(\mathbf{x<em _t="&lt;t">{\mathbf{z}</em>}}\right)\right)}$, where $h_{\theta}\left(\mathbf{x<em _t="&lt;t">{\mathbf{z}</em>}}\right)$ denotes the hidden representation of $\mathbf{x<em _t="&lt;t">{\mathbf{z}</em>}}$ produced by the shared Transformer network after proper masking. Now notice that the representation $h_{\theta}\left(\mathbf{x<em _t="&lt;t">{\mathbf{z}</em>$. Consequently, the same distribution is predicted regardless of the target position, which is not able to learn useful representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to re-parameterize the next-token distribution to be target position aware:}}\right)$ does not depend on which position it will predict, i.e., the value of $z_{t</p>
<p>$$
p_{\theta}\left(X_{z_{t}}=x \mid \mathbf{x}<em _t="&lt;t">{z</em>}}\right)=\frac{\exp \left(e\left(x\right)^{\top} g_{\theta}\left(\mathbf{x<em _t="&lt;t">{\mathbf{z}</em>}}, z_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{\top} g_{\theta}\left(\mathbf{x<em _t="&lt;t">{\mathbf{z}</em>
$$}}, z_{t}\right)\right)</p>
<p>where $g_{\theta}\left(\mathbf{x}<em _t="&lt;t">{\mathbf{z}</em>$ as input.}}, z_{t}\right)$ denotes a new type of representations which additionally take the target position $z_{t</p>
<p>Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity in target prediction, how to formulate $g_{\theta}\left(\mathbf{x}<em _t="&lt;t">{\mathbf{z}</em>}}, z_{t}\right)$ remains a non-trivial problem. Among other possibilities, we propose to "stand" at the target position $z_{t}$ and rely on the position $z_{t}$ to gather information from the context $\mathbf{x<em _t="&lt;t">{\mathbf{z}</em>}}$ through attention. For this parameterization to work, there are two requirements that are contradictory in a standard Transformer architecture: (1) to predict the token $x_{z_{t}}, g_{\theta}\left(\mathbf{x<em _wzxhzdk:3_t_="<t}}, z_{t}\right)$ should only use the position $z_{t}$ and not the content $x_{z_{t}}$, otherwise the objective becomes trivial; (2) to predict the other tokens $x_{z_{j}}$ with $j>t," g__theta="g_{\theta">{\mathbf{z}</em>}\left(\mathbf{x<em _t="&lt;t">{\mathbf{z}</em>$ to provide full contextual information. To resolve such a contradiction, we propose to use two sets of hidden representations instead of one:}}, z_{t}\right)$ should also encode the content $x_{z_{t}</p>
<ul>
<li>The content representation $h_{\theta}\left(\mathbf{x}<em _t="&lt;t">{\mathbf{z}</em>$ itself.}}\right)$, or abbreviated as $h_{z_{t}}$, which serves a similar role to the standard hidden states in Transformer. This representation encodes both the context and $x_{z_{t}</li>
<li>The query representation $g_{\theta}\left(\mathbf{x}<em _t="&lt;t">{\mathbf{z}</em>}}, z_{t}\right)$, or abbreviated as $g_{z_{t}}$, which only has access to the contextual information $\mathbf{x<em _t="&lt;t">{\mathbf{z}</em>$, as discussed above.}}$ and the position $z_{t}$, but not the content $x_{z_{t}</li>
</ul>
<p>Computationally, the first layer query stream is initialized with a trainable vector, i.e. $g_{t}^{(0)}=w$, while the content stream is set to the corresponding word embedding, i.e. $h_{i}^{(0)}=e\left(x_{i}\right)$. For each self-attention layer $m=1, \ldots, M$, the two streams of representations are schematically ${ }^{2}$ updated</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>with a shared set of parameters as follows (illustrated in Figures 1 (a) and (b)):</p>
<p>$g_{z_{t}}^{(m)} \leftarrow \operatorname{Attention}\left(\mathrm{Q}=g_{z_{t}}^{(m-1)}, \mathrm{KV}=\mathbf{h}<em _t="&lt;t">{\hat{s}</em>\right)$
$h_{z_{t}}^{(m)} \leftarrow \operatorname{Attention}\left(\mathrm{Q}=h_{z_{t}}^{(m-1)}, \mathrm{KV}=\mathbf{h}}}^{(m-1)} ; \theta\right), \quad\left(\right.$ query stream: use $z_{t}$ but cannot see $\left.x_{z_{t}<em _wzxhzdk:4_c="<t}}^{(m-1)} ; \theta\right), \quad\left(\right.$ content stream: use both $z_{t}$ and $\left.x_{z_{t}}\right)$.
where $\mathrm{Q}, \mathrm{K}, \mathrm{V}$ denote the query, key, and value in an attention operation [33]. The update rule of the content representations is exactly the same as the standard self-attention, so during finetuning, we can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally, we can use the last-layer query representation $g_{z_{t}}^{(M)}$ to compute Eq. (4).
Partial Prediction While the permutation language modeling objective (3) has several benefits, it is a much more challenging optimization problem due to the permutation and causes slow convergence in preliminary experiments. To reduce the optimization difficulty, we choose to only predict the last tokens in a factorization order. Formally, we split $\mathbf{z}$ into a non-target subsequence $\mathbf{z}_{\leq c}$ and a target subsequence $\mathbf{z}_{>c">{\hat{s}</em>$, where $c$ is the cutting point. The objective is to maximize the log-likelihood of the target subsequence conditioned on the non-target subsequence, i.e.,</p>
<p>$$
\max <em _mathbf_z="\mathbf{z">{\theta} \quad \mathbb{E}</em>} \sim \mathcal{Z<em _theta="\theta">{T}}\left[\log p</em>}\left(\mathbf{x<em _c="&gt;c">{\mathbf{z}</em>}} \mid \mathbf{x<em _leq="\leq" c="c">{\mathbf{z}</em>}}\right)\right]=\mathbb{E<em T="T">{\mathbf{z} \sim \mathcal{Z}</em>}}\left[\sum_{t=c+1}^{|\mathbf{z}|} \log p_{\theta}\left(x_{z_{t}} \mid \mathbf{x<em _t="&lt;t">{\mathbf{z}</em>\right)\right]
$$}</p>
<p>Note that $\mathbf{z}_{&gt;c}$ is chosen as the target because it possesses the longest context in the sequence given the current factorization order $\mathbf{z}$. A hyperparameter $K$ is used such that about $1 / K$ tokens are selected for predictions; i.e., $|\mathbf{z}| /(|\mathbf{z}|-c) \approx K$. For unselected tokens, their query representations need not be computed, which saves speed and memory.</p>
<h1>2.4 Incorporating Ideas from Transformer-XL</h1>
<p>Since our objective function fits in the AR framework, we incorporate the state-of-the-art AR language model, Transformer-XL [9], into our pretraining framework, and name our method after it. We integrate two important techniques in Transformer-XL, namely the relative positional encoding scheme and the segment recurrence mechanism. We apply relative positional encodings based on the original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden states from previous segments. Without loss of generality, suppose we have two segments taken from a long sequence $\mathbf{s}$; i.e., $\hat{\mathbf{x}}=\mathbf{s}<em 2="2" T="T" T_1:="T+1:">{1: T}$ and $\mathbf{x}=\mathbf{s}</em>$, the attention update with memory can be written as}$. Let $\hat{\mathbf{z}}$ and $\mathbf{z}$ be permutations of $[1 \cdots T]$ and $[T+1 \cdots 2 T]$ respectively. Then, based on the permutation $\hat{\mathbf{z}}$, we process the first segment, and then cache the obtained content representations $\hat{\mathbf{h}}^{(m)}$ for each layer $m$. Then, for the next segment $\mathbf{x</p>
<p>$$
h_{z_{t}}^{(m)} \leftarrow \operatorname{Attention}\left(\mathrm{Q}=h_{z_{t}}^{(m-1)}, \mathrm{KV}=\left[\hat{\mathbf{h}}^{(m-1)}, \mathbf{h}_{\mathbf{z} \leq t}^{(m-1)}\right] ; \theta\right)
$$</p>
<p>where [.,.] denotes concatenation along the sequence dimension. Notice that positional encodings only depend on the actual positions in the original sequence. Thus, the above attention update is independent of $\hat{\mathbf{z}}$ once the representations $\hat{\mathbf{h}}^{(m)}$ are obtained. This allows caching and reusing the memory without knowing the factorization order of the previous segment. In expectation, the model learns to utilize the memory over all factorization orders of the last segment. The query stream can be computed in the same way. Finally, Figure 1 (c) presents an overview of the proposed permutation language modeling with two-stream attention (see Appendix A. 7 for more detailed illustration).</p>
<h3>2.5 Modeling Multiple Segments</h3>
<p>Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in question answering. We now discuss how we pretrain XLNet to model multiple segments in the autoregressive framework. During the pretraining phase, following BERT, we randomly sample two segments (either from the same context or not) and treat the concatenation of two segments as one sequence to perform permutation language modeling. We only reuse the memory that belongs to the same context. Specifically, the input to our model is the same as BERT: [CLS, A, SEP, B, SEP], where "SEP" and "CLS" are two special symbols and "A" and "B" are the two segments. Although</p>
<p>we follow the two-segment data format, XLNet-Large does not use the objective of next sentence prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.4).
Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment embedding to the word embedding at each position, we extend the idea of relative encodings from Transformer-XL to also encode the segments. Given a pair of positions $i$ and $j$ in the sequence, if $i$ and $j$ are from the same segment, we use a segment encoding $\mathbf{s}<em _="+">{i j}=\mathbf{s}</em>}$or otherwise $\mathbf{s<em -="-">{i j}=\mathbf{s}</em>}$, where $\mathbf{s<em -="-">{+}$and $\mathbf{s}</em>}$are learnable model parameters for each attention head. In other words, we only consider whether the two positions are within the same segment, as opposed to considering which specific segments they are from. This is consistent with the core idea of relative encodings; i.e., only modeling the relationships between positions. When $i$ attends to $j$, the segment encoding $\mathbf{s<em i="i" j="j">{i j}$ is used to compute an attention weight $a</em>}=\left(\mathbf{q<em i="i" j="j">{i}+\mathbf{b}\right)^{\top} \mathbf{s}</em>}$, where $\mathbf{q<em i="i" j="j">{i}$ is the query vector as in a standard attention operation and $\mathbf{b}$ is a learnable head-specific bias vector. Finally, the value $a</em>$ is added to the normal attention weight. There are two benefits of using relative segment encodings. First, the inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of finetuning on tasks that have more than two input segments, which is not possible using absolute segment encodings.</p>
<h1>2.6 Discussion</h1>
<p>Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e., only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT and XLNet, partial prediction plays a role of reducing optimization difficulty by only predicting tokens with sufficient context. However, the independence assumption discussed in Section 2.1 disables BERT to model dependency between targets.
To better understand the difference, let's consider a concrete example [New, York, is, a, city]. Suppose both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize $\log p($ New York $\mid$ is a city). Also suppose that XLNet samples the factorization order [is, a, city, New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:</p>
<p>$$
\begin{gathered}
\mathcal{J}<em _XLNet="{XLNet" _text="\text">{\text {BERT }}=\log p(\text { New } \mid \text { is a city })+\log p(\text { York } \mid \text { is a city }) \
\mathcal{J}</em>)
\end{gathered}
$$}}=\log p(\text { New } \mid \text { is a city })+\log p(\text { York } \mid \text { New, is a city </p>
<p>Notice that XLNet is able to capture the dependency between the pair (New, York), which is omitted by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and (York, city), it is obvious that XLNet always learns more dependency pairs given the same target and contains "denser" effective training signals.
For more formal analysis and further discussion, please refer to Appendix A.5.</p>
<h2>3 Experiments</h2>
<h3>3.1 Pretraining and Implementation</h3>
<p>Following BERT [10], we use the BooksCorpus [40] and English Wikipedia as part of our pretraining data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [26], ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics to aggressively filter out short or low-quality articles for ClueWeb 2012-B and Common Crawl, which results in 19GB and 110GB text respectively. After tokenization with SentencePiece [17], we obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total.
Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size. During pretraining, we always use a full sequence length of 512. Firstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks on BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the original BERT. Then, we scale up the training of XLNet-Large by using all the datasets described above. Specifically, we train on 512 TPU v3 chips for 500 K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192 , which takes about 5.5 days. It was</p>
<p>observed that the model still underfits the data at the end of training. Finally, we perform ablation study (section 3.4) based on the XLNet-Base-wikibooks.</p>
<p>Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set the partial prediction constant $K$ as 6 (see Section 2.3). Our finetuning procedure follows BERT [10] except otherwise specified ${ }^{3}$. We employ an idea of span-based prediction, where we first sample a length $L \in[1, \cdots, 5]$, and then randomly select a consecutive span of $L$ tokens as prediction targets within a context of $(K L)$ tokens.</p>
<p>We use a variety of natural language understanding datasets to evaluate the performance of our method. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.</p>
<h1>3.2 Fair Comparison with BERT</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">SQuAD1.1</th>
<th style="text-align: center;">SQuAD2.0</th>
<th style="text-align: center;">RACE</th>
<th style="text-align: center;">MNLI</th>
<th style="text-align: center;">QNLI</th>
<th style="text-align: center;">QQP</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;">MRPC</th>
<th style="text-align: center;">CoLA</th>
<th style="text-align: center;">STS-B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT-Large <br> (Best of 3)</td>
<td style="text-align: center;">$86.7 / 92.8$</td>
<td style="text-align: center;">$82.8 / 85.5$</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">90.2</td>
</tr>
<tr>
<td style="text-align: left;">XLNet-Large- <br> wikibooks</td>
<td style="text-align: center;">$88.2 / 94.0$</td>
<td style="text-align: center;">$85.1 / 87.8$</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">91.1</td>
</tr>
</tbody>
</table>
<p>Table 1: Fair comparison with BERT. All models are trained using the same data and hyperparameters as in BERT. We use the best of 3 BERT variants for comparison; i.e., the original BERT, BERT with whole word masking, and BERT without next sentence prediction.</p>
<p>Here, we first compare the performance of BERT and XLNet in a fair setting to decouple the effects of using more data and the improvement from BERT to XLNet. In Table 1, we compare (1) best performance of three different variants of BERT and (2) XLNet trained with the same data and hyperparameters. As we can see, trained on the same data with an almost identical training recipe, XLNet outperforms BERT by a sizable margin on all the considered datasets.</p>
<h3>3.3 Comparison with RoBERTa: Scaling Up</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">RACE</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Middle</th>
<th style="text-align: center;">High</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">NDCG@20</th>
<th style="text-align: center;">ERR@20</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT [28]</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">DRMM [13]</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">13.8</td>
</tr>
<tr>
<td style="text-align: left;">BERT [25]</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">KNRM [8]</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">14.9</td>
</tr>
<tr>
<td style="text-align: left;">BERT+DCMN ${ }^{+}$[38]</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">Conv [8]</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">18.1</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa [21]</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">BERT $^{\dagger}$</td>
<td style="text-align: center;">30.53</td>
<td style="text-align: center;">18.67</td>
</tr>
<tr>
<td style="text-align: left;">XLNet</td>
<td style="text-align: center;">$\mathbf{8 5 . 4}$</td>
<td style="text-align: center;">$\mathbf{8 8 . 6}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 0}$</td>
<td style="text-align: center;">XLNet</td>
<td style="text-align: center;">$\mathbf{3 1 . 1 0}$</td>
<td style="text-align: center;">$\mathbf{2 0 . 2 8}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task, and on ClueWeb09-B, a document ranking task. $*$ indicates using ensembles. $\dagger$ indicates our implementations. "Middle" and "High" in RACE are two subsets representing middle and high school difficulty levels. All BERT, RoBERTa, and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).</p>
<p>After the initial publication of our manuscript, a few other pretrained models were released such as RoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from 1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, we exclude ALBERT from the following results as it is hard to lead to scientific conclusions. To obtain relatively fair comparison with RoBERTa, the experiment in this section is based on full data and reuses the hyper-parameters of RoBERTa, as described in section 3.1.</p>
<p>The results are presented in Tables 2 (reading comprehension \&amp; document ranking), 3 (question answering), 4 (text classification) and 5 (natural language understanding), where XLNet generally outperforms BERT and RoBERTa. In addition, we make two more interesting observations:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">SQuAD2.0</th>
<th style="text-align: center;">EM</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">SQuAD1.1</th>
<th style="text-align: center;">EM</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dev set results (single model)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BERT [10]</td>
<td style="text-align: center;">78.98</td>
<td style="text-align: center;">81.77</td>
<td style="text-align: center;">BERT $\dagger$ [10]</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">90.9</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa [21]</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">RoBERTa [21]</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">94.6</td>
</tr>
<tr>
<td style="text-align: left;">XLNet</td>
<td style="text-align: center;">$\mathbf{8 7 . 9}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 6}$</td>
<td style="text-align: center;">XLNet</td>
<td style="text-align: center;">$\mathbf{8 9 . 7}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 1}$</td>
</tr>
</tbody>
</table>
<p>Test set results on leaderboard (single model, as of Dec 14, 2019)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">BERT [10]</th>
<th style="text-align: center;">80.005</th>
<th style="text-align: center;">83.061</th>
<th style="text-align: center;">BERT [10]</th>
<th style="text-align: center;">85.083</th>
<th style="text-align: center;">91.835</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RoBERTa [21]</td>
<td style="text-align: center;">86.820</td>
<td style="text-align: center;">89.795</td>
<td style="text-align: center;">BERT $[$ [10]</td>
<td style="text-align: center;">87.433</td>
<td style="text-align: center;">93.294</td>
</tr>
<tr>
<td style="text-align: left;">XLNet</td>
<td style="text-align: center;">$\mathbf{8 7 . 9 2 6}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 6 8 9}$</td>
<td style="text-align: center;">XLNet</td>
<td style="text-align: center;">$\mathbf{8 9 . 8 9 8} \ddagger$</td>
<td style="text-align: center;">$\mathbf{9 5 . 0 8 0} \ddagger$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results on SQuAD, a reading comprehension dataset. $\dagger$ marks our runs with the official code. * indicates ensembles. $\ddagger$ : We are not able to obtain the test results of our latest model on SQuAD1.1 from the organizers after submitting our result for more than one month, and thus report the results of an older version for the SQuAD1.1 test set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">IMDB</th>
<th style="text-align: center;">Yelp-2</th>
<th style="text-align: center;">Yelp-5</th>
<th style="text-align: center;">DBpedia</th>
<th style="text-align: center;">AG</th>
<th style="text-align: center;">Amazon-2</th>
<th style="text-align: center;">Amazon-5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CNN [15]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2.90</td>
<td style="text-align: center;">32.39</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">6.57</td>
<td style="text-align: center;">3.79</td>
<td style="text-align: center;">36.24</td>
</tr>
<tr>
<td style="text-align: left;">DPCNN [15]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2.64</td>
<td style="text-align: center;">30.58</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">6.87</td>
<td style="text-align: center;">3.32</td>
<td style="text-align: center;">34.81</td>
</tr>
<tr>
<td style="text-align: left;">Mixed VAT [31, 23]</td>
<td style="text-align: center;">4.32</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">4.95</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">ULMFiT [14]</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">2.16</td>
<td style="text-align: center;">29.98</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">5.01</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">BERT [35]</td>
<td style="text-align: center;">4.51</td>
<td style="text-align: center;">1.89</td>
<td style="text-align: center;">29.32</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2.63</td>
<td style="text-align: center;">34.17</td>
</tr>
<tr>
<td style="text-align: left;">XLNet</td>
<td style="text-align: center;">$\mathbf{3 . 2 0}$</td>
<td style="text-align: center;">$\mathbf{1 . 3 7}$</td>
<td style="text-align: center;">$\mathbf{2 7 . 0 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 0}$</td>
<td style="text-align: center;">$\mathbf{4 . 4 5}$</td>
<td style="text-align: center;">$\mathbf{2 . 1 1}$</td>
<td style="text-align: center;">$\mathbf{3 1 . 6 7}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison with state-of-the-art error rates on the test sets of several text classification datasets. All BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">MNLI</th>
<th style="text-align: center;">QNLI</th>
<th style="text-align: center;">QQP</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;">MRPC</th>
<th style="text-align: center;">CoLA</th>
<th style="text-align: center;">STS-B</th>
<th style="text-align: center;">WNLI</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Single-task single models on dev</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BERT [2]</td>
<td style="text-align: center;">$86.6 /-$</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa [21]</td>
<td style="text-align: center;">$90.2 / 90.2$</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">$\mathbf{8 6 . 6}$</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">$\mathbf{9 0 . 9}$</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">XLNet</td>
<td style="text-align: center;">$\mathbf{9 0 . 8 / 9 0 . 8}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 9}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 3}$</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">$\mathbf{9 7 . 0}$</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">$\mathbf{6 9 . 0}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 5}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Multi-task ensembles on test (from leaderboard as of Oct 28, 2019)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">MT-DNN* [20]</td>
<td style="text-align: center;">$87.9 / 87.4$</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">89.0</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa* [21]</td>
<td style="text-align: center;">$90.8 / 90.2$</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">89.0</td>
</tr>
<tr>
<td style="text-align: left;">XLNet*</td>
<td style="text-align: center;">$\mathbf{9 0 . 9 / 9 0 . 9}^{\dagger}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 0}^{\dagger}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 4}^{\dagger}$</td>
<td style="text-align: center;">$\mathbf{8 8 . 5}$</td>
<td style="text-align: center;">$\mathbf{9 7 . 1}^{\dagger}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 9}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 0}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Results on GLUE. * indicates using ensembles, and $\dagger$ denotes single-task results in a multi-task row. All dev results are the median of 10 runs. The upper section shows direct comparison on dev data and the lower section shows comparison with state-of-the-art results on the public leaderboard.</p>
<ul>
<li>For explicit reasoning tasks like SQuAD and RACE that involve longer context, the performance gain of XLNet is usually larger. This superiority at dealing with longer context could come from the Transformer-XL backbone in XLNet.</li>
<li>For classification tasks that already have abundant supervised examples such as MNLI ( $&gt;390 \mathrm{~K}$ ), Yelp ( $&gt;560 \mathrm{~K}$ ) and Amazon ( $&gt;3 \mathrm{M}$ ), XLNet still lead to substantial gains.</li>
</ul>
<h1>3.4 Ablation Study</h1>
<p>We perform an ablation study to understand the importance of each design choice based on four datasets with diverse characteristics. Specifically, there are three main aspects we hope to study:</p>
<ul>
<li>The effectiveness of the permutation language modeling objective alone, especially compared to the denoising auto-encoding objective used by BERT.</li>
<li>The importance of using Transformer-XL as the backbone neural architecture.</li>
<li>The necessity of some implementation details including span-based prediction, the bidirectional input pipeline, and next-sentence prediction.</li>
</ul>
<p>With these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implementation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidirectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the BooksCorpus. All results reported are the median of 5 runs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">#</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">RACE</th>
<th style="text-align: center;">SQuAD2.0</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MNLI</th>
<th style="text-align: center;">SST-2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">$\mathrm{m} / \mathrm{mm}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">BERT-Base</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">76.30</td>
<td style="text-align: center;">73.66</td>
<td style="text-align: center;">84.34/84.65</td>
<td style="text-align: center;">92.78</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">DAE + Transformer-XL</td>
<td style="text-align: center;">65.03</td>
<td style="text-align: center;">79.56</td>
<td style="text-align: center;">76.80</td>
<td style="text-align: center;">84.88/84.45</td>
<td style="text-align: center;">92.60</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">XLNet-Base $(K=7)$</td>
<td style="text-align: center;">66.05</td>
<td style="text-align: center;">81.33</td>
<td style="text-align: center;">78.46</td>
<td style="text-align: center;">85.84/85.43</td>
<td style="text-align: center;">92.66</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">XLNet-Base $(K=6)$</td>
<td style="text-align: center;">66.66</td>
<td style="text-align: center;">80.98</td>
<td style="text-align: center;">78.18</td>
<td style="text-align: center;">85.63/85.12</td>
<td style="text-align: center;">93.35</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">- memory</td>
<td style="text-align: center;">65.55</td>
<td style="text-align: center;">80.15</td>
<td style="text-align: center;">77.27</td>
<td style="text-align: center;">85.32/85.05</td>
<td style="text-align: center;">92.78</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">- span-based pred</td>
<td style="text-align: center;">65.95</td>
<td style="text-align: center;">80.61</td>
<td style="text-align: center;">77.91</td>
<td style="text-align: center;">85.49/85.02</td>
<td style="text-align: center;">93.12</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">- bidirectional data</td>
<td style="text-align: center;">66.34</td>
<td style="text-align: center;">80.65</td>
<td style="text-align: center;">77.87</td>
<td style="text-align: center;">85.31/84.99</td>
<td style="text-align: center;">92.66</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">+ next-sent pred</td>
<td style="text-align: center;">66.76</td>
<td style="text-align: center;">79.83</td>
<td style="text-align: center;">76.94</td>
<td style="text-align: center;">85.32/85.09</td>
<td style="text-align: center;">92.89</td>
</tr>
</tbody>
</table>
<p>Table 6: The results of BERT on RACE are taken from [38]. We run BERT on the other datasets using the official implementation and the same hyperparameter search space as XLNet. $K$ is a hyperparameter to control the optimization difficulty (see Section 2.3).</p>
<p>Examining rows 1 - 4 of Table 6, we can see both Transformer-XL and the permutation LM clearly contribute the superior performance of XLNet over BERT. Moreover, if we remove the memory caching mechanism (row 5), the performance clearly drops, especially for RACE which involves the longest context among the 4 tasks. In addition, rows 6 - 7 show that both span-based prediction and the bidirectional input pipeline play important roles in XLNet. Finally, we unexpectedly find the the next-sentence prediction objective proposed in the original BERT does not necessarily lead to an improvement in our setting. Hence, we exclude the next-sentence prediction objective from XLNet.
Finally, we also perform a qualitative study of the attention patterns, which is included in Appendix A. 6 due to page limit.</p>
<h1>4 Conclusions</h1>
<p>XLNet is a generalized AR pretraining method that uses a permutation language modeling objective to combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to work seamlessly with the AR objective, including integrating Transformer-XL and the careful design of the two-stream attention mechanism. XLNet achieves substantial improvement over previous pretraining objectives on various tasks.</p>
<h2>Acknowledgments</h2>
<p>The authors would like to thank Qizhe Xie and Adams Wei Yu for providing useful feedback on the project, Jamie Callan for providing the ClueWeb dataset, Youlong Cheng, Yanping Huang and Shibo Wang for providing ideas to improve our TPU implementation, Chenyan Xiong and Zhuyun Dai for clarifying the setting of the document ranking task. ZY and RS were supported by the Office of Naval Research grant N000141812861, the National Science Foundation (NSF) grant IIS1763562, the Nvidia fellowship, and the Siebel scholarship. ZD and YY were supported in part by NSF under the grant IIS-1546329 and by the DOE-Office of Science under the grant ASCR #KJ040201.</p>
<h2>References</h2>
<p>[1] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.
[2] Anonymous. Bam! born-again multi-task networks for natural language understanding. anonymous preprint under review, 2018.
[3] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853, 2018.</p>
<p>[4] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer neural networks. In Advances in Neural Information Processing Systems, pages 400-406, 2000.
[5] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. Clueweb09 data set, 2009.
[6] Common Crawl. Common crawl. URl: http://http://commoncrawl. org, 2019.
[7] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079-3087, 2015.
[8] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks for soft-matching n-grams in ad-hoc search. In Proceedings of the eleventh ACM international conference on web search and data mining, pages 126-134. ACM, 2018.
[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[11] William Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: better text generation via filling in the_. arXiv preprint arXiv:1801.07736, 2018.
[12] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for distribution estimation. In International Conference on Machine Learning, pages 881-889, 2015.
[13] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, pages 55-64. ACM, 2016.
[14] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018.
[15] Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text categorization. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 562-570, 2017.
[16] Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz. A surprisingly robust trick for winograd schema challenge. arXiv preprint arXiv:1905.06290, 2019.
[17] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.
[18] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.
[19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.
[20] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. arXiv preprint arXiv:1901.11504, 2019.
[21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
[22] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems, pages $6294-6305,2017$.
[23] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semisupervised text classification. arXiv preprint arXiv:1605.07725, 2016.
[24] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016.
[25] Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, and Dong Yu. Improving question answering with external knowledge. arXiv preprint arXiv:1902.00993, 2019.</p>
<p>[26] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword fifth edition, linguistic data consortium. Technical report, Technical Report. Linguistic Data Consortium, Philadelphia, Tech. Rep., 2011.
[27] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.
[28] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018.
[29] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.
[30] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.
[31] Devendra Singh Sachan, Manzil Zaheer, and Ruslan Salakhutdinov. Revisiting lstm networks for semi-supervised text classification via mixed objective function. 2018.
[32] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural autoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):71847220, 2016.
[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.
[34] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings of ICLR.
[35] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data augmentation. arXiv preprint arXiv:1904.12848, 2019.
[36] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR conference on research and development in information retrieval, pages 55-64. ACM, 2017.
[37] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.
[38] Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou. Dual comatching network for multi-choice reading comprehension. arXiv preprint arXiv:1901.09381, 2019.
[39] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in neural information processing systems, pages 649-657, 2015.
[40] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19-27, 2015.</p>
<h1>A Target-Aware Representation via Two-Stream Self-Attention</h1>
<h2>A. 1 A Concrete Example of How Standard LM Parameterization Fails</h2>
<p>In this section, we provide a concrete example to show how the standard language model parameterization fails under the permutation objective, as discussed in Section 2.3. Specifically, let's consider two different permutations $\mathbf{z}^{(1)}$ and $\mathbf{z}^{(2)}$ satisfying the following relationship</p>
<p>$$
\mathbf{z}<em _t="&lt;t">{&lt;t}^{(1)}=\mathbf{z}</em>}^{(2)}=\mathbf{z<em t="t">{&lt;t} \quad \text { but } \quad z</em>
$$}^{(1)}=i \neq j=z_{t}^{(2)</p>
<p>Then, substituting the two permutations respectively into the naive parameterization, we have</p>
<p>$$
\underbrace{p_{\theta}\left(X_{i}=x \mid \mathbf{x}<em _t="&lt;t">{\mathbf{z}</em>}}\right)<em t="t">{z</em>}^{(1)}=i, \mathbf{z<em _t="&lt;t">{&lt;t}^{(1)}=\mathbf{z}</em>}}=\underbrace{p_{\theta}\left(X_{j}=x \mid \mathbf{x<em _t="&lt;t">{\mathbf{z}</em>}}\right)<em t="t">{z</em>}^{(1)}=j, \mathbf{z<em _t="&lt;t">{&lt;t}^{(2)}=\mathbf{z}</em>}}=\frac{\exp \left(e\left(x\right)^{\top} h\left(\mathbf{x<em _t="&lt;t">{\mathbf{z}</em>}}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{\top} h\left(\mathbf{x<em _t="&lt;t">{\mathbf{z}</em>
$$}}\right)\right)</p>
<p>Effectively, two different target positions $i$ and $j$ share exactly the same model prediction. However, the ground-truth distribution of two positions should certainly be different.</p>
<h2>A. 2 Two-Stream Attention</h2>
<p>Here, we provide the implementation details of the two-stream attention with a Transformer-XL backbone.
Initial represetation:</p>
<p>$$
\forall t=1, \ldots, T: \quad h_{t}=e\left(x_{t}\right) \quad \text { and } \quad g_{t}=w
$$</p>
<p>Cached layer- $m$ content represetation (memory) from previous segment: $\tilde{\mathbf{h}}^{(m)}$
For the Transformer-XL layer $m=1, \cdots, M$, attention with relative positional encoding and position-wise feed-forward are consecutively employed to update the represetntations:</p>
<p>$$
\begin{aligned}
&amp; \forall t=1, \ldots, T: \quad \hat{h}<em t="t">{\hat{z}</em>}}^{(m)}=\operatorname{LayerNorm}\left(h_{\hat{z<em _hat_z="\hat{z">{t}}^{(m-1)}+\operatorname{RelAttn}\left(h</em><em _mathbf_z="\mathbf{z">{t}}^{(m-1)},\left[\tilde{\mathbf{h}}^{(m-1)}, \mathbf{h}</em>\right]\right)\right) \
&amp; h_{\hat{z}} \leq t}^{(m-1)<em _hat_z="\hat{z">{t}}^{(m)}=\operatorname{LayerNorm}\left(\hat{h}</em><em _hat_z="\hat{z">{t}}^{(m)}+\operatorname{PosFF}\left(\hat{h}</em><em _hat_z="\hat{z">{t}}^{(m)}\right)\right) \
&amp; \hat{g}</em><em _hat_z="\hat{z">{t}}^{(m)}=\operatorname{LayerNorm}\left(g</em><em _hat_z="\hat{z">{t}}^{(m-1)}+\operatorname{RelAttn}\left(g</em><em _mathbf_z="\mathbf{z">{t}}^{(m-1)},\left[\tilde{\mathbf{h}}^{(m-1)}, \mathbf{h}</em><em _hat_z="\hat{z">{&lt;t}}^{(m-1)}\right]\right)\right) \
&amp; g</em><em _hat_z="\hat{z">{t}}^{(m)}=\operatorname{LayerNorm}\left(\hat{g}</em><em _hat_z="\hat{z">{t}}^{(m)}+\operatorname{PosFF}\left(\hat{g}</em>\right)\right)
\end{aligned}
$$}_{t}}^{(m)</p>
<p>Target-aware prediction distribution:</p>
<p>$$
p_{\theta}\left(X_{\hat{z}<em _hat_z="\hat{z">{t}}=x \mid \mathbf{x}</em><em z__t="z_{t">{&lt;t}}\right)=\frac{\exp \left(e\left(x\right)^{\top} g</em>
$$}}^{(M)}\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{\top} g_{z_{t}}^{(M)}\right)</p>
<h2>A. 3 Datasets</h2>
<h2>A.3.1 RACE Dataset</h2>
<p>The RACE dataset [18] contains near 100K questions taken from the English exams for middle and high school Chinese students in the age range between 12 to 18, with the answers generated by human experts. This is one of the most difficult reading comprehension datasets that involve challenging reasoning questions. Moreover, the average length of the passages in RACE are longer than 300, which is significantly longer than other popular reading comprehension datasets such as SQuAD [29]. As a result, this dataset serves as a challenging benchmark for long text understanding. We use a sequence length of 512 during finetuning.</p>
<h2>A.3.2 SQuAD</h2>
<p>SQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [30] contains questions that always have a corresponding answer in the given passages, while SQuAD2.0 [29] introduces unanswerable questions. To finetune an XLNet on SQuAD2.0, we jointly apply a logistic regression loss for answerability prediction similar to classification tasks and a standard span extraction loss for question answering [10].</p>
<h1>A.3.3 Text classification Datasets</h1>
<p>Following previous work on text classification [39, 23], we evaluate XLNet on the following benchmarks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5.</p>
<h2>A.3.4 GLUE Dataset</h2>
<p>The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels are removed from the publicly released version, and all the practitioners must submit their predictions on the evaluation server to obtain test set results. In Table 5, we present results of multiple settings, including single-task and multi-task, as well as single models and ensembles. In the multi-task setting, we jointly train an XLNet on the four largest datasets-MNLI, SST-2, QNLI, and QQP-and finetune the network on the other datasets. Only single-task training is employed for the four large datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [20] for our test set submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a standard classification paradigm. For WNLI, we use the loss described in [16].</p>
<h2>A.3.5 ClueWeb09-B Dataset</h2>
<p>Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the performance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on 50 M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries without finetuning, and employ a kernel pooling network [36] to rank the documents.</p>
<h2>A. 4 Hyperparameters</h2>
<h2>A.4.1 Pretraining Hyperparameters</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Hparam</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Number of layers</td>
<td style="text-align: center;">24</td>
</tr>
<tr>
<td style="text-align: left;">Hidden size</td>
<td style="text-align: center;">1024</td>
</tr>
<tr>
<td style="text-align: left;">Number of attention heads</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">Attention head size</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">FFN inner hidden size</td>
<td style="text-align: center;">4096</td>
</tr>
<tr>
<td style="text-align: left;">Hidden Dropout</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">GeLU Dropout</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Attention dropout</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">Partial prediction $K$</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">Max sequence length</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: center;">8192</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: center;">$4 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: left;">Number of steps</td>
<td style="text-align: center;">500 K</td>
</tr>
<tr>
<td style="text-align: left;">Warmup steps</td>
<td style="text-align: center;">40,000</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate decay</td>
<td style="text-align: center;">linear</td>
</tr>
<tr>
<td style="text-align: left;">Adam epsilon</td>
<td style="text-align: center;">$1 \mathrm{e}-6$</td>
</tr>
<tr>
<td style="text-align: left;">Weight decay</td>
<td style="text-align: center;">0.01</td>
</tr>
</tbody>
</table>
<p>Table 7: Hyperparameters for pretraining.
The hyperparameters used for pretraining XLNet are shown in Table 7.</p>
<h2>A.4.2 Hyperparameters for Finetuning</h2>
<p>The hyperparameters used for finetuning XLNet on various tasks are shown in Table 8. "Layer-wise decay" means exponentially decaying the learning rates of individual layers in a top-down manner. For example, suppose the 24 -th layer uses a learning rate $l$, and the Layer-wise decay rate is $\alpha$, then the learning rate of layer $m$ is $l \alpha^{24-m}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hparam</th>
<th style="text-align: center;">RACE</th>
<th style="text-align: center;">SQuAD</th>
<th style="text-align: center;">MNLI</th>
<th style="text-align: center;">Yelp-5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dropout</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Attention dropout</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Max sequence length</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
<td style="text-align: center;">$3 \mathrm{e}-5$</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
<td style="text-align: center;">$1 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Number of steps</td>
<td style="text-align: center;">12 K</td>
<td style="text-align: center;">8 K</td>
<td style="text-align: center;">10 K</td>
<td style="text-align: center;">10 K</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate decay</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">linear</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Weight decay</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Adam epsilon</td>
<td style="text-align: center;">$1 \mathrm{e}-6$</td>
<td style="text-align: center;">$1 \mathrm{e}-6$</td>
<td style="text-align: center;">$1 \mathrm{e}-6$</td>
<td style="text-align: center;">$1 \mathrm{e}-6$</td>
</tr>
<tr>
<td style="text-align: left;">Layer-wise lr decay</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
</tr>
</tbody>
</table>
<p>Table 8: Hyperparameters for finetuning.</p>
<h1>A. 5 Discussion and Analysis</h1>
<h2>A.5.1 Comparison with BERT</h2>
<p>To prove a general point beyond one example, we now turn to more formal expressions. Inspired by previous work [37], given a sequence $\mathbf{x}=\left[x_{1}, \cdots, x_{T}\right]$, we define a set of target-context pairs of interest, $\mathcal{I}={(x, \mathcal{U})}$, where $\mathcal{U}$ is a set of tokens in $\mathbf{x}$ that form a context of $x$. Intuitively, we want the model to learn the dependency of $x$ on $\mathcal{U}$ through a pretraining loss term $\log p(x \mid \mathcal{U})$. For example, given the above sentence, the pairs of interest $\mathcal{I}$ could be instantiated as:</p>
<p>$$
\mathcal{I}={(x=\text { York }, \mathcal{U}={\text { New }}),(x=\text { York }, \mathcal{U}={\text { city }}),(x=\text { York }, \mathcal{U}={\text { New, city }}), \cdots}
$$</p>
<p>Note that $\mathcal{I}$ is merely a virtual notion without unique ground truth, and our analysis will hold regardless of how $\mathcal{I}$ is instantiated.
Given a set of target tokens $\mathcal{T}$ and a set of non-target tokens $\mathcal{N}=\mathbf{x} \backslash \mathcal{T}$, BERT and XLNet both maximize $\log p(\mathcal{T} \mid \mathcal{N})$ but with different formulations:</p>
<p>$$
\mathcal{J}<em _in="\in" _mathcal_T="\mathcal{T" x="x">{\text {BERT }}=\sum</em>}} \log p(x \mid \mathcal{N}) ; \quad \mathcal{J<em _in="\in" _mathcal_T="\mathcal{T" x="x">{\text {XLNet }}=\sum</em>\right)
$$}} \log p\left(x \mid \mathcal{N} \cup \mathcal{T}_{&lt;x</p>
<p>where $\mathcal{T}<em x="x">{&lt;x}$ denote tokens in $\mathcal{T}$ that have a factorization order prior to $x$. Both objectives consist of multiple loss terms in the form of $\log p\left(x \mid \mathcal{V}</em>}\right)$. Intuitively, if there exists a target-context pair $(x, \mathcal{U}) \in \mathcal{I}$ such that $\mathcal{U} \subseteq \mathcal{V<em x="x">{x}$, then the loss term $\log p\left(x \mid \mathcal{V}</em>$.
Given the definition, let's consider two cases:}\right)$ provides a training signal to the dependency between $x$ and $\mathcal{U}$. For convenience, we say a target-context pair $(x, \mathcal{U}) \in \mathcal{I}$ is covered by a model (objective) if $\mathcal{U} \subseteq \mathcal{V}_{x</p>
<ul>
<li>If $\mathcal{U} \subseteq \mathcal{N}$, the dependency $(x, \mathcal{U})$ is covered by both BERT and XLNet.</li>
<li>If $\mathcal{U} \subseteq \mathcal{N} \cup \mathcal{T}<em _x="&lt;x">{&lt;x}$ and $\mathcal{U} \cap \mathcal{T}</em> \neq \emptyset$, the dependency can only be covered by XLNet but not BERT. As a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet objective contains more effective training signals, which empirically leads to better performance in Section 3.</li>
</ul>
<h2>A.5.2 Comparison with Language Modeling</h2>
<p>Borrowing examples and notations from Section A.5.1, a standard AR language model like GPT [28] is only able to cover the dependency $(x=\operatorname{York}, \mathcal{U}={\mathrm{New}})$ but not $(x=\mathrm{New}, \mathcal{U}={\mathrm{York}})$. XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a limitation of AR language modeling can be critical in real-world applications. For example, consider a span extraction question answering task with the context "Thom Yorke is the singer of Radiohead" and the question "Who is the singer of Radiohead". The representations of "Thom Yorke" are not dependent on "Radiohead" with AR language modeling and thus they will not be chosen as the answer by the standard approach that employs softmax over all token representations. More formally, consider a context-target pair $(x, \mathcal{U})$ :</p>
<ul>
<li>
<p>If $\mathcal{U} \nsubseteq \mathcal{T}<em _x="&lt;x">{&lt;x}$, where $\mathcal{T}</em>$ denotes the tokens prior to $x$ in the original sequence, AR language modeling is not able to cover the dependency.</p>
</li>
<li>
<p>In comparison, XLNet is able to cover all dependencies in expectation.</p>
</li>
</ul>
<p>Approaches like ELMo [27] concatenate forward and backward language models in a shallow manner, which is not sufficient for modeling deep interactions between the two directions.</p>
<h1>A.5.3 Bridging the Gap Between Language Modeling and Pretraining</h1>
<p>With a deep root in density estimation ${ }^{4}[4,32,24]$, language modeling has been a rapidly-developing research area $[9,1,3]$. However, there has been a gap between language modeling and pretraining due to the lack of the capability of bidirectional context modeling, as analyzed in Section A.5.2. It has even been challenged by some machine learning practitioners whether language modeling is a meaningful pursuit if it does not directly improve downstream tasks ${ }^{5}$. XLNet generalizes language modeling and bridges such a gap. As a result, it further "justifies" language modeling research. Moreover, it becomes possible to leverage the rapid progress of language modeling research for pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness of the latest language modeling progress.</p>
<h2>A. 6 Qualitative Analysis of Attention Patterns</h2>
<p>We compare the attention pattern of BERT and XLNet without finetuning. Firstly, we found 4 typical patterns shared by both, as shown in Fig. 2.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Attention patterns shared by XLNet and BERT. Rows and columns represent query and key respectively.</p>
<p>More interestingly, in Fig. 3, we present 3 patterns that only appear in XLNet but not BERT: (a) The self-exclusion pattern attends to all other tokens but itself, probably offering a fast way to gather global information; (b) The relative-stride pattern attends to positions every a few stride apart relative to the query position; (c) The one-side masked pattern is very similar to the lower-left part of Fig. 1-(d), with the upper-right triangle masked out. It seems that the model learns not to attend the relative right half. Note that all these three unique patterns involve the relative positions rather than absolute ones, and hence are likely enabled by the "relative attention" mechanism in XLNet. We conjecture these unique patterns contribute to the performance advantage of XLNet. On the other hand, the proposed permutation LM objective mostly contributes to a better data efficiency, whose effects may not be obvious from qualitative visualization.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Attention patterns that appear only in XLNet. Rows and columns represent query and key respectively.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Illustration of the permutation language modeling objective for predicting $x_{3}$ given the same input sequence $\mathbf{x}$ but with different factorization orders.</p>
<h1>A. 7 Visualizing Memory and Permutation</h1>
<p>In this section, we provide a detailed visualization of the proposed permutation language modeling objective, including the mechanism of reusing memory (aka the recurrence mechanism), how we use attention masks to permute the factorization order, and the difference of the two attention streams.</p>
<p>As shown in Figure 5 and 6, given the current position $z_{t}$, the attention mask is decided by the permutation (or factorization order) $\mathbf{z}$ such that only tokens the occur before $z_{t}$ in the permutation can be attended; i.e., positions $z_{i}$ with $i&lt;t$. Moreover, comparing Figure 5 and 6 , we can see how the query stream and the content stream work differently with a specific permutation through attention masks. The main difference is that the query stream cannot do self-attention and does not have access to the token at the position, while the content stream performs normal self-attention.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: A detailed illustration of the content stream of the proposed objective with both the joint view and split views based on a length-4 sequence under the factorization order [3, 2, 4, 1].
Note that if we ignore the query representation, the computation in this figure is simply the standard self-attention, though with a particular attention mask.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: A detailed illustration of the query stream of the proposed objective with both the joint view and split views based on a length-4 sequence under the factorization order [3, 2, 4, 1].
The dash arrows indicate that the query stream cannot access the token (content) at the same position, but only the location information.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ The problem of language modeling is essentially density estimation for text data.
${ }^{5}$ https://openreview.net/forum?id=HJePno0c7m&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
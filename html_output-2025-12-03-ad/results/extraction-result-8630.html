<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8630 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8630</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8630</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-257636734</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.12023v2.pdf" target="_blank">Logical Reasoning over Natural Language as Knowledge Representation: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Logical reasoning is central to human cognition and intelligence. It includes deductive, inductive, and abductive reasoning. Past research of logical reasoning within AI uses formal language as knowledge representation and symbolic reasoners. However, reasoning with formal language has proved challenging (e.g., brittleness and knowledge-acquisition bottleneck). This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation and pretrained language models as reasoners, including philosophical definition and categorization of logical reasoning, advantages of the new paradigm, benchmarks and methods, challenges of the new paradigm, possible future directions, and relation to related NLP fields. This new paradigm is promising since it not only alleviates many challenges of formal representation but also has advantages over end-to-end neural methods. This survey focus on transformer-based LLMs explicitly working on deductive, inductive, and abductive reasoning over English representation.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8630.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8630.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-large on D* / ParaRules</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-large evaluated on D* and ParaRules hypothesis-classification datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finetuned RoBERTa-large (transformer encoder) was evaluated on synthetic deductive-hypothesis-classification datasets (D* family and ParaRules) and shown to achieve very high test accuracy on these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformers as soft reasoners over language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A transformer-based encoder pretrained on large corpora (cited as RoBERTa-large in the surveyed literature) and fine-tuned on deductive reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>D* / ParaRules (hypothesis classification, deductive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Hypothesis classification task: given a theory (facts and rules) and a hypothesis, decide whether the hypothesis is entailed (True/False/Unknown) by the theory; datasets are synthetic/natural-language rule-bases (D* family, ParaRules).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised fine-tuning of RoBERTa-large on the deductive-hypothesis-classification training data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported test accuracy >95% on D* and ParaRules test sets (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>High accuracy reported on the cited synthetic benchmarks, but later robustness studies (surveyed) show transformers can be brittle under logical perturbations and adversarial inputs; generalization to more complex/realistic deductive tasks is not guaranteed.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Fine-tuning a transformer encoder on synthetic deductive datasets can yield high in-distribution accuracy, indicating pretrained LMs can learn to emulate rule entailment on those benchmarks; however, robustness and out-of-distribution generalization remain concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical Reasoning over Natural Language as Knowledge Representation: A Survey', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8630.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8630.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-11B on D*-Ab / ProofWriter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-11B fine-tuned in ProofWriter / D*-Ab abductive and deductive evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The T5-11B text-to-text transformer was fine-tuned to generate implications, proofs, and abductive statements (ProofWriter) and was also evaluated on the D*-Ab abductive dataset, achieving high accuracy on the latter.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Proofwriter: Generating implications, proofs, and abductive statements over natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large text-to-text transformer (T5 family) used in cited works as the base model; fine-tuned for proof generation, implication enumeration, and abductive inference tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Proof generation / D*-Ab (deductive & abductive tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Proof generation: produce a proof tree explaining entailment; D*-Ab: abductive reasoning dataset variant of D* testing abductive hypothesis plausibility (P(h|C,O)).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised fine-tuning of T5-11B for generation of correctness labels and linearized proofs; used in ProofWriter framework and evaluated on D*-Ab.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Cited result: fine-tuned T5-11B reached high test accuracy of 93% on D*-Ab (as reported in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although high accuracy on D*-Ab is reported, the survey highlights general limitations: modular/stepwise methods are preferred to reduce hallucinations, and verifier modules are often needed to ensure step validity; overall robustness and scalability remain issues.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Large seq2seq models like T5-11B can be fine-tuned to produce proofs and perform abductive/deductive tasks with strong performance on curated benchmarks, but ensuring stepwise faithfulness (verifiers) and handling more complex, realistic knowledge remain open problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical Reasoning over Natural Language as Knowledge Representation: A Survey', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8630.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8630.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot prompting on FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot prompting of LLMs evaluated on FOLIO (first-order logic natural-language reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Few-shot prompting of (large) language models was evaluated on FOLIO (a benchmark for natural-language reasoning with first-order logic) and found to perform only slightly better than random on that dataset according to the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FOLIO: natural language reasoning with first-order logic</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Few-shot prompted LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large generative language models used in a few-shot prompting regime (no or minimal finetuning) evaluated on first-order logic style natural-language reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO (first-order logic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Natural-language reasoning framed as first-order logic problems where models must perform logical inference consistent with FOL semantics over textual facts and rules.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot prompting (providing a handful of demonstration examples to a pretrained LLM) and comparison to finetuned medium-sized models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey reports few-shot prompted LLMs performed only slightly better than random on FOLIO; finetuned medium-sized models were also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Few-shot prompting vs finetuned medium-sized LMs: few-shot showed weak performance (near-random) per the cited evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Few-shot prompting struggles with strict first-order-logic style reasoning; LLMs without finetuning or specialised architectures perform poorly on FOL-style benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Prompting alone (few-shot) is insufficient for robust first-order logical reasoning; finetuning or specialized methods are necessary for meaningful performance on FOL-like benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical Reasoning over Natural Language as Knowledge Representation: A Survey', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8630.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8630.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRover / multiPRover (embedding-based proof)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PRover and multiPRover: embedding-based proof generation with transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PRover and multiPRover use transformer final-layer embeddings to classify nodes and edges to construct proofs from a theory, treating proof generation as prediction over embeddings rather than pure generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prover: Proof generation for interpretable reasoning over rules</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer encoders (used for embedding-based proof prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer models (encoder-based) whose final-layer embeddings are averaged per knowledge item and passed to node and edge classifiers to predict proof tree structure.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Proof generation (deductive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Given a theory and hypothesis, generate a directed proof tree of premises and intermediate conclusions that entail the hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Single-inference embedding extraction: average final-layer embeddings per knowledge item; node and edge classifiers predict proof graph; structural constraints enforce valid proof shapes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Embedding-based single-step methods can lack interpretability and may be limited in compositional generalization; later modular/multi-step and verifier-based methods were proposed to address these issues.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Embedding-based approaches provide a computationally efficient way to extract structured proofs from LMs, but they motivated stage-2 and stage-3 modular approaches due to concerns about interpretability, compositional generalization, and validity of inference steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical Reasoning over Natural Language as Knowledge Representation: A Survey', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8630.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8630.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Modular stepwise + verifier methods (Entailer, TeachMe, ADGV, NLProofS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modular stepwise proof generation systems with inference modules, controllers, and verifiers (examples: Entailer, TeachMe, ADGV, NLProofS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of methods that decompose proof generation into modules (deduction/abduction inference modules, a reasoning controller performing search, and explicit verifier modules) to improve faithfulness and reduce hallucinations in LLM-based reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Entailer: Answering questions with faithful and truthful chains of reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pipeline of LLM modules (deduction/abduction modules + verifier LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Systems composed of multiple LLM modules, each performing a role: inference (forward/backward reasoning), a controller that searches/selects premises, and a verifier LLM to check each generated inference step.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Proof generation / hypothesis classification (deductive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step proof generation where each inference step is produced and validated iteratively to produce a final proof tree consistent with model beliefs and dataset theory.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Modular, stepwise reasoning with explicit verifier(s) â€” verifier can be an extra LLM or implemented via round-trip consistency between deduction and abduction modules; some systems support human correction (TeachMe).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to all-at-once generative systems, modular + verifier methods aim to reduce hallucination and improve faithfulness; the survey cites that All-At-Once often performs worse on more complex problems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Modular systems incur additional computational cost (multiple LLM inferences per step); verifier performance depends on the verifier's own knowledge and can still rely on the model's internal beliefs; search complexity can be large.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Adding explicit verification and stepwise control improves faithfulness and reduces invalid reasoning steps, but raises computational cost and relies on the verifier's reliability; interactive corrective loops (TeachMe) can further improve behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical Reasoning over Natural Language as Knowledge Representation: A Survey', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8630.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8630.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adversarial / robustness findings (Richardson & Sabharwal, Gaskell et al., Sanyal et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical studies of transformer robustness on deductive reasoning (adversarial attacks and logical perturbation evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple studies evaluated transformer-based LMs on synthetic deductive benchmarks with adversarial or logically perturbed inputs, finding both success under large/complex training and significant brittleness under carefully designed perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logically consistent adversarial attacks for soft theorem provers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based LMs (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer models evaluated across synthetic deductive benchmarks and adversarial perturbations to analyze robustness of logical inference behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Synthetic hypothesis-classification and deductive benchmarks (robustness/evasion tests)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks that probe whether models perform correct variable binding, resist adversarial instances where queries appear inside rule bodies, and remain stable under minimal logical edits and equivalence transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Adversarial example generation and logical perturbation benchmarks; constructing complex synthetic training examples to test scale/generalization; evaluating model susceptibility to variable binding errors and literal-text traps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Mixed: Gaskell et al. report that with sufficiently large/complex training, transformers can perform well and show some generalization; Richardson & Sabharwal and Sanyal et al. report that transformers are often fooled by adversarial patterns and are not robust to logical perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Transformers can be fooled when the query appears inside a rule body, struggle to correctly bind variables across rule sides, and are sensitive to minimal logical edits; robustness is a major limitation compared to symbolic reasoners.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>While scaling and appropriate training data can improve performance, transformer-based LMs lack the robustness of symbolic reasoners and are vulnerable to logically-structured adversarial inputs; dedicated robustness benchmarks and methods are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical Reasoning over Natural Language as Knowledge Representation: A Survey', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8630.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8630.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inductive CoLM and rule generation approaches</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Language-Models (CoLM) for inductive rule generation and multi-stage generate-then-verify pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For inductive reasoning (rule generation), systems commonly use a generate-filter pipeline (e.g., CoLM) where one LLM proposes candidate rules from facts and other LLMs verify rules across multiple philosophical/consistency criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models as inductive reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Language-Models (multiple LLMs in pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A modular pipeline: one or more LLMs generate candidate rules (inductive hypotheses) from sets of facts; additional LLMs act as verifiers/filters evaluating rule validity along multiple criteria (consistency, generality, non-conflict).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Rule generation and rule verification (inductive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Given multiple facts, induce a more-general rule that entails those facts and generalizes beyond them; verification filters assess multiple requirements (e.g., non-conflict with facts, philosophical criteria of inductive validity).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Generate-and-filter: LLM-based rule population followed by multiple verifier LLMs enforcing different inductive criteria; variants include chain-of-thought prompting and iterative refinement (generate -> verify -> refine).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Rule generation quality is limited by reliance on out-of-the-box LLMs, costly expert annotation for evaluation, and difficulty ensuring induced rules are novel, correct, and non-conflicting; automatic verification remains imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Inductive reasoning with LLMs requires multi-stage verification to filter spurious generalizations; modular generate-then-verify pipelines are a promising architecture but need better verifiers and scalable evaluation methods for reliable rule induction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical Reasoning over Natural Language as Knowledge Representation: A Survey', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>Proofwriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>FOLIO: natural language reasoning with first-order logic <em>(Rating: 2)</em></li>
                <li>Prover: Proof generation for interpretable reasoning over rules <em>(Rating: 2)</em></li>
                <li>Logically consistent adversarial attacks for soft theorem provers <em>(Rating: 2)</em></li>
                <li>Robustlr: Evaluating robustness to logical perturbation in deductive reasoning <em>(Rating: 2)</em></li>
                <li>Language models as inductive reasoners <em>(Rating: 2)</em></li>
                <li>Entailment tree explanations via iterative retrieval-generation reasoner <em>(Rating: 1)</em></li>
                <li>Probing the limits of rule reasoning in transformers through natural language satisfiability <em>(Rating: 1)</em></li>
                <li>Selection-inference: Exploiting large language models for interpretable logical reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8630",
    "paper_id": "paper-257636734",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "RoBERTa-large on D* / ParaRules",
            "name_full": "RoBERTa-large evaluated on D* and ParaRules hypothesis-classification datasets",
            "brief_description": "Finetuned RoBERTa-large (transformer encoder) was evaluated on synthetic deductive-hypothesis-classification datasets (D* family and ParaRules) and shown to achieve very high test accuracy on these benchmarks.",
            "citation_title": "Transformers as soft reasoners over language",
            "mention_or_use": "mention",
            "model_name": "RoBERTa-large",
            "model_description": "A transformer-based encoder pretrained on large corpora (cited as RoBERTa-large in the surveyed literature) and fine-tuned on deductive reasoning datasets.",
            "model_size": null,
            "reasoning_task_name": "D* / ParaRules (hypothesis classification, deductive reasoning)",
            "reasoning_task_description": "Hypothesis classification task: given a theory (facts and rules) and a hypothesis, decide whether the hypothesis is entailed (True/False/Unknown) by the theory; datasets are synthetic/natural-language rule-bases (D* family, ParaRules).",
            "method_or_approach": "Supervised fine-tuning of RoBERTa-large on the deductive-hypothesis-classification training data.",
            "performance": "Reported test accuracy &gt;95% on D* and ParaRules test sets (as cited).",
            "baseline_comparison": null,
            "limitations_or_failures": "High accuracy reported on the cited synthetic benchmarks, but later robustness studies (surveyed) show transformers can be brittle under logical perturbations and adversarial inputs; generalization to more complex/realistic deductive tasks is not guaranteed.",
            "insights_or_conclusions": "Fine-tuning a transformer encoder on synthetic deductive datasets can yield high in-distribution accuracy, indicating pretrained LMs can learn to emulate rule entailment on those benchmarks; however, robustness and out-of-distribution generalization remain concerns.",
            "uuid": "e8630.0",
            "source_info": {
                "paper_title": "Logical Reasoning over Natural Language as Knowledge Representation: A Survey",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "T5-11B on D*-Ab / ProofWriter",
            "name_full": "T5-11B fine-tuned in ProofWriter / D*-Ab abductive and deductive evaluations",
            "brief_description": "The T5-11B text-to-text transformer was fine-tuned to generate implications, proofs, and abductive statements (ProofWriter) and was also evaluated on the D*-Ab abductive dataset, achieving high accuracy on the latter.",
            "citation_title": "Proofwriter: Generating implications, proofs, and abductive statements over natural language",
            "mention_or_use": "mention",
            "model_name": "T5-11B",
            "model_description": "A large text-to-text transformer (T5 family) used in cited works as the base model; fine-tuned for proof generation, implication enumeration, and abductive inference tasks.",
            "model_size": "11B",
            "reasoning_task_name": "Proof generation / D*-Ab (deductive & abductive tasks)",
            "reasoning_task_description": "Proof generation: produce a proof tree explaining entailment; D*-Ab: abductive reasoning dataset variant of D* testing abductive hypothesis plausibility (P(h|C,O)).",
            "method_or_approach": "Supervised fine-tuning of T5-11B for generation of correctness labels and linearized proofs; used in ProofWriter framework and evaluated on D*-Ab.",
            "performance": "Cited result: fine-tuned T5-11B reached high test accuracy of 93% on D*-Ab (as reported in the survey).",
            "baseline_comparison": null,
            "limitations_or_failures": "Although high accuracy on D*-Ab is reported, the survey highlights general limitations: modular/stepwise methods are preferred to reduce hallucinations, and verifier modules are often needed to ensure step validity; overall robustness and scalability remain issues.",
            "insights_or_conclusions": "Large seq2seq models like T5-11B can be fine-tuned to produce proofs and perform abductive/deductive tasks with strong performance on curated benchmarks, but ensuring stepwise faithfulness (verifiers) and handling more complex, realistic knowledge remain open problems.",
            "uuid": "e8630.1",
            "source_info": {
                "paper_title": "Logical Reasoning over Natural Language as Knowledge Representation: A Survey",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Few-shot prompting on FOLIO",
            "name_full": "Few-shot prompting of LLMs evaluated on FOLIO (first-order logic natural-language reasoning)",
            "brief_description": "Few-shot prompting of (large) language models was evaluated on FOLIO (a benchmark for natural-language reasoning with first-order logic) and found to perform only slightly better than random on that dataset according to the survey.",
            "citation_title": "FOLIO: natural language reasoning with first-order logic",
            "mention_or_use": "mention",
            "model_name": "Few-shot prompted LLMs",
            "model_description": "Large generative language models used in a few-shot prompting regime (no or minimal finetuning) evaluated on first-order logic style natural-language reasoning tasks.",
            "model_size": null,
            "reasoning_task_name": "FOLIO (first-order logic reasoning)",
            "reasoning_task_description": "Natural-language reasoning framed as first-order logic problems where models must perform logical inference consistent with FOL semantics over textual facts and rules.",
            "method_or_approach": "Few-shot prompting (providing a handful of demonstration examples to a pretrained LLM) and comparison to finetuned medium-sized models.",
            "performance": "Survey reports few-shot prompted LLMs performed only slightly better than random on FOLIO; finetuned medium-sized models were also evaluated.",
            "baseline_comparison": "Few-shot prompting vs finetuned medium-sized LMs: few-shot showed weak performance (near-random) per the cited evaluation.",
            "limitations_or_failures": "Few-shot prompting struggles with strict first-order-logic style reasoning; LLMs without finetuning or specialised architectures perform poorly on FOL-style benchmarks.",
            "insights_or_conclusions": "Prompting alone (few-shot) is insufficient for robust first-order logical reasoning; finetuning or specialized methods are necessary for meaningful performance on FOL-like benchmarks.",
            "uuid": "e8630.2",
            "source_info": {
                "paper_title": "Logical Reasoning over Natural Language as Knowledge Representation: A Survey",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "PRover / multiPRover (embedding-based proof)",
            "name_full": "PRover and multiPRover: embedding-based proof generation with transformers",
            "brief_description": "PRover and multiPRover use transformer final-layer embeddings to classify nodes and edges to construct proofs from a theory, treating proof generation as prediction over embeddings rather than pure generation.",
            "citation_title": "Prover: Proof generation for interpretable reasoning over rules",
            "mention_or_use": "mention",
            "model_name": "Transformer encoders (used for embedding-based proof prediction)",
            "model_description": "Transformer models (encoder-based) whose final-layer embeddings are averaged per knowledge item and passed to node and edge classifiers to predict proof tree structure.",
            "model_size": null,
            "reasoning_task_name": "Proof generation (deductive reasoning)",
            "reasoning_task_description": "Given a theory and hypothesis, generate a directed proof tree of premises and intermediate conclusions that entail the hypothesis.",
            "method_or_approach": "Single-inference embedding extraction: average final-layer embeddings per knowledge item; node and edge classifiers predict proof graph; structural constraints enforce valid proof shapes.",
            "performance": null,
            "baseline_comparison": null,
            "limitations_or_failures": "Embedding-based single-step methods can lack interpretability and may be limited in compositional generalization; later modular/multi-step and verifier-based methods were proposed to address these issues.",
            "insights_or_conclusions": "Embedding-based approaches provide a computationally efficient way to extract structured proofs from LMs, but they motivated stage-2 and stage-3 modular approaches due to concerns about interpretability, compositional generalization, and validity of inference steps.",
            "uuid": "e8630.3",
            "source_info": {
                "paper_title": "Logical Reasoning over Natural Language as Knowledge Representation: A Survey",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Modular stepwise + verifier methods (Entailer, TeachMe, ADGV, NLProofS)",
            "name_full": "Modular stepwise proof generation systems with inference modules, controllers, and verifiers (examples: Entailer, TeachMe, ADGV, NLProofS)",
            "brief_description": "A class of methods that decompose proof generation into modules (deduction/abduction inference modules, a reasoning controller performing search, and explicit verifier modules) to improve faithfulness and reduce hallucinations in LLM-based reasoning.",
            "citation_title": "Entailer: Answering questions with faithful and truthful chains of reasoning",
            "mention_or_use": "mention",
            "model_name": "Pipeline of LLM modules (deduction/abduction modules + verifier LLMs)",
            "model_description": "Systems composed of multiple LLM modules, each performing a role: inference (forward/backward reasoning), a controller that searches/selects premises, and a verifier LLM to check each generated inference step.",
            "model_size": null,
            "reasoning_task_name": "Proof generation / hypothesis classification (deductive reasoning)",
            "reasoning_task_description": "Multi-step proof generation where each inference step is produced and validated iteratively to produce a final proof tree consistent with model beliefs and dataset theory.",
            "method_or_approach": "Modular, stepwise reasoning with explicit verifier(s) â€” verifier can be an extra LLM or implemented via round-trip consistency between deduction and abduction modules; some systems support human correction (TeachMe).",
            "performance": null,
            "baseline_comparison": "Compared to all-at-once generative systems, modular + verifier methods aim to reduce hallucination and improve faithfulness; the survey cites that All-At-Once often performs worse on more complex problems.",
            "limitations_or_failures": "Modular systems incur additional computational cost (multiple LLM inferences per step); verifier performance depends on the verifier's own knowledge and can still rely on the model's internal beliefs; search complexity can be large.",
            "insights_or_conclusions": "Adding explicit verification and stepwise control improves faithfulness and reduces invalid reasoning steps, but raises computational cost and relies on the verifier's reliability; interactive corrective loops (TeachMe) can further improve behavior.",
            "uuid": "e8630.4",
            "source_info": {
                "paper_title": "Logical Reasoning over Natural Language as Knowledge Representation: A Survey",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Adversarial / robustness findings (Richardson & Sabharwal, Gaskell et al., Sanyal et al.)",
            "name_full": "Empirical studies of transformer robustness on deductive reasoning (adversarial attacks and logical perturbation evaluations)",
            "brief_description": "Multiple studies evaluated transformer-based LMs on synthetic deductive benchmarks with adversarial or logically perturbed inputs, finding both success under large/complex training and significant brittleness under carefully designed perturbations.",
            "citation_title": "Logically consistent adversarial attacks for soft theorem provers",
            "mention_or_use": "mention",
            "model_name": "Transformer-based LMs (generic)",
            "model_description": "Pretrained transformer models evaluated across synthetic deductive benchmarks and adversarial perturbations to analyze robustness of logical inference behaviors.",
            "model_size": null,
            "reasoning_task_name": "Synthetic hypothesis-classification and deductive benchmarks (robustness/evasion tests)",
            "reasoning_task_description": "Tasks that probe whether models perform correct variable binding, resist adversarial instances where queries appear inside rule bodies, and remain stable under minimal logical edits and equivalence transformations.",
            "method_or_approach": "Adversarial example generation and logical perturbation benchmarks; constructing complex synthetic training examples to test scale/generalization; evaluating model susceptibility to variable binding errors and literal-text traps.",
            "performance": "Mixed: Gaskell et al. report that with sufficiently large/complex training, transformers can perform well and show some generalization; Richardson & Sabharwal and Sanyal et al. report that transformers are often fooled by adversarial patterns and are not robust to logical perturbations.",
            "baseline_comparison": null,
            "limitations_or_failures": "Transformers can be fooled when the query appears inside a rule body, struggle to correctly bind variables across rule sides, and are sensitive to minimal logical edits; robustness is a major limitation compared to symbolic reasoners.",
            "insights_or_conclusions": "While scaling and appropriate training data can improve performance, transformer-based LMs lack the robustness of symbolic reasoners and are vulnerable to logically-structured adversarial inputs; dedicated robustness benchmarks and methods are needed.",
            "uuid": "e8630.5",
            "source_info": {
                "paper_title": "Logical Reasoning over Natural Language as Knowledge Representation: A Survey",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Inductive CoLM and rule generation approaches",
            "name_full": "Chain-of-Language-Models (CoLM) for inductive rule generation and multi-stage generate-then-verify pipelines",
            "brief_description": "For inductive reasoning (rule generation), systems commonly use a generate-filter pipeline (e.g., CoLM) where one LLM proposes candidate rules from facts and other LLMs verify rules across multiple philosophical/consistency criteria.",
            "citation_title": "Language models as inductive reasoners",
            "mention_or_use": "mention",
            "model_name": "Chain-of-Language-Models (multiple LLMs in pipeline)",
            "model_description": "A modular pipeline: one or more LLMs generate candidate rules (inductive hypotheses) from sets of facts; additional LLMs act as verifiers/filters evaluating rule validity along multiple criteria (consistency, generality, non-conflict).",
            "model_size": null,
            "reasoning_task_name": "Rule generation and rule verification (inductive reasoning)",
            "reasoning_task_description": "Given multiple facts, induce a more-general rule that entails those facts and generalizes beyond them; verification filters assess multiple requirements (e.g., non-conflict with facts, philosophical criteria of inductive validity).",
            "method_or_approach": "Generate-and-filter: LLM-based rule population followed by multiple verifier LLMs enforcing different inductive criteria; variants include chain-of-thought prompting and iterative refinement (generate -&gt; verify -&gt; refine).",
            "performance": null,
            "baseline_comparison": null,
            "limitations_or_failures": "Rule generation quality is limited by reliance on out-of-the-box LLMs, costly expert annotation for evaluation, and difficulty ensuring induced rules are novel, correct, and non-conflicting; automatic verification remains imperfect.",
            "insights_or_conclusions": "Inductive reasoning with LLMs requires multi-stage verification to filter spurious generalizations; modular generate-then-verify pipelines are a promising architecture but need better verifiers and scalable evaluation methods for reliable rule induction.",
            "uuid": "e8630.6",
            "source_info": {
                "paper_title": "Logical Reasoning over Natural Language as Knowledge Representation: A Survey",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "Proofwriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "FOLIO: natural language reasoning with first-order logic",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "Prover: Proof generation for interpretable reasoning over rules",
            "rating": 2,
            "sanitized_title": "prover_proof_generation_for_interpretable_reasoning_over_rules"
        },
        {
            "paper_title": "Logically consistent adversarial attacks for soft theorem provers",
            "rating": 2,
            "sanitized_title": "logically_consistent_adversarial_attacks_for_soft_theorem_provers"
        },
        {
            "paper_title": "Robustlr: Evaluating robustness to logical perturbation in deductive reasoning",
            "rating": 2,
            "sanitized_title": "robustlr_evaluating_robustness_to_logical_perturbation_in_deductive_reasoning"
        },
        {
            "paper_title": "Language models as inductive reasoners",
            "rating": 2,
            "sanitized_title": "language_models_as_inductive_reasoners"
        },
        {
            "paper_title": "Entailment tree explanations via iterative retrieval-generation reasoner",
            "rating": 1,
            "sanitized_title": "entailment_tree_explanations_via_iterative_retrievalgeneration_reasoner"
        },
        {
            "paper_title": "Probing the limits of rule reasoning in transformers through natural language satisfiability",
            "rating": 1,
            "sanitized_title": "probing_the_limits_of_rule_reasoning_in_transformers_through_natural_language_satisfiability"
        },
        {
            "paper_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "rating": 1,
            "sanitized_title": "selectioninference_exploiting_large_language_models_for_interpretable_logical_reasoning"
        }
    ],
    "cost": 0.017806,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Logical Reasoning over Natural Language as Knowledge Representation: A Survey
16 Feb 2024</p>
<p>Zonglin Yang zonglin.yang@ntu.edu.sg 
Nanyang Technological University</p>
<p>Xinya Du xinya.du@utdallas.edu 
University of Texas at Dallas</p>
<p>Rui Mao rui.mao@ntu.edu.sg 
Nanyang Technological University</p>
<p>Jinjie Ni jinjie001@ntu.edu.sg 
Nanyang Technological University</p>
<p>Erik Cambria cambria@ntu.edu.sg 
Nanyang Technological University</p>
<p>Logical Reasoning over Natural Language as Knowledge Representation: A Survey
16 Feb 20249F8C80EE974DDD18C63C667DDC6C8A32arXiv:2303.12023v2[cs.CL]
Logical reasoning is central to human cognition and intelligence.It includes deductive, inductive, and abductive reasoning.Past research of logical reasoning within AI uses formal language as knowledge representation and symbolic reasoners.However, reasoning with formal language has proved challenging (e.g., brittleness and knowledge-acquisition bottleneck).This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation and pretrained language models as reasoners, including philosophical definition and categorization of logical reasoning, advantages of the new paradigm, benchmarks and methods, challenges of the new paradigm, possible future directions, and relation to related NLP fields.This new paradigm is promising since it not only alleviates many challenges of formal representation but also has advantages over end-to-end neural methods.This survey focus on transformer-based LLMs explicitly working on deductive, inductive, and abductive reasoning over English representation.</p>
<p>Introduction</p>
<p>An argument consists of premise(s) and a conclusion.Logical reasoning is a form of thinking in which premises and relations between premises are used in a rigorous manner to infer conclusions that are entailed (or implied) by the premises and the relations (Nunes, 2012).It consists of three reasoning types, namely deductive reasoning, inductive reasoning, and abductive reasoning (Flach and Kakas, 2000) (more illustration on the categorization can be found in Â§2).It is important since the ability to reach logical conclusions on the basis of prior information is recognized as central to human cognition and intelligence (Goel et al., 2017).</p>
<p>The past research of logical reasoning within AI uses formal language (e.g., first-order logic) as knowledge representation and symbolic reasoners (Muggleton and Raedt, 1994).This paradigm has resulted in impressive applications such as expert systems (Metaxiotis et al., 2002).However, building and reasoning over formal language have proved challenging (Musen and Van der Lei, 1988), with representative disadvantages of brittleness (an expert system fails as long as its knowledge base does not contain complete knowledge for a problem) and knowledge-acquisition bottleneck (human experts are needed to encode their knowledge with formal representation).</p>
<p>Since the rapid development in language models, natural language has been explored as a new knowledge representation, and large language models (LLMs) have been used as a new reasoner for deductive reasoning (Clark et al., 2020), abductive reasoning (Bhagavatula et al., 2020), and inductive reasoning (Yang et al., 2022b).Therefore, all three reasoning types of logical reasoning have been investigated with natural language as knowledge representation.This research also shows that LLMs can be finetuned or prompted to perform well for each of the reasoning types.</p>
<p>In this paper, we summarize the three previously separately investigated logical reasoning types to-gether, referred as logical reasoning (from the perspectives of deductive, inductive, and abductive reasoning) over natural language as knowledge representation and PLMs as reasoners (LRNLP), and provide an in-depth survey of LRNLP.</p>
<p>Illustrated in Figure 1, LRNLP means a new paradigm for logical reasoning that uses new knowledge representation (natural language) and new reasoner (LLM).LRNLP can also be seen as a set of tasks on the three reasoning with a constraint of natural language representation and LLM reasoners.The latest methods for the LRNLP tasks are generally modular: multiple LLMs each as one module playing a different function, combined together to perform complex tasks.They make one step of reasoning with one inference of LLM.For complex problems, they usually have access to a knowledge base that stores relevant textual knowledge to be retrieved as premises to support the reasoning process to reach a conclusion, which might be used as a new premise for the next step's reasoning.By iteratively repeating this process, a final conclusion may be made.Although it looks similar to expert systems, we discuss how LRNLP is possible to overcome many main challenges of the previous paradigm such as brittleness in Â§3.1.</p>
<p>In addition to the comparison with formal language, in Â§3.2 we discuss that LRNLP could be viewed as a new type of neural-symbolic method, which has unique advantages over existing neurosymbolic methods.We also discuss how LRNLP, as a neuro-symbolic method, has advantages over existing end-to-end neural methods (e.g., explainability, controllability, less catastrophic forgetting) in Â§3.3.These advantages make an LRNLP system possible to deal with many challenging problems.</p>
<p>In the remaining sections of this survey, we review papers on LRNLP (including deductive reasoning Â§4, inductive reasoning Â§5, and abductive reasoning Â§6), and list challenges ( Â§7.Our main focus is to understand the language model's logical reasoning ability through the three sub-types of logical reasoning to provide finer analysis and avoid ambiguity on which type of reasoning it is conducting.Therefore, we focus on papers that using transformer-based LLMs explicitly working on deductive, inductive, or abductive reasoning tasks.These papers all adopt English as knowledge representation.In Â§A.1 we discuss the relation of LRNLP to related NLP fields (e.g., commonsense reasoning), which could help to form a clear shape of LRNLP in NLP.For each reasoning sub-type, we summarize existing task formulations, datasets, and methods under each task.</p>
<p>Definition and Categorization</p>
<p>There are many subjects related to logical reasoning, including philosophy, logic, and AI.Among them, the definition and categorization aspects of logical reasoning are handled by philosophy research.However, debate exists in philosophy research on the categorization of logical reasoning.We leave a detailed description of the debate in philosophy research in Â§A.2 and only leave the conclusions here according to philosophy research.</p>
<p>In general, logical reasoning consists of deductive, inductive, and abductive reasoning (Console and Saitta, 2000).Given an argument consisting of premises and a conclusion, we define the sub-type of logical reasoning it involves below:</p>
<p>Definition for deductive reasoning: the premises can conclusively provide support for the conclusion, i.e. if the premises are all true, it would be impossible for the conclusion to be false.</p>
<p>Definition for inductive reasoning: the premises cannot conclusively provide support for the conclusion, since the conclusion generalizes existing information in premises to new knowledge, which has a wider applicable scope than those in premises.</p>
<p>Definition for abductive reasoning: the premises cannot conclusively provide support for the conclusion, since the conclusion contains more specific information over the premises (most commonly used as generating most probable explanations).</p>
<p>Please note that according to Console and Saitta (2000), inductive reasoning and abductive reasoning are not exclusive to each other.</p>
<p>Advantages of LRNLP</p>
<p>Advantages over Formal Language</p>
<p>Building and reasoning over formal language have proved challenging (Musen and Van der Lei, 1988;Cropper et al., 2022), with disadvantages such as (1) brittleness (expert system fails when its knowledge base does not contain complete knowledge for a problem), (2) knowledge-acquisition bottleneck (human experts are needed to encode their knowledge with formal representation), (3) inability to handle raw data such as natural language, (4) sensitivity to label errors, and (5) failure to recognize different symbols with similar meanings.</p>
<p>Nevertheless, the new paradigm of logical reasoning, LRNLP, has systematic strengths over these challenges.Specifically, LLMs contain knowledge themselves (Davison et al., 2019), which makes it possible for them to provide good answers even when some required explicit knowledge is not present in a knowledge base Talmor et al., 2020 (less brittle), and be less affected by input errors (Meng et al., 2021).In addition, with natural language as knowledge representation, such a system can naturally handle raw input, and it is possible to utilize the enormous web corpora to automatically construct rule bases using information extraction Ji, 2018 (less affected by knowledgeacquisition bottleneck); using embeddings for concepts (Mikolov et al., 2013), it semantically "understands" the meaning of symbols and therefore robust for paraphrasing.</p>
<p>Advantages over Neuro-symbolic Systems</p>
<p>LRNLP could be seen as a new type of neurosymbolic in addition to the existing 6 types (Kautz, 2022), as its goal and design of methodology are typically symbolic (logical reasoning with knowledge bases), while avoiding any symbolic representation, using (currently pure) neural methods.Therefore LRNLP can avoid many bottlenecks of the other neuro-symbolic methods caused by symbolic representation, such as symbolic knowledge acquisition and scalability (Wang and Yang, 2022).</p>
<p>Advantages over E2E Neural Methods</p>
<p>As a neuro-symbolic method, LRNLP systematically has some advantages over end-to-end neural methods, such as interpretability Cambria et al., 2023 (since it is ususally stepwise), more controllability (LRNLP reasons following a given knowledge base), and less catastrophic forgetting (LRNLP uses an explicit knowledge base).</p>
<p>Deductive Reasoning</p>
<p>Existing Task Formulations</p>
<p>Existing tasks for deductive reasoning can be summarized as hypothesis classification, proof generation, proof generation with incomplete information, and implication enumeration.Datasets for tasks are summarized in Table 1."Proof generation" tab with âœ— means it is for hypothesis classification task.</p>
<p>Hypothesis Classification Each data example for hypothesis classification task is a tuple (theory, hypothesis, correctness), where theory typically has the form (f act * , rule * ), hypothesis is a question, and correctness can be
* âœ— âœ— âœ“ âœ“ âœ“ âœ— 500k ParaRules âœ“ âœ— âœ“ âœ“ âœ“ âœ— 40k Birds-electricity âœ“ âœ“ âœ“ âœ“ âœ“ âœ— 5k Leap-of-thought âœ— âœ“ âœ— âœ“ âœ— âœ— 33k PARARULE-Plus âœ— âœ— âœ“ âœ“ âœ“ âœ— 400k FOLIO âœ“ âœ“ âœ“ âœ“ âœ“ âœ— 1,435 D<em>(CWA) âœ— âœ— âœ“ âœ“ âœ“ âœ“ 500k D</em>(OWA) âœ— âœ— âœ“ âœ“ âœ— âœ“ 500k EntailmentBank âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ 1,840 ENWN âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ 100
Table 1: Summary of deductive reasoning datasets: D<em>, ParaRules &amp; birds-electricity (Clark et al., 2020); leapof-thought (Talmor et al., 2020); PARARULE-Plus (Bao et al., 2022); FOLIO (Han et al., 2022); D</em>(CWA) &amp; D*(OWA) Tafjord et al., 2021;EntailmentBank (Dalvi et al., 2021); ENWN (Sprague et al., 2022).</p>
<p>T rue or F alse (or U nknown).This task requires to predict the correctness for the hypothesis given the theory.</p>
<p>Proof Generation The proof generation task has the same setting as the hypothesis classification task, except that in addition to predicting a correctness, the proof generation task also requires providing a proof given theory to explain the correctness.The proof is a directed tree (N , E) with nodes n âˆˆ N and edges e âˆˆ E. Each node is an item of knowledge in theory (usually a f act or a rule), or a generated intermediate reasoning conclusion, or the hypothesis itself; Each edge points from a premise node to a conclusion node to form a deductive argument, which typically needs one-step inference (not multi-step).</p>
<p>Proof Generation with Incomplete Information</p>
<p>This task is the same as the proof generation task, except that theory lacks one node to form a complete proof .Specifically, given theory, it requires to predict the correctness of hypothesis with a proof , as well as recovering the missing node.</p>
<p>Implication Enumeration Given a theory, this task requires to enumerate implications of the theory, using deductive reasoning.</p>
<p>Methods</p>
<p>Hypothesis Classification</p>
<p>There are mainly three categories of methods for the hypothesis classification task.The first category only conducts the classification task itself; the second category can predict correctness as well as generate a proof .However, the correctness is not necessarily consistent with the predicted proof .Verifier Human-authored   realistic proof</p>
<p>Stage</p>
<p>PRover (Saha et al., 2020)
âœ— âœ“ âœ— N/A N/A âœ— âœ— 1 multiPRover (Saha et al., 2021) âœ— âœ“ âœ— N/A N/A âœ— âœ— 1 EntailmentWriter (Dalvi et al., 2021) âœ“ âœ“ âœ— N/A N/A âœ— âœ“ 1
ProofWriter (Tafjord et al., 2021) The third category is similar to the second, except that correctness always follows proof .
âœ“ âœ— âœ“ â†’ âœ— âœ— âœ— 2 EVR (Liang et al., 2021) âœ“ âœ— âœ“ â† âœ— âœ— âœ— 2 IBR (Qu et al., 2022) âœ— âœ“ âœ“ â† âœ“ âœ— âœ— 2 IRGR (Ribeiro et al., 2022) âœ“ âœ“ âœ“ â†’ âœ“ âœ— âœ“ 2 SI (Creswell et al., 2022) âœ“ âœ— âœ“ â†’ âœ“ âœ— âœ— 2 FaiRR (Sanyal et al., 2022b) âœ“ âœ— âœ“ â†’ âœ“ âœ— âœ— 2 MetGen (Hong et al., 2022) âœ“ âœ— âœ“ Both âœ“ âœ— âœ“ 2 SCSearch (Bostrom et al., 2022) âœ“ âœ— âœ“ â†’ âœ“ âœ— âœ“ 2 ADGV (Sprague et al., 2022) âœ“ âœ— âœ“ Both âœ“ âœ“ âœ“ 3 NLProofS (Yang et al., 2022a) âœ“ âœ“ âœ“ â†’ âœ“ âœ“ âœ“ 3 Entailer (Tafjord et al., 2022) âœ“ âœ“ âœ“ â† âœ“ âœ“ âœ“ 3 Teachme (Dalvi et al., 2022) âœ“ âœ“ âœ“ â† âœ“ âœ“ âœ— 3
Until now, methods from the first category directly use transformer-based LLMs (Vaswani et al., 2017), aiming at analyzing and benchmarking their performance.Specifically, Clark et al. (2020) find that finetuned RoBERTa-large (Liu et al., 2019) can achieve 95%+ accuracy on the test set of D* and ParaRules datasets; Talmor et al. (2020) further demonstrate that LLMs can be finetuned to reliably perform deductive reasoning using both implicit, pretrained knowledge and explicit natural language statements (theory) to make predictions; Han et al. (2022) evaluate finetuned medium-sized language models and few-shot prompting on LLMs on the FOLIO dataset.However, they find that LLM with few-shot prompting only performs slightly better than random results.</p>
<p>The second category methods typically infer LLM only once, and then utilize the final layer embeddings or generations to obtain correctness and proof .Specifically, PRover (Saha et al., 2020) and multiPRover (Saha et al., 2021) use the [CLS] token to predict correctness, and leverage the final layer embeddings of knowledge items in theory to generate proof ; All-At-Once ProofWriter (Tafjord et al., 2021) and EntailmentWriter (Dalvi et al., 2021) generate correctness and linearized proof at the same time.</p>
<p>The third category methods create a proof first, and then predict correctness from the proof .Â§4.2.2 illustrates these methods in detail.</p>
<p>Proof Generation</p>
<p>Methods for this task are summarized in Table 2. Current methods for the proof generation task roughly consist of three stages.In each stage, one key new technique is considered and developed.In stage 1, LLMs are used for forming proof in one inference step.In stage 2, modular-based, stepwise frameworks are developed to create proof (each module is usually implemented with a single LLM).In stage 3, a verifier is added as a new module to make sure that each reasoning step reflects the belief of LLMs.We summarize the experiment results in Â§A.8 and the model structures in Â§A.9.</p>
<p>Methods for stage 1 typically utilize the last layer embeddings (Saha et al., 2020(Saha et al., , 2021) ) or generations (Tafjord et al., 2021;Dalvi et al., 2021) to create proof .Methods utilizing embedding typically (1) obtain an averaged embedding for each knowledge item in theory, and (2) pass each embedding to a node classifier, and each embedding pairs to an edge classifier to predict nodes and edges for proof .Constraints are usually used to enforce the structure of proof .Generation methods directly generate linearized correctness and full proof given linearized theory and hypothesis.</p>
<p>The motivations of stage 2 methods are generally concerned with end-to-end methods, which is considered to lack interpretability (Liang et al., 2021;Qu et al., 2022;Sanyal et al., 2022b;Bostrom et al., 2022), suffer from compositional generalization problems (Liang et al., 2021;Creswell et al., 2022), have limited input size (Ribeiro et al., 2022), are not causal (Creswell et al., 2022), and lack constraints on inference validity (Hong et al., 2022).</p>
<p>Methods in stage 2 can be summarized as having two components, an inference module and a reasoning controller.The inference module can be a deduction module (Tafjord et al., 2021;Ribeiro et al., 2022;Creswell et al., 2022;Sanyal et al., 2022b;Bostrom et al., 2022), an abduction module (Liang et al., 2021;Qu et al., 2022), or both (Hong et al., 2022;Sprague et al., 2022).The deduction module performs deductive reasoning, and reasons forwardly from theory to hypothesis to construct proof ; the abduction module performs abductive reasoning, and reasons backwardly from hypothesis to theory to construct proof .The reasoning controller in general performs a search process that each step it searches through the theory and generated intermediate conclusions space to select (retrieve) premises for the next step inference.The search processes include exhaustive search (Tafjord et al., 2021;Liang et al., 2021) or heuristic search (Qu et al., 2022;Ribeiro et al., 2022;Creswell et al., 2022;Sanyal et al., 2022b;Bostrom et al., 2022;Hong et al., 2022;Sprague et al., 2022).The reasoning controller usually can also stop the search process if it detects the goal.</p>
<p>Motivation of stage 3 methods is similar, basically that stage 2 methods lack explicit verifiers to avoid hallucinating invalid steps (Yang et al., 2022a), and to ensure that the inference processes reflect LLM's own beliefs (Tafjord et al., 2022).</p>
<p>Methods in stage 3 can be summarized as utilizing explicit verifier(s) (implemented with a LLM) to check the validity of each inference step.One way is to add a new module (additional to the inference module and reasoning controller in stage 2), working as a "fact checker" to verify the generated inference step (Yang et al., 2022a;Tafjord et al., 2022); The other one, called round-trip consistency, is only suitable for methods that use both deduction and abduction modules, where deduction and abduction modules work as the verifier for each other (Sprague et al., 2022).</p>
<p>In addition to the general 3 stages, a new aspect is attended to, which is whether teachable by humans.Build based on Entailer (Tafjord et al., 2022), TeachMe (Dalvi et al., 2022) shows that user corrections can help override erroneous model beliefs, and that a system can gradually improve by accumulating user corrections.Compared to Entailer, it adds an interaction module and a dynamic memory module to obtain and store human corrections.</p>
<p>Proof with Incomplete Information</p>
<p>ADGV (Sprague et al., 2022) is the only method focusing on this task.It uses both deduction and abduction modules, and the reasoning controller performs heuristic search.The abduction module is used to recover the missing premise.</p>
<p>Implication Enumeration</p>
<p>Tafjord et al. ( 2021) is the only paper mentioned this task.They compare the performance of "All-At-Once" and "Iterative" ProofWriter on this task.They find that "All-At-Once" performs worse, mainly because it struggles with problems that are more complex than training examples.</p>
<p>Robustness of LLM as Reasoner</p>
<p>The previously introduced methods only focus on solving the deductive reasoning tasks, while it is unclear whether LLMs can be used as robust deductive reasoners.To investigate the problem, Gaskell et al. ( 2022) create a more challenging synthetic dataset on hypothesis classification task in terms of complexity, and test LLM's performance on it.They find that with large and complex enough training examples, transformers can perform well on the dataset.In addition, they find that transformers exhibit some degree of generalization and scale-invariance ability; Richardson and Sabharwal (2022) propose an adversarial attack method for synthetic datasets on the hypothesis classification task.They find that transformers are often fooled if the query literally appears within the body of a rule, and transformers struggle to correctly bind variables on either side of a rule; Sanyal et al. (2022a) proposed a synthetic deductive reasoning dataset to evaluate the robustness of language models to minimal logical edits in the inputs and different logical equivalence conditions, and find that LLMs are not robust to their proposed logical perturbations.</p>
<p>Inductive Reasoning</p>
<p>Existing Task Formulations</p>
<p>Existing tasks for inductive reasoning can be summarized as rule verification and rule generation tasks.Datasets for the tasks are summarized in Table 3. "Generation" tab with âœ— means it is for the rule verification task.
-norm âœ— âœ— âœ“ âœ— âœ— âœ— âœ— 23k DEERLET âœ— âœ“ âœ“ âœ“ âœ“ âœ— âœ— 846 DEER âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ— 1.2k ARC âœ— âœ— - âœ— âœ— - âœ— 1k OpenD5 âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ - 675 C-LBD âœ“ âœ— âœ“ âœ“ âœ“ âœ“ âœ“ 67k TOMATO âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ 50
Table 3: Summary of inductive reasoning datasets: property-norm (Misra et al., 2022), DEERLET and DEER (Yang et al., 2022b), ARC (Chollet, 2019), OpenD5 (Zhong et al., 2023), C-LBD (Wang et al., 2023a), and TOMATO (Yang et al., 2023b)."Not restricted rule types" means whether the data is not restricted in a specific topic (e.g., taxonomic).</p>
<p>Rule Verification Given a generated rule and f acts where the rule is generated from, the task is to classify whether the rule can be accepted.The current evaluation aspects are from requirements of both inductive reasoning and natural language.</p>
<p>Rule Generation Given multiple manually selected f acts with similar patterns, the task is to induce a rule that (1) can entail the f acts, and ( 2) is more general than all of the f acts.Here "more general" means larger information coverage scope.More detailed illustrations can be found in Â§A.10.</p>
<p>Scientific Hypotheses Generation This task is similar to Rule Generation task but is much more challenging in that the generated rule should not be commonsense knowledge but scientific hypotheses that are even new to humanity.</p>
<p>Methods</p>
<p>Rule Generation methods almost always have a Rule Verification step after the initial generation of rules.To have a clearer overview, we separately introduce the framing or methods of the two tasks.Another group of works' (Zhu et al., 2023;Wang et al., 2023b;Qiu and Jiang, 2023) adopted rule verification criteria is compliant with one of the key re-quirements proposed by Yang et al. (2022b), which is that rule and f acts should not be in conflict.They focus on inducing (executable) rule from synthetic f acts such as a sequence of number (example rule: find the smallest number), arithmetic calculation (example rule: "6+4=10"), or changes of 2D grid images (example rule: executable code for moving the grids).They verify rules by checking the consistency of the labels of annotated examples (f acts) and the results of rules.</p>
<p>Rule Verification</p>
<p>Rule Generation</p>
<p>Yang et al. (2022b) assume that the inductive reasoning task is so difficult that a proper system should contain a rule populator and (multiple) rule verifiers that filter bad rules from different aspects.Accordingly, they propose a framework named chain-of-language-models (CoLM), where one LLM generates rules given f acts, the other four LLMs filter generated rules mainly based on philosophical requirements of inductive reasoning.</p>
<p>Besides the rule generation and filtering process, Zhu et al. (2023) further propose to generate rules based on chain-of-thought prompting, and verify rules based on whether the rules can be used to deduce the annotated answer correctly; Wang et al. (2023b) further propose that under synthetic datasets, executable code can be generated for the textual rules and verify the rules by executing the code and comparing the results with groundtruth annotation; Qiu and Jiang (2023) further propose a third stage of "rule refinement," (leveraging feedback and generate again) and that iteratively repeating the three stages can obtain better rules.</p>
<p>Scientific Hypotheses Generation</p>
<p>Zhong et al. ( 2023) focuses on proposing hypotheses (from many disciplines) from a research goal and two comparable corpora.Their method also follows a generate-filter process, where LLMs are used for the filtering stage.Wang et al. (2023a) focus on proposing NLP hypotheses from a seed term and background context.Before the hypotheses generation module, they build knowledge graphs to associate academic terms, and retrieve some of the terms as inspirations.Yang et al. (2023b) focuses on proposing social science and business hypotheses only from a pile of raw web corpora.To utilize raw web corpora, they expand generate-filter modules with a background finder module and an inspiration finder module.They also propose three feedback mechanisms named past feedback, present Dataset Human written Realistic Multi-step Theory included Generation Size
Î±NLI âœ“ âœ“ âœ— âœ— âœ— 22k Î±NLG âœ“ âœ“ âœ— âœ— âœ“ 76k AbductionRules âœ— âœ— âœ— âœ“ âœ“ 114k D<em>-Ab âœ— âœ— âœ“ âœ“ âœ“ 14k
Table 4: Summary of abductive reasoning datasets: Î±NLI and Î±NLG (Bhagavatula et al., 2020), Abduc-tionRules (Young et al., 2022), and D</em>-Ab (Tafjord et al., 2021)."Realistic" means whether the data is consistent with the real world."Multi-step" means whether multiple reasoning steps are needed to get the result.</p>
<p>feedback, and future feedback to help the intercommunications between modules to induce more novel, valid, and helpful hypotheses.</p>
<p>6 Abductive Reasoning</p>
<p>Existing Task Formulations</p>
<p>Existing tasks for abductive reasoning can be summarized as explanation classification, and explanation generation w/o and w/ theory.Datasets for the tasks are summarized in Table 4.In the table, the "generation" tab and "theory included" tab can be used to determine the task it is used for.In addition to knowledge integration, many different aspects of explanation classification tasks are also investigated.Specifically, Bhagavatula et al. (2020) rewrite the objective using Bayes Rule and formulate a set of probabilistic models that make various independence assumptions on the new objective.They find that the most sophisticated probabilistic model works the best; Zhu et al. (2020) frame this task as a ranking task to also measure the plausibility of hypothesis in addition to discriminating it; Paul and Frank (2021) conduct this task in an unsupervised setting by pretraining on a counterfactual reasoning dataset, which is related to abductive reasoning.Kadikis et al. (2022) propose a method to select suitable LLMs for this task.It is based on the cosine similarity of embed(O 1 , O 2 ) and embed(h i ) for each LLM without finetuning.Zhao et al. (2023) assume that different h are mutually exclusive, and improve performance by incorporating an additional loss item as regularization to enforce an unbalanced probability prediction over different h.Chan et al. (2023) exploit inter-sentential coherence and the model consistency to develop a prompt tuning model.</p>
<p>Explanation Generation without Theory</p>
<p>In general, methods for this task either incorporate knowledge or improve the decoding method to be more suitable for this task.</p>
<p>For knowledge integration, Bhagavatula et al. (2020) utilize textual knowledge generated from COMET and investigate two ways of knowledge integration -via texts or via embeddings, and find that the embedding-based method is more effective; Ji et al. (2020) leverage structural knowledge from ConceptNet (Speer et al., 2017) for this task.</p>
<p>For improving decoding method, Qin et al. (2020) are motivated by the fact that the target h + to generate happens before O 2 .They accordingly propose an unsupervised decoding algorithm that can incorporate both past and future contexts.</p>
<p>Explanation Generation with Theory</p>
<p>Challenges and Opportunities</p>
<p>We list more challenges and opportunities in Â§A.11.</p>
<p>Computationally Efficient Reasoner Many tasks in logical reasoning over formal language have very high algorithmic complexity (Muggleton et al., 2012).Thanks to the low computational cost of each deduction step over formal language, such complex tasks could be possible.However, each deduction step in LRNLP typically costs one inference of an LLM, which makes tasks with high algorithmic complexity nearly prohibitive.</p>
<p>Robust Deductive Reasoner Symbolic deductive reasoners are not restricted to training data distributions, while neural deductive reasoners are restricted to their training data (Gontier et al., 2020;Richardson and Sabharwal, 2022).In addition, neural deductive reasoners are also vulnerable to adversarial attacks (Gaskell et al., 2022), while symbolic reasoners are robust to the attacks.The lack of robustness can lead to restricted application domains and incorrect deductive inferences.</p>
<p>Better Automatic Evaluation Metrics It is generally difficult to automatically evaluate generative reasoning implications, especially with realistic and not synthetic datasets.The difficulty mainly lies in that the same semantic meaning can be expressed with diversified forms, and that different conclusions might be all acceptable (especially in abductive and inductive reasoning).This may lead to biased evaluation when using automatic metrics.</p>
<p>More Impacts on (NLP) Applications</p>
<p>As illustrated in Â§3, overall LRNLP can be seen as a new type of neuro-symbolic method, which takes the advantages from both the symbolic and sub-symbolic aspects.These characteristics make an LRNLP system possible (but might still be challenging) to deal with many (NLP) applications such as medical diagnosis and legal NLP tasks, since many medical and legal problems could be seen as pure logical reasoning problems with very large rule bases (e.g., medical knowledge and laws).</p>
<p>Probabilistic Inference In reality, pure deductive reasoning has not always been used.When people include "likely" in their expressions, uncertainty is introduced, which makes the reasoning process probabilistic; in addition, inductive reasoning and abductive reasoning are by default nonmonotonic reasoning.This uncertainty aspect has not been focused in current research.It is probably beneficial to learn from how symbolic reasoning handles uncertainty (Halpern, 2017).</p>
<p>Reasoning with Incomplete Information The current proof generation task requires all necessary premises provided to create a proof tree.Only one work (Sprague et al., 2022) focuses on proof generation with the incomplete information task.However, the task they adopt only overlooks one premise, while in reality more might be missing.</p>
<p>Inductive Reasoning on Web Corpora Currently, the dataset for rule generation tasks in inductive reasoning provides manually selected facts (Yang et al., 2022b).However, to best leverage a system's ability to handle natural language, it should be able to work on raw web corpora to induce rules, which leads to a more challenging task of inductive reasoning on web corpora.</p>
<p>Abductive Reasoning with (Long) Theory</p>
<p>Many tasks such as medical diagnosis conduct abductive reasoning with a long theory (e.g., medical knowledge).However, current abductive reasoning research only covers abductive commonsense reasoning (Bhagavatula et al., 2020) without given theory, or only given short, synthetic, not realistic knowledge as theory (Tafjord et al., 2021).</p>
<p>Interactions between Reasoning Types Multiple reasoning types can be used together for complex tasks.Existing works only utilize deductive reasoning with abductive reasoning to create a proof tree (Hong et al., 2022;Sprague et al., 2022).However, many other collaborations are possible, such as using inductive reasoning to collect a (large) rule base, which is to be used as the theory base for deductive reasoning.</p>
<p>Conclusion</p>
<p>In this survey, we review papers using transformerbased LLMs explicitly working on deductive, inductive, and abductive reasoning over English representation.Specifically, we have introduced the philosophical foundations, advantages of LRNLP, benchmarks and methods, challenges of LRNLP, possible future directions, and the relation of LRNLP to related NLP fields ( Â§A.1).</p>
<p>Limitations</p>
<p>In consideration of space constraints, this paper focuses more on (1) providing a high-level overview and prospect of the LRNLP field (e.g., advantages and challenges of the field), and (2) delineating the broader evolutionary trajectories of pertinent methodologies.It might not include all the details of the surveyed papers.</p>
<p>A Appendix</p>
<p>A.1 Relation to Related (NLP) Fields</p>
<p>In this section, we first introduce related NLP fields to general logical reasoning, then introduce fields that are only related to deductive reasoning, inductive reasoning, or abductive reasoning.We hope that this section could be helpful to form a clear shape of LRNLP in NLP.</p>
<p>A.1.1 Logical Reasoning</p>
<p>There are some previous works involve the term "logical reasoning", but do not provide a specification on which sub-type of logical reasoning they involve.In many cases these works are more close to "natural language inference", which adopts datasets where the data involve a mixture of multiple subtypes of logical reasoning, making it hard to analyze from each sub-type.Therefore we do not include these works in this survey.</p>
<p>Neuro-Symbolic Computing Neural-symbolic computing is a hybrid of symbolism and connectionism to exploit advantages from both sides (Wang and Yang, 2022;Cambria et al., 2022).The knowledge representation of its symbolic part basically is a knowledge graph or propositional logic or first-order logic (Wang and Yang, 2022).LRNLP could be seen as a new type of neurosymbolic in addition to the existing 6 types summarized by Kautz (2022), as its goal and design of methodology are typically symbolic (logical reasoning with knowledge bases), while avoiding any symbolic representation, using (currently pure) neural methods.</p>
<p>Natural Language Inference Natural language inference (NLI) is generally considered as the semantic concepts of entailment and contradiction (Bowman et al., 2015).Here logical reasoning tasks can be viewed as special types of NLI focusing on particular reasoning aspects.</p>
<p>Question Answering</p>
<p>The form of LRNLP looks similar to question answering (QA), however, QA is conducting one-step logical reasoning only when the context provides enough information to answer the question (deductive reasoning), or the answer is a generalization of an argument in context or question (inductive reasoning), or the answer is to provide explanations to the question (abductive reasoning).</p>
<p>Commonsense Reasoning Commonsense reasoning (CR) and logical reasoning (LR) are similar in that they both involve "knowledge" and "reasoning".Compared to LR, CR focuses more on the "knowledge" aspect.Some typical tasks include whether a system has commonsense knowledge (Bosselut et al., 2019;Yang et al., 2020), and whether a system's answer is commonsenseknowledge-aware (Bisk et al., 2020); LR focuses more on the "reasoning" aspect, e.g., whether a system's i/o behaviors follow reasoning requirements (Clark et al., 2020).</p>
<p>Chain</p>
<p>of Thoughts Chain of thoughts (COT) (Wei et al., 2022) is a prompting technique that can elicit the step-by-step reasoning ability of LLMs without finetuning.</p>
<p>COT can potentially be used for each of the three sub-reasoning types of logical reasoning.In fact, for a given (commonsense reasoning) question, some reasoning steps of COT could be deductive, and others can be inductive or abductive.Since the purpose of this paper is to provide a finer analysis on logical reasoning, we do not intentionally cover prompting techniques such as COT.</p>
<p>It is also argued by several modular-based deductive reasoning methods that COT's reasoning is not causal (Creswell et al., 2022), limited by input size (Ribeiro et al., 2022), and contains unrelated or incorrect steps (Hong et al., 2022;Tafjord et al., 2022).</p>
<p>Overall, it could be interesting to use COTrelated methods specifically for deductive, inductive, or abductive reasoning (as opposed to modularbased methods), and it is a less-explored research direction.</p>
<p>A.1.2 Deductive Reasoning</p>
<p>Multi-hop Reasoning Compared to proof generation, many multi-hop reasoning tasks (Yang et al., 2018;Jiang et al., 2020;Min et al., 2019;Sinha et al., 2019) are much simpler, often being single-branched (Qu et al., 2022), consisting of only 2-3 supporting facts, and are more coarsegrained, involving large chunks of texts such as passages instead of simple, short sentences (Yang et al., 2022a).</p>
<p>Nevertheless, some multi-hop reasoning datasets can be considerd as conducting deductive reasoning.For instance, for each data in CLUTRR (Sinha et al., 2019) dataset, a set of facts that can make conclusive support to the target kinship relation is included in background information as input for each target relation, hence from the philosophical definition (Salmon, 1989), it requires to perform deductive reasoning.</p>
<p>Mathematical Reasoning In many mathematical reasoning tasks such as math word problem solving (Koncel-Kedziorski et al., 2015) and geometry problem solving (Seo et al., 2015), the conclusion can be conclusively entailed by the premise.Therefore these tasks belong to deductive reasoning.We do not review math-related papers because we want to focus solely on the challenge of deductive reasoning while mathematical reasoning involves numbers in the text, which introduces additional challenges.</p>
<p>A.1.3 Inductive Reasoning</p>
<p>Information Extraction Information Extraction (IE) is a task of extracting pre-specified types of facts from written texts or speech transcripts, and converting them into structured representations (Ji, 2018).The rule generation task here also extracts rules from facts represented in written texts.The difference is that IE pursues extracting the exact information from existing texts, while inductive reasoning aspires to induce more general rules from existing texts, where the information in rules goes beyond what is exactly stated in the texts.</p>
<p>Case-based Reasoning Case-based Reasoning (CBR) is a classic AI subject, whose methods share a general methodology of four steps: retrieve, reuse, revise, and retain (Aamodt and Plaza, 1994).Recently there has been research works devoting to bridging the research of CBR and NLP, by using NLP techniques for CBR challenges (Yang et al., 2023a) and improving NLP tasks with CBR methodologies (Das et al., 2021(Das et al., , 2022;;Yang et al., 2023a;Thai et al., 2023).CBR could be seen as a type of analogical reasoning (Kolodner, 1997), and analogical reasoning belongs to inductive reasoning (Salmon, 1989).However, CBR is a different inductive reasoning type than the "generalization" process (from facts to rules) described in Flach and Kakas (2000), but more on the general description on inductive reasoning (Salmon, 1989) that premises cannot conclusively provide support to the conclusion.</p>
<p>A.1.4 Abductive Reasoning</p>
<p>Causal Reasoning In logic research, causal reasoning aims at an epistemological problem of estab-lishing precise causal relationships between causes and effects.It is generally considered a form of inductive reasoning (Goertzel et al., 2011), since inductive reasoning is to derive rules that lead from one to another.When the focus is to derive possible causes from effects, the problem belongs to abductive reasoning (Goertzel et al., 2011).</p>
<p>A.2 Full Details About the Definition and Categorization of Logical Reasoning</p>
<p>There are many subjects related to logical reasoning, including philosophy, logic, and AI.Among them, the definition and categorization aspects of logical reasoning are handled by philosophy research.However, debate exists in philosophy research on the categorization of logical reasoning.One group believes that every argument can be classified as either deduction argument, inductive argument, or fallacy (Salmon, 1989).Without considering fallacy, given that an argument consists of premises and a conclusion, when the premises can conclusively provide support to the conclusion (which means that if the premises of the argument were all true, it would be impossible for the conclusion of the argument to be false), this argument is a deductive argument.Conversely, when the premises can not conclusively provide support to the conclusion, the argument is inductive.</p>
<p>The other group has the same definition of deductive reasoning, but they believe that further categorization of non-deductive reasoning is necessary.Without considering fallacy, they believe in a trichotomy of deductive, inductive, and abductive reasoning (Peirce, 1974).However, even for the second group, the definition and difference between inductive and abductive reasoning are also controversy (Flach and Kakas, 2000).</p>
<p>Nevertheless, Console and Saitta (2000) argue that from the utility perspective of AI, a distinction between inductive and abductive reasoning is possible: both inductive and abductive reasoning provide explanations about the world but their explanations differ in the degree of generality.For instance, an inductive hypothesis allows the validity of properties, observed on a set of individuals, to be generalized to other individuals not in the observations, whereas an abductive one allows unobserved properties to be applied to observed individuals.</p>
<p>Considering that inductive and abductive reasoning can be distinctive enough when formulated in NLP, in this paper, we adopt the second group, particularly Console and Saitta (2000)'s view of definition and categorization of logical reasoning.</p>
<p>Specifically, the difference between inductive and abductive reasoning is that, both inductive and abductive reasoning provide explanations about the world but their explanations differ in the degree of generality.</p>
<p>For instance, an inductive hypothesis allows the validity of properties, observed on a set of individuals, to be generalized to other individuals not in the observations, whereas an abductive one allows unobserved properties to be applied to observed individuals.</p>
<p>The distinction between inductive and abductive hypotheses strictly parallels the dichotomy extension vs. intension, or generality vs. informativeness.In other words, an inductive hypothesis extends or generalizes to unobserved individuals, while an abductive one provides more specific information (e.g., unobserved properties) about existing specific individuals.</p>
<p>For example, if a white ball is found in a bag, inductive reasoning might lead to the conclusion that "all balls in this bag are white", while abductive reasoning might lead to the conclusion that "someone put the white ball into this bag".</p>
<p>In this example, the inductive hypothesis generalizes the property of the existing individual (the white ball) to unobserved individuals (other notseen balls in the bag), while the abductive hypothesis provides more specific information about the current individual (who put this ball to the bag).</p>
<p>To summarize in simple words, in common situations, pure inductive reasoning is to only provide (usually sample to population) generalizations, while pure abductive reasoning is to only provide specific explanations.</p>
<p>Overall, even in the philosophical literature (which takes charge of the research on the definition of logical reasoning), a clear definition for all three types of logical reasoning is rare, but more on the description of the difference between types of logical reasoning (since a clear definition is still under debate).The difference can be illustrated does not mean a precise definition can be given.Nevertheless, considering the above-discussed philosophical literature, we try our best to give a definition below for a more straightforward understanding:</p>
<p>Given an argument consisting of premises and a conclusion, we define the sub-type of logical reasoning it involves below:</p>
<p>Definition for deductive reasoning: the premises can conclusively provide support for the conclu-sion, i.e. if the premises are all true, it would be impossible for the conclusion to be false.</p>
<p>Definition for inductive reasoning: the premises cannot conclusively provide support for the conclusion, since the conclusion generalizes existing information in premises to new knowledge, which has a wider applicable scope than those in premises.</p>
<p>Definition for abductive reasoning: the premises cannot conclusively provide support for the conclusion, since the conclusion contains more specific information over the premises (most commonly used as generating most probable explanations).</p>
<p>Please note that according to Console and Saitta (2000), inductive reasoning and abductive reasoning are not exclusive to each other, i.e., inductive reasoning and abductive reasoning overlap with each other.</p>
<p>A.3 Why We Choose Definition in Section 2</p>
<p>Firstly, some other definitions (e.g., from Pieces) are not in conflict with the one we adopted.Secondly, other definitions lack a clear boundary between different types of reasoning, while our adopted definitions clearly delineate such boundaries (e.g., general vs. specific for inductive and abductive reasoning).</p>
<p>To elaborate why there's no contradiction, specifically, Pierce's definition is "inference to the most plausible explanation for incomplete observations".Here "explanation" refers to not guaranteed and specific information.An example about the discussion on "specific" can be found in Â§A.10.We use definition in Â§2 because other definitions lack a clear boundary between deductive, inductive, and abductive reasoning, while our adopted definitions clearly delineate such boundaries (e.g., general vs. specific for inductive and abductive reasoning; guaranteed vs. not guaranteed for deductive and the remaining two reasoning).</p>
<p>A.4 Related Surveys on Reasoning</p>
<p>Huang and Chang (2022); Qiao et al. (2022) mainly reviews the prompting techniques for LLMs, but do not focus on papers that specialized on logical reasoning (the coverage of the two fields are quite different).Yu et al. (2023) also review papers related to reasoning.However, (1) they do not focus on logical reasoning, and do not organize their survey based on the three sub-types of logical reasoning.Particularly, only a small section discusses this topic; (2) their definition on the deductive, inductive, and abductive reasoning lacks a philosophy foundation (no reference), and is confusing.Particularly, from their definition, it is unclear on the difference between inductive and abductive reasoning.Specifically, it is unclear on what is the difference between the "more general rule" and "best explanation"?A more general rule, such as Newton's Law, can also serve as the best explanation about phenomenons related to object movement.In the contrary, this survey's definition is based on philosophy literature (Console and Saitta, 2000), and our definition can clearly differ between inductive and abductive reasoning.The difference lies in that inductive reasoning is about "general", and abductive reasoning is about "specific", while a specific conclusion, such as "it must have rained since the lawn is wet", is commonly used as "the best explanation".But the point of abductive reasoning is about "specific", not "explanation", since inductive reasoning can also provide explanation (Flach and Kakas, 2000).We illustrate in Â§2 that there has been various forms of definition for the three reasoning types during the thousands of years of development of the philosophy research.The variance of definition should be aware and the definitions should be given with a systematic view of the philosophy research to avoid confusion.We provide a detailed discussion about the categorization of logical reasoning from a philosophy perspective in Â§ A.2. Yu et al. (2023) do not stress and organize the survey from the three sub-types of logical reasoning.Xu et al. (2023) provides a comprehensive evaluation of the logical reasoning ability of LLMs.They are not to provide a survey but to use LLMs on the existing logical reasoning datasets.Lakoff (1970); McCarthy (1990) are the first few works to take a close look at the connection between logical reasoning and natural language.Dagan et al. (2005) proposed evaluating logical reasoning through the comparison of two natural language texts.MacCartney and Manning (2014) apply logical reasoning in natural language inference through iterative editing of natural language.</p>
<p>A.5 Other Inductive Reasoning Papers Implicit Rule Verification Misra et al. (2022) analyze language model's ability to generalize novel property knowledge (has sesamoid bones) from concept(s) (robins) to others (sparrows, canaries).As illustrated in Â§A.10, they analyze the language models' ability to classify a new fact (but not a rule) as correct or not, given facts.It could be seen that the correctness of a rule is implicitly predicted by testing multiple facts entailed by the rule.Two main reasons for the abundance of works in the deductive reasoning domain could be that (1) more challenging benchmarks have been constructed during the last few years, and (2) deductive reasoning could be one of the most commonly used reasoning types in common life.We think the main reason for the little attention drawn to abductive reasoning in recent years is that the benchmarks for abductive reasoning are relatively old and less challenging for LLMs.Inductive reasoning could be a promising research topic since there have been few works in the domain, and it involves very challenging tasks such as proposing new scientific findings.</p>
<p>Symbolic Rule Generation</p>
<p>In general, there has been no framework which is proposed to address all three reasoning domains.However, LLMs generally can exhibit all three reasoning abilities to some extent.It would be interesting for future works to analyze the effect of the pretraining method and scale of LLM on the three reasoning abilities.</p>
<p>A.7 Relation Between LRNLP and neuro-symbolic</p>
<p>A large proportion of recent papers on deductive reasoning and abductive reasoning leverage a natural language-based knowledge base, and reason over retrieved knowledge from the knowledge base to reach a certain goal (Tafjord et al., 2021;Liang et al., 2021;Qu et al., 2022;Ribeiro et al., 2022;Creswell et al., 2022;Sanyal et al., 2022b;Hong et al., 2022;Bostrom et al., 2022;Yang et al., 2022a;Tafjord et al., 2022;Dalvi et al., 2022).This pattern is very similar to the methodology design of neuro-symbolic methods, which is to retrieve symbolic knowledge and reason over the retrieved symbolic knowledge.The main difference is that LRNLP adopts natural language as knowledge representation but not symbolic knowledge.Because of the similarity in the methodology design, we consider that LRNLP could be seen as a type of neuro-symbolic methods but without many disadvantages of symbolic representation such as symbolic knowledge acquisition and scalability.</p>
<p>In addition, due to the high similarity in the methodology design to neuro-symbolic, LRNLP also shares some advantages with neuro-symbolic such as explainability.The reason is that the iterative retrieving and reasoning will make the decision-making process more interpretable on the intermediate reasoning steps, and which knowledge is used for each reasoning step.</p>
<p>A.8 Experiments Summarization</p>
<p>In this section, we summarize the experiment results of an important and literature-abundant task.</p>
<p>Until now there has been only one or two papers working on inductive reasoning.Methods for abductive reasoning generally leverage different resources (such as multi-task, additional knowledge resources, and ancillary loss) and lack a progressive relationship between each other, therefore are less comparable.Currently, the P roof Generation task in deductive reasoning is the most literatureabundant, and methods for this task have progressive relationships with each other.Therefore here we mainly summarize results and analyze for the P roof Generation task.</p>
<p>Table 5 shows the summarized experiment results.We select the most widely used tasks to display their performance.Among the task, the setting of ParaRules is trained on D3 (D<em> dataset with depth 3) and tested on the ParaRules test set; the setting of Birds-Electricity is trained on D5 (D</em> dataset with depth 5) and tested on bird-electricity set; setting for EntailmentBank is the task 3 which uses full corpus as input (so that many distractors exist in input); setting for OBQA and QuaRTz are zero-shot setting while model pre-trained on another dataset (EntailmentBank).</p>
<p>Among the methods, Creswell et al. ( 2022) and Bostrom et al. (2022) design unique metrics using EntailmentBank dataset.Sprague et al. (2022) focus on a unique task (proof generation task with incomplete information), therefore we do not list their experiments results in the table.Specifically, Creswell et al. (2022) work on metric of accuracy.Bostrom et al. (2022) work on metrics of Goal%, #Steps, where Goal% measures the number of valid goals reached by each system, and #Steps measures the number of steps expanded before reaching a valid goal.Sprague et al. (2022) work on "proof generation task with incomplete information", which naturally performs worse than the "proof generation task".</p>
<p>Overall methods for proof generation tasks tend to use different datasets for evaluation, making them less comparable.</p>
<p>A.9 Model Structure, Pretraining Data Used, and Experiment Settings</p>
<p>Table 6 shows a collection of model structure, pretraining data usage, and experiment settings for methods in hypothesis classification and proof generation tasks.</p>
<p>In general, RoBERTa-large and T5-11B are the most adopted base models.</p>
<p>A.10 Meaning of "More General" Required by Inductive Reasoning</p>
<p>This section is collected from Yang et al. (2022b)'s appendix, to help illustrate inductive reasoning.Given an argument consisting of a premise and a conclusion, if the conclusion involves new information that is not covered by the premise and can not be conclusively entailed by the premise, the argument is an inductive argument (Salmon, 1989).</p>
<p>When the conclusion has a larger scope of information coverage than the premise, and can entail the premise, it can be said that the conclusion is "more general" to the premise (Yang et al., 2022b).In this case, we termed the premise as a "fact", and the conclusion as a "rule"; When the conclusion contains new pieces of information and cannot entail the premise, as defined by Salmon (1989), the argument is still an inductive argument.But in this case, we termed the premise as a "fact", and the conclusion as another "fact".</p>
<p>For instance, if facts that are about cats and dogs are good accompaniment of humans, then some examples of a "more general" rule can be (1) mammals are good accompaniment of humans, or (2) domesticated animals are good accompaniment of humans, or (3) animals with four legs are good accompaniment of human.</p>
<p>In these examples, the rules cover a larger scope than the facts (e.g., mammals compared to cats; finetuning 1 Entailer T5-11B -finetuning 1 Teachme T5-11B -human interaction 1 Table 6: A collection of model usage, (second-round) pretraining data usage, and experiment setting for methods in hypothesis classification (denoted as "0" in "Task" column) and proof generation (denoted as "1" in "Task" column) tasks.domesticated animals compared to cats), and therefore the rules are "more general" than the facts.</p>
<p>"More general" means not only about finding higher taxonomic rank, but can be in unlimited forms.For instance, if the fact is about the Sun rises and falls every day, then some examples of a "more general" rule can be (1) the Earth is the king of the universe or (2) the Earth is rotating itself.</p>
<p>Both rule examples are "more general" than the given fact, since the rule can entail not only the given fact, but also other not mentioned facts such as the observable movements of the other stars in the Milky Way.</p>
<p>A.11 Other Challenges and Possible Future Directions</p>
<p>Reliable Rule Generation Currently, the rule generation method in inductive reasoning relies on out-of-box LLMs, since a finetuned rule generation model could be restricted in a domain.The annotation of an inductive reasoning dataset should only be done by experts and is very time consuming (Yang et al., 2022b).Given the two restrictions, how to improve the quality of generated rules given related facts could be a challenging open problem.</p>
<p>Reliable Explanation Generation Abduction is a form of non-monotonic reasoning (Paul, 1993), and potentially has a large search space of conclusions given premises.Therefore, how to generate more (all) reasonable explanations can be challenging (Bhagavatula et al., 2020).</p>
<p>Building Larger Benchmarks For complicated reasoning tasks especially in realistic and natural language settings, usually experts are needed for annotation, and the process is very timeconsuming (Dalvi et al., 2021;Sprague et al., 2022;Yang et al., 2022b).Therefore it can be challenging to construct significantly larger benchmarks.</p>
<p>Understanding the Internal Mechanism of LLMs for Reasoning Until now research works only focused on investigating whether the input/output behaviors of LLMs can be used to simulate a reasoner (Clark et al., 2020) or complete reasoning tasks.However, it is still a challenging open research question to understand the internal mechanism of LLMs for reasoning.</p>
<p>Reliable Verifier Most current verifiers (refiners) relies on the internal beliefs of LLMs to select (improve) from generated rules and mitigate halluci-nation.It's doubtful whether LLMs have obtained the necessary knowledge.</p>
<p>Figure 1 :
1
Figure 1: Comparison between the previous paradigm which uses formal representation and symbolic reasoner, and the new paradigm which uses natural language as knowledge representation and LLM as reasoner.</p>
<p>Tafjord et al. (2021) explore the ability of a fine-tuned T5-11B(Raffel et al., 2020) on P (h|C, O).Their results indicate that finetuned T5-11B can reach a high test accuracy of 93% on D*-Ab.</p>
<p>Table 2 :
2
Methods for Proof Generation task."Generation based" means whether proof is created by generative inference model, otherwise is by utilizing embeddings to classify nodes and edges of proof ."Inference w/ hypothesis" means whether hypothesis is provided during inference.â†’ and â† denote forward/backward stepwise proof generation."Heuristic search" with âœ— means exhaustive search."Human-authored realistic proof" means whether the dataset adopted uses human-authored proof , whose contents are consistent with the real world.</p>
<p>Explanation Classification Given observation O 1 at time t 1 , observation O 2 at time t 2 (t 2 &gt; t 1 ), a plausible hypothesis h + and a implausible hypothesis h âˆ’ that explain O 1 and O 2 , this task is to select the most plausible hypothesis from h + and h âˆ’ .O 1 and O 2 each contains a single sentence.Explanation Generation without Theory Given observation O 1 at time t 1 , observation O 2 at time t 2 (t 2 &gt; t 1 ), this task is to generate a valid hypothesis h + given O 1 and O 2 .O 1 and O 2 each is described in a single sentence.Explanation Generation with Theory Given a theory C and a possible observation O not provable from C, the task is to generate a new hypothetical fact h such that C âˆª {h} |= O.Here C contains multiple facts and rules, where each fact or rule contains a single sentence.O is in single sentence.
Paul and Frank (2020) encode and incorporateknowledge from COMET's generation (Bosselutet al., 2019) directly into transformer's internal at-tention; Lourie et al. (2021) and Paul and Frank(2021) incorporate knowledge by multi-task train-ing; Du et al. (2021) incorporate knowledge withan additional pre-training stage using ARI inde-pendent story corpora;6.2 Methods6.2.1 Explanation ClassificationMethods for this task generally introduce knowl-edge in various ways to improve performance.Specifically, Mitra et al. (2019) explore ways toincorporate additional unstructured textual knowl-edge retrieved from a story corpus through prompt;</p>
<p>Wang and Pan (2022)propose attentive memories with novel differentiable logic operators to induce symbolic rules from texts.
A.6 Research Trend in the Three Sub-Typesof Logical ReasoningOut of the three reasoning types, deductive reason-ing has drawn the most research attention, and hasthe most abundant of works, especially in 2022.Abductive reasoning has drawn much attention in2020 and 2021 but has few works in 2022 and 2023.Inductive reasoning is only proposed at the end of2022, having the least number of works. However,inductive reasoning has attracted much attentionsince the second half year of 2023.
Ethics StatementThis article follows the ACL Code of Ethics.To our knowledge, there are no foreseeable potential risks to use the datasets and methods in this paper.
Case-based reasoning:foundational issues,methodological variations,and system approaches. A Aamodt, E Plaza, 1994AI communications</p>
<p>Multi-step deductive reasoning over natural language: An empirical study on out-of-distribution generalisation. Qiming Bao, Alex Yuxuan Peng, Tim Hartill, Neset Tan, Zhenyun Deng, Michael Witbrock, Jiamou Liu, The 2nd International Joint Conference on Learning and Reasoning and 16th International Workshop on Neural-Symbolic Learning and Reasoning. 2022. 2022</p>
<p>Abductive commonsense reasoning. Chandra Bhagavatula, Le Ronan, Chaitanya Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Wen-Tau Downey, Yejin Yih, Choi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>PIQA: reasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Le Ronan, Jianfeng Bras, Yejin Gao, Choi, The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020. February 7-12, 20202020The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</p>
<p>COMET: commonsense transformers for automatic knowledge graph construction. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi, 10.18653/v1/p19-1470Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Long Papers. the 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, Italy2019. July 28-August 2, 20191Association for Computational Linguistics</p>
<p>Natural language deduction through search over statement compositions. Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, Greg Durrett, CoRR, abs/2201.060282022</p>
<p>A large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, 10.18653/v1/d15-1075Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalThe Association for Computational Linguistics2015. 2015. September 17-21, 2015</p>
<p>Senticnet 7: A commonsense-based neurosymbolic AI framework for explainable sentiment analysis. Erik Cambria, Qian Liu, Sergio Decherchi, Frank Xing, Kenneth Kwok, Proceedings of the Thirteenth Language Resources and Evaluation Conference, LREC 2022. the Thirteenth Language Resources and Evaluation Conference, LREC 2022Marseille, FranceEuropean Language Resources Association2022. 20-25 June 2022</p>
<p>A survey on XAI and natural language explanations. Erik Cambria, Lorenzo Malandri, Fabio Mercorio, Mario Mezzanzanica, Navid Nobani, 10.1016/j.ipm.2022.103111Inf. Process. Manag. 6011031112023</p>
<p>Chunkit Chan, Xin Liu, Tsz Ho Chan, Jiayang Cheng, Yangqiu Song, Ginny Wong, Simon See, arXiv:2309.08303Self-consistent narrative prompts on abductive natural language inference. 2023arXiv preprint</p>
<p>On the measure of intelligence. FranÃ§ois Chollet, CoRR, abs/1911.015472019</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, 10.24963/ijcai.2020/537Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. the Twenty-Ninth International Joint Conference on Artificial Intelligence20202020</p>
<p>On the relations between abductive and inductive explanation. Abduction and Induction: Essays on their Relation and Integration. Luca Console, Lorenza Saitta, 2000</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, 10.48550/arXiv.2205.09712CoRR, abs/2205.097122022</p>
<p>Inductive logic programming at 30. Andrew Cropper, Sebastijan Dumancic, Richard Evans, Stephen H Muggleton, 10.1007/s10994-021-06089-1Mach. Learn. 11112022</p>
<p>The pascal recognising textual entailment challenge. Ido Dagan, Oren Glickman, Bernardo Magnini, Machine learning challenges workshop. Springer2005</p>
<p>Explaining answers with entailment trees. Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, Peter Clark, 10.18653/v1/2021.emnlp-main.585Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaAssociation for Computational Linguistics2021. 7-11 November, 2021</p>
<p>Towards teachable reasoning systems. Bhavana Dalvi, Oyvind Tafjord, Peter Clark, 10.48550/arXiv.2204.13074CoRR, abs/2204.130742022</p>
<p>Knowledge base question answering by case-based reasoning over subgraphs. Rajarshi Das, Ameya Godbole, Ankita Naik, Elliot Tower, Manzil Zaheer, Hannaneh Hajishirzi, Robin Jia, Andrew Mccallum, International Conference on Machine Learning, ICML 2022. Baltimore, Maryland, USAPMLR2022. July 2022162of Proceedings of Machine Learning Research</p>
<p>Casebased reasoning for natural language queries over knowledge bases. Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, Andrew Mccallum, 10.18653/v1/2021.emnlp-main.755Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaDominican Republic2021. 7-11 November, 2021Association for Computational Linguistics</p>
<p>Commonsense knowledge mining from pretrained models. Joe Davison, Joshua Feldman, Alexander M Rush, 10.18653/v1/D19-1109Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, China2019. November 3-7, 2019Association for Computational Linguistics</p>
<p>Learning event graph knowledge for abductive reasoning. Li Du, Xiao Ding, Ting Liu, Bing Qin, 10.18653/v1/2021.acl-long.403Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 20212021. August 1-6, 20211Virtual Event. Association for Computational Linguistics</p>
<p>Abductive and inductive reasoning: background and issues. A Peter, Antonis C Flach, Kakas, Abduction and induction. Springer2000</p>
<p>Logically consistent adversarial attacks for soft theorem provers. Alexander Gaskell, Yishu Miao, Francesca Toni, Lucia Specia, 10.24963/ijcai.2022/573Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022. the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022Vienna, Austria2022. July 2022</p>
<p>The reasoning brain: The interplay between cognitive neuroscience and theories of reasoning. Vinod Goel, Gorka Navarrete, Ira A Noveck, JÃ©rÃ´me Prado, 2017</p>
<p>Ben Goertzel, Nil Geisweiller, Lucio Coelho, Predrag JaniÄiÄ‡, Cassio Pennachin, Real-World Reasoning: Toward Scalable, Uncertain Spatiotemporal, Contextual and Causal Inference. Springer Science &amp; Business Media20112</p>
<p>Measuring systematic generalization in neural proof generation with transformers. Nicolas Gontier, Koustuv Sinha, Siva Reddy, Christopher Pal, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. NeurIPS2020. 2020. 2020. December 6-12, 2020</p>
<p>Reasoning about uncertainty. Joseph Y Halpern, 2017MIT press</p>
<p>FOLIO: natural language reasoning with first-order logic. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R Joty, Alexander R Fabbri, Wojciech Kryscinski, 10.48550/arXiv.2209.00840CoRR, abs/2209.008402022Xi Victoria Lin, Caiming Xiong, and Dragomir Radev</p>
<p>METGEN: A modulebased entailment tree generation framework for answer explanation. Ruixin Hong, Hongming Zhang, Xintong Yu, Changshui Zhang, 10.18653/v1/2022.findings-naacl.145Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, WA, United StatesAssociation for Computational Linguistics2022. July 10-15, 2022</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.48550/arXiv.2212.10403CoRR, abs/2212.104032022</p>
<p>Language generation with multi-hop reasoning on commonsense knowledge graph. Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, Minlie Huang, 10.18653/v1/2020.emnlp-main.54Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020. the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020Online2020. November 16-20, 2020Association for Computational Linguistics</p>
<p>Information extraction. Ji Heng, 10.1007/978-1-4614-8265-9_204Encyclopedia of Database Systems. Ling Liu, M Tamer Ã–zsu, Springer2018Second Edition</p>
<p>Hover: A dataset for many-hop fact extraction and claim verification. Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Kumar Singh, Mohit Bansal, 10.18653/v1/2020.findings-emnlp.309Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics2020. 16-20 November 2020EMNLP 2020 of Findings of ACL</p>
<p>Embarrassingly simple performance prediction for abductive natural language inference. Emils Kadikis, Vaibhav Srivastav, Roman Klinger, 10.18653/v1/2022.naacl-main.441Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022Seattle, WA, United StatesAssociation for Computational Linguistics2022. July 10-15, 2022</p>
<p>The third ai summer: Aaai robert s. engelmore memorial lecture. Henry Kautz, AI Magazine. 4312022</p>
<p>Educational implications of analogy: A view from case-based reasoning. Janet L Kolodner, American psychologist. 521571997</p>
<p>Parsing algebraic word problems into equations. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, Siena Dumas, Ang , 10.1162/tacl_a_00160Trans. Assoc. Comput. Linguistics. 32015</p>
<p>Linguistics and natural logic. George Lakoff, Synthese. 221-21970</p>
<p>Explainable multi-hop verbal reasoning through internal monologue. Zhengzhong Liang, Steven Bethard, Mihai Surdeanu, 10.18653/v1/2021.naacl-main.97Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, OnlineAssociation for Computational Linguistics2021. June 6-11, 2021</p>
<p>Roberta: A robustly optimized BERT pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, CoRR, abs/1907.116922019</p>
<p>UNICORN on RAINBOW: A universal commonsense reasoning model on a new multitask benchmark. Nicholas Lourie, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. AAAI Press2021. February 2-9, 2021</p>
<p>Natural logic and natural language inference. Bill Maccartney, Christopher D Manning, Computing Meaning. Springer20144</p>
<p>An example for natural language understanding and the ai problems it raises. John Mccarthy, Formalizing Common Sense: Papers by John McCarthy. 1990355</p>
<p>Distantlysupervised named entity recognition with noiserobust learning and language model augmented selftraining. Yu Meng, Yunyi Zhang, Jiaxin Huang, Xuan Wang, Yu Zhang, Heng Ji, Jiawei Han, 10.18653/v1/2021.emnlp-main.810Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaAssociation for Computational Linguistics2021. 7-11 November, 2021</p>
<p>Expert systems in production planning and scheduling: A state-of-the-art survey. Kostas S Metaxiotis, Dimitris Askounis, John E Psarras, 10.1023/A%3A1016064126976J. Intell. Manuf. 1342002</p>
<p>Distributed representations of words and phrases and their compositionality. TomÃ¡s Mikolov, Ilya Sutskever, Kai Chen, Gregory S Corrado, Jeffrey Dean, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held. Lake Tahoe, Nevada, United States2013. December 5-8, 2013</p>
<p>Multi-hop reading comprehension through question decomposition and rescoring. Sewon Min, Victor Zhong, Luke Zettlemoyer, Hannaneh Hajishirzi, 10.18653/v1/p19-1613Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Long Papers. the 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational Linguistics2019. July 28-August 2, 20191</p>
<p>A property induction framework for neural language models. Kanishka Misra, Julia Taylor Rayz, Allyson Ettinger, 10.48550/arXiv.2205.06910CoRR, abs/2205.069102022</p>
<p>Exploring ways to incorporate additional knowledge to improve natural language commonsense question answering. Arindam Mitra, Pratyay Banerjee, CoRR, abs/1909.088552019Kuntal Kumar Pal, Swaroop Mishra, and Chitta Baral</p>
<p>Inductive logic programming: Theory and methods. Stephen H Muggleton, Luc De, Raedt , 10.1016/0743-1066(94)90035-3J. Log. Program. 19201994</p>
<p>ILP turns 20 -biography and future challenges. H Stephen, Luc Muggleton, David De Raedt, Ivan Poole, Peter A Bratko, Katsumi Flach, Ashwin Inoue, Srinivasan, 10.1007/s10994-011-5259-2Mach. Learn. 8612012</p>
<p>Of brittleness and bottlenecks: Challenges in the creation of pattern-recognition and expert-system models. A Mark, Johan Musen, Van Der Lei, Machine Intelligence and Pattern Recognition. Elsevier19887</p>
<p>Logical Reasoning and Learning. Terezinha Nunes, 10.1007/978-1-4419-1428-6_7902012Springer USBoston, MA</p>
<p>Social commonsense reasoning with multi-head knowledge attention. Debjit Paul, Anette Frank, 10.18653/v1/2020.findings-emnlp.267Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics2020. 16-20 November 2020EMNLP 2020 of Findings of ACL</p>
<p>Generating hypothetical events for abductive inference. Debjit Paul, Anette Frank, 10.18653/v1/2021.starsem-1.6Proceedings of <em>SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics, </em>SEM 2021. <em>SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics, </em>SEM 2021Association for Computational Linguistics2021. August 5-6, 2021</p>
<p>Approaches to abductive reasoning: an overview. Gabriele Paul, 10.1007/BF00849080Artif. Intell. Rev. 721993</p>
<p>Collected papers of charles sanders peirce. Charles Sanders, Peirce , 1974Harvard University Press5</p>
<p>Reasoning with language model prompting: A survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, 10.48550/arXiv.2212.09597CoRR, abs/2212.095972022</p>
<p>Back to the future: Unsupervised backprop-based decoding for counterfactual and abductive commonsense reasoning. Lianhui Qin, Vered Shwartz, Peter West, Chandra Bhagavatula, Jena D Hwang, Le Ronan, Antoine Bras, Yejin Bosselut, Choi, 10.18653/v1/2020.emnlp-main.58Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics2020. 2020. November 16-20, 2020</p>
<p>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. Linlu Qiu, Liwei Jiang, 2023</p>
<p>Interpretable proof generation via iterative backward reasoning. Hanhao Qu, Yu Cao, Jun Gao, Liang Ding, Ruifeng Xu, 10.18653/v1/2022.naacl-main.216Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022Seattle, WA, United StatesAssociation for Computational Linguistics2022. July 10-15, 2022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 211402020</p>
<p>Entailment tree explanations via iterative retrieval-generation reasoner. Danilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Rui Dong, Xiaokai Wei, Henghui Zhu, Xinchi Chen, Peng Xu, Zhiheng Huang, Andrew O Arnold, Dan Roth, 10.18653/v1/2022.findings-naacl.35Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, WA, United StatesAssociation for Computational Linguistics2022. July 10-15, 2022</p>
<p>Pushing the limits of rule reasoning in transformers through natural language satisfiability. Kyle Richardson, Ashish Sabharwal, Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence. AAAI Press2022. February 22 -March 1, 20222022The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event</p>
<p>Prover: Proof generation for interpretable reasoning over rules. Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, Mohit Bansal, 10.18653/v1/2020.emnlp-main.9Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics2020. 2020. November 16-20, 2020</p>
<p>multiprover: Generating multiple proofs for improved interpretability in rule reasoning. Swarnadeep Saha, Prateek Yadav, Mohit Bansal, 10.18653/v1/2021.naacl-main.287Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021Association for Computational Linguistics2021. June 6-11, 2021</p>
<p>Introduction to logic and critical thinking. Merrilee H Salmon, 1989</p>
<p>Robustlr: Evaluating robustness to logical perturbation in deductive reasoning. Soumya Sanyal, Zeyi Liao, Xiang Ren, 10.48550/arXiv.2205.12598CoRR, abs/2205.125982022a</p>
<p>Fairr: Faithful and robust deductive reasoning over natural language. Soumya Sanyal, Harman Singh, Xiang Ren, 10.18653/v1/2022.acl-long.77Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022b. May 22-27, 20221ACL 2022</p>
<p>Solving geometry problems: Combining text and diagram interpretation. Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, Clint Malcolm, 10.18653/v1/d15-1171Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalThe Association for Computational Linguistics2015. 2015. September 17-21, 2015</p>
<p>CLUTRR: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, 10.18653/v1/D19-1458Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational Linguistics2019. November 3-7, 2019</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Thirty-first AAAI conference on artificial intelligence. 2017</p>
<p>Natural language deduction with incomplete information. Zayne Sprague, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett, 10.48550/arXiv.2211.00614CoRR, abs/2211.006142022</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021. Association for Computational Linguistics2021. August 1-6, 2021ACL/IJCNLP 2021 of Findings of ACL</p>
<p>Entailer: Answering questions with faithful and truthful chains of reasoning. Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark, 10.48550/arXiv.2210.12217CoRR, abs/2210.122172022</p>
<p>Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant, Advances in Neural Information Processing Systems. 202033</p>
<p>Dung Thai, Dhruv Agarwal, Mudit Chaudhary, Rajarshi Das, Manzil Zaheer, Jay-Yoon Lee, Hannaneh Hajishirzi, Andrew Mccallum, arXiv:2305.14815Machine reading comprehension using case-based reasoning. 2023arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Learning to generate novel scientific directions with contextualized literature-based discovery. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.48550/arXiv.2305.14259CoRR, abs/2305.142592023a</p>
<p>Hypothesis search: Inductive reasoning with language models. Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D Goodman, 10.48550/arXiv.2309.05660CoRR, abs/2309.056602023b</p>
<p>Wenguan Wang, Yi Yang, arXiv:2210.15889Towards dataand knowledge-driven artificial intelligence: A survey on neuro-symbolic computing. 2022arXiv preprint</p>
<p>Deep inductive logic reasoning for multi-hop reading comprehension. Wenya Wang, Sinno Jialin Pan, 10.18653/v1/2022.acl-long.343Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022. May 22-27, 20221ACL 2022</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H Chi, Quoc Le, Denny Zhou, CoRR, abs/2201.119032022</p>
<p>Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, arXiv:2306.09841Are large language models really good logical reasoners? a comprehensive evaluation from deductive, inductive and abductive views. Jun Liu, and Erik Cambria. 2023arXiv preprint</p>
<p>Generating natural language proofs with verifier-guided search. Kaiyu Yang, Jia Deng, Danqi Chen, 10.48550/arXiv.2205.12443CoRR, abs/2205.124432022a</p>
<p>Hotpotqa: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/d18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018. October 31 -November 4. 2018</p>
<p>Language models as inductive reasoners. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, 10.48550/arXiv.2212.10923CoRR, abs/2212.109232022b</p>
<p>End-to-end case-based reasoning for commonsense knowledge base completion. Zonglin Yang, Xinya Du, Erik Cambria, Claire Cardie, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational LinguisticsDubrovnik, CroatiaAssociation for Computational Linguistics2023a</p>
<p>Soujanya Poria, and Erik Cambria. 2023b. Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, 10.48550/arXiv.2309.02726CoRR, abs/2309.02726</p>
<p>Improving event duration prediction via time-aware pre-training. Zonglin Yang, Xinya Du, Alexander M Rush, Claire Cardie, 10.18653/v1/2020.findings-emnlp.302Findings of the Association for Computational Linguistics: EMNLP 2020. 2020. 16-20 November 2020EMNLP 2020 of Findings of ACL. Association for Computational Linguistics</p>
<p>Abductionrules: Training transformers to explain unexpected inputs. Nathan Young, Qiming Bao, Joshua Bensemann, Michael Witbrock, 10.18653/v1/2022.findings-acl.19Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022. May 22-27, 2022</p>
<p>Fei Yu, Hongbo Zhang, Benyou Wang, arXiv:2303.14725Nature language reasoning, a survey. 2023arXiv preprint</p>
<p>Abductive commonsense reasoning exploiting mutually exclusive explanations. Wenting Zhao, Justin T Chiu, Claire Cardie, Alexander M Rush, arXiv:2305.146182023arXiv preprint</p>
<p>Goal driven discovery of distributional differences via language descriptions. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt, 10.48550/arXiv.2302.14233CoRR, abs/2302.142332023</p>
<p>L2r 2 : Leveraging ranking for abductive reasoning. Yunchang Zhu, Liang Pang, Yanyan Lan, Xueqi Cheng, 10.1145/3397271.3401332Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event. the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual EventChinaACM2020. July 25-30, 2020</p>
<p>Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, Hanjun Dai, arXiv:2310.07064Large language models can learn rules. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8755 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8755</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8755</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-277510002</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.02181v1.pdf" target="_blank">A Survey of Scaling in Large Language Model Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> The rapid advancements in large Language models (LLMs) have significantly enhanced their reasoning capabilities, driven by various strategies such as multi-agent collaboration. However, unlike the well-established performance improvements achieved through scaling data and model size, the scaling of reasoning in LLMs is more complex and can even negatively impact reasoning performance, introducing new challenges in model alignment and robustness. In this survey, we provide a comprehensive examination of scaling in LLM reasoning, categorizing it into multiple dimensions and analyzing how and to what extent different scaling strategies contribute to improving reasoning capabilities. We begin by exploring scaling in input size, which enables LLMs to process and utilize more extensive context for improved reasoning. Next, we analyze scaling in reasoning steps that improves multi-step inference and logical consistency. We then examine scaling in reasoning rounds, where iterative interactions refine reasoning outcomes. Furthermore, we discuss scaling in training-enabled reasoning, focusing on optimization through iterative model improvement. Finally, we review applications of scaling across domains and outline future directions for further advancing LLM reasoning. By synthesizing these diverse perspectives, this survey aims to provide insights into how scaling strategies fundamentally enhance the reasoning capabilities of LLMs and further guide the development of next-generation AI systems.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8755.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8755.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-correction / Self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-correction / Self-reflection (meta-reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general class of methods where an LLM generates an initial response, produces feedback or verification about that response (intrinsically or via external critics/tools), and then generates a revised/refined answer to improve correctness and coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey-level references to a wide range of large language models and sizes; no single model or size is consistently reported in the survey for this class of methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-correction / Self-reflection (generate-feedback-refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an initial answer, generate feedback (either intrinsic verification questions, low-probability token checks, or critiques from an external model/tool), then produce a refined response conditioned on input, initial answer, and feedback. Sources of feedback include intrinsic prompts or external critics/tools.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>various reasoning and NLP tasks (survey-level)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Described broadly across tasks requiring multi-step reasoning, factuality, summarization, and other NLP tasks where iterative refinement can be applied; no single benchmark numbers reported in the survey for this general category.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering (intrinsic feedback prompts), external critic models or tools (search, code interpreters), sometimes process-reward models — implemented at generation time or post-hoc.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey cites multiple works showing LLMs can self-correct with prompt engineering and that generate-then-reflect pipelines produce improved refined responses qualitatively; specific quantitative results are not reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Efficiency concerns (iterative generation increases inference time); dependence on critic/tool accuracy; generation-time correction may be infeasible for long outputs (e.g., summarization) because accurate feedback often requires full output; potential failure when critics provide incorrect feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Discussed alongside chain-of-thought and post-hoc correction methods; self-correction often complements CoT but has distinct efficiency and applicability tradeoffs (e.g., generation-time vs post-hoc).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scaling in Large Language Model Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8755.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8755.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoVe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoVe (verification-question planning and answering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that plans verification questions to check an initial LLM response, answers those verification questions, and uses the results to produce a revised answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLMs (survey reference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not specified in the survey; CoVe is described conceptually as an intrinsic self-verification approach.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Verification-question based self-correction (CoVe)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>After generating an initial response, the model plans a set of verification questions targeting the answer's weak points, answers those questions, and then revises the original answer using those answers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>general reasoning and verification tasks (survey-level)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to improve correctness of multi-step reasoning answers by targeted verification; no single benchmark numbers provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered intrinsic verification question generation and answering (no external critic required in the description).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey states CoVe uses planned verification to systematically improve final responses; evidence cited qualitatively but numeric performance not reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in the survey; general caveats of generation-time self-verification apply (efficiency, quality of generated verification questions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presented as an intrinsic-feedback style method distinct from external-critic approaches like REFINER/CRITIC.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scaling in Large Language Model Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8755.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8755.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLARE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLARE (iterative low-probability token checking)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsic self-correction method that iteratively generates the next token/sentence and checks whether it contains low-probability tokens as a heuristic for likely errors, using that signal to correct generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLMs (survey reference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey-level mention without specific model sizes or training details.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Low-probability-token iterative self-correction (FLARE)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iteratively generate temporary next sentences and detect low-probability tokens as indicators of potential mistakes; use this signal to trigger corrections during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>general generation/reasoning tasks (survey-level)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Intended to detect and correct local errors during generation, particularly applicable to sequential generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Intrinsic probability-based monitoring (internal token probability heuristics) during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey cites FLARE as an example of iterative self-correction that uses model-internal signals; quantitative outcomes not provided in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Generation-time monitoring may not work for tasks where quality can only be assessed after full output generation; efficiency overhead of checking during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with post-hoc correction and external-critic approaches; FLARE is an intrinsic detection/correction heuristic.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scaling in Large Language Model Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8755.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8755.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REFINER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REFINER: Reasoning Feedback on Intermediate Representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that obtains automated feedback on intermediate reasoning representations via a critic model and uses that feedback to refine final outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>REFINER: Reasoning Feedback on Intermediate Representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLMs (survey-level reference to REFINER framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey references REFINER as an approach that interacts with a critic model; the survey does not list the critic or base model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Critic-mediated intermediate feedback (REFINER)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Interact with a critic model that analyzes intermediate reasoning traces and provides automated feedback; use that feedback to revise and improve the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>reasoning tasks that use intermediate representations (survey-level)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Applies to tasks where intermediate reasoning steps can be represented and critiqued; survey does not provide a specific benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External critic model providing automated feedback on intermediate representations (tool-interactive critique).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey cites REFINER as an example of external-feedback based self-correction; quantitative improvements are not reproduced in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Effectiveness depends on critic accuracy and coverage; critic can be a bottleneck and adds computation and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared qualitatively with intrinsic self-reflection and tool-interactive critiquing (e.g., CRITIC).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scaling in Large Language Model Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8755.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8755.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRITIC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRITIC (tool-interactive critiquing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach where an LLM interacts with external tools (search engines, code interpreters) to verify aspects of an initial output and then amends the output based on critiques from those verification tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Critic: Large language models can self-correct with tool-interactive critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLMs (survey-level)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey references CRITIC as a tool-interactive self-correction pipeline; specific model sizes/training details are not provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Tool-interactive critiquing (CRITIC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Initial output is evaluated using external tools (search, code execution, etc.) and the information from tools is used to critique and refine the model's answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>tasks requiring external verification (e.g., factual QA, code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used where external tool outputs can validate or check particular aspects of a response; survey does not present benchmark numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External tools/search and tool-interaction pipelines that provide evidence or executable checks, driving revision.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey notes CRITIC as an example that interacts with external tools to verify and amend outputs; no numeric results included in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Depends on tool reliability and coverage; introduces additional inference-time latency and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Placed in contrast with intrinsic self-reflection methods and REFINER-like critic models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scaling in Large Language Model Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8755.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8755.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quiet-star / Quiet-STaR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quiet-star (Quiet-STaR): token-wise parallel self-thinking before speaking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An efficiency-oriented method that enables parallel tokenwise sampling for 'thinking' using special learnable tokens and extended teacher-forcing to reduce inference-time costs of iterative self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Quiet-star: Language models can teach themselves to think before speaking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLMs (survey reference to Quiet-star)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey-level mention; Quiet-star proposes architectural/sampling changes (tokenwise parallel sampling) and is described conceptually in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Tokenwise parallel self-reflection (Quiet-STaR)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Designs learnable tokens to indicate thought start/end and uses a tokenwise parallel sampling algorithm with extended teacher-forcing to parallelize generation of internal thoughts before producing final output, reducing inference-time overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>general reasoning tasks where generation-time self-refinement is used</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Aimed at tasks where iterative generation/feedback pipelines are otherwise computationally expensive; survey does not report explicit benchmark numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Specialized sampling algorithm and training technique (tokenwise parallel sampling, learnable thought delimiters, extended teacher-forcing) — an inference-time algorithmic change rather than simple prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey highlights Quiet-star as an approach addressing efficiency limitations of self-refinement; specific empirical results are not included in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey notes Quiet-star addresses scaling inefficiency but does not detail limitations; practical effectiveness depends on implementation and model compatibility.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presented as an efficiency-focused alternative to serial generate-reflect pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scaling in Large Language Model Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8755.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8755.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Post-hoc correction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Post-hoc correction (refinement after full generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A refinement strategy where the model generates a complete output and then refines that final output afterward rather than attempting generation-time correction of intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey-level conceptual description; not tied to a specific model in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Post-hoc correction</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Produce the final output, then generate feedback on the full output and produce a corrected/refined version (post-hoc), useful when intermediate quality assessment is infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>long-output tasks (e.g., summarization) where intermediate assessment is hard</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where quality cannot be judged until the whole output is produced; post-hoc correction aims to overcome generation-time correction limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered or tool-interactive postprocessing applied after full generation (post-hoc prompts, external evaluation to guide revision).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey recommends post-hoc correction when generation-time feedback is infeasible; no numerical comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires an extra generation pass and may still be limited by the model's ability to interpret critique of a long output; adds latency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with generation-time correction methods (e.g., intrinsic-token checks) which may not apply to long outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scaling in Large Language Model Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8755.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8755.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concrete iterative refinement method where the model generates an answer, creates self-feedback, and refines its answer over multiple iterations (as described in the cited Self-refine paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLMs (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The referenced Self-refine work proposes iterative self-feedback; the survey references it but does not detail models/sizes used.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative self-feedback (Self-refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an initial answer, produce self-feedback describing errors or improvements, then generate a revised answer; can be repeated for multiple refinement cycles depending on the implementation in the original work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>general reasoning and generation tasks (survey-level)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Applied to tasks where iterative self-feedback can improve final output quality; the survey does not provide benchmark metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered intrinsic self-feedback loop (model critiques its own outputs and regenerates).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey cites Self-refine as a representative iterative refinement method; quantitative results from that paper are not included in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>General limitations of iterative self-feedback apply (cost, potential for reinforcing errors if feedback is poor); survey does not enumerate ablations for Self-refine.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned alongside other intrinsic and external-feedback iterative methods.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scaling in Large Language Model Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8755.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8755.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-agent approach that uses verbal reinforcement learning loops where agents generate actions/answers, reflect using stored traces and rewards, and update behavior; often framed as agents learning from verbal feedback over iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>language-agent frameworks (survey-level reference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey references Reflexion as an agentic framework that couples generation with reflection and RL-style updates; specific model sizes or training details not provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Verbal RL reflection (Reflexion)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Language agents iteratively generate, reflect on outcomes (using verbal traces and reward signals), and adapt future generations/actions — combines iterative reflection with reinforcement learning principles.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>agentic reasoning and long-horizon tasks (survey-level)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Designed for tasks where agents benefit from iterative learning from reflection and feedback across episodes; survey mentions Reflexion among agentic learning approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Agentic loop combining generation, verbalized reflection, and RL-style reward signals; uses stored traces/memories for iterative improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey lists Reflexion as an example of verbal RL for iterative improvement in agentic settings; numerical results are not included in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Agentic RL loops add complexity and compute; success depends on reward signals and memory management; survey does not report specific failure cases for Reflexion.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared conceptually to other RL-based and iterative-refinement methods in the model-optimization and multi-round reasoning sections.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scaling in Large Language Model Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8755.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8755.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency (multiple-chain CoT sampling and voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parallel-sampling method that generates multiple chain-of-thought answers for the same prompt and selects the final answer by majority voting or another selection mechanism to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (used in CoT research; survey cites SC conceptually)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-consistency is described as a decoding/selection strategy applied at inference to CoT-capable LLMs; specific model sizes are not listed in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-consistency (parallel multiple-CoT sampling + selection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample many chain-of-thought outputs in parallel from the model and select the final answer by majority frequency or by scoring (self-consistency/Best-of-N approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>multi-step reasoning benchmarks (e.g., tasks typically solved with Chain-of-Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Designed for reasoning tasks where sampling multiple reasoning chains and aggregating reduces stochastic errors; the survey references this technique but does not provide numerical benchmarks in-place.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Repeated sampling + selection/aggregation at inference (not a generate-then-reflect loop but an ensemble/consensus mechanism).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey describes self-consistency and Best-of-N as effective ways to mitigate greedy-decoding errors and improve CoT outputs; precise numeric improvements are not reproduced in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Computationally expensive (many samples); can be inefficient and may allocate budget to poor branches; may still require pruning/search strategies for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with tree-search and prioritized sampling methods that aim to allocate compute more efficiently than naive parallel sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scaling in Large Language Model Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8755.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8755.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Agent Debate (MAD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Debate (MAD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A debate-style, multi-round interaction framework in which multiple agent LLMs argue for different answers and a judge model evaluates the debate to determine a final answer; iterative rounds of debate aim to refine accuracy and reduce convergence to incorrect answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving factuality and reasoning in language models through multiagent debate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multi-agent LLM systems (agents and judge LLMs, unspecified sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey-level description of MAD: multiple LLM agents (possibly of different strengths) and a judge LLM oversee debate rounds; specific architectures/sizes vary by cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Multi-round debate with judge evaluation (MAD)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Agents iteratively debate (tit-for-tat style) with a judge overseeing; judge combines evidence across rounds and selects final answer, enabling iterative refinement via adversarial exchange.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>factuality and reasoning tasks (survey-level)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to improve factual accuracy and reasoning through adversarial multi-agent discourse; specific benchmarks are discussed in cited works but numeric values are not provided in the survey extract.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Structured multi-agent dialogue (debate) with an evaluating judge model; iterative rounds of argumentation provide refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey reports MAD leads to higher disagreement (useful for avoiding false consensus) and that debate frameworks improve factuality/reasoning in some settings; one cited study found performance benefits up to about three rounds, beyond which returns diminish.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Increasing rounds can introduce confusion for weaker models; performance plateaus or deteriorates beyond a threshold of rounds; scaling number of agents or rounds can introduce coordination overhead and diminishing returns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to Self-Reflection approaches: MAD yields higher disagreement and can reduce convergence on incorrect answers; consultancy-style interactions behave differently (judge accuracy gains over rounds), showing the debate paradigm's behavior is framework-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Scaling in Large Language Model Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>REFINER: Reasoning Feedback on Intermediate Representations <em>(Rating: 2)</em></li>
                <li>Critic: Large language models can self-correct with tool-interactive critiquing <em>(Rating: 2)</em></li>
                <li>Quiet-star: Language models can teach themselves to think before speaking <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 2)</em></li>
                <li>Self-generated in-context learning: Leveraging autoregressive language models as a demonstration generator <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Badchain: Backdoor chain-of-thought prompting for large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8755",
    "paper_id": "paper-277510002",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Self-correction / Self-reflection",
            "name_full": "Self-correction / Self-reflection (meta-reasoning)",
            "brief_description": "A general class of methods where an LLM generates an initial response, produces feedback or verification about that response (intrinsically or via external critics/tools), and then generates a revised/refined answer to improve correctness and coherence.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "various LLMs (unspecified)",
            "model_description": "Survey-level references to a wide range of large language models and sizes; no single model or size is consistently reported in the survey for this class of methods.",
            "reflection_method_name": "Self-correction / Self-reflection (generate-feedback-refine)",
            "reflection_method_description": "Generate an initial answer, generate feedback (either intrinsic verification questions, low-probability token checks, or critiques from an external model/tool), then produce a refined response conditioned on input, initial answer, and feedback. Sources of feedback include intrinsic prompts or external critics/tools.",
            "task_name": "various reasoning and NLP tasks (survey-level)",
            "task_description": "Described broadly across tasks requiring multi-step reasoning, factuality, summarization, and other NLP tasks where iterative refinement can be applied; no single benchmark numbers reported in the survey for this general category.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt engineering (intrinsic feedback prompts), external critic models or tools (search, code interpreters), sometimes process-reward models — implemented at generation time or post-hoc.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey cites multiple works showing LLMs can self-correct with prompt engineering and that generate-then-reflect pipelines produce improved refined responses qualitatively; specific quantitative results are not reported in this survey.",
            "limitations_or_failure_cases": "Efficiency concerns (iterative generation increases inference time); dependence on critic/tool accuracy; generation-time correction may be infeasible for long outputs (e.g., summarization) because accurate feedback often requires full output; potential failure when critics provide incorrect feedback.",
            "comparison_to_other_methods": "Discussed alongside chain-of-thought and post-hoc correction methods; self-correction often complements CoT but has distinct efficiency and applicability tradeoffs (e.g., generation-time vs post-hoc).",
            "ablation_study_results": null,
            "uuid": "e8755.0",
            "source_info": {
                "paper_title": "A Survey of Scaling in Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CoVe",
            "name_full": "CoVe (verification-question planning and answering)",
            "brief_description": "A method that plans verification questions to check an initial LLM response, answers those verification questions, and uses the results to produce a revised answer.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "unspecified LLMs (survey reference)",
            "model_description": "Not specified in the survey; CoVe is described conceptually as an intrinsic self-verification approach.",
            "reflection_method_name": "Verification-question based self-correction (CoVe)",
            "reflection_method_description": "After generating an initial response, the model plans a set of verification questions targeting the answer's weak points, answers those questions, and then revises the original answer using those answers.",
            "task_name": "general reasoning and verification tasks (survey-level)",
            "task_description": "Used to improve correctness of multi-step reasoning answers by targeted verification; no single benchmark numbers provided in the survey.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt-engineered intrinsic verification question generation and answering (no external critic required in the description).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey states CoVe uses planned verification to systematically improve final responses; evidence cited qualitatively but numeric performance not reported in the survey.",
            "limitations_or_failure_cases": "Not detailed in the survey; general caveats of generation-time self-verification apply (efficiency, quality of generated verification questions).",
            "comparison_to_other_methods": "Presented as an intrinsic-feedback style method distinct from external-critic approaches like REFINER/CRITIC.",
            "ablation_study_results": null,
            "uuid": "e8755.1",
            "source_info": {
                "paper_title": "A Survey of Scaling in Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "FLARE",
            "name_full": "FLARE (iterative low-probability token checking)",
            "brief_description": "An intrinsic self-correction method that iteratively generates the next token/sentence and checks whether it contains low-probability tokens as a heuristic for likely errors, using that signal to correct generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "unspecified LLMs (survey reference)",
            "model_description": "Survey-level mention without specific model sizes or training details.",
            "reflection_method_name": "Low-probability-token iterative self-correction (FLARE)",
            "reflection_method_description": "Iteratively generate temporary next sentences and detect low-probability tokens as indicators of potential mistakes; use this signal to trigger corrections during generation.",
            "task_name": "general generation/reasoning tasks (survey-level)",
            "task_description": "Intended to detect and correct local errors during generation, particularly applicable to sequential generation tasks.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Intrinsic probability-based monitoring (internal token probability heuristics) during generation.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey cites FLARE as an example of iterative self-correction that uses model-internal signals; quantitative outcomes not provided in this survey.",
            "limitations_or_failure_cases": "Generation-time monitoring may not work for tasks where quality can only be assessed after full output generation; efficiency overhead of checking during generation.",
            "comparison_to_other_methods": "Contrasted with post-hoc correction and external-critic approaches; FLARE is an intrinsic detection/correction heuristic.",
            "ablation_study_results": null,
            "uuid": "e8755.2",
            "source_info": {
                "paper_title": "A Survey of Scaling in Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "REFINER",
            "name_full": "REFINER: Reasoning Feedback on Intermediate Representations",
            "brief_description": "A method that obtains automated feedback on intermediate reasoning representations via a critic model and uses that feedback to refine final outputs.",
            "citation_title": "REFINER: Reasoning Feedback on Intermediate Representations",
            "mention_or_use": "mention",
            "model_name": "unspecified LLMs (survey-level reference to REFINER framework)",
            "model_description": "Survey references REFINER as an approach that interacts with a critic model; the survey does not list the critic or base model sizes.",
            "reflection_method_name": "Critic-mediated intermediate feedback (REFINER)",
            "reflection_method_description": "Interact with a critic model that analyzes intermediate reasoning traces and provides automated feedback; use that feedback to revise and improve the final answer.",
            "task_name": "reasoning tasks that use intermediate representations (survey-level)",
            "task_description": "Applies to tasks where intermediate reasoning steps can be represented and critiqued; survey does not provide a specific benchmark.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "External critic model providing automated feedback on intermediate representations (tool-interactive critique).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey cites REFINER as an example of external-feedback based self-correction; quantitative improvements are not reproduced in the survey text.",
            "limitations_or_failure_cases": "Effectiveness depends on critic accuracy and coverage; critic can be a bottleneck and adds computation and complexity.",
            "comparison_to_other_methods": "Compared qualitatively with intrinsic self-reflection and tool-interactive critiquing (e.g., CRITIC).",
            "ablation_study_results": null,
            "uuid": "e8755.3",
            "source_info": {
                "paper_title": "A Survey of Scaling in Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CRITIC",
            "name_full": "CRITIC (tool-interactive critiquing)",
            "brief_description": "An approach where an LLM interacts with external tools (search engines, code interpreters) to verify aspects of an initial output and then amends the output based on critiques from those verification tools.",
            "citation_title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "mention_or_use": "mention",
            "model_name": "unspecified LLMs (survey-level)",
            "model_description": "Survey references CRITIC as a tool-interactive self-correction pipeline; specific model sizes/training details are not provided in the survey.",
            "reflection_method_name": "Tool-interactive critiquing (CRITIC)",
            "reflection_method_description": "Initial output is evaluated using external tools (search, code execution, etc.) and the information from tools is used to critique and refine the model's answer.",
            "task_name": "tasks requiring external verification (e.g., factual QA, code generation)",
            "task_description": "Used where external tool outputs can validate or check particular aspects of a response; survey does not present benchmark numbers.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "External tools/search and tool-interaction pipelines that provide evidence or executable checks, driving revision.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey notes CRITIC as an example that interacts with external tools to verify and amend outputs; no numeric results included in the survey.",
            "limitations_or_failure_cases": "Depends on tool reliability and coverage; introduces additional inference-time latency and complexity.",
            "comparison_to_other_methods": "Placed in contrast with intrinsic self-reflection methods and REFINER-like critic models.",
            "ablation_study_results": null,
            "uuid": "e8755.4",
            "source_info": {
                "paper_title": "A Survey of Scaling in Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Quiet-star / Quiet-STaR",
            "name_full": "Quiet-star (Quiet-STaR): token-wise parallel self-thinking before speaking",
            "brief_description": "An efficiency-oriented method that enables parallel tokenwise sampling for 'thinking' using special learnable tokens and extended teacher-forcing to reduce inference-time costs of iterative self-reflection.",
            "citation_title": "Quiet-star: Language models can teach themselves to think before speaking",
            "mention_or_use": "mention",
            "model_name": "unspecified LLMs (survey reference to Quiet-star)",
            "model_description": "Survey-level mention; Quiet-star proposes architectural/sampling changes (tokenwise parallel sampling) and is described conceptually in the survey.",
            "reflection_method_name": "Tokenwise parallel self-reflection (Quiet-STaR)",
            "reflection_method_description": "Designs learnable tokens to indicate thought start/end and uses a tokenwise parallel sampling algorithm with extended teacher-forcing to parallelize generation of internal thoughts before producing final output, reducing inference-time overhead.",
            "task_name": "general reasoning tasks where generation-time self-refinement is used",
            "task_description": "Aimed at tasks where iterative generation/feedback pipelines are otherwise computationally expensive; survey does not report explicit benchmark numbers.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Specialized sampling algorithm and training technique (tokenwise parallel sampling, learnable thought delimiters, extended teacher-forcing) — an inference-time algorithmic change rather than simple prompt engineering.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey highlights Quiet-star as an approach addressing efficiency limitations of self-refinement; specific empirical results are not included in the survey text.",
            "limitations_or_failure_cases": "Survey notes Quiet-star addresses scaling inefficiency but does not detail limitations; practical effectiveness depends on implementation and model compatibility.",
            "comparison_to_other_methods": "Presented as an efficiency-focused alternative to serial generate-reflect pipelines.",
            "ablation_study_results": null,
            "uuid": "e8755.5",
            "source_info": {
                "paper_title": "A Survey of Scaling in Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Post-hoc correction",
            "name_full": "Post-hoc correction (refinement after full generation)",
            "brief_description": "A refinement strategy where the model generates a complete output and then refines that final output afterward rather than attempting generation-time correction of intermediate steps.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "various LLMs (unspecified)",
            "model_description": "Survey-level conceptual description; not tied to a specific model in the survey.",
            "reflection_method_name": "Post-hoc correction",
            "reflection_method_description": "Produce the final output, then generate feedback on the full output and produce a corrected/refined version (post-hoc), useful when intermediate quality assessment is infeasible.",
            "task_name": "long-output tasks (e.g., summarization) where intermediate assessment is hard",
            "task_description": "Tasks where quality cannot be judged until the whole output is produced; post-hoc correction aims to overcome generation-time correction limitations.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt-engineered or tool-interactive postprocessing applied after full generation (post-hoc prompts, external evaluation to guide revision).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey recommends post-hoc correction when generation-time feedback is infeasible; no numerical comparisons provided.",
            "limitations_or_failure_cases": "Requires an extra generation pass and may still be limited by the model's ability to interpret critique of a long output; adds latency.",
            "comparison_to_other_methods": "Contrasted with generation-time correction methods (e.g., intrinsic-token checks) which may not apply to long outputs.",
            "ablation_study_results": null,
            "uuid": "e8755.6",
            "source_info": {
                "paper_title": "A Survey of Scaling in Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Self-refine",
            "name_full": "Self-refine: Iterative refinement with self-feedback",
            "brief_description": "A concrete iterative refinement method where the model generates an answer, creates self-feedback, and refines its answer over multiple iterations (as described in the cited Self-refine paper).",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": "unspecified LLMs (referenced work)",
            "model_description": "The referenced Self-refine work proposes iterative self-feedback; the survey references it but does not detail models/sizes used.",
            "reflection_method_name": "Iterative self-feedback (Self-refine)",
            "reflection_method_description": "Generate an initial answer, produce self-feedback describing errors or improvements, then generate a revised answer; can be repeated for multiple refinement cycles depending on the implementation in the original work.",
            "task_name": "general reasoning and generation tasks (survey-level)",
            "task_description": "Applied to tasks where iterative self-feedback can improve final output quality; the survey does not provide benchmark metrics.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt-engineered intrinsic self-feedback loop (model critiques its own outputs and regenerates).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey cites Self-refine as a representative iterative refinement method; quantitative results from that paper are not included in the survey text.",
            "limitations_or_failure_cases": "General limitations of iterative self-feedback apply (cost, potential for reinforcing errors if feedback is poor); survey does not enumerate ablations for Self-refine.",
            "comparison_to_other_methods": "Positioned alongside other intrinsic and external-feedback iterative methods.",
            "ablation_study_results": null,
            "uuid": "e8755.7",
            "source_info": {
                "paper_title": "A Survey of Scaling in Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A language-agent approach that uses verbal reinforcement learning loops where agents generate actions/answers, reflect using stored traces and rewards, and update behavior; often framed as agents learning from verbal feedback over iterations.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "language-agent frameworks (survey-level reference)",
            "model_description": "Survey references Reflexion as an agentic framework that couples generation with reflection and RL-style updates; specific model sizes or training details not provided in the survey.",
            "reflection_method_name": "Verbal RL reflection (Reflexion)",
            "reflection_method_description": "Language agents iteratively generate, reflect on outcomes (using verbal traces and reward signals), and adapt future generations/actions — combines iterative reflection with reinforcement learning principles.",
            "task_name": "agentic reasoning and long-horizon tasks (survey-level)",
            "task_description": "Designed for tasks where agents benefit from iterative learning from reflection and feedback across episodes; survey mentions Reflexion among agentic learning approaches.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Agentic loop combining generation, verbalized reflection, and RL-style reward signals; uses stored traces/memories for iterative improvement.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey lists Reflexion as an example of verbal RL for iterative improvement in agentic settings; numerical results are not included in the survey.",
            "limitations_or_failure_cases": "Agentic RL loops add complexity and compute; success depends on reward signals and memory management; survey does not report specific failure cases for Reflexion.",
            "comparison_to_other_methods": "Compared conceptually to other RL-based and iterative-refinement methods in the model-optimization and multi-round reasoning sections.",
            "ablation_study_results": null,
            "uuid": "e8755.8",
            "source_info": {
                "paper_title": "A Survey of Scaling in Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Self-consistency (SC)",
            "name_full": "Self-consistency (multiple-chain CoT sampling and voting)",
            "brief_description": "A parallel-sampling method that generates multiple chain-of-thought answers for the same prompt and selects the final answer by majority voting or another selection mechanism to improve robustness.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "mention",
            "model_name": "various LLMs (used in CoT research; survey cites SC conceptually)",
            "model_description": "Self-consistency is described as a decoding/selection strategy applied at inference to CoT-capable LLMs; specific model sizes are not listed in the survey text.",
            "reflection_method_name": "Self-consistency (parallel multiple-CoT sampling + selection)",
            "reflection_method_description": "Sample many chain-of-thought outputs in parallel from the model and select the final answer by majority frequency or by scoring (self-consistency/Best-of-N approaches).",
            "task_name": "multi-step reasoning benchmarks (e.g., tasks typically solved with Chain-of-Thought)",
            "task_description": "Designed for reasoning tasks where sampling multiple reasoning chains and aggregating reduces stochastic errors; the survey references this technique but does not provide numerical benchmarks in-place.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Repeated sampling + selection/aggregation at inference (not a generate-then-reflect loop but an ensemble/consensus mechanism).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey describes self-consistency and Best-of-N as effective ways to mitigate greedy-decoding errors and improve CoT outputs; precise numeric improvements are not reproduced in the survey.",
            "limitations_or_failure_cases": "Computationally expensive (many samples); can be inefficient and may allocate budget to poor branches; may still require pruning/search strategies for efficiency.",
            "comparison_to_other_methods": "Contrasted with tree-search and prioritized sampling methods that aim to allocate compute more efficiently than naive parallel sampling.",
            "ablation_study_results": null,
            "uuid": "e8755.9",
            "source_info": {
                "paper_title": "A Survey of Scaling in Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Multi-Agent Debate (MAD)",
            "name_full": "Multi-Agent Debate (MAD)",
            "brief_description": "A debate-style, multi-round interaction framework in which multiple agent LLMs argue for different answers and a judge model evaluates the debate to determine a final answer; iterative rounds of debate aim to refine accuracy and reduce convergence to incorrect answers.",
            "citation_title": "Improving factuality and reasoning in language models through multiagent debate",
            "mention_or_use": "mention",
            "model_name": "multi-agent LLM systems (agents and judge LLMs, unspecified sizes)",
            "model_description": "Survey-level description of MAD: multiple LLM agents (possibly of different strengths) and a judge LLM oversee debate rounds; specific architectures/sizes vary by cited work.",
            "reflection_method_name": "Multi-round debate with judge evaluation (MAD)",
            "reflection_method_description": "Agents iteratively debate (tit-for-tat style) with a judge overseeing; judge combines evidence across rounds and selects final answer, enabling iterative refinement via adversarial exchange.",
            "task_name": "factuality and reasoning tasks (survey-level)",
            "task_description": "Used to improve factual accuracy and reasoning through adversarial multi-agent discourse; specific benchmarks are discussed in cited works but numeric values are not provided in the survey extract.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Structured multi-agent dialogue (debate) with an evaluating judge model; iterative rounds of argumentation provide refinement.",
            "number_of_iterations": 3,
            "evidence_for_improvement": "Survey reports MAD leads to higher disagreement (useful for avoiding false consensus) and that debate frameworks improve factuality/reasoning in some settings; one cited study found performance benefits up to about three rounds, beyond which returns diminish.",
            "limitations_or_failure_cases": "Increasing rounds can introduce confusion for weaker models; performance plateaus or deteriorates beyond a threshold of rounds; scaling number of agents or rounds can introduce coordination overhead and diminishing returns.",
            "comparison_to_other_methods": "Compared to Self-Reflection approaches: MAD yields higher disagreement and can reduce convergence on incorrect answers; consultancy-style interactions behave differently (judge accuracy gains over rounds), showing the debate paradigm's behavior is framework-dependent.",
            "ablation_study_results": null,
            "uuid": "e8755.10",
            "source_info": {
                "paper_title": "A Survey of Scaling in Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "REFINER: Reasoning Feedback on Intermediate Representations",
            "rating": 2,
            "sanitized_title": "refiner_reasoning_feedback_on_intermediate_representations"
        },
        {
            "paper_title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "rating": 2,
            "sanitized_title": "critic_large_language_models_can_selfcorrect_with_toolinteractive_critiquing"
        },
        {
            "paper_title": "Quiet-star: Language models can teach themselves to think before speaking",
            "rating": 2,
            "sanitized_title": "quietstar_language_models_can_teach_themselves_to_think_before_speaking"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 2,
            "sanitized_title": "improving_factuality_and_reasoning_in_language_models_through_multiagent_debate"
        },
        {
            "paper_title": "Self-generated in-context learning: Leveraging autoregressive language models as a demonstration generator",
            "rating": 1,
            "sanitized_title": "selfgenerated_incontext_learning_leveraging_autoregressive_language_models_as_a_demonstration_generator"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Badchain: Backdoor chain-of-thought prompting for large language models",
            "rating": 1,
            "sanitized_title": "badchain_backdoor_chainofthought_prompting_for_large_language_models"
        }
    ],
    "cost": 0.022213749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey of Scaling in Large Language Model Reasoning
2 Apr 2025</p>
<p>Zihan Chen 
Song Wang 
Zhen Tan 
Xingbo Fu 
Zhenyu Lei 
Peng Wang 
Huan Liu huanliu@asu.edu 
Cong Shen 
Jundong Li jundong@virginia.edu </p>
<p>University of Virginia Charlottesville
VAUSA</p>
<p>University of Virginia Charlottesville
VAUSA</p>
<p>Arizona State University Tempe
AZUSA</p>
<p>University of Virginia Charlottesville
VAUSA</p>
<p>University of Virginia Charlottesville
VAUSA</p>
<p>University of Virginia Charlottesville
VAUSA</p>
<p>Arizona State University Tempe
AZUSA</p>
<p>University of Virginia Charlottesville
VAUSA</p>
<p>University of Virginia Charlottesville
VAUSA</p>
<p>Conference acronym 'XX
03-05, 2018June, WoodstockNY</p>
<p>A Survey of Scaling in Large Language Model Reasoning
2 Apr 20252ACE40D99C67FEF48DE1B99B89CBBA42arXiv:2504.02181v1[cs.AI]Large language modelsLLM ReasoningScaling
The rapid advancements in large Language models (LLMs) have significantly enhanced their reasoning capabilities, driven by various strategies such as multi-agent collaboration.However, unlike the well-established performance improvements achieved through scaling data and model size, the scaling of reasoning in LLMs is more complex and can even negatively impact reasoning performance, introducing new challenges in model alignment and robustness.In this survey, we provide a comprehensive examination of scaling in LLM reasoning, categorizing it into multiple dimensions and analyzing how and to what extent different scaling strategies contribute to improving reasoning capabilities.We begin by exploring scaling in input size, which enables LLMs to process and utilize more extensive context for improved reasoning.Next, we analyze scaling in reasoning steps that improves multi-step inference and logical consistency.We then examine scaling in reasoning rounds, where iterative interactions refine reasoning outcomes.Furthermore, we discuss scaling in training-enabled reasoning, focusing on optimization through iterative model improvement.Finally, we review applications of scaling across domains and outline future directions for further advancing LLM reasoning.By synthesizing these diverse perspectives, this survey aims to provide insights into how scaling strategies fundamentally enhance the reasoning capabilities of LLMs and further guide the development of nextgeneration AI systems.CCS Concepts• Computing methodologies → Artificial intelligence; Machine learning; Natural language processing.</p>
<p>Introduction</p>
<p>Recently, Large Language Models (LLMs) have rapidly evolved, demonstrating remarkable advancements across various natural language processing (NLP) tasks, including text generation, comprehension, and problem-solving [67,68,153,[214][215][216].One of the key driving forces behind these improvements is scaling, where increasing the size of training data and model parameters has led to substantial performance gains [71,85,195].Scaling has played a pivotal role in the development of state-of-the-art LLMs such as GPT-4 [133], and Gemini [176], enabling them to generalize across a broad range of tasks with unprecedented accuracy and fluency [185].The empirical success of scaling laws has reinforced the notion that simply increasing model size and data availability can significantly enhance LLM capabilities [25,31,129].However, while such scaling strategies have led to more powerful models, they do not fully explain improvements in complex reasoning tasks, which require structured thinking, multi-step inference, and logical consistency [40,47,154].</p>
<p>Notably, unlike simpler tasks that rely on memorization or direct retrieval of information, reasoning demands deeper cognitive-like processes, including step-by-step deductions, counterfactual reasoning, and planning [83,141].While early LLMs exhibited shallow reasoning abilities [12,116], recent advancements have introduced techniques aimed at enhancing LLM reasoning performance through various strategies [33,54,164].For instance, s1 [130] explicitly extends the reasoning length, enabling models to engage in deeper, iterative reasoning that can identify and correct errors in previous inference steps.However, scaling reasoning length does not always guarantee improved performance-simply increasing the number of reasoning steps may introduce redundancy, compounding errors, or even diminished accuracy [74,124,148].This highlights the complex and non-trivial nature of scaling in reasoning, necessitating a deeper investigation into how different scaling strategies influence LLM reasoning effectiveness and when they yield diminishing returns.</p>
<p>This survey aims to provide a comprehensive examination of scaling in LLM reasoning.Particularly, we categorize it into multiple dimensions and analyze how and to what extent different scaling strategies contribute to improved reasoning performance.We begin by discussing scaling in input size, which enables models to leverage larger contexts for reasoning.We then explore scaling in reasoning steps, which improves step-by-step logical inference.Next, we examine scaling in reasoning rounds, where LLMs iteratively refine their answers through interaction in multi-agent collaboration and debate.We further investigate scaling in trainingenabled reasoning, which enhances reasoning capabilities through model optimization.Additionally, we discuss the applications of scaling in real-world reasoning tasks and outline future directions for research in this field.</p>
<p>By systematically reviewing the scaling of reasoning in LLMs, this survey aims to bridge the gap between empirical scaling strategies and reasoning improvements.This provides insights into when and why scaling enhances reasoning and occasionally introduces limitations.We hope this work will serve as a valuable resource for researchers and practitioners in advancing LLM reasoning through effective and efficient scaling techniques.</p>
<p>Scaling in Input Sizes</p>
<p>As LLMs scale, their ability to process larger input contexts becomes increasingly important for enhancing reasoning, retrieval, and adaptability.Providing more contextual information allows models to make more informed and robust inferences.However, longer inputs also bring challenges, including higher computational costs, memory constraints, and efficiency bottlenecks.This section examines key strategies for scaling input sizes-such as ICL, RAG, and memory-augmented LLMs-highlighting their strengths, limitations, and impact on reasoning performance.</p>
<p>In-Context Learning</p>
<p>In-Context Learning (ICL) enables LLMs to adapt to new tasks without parameter updates by conditioning on demonstrations within the input prompt.Various algorithms have been developed to improve ICL performance by optimizing demonstration selection [26,150,188,221], ordering [105,109], and formatting [77,108,180].While research has observed, context scaling in ICL, where model performance improves as the number of in-context examples increases [1,12,116,125], traditional ICL methods remain constrained by the maximum input context length, limiting them to a few-shot setting [38].Although some works, such as SAICL [13], modify the attention structure to scale ICL to hundreds of demonstrations [55,92,93], they do not fully explore the potential benefits and challenges of utilizing a significantly larger number of examples.With the expansion of context windows, researchers are now investigating many-shot ICL, where models leverage hundreds or even thousands of demonstrations [2,8].Studies have shown significant performance gains across a wide range of generative and discriminative tasks when scaling from few-shot to manyshot ICL [139,169,245].However, as the number of in-context demonstrations increases from a few to many, performance tends to plateau and, in some cases, even decline.To address these challenges and enhance the effectiveness and robustness of many-shot ICL, several methods have been proposed [6,181,232].For example, DrICL [232] adjusts demonstration weights using reinforcement learning-inspired cumulative advantages, improving generalization.BRIDGE [181] automatically identifies a subset of influential examples and utilizes this subset to generate additional high-quality demonstrations, further enhancing ICL performance.</p>
<p>Retrieval-Augmented Generation</p>
<p>Retrieval-Augmented Generation (RAG) has become a widely adopted strategy to address the limitations of LLMs, such as hallucinations and restricted generalization to concepts beyond their training data [53,68,72,87].By incorporating retrieved external information, RAG enhances factual grounding and expands the model's accessible knowledge base.However, traditional RAG operates on short retrieval units, requiring the retriever to scan a massive document corpus to find relevant passages [16,146,213].This approach is constrained by input context length limitations, making longcontext RAG a challenge.A common strategy is document chunking [153,214], where LLMs retrieve relevant chunks instead of full documents.However, defining optimal chunk boundaries is difficult, often leading to semantic incoherence and contextual loss, which degrade retrieval effectiveness [96].Recent advances in longcontext LLMs allow models to process millions of tokens [176].Integrating RAG with long-context LLMs enables the processing of extended contexts while reducing semantic incoherence in chunked retrieval [95,215,216].</p>
<p>As input length increases, the burden on retrieval systems grows.LongRAG [67] mitigates this by grouping related documents, reducing the number of retrieval operations while maintaining relevance.ReComp [214] addresses this challenge by compressing retrieved documents into textual summaries before in-context integration, ensuring information remains concise yet informative.Despite these improvements, a key challenge known as "lost-in-the-middle" bias arises [107], where LLMs assign less importance to passages in the middle of a retrieved context.MOI [85] counters this bias by aggregating inference calls from permuted retrieval orders, ensuring a more balanced weighting across the retrieved information.</p>
<p>Another dimension of scaling RAG involves expanding the amount of data available at inference time [9,143,182,183].Shao et al. [160] find that increasing datastore size monotonically improves performance across various language modeling and downstream tasks without clear saturation.Their MASSIVEDS datastore, containing trillions of tokens, is designed to support large-scale retrieval efficiently.Further, Yue et al. [228] explore inference-time scaling, showing that allocating more retrieval computation leads to nearly linear performance gains when optimally distributed.Their work introduces a predictive model for optimizing retrieval parameters under computational constraints.</p>
<p>Scaling in LLM Reasoning</p>
<p>Memory-Augmented LLMs</p>
<p>Scaling reasoning capabilities of LLMs often necessitates extending their effective context beyond the limited token windows supported by existing architectures [189].Although increasing context length allows LLMs to process longer sequences, such scaling alone quickly encounters computational bottlenecks and diminishing returns due to quadratic complexity in attention mechanisms [44].Moreover, even very long-context models struggle to efficiently capture and retrieve critical historical information from past interactions, leading to degraded reasoning performance over extended contexts [45].To address these limitations, memory augmentation strategies have emerged, enabling LLMs to persistently store, manage, and dynamically retrieve relevant contextual information.Current memory augmentation approaches typically follow two directions: internal architectural modifications to enhance the model's inherent memory capabilities and external memory mechanisms that extend the model context through additional memory components.</p>
<p>Architectural adaptations focus on internalizing long-term dependencies within the model itself.This includes techniques such as augmenting attention mechanisms to better capture extended context [104,113], refining key-value cache mechanisms to optimize retrieval efficiency over long sequences [94,110], and modifying positional encodings to enhance length generalization [235,236].While effective, these modifications require direct intervention in the model's structure, making them impractical for proprietary or black-box API-based LLMs.</p>
<p>An alternative approach is the integration of external memory modules to supplement the model's limited native context window.Summarization-based methods [114,121,187,190] condense past interactions into structured representations that can be efficiently retrieved during inference.However, fixed-granularity summarization risks fragmenting the discourse, leading to incoherent retrieval.To address this, recent advancements incorporate dynamic memory mechanisms that adaptively refine stored information.RMM [173] exemplifies this strategy by leveraging retrospective reflection to improve retrieval selection, ensuring that the model accesses the most relevant and contextually cohesive knowledge.</p>
<p>Scaling memory-augmented LLMs requires balancing efficiency with contextual fidelity.A key challenge is mitigating memory saturation, where excessive storage of past interactions results in retrieval inefficiencies.Techniques such as hierarchical memory organization [160] and retrieval-conditioned compression [214] help alleviate this issue by structuring and filtering stored context dynamically.As research progresses, the convergence of retrievalaugmented memory with scalable long-context architectures offers a promising avenue for enabling LLMs to maintain reasoning consistency over prolonged interactions.</p>
<p>Scaling in Reasoning Steps</p>
<p>Complex reasoning tasks often require multi-step computation, where models must decompose problems, iteratively refine solutions, and verify correctness.Scaling the depth and breadth of reasoning can enhance logical consistency and problem-solving performance, but it also introduces risks such as overthinking and increased computational cost.This section explores key approaches for scaling reasoning, including Chain-of-Thought prompting and meta-reasoning techniques.We examine methods that improve reasoning by encouraging models to "think in more steps, " as well as strategies to mitigate the challenges that arise from deeper reasoning processes.</p>
<p>Chain-of-Thought</p>
<p>Chain-of-thought (CoT) prompting, which enhances the reasoning capabilities of LLMs by stimulating detailed, step-by-step deliberation, either through zero-shot [79] or few-shot demonstrations [196], has emerged as a key technique for solving complex tasks.Since LLMs operate probabilistically [63,82], greedy decoding may not always produce the optimal answer [192].To mitigate this, repeated sampling approaches, such as self-consistency [191] and Best-of-N [11,131], generate multiple reasoning chains in parallel and select the best answer based on frequency, external reward models, or auxiliary verifiers.</p>
<p>Although simple parallel sampling is computationally straightforward, it remains inefficient and suboptimal by randomly allocating the test-time computation budget to less promising branches [168,204].To mitigate this issue, researchers have explored strategies that prioritize promising reasoning paths or intermediate steps over less viable alternatives to effectively prune the search space by applying tree search-enabled reasoning [75,112,126,132,159,191,220] Generally, it structures the reasoning process as a branching tree, where each node represents a discrete thinking step, and branches correspond to different potential solution paths.Like CoT which organizes reasoning in a hierarchical manner, tree search-enabled reasoning enables LLMs to decompose intricate problems into manageable components.However, LLM reasoning with tree search can maintain awareness of multiple hypothesis threads simultaneously and systematically explore the solution space through different search algorithms (e.g., BFS or DFS), making it more powerful for handling complex problems.</p>
<p>The pioneering work CoT-SC [191] extends CoT to the tree structure, where multiple CoTs originate from the same initial (root) prompt, forming a "tree of chains".The chain that provides the best outcome to the initial question, is selected as the final answer.Skeleton-of-Thought (SoT) [132] instead effectively harnesses a tree with a specific level of depth.It performs reasoning through a divide-and-conquer manner, which significantly reduces the generation latency of LLMs.In the first prompt, the LLM is instructed to generate a skeleton of the answer, i.e., a list of points that can be answered independently.For each point, a new prompt is issued in parallel to address only the corresponding part of the question.</p>
<p>Recently, numerous studies have explored Tree of Thoughts (ToT) [112,220] for tree search-enabled reasoning.Compared to CoT-SC where multiple CoTs originate from the same initial (root) prompt, ToT employs a tree structure to decompose a problem into subproblems and solve them using separate LLM prompts.Unlike ToT using multiple prompts, Algorithm of Thoughts (AoT) [159] uses only a single prompt with in-context examples formulated in an algorithmic fashion.Tree of Uncertain Thought (TouT) [126] enhances ToT with local uncertainty scores by incorporating the variance of multiple LLM responses into the state evaluation function.Tree of Clarifications (ToC) [75] focuses on answering ambiguous questions using ToT.It first retrieves relevant external information and then recursively prompts an LLM to construct a disambiguation tree for the initial question.</p>
<p>Meta-Reasoning and Calibration</p>
<p>Numerous works [35,49,69,141,230] have shown that LLMs have inherited capabilities of self-correction with proper prompt engineering.Typically, an LLM can self-reflect its responses by generating feedback on its answers.It first generates an initial response to an input question.Next, it generates feedback given the original input and its initial response.Finally, it generates a refined response given the input, initial response, and feedback.Generally, self-correction may rely on different sources of feedback, including intrinsic prompts and external information.Intrinsic prompts let LLMs generate feedback on their own responses.For example, CoVe [35] plans verification questions to check an initial response and then systematically answers those questions in order to finally produce an improved revised response.FLARE [69] performs selfcorrection by iteratively generating a temporary next sentence and check whether it contains low-probability tokens.In contrast, external information enables LLMs to rely on external tools, such as external knowledge from search engines, oracle information, and task-specific metrics, to enhance self-correction.For example, RE-FINER [141] interacts with a critic model that provides automated feedback on the reasoning.CRITIC [49] interacts with external tools like search engines and code interpreters to verify the desired aspects of an initial output and subsequently amends the output based on the critiques from the verification.</p>
<p>One major concern centers around the efficiency of self-refinement: LLMs need to generate feedback and refined responses iteratively, which can significantly increase the inference time of LLMs.To overcome the scaling issue, Quiet-STaR [230] designs a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique.Another concern is caused by generation-time correction.Prevalent self-correction approaches are based on generation-time correction, heavily depending on the capacity of the critic model to provide accurate quantifiable feedback for intermediate outputs.Nevertheless, this might be quite challenging for many NLP tasks with long token sizes, such as summarization-the summary can be accurately assessed only after the entire summary is generated.This limitation makes generation-time correction infeasible in many NLP tasks.One solution to this issue is post-hoc correction [137].Unlike general generation-time correction which generates feedback on the intermediate reasoning steps, post-hoc correction involves refining the output after it has been generated.</p>
<p>Scaling in Reasoning Rounds</p>
<p>Beyond single-step or sequential reasoning, iterative multi-round reasoning enables LLMs to refine responses, debate alternatives, and integrate external feedback.However, scaling the number of reasoning rounds introduces challenges related to efficiency, redundancy, and diminishing performance returns.This section explores key approaches that leverage iterative interaction, including multi-agent collaboration, debate-based reasoning, and human-LLM interaction.</p>
<p>Multi-Agent Collaboration</p>
<p>Recently, researchers have explored the effectiveness of multi-agent collaboration, where multiple LLMs work together in a coordinated manner to achieve improved problem-solving capabilities [73,100].In particular, in these frameworks, each LLM (agent) is assigned a distinct role-such as planner, executor, verifier, or critic-and iteratively refines its output through structured interactions with other agents [217].For example, CAMEL [89] introduced a framework where LLM agents assume different personas and interact through structured role-playing, enabling more effective task completion through multi-turn communication.The core idea is to enhance the specialization and division of labor among LLMs, ensuring that different agents contribute unique perspectives to improve overall task performance.Unlike single-agent systems, which rely on an LLM's internal reasoning capability [51,199], multi-agent frameworks distribute tasks across multiple agents that engage in iterative interactions [89].</p>
<p>Increasing the number of agents can improve task diversity and allow for role specialization, where different agents assume distinct functions such as problem decomposition, tool usage, or evaluation [52].Research has demonstrated that larger multi-agent systems can achieve greater accuracy and better adaptability in openended reasoning tasks, as seen in software development frameworks like MetaGPT [58].However, there is a saturation point-beyond a certain number of agents, performance plateaus or even deteriorates due to conflicting reasoning paths, redundancy, and increased coordination overhead [100].This suggests that while scaling improves multi-agent efficacy up to a certain threshold, naive expansion leads to diminishing returns without structured coordination mechanisms.Nevertheless, introducing hierarchical structures, where some LLMs serve as supervisors while others act as task executors, has shown consistent improvements in task accuracy and efficiency [14].Another interesting finding is introduced in LLM Harmony [148], which optimizes inter-agent communication by structuring dialogue between multiple LLM agents.Instead of simple turn-based exchanges, this framework enables agents to dynamically negotiate task objectives, delegate subtasks, and refine outputs iteratively.The results suggest that scaling the number of interacting agents improves performance only when they are given complementary roles, while increasing homogeneous agents leads to redundant reasoning patterns.</p>
<p>Debate-Based Reasoning</p>
<p>Beyond the general framework of leveraging multiple LLMs for collaborative task execution, researchers have also explored the use of LLMs in multi-round reasoning to enhance reasoning effectiveness.Specifically, in these frameworks, each LLM (or agent) functions as a debater, engaging in discourse to challenge and persuade others while refining its own reasoning through iterative exchanges.A pioneering work in this area, Multi-Agent Debate (MAD) [99], introduces a framework in which multiple agents engage in a structured debate following a "tit-for-tat" mechanism, with a designated judge overseeing the discussion to arrive at a definitive answer.The core idea is to encourage diverse perspectives among agents, fostering deeper contemplation and critical thinking.The authors demonstrate that the debate framework leads to significantly higher disagreement levels compared to Self-Reflection [119,165], thereby reducing the risk of models converging on incorrect answers.Given these advantages, researchers have proposed various debate-based frameworks that enhance both reasoning capabilities and factual accuracy [40].The scaling effect in debate frameworks manifests in multiple dimensions.In [74], the authors find that when employing a judge LLM to evaluate responses from debater LLMs, increasing the number of debate rounds does not necessarily lead to greater clarity-especially for weaker models, where additional rounds introduce confusion rather than improving accuracy.However, in consultancy-based interactions, where a single LLM attempts to persuade a judge LLM, the judge's accuracy improves over successive rounds.Notably, enhancing the persuasiveness of debater LLMs-making them more effective at convincing the judge-has been shown to yield performance improvements.This scaling effect provides further insights into optimizing debate-based reasoning frameworks.Similarly, [124] suggests that scaling LLM debates with increasingly skilled debaters (e.g., progressing from AI to human debaters) enhances oversight mechanisms, improving overall debate efficacy, whereas consultancy frameworks tend to perform worse under similar conditions.Distinct from these approaches, [142] proposes embedding-based communication to facilitate debate, enabling smaller LLMs to retain stronger debate capabilities by mitigating information loss.Their findings indicate that increasing the number of debate rounds improves performance up to a threshold of three rounds, beyond which additional rounds provide diminishing returns.In summary, the scaling effect in debate frameworks is not straightforward; simply increasing the number of LLMs or debate rounds does not necessarily lead to continued performance improvements beyond a certain threshold.However, multiple studies highlight that enhancing the reasoning capabilities and persuasiveness of debater LLMs can lead to substantial performance gains.</p>
<p>Human-LLM Interaction</p>
<p>Scaling LLM reasoning is not solely a function of model size and context window but also hinges on the quality and depth of human interactions [4].Human-in-the-loop frameworks [203] enhance LLM performance by integrating iterative refinement, feedback-driven prompting, and adaptive response generation.This interaction paradigm shifts LLMs from static inference engines to dynamically evolving agents capable of learning from user interventions.</p>
<p>Recent work explores multi-turn reasoning scenarios where users provide incremental clarifications or corrections, allowing models to refine their responses iteratively [80,119].This process mirrors how humans engage in collaborative problem-solving, gradually converging on an accurate and well-structured answer.Methods such as self-reflection prompting [165] and feedback-based reinforcement learning [18] demonstrate improvements in factual consistency and reasoning depth by enabling LLMs to assess and revise their own outputs.</p>
<p>A key challenge in human-LLM interaction is balancing efficiency with adaptability.Over-reliance on explicit feedback mechanisms can introduce cognitive overhead for users, while insufficient adaptability limits the model's ability to incorporate nuanced human guidance.Recent strategies mitigate this tradeoff through adaptive interaction mechanisms, such as retrieval-enhanced dialogue memory [138] and user-intent modeling [91], allowing LLMs to anticipate user needs and refine responses proactively.</p>
<p>As interaction frameworks scale, ensuring alignment with human cognitive processes remains critical.Fine-tuning strategies that incorporate user feedback loops have shown promise in enhancing model interpretability and trustworthiness [76].Furthermore, inference-time intervention mechanisms [122,172] enable LLMs to allocate computational resources efficiently based on user engagement patterns.By refining the synergy between LLMs and human oversight, interactive reasoning systems hold the potential to scale beyond static prompt-response architectures, evolving towards more adaptive and contextually aware AI assistants.</p>
<p>Scaling in Model Optimization</p>
<p>Beyond inference-time techniques, scaling model optimization can enhance LLM reasoning through reinforcement learning (RL) and latent-space processing.While RL-based reasoning helps align the model's behavior with human intentions and enhances model performance across diverse tasks, it faces diminishing returns, requiring better policy optimization and adaptive reward modeling.Meanwhile, looped transformers can improve reasoning depth efficiently by iterating over representations, reducing the need for larger models.This section explores RL-based fine-tuning and latent-space reasoning, highlighting their impact on scalable reasoning.</p>
<p>Reinforcement Learning</p>
<p>Although previous studies have shown that distilling knowledge from superior LLMs, regardless of whether supervised fine-tuning (SFT) data are amassed in large quantities or carefully curated [222,239], can enhance the reasoning abilities of smaller models for solving complex tasks [57,120,166], recent studies contend that, merely increasing the volume of SFT data typically yields only a loglinear performance improvement [227].Moreover, models trained exclusively on SFT data tend to overfit by memorizing the training set, thereby struggling to generalize to out-of-distribution (OOD) tasks [30].To address these challenges, reinforcement learning (RL) has emerged as a key approach in LLM post-training, aligning models with human preferences [135,147] and enhancing their reasoning abilities [50,161,218].</p>
<p>Fine-tuning LLMs using RL involves optimizing the model, typically via policy gradient algorithms such as Proximal Policy Optimization (PPO) [158], to maximize the response's reward.This process can leverage explicit reward models such as outcome reward models (ORM), which compute reward based on the entire response or using heuristic or rule-based functions to assess the final answer, and process reward models (PRM), which compute reward at each intermediate step, either from human annotations [102,178] or Monte Carlo (MC) estimation [186,233].</p>
<p>A key challenge in PPO is its computational overhead [3].Since PPO constrains policy updates to remain close to a reference model, it requires an actor, a reference, and a reward model when computing reward, and further needs a critic model to estimate the advantage using Generalized Advantage Estimation (GAE) [157].To mitigate this issue and stabilize the training process, Ahmadian et al. [3] and Hu [60] suggest replacing the complicated PPO with vanilla REINFORCE by modeling the entire generation as a single action and removing the critic model in PPO.Shao et al. [161] introduces GRPO, which substitutes GAE in PPO with moving average of all rewards from the group of responses of the same prompt.These simplified PPO variants enhance scalability, making large-scale training more practical.</p>
<p>Recent studies indicates that conducting RL-based fine-tuning after SFT can further enhance the reasoning abilities of LLMs.ReFT [118] first performs a warm-up SFT on distilled CoT data followed by PPO to refine the model.DeepSeek-R1 [50] shares a similar strategy as ReFT but employs self-training by directly applying GRPO to the base model.This base model is then used to generate long-form CoT data for the warm-up SFT stage, after which GRPO is applied again to the SFT model, ultimately achieving reasoning performance comparable to OpenAI-o1 [62].They observed an "aha-moment" during the training of DeepSeek-R1-Zero, where the model learned to rethink as the response length increased.Following DeepSeek-R1, recent works observed similar phenomena such as "aha-moment" and think related words on different tasks, including real-world software engineering [198], logical puzzles [210], and automated theorem proving [37] when scaling up the training steps and response length using RL-based fine-tuning.</p>
<p>However, reasoning models trained with RL to generate long CoT responses may also encounter challenges such as "underthinking" [193], where models frequently switch between reasoning branches without engaging in deep thought, and "overthinking" [22], which suggests that excessive reasoning on simple questions can sometimes degrade performance.Additionally, recent studies [59] argue that scaling the number of response samples and increasing the size of the policy model, while keeping the reward model fixed, is less efficient compared to scaling during pre-training.</p>
<p>Latent-Space Reasoning</p>
<p>In explicit reasoning [196], models generate intermediate steps before producing the final output.While this approach breaks down complex tasks into simpler steps, it can be verbose and computationally expensive.To improve inference efficiency, models can perform reasoning in latent space, skipping the need for explicit verbalization [33,164].For instance, Deng et al. [33] propose distilling multi-step reasoning into latent representations across layers, allowing the model to solve complex problems in a single forward pass, thereby improving efficiency and scalability.Similarly, Co-CoMix [170] trains LLMs to predict selected semantic concepts from their hidden states.By interleaving token embeddings with highlevel continuous concepts, the model enhances abstract reasoning while reducing data and computational costs.Moreover, language space is not always optimal for reasoning.Hao et al. [54] observe that most word tokens contribute to textual coherence rather than reasoning, while certain critical tokens require complex planning.To address this, they introduce Coconut [54], which iteratively processes hidden states and enables parallel exploration of multiple reasoning paths.To further enhance deep reasoning without parameter expansion, ITT [23] dynamically allocates computation to critical tokens and iteratively refines representations.The iterative paradigm is also leveraged for test-time scaling, improving efficiency [47,128].For example, Saunshi et al. [154] demonstrate that scaling model depth can be achieved with a limited parameter budget through looping, introducing a new scaling paradigm based on iterative latent space transformations rather than increasing model size.</p>
<p>Application 6.1 AI Research</p>
<p>Scaling in LLMs has fundamentally reshaped AI research, both extending traditional domains and opening entirely new research avenues.This section explores how scaling has influenced three critical areas: LLM-as-a-Judge, fact-checking, and dialogue systems.</p>
<p>LLM-as-a-Judge.Using LLMs to evaluate model outputs or other models has emerged as a pivotal research direction, enabling evaluation at scale beyond traditional approaches and human assessment [88].Notably, larger models demonstrate a significantly higher correlation with human preferences compared to their smaller counterparts [238].To further improve evaluation quality, recent work has explored multi-step reasoning processes [151], where scaling the number of reasoning steps enhances evaluation capabilities [29].Additionally, scaling across multiple judge models has emerged as an effective approach to improve evaluation reliability [98].Different LLMs functioning as agents collaborate through multi-round discussions before reaching a final judgment, thereby enhancing evaluation consistency [145].</p>
<p>Fact-Checking.The capacity of AI systems to generate misinformation has driven substantial research into automated fact checking [32,201,241].Initial fact verification approaches relied on smaller models with limited contextual understanding, primarily focusing on matching claims to evidence [32].Large-scale LLMs have shown remarkable fact-checking capabilities by supporting fact-checkers with their extensive knowledge and sophisticated reasoning [175].Scaling in reasoning steps has been demonstrated to improve claim detection, making the process more methodical [156].Additionally, RAG has been employed for evidence-backed fact-checking with reduced hallucination and improved performance, with performance scaling with the number of retrieved documents [167].Multi-agent systems have been widely implemented for fact-checking, where multiple imperfect fact-checkers can collectively provide reliable assessments [179].</p>
<p>Dialogue Systems.Dialogue systems represent the most visible application of LLM scaling [43,223,237], where advances in context length, reasoning steps, and training data have dramatically transformed interactive capabilities.Enhanced context handling has significantly impacted dialogue coherence and consistency.Scaling of context provides dialogue agents with more information, enabling more informative long-term conversations [7,173].External augmentation has been widely adopted to facilitate long-term dialogue as well.Commonly integrated external knowledge, including commonsense [184], medical [21], and psychological [24] knowledge, serves as supplementary guidance for the reasoning process, ensuring logical coherence across extended contexts.Multi-agent dialogue systems have also demonstrated exceptional capabilities, where multiple LLMs collaborate to comprehensively evaluate and select the most appropriate responses [42].</p>
<p>Production</p>
<p>The scaling reasoning capabilities of LLMs have significantly enhanced production applications, particularly in software development, data science workflows, and interactive AI systems.This subsection discusses these areas with illustrative examples.</p>
<p>Software Development.The scaling reasoning capabilities of LLMs enhance software development by enabling a better understanding of complex coding tasks and facilitating accurate multistep reasoning over intricate software dependencies and structures.Advanced reasoning techniques, such as chain-of-thought prompting, allow code-generation assistants to systematically approach and solve coding tasks [20,66].Furthermore, structured reasoning strategies can effectively handle larger coding contexts and reduce developer cognitive load during debugging and iterative improvement processes [66].</p>
<p>Data Science Workflows.Scaling reasoning in LLMs substantially improves data science workflows by enabling sophisticated analytical and exploratory tasks.Multi-step reasoning allows LLMs to iteratively explore, interpret, and synthesize insights from diverse datasets [171], effectively supporting hypothesis generation and validation processes [162,202].Retrieval-augmented reasoning frameworks extend these capabilities further by dynamically integrating external knowledge during reasoning, thus enriching the comprehensiveness of exploratory analysis [143].Multi-agent systems are also proposed to collaboratively solve real-world data science challenges [97].</p>
<p>Interactive AI Systems.Scaling reasoning steps and context length transforms interactive AI systems by significantly improving their adaptability and context-awareness.Expanded reasoning capabilities enable dialogue agents to maintain coherent and informative long-term interactions, effectively integrating historical context and external knowledge [7,43].Multi-agent systems leverage iterative refinement and structured verification among specialized reasoning agents, further enhancing accuracy and reducing errors such as hallucinations [42].Interactive AI environments such as LLM-based Cursor [34] leverage LLMs' contextual reasoning to facilitate precise user interactions, enabling targeted queries and refined outputs.</p>
<p>Science</p>
<p>The scaling of LLMs has significantly benefited scientific domains, with medicine, finance, and disaster management emerging as prominent application areas.</p>
<p>Medical Domain.The medical domain has experienced remarkable advances through scaled LLMs.Research demonstrates that increasing model size leads to enhanced medical reasoning capabilities, with performance on medical questions improving proportionally [10,101,115,242].This pattern extends to diagnostic reasoning [48,155], where larger models can identify complex disease progression patterns that smaller models miss [46,56,229].Multi-round reasoning approaches such as CoT have demonstrated exceptional effectiveness in medical diagnosis [106,200], with additional reasoning steps yielding more accurate diagnoses [17,61] by enabling consideration of alternative explanations and confounding factors.RAG techniques enhance medical question answering, with performance improving as the number of retrieved snippets increases [212].Many-shot ICL shows particular efficacy for drug design tasks, with performance scaling with the number of examples provided [127].Additionally, multi-agent LLM frameworks that simulate medical team consultations have demonstrated superior diagnostic accuracy, with specialized agents collaborating on complex cases to outperform single LLMs when benchmarked against gold-standard diagnoses [41,78].</p>
<p>Finance.Financial applications demonstrate improved performance with large-scale LLMs.Studies indicate that fine-tuned large-scale LLMs substantially outperform smaller alternatives [70], with performance scaling with model size [90,144] across financial decisionmaking tasks.The multi-step reasoning capabilities of scaled LLMs prove particularly valuable for complex financial analysis, significantly outperforming direct approaches [144,243].Financial sentiment analysis benefits from increased numbers of examples in many-shot ICL scenarios [2].RAG-based approaches incorporating banking webpages and policy guides improve question-answering performance, with results scaling with the number of retrieved documents [234].Multi-agent debate frameworks yield promising results in investment and trading decision scenarios [209,225,226], with specialized agents covering distinct functions outperforming single-agent approaches.</p>
<p>Disaster Management.Disaster management has undergone substantial transformation through large-scale LLMs [86].Social media text classification for disaster types has improved significantly through LLM fine-tuning compared to traditional machine learning methods [39,224].The in-context learning capabilities of large-scale LLMs enable context-aware disaster applications including conversational agents for disaster-related queries and situational analysis [134,149].Large-scale disaster knowledge graphs enhance incontext learning through retrieval augmentation, enabling LLMs to generate more informative and less hallucinated responses [19,205].For high-stakes disaster-related decision-making, multi-agent LLM approaches have been effectively deployed to facilitate adaptive and collaborative decision processes [36,177], largely outperforming a single LLM.</p>
<p>Future Directions</p>
<p>Efficiency in Scalable Reasoning.Scaling reasoning capability in LLMs enhances their ability to solve complex problems but also increases response length, making it inefficient for simpler tasks.However, current LLMs apply uniform reasoning effort across all queries, leading to unnecessary computational overhead.A key direction for improvement is adaptive reasoning frameworks, where models dynamically adjust the depth of reasoning based on task difficulty [197,231].For example, "Proposer-Verifier" framework [168] offers a promising approach by generating multiple candidate solutions and selecting the most reliable one through verification, reducing redundant reasoning steps while maintaining accuracy.However, achieving dynamic computation allocation requires robust uncertainty estimation, ensuring that models allocate resources efficiently without excessive overhead.</p>
<p>Another challenge is balancing search-based reasoning methods with computational cost.Approaches like ToT and Monte Carlo search refine reasoning iteratively but incur significant compute overhead.Selective pruning strategies that eliminate irrelevant reasoning paths while maintaining solution integrity could help optimize performance [211].Additionally, RL-based multi-step reasoning faces credit assignment issues, where sparse rewards make optimizing intermediate reasoning steps difficult [82].Future work should explore hybrid reward models [163] that combine processbased supervision (evaluating stepwise correctness) with outcomebased rewards (final answer validation) to improve long-horizon reasoning stability and efficiency.</p>
<p>Beyond single-model scaling, collaborative multi-agent systems present a promising avenue for large-scale reasoning [84,136], but they also introduce significant coordination overhead.As the number of agents increases, computational redundancy and inefficient communication can slow down reasoning instead of improving it [51].One approach to mitigate this is dynamic agent selection [111], where the system dynamically selects only the most relevant agents for a given reasoning task while discarding redundant ones.Another strategy is hierarchical multi-agent reasoning, where a smaller subset of expert agents handles complex queries, while simpler queries are resolved by lightweight, lowercost agents.Additionally, inter-agent communication should be optimized through compressed latent representations rather than verbose token-based exchanges, further reducing computational overhead [244].Future research should explore pruning and optimization techniques that enable multi-agent systems to scale efficiently without unnecessary computational waste, ensuring that reasoning is distributed optimally across agents.</p>
<p>Inverse Scaling and Stability.Inverse scaling refers to the phenomenon where LLMs unexpectedly perform worse on certain tasks, contradicting standard scaling laws that predict consistent improvements with increased model size.Lin et al. [103] first observed this effect when evaluating LLMs such as GPT-2 and GPT-3 on truthfulness tasks, noting that common training objectives incentivize imitative falsehoods, where models produce false but high-likelihood responses due to patterns in their training distribution.McKenzie et al. [123] systematically analyzed different datasets exhibiting inverse scaling and identified key causes like solving distractor tasks instead of intended tasks.</p>
<p>While inverse scaling is widely observed, Wei et al. [194] challenge its universality, showing that some tasks previously exhibiting inverse scaling follow a U-shaped scaling trend-where performance initially declines with increasing model size but later recovers at even larger scales.This suggests that larger models can sometimes unlearn distractor tasks and correct their errors, emphasizing the importance of evaluating scaling trends beyond mid-sized models.</p>
<p>Since scaling laws were originally developed in the context of pretraining, they remain decoupled from downstream task performance, making it an open question of how to systematically predict and mitigate inverse scaling across different reasoning benchmarks.Additionally, challenges like reward hacking [5]-where models exploit superficial signals rather than true reasoning improvements-necessitate adaptive reward models to maintain stability in multi-step reasoning.Future work should focus on developing predictive models for inverse scaling, refining adaptive fine-tuning methods, and leveraging world models for richer environmental feedback, ensuring that multi-step reasoning generalizes effectively across domains such as code generation, planning, question answering, and cross-lingual tasks.</p>
<p>Security Risks in Scaled Reasoning Models.While CoT prompting enhances LLMs' ability to perform structured reasoning, it also introduces new security vulnerabilities, particularly backdoor attacks that manipulate the model's reasoning process.BadChain [207] exploits the model's step-by-step reasoning by injecting backdoor reasoning steps, causing malicious alterations in the final response when a hidden trigger is present in the query.Similarly, H-CoT [83] manipulates the model's internal reasoning pathways, hijacking its safety mechanisms to weaken its ability to detect harmful content.While defenses such as backdoor detection (CBD) [208] and modified decoding strategies [65] offer some protection, their effectiveness against novel attacks remains largely unexplored.This highlights the urgent need for more robust defenses capable of adapting to emerging threats.</p>
<p>Unlike CoT, RAG integrates external data sources, making them prone to data extraction attacks [28].Existing defenses primarily focus on retrieval corruption attacks [174,206,240], aiming to maintain performance, but data leakage prevention remains an underexplored area.For example, RAG-Thief demonstrates how attackers can extract scalable amounts of private data from proprietary retrieval databases [64].Beyond attacks on individual LLMs, the scaling of multi-agent reasoning systems introduces new attack surfaces.AgentPoison [27] specifically targets RAG-based and memory-augmented LLM agents, poisoning long-term memory or altering the knowledge base to induce faulty reasoning over time.As multi-agent LLM systems grow in scale, collusive behaviors among malicious agents present an even greater risk [219].BlockAgents proposes a blockchain-integrated framework for LLM-based cooperative multi-agent systems, mitigating Byzantine behaviors that arise from adversarial agents [15].</p>
<p>As AI adoption increases, the computational and environmental costs of inference also become a growing concern [117,140,152].Large-scale LLMs demand significant energy resources on inference [140].This opens the door to a new form of attack, OverThink attack [81], where an adversary intentionally inflates the number of reasoning tokens in an LLM's response, drastically increasing financial and computational costs.As LLM reasoning continues to scale, deploying cost-effective safeguards against such attacks will become necessary for sustainable AI deployment.</p>
<p>Conclusion</p>
<p>In this survey, we provided a comprehensive analysis of how scaling strategies influence reasoning capabilities in large language models.We examined four major dimensions-scaling in input sizes, reasoning steps, reasoning rounds, and model optimization-highlighting the methods, benefits, and challenges in each.While scaling improves LLM reasoning across many domains, it also introduces limitations such as computational inefficiency, instability, and new security risks.We emphasized emerging directions to address these issues, including adaptive computation, robust optimization, and safe multi-agent coordination.As LLMs continue to evolve, understanding and refining scalable reasoning will be key to building more capable, trustworthy, and efficient AI systems.</p>
<p>Figure 1 :
1
Figure 1: Taxonomy for Scaling in Large Language Model Reasoning.</p>
<p>Context-Scaling versus Task-Scaling in In-Context Learning. Amirhesam Abedsoltan, Adityanarayanan Radhakrishnan, Jingfeng Wu, Mikhail Belkin, arXiv-24102024. 2024arXiv e-prints</p>
<p>Manyshot in-context learning. Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, Advances in Neural Information Processing Systems. 372024. 2024</p>
<p>Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, Sara Hooker, arXiv:2402.14740Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. 2024. 2024arXiv preprint</p>
<p>Dana Alsagheer, Rabimba Karanjai, Nour Diallo, Weidong Shi, Yang Lu, Suha Beydoun, Qiaoning Zhang, arXiv:2403.09798Comparing rationality between large language models and humans: Insights and open questions. 2024. 2024arXiv preprint</p>
<p>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané, arXiv:1606.06565Concrete problems in AI safety. 2016. 2016arXiv preprint</p>
<p>Revisiting In-Context Learning with Long Context Language Models. Jinheon Baek, Jae Sun, Prakhar Lee, Siddharth Gupta, Prateek Dalmia, Kolhar, arXiv:2412.169262024. 2024arXiv preprint</p>
<p>Example-based chat-oriented dialogue system with personalized long-term memory. Jeesoo Bang, Hyungjong Noh, Yonghee Kim, Gary Geunbae, Lee , 2015 International Conference on Big Data and Smart Computing (BIGCOMP). IEEE2015</p>
<p>In-context learning with long-context models: An in-depth exploration. Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Graham Matthew R Gormley, Neubig, arXiv:2405.002002024. 2024arXiv preprint</p>
<p>Improving language models by retrieving from trillions of tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, International conference on machine learning. PMLR2022</p>
<p>How large language models perform on the united states medical licensing examination: a systematic review. Dana Brin, Vera Sorin, Eli Konen, Girish Nadkarni, Benjamin S Glicksberg, Eyal Klang, MedRxiv. 2023. 2023</p>
<p>Large language monkeys: Scaling inference compute with repeated sampling. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Christopher Quoc V Le, Azalia Ré, Mirhoseini, arXiv:2407.217872024. 2024arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>Scaling in-context demonstrations with structured attention. Tianle Cai, Kaixuan Huang, Jason D Lee, Mengdi Wang, arXiv:2307.026902023. 2023arXiv preprint</p>
<p>Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, Denny Zhou, arXiv:2305.17126Large language models as tool makers. 2023. 2023arXiv preprint</p>
<p>BlockAgents: Towards Byzantine-Robust LLM-Based Multi-Agent Coordination via Blockchain. Bei Chen, Gaolei Li, Xi Lin, Zheng Wang, Jianhua Li, Proceedings of the ACM Turing Award Celebration Conference-China 2024. the ACM Turing Award Celebration Conference-China 20242024</p>
<p>Reading wikipedia to answer open-domain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, arXiv:1704.000512017. 2017arXiv preprint</p>
<p>Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, Benyou Wang, arXiv:2412.18925Huatuogpt-o1, towards medical complex reasoning with llms. 2024. 2024arXiv preprint</p>
<p>Aleksei Petrenko, Jackson Hamburger. Kevin Chen, Marco Cusumano-Towner, Brody Huval, arXiv:2502.01600Vladlen Koltun, and Philipp Krähenbühl. 2025. Reinforcement Learning for Long-Horizon Interactive LLM Agents. 2025arXiv preprint</p>
<p>Enhancing emergency decision-making with knowledge graphs and large language models. Minze Chen, Zhenxiang Tao, Weitong Tang, Tingxin Qin, Rui Yang, Chunli Zhu, International Journal of Disaster Risk Reduction. 1131048042024. 2024</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021. 2021arXiv preprint</p>
<p>Siyuan Chen, Mengyue Wu, Kenny Q Zhu, Kunyao Lan, Zhiling Zhang, Lyuchun Cui, arXiv:2305.13614LLM-empowered chatbots for psychiatrist and patient simulation: application and evaluation. 2023. 2023arXiv preprint</p>
<p>Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, arXiv:2412.21187Do not think that much for 2+ 3=? on the overthinking of o1-like llms. 2024. 2024arXiv preprint</p>
<p>Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang, arXiv:2502.13842Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking. 2025. 2025arXiv preprint</p>
<p>Yirong Chen, Xiaofen Xing, Jingkai Lin, Huimin Zheng, Zhenyu Wang, Qi Liu, Xiangmin Xu, arXiv:2311.00273SoulChat: Improving LLMs' empathy, listening, and comfort abilities through fine-tuning with multi-turn empathy conversations. 2023. 2023arXiv preprint</p>
<p>Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, arXiv:2311.16079Meditron-70b: Scaling medical pretraining for large language models. 2023. 2023arXiv preprint</p>
<p>FastGAS: Fast Graph-based Annotation Selection for In-Context Learning. Zihan Chen, Song Wang, Cong Shen, Jundong Li, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>Agentpoison: Red-teaming llm agents via poisoning memory or knowledge bases. Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, Bo Li, Advances in Neural Information Processing Systems. 372024. 2024</p>
<p>Pengzhou Cheng, Yidong Ding, Tianjie Ju, Zongru Wu, Wei Du, Ping Yi, Zhuosheng Zhang, Gongshen Liu, arXiv:2405.13401Trojanrag: Retrieval-augmented generation can be backdoor driver in large language models. 2024. 2024arXiv preprint</p>
<p>TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for LLM-as-a-Judge. Cheng-Han Chiang, Hung-Yi Lee, Michal Lukasik, arXiv:2503.043812025. 2025arXiv preprint</p>
<p>Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Sergey Quoc V Le, Yi Levine, Ma, arXiv:2501.17161Sft memorizes, rl generalizes: A comparative study of foundation model post-training. 2025. 2025arXiv preprint</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Journal of Machine Learning Research. 252024. 2024</p>
<p>Human-in-theloop Artificial Intelligence for Fighting Online Misinformation: Challenges and Opportunities. Gianluca Demartini, Stefano Mizzaro, Damiano Spina, IEEE Data Eng. Bull. 432020. 2020</p>
<p>Implicit chain of thought reasoning via knowledge distillation. Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, Stuart Shieber, arXiv:2311.014602023. 2023arXiv preprint</p>
<p>AI-Enhanced Cursor Navigator. Dr S Rama Devi, U Ch Ommi, Bhagyasri, Sravanthi, Sl Chaitrika, Priyanka, Swarna, ; R Srilekha, Chaitrika, Priyanka, M Swarna, M Srilekha, 2024. May 10, 2024. 2024AI-Enhanced Cursor Navigator</p>
<p>Chain-of-Verification Reduces Hallucination in Large Language Models. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>Antoine Dolant, Praveen Kumar, arXiv:2502.10978Agentic LLM Framework for Adaptive Decision Discourse. 2025. 2025arXiv preprint</p>
<p>Kefan Dong, Tengyu Ma, arXiv-2502STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving. 2025. 2025arXiv e-prints</p>
<p>A Survey on In-context Learning. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Identifying citizen-related issues from social media using llm-based data augmentation. Vitor Gaboardi Dos Santos, Guto Leoni Santos, Theo Lynn, Boualem Benatallah, International Conference on Advanced Information Systems Engineering. Springer2024</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, Forty-first International Conference on Machine Learning. 2023</p>
<p>Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, Jingren Zhou, arXiv:2402.09742Ai hospital: Benchmarking large language models in a multi-agent medical interaction simulator. 2024. 2024arXiv preprint</p>
<p>A multi-agent conversational recommender system. Jiabao Fang, Shen Gao, Pengjie Ren, Xiuying Chen, Suzan Verberne, Zhaochun Ren, arXiv:2402.011352024. 2024arXiv preprint</p>
<p>Leveraging large language models in conversational recommender systems. Luke Friedman, Sameer Ahuja, David Allen, Zhenning Tan, Hakim Sidahmed, Changbo Long, Jun Xie, Gabriel Schubiner, Ajay Patel, Harsh Lara, arXiv:2305.079612023. 2023arXiv preprint</p>
<p>Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu, Xuetao Wei, Tong Xu, Xiangyu Zhao, arXiv:2502.18845Sliding Window Attention Training for Efficient Large Language Models. 2025. 2025arXiv preprint</p>
<p>U-NIAH: Unified RAG and LLM Evaluation for Long Context Needle. Yunfan Gao, Yun Xiong, Wenlong Wu, Zijing Huang, Bohan Li, Haofen Wang, arXiv:2503.003532025. 2025A-Haystack. arXiv preprint</p>
<p>Stepforward structuring disease phenotypic entities with LLMs for disease understanding. Álvaro García-Barragán, Alberto González Calatayud, Lucía Prieto-Santamaría, Víctor Robles, Ernestina Menasalvas, Alejandro Rodríguez, 2024 IEEE 37th International Symposium on Computer-Based Medical Systems (CBMS). IEEE2024</p>
<p>Jonas Geiping, Sean Mcleish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Tom Goldstein, arXiv:2502.05171Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach. 2025. 2025arXiv preprint</p>
<p>Large language model influence on diagnostic reasoning: a randomized clinical trial. Ethan Goh, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A Cool, Zahir Kanjee, Andrew S Parsons, Neera Ahuja, JAMA Network Open. 72024. 2024</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382023. 2023arXiv preprint</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025. 2025arXiv preprint</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.016802024. 2024arXiv preprint</p>
<p>Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.01680[cs.CLLarge Language Model based Multi-Agents: A Survey of Progress and Challenges. 2024</p>
<p>Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Mingwei Chang, International conference on machine learning. PMLR2020</p>
<p>Training large language models to reason in a continuous latent space. Shibo Hao, Sainbayar Sukhbaatar, Dijia Su, Xian Li, Zhiting Hu, Jason Weston, Yuandong Tian, arXiv:2412.067692024. 2024arXiv preprint</p>
<p>Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, Furu Wei, arXiv:2212.06713Structured prompting: Scaling in-context learning to 1,000 examples. 2022. 2022arXiv preprint</p>
<p>LLM-guided spatio-temporal disease progression modelling. Tiantian He, An Zhao, Elinor Thompson, Anna Schroder, Ahmed Abdulaal, Frederik Barkhof, Daniel C Alexander, n. d.</p>
<p>Namgyu Ho, Laura Schmid, Se-Young Yun, arXiv:2212.10071Large language models are reasoning teachers. 2022. 2022arXiv preprint</p>
<p>Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, arXiv:2308.00352[cs.AILingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2023. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. </p>
<p>Zhenyu Hou, Pengfan Du, Yilin Niu, Zhengxiao Du, Aohan Zeng, Xiao Liu, Minlie Huang, Hongning Wang, Jie Tang, Yuxiao Dong, arXiv:2412.06000Does RLHF Scale? Exploring the Impacts From Data, Model, and Method. 2024. 2024arXiv preprint</p>
<p>REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models. Jian Hu, arXiv:2501.032622025. 2025arXiv preprint</p>
<p>Zhongzhen Huang, Gui Geng, Shengyi Hua, Zhen Huang, Haoyang Zou, Shaoting Zhang, Pengfei Liu, Xiaofan Zhang, arXiv:2501.06458O1 Replication Journey-Part 3: Inference-time Scaling for Medical Reasoning. 2025. 2025arXiv preprint</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, arXiv:2412.16720Openai o1 system card. 2024. 2024arXiv preprint</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, ACM computing surveys. 552023. 2023</p>
<p>Ragthief: Scalable extraction of private data from retrieval-augmented generation applications with agent-based attacks. Changyue Jiang, Xudong Pan, Geng Hong, Chenfu Bao, Min Yang, arXiv:2411.141102024. 2024arXiv preprint</p>
<p>SafeChain: Safety of Language Models with Long Chain-of. Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, Radha Poovendran, arXiv:2502.120252025. 2025-Thought Reasoning Capabilities. arXiv preprint</p>
<p>Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, Sunghun Kim, arXiv:2406.00515A survey on large language models for code generation. 2024. 2024arXiv preprint</p>
<p>Longrag: Enhancing retrieval-augmented generation with long-context llms. Ziyan Jiang, Xueguang Ma, Wenhu Chen, arXiv:2406.153192024. 2024arXiv preprint</p>
<p>Active retrieval augmented generation. Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Active retrieval augmented generation. Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Scalable fine-tunning strategies for llms in finance domain-specific application for credit union. Kartheek Kalluri, 2024</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020. 2020arXiv preprint</p>
<p>Dense Passage Retrieval for Open-Domain Question Answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, S H Patrick, Ledell Lewis, Sergey Wu, Danqi Edunov, Wen-Tau Chen, Yih, EMNLP. 2020</p>
<p>Zachary Kenton, Noah Y Siegel, János Kramár, Jonah Brown-Cohen, Samuel Albanie, Jannis Bulian, Rishabh Agarwal, David Lindner, Yunhao Tang, Noah D Goodman, arXiv:2407.04622On scalable oversight with weak LLMs judging strong LLMs. 2024. 2024arXiv preprint</p>
<p>Debating with more persuasive LLMs leads to more truthful answers. Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Tim Samuel R Bowman, Ethan Rocktäschel, Perez, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024</p>
<p>Tree of clarifications: Answering ambiguous questions with retrievalaugmented large language models. Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, Jaewoo Kang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent Reinforcement Learning in USV Swarm. Hyeonjun Kim, Kanghoon Lee, Junho Park, Jiachen Li, Jinkyoo Park, arXiv:2503.037962025. 2025arXiv preprint</p>
<p>Self-generated in-context learning: Leveraging autoregressive language models as a demonstration generator. Joon Hyuhng, Hyunsoo Kim, Junyeob Cho, Taeuk Kim, Kang Kim, Min Yoo, Sang-Goo Lee, arXiv:2206.080822022. 2022arXiv preprint</p>
<p>Mdagents: An adaptive collaboration of llms for medical decision-making. Yubin Kim, Chanwoo Park, Hyewon Jeong, Yik Siu Chan, Xuhai Xu, Daniel Mcduff, Hyeonhoon Lee, Marzyeh Ghassemi, Cynthia Breazeal, Hae Park, Advances in Neural Information Processing Systems. 372024. 2024</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 352022. 2022</p>
<p>Understanding the effects of iterative prompting on truthfulness. Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju, arXiv:2402.066252024. 2024arXiv preprint</p>
<p>Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, Eugene Bagdasarian, arXiv:2502.02542OVERTHINKING: Slowdown Attacks on Reasoning LLMs. 2025. 2025arXiv preprint</p>
<p>Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip Hs Torr, Salman Khan, Fahad Shahbaz Khan, arXiv:2502.21321LLM Post-Training: A Deep Dive into Reasoning Large Language Models. 2025. 2025arXiv preprint</p>
<p>H-cot: Hijacking the chain-of-thought safety reasoning mechanism to jailbreak large reasoning models, including openai o1/o3, deepseek-r1, and gemini 2.0 flash thinking. Martin Kuo, Jianyi Zhang, Aolin Ding, Qinsi Wang, Louis Divalentin, Yujia Bao, Wei Wei, Da-Cheng Juan, Hai Li, Yiran Chen, arXiv:2502.128932025. 2025arXiv preprint</p>
<p>Multi-agent causal discovery using large language models. Xin Hao Duong Le, Zhang Xia, Chen, arXiv:2407.150732024. 2024arXiv preprint</p>
<p>Youngwon Lee, Seung-Won Hwang, Daniel Campos, Filip Graliński, Zhewei Yao, Yuxiong He, arXiv:2412.10684Inference Scaling for Bridging Retrieval and Augmented Generation. 2024. 2024arXiv preprint</p>
<p>Harnessing Large Language Models for Disaster Management: A Survey. Zhenyu Lei, Yushun Dong, Weiyu Li, Rong Ding, Qi Wang, Jundong Li, arXiv:2501.069322025. 2025arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in neural information processing systems. 332020. 2020</p>
<p>Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, arXiv:2411.16594From generation to judgment: Opportunities and challenges of llm-as-a-judge. 2024. 2024arXiv preprint</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent. Haohang Li, Yupeng Cao, Yangyang Yu, Shashidhar Reddy Javaji, Zhiyang Deng, Yueru He, Yuechen Jiang, Zining Zhu, Koduvayur Subbalakshmi, Guojun Xiong, arXiv:2412.181742024. 2024arXiv preprint</p>
<p>Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang, Jian-Yun Nie, Ji-Rong Wen, arXiv:2305.10998The web can be your oyster for improving large language models. 2023. 2023arXiv preprint</p>
<p>. Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong Wu, Lingpeng Kong, arXiv:2302.049312023. 2023arXiv preprintIn-context learning with many demonstration examples</p>
<p>Xingxuan Li, Xuan-Phi Nguyen, arXiv:2404.00570Shafiq Joty, and Lidong Bing. 2024. ParaICL: Towards Robust Parallel In-Context Learning. 2024arXiv preprint</p>
<p>Yucheng Li, Huiqiang Jiang, Qianhui Wu, Xufang Luo, Surin Ahn, Chengruidong Zhang, Dongsheng Amir H Abdi, Jianfeng Li, Yuqing Gao, Yang, arXiv:2412.10319Scbench: A kv cache-centric analysis of long-context methods. 2024. 2024arXiv preprint</p>
<p>Retrieval augmented generation or long-context llms? a comprehensive study and hybrid approach. Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track. the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track2024</p>
<p>Un-certaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling for Retrieval-Augmented Generation. Zixuan Li, Jing Xiong, Fanghua Ye, Chuanyang Zheng, Xun Wu, Jianqiao Lu, Zhongwei Wan, Xiaodan Liang, Chengming Li, Zhenan Sun, arXiv:2410.027192024. 2024arXiv preprint</p>
<p>Autokaggle: A multiagent framework for autonomous data science competitions. Ziming Li, Qianbo Zang, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian Yang, Jiaheng Liu, arXiv:2410.204242024. 2024arXiv preprint</p>
<p>Debatrix: Multi-dimensional debate judge with iterative chronological analysis based on llm. Jingcong Liang, Rong Ye, Meng Han, Ruofei Lai, Xinyu Zhang, Xuanjing Huang, Zhongyu Wei, arXiv:2403.080102024. 2024arXiv preprint</p>
<p>Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, Zhaopeng Tu, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, arXiv:2305.191182023. 2023arXiv preprint</p>
<p>Can large language models reason about medical questions?. Valentin Liévin, Egeberg Christoffer, Andreas Geert Hother, Ole Motzfeldt, Winther, Patterns. 532024. 2024</p>
<p>. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, Edwards, Leike, John Schulman, Ilya Sutskever, and Karl CobbeJanBowen Baker, Teddy Leen. d.</p>
<p>Let's verify step by step. The Twelfth International Conference on Learning Representations. </p>
<p>Truthfulqa: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, arXiv:2109.079582021. 2021arXiv preprint</p>
<p>Ring attention with blockwise transformers for near-infinite context. Hao Liu, Matei Zaharia, Pieter Abbeel, arXiv:2310.018892023. 2023arXiv preprint</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, arXiv:2101.06804What Makes Good In-Context Examples for GPT-3?. 2021. 2021arXiv preprint</p>
<p>Medcot: Medical chain of thought via hierarchical expert. Jiaxiang Liu, Yuan Wang, Jiawei Du, Joey Tianyi Zhou, Zuozhu Liu, arXiv:2412.137362024. 2024arXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. Kevin Nelson F Liu, John Lin, Ashwin Hewitt, Michele Paranjape, Fabio Bevilacqua, Percy Petroni, Liang, Transactions of the Association for Computational Linguistics. 122024. 2024</p>
<p>In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering. Sheng Liu, Haotian Ye, Lei Xing, James Y Zou, International Conference on Machine Learning. PMLR2024</p>
<p>Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning. Yinpeng Liu, Jiawei Liu, Xiang Shi, Qikai Cheng, Yong Huang, Wei Lu, arXiv:2402.107382024. 2024arXiv preprint</p>
<p>ChunkKV: Chunk-based keyvalue cache management for transformer models. Yutong Liu, Pengfei Yang, Hao Zhou, arXiv:2501.114072025. 2025arXiv preprint</p>
<p>Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization. Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, Diyi Yang, arXiv:2310.021702023. 2023arXiv preprint</p>
<p>Large language model guided tree-of-thought. Jieyi Long, arXiv:2305.082912023. 2023arXiv preprint</p>
<p>Sparser is faster and less is more: Efficient sparse attention for long-range transformers. Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu, arXiv:2406.167472024. 2024arXiv preprint</p>
<p>Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, Yunsheng Wu, arXiv:2308.082392023. 2023arXiv preprint</p>
<p>Keer Lu, Zheng Liang, Da Pan, Shusen Zhang, Xin Wu, Weipeng Chen, Zenan Zhou, Guosheng Dong, Bin Cui, Wentao Zhang, arXiv:2501.11885Med-R 2 : Crafting Trustworthy LLM Physicians through Retrieval and Reasoning of Evidence-Based Medicine. 2025. 2025arXiv preprint</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, arXiv:2104.087862021. 2021arXiv preprint</p>
<p>Power hungry processing: Watts driving the cost of ai deployment. Sasha Luccioni, Yacine Jernite, Emma Strubell, Proceedings of the 2024 ACM conference on fairness. the 2024 ACM conference on fairness2024</p>
<p>Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li, arXiv:2401.08967Reft: Reasoning with reinforced fine-tuning. 2024. 2024arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Teaching small language models to reason. Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn, arXiv:2212.084102022. 2022arXiv preprint</p>
<p>Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang, arXiv:2402.17753Evaluating very long-term conversational memory of llm agents. 2024. 2024arXiv preprint</p>
<p>Adaptive inference-time compute: Llms can predict if they can do better, even mid-generation. Rohin Manvi, Anikait Singh, Stefano Ermon, arXiv:2410.027252024. 2024arXiv preprint</p>
<p>Inverse Scaling: When Bigger Isn't Better. Ir Mckenzie, Lyzhov, Pieler, Parrish, Mueller, Prabhu, Mclean, Kirtland, Ross, Liu, Transactions on machine learning research. 2024. 2024</p>
<p>Julian Michael, Salsabila Mahdi, David Rein, Jackson Petty, Julien Dirani, Vishakh Padmakumar, Samuel R Bowman, arXiv:2311.08702Debate helps supervise unreliable experts. 2023. 2023arXiv preprint</p>
<p>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Tree of uncertain thoughts reasoning for large language models. Shentong Mo, Miao Xin, ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2024</p>
<p>Many-shot in-context learning for molecular inverse design. Saeed Moayedpour, Alejandro Corrochano-Navarro, Faryad Sahneh, Shahriar Noroozizadeh, Alexander Koetter, Jiri Vymetal, Lorenzo Kogler-Anele, Pablo Mas, Yasser Jangjou, Sizhen Li, arXiv:2407.190892024. 2024arXiv preprint</p>
<p>Amirkeivan Mohtashami, Matteo Pagliardini, Martin Jaggi, arXiv:2310.10845CoT-Former: A Chain-of-Thought Driven Architecture with Budget-Adaptive Computation Cost at Inference. 2023. 2023arXiv preprint</p>
<p>Scaling data-constrained language models. Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, Colin A Raffel, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, arXiv:2501.19393Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. 2025arXiv preprint</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Webgpt: Browser-assisted question-answering with human feedback. 2021. 2021arXiv preprint</p>
<p>Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong , Yu Wang, arXiv:2307.15337Skeleton-of-thought: Prompting llms for efficient parallel generation. 2023. 2023arXiv preprint</p>
<p>. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Red Anadkat, Igor Avila, Suchir Babuschkin, Valerie Balaji, Paul Balcom, Haiming Baltescu, Mohammad Bao, Jeff Bavarian, Irwan Belgum, Jake Bello, Gabriel Berdine, Christopher Bernadett-Shapiro, Lenny Berner, Oleg Bogdonoff, Madelaine Boiko, Anna-Luisa Boyd, Greg Brakman, Tim Brockman, Miles Brooks, Kevin Brundage, Trevor Button, Rosie Cai, Andrew Campbell, Brittany Cann, Chelsea Carey, Rory Carlson, Brooke Carmichael, Che Chan, Fotis Chang, Derek Chantzis, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chen, Chester Chess, Casey Cho, Hyung Won Chu, Dave Chung, Jeremiah Cummings, Yunxing Currier, Cory Dai, Thomas Decareaux, Noah Degry, Damien Deutsch, Arka Deville, David Dhar, Steve Dohan, Sheila Dowling, Adrien Dunning, Atty Ecoffet, Tyna Eleti, David Eloundou, Liam Farhi, Niko Fedus, Simón Felix, Juston Posada Fishman, Isabella Forte, Leo Fulford, Elie Gao, Christian Georges, Vik Gibson, Tarun Goel, Gabriel Gogineni, Rapha Goh, Jonathan Gontijo-Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Gross, Shane Shixiang, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Yuchen Harris, Mike He, Johannes Heaton, Chris Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Brandon Hoeschele, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Joanne Jain, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Heewoo Jonn, Tomer Jun, Łukasz Kaftan, Ali Kaiser, Ingmar Kamali, Nitish Kanitscheider, Tabarak Shirish Keskar, Logan Khan, Jong Wook Kilpatrick, Christina Kim, Yongjik Kim, Jan Hendrik Kim, Jamie Kirchner, Matt Kiros, Daniel Knight, Łukasz Kokotajlo, Andrew Kondraciuk, Aris Kondrich, Kyle Konstantinidis, Gretchen Kosic, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Levy, Ming Chak, Rachel Li, Molly Lim, Stephanie Lin, Mateusz Lin, Theresa Litwin, Ryan Lopez, Patricia Lowe, Anna Lue, Kim Makanju, Sam Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, Mayne ; Aalok, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Ashvin Mishkin ; Mély, Reiichiro Nair, Rajeev Nakano, Arvind Nayak, Richard Neelakantan, Hyeonwoo Ngo, Long Noh, Ouyang, O' Cullen, Jakub Keefe, Alex Pachocki, Joe Paino, Ashley Palermo, Giambattista Pantuliano, Joel Parascandolo, Emy Parish, Alex Parparita, Mikhail Passos, Andrew Pavlov, Adam Peng, Kyla Perelman, Toki Sheppard, Jessica Sherbakov, Sarah Shieh, Pranav Shoker, Szymon Shyam, Eric Sidor, Maddie Sigler, Jordan Simens, Katarina Sitkin, Ian Slama, Benjamin Sohl, Yang Sokolowsky, Natalie Song, Staudacher, Wei, Akila Cj Weinmann, Peter Welihinda, Jiayi Welinder, Lilian Weng, Matt Weng, Dave Wiethoff, Clemens Willner, Samuel Winter, Hannah Wolrich, Lauren Wong, Sherwin Workman, Jeff Wu, Michael Wu, Kai Wu, Tao Xiao, Sarah Xu, Kevin Yoo, Qiming Yu, Wojciech Yuan, Rowan Zaremba, Chong Zellers, Marvin Zhang, Shengjia Zhang, Zhao, Felipe Petroski Such. Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam,Filipe de Avila Belbute Peres ; Juan Felipe Cerón Uribe, Andrea Vallone, Arun VijayvergiyaTianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL</p>
<p>Llm-assisted crisis management: Building advanced llm platforms for effective emergency response and public collaboration. Eric Hakan T Otal, Stern, Canbaz Abdullah, 2024 IEEE Conference on Artificial Intelligence (CAI). IEEE2024</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 352022. 2022</p>
<p>Deonna M Owens, Ryan A Rossi, Sungchul Kim, Tong Yu, Franck Dernoncourt, Xiang Chen, Ruiyi Zhang, Jiuxiang Gu, arXiv:2409.13884Hanieh Deilamsalehy, and Nedim Lipka. 2024. A multi-llm debiasing framework. 2024arXiv preprint</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse automated correction strategies. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang, Wang , Transactions of the Association for Computational Linguistics. 122024. 2024</p>
<p>Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, Chin-Yew Lin, Vicky Zhao, Lili Qiu, arXiv:2502.05589On Memory Construction and Retrieval for Personalized Conversational Agents. 2025. 2025arXiv preprint</p>
<p>Francisco Core, Andrew Park, Ekdeep Lee, Yongyi Singh Lubana, Maya Yang, Kento Okawa, Martin Nishi, Hidenori Wattenberg, Tanaka, arXiv:2501.00070Iclr: In-context learning of representations. 2024. 2024arXiv preprint</p>
<p>The carbon footprint of machine learning training will plateau, then shrink. David Patterson, Joseph Gonzalez, Urs Hölzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, Maud David R So, Jeff Texier, Dean, Computer. 552022. 2022</p>
<p>REFINER: Reasoning Feedback on Intermediate Representations. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, Proceedings of the 18th Conference of the European Chapter. Long Papers. the 18th Conference of the European Chapterthe Association for Computational Linguistics20241</p>
<p>LET MODELS SPEAK CIPHERS: MULTIAGENT DEBATE THROUGH EMBEDDINGS. Chau Pham, Boyi Liu, Yingxiang Yang, Zhengyu Chen, Tianyi Liu, Jianbo Yuan, Bryan A Plummer, Zhaoran Wang, Hongxia Yang, ICLR 202412th International Conference on Learning Representations. 2024</p>
<p>Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Dmytro Okhonko, Samuel Broscheit, Gautier Izacard, Patrick Lewis, Barlas Oğuz, Edouard Grave, Wen-Tau Yih, arXiv:2112.09924The web is your oyster-knowledge-intensive NLP against a very large web corpus. 2021. 2021arXiv preprint</p>
<p>Lingfei Qian, Weipeng Zhou, Yan Wang, Xueqing Peng, Jimin Huang, Qianqian Xie, arXiv:2502.08127Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance. 2025. 2025arXiv preprint</p>
<p>Enhancing LLM-as-a-Judge via Multi-Agent Collaboration. Yiyue Qian, Shinan Zhang, Yun Zhou, Haibo Ding, Diego Socolinsky, Yi Zhang, 2025. 2025</p>
<p>RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering. Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, Haifeng Wang, arXiv:2010.081912020. 2020arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Llm harmony: Multi-agent communication for problem solving. Sumedh Rasal, arXiv:2401.013122024. 2024arXiv preprint</p>
<p>Disasterqa: A benchmark for assessing the performance of llms in disaster response. Rajat Rawat, arXiv:2410.207072024. 2024arXiv preprint</p>
<p>Learning To Retrieve Prompts for In-Context Learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, Tianlu Wang, arXiv:2501.18099Learning to Plan &amp; Reason for Evaluation with Thinking-LLM-asa-Judge. 2025. 2025arXiv preprint</p>
<p>From words to watts: Benchmarking the energy costs of large language model inference. Siddharth Samsi, Dan Zhao, Joseph Mcdonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, Vijay Gadepally, 2023 IEEE High Performance Extreme Computing Conference (HPEC). IEEE2023</p>
<p>Raptor: Recursive abstractive processing for tree-organized retrieval. Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D Manning, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Reasoning with Latent Thoughts: On the Power of Looped Transformers. Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, Sashank J Reddi, arXiv:2502.174162025. 2025arXiv preprint</p>
<p>Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine. Thomas Savage, Ashwin Nayak, Robert Gallo, Ekanath Rangan, Jonathan H Chen, NPJ Digital Medicine. 7202024. 2024</p>
<p>Openfact at checkthat! 2023: head-to-head gpt vs. bert-a comparative study of transformers language models for the detection of check-worthy claims. Marcin Sawiński, Krzysztof Węcel, Paulina Ewelina, Milena Księżniak, Włodzimierz Stróżyna, Piotr Lewoniewski, Witold Stolarski, Abramowicz, CEUR Workshop Proceedings. 20233497</p>
<p>John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel, arXiv:1506.02438High-dimensional continuous control using generalized advantage estimation. 2015. 2015arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017. 2017arXiv preprint</p>
<p>Algorithm of thoughts: Enhancing exploration of ideas in large language models. Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Ruoxi Jia, Ming Jin, arXiv:2308.103792023. 2023arXiv preprint</p>
<p>Scaling retrieval-based language models with a trillion-token datastore. Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, Pang Wei, W Koh, Advances in Neural Information Processing Systems. 372024. 2024</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Li, Wu, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024. 2024arXiv preprint</p>
<p>Towards natural language interfaces for data visualization: A survey. Leixian Shen, Enya Shen, Yuyu Luo, Xiaocong Yang, Xuming Hu, Xiongshuai Zhang, Zhiwei Tai, Jianmin Wang, IEEE transactions on visualization and computer graphics. 292022. 2022</p>
<p>Improving reinforcement learning from human feedback using contrastive rewards. Wei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, Yang Liu, arXiv:2403.077082024. 2024arXiv preprint</p>
<p>Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, Jiuxiang Gu, arXiv:2501.19201Efficient Reasoning with Hidden Thinking. 2025. 2025arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Distilling reasoning capabilities into smaller language models. Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan, Findings of the Association for Computational Linguistics: ACL 2023. 2023. 2023</p>
<p>Evidence-backed fact checking using RAG and few-shot in-context learning with LLMs. Ronit Singhal, Pransh Patwa, Parth Patwa, Aman Chadha, Amitava Das, arXiv:2408.120602024. 2024arXiv preprint</p>
<p>Scaling llm testtime compute optimally can be more effective than scaling model parameters. Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar, arXiv:2408.033142024. 2024arXiv preprint</p>
<p>Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary Empirical Study. Mingyang Song, Mao Zheng, Xuan Luo, Yue Pan, arXiv:2406.116292024. 2024arXiv preprint</p>
<p>Jihoon Tack, Jack Lanchantin, Jane Yu, Andrew Cohen, Ilia Kulikov, Janice Lan, Shibo Hao, Yuandong Tian, Jason Weston, Xian Li, arXiv:2502.08524LLM Pretraining with Continuous Concepts. 2025. 2025arXiv preprint</p>
<p>Large language models for data annotation: A survey. Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu, arXiv:2402.134462024. 2024arXiv preprint</p>
<p>. Zhen Tan, Jie Peng, Tianlong Chen, Huan Liu, arXiv:2403.056362024. 2024Tuning-Free Accountable Intervention for LLM Deployment-A Metacognitive Approach. arXiv preprint</p>
<p>Zhen Tan, Jun Yan, I-Hung Hsu, Rujun Han, Zifeng Wang, Long T Le, Yiwen Song, Yanfei Chen, Hamid Palangi, George Lee, Anand Iyer, Tianlong Chen, Huan Liu, Chen-Yu Lee, Tomas Pfister, arXiv:2503.08026[cs.CLProspect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents. 2025</p>
<p>Glue pizza and eat rocks-Exploiting Vulnerabilities in Retrieval-Augmented Generative Models. Zhen Tan, Chengshuai Zhao, Raha Moraffah, Yifan Li, Song Wang, Jundong Li, Tianlong Chen, Huan Liu, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Minicheck: Efficient factchecking of llms on grounding documents. Liyan Tang, Philippe Laban, Greg Durrett, arXiv:2404.107742024. 2024arXiv preprint</p>
<p>Gemini Team, Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Wang, arXiv:2403.05530Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. 2024. 2024arXiv preprint</p>
<p>Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, O' Barry, Sullivan, Hoang D Nguyen, arXiv:2501.06322Multi-Agent Collaboration Mechanisms: A Survey of LLMs. 2025. 2025arXiv preprint</p>
<p>Solving math word problems with process-and outcome-based feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, arXiv:2211.142752022. 2022arXiv preprint</p>
<p>. Ashwin Verma, Soheil Mohajer, Behrouz Touri, arXiv:2503.021162025. 2025Multi-Agent Fact Checking. arXiv preprint</p>
<p>Teach better or show smarter? on instructions and exemplars in automatic prompt optimization. Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Sercan Arik, Advances in Neural Information Processing Systems. 372025. 2025</p>
<p>From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation. Xingchen Wan, Han Zhou, Ruoxi Sun, Hootan Nakhost, Ke Jiang, Sercan Ö Arık, arXiv:2502.003302025. 2025arXiv preprint</p>
<p>InstructRetro: instruction tuning post retrievalaugmented pretraining. Boxin Wang, Wei Ping, Lawrence Mcafee, Peng Xu, Bo Li, Mohammad Shoeybi, Bryan Catanzaro, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024</p>
<p>Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study. Boxin Wang, Wei Ping, Peng Xu, Lawrence Mcafee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Unims-rag: A unified multisource retrieval-augmented generation for personalized dialogue systems. Hongru Wang, Wenyu Huang, Yang Deng, Rui Wang, Zezhong Wang, Yufei Wang, Fei Mi, Jeff Z Pan, Kam-Fai Wong, arXiv:2401.132562024. 2024arXiv preprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Jirong Wen, 10.1007/s11704-024-40231-1Frontiers of Computer Science. 1862024. March 2024</p>
<p>Peiyi Wang, Lei Li, Zhihong Shao, Damai Xu, Yifei Dai, Deli Li, Yu Chen, Zhifang Wu, Sui, arXiv:2312.08935Math-shepherd: Verify and reinforce llms step-by-step without human annotations. 2023. 2023arXiv preprint</p>
<p>Recursively summarizing enables long-term dialogue memory in large language models. Qingyue Wang, Liang Ding, Yanan Cao, Zhiliang Tian, Shi Wang, Dacheng Tao, Li Guo, arXiv:2308.150222023. 2023arXiv preprint</p>
<p>Mixture of Demonstrations for In-Context Learning. Song Wang, Zihan Chen, Chengshuai Shi, Cong Shen, Jundong Li, Advances in Neural Information Processing Systems. 372025. 2025</p>
<p>Beyond the limits: A survey of techniques to extend the context length in large language models. Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, Armaghan Eshaghi, arXiv:2402.022442024. 2024arXiv preprint</p>
<p>Adaptive Retrieval-Augmented Generation for Conversational Systems. Xi Wang, Procheta Sen, Ruizhe Li, Emine Yilmaz, arXiv:2407.217122024. 2024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022. 2022arXiv preprint</p>
<p>Chain-of-thought reasoning without prompting. Xuezhi Wang, Denny Zhou, arXiv:2402.102002024. 2024arXiv preprint</p>
<p>Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, arXiv:2501.18585Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs. 2025. 2025arXiv preprint</p>
<p>Inverse scaling can become U-shaped. Jason Wei, Najoung Kim, Yi Tay, Quoc Le, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2023</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022. 2022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 352022. 2022</p>
<p>Ting-Ruen, Haowei Wei, Xuyang Liu, Yi Wu, Fang, arXiv:2502.14333A Survey on Feedback-based Multi-step Reasoning for Large Language Models on Mathematics. 2025. 2025arXiv preprint</p>
<p>Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, Sida I Wang, arXiv:2502.18449SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution. 2025. 2025arXiv preprint</p>
<p>LLM-powered Autonomous Agents. Lilian Weng, 2023. Jun 2023</p>
<p>Large Language Models With Holistically Thought Could Be Better Doctors. Yixuan Weng, Bin Li, Fei Xia, Minjun Zhu, Bin Sun, Shizhu He, Shengping Liu, Kang Liu, Shutao Li, Jun Zhao, CCF International Conference on Natural Language Processing and Chinese Computing. Springer2024</p>
<p>The impact and opportunities of Generative AI in fact-checking. Robert Wolfe, Tanushree Mitra, Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency. the 2024 ACM Conference on Fairness, Accountability, and Transparency2024</p>
<p>Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, arXiv:2303.04671Visual chatgpt: Talking, drawing and editing with visual foundation models. 2023. 2023arXiv preprint</p>
<p>A survey of human-in-the-loop for machine learning. Xingjiao Wu, Luwei Xiao, Yixuan Sun, Junhang Zhang, Tianlong Ma, Liang He, Future Generation Computer Systems. 1352022. 2022</p>
<p>Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, Yiming Yang, arXiv:2408.007242024. 2024arXiv preprint</p>
<p>A question and answering service of typhoon disasters based on the t5 large language model. Yongqi Xia, Yi Huang, Qianqian Qiu, Xueying Zhang, Lizhi Miao, Yixiang Chen, ISPRS International Journal of Geo-Information. 131652024. 2024</p>
<p>Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, Prateek Mittal, arXiv:2405.15556Certifiably robust rag against retrieval corruption. 2024. 2024arXiv preprint</p>
<p>Badchain: Backdoor chain-of-thought prompting for large language models. Zhen Xiang, Fengqing Jiang, Zidi Xiong, Radha Bhaskar Ramasubramanian, Bo Poovendran, Li, arXiv:2401.122422024. 2024arXiv preprint</p>
<p>CBD: A certified backdoor detector based on local dominant probability. Zhen Xiang, Zidi Xiong, Bo Li, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Yijia Xiao, Edward Sun, Di Luo, Wei Wang, arXiv:2412.20138TradingAgents: Multi-Agents LLM Financial Trading Framework. 2024. 2024arXiv preprint</p>
<p>Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, Chong Luo, arXiv:2502.14768Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. 2025. 2025arXiv preprint</p>
<p>Monte carlo tree search boosts reasoning via iterative preference learning. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P Lillicrap, Kenji Kawaguchi, Michael Shieh, arXiv:2405.004512024. 2024arXiv preprint</p>
<p>Benchmarking retrieval-augmented generation for medicine. Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, Arnold Overwijk, arXiv:2007.00808Approximate nearest neighbor negative contrastive learning for dense text retrieval. 2020. 2020arXiv preprint</p>
<p>Recomp: Improving retrievalaugmented lms with compression and selective augmentation. Fangyuan Xu, Weijia Shi, Eunsol Choi, arXiv:2310.044082023. 2023arXiv preprint</p>
<p>Retrieval meets long context large language models. Peng Xu, Wei Ping, Xianchao Wu, Lawrence Mcafee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, Bryan Catanzaro, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Peng Xu, Wei Ping, Xianchao Wu, Chejian Xu, Zihan Liu, Mohammad Shoeybi, Bryan Catanzaro, arXiv:2407.14482Chatqa 2: Bridging the gap to proprietary llms in long context and rag capabilities. 2024. 2024arXiv preprint</p>
<p>Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game. Zelai Xu, Chao Yu, Fei Fang, Yu Wang, Yi Wu, arXiv:2310.18940[cs.AI2024</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.15115Qwen2. 5 technical report. 2024. 2024arXiv preprint</p>
<p>Watch out for your agents! investigating backdoor threats to llm-based agents. Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, Xu Sun, Advances in Neural Information Processing Systems. 372024. 2024</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in neural information processing systems. 362023. 2023</p>
<p>Compositional exemplars for in-context learning. Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong, International Conference on Machine Learning. PMLR2023</p>
<p>Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, Pengfei Liu, arXiv:2502.03387LIMO: Less is More for Reasoning. 2025. 2025arXiv preprint</p>
<p>A survey on recent advances in llm-based multi-turn dialogue systems. Zihao Yi, Jiarui Ouyang, Yuwen Liu, Tianhao Liao, Zhe Xu, Ying Shen, arXiv:2402.180132024. 2024arXiv preprint</p>
<p>Kai Yin, Chengkai Liu, Ali Mostafavi, Xia Hu, arXiv:2406.15477Crisissense-llm: Instruction fine-tuned large language model for multi-label social media text classification in disaster informatics. 2024. 2024arXiv preprint</p>
<p>FinMem: A performance-enhanced LLM trading agent with layered memory and character design. Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu, Jordan W Suchow, Khaldoun Khashanah, Proceedings of the AAAI Symposium Series. the AAAI Symposium Series20243</p>
<p>Fincon: A synthesized llm multi-agent system with conceptual verbal reinforcement for enhanced financial decision making. Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yuechen Jiang, Yupeng Cao, Zhi Chen, Jordan Suchow, Zhenyu Cui, Rong Liu, Advances in Neural Information Processing Systems. 372024. 2024</p>
<p>Scaling relationship on learning mathematical reasoning with large language models. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, Jingren Zhou, arXiv:2308.018252023. 2023arXiv preprint</p>
<p>Inference Scaling for Long-Context Retrieval Augmented Generation. Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, Michael Bendersky, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Towards Maps of Disease Progression: Biomedical Large Language Model Latent Spaces For Representing Disease Phenotypes And Pseudotime. Rafael Zamora-Resendiz, Ifrah Khurram, Silvia Crivelli, medRxiv. 2024. 2024</p>
<p>Quiet-star: Language models can teach themselves to think before speaking. Eric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah Goodman, First Conference on Language Modeling. 2024</p>
<p>Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, Rishabh Agarwal, arXiv:2408.15240Generative verifiers: Reward modeling as nexttoken prediction. 2024. 2024arXiv preprint</p>
<p>More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives. Xiaoqing Zhang, Ang Lv, Yuhan Liu, Flood Sung, Wei Liu, Shuo Shang, Xiuying Chen, Rui Yan, arXiv:2501.040702025. 2025arXiv preprint</p>
<p>Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin, arXiv:2501.07301The lessons of developing process reward models in mathematical reasoning. 2025. 2025arXiv preprint</p>
<p>Optimizing LLM based retrieval augmented generation pipelines in the financial domain. Yiyun Zhao, Prateek Singh, Hanoz Bhathena, Bernardo Ramos, Aviral Joshi, Swaroop Gadiyaram, Saket Sharma, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20246Industry Track</p>
<p>Cape: Contextadaptive positional encoding for length extrapolation. Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, arXiv-24052024. 2024arXiv e-prints</p>
<p>Dape: Data-adaptive positional encoding for length extrapolation. Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Advances in Neural Information Processing Systems. 372024. 2024</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P Xing, arXiv:2309.11998Lmsyschat-1m: A large-scale real-world llm conversation dataset. 2023. 2023arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Lima: Less is more for alignment. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Huichi Zhou, Kin-Hei Lee, Zhonghao Zhan, Yue Chen, Zhenhao Li, arXiv:2501.00879TrustRAG: Enhancing Robustness and Trustworthiness in RAG. 2025. 2025arXiv preprint</p>
<p>Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions. Jiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G Parker, Munmun De Choudhury, Proceedings of the 2023 CHI conference on human factors in computing systems. the 2023 CHI conference on human factors in computing systems2023</p>
<p>Revisiting the Scaling Effects of LLMs on Medical Reasoning Capabilities. Yuxuan Zhou, Xien Liu, Chen Ning, Xiao Zhang, Chenwei Yan, Xiangling Fu, Ji Wu, </p>
<p>TAT-LLM: A Specialized Language Model for Discrete Reasoning over Financial Tabular and Textual Data. Fengbin Zhu, Ziyang Liu, Fuli Feng, Chao Wang, Moxin Li, Tat Seng, Chua , Proceedings of the 5th ACM International Conference on AI in Finance. the 5th ACM International Conference on AI in Finance2024</p>
<p>Hang Zou, Qiyang Zhao, Lina Bariah, Yu Tian, Mehdi Bennis, Samson Lasaulce, Mérouane Debbah, Faouzi Bader, arXiv:2402.16631GenAINet: Enabling wireless collective intelligence via knowledge transfer and reasoning. 2024. 2024arXiv preprint</p>
<p>Retrieval or Global Context Understanding? On Many-Shot In-Context Learning for Long-Context Evaluation. Kaijian Zou, Muhammad Khalifa, Lu Wang, arXiv:2411.071302024. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
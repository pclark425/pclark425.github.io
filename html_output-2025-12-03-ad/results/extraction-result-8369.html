<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8369 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8369</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8369</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-baf257116866b91781b9259662feccd59b5234a2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/baf257116866b91781b9259662feccd59b5234a2" target="_blank">Understanding Addition in Transformers</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> An in-depth analysis of a one-layer Transformer model trained for integer addition is presented, revealing that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions.</p>
                <p><strong>Paper Abstract:</strong> Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8369.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8369.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>1-layer addition Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-layer autoregressive Transformer trained for n-digit integer addition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-layer autoregressive Transformer (experimented with 2/3/4 attention heads, typically 3) trained on n-digit integer addition (5/10/15 digits). The model implements digit-wise parallel computation via attention heads that implement BaseAdd / MakeCarry / MakeSum subtasks and an MLP that combines head outputs (treated as bigram/trigram keys) to produce answer digits one token before output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom 1-layer Transformer (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive single Transformer block (n_layers=1) with experiments using n_heads in {2,3,4} (primary analysis on 3 heads). Trained on streamed synthetic addition data (≈1.5–1.8M training examples, sampled from ~10B possible 5-digit questions); experiments run for 5/10/15-digit addition variants. Architecture details beyond 1 layer / head counts not specified (embedding/hidden sizes not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>n-digit integer addition (5-, 10-, 15-digit variants); autoregressive output of the full summed digits (model must emit all answer tokens sequentially).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Digit-wise parallel streams: model decomposes addition into per-position subtasks. Identified base functions: BaseAdd (BA: (d_i+d'_i) mod 10), MakeCarry1 (MC1: whether d_i+d'_i ≥ 10), MakeSum9 (MS9: whether d_i+d'_i == 9). Compound/propagation functions: UseCarry1 (UC1) and UseSum9 (US9) (US9 handles cascade carry-through-9 behavior). Attention heads form a 'double staircase' pattern attending to corresponding digit pairs; heads are time-offset so each answer digit is computed from 3 head outputs (a 10-state head for BA, a tri-state head for MC1/MS9, and a binary head for distant MC1). The MLP acts as a key-value memory combining the heads' discrete outputs into the final digit (conceptualized as a 2×3×10 = 60-key trigram lookup). Computation is concentrated in a short window (6 rows/tokens) immediately prior to output; the algorithm is effectively stateless across loop iterations (each digit computed independently, with limited cascade handling).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Attention visualization (token-by-token attention maps), per-token / per-head mean ablation (residual stream and attention outputs), automated behavior-filtering search for nodes implementing BA/MC1/US9, per-digit and per-task training-loss decomposition, curated test set of >100 hand-crafted cases (BA, MC1, US9 simple and cascade cases), row-wise ablation of residuals to find computation window, head-wise ablation to locate task-specific heads.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported losses (negative log-likelihood) and ablation-loss effects rather than raw accuracy; examples: final overall training loss for 5-digit model ≈0.009; per-digit final losses for 5-digit model: A5 <0.001, A4 <0.001, A3 0.003, A2 0.008, A1 0.046, A0 0.001 (Table 2). Per-question-type final losses (5-digit): BA 0.021, MC1 0.001, US9 <0.001 (Table 3), noting enriched training sampling for rare cases. Row-wise ablation: ablating rows 11–16 (the 6 computation rows) causes large loss increases (e.g., row 15 loss 1.181 vs background ≈0.07) indicating computation is concentrated there (Table 4). Head ablation: ablating the head responsible for BA increases average loss dramatically (~50×; example values: head ablation average loss 4.712 in Table 6 and other reported head-specific losses up to 3.7 in text depending on head numbering). Test-case failure statistics (from curated set) show systematic single-digit failures matching the per-row computation order (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Primary failure mode is inability to reliably handle cascading 'sum-to-9' carry chains (US9 cascades) that require multi-column right-to-left carry propagation; the learned algorithm is effectively left-to-right/time-dense and only partially implements US9, so rare cascade cases (e.g., 445+555=1000 or long runs like 999999999+1) produce high loss/errors. Other observed issues: one high-value digit (highest-position digits like Digit 4 / A4/A5) can be implemented with a slightly different, faster/low-loss algorithm (model-specific asymmetry), some task-packing occurs when fewer heads are used (2-head case packs tasks into heads, 4-head case may split a logical task across heads), and the precise MLP memorization mapping is hypothesised but not fully proven via ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Multiple converging observations: (1) attention maps show a structured double-staircase where heads attend to corresponding digit pairs with a 3-token time-offset; (2) mean-ablation of residual stream per row demonstrates computation is performed only in a short window of 6 rows (rows 11–16) and each row primarily affects a single answer digit (Tables 4–5); (3) head-wise ablation causes large, task-specific loss increases (one head's ablation causes ~50× loss increase on BA questions), demonstrating head-specialization for BA/MC1/MS9; (4) per-digit/per-task training loss trajectories show phased discovery (BA discovered before UC1 before US9) and semi-independent digit training curves; (5) automated behavior-filtering and ablation localized nodes implementing BA/MC1/US9; (6) MLP capacity and behavior (consistent mapping of discrete head outputs to final digits) is consistent with key-value memory behavior reported in related work (Geva et al., 2021). Combined, these support a mechanistic decomposition: attention performs digit-pair information transfer, heads compute discrete small-state predicates/values, and the MLP performs a discrete lookup to emit the digit.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Clear counterexamples: cascading US9 cases and long carry chains (e.g., 999,999,999 + 1) where the model shows high loss and incorrect outputs—these expose the limits of the learned left-to-right, stateless per-digit computation. Ambiguities/inconsistencies: head numbering/identity across different analyses in the paper is not perfectly consistent (text and tables label the critical BA head differently), and the MLP's exact stored mappings are hypothesized but not experimentally proven; model variations (different random seeds / output formatting) reproduce the broad algorithm but with minor implementation differences, indicating the algorithm is robust but not identical across seeds. The MLP ablation and detailed causal role remain to be fully isolated experimentally (listed in Limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Addition in Transformers', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A mathematical framework for transformer circuits <em>(Rating: 2)</em></li>
                <li>Transformer feed-forward layers are key-value memories <em>(Rating: 2)</em></li>
                <li>Zoom in: An introduction to circuits <em>(Rating: 2)</em></li>
                <li>Progress measures for grokking via mechanistic interpretability <em>(Rating: 2)</em></li>
                <li>Locating and editing factual associations in gpt <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8369",
    "paper_id": "paper-baf257116866b91781b9259662feccd59b5234a2",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "1-layer addition Transformer",
            "name_full": "One-layer autoregressive Transformer trained for n-digit integer addition",
            "brief_description": "A single-layer autoregressive Transformer (experimented with 2/3/4 attention heads, typically 3) trained on n-digit integer addition (5/10/15 digits). The model implements digit-wise parallel computation via attention heads that implement BaseAdd / MakeCarry / MakeSum subtasks and an MLP that combines head outputs (treated as bigram/trigram keys) to produce answer digits one token before output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Custom 1-layer Transformer (this paper)",
            "model_description": "Autoregressive single Transformer block (n_layers=1) with experiments using n_heads in {2,3,4} (primary analysis on 3 heads). Trained on streamed synthetic addition data (≈1.5–1.8M training examples, sampled from ~10B possible 5-digit questions); experiments run for 5/10/15-digit addition variants. Architecture details beyond 1 layer / head counts not specified (embedding/hidden sizes not reported).",
            "arithmetic_task_type": "n-digit integer addition (5-, 10-, 15-digit variants); autoregressive output of the full summed digits (model must emit all answer tokens sequentially).",
            "mechanism_or_representation": "Digit-wise parallel streams: model decomposes addition into per-position subtasks. Identified base functions: BaseAdd (BA: (d_i+d'_i) mod 10), MakeCarry1 (MC1: whether d_i+d'_i ≥ 10), MakeSum9 (MS9: whether d_i+d'_i == 9). Compound/propagation functions: UseCarry1 (UC1) and UseSum9 (US9) (US9 handles cascade carry-through-9 behavior). Attention heads form a 'double staircase' pattern attending to corresponding digit pairs; heads are time-offset so each answer digit is computed from 3 head outputs (a 10-state head for BA, a tri-state head for MC1/MS9, and a binary head for distant MC1). The MLP acts as a key-value memory combining the heads' discrete outputs into the final digit (conceptualized as a 2×3×10 = 60-key trigram lookup). Computation is concentrated in a short window (6 rows/tokens) immediately prior to output; the algorithm is effectively stateless across loop iterations (each digit computed independently, with limited cascade handling).",
            "probing_or_intervention_method": "Attention visualization (token-by-token attention maps), per-token / per-head mean ablation (residual stream and attention outputs), automated behavior-filtering search for nodes implementing BA/MC1/US9, per-digit and per-task training-loss decomposition, curated test set of &gt;100 hand-crafted cases (BA, MC1, US9 simple and cascade cases), row-wise ablation of residuals to find computation window, head-wise ablation to locate task-specific heads.",
            "performance_metrics": "Reported losses (negative log-likelihood) and ablation-loss effects rather than raw accuracy; examples: final overall training loss for 5-digit model ≈0.009; per-digit final losses for 5-digit model: A5 &lt;0.001, A4 &lt;0.001, A3 0.003, A2 0.008, A1 0.046, A0 0.001 (Table 2). Per-question-type final losses (5-digit): BA 0.021, MC1 0.001, US9 &lt;0.001 (Table 3), noting enriched training sampling for rare cases. Row-wise ablation: ablating rows 11–16 (the 6 computation rows) causes large loss increases (e.g., row 15 loss 1.181 vs background ≈0.07) indicating computation is concentrated there (Table 4). Head ablation: ablating the head responsible for BA increases average loss dramatically (~50×; example values: head ablation average loss 4.712 in Table 6 and other reported head-specific losses up to 3.7 in text depending on head numbering). Test-case failure statistics (from curated set) show systematic single-digit failures matching the per-row computation order (Table 5).",
            "error_types_or_failure_modes": "Primary failure mode is inability to reliably handle cascading 'sum-to-9' carry chains (US9 cascades) that require multi-column right-to-left carry propagation; the learned algorithm is effectively left-to-right/time-dense and only partially implements US9, so rare cascade cases (e.g., 445+555=1000 or long runs like 999999999+1) produce high loss/errors. Other observed issues: one high-value digit (highest-position digits like Digit 4 / A4/A5) can be implemented with a slightly different, faster/low-loss algorithm (model-specific asymmetry), some task-packing occurs when fewer heads are used (2-head case packs tasks into heads, 4-head case may split a logical task across heads), and the precise MLP memorization mapping is hypothesised but not fully proven via ablation.",
            "evidence_for_mechanism": "Multiple converging observations: (1) attention maps show a structured double-staircase where heads attend to corresponding digit pairs with a 3-token time-offset; (2) mean-ablation of residual stream per row demonstrates computation is performed only in a short window of 6 rows (rows 11–16) and each row primarily affects a single answer digit (Tables 4–5); (3) head-wise ablation causes large, task-specific loss increases (one head's ablation causes ~50× loss increase on BA questions), demonstrating head-specialization for BA/MC1/MS9; (4) per-digit/per-task training loss trajectories show phased discovery (BA discovered before UC1 before US9) and semi-independent digit training curves; (5) automated behavior-filtering and ablation localized nodes implementing BA/MC1/US9; (6) MLP capacity and behavior (consistent mapping of discrete head outputs to final digits) is consistent with key-value memory behavior reported in related work (Geva et al., 2021). Combined, these support a mechanistic decomposition: attention performs digit-pair information transfer, heads compute discrete small-state predicates/values, and the MLP performs a discrete lookup to emit the digit.",
            "counterexamples_or_challenges": "Clear counterexamples: cascading US9 cases and long carry chains (e.g., 999,999,999 + 1) where the model shows high loss and incorrect outputs—these expose the limits of the learned left-to-right, stateless per-digit computation. Ambiguities/inconsistencies: head numbering/identity across different analyses in the paper is not perfectly consistent (text and tables label the critical BA head differently), and the MLP's exact stored mappings are hypothesized but not experimentally proven; model variations (different random seeds / output formatting) reproduce the broad algorithm but with minor implementation differences, indicating the algorithm is robust but not identical across seeds. The MLP ablation and detailed causal role remain to be fully isolated experimentally (listed in Limitations).",
            "uuid": "e8369.0",
            "source_info": {
                "paper_title": "Understanding Addition in Transformers",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A mathematical framework for transformer circuits",
            "rating": 2
        },
        {
            "paper_title": "Transformer feed-forward layers are key-value memories",
            "rating": 2
        },
        {
            "paper_title": "Zoom in: An introduction to circuits",
            "rating": 2
        },
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability",
            "rating": 2
        },
        {
            "paper_title": "Locating and editing factual associations in gpt",
            "rating": 1
        }
    ],
    "cost": 0.01117575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Understanding Addition In Transformers</h1>
<p>Philip Quirke<br>Apart Research</p>
<p>Fazl Barez<br>Apart Research<br>University of Oxford</p>
<h4>Abstract</h4>
<p>Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper provides a comprehensive analysis of a one-layer Transformer model trained to perform $n$-digit integer addition. Our findings suggest that the model dissects the task into parallel streams dedicated to individual digits, employing varied algorithms tailored to different positions within the digits. Furthermore, we identify a rare scenario characterized by high loss, which we explain. By thoroughly elucidating the model's algorithm, we provide new insights into its functioning. These findings are validated through rigorous testing and mathematical modeling, thereby contributing to the broader fields of model understanding and interpretability. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.</p>
<h2>1 INTRODUCTION</h2>
<p>Understanding the underlying mechanisms of machine learning models is essential for ensuring their safety and reliability (Barez et al., 2023; Olah et al., 2020b; Doshi-Velez and Kim, 2017; Hendrycks and Mazeika, 2022). By unraveling the inner workings of these models, we can better understand their strengths, limitations, and potential failure modes, enabling us to develop more robust and trustworthy systems. Specifically, the sub-field of mechanistic interpretability within machine learning interpretability aims to dissect the behavior of individual neurons and their interconnections in neural networks (Räuker et al., 2022). Recent interpretability work has explored how transformers make predictions (Neo et al., 2024), analyzed reward model divergence in large language models (Marks et al., 2024), and highlighted the importance of such analyses for measuring value alignment (Barez and Torr, 2023). This pursuit is part of a larger endeavor to make the decision-making processes of complex machine learning models transparent and understandable.</p>
<p>Although models like Transformers have shown remarkable performance on a myriad of tasks, their complexity makes them challenging to interpret. Their multi-layered architecture and numerous parameters make it difficult to comprehend how they derive specific outputs (Vig, 2019). Further, while simple arithmetic tasks like integer addition may be trivial for humans, understanding how a machine learning model like a Transformer performs such an operation is far from straightforward (Liu and Low, 2023).</p>
<p>In this work, we offer an in-depth analysis of a one-layer Transformer model performing $n$-digit integer addition. We show that the model separates the addition task into independent digit-specific streams of work, which are computed in parallel. Different algorithms are employed for predicting the first, middle, and last digits of the answer. The model's behavior is influenced by the compact nature of the task and the specific format in which the question is presented. Despite having the opportunity to begin calculations early, the model actually starts later. The calculations are performed in a time-dense manner, enabling the model to add two 5-digit numbers to produce a 6-digit answer in just 6 steps (See Fig. 1). A rare use case with high loss was predicted by analysis and proved to exist via experimentation. Our findings shed light on understanding and interpreting transformers. These insights may also have implications for AI safety and alignment.</p>
<p>Our results demonstrate the transformer's unique approach applies to integer addition across various digit lengths (see Appendixes B and C). This transformer architecture, with its self-attention mechanism and ability to capture long-range dependencies, offers a powerful and flexible framework for</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>modeling sequential data. Our theoretical framework provides a mathematical justification for the model's behavior, substantiating our empirical observations and offering a foundation for future work in this domain.</p>
<p>Our main contributions are four-fold:</p>
<ul>
<li>Reformulation of the traditional mathematical rules of addition into a framework more applicable to Transformers.</li>
<li>Detailed explanation of the model's (low loss) implementation of the addition algorithm, including the problem and model constraints that informed the algorithm design.</li>
<li>Identification of a rare use case where the model is not safe to use (has high loss), and explanation of the root cause.</li>
<li>Demonstration of a successful approach to elucidating a model algorithm via rigorous analysis from first principles, detailed investigation of model training and prediction behaviours, with targeted experimentation, leading to deep understanding of the model.</li>
</ul>
<p>Below, we provide an overview of related work (§3), discuss our methodology (§4), describe our mathematical framework (§5), our analysis of model training (§6) and model predictions (§7). We conclude with a summary of our findings and directions for future research (§9).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the transformer model's attention pattern when adding two 5-digit integers. The model attends to digit pairs sequentially from left to right, resulting in a "double staircase" pattern across rows. A: The 5 digit question is revealed token by token. The " 10 s of thousands" digit is revealed first. B: From the " $=$ " token, the model attention heads focus on successive pairs of digits, giving a "double staircase" attention pattern. C: The 3 heads are time-offset from each other by 1 token such that, in each row, data from 3 tokens is available. D: To calculate A3, the 3 heads do independent simple calculations on D3, D2 and D1. The results are combined by the MLP layer using trigrams. A3 is calculated one token before it is needed. This approach applies to all answer digits, with the first and last digits using slight variations of the approach.</p>
<h1>2 BACKGROUND</h1>
<p>We focus on a single-layer transformer model with a vocabulary of size $V$ containing a set of symbols $\mathcal{V}$. The model converts input (e.g., a sequence of symbols from $\mathcal{V}$ ) into an input sequence $\left(\mathbf{x}<em p="p">{1}, \ldots, \mathbf{x}</em>}\right)$, where each $\mathbf{x<em e="e">{i} \in \mathbb{R}^{V}$ is a one-hot vector representing the corresponding symbol from $\mathcal{V}$. The input tokens are mapped to $d</em>$-dimensional embeddings by multiplying with an embedding</p>
<p>matrix $\mathbf{E} \in \mathbb{R}^{d_{o} \times V}$, where the $i$-th column of $\mathbf{E}$ represents the embedding of the $i$-th symbol in $\mathcal{V}$. The resulting sequence of embeddings is denoted as $\left(\mathbf{e}<em p="p">{1}, \ldots, \mathbf{e}</em>}\right)$, where $\mathbf{e<em i="i">{i}=\mathbf{E x}</em>$.} \in \mathbb{R}^{d_{o}}$. The model processes the input embeddings using a mechanism called "self-attention". Each input embedding $\mathbf{e}_{i}$ is passed through a self-attention mechanism that calculates weighted relationships between all input embeddings - capturing the importance of each embedding relative to others. The model then aggregates these weighted representations to produce contextually enriched representations for each embedding. The contextually enriched representations produced by the self-attention mechanism are subsequently fed through feedforward neural networks (i.e., multilayer perceptrons, MLPs) to refine their information. Finally, the output tokens are generated based on the refined representations and converted back to human-readable format using the vocabulary $\mathcal{V</p>
<h1>3 Related Work</h1>
<p>Interpreting and reverse engineering neural networks and transformers to find meaningful circuits has been an area of active research. Olah et al. (2020a) argued that by studying the connections between neurons and their weights, we can find meaningful algorithms (aka Circuits) in a "vision" neural network. Elhage et al. (2021) extended this approach to transformers, conceptualizing their operation in a mathematical framework that allows significant understanding of how transformer operate internally. Various tools (Foote et al., 2023; Conmy et al., 2023b; Garde et al., 2023) use this framework to semi-automate some aspects of reverse engineering. Nanda et al. (2023) reverseengineered modular addition (e.g. $5+7 \bmod 10=2$ ) showing the model used discrete Fourier transforms and trigonometric identities to convert modular addition to rotation about a circle.</p>
<p>Nanda and Lieberum (2022) have argued models comprise multiple circuits. They gave examples, including the distinct training loss curve per answer digit in 5-digit integer addition, but did not identify the underlying circuits. This work investigates and explains the circuits in n-digit integer addition.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: For 5-digit integer addition, these per-digit training loss curves show the model trains each answer digit semi-independently. The first answer digit A5 which is always 1 or 0 is learnt much more quickly than other digits.</p>
<p>Circuit analysis can extract graphical circuit representations and analyze component interactions (Bau et al., 2017). To enable analysis and interpretability, techniques in works like Petersen et al. (2021) symbolically reverse-engineer networks by recovering computational expressions. Research including Seth (2005) advocate analyzing networks causally, introducing structural causal models to infer mechanisms. Examinations of sequence models like Petersen et al. (2021) have analyzed the emergence and interaction of modular components during training. Lan and Barez (2024) analyzed and compared shared circuit subgraphs for related sequence continuation tasks in a large language model. Evolutionary perspectives such as Miikkulainen (2021) elucidate how selection pressures shape hierarchical representations. Lo et al. (2024) show how such representations, even if pruned, can quickly re-emerge in models after little re-training.</p>
<p>Information bottleneck analyses including Kawaguchi et al. (2023) relate bottlenecks to abstractions and modularization arising from forced compression. Surveys like Carbonneau et al. (2022) overview techniques to disentangle explanatory factors into separate latent dimensions. Novel objectives</p>
<p>proposed in works like Conmy et al. (2023a) improve interpretability by encouraging modularity and disentanglement.</p>
<h1>4 Methodology</h1>
<p>A successful integer addition model must cope with a very large question and answer space. For 5 digit addition, there are 10 billion distinct questions (e.g. "54321+77779=") and 200,000 possible answers. Just one token (the " $=$ ") after seeing the complete question, the model must predict the first answer digit. It must correctly predict all 6 answers digits.</p>
<p>Figure 3: We refer to individual tokens in a 5-digit addition question as D4, .. D0, and D'4, .., D'0 and the answer tokens as A5, .., A0.</p>
<p>Our model was trained on 1.8 million out of 10 billion questions. After training, the model predicts answers to questions with low loss, showing the model does not rely on memorisation of training data. Fig. 2 shows the model trains each digit semi-independently suggesting the model performs integer addition by breaking down the task into parallel digit-specific streams of computation.</p>
<p>The Transformer model algorithms often differ significantly from our initial expectations. The easiest addition process for humans, given the digits can be processed in any order, is to add the unit digits first before moving on to higher value digits. This autoregressive transformer model processes text from left to right, so the model predicts the higher value digits (e.g. thousands) of the answer before the lower value digits (e.g. units). It can't use the human process.</p>
<p>A key component of addition is the need to sum each digit in the first number with the corresponding digit in the second number. Transformer models contain "attention heads",the only computational sub-component of a model that can move information between positions (aka digits or tokens). Visualising which token(s) each attention head focused on in each row of the calculation provided insights. While our model works with 2, 3 or 4 attention heads, 3 attention heads give the most easily interpreted attention patterns. Fig. 4 shows the attention pattern for a single 5 digit addition calculation using 3 attention heads. Appendix C shows the same pattern for 10 and 15 digit addition. Appendix C shows the pattern with 2 or 4 attention heads.</p>
<p>While it's clear the model is calculating answer digits from highest value to lowest value, using the attention heads, it's not clear what calculation each attention head is doing, or how the attention heads are composed together to perform addition.</p>
<h2>5 Mathematical Framework</h2>
<p>To help investigate, we created a mathematical framework describing what any algorithm must do if it is to perform addition correctly. Our intuition is that the model a) incrementally discovers a necessary and sufficient set of addition sub-tasks (minimising complexity), b) discovers these sub-tasks semi-independently (maximising parallelism), and c) treats each digit semi-independently (more parallelism). Our framework reflects this.</p>
<p>To explain the framework, Let $D=\left(d_{n-1}, \ldots, d_{1}, d_{0}\right)$ and $D^{\prime}=\left(d_{n-1}^{\prime}, \ldots, d_{1}^{\prime}, d_{0}^{\prime}\right)$ be two $n$ digitof digits. We assert that the framework utilizes three base functions that operate on individual digit pairs. The first is Base Add ( aka BA ), which calculates the sum of two digits $D_{i}$ and $D_{i}^{\prime}$ modulo 10, ignoring any carry over from previous columns. The second is Make Carry 1 (aka MC1), which evaluates if adding digits $D_{i}$ and $D_{i}^{\prime}$ results in a carry over of 1 to the next column. The third is Make Sum 9 (aka MS9 ), which evaluates if $D_{i}+D_{i}^{\prime}=9$ exactly.</p>
<p>In addition, the framework uses two compound functions that chain operations across digits. The first is Use Carry 1 ( aka UC1 ), which takes the previous column's carry output and adds it to the sum of the current digit pair. The second is Use Sum 9 ( aka US9 ), which propagates (aka cascades) a carry</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: The attention pattern, for a model with 3 attention heads, performing a single 5 digit addition. The pattern is 18 by 18 squares (as $54321+77779=132100$ is 18 tokens). Time proceeds vertically downwards, with one additional token being revealed horizontally at each row, giving the overall triangle shape. After the question is fully revealed (at row 11), each head starts attending to pairs of question digits from left to right (i.e. high-value digits before lower-value digits) giving the "double staircase" shape. The three heads attend to a given digit pair in three different rows, giving a time ordering of heads.
over of 1 to the next column if the current column sums to 9 and the previous column generated a carry over. US9 is the most complex task as it spans three digits. For some rare questions (e.g. 00555 $+00445=01000$ ) US9 applies to up to four sequential digits, causing a chain effect, with the MC1 cascading through multiple digits. This cascade requires a time ordering of the US9 calculations from lower to higher digits.
These tasks occur in the training data with different, predictable frequencies (e.g. BA is common, US9 is rarer). Compound tasks are reliant on the base tasks and so discovered later in training. The discovery of each task reduces the model loss by a different, predictable amount (e.g. BA by $50 \%$, US9 by $5 \%$ ). Combining these facts give an expected order of task discovery during training as shown in Fig. 5. We use this mathematical framework solely for analysis to gain insights. The model training and all loss calculations are completely independent of this mathematical framework.</p>
<h1>6 Training Analysis</h1>
<p>Fig. 2 shows the model trains each digit semi-independently. Armed with the mathematical framework, we investigated each digit separately. The Digit 0 calculation is the least interesting as it only uses BA (not UC1 or US9 ). Once discovered, Digit 0 always quickly refines to have the lowest loss and least noise (as expected). (Graphs in Appendix B.)
For the other digits, we categorised the training data into 3 non-overlapping subsets aligned to the BA, UC1 and US9 tasks, and graphed various combinations, finding interesting results. The US9 graphs are much noisier than other graphs (Fig. 6). We found that the model has low loss on simple US9 cases (e.g. $45+55=100$ ) but has high loss on US9 cascades (e.g. $445+555=1000$ ) where the MC1 must be propagated "right to left" two 3 or 4 columns. The model can't perform these rare use cases safely, as it has a "left to right" algorithm.
Graphing the BA and UC1 use cases side by side for any one of the Digits 1, 2 and 3 shows an interesting pattern (Fig. 7). In Phase 1, both tasks have the same (high) loss. In Phase 2, both curves drop quickly but the BA curve drops faster than the UC1 curve. This "time lag" matches our expectation that the BA task must be accurate before the UC1 task can be accurate. In Phase 3, both tasks' loss curve decrease slowly over time.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: The mathematical framework (our method) predicts that during training, tasks are learnt for each digit independently, progressively increasing per digit accuracy (i.e. decreasing loss) shown as percentages. Mathematical rules cause dependencies between digits, giving an predicted ordering for perfect (i.e. zero loss) addition. The chain of blue squares relate to questions like $99999+00001=$ 100000 where the MC1 in digit 0 causes US9 cascades through multiple other digits.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: High variability in the per digit training loss for US9 cases caused by the model's inability to reliably do cascading US9 cases such as $445+555=1000$.</p>
<p>Both the BA and UC1 tasks need to move data between tokens, and so will be implemented in attention head(s). Fig. 7 shows they are trained semi-independently. We choose the number of attention heads in our model with the clearest separation of tasks in the attention pattern. We find (later) that our model has separate attention heads for the BA and UC1 tasks. Digit 4, the highest question digit, has a significantly different loss curve (shown in Fig. 8) than Digits 1, 2 and 3. This is partially explained by Digit 4 only having simple use cases (i.e. no US9 cascades). This does not explain the BA or UC1 differences. This difference persists with different seed values, and with 10 or 15 digit addition. We explain this difference later.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Training loss for digit 3 showing that, in Phase 2, the refining of Use Carry 1 lags behind Base Add. Base Add and Use Carry 1 are refined separately and have separate calculation algorithms. The 3 phases seem to correspond to "memorisation", "algorithm discovery" and "cleanup".
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Training loss for digit 4 starts and stays lower for all tasks than it does for digits 1, 2 and 3. Digit 4 has a different calculation algorithm from digits 1, 2 and 3.</p>
<h1>7 Prediction Analysis</h1>
<p>Using the ablation interventions technique we overrode (mean ablated) the model memory (residual stream) at each row (aka token position) and confirmed that the addition algorithm does not use any data generated in rows 0 to 10 inclusive. In these rows the model has not yet seen the full question and every digit in the question is independent of every other digit, making accurate answer prediction infeasible. The model also does not use the last (17th) row. Therefore, the addition answer calculation is started and completed in 6 rows ( 11 to 16). Further ablation experiments confirmed that the A0 to A4 answers are calculated one row before being revealed. (Details in Appendix H.)
The model has slightly different algorithms for the first answer digits (A5 and A4), the middle answer digits (A3 and A2) and the last answer digits (A1 and A0). Fig. 1 has a simplified version of how the model calculates the answer digit A3. Fig. 9 has more details. For 5 digit addition, there are 2 middle answer digits (A3 and A2) whereas for 15 digit addition there are 12 middle answer digits that use this algorithm.</p>
<p>The A3 addition algorithm has three clauses related to digits 3, 2 and 1. Ablating each head in turn shows that the 3rd head has most impact on loss, the 2nd head has less impact, and the 1st head has little impact. This aligns with the intuition that the sum "D3 + D'3" matters most, the MC1 from the previous digit ( $\mathrm{D} 2+\mathrm{D}^{\prime} 2$ ) matters less, and the rare MC1 from the previous previous digit ( $\mathrm{D} 1+\mathrm{D}^{\prime} 1$ ) matters least. The last two answer digits, A1 and A0, use a simplified a version of the A3 algorithm, as some clauses are not necessary.
The A3 algorithm can also be applied to A4. But the Digit 4 training curve is better (faster) than the middle digits. The attention patterns show that for A4, the model is using all the heads in row 11 (the " $=$ " input token) when the A3 algorithm doesn't require this. Uniquely, A4 utilises more "compute" than is available to A3, A2, A1 or A0. We assume the model uses this advantage to implement a faster-training and lower-loss algorithm for A5 and A4. We haven't worked out the details of this.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: A: To predict answer digit A3, the addition algorithm must combine information from digits 3, 2 and 1. B: The 1st head calculates MC1 on digit 1. C: The 2nd head calculates MC1 and MS9 (at most one of which can be true) on digit 2. D: The 3rd head calculates Base Add on digit 3. E: The MLP layer uses trigrams to combine the information from the 3 heads to give the final answer A3, one row before it is output. Appendix G shows this algorithm as pseudocode.</p>
<p>Mean ablating the 1st or 2nd head slightly increased the average loss for BA questions from 0.05 to 0.08 , whereas ablating the 3rd head substantially increased the loss to 3.7 , confirming that the 3rd head is doing the BA task. (Details in Appendix H.)</p>
<p>The MLP can be thought of as a "key-value pair" memory (Meng et al., 2022; Geva et al., 2021) that can hold many bigrams and trigrams. We claim our MLP pulls together the two-state 1st head result, the tri-state 2nd head result and the ten-state 3rd head result value, treating them as a trigram with 60 ( $2 \times 3 \times 10$ ) possible keys. For each digit, the MLP has memorised the mapping of these 60 keys to the 60 correct digit answers ( 0 to 9 ). We haven't proven this experimentally. Our MLP is sufficiently large to store this many mappings with zero interference between mappings (Elhage et al., 2022).</p>
<p>Despite being feasible, the model does not calculate the task MC1 in rows 7 to 11. Instead, it completes each answer digit calculation in 1 row, possibly because there are training optimisation benefits in generating a "compact" algorithm.</p>
<p>This algorithm explains all the observed prediction behaviour - including the fact that the model can calculate a simple US9 case but not a cascading US9 case. We assume that, given the dense nature of the question and answer, and the small model size, the model does not have sufficient time and compute resources to implement both UC1 and US9 accurately, and so preferences implementing the more common ( UC1 ) case, and only partially implements the more complex and rare (US9 ) case.</p>
<h1>8 Algorithm Reuse</h1>
<p>We explored whether the above algorithm is learned by similar models. We trained a separate 1-layer model for 5-digit addition with a different random seed and optimization algorithm. The new model's answer format requires it to predict " + " as the first answer token, e.g., "12345+67890=+080235". Despite these changes, Figures 24, 25, and 26 show that the new model's behavior has many similarities to the previous model.</p>
<p>Intervention ablation demonstrates that the new model uses the BaseAdd, MakeCarry, and MakeSum sub-tasks in the same way as the previous model. The new model exhibits the same</p>
<p>strengths and weaknesses, e.g., it can calculate a simple MS case but not a cascading MS case. We claim that the new and previous models use essentially the same algorithm, with minor variations.</p>
<p>Our analysis suggests that the transformer architecture, when trained on the addition task, converges to a consistent algorithm for performing digit-wise addition. This algorithm leverages the self-attention mechanism to discover and execute the necessary sub-tasks in a parallel and semi-independent manner. Despite differences in random initialization, optimization algorithms, and output formatting, the models exhibit similar internal behavior and capabilities, indicating a robust algorithmic solution emerges from the transformer's architecture and training process.</p>
<h1>9 CONCLUSIONS</h1>
<p>This work demonstrates a successful approach to reverse engineering and elucidating the emergent algorithm within a transformer model trained on integer addition. By combining mathematical analysis, empirical investigation of training and prediction, and targeted experimentation, we are able to explain how the model divides the task into parallel digit-specific streams, employs distinct subroutines for different digit positions, postpones calculations until the last possible moment yet executes them rapidly, and struggles with a specific rare case.</p>
<p>Our theoretical framework of necessary addition subtasks provides a foundation for the model's behavior. The digit-wise training loss curves reveal independent refinement consistent with separate digit-specific circuits. Attention patterns illustrate staging and time-ordering of operations. Controlled ablation experiments validate our hypothesis about algorithmic elements' roles. Together these methods enable a detailed accounting of the model's addition procedure.</p>
<p>This methodology for mechanistic interpretability, when applied to broader tasks and larger models, can offer insights into not just what computations occur inside complex neural networks, but how and why those computations arise. Such elucidation will be increasingly important for ensuring the safety, reliability and transparency of AI systems.</p>
<h2>10 LIMITATIONS AND Future Work</h2>
<p>One concrete limitation of our current model is its difficulty handling the rare case of adding two 9-digit numbers that sum to 1 billion (e.g. $999,999,999+1$ ). Further investigation is needed to understand why this specific edge case poses a challenge and to develop strategies to improve the model's performance, such as targeted data augmentation or architectural modifications.</p>
<p>Regarding the MLP component, a detailed ablation study could elucidate its precise role and contributions. Systematically removing or retraining this component while monitoring performance changes could shed light on whether it is essential for the overall algorithm or plays a more auxiliary part. For future work, a natural next step is to apply our framework to reverse-engineer integer subtraction models. Subtraction shares some commonalities with addition but also introduces new complexes like borrowing. Extending our approach to handle such nuances would demonstrate its generalisability.</p>
<p>For multiplication, an interesting avenue is to first pre-train a large transformer model on addition using our verified modules as a starting component. Then, expand the model's capacity and fine-tune on multiplication data. This could facilitate more rapid acquisition of the multiplication algorithm by providing a strong inductive bias grounded in robust addition skills. Specific experiments could then identify the emergence of new multiplication-specific subroutines and their integration with the addition circuits.</p>
<p>Furthermore, domains like symbolic AI, program synthesis, or general reasoning may benefit from embedding multiple specialized algorithmic components like our addition circuit within larger language models. Insights from our work could guide the controlled emergence and combination of diverse task-specific capabilities. Overall, while making models more interpretable is valuable, the ultimate aim is developing safer, more reliable, and more controllable AI systems. Our work highlight one path toward that goal through understanding of addition in neural network computations.</p>
<h1>11 REPRODUCIbILITY STATEMENT</h1>
<p>To facilitate the reproduction of our empirical results on understanding and interpreting addition in one-layer transformers, and further studying the properties of more complex transformers on more complex tasks that would build on a single layer, we release all our code and resources used in this work. Furthermore, we offer explicit constructions of one layer transformers used in this paper.</p>
<h2>12 ACKNOWLEDGEMENTS</h2>
<p>We are thankful to Jason Hoelscher-Obermaier for his comments on the earlier draft, Esben Kran for organising the Interpretability Hackathon and Clement Neo for his comments on the paper and assistance with some of the figures. This project was supported by Apart Lab.</p>
<h2>REFERENCES</h2>
<p>F. Barez and P. Torr. Measuring value alignment, 2023.
F. Barez, H. Hasanbieg, and A. Abbate. System iii: Learning with domain knowledge for safety constraints, 2023.
D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba. Network dissection: Quantifying interpretability of deep visual representations. Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3319-3327, 2017.
M.-A. Carbonneau, J. Zaidi, J. Boilard, and G. Gagnon. Measuring disentanglement: A review of metrics, 2022.
A. Conmy, A. Mavor-Parker, A. Lynch, S. Heimersheim, and A. Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability, 04 2023a.
A. Conmy, A. N. Mavor-Parker, A. Lynch, S. Heimersheim, and A. Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability, 2023b.
F. Doshi-Velez and B. Kim. Towards a rigorous science of interpretable machine learning, 2017.
N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. A mathematical framework for transformer circuits. https://transformer-circuits. pub/2021/framework/index.html, 2021.
N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds, R. Lasenby, D. Drain, C. Chen, R. Grosse, S. McCandlish, J. Kaplan, D. Amodei, M. Wattenberg, and C. Olah. Toy models of superposition, 2022.
A. Foote, N. Nanda, E. Kran, I. Konstas, S. Cohen, and F. Barez. Neuron to graph: Interpreting language model neurons at scale, 2023.
A. Garde, E. Kran, and F. Barez. Deepdecipher: Accessing and investigating neuron activation in large language models, 2023.
M. Geva, R. Schuster, J. Berant, and O. Levy. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484-5495, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.446. URL https://aclanthology.org/2021.emnlp-main. 446.
D. Hendrycks and M. Mazeika. X-risk analysis for ai research, 2022.
K. Kawaguchi, Z. Deng, X. Ji, and J. Huang. How does information bottleneck help deep learning?, 2023.</p>
<p>M. Lan and F. Barez. Interpreting shared circuits for ordered sequence prediction in a large language model, 2024.
T. Liu and B. K. H. Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks, 2023.
M. Lo, S. B. Cohen, and F. Barez. Large language models relearn removed concepts, 2024.
L. Marks, A. Abdullah, C. Neo, R. Arike, P. Torr, and F. Barez. Beyond training objectives: Interpreting reward model divergence in large language models, 2024.
K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in gpt. https://proceedings.neurips.cc/paper_files/paper/2022/file/ 6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf, 2022.
R. Miikkulainen. Creative ai through evolutionary computation: Principles and examples. https: //doi.org/10.1007/s42979-021-00540-9, 2021.
N. Nanda and T. Lieberum. Mechanistic interpretability analysis of grokking. https://www. alignmentforum.org/posts/N6WM6hs7RQMKDhYjB, 2022.
N. Nanda, L. Chan, T. Lieberum, J. Smith, and J. Steinhardt. Progress measures for grokking via mechanistic interpretability, 2023.
C. Neo, S. B. Cohen, and F. Barez. Interpreting context look-ups in transformers: Investigating attention-mlp interactions, 2024.
C. Olah, N. Cammarata, L. Schubert, G. Goh, M. Petrov, and S. Carter. Zoom in: An introduction to circuits. https://distill.pub/2020/circuits/zoom-in/, 2020a.
C. Olah, N. Cammarata, L. Schubert, G. Goh, M. Petrov, and S. Carter. Zoom in: An introduction to circuits. Distill, 5(3):e00024.001, 2020b.
A. Petersen, P. Voigtlaender, and A. Krause. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. Advances in Neural Information Processing Systems, 34, 2021.
T. Räuker, A. Ho, S. Casper, and D. Hadfield-Menell. Toward transparent ai: A survey on interpreting the inner structures of deep neural networks. arXiv preprint arXiv:2207.13243, 2022.
A. K. Seth. Causal connectivity of evolved neural networks during behavior. Network: Computation in Neural Systems, 16(1):35-54, 2005. doi: 10.1080/09548980500238756. URL https://doi . org/10.1080/09548980500238756.
J. Vig. A multiscale visualization of attention in the transformer model. arXiv preprint arXiv:1906.05714, 2019.</p>
<h1>A Appendix - Number of Attention Heads</h1>
<p>The model can be successfully trained with 2, 3 or 4 attention heads (Refer Figs. 10, 11, 12). However as described in Fig. 11, the 3 attention heads case is easier to interpret than the 2 or 4 heads cases.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: For 5 digit addition, using 2 attention heads works, but the model attends to multiple token pairs in a single head, suggesting that multiple tasks are being packed into a single head, which makes it harder to interpret.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: For 5 digit addition, using 3 attention heads gives the best separation with the heads having distinct, non-overlapping attention pattern. Ablating any head increases the model loss, showing all 3 heads are useful.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: For 5 digit addition, using 4 attention heads gives an attention pattern where the H1 and H2 staircases overlap perfectly. Ablating one of H1 or H2 increases loss. The similarity in H1 and H2's attention patterns suggests they are "splitting" a single logical task. Spliting is feasible as Elhage et al. (2021) says attention heads are independent and additive.</p>
<h1>B APPENDIX - TRAINING LoSS BY NuMBER OF DIGITS</h1>
<p>The model can be successfully trained with 5, 10, 15, etc digits (Refer Figs. 13, 14, 15). For a given loss threshold, 15 digit addition takes longer to train than 10 digit addition, which takes longer to train than 5 digit addition.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: The per-digit loss curves over 1500 training epochs for $\mathbf{5}$ digit addition.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14: The per-digit loss curves over 1500 training epochs for $\mathbf{1 0}$ digit addition.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 15: The per-digit loss curves over 1500 training epochs for $\mathbf{1 5}$ digit addition.</p>
<h2>C APPENDIX - AtTENTION PATtERns by NuMBER of Digits</h2>
<p>The model can be successfully trained with 5, 10, 15, etc digits (Refer Figs. 16, 17, 18). The attention patterns for these cases show similarities that aid interpretation.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 16: For $\mathbf{5}$ digit addition, and 3 attention heads, the attention pattern has a strong doublestaircase shape. Each step of the staircase is 3 blocks wide, showing the attention heads are attending to different input tokens in each row.
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 17: For $\mathbf{1 0}$ digit addition, and 3 attention heads, the attention pattern has a strong doublestaircase shape. Each step of the staircase is 3 blocks wide, duplicating the Fig. 16 pattern.
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 18: For $\mathbf{1 5}$ digit addition, and 3 attention heads, the attention pattern has a strong doublestaircase shape. Each step of the staircase is 3 blocks wide, duplicating the Fig. 16 pattern.</p>
<h1>D APPENDIX - MODEL CONFIGURATION</h1>
<p>A Colab notebook was used for experimentation:</p>
<ul>
<li>
<p>It runs on a T4 GPU with each experiment taking a few minutes to run.</p>
</li>
<li>
<p>The key parameters (which can all be altered) are:</p>
</li>
<li>
<p>n_layers $=1$; This is a one layer Transformer</p>
</li>
<li>n_heads $=3$; There are 3 attention heads</li>
<li>
<p>n_digits $=5$; Number of digits in the addition question</p>
</li>
<li>
<p>It uses a new batch of data each training step (aka Infinite Training Data) to minimise memorisation.</p>
</li>
<li>During a training run the model processes about 1.5 million training datums. For the 5 digit addition problem there are 100,000 squared (that is 10 billion) possible questions. So, the training data is much less than $1 \%$ of the possible questions.</li>
<li>US9 cascades (e.g. $44445+55555=100000,54321+45679=1000000,44450+55550=10000$, $1234+8769=10003$ ) are exceedingly rare. To speed up training, the data generator was enhanced to increase the frequency of these cases in the training data.</li>
</ul>
<h1>E APPENDIX - SOLVING A JIGSAW USING MANY PEOPLE</h1>
<p>Understanding transformer algorithm implementation requires new ways of thinking.
Here is a useful analogy for changing how we approach a problem to more closely resemble how a Transformer would; a jigsaw puzzle. A single person could solve it using a combination of meta knowledge of the problem (placing edge pieces first), categorisation of resources (putting like-coloured pieces into piles), and an understanding of the expected outcome (looking at the picture on the box).</p>
<p>But if instead we had one person for each piece in the puzzle, who only knew their piece, and could only place it once another piece that it fit had already been placed, but couldn't talk to the other people, and did not know the expected overall picture, the strategy for solving the jigsaw changes dramatically.</p>
<p>When they start solving the jigsaw, the 4 people holding corner pieces place them. Then 8 people holding corner-adjacent edge pieces can place them. The process continues, until the last piece is placed near the middle of the jigsaw.</p>
<p>We posit that this approach parallels how transformer models work. There is no pre-agreed overall strategy or communication or co-ordination between people (circuits) - just some "rules of the game" to obey. The people think independently and take actions in parallel. The tasks are implicitly time ordered by the game rules.</p>
<h2>F APPENDIX - Use Sum 9 Training Loss Graph</h2>
<p>Fig. 19 shows a training loss curve for just BA and UC1 tasks. The model has high loss on Use Sum 9 cascades shown as high variability in the loss graph (See Fig. 20).
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 19: Training loss for digit 3 showing that, in Phase 2, the refining of Use Carry 1 (red line) lags Base Add (blue line). This supports the claim that Base Add and Use Carry 1 are learnt and refined separately and perform different calculation tasks.</p>
<p><img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 20: Training loss for digit 3 for the Base Add, Use Carry 1 and Use Sum 9 tasks, showing the model struggles to reduce loss on Use Sum 9 (green line) compared to Base Add and Use Carry 1 .</p>
<h1>G Appendix - Model Algorithm as Pseudocode</h1>
<p>The Algorithm 1 table below reproduces the model's addition algorithm summarised in Fig. 9 as pseudocode. This code calculates the largest digit first, handling simple (but not cascading) UseSum9 cases, and returns the answer.</p>
<p>Note that the pseudocode does not retain any information between passes through the for loop - this corresponds to each digit being calculated independently of the other digits.</p>
<h2>H APPENDIX - The Loss function and loss MEASURes</h2>
<p>The loss function is simple:</p>
<ul>
<li>Per Digit Loss: For "per digit" graphs and analysis, for a given answer digit, the loss used is negative log likelihood.</li>
<li>All Digits Loss: For "all answer digits" graphs and analysis, the loss used is the mean of the "per digit" loss across all the answer digits.</li>
</ul>
<p>The final training loss varies with the number of digits in the question as shown in Tab. 1. The final training loss for each digit in the question varies as shown in Tab. 2.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Size of question</th>
<th style="text-align: center;">Final training loss</th>
<th style="text-align: left;">Example question</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">5 digit addition</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: left;">$11111+22222=033333$</td>
</tr>
<tr>
<td style="text-align: center;">10 digit addition</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: left;">$1111111111+2222222222=0333333333$</td>
</tr>
<tr>
<td style="text-align: center;">15 digit addition</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: left;">$\begin{aligned} &amp; 111111111111111+22222222222222 \ &amp; 033333333333333 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Overall Average</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: left;">General Performance</td>
</tr>
</tbody>
</table>
<p>Table 1: The final training loss after 5000 training epochs, each containing 64 questions, using All Digits Loss, for different size addition models.</p>
<p>We categorize each training question by which calculations its need to get the correct answer:</p>
<ul>
<li>BA : BaseAdd questions only need "base add" calculations.</li>
<li>MC1 : MakeCarry1 questions need both "use carry 1" and "base add" calculations.</li>
<li>US9 : UseSum9 questions need "use sum 9", "use carry 1" and "base add" calculations.</li>
</ul>
<p>We measured the loss per question category as shown in Tab. 3. The frequency of these question types differ significantly ( $B A=61 \%, \mathrm{MC} 1=33 \%, \mathrm{US} 9=6 \%$ ) in the enriched training data so the final training loss values, at this level of detail, are not very informative.</p>
<p>After training, we used the model to give answers to questions. To understand whether the calculations at position n are important, we look at the impact on loss of ablating all attention heads at that token</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 n-digit integer addition algorithm
    function CALCULATEANSWERDIGITS( \(n\) digits, \(q 1, q 2\) )
        answer \(\leftarrow 0 \quad \triangleright\) Initialize the answer to zero
        for all \(i=0, \ldots, n\) digits -1 do
            \(\operatorname{pos} \leftarrow n\) digits \(-i-1 \quad \triangleright\) Loop over each digit
            \(\operatorname{prev} \operatorname{pos} \leftarrow \operatorname{pos}-1 \quad \triangleright\) Current position from the right
            prev \(\operatorname{prev} \operatorname{pos} \leftarrow \operatorname{pos}-2 \quad \triangleright\) Previous position from the right
            \(\operatorname{rev}\) prev \(\leftarrow \operatorname{pos}-2 \quad \triangleright\) Postion before the previous
            \(\operatorname{mcl}\) prev \(\operatorname{prev} \leftarrow 0 \quad \triangleright\) Start calculation of carry from two positions before
            if \(\operatorname{prev}\) prev pos \(\geq 0\) then
                if \(q 1_{\text {prev } \operatorname{prev} \text { pos }}+q 2_{\text {prev } \operatorname{prev} \text { pos }} \geq 10\) then
                    \(\operatorname{mcl}\) prev \(\leftarrow 1 \quad \triangleright\) Carry from two positions before found
                    end if
            end if
            \(\operatorname{mcl}\) prev \(\leftarrow 0 \quad \triangleright\) Start calculation of carry from previous position
            if \(\operatorname{prev}\) pos \(\geq 0\) then
                if \(q 1_{\text {prev pos }}+q 2_{\text {prev pos }} \geq 10\) then
                    \(\operatorname{mcl}\) prev \(\leftarrow 1 \quad \triangleright\) Carry from the previous position found
                end if
            end if
            ms9 prev \(\leftarrow 0 \quad \triangleright\) Start calculation of sum of 9 in previous position
            if \(\operatorname{prev}\) pos \(\geq 0\) then
                if \(q 1_{\text {prev pos }}+q 2_{\text {prev pos }}==9\) then
                    ms9 prev \(\leftarrow 1 \quad \triangleright\) Sum of 9 in previous position found
                end if
            end if
            prev \(\operatorname{prev} \leftarrow 0 \quad \triangleright\) Start calculation if carry from two positions before is needed
            if \(\operatorname{mcl}\) prev \(==0\) then
                if ms9 prev \(==1\) then
                    if \(\operatorname{mcl}\) prev \(\leftarrow 1\) then
                    \(\operatorname{prev}\) prev \(\leftarrow 1 \quad \triangleright\) Carry from two positions before is needed
                    end if
                    end if
            end if
            digitanswer \(\leftarrow q 1_{\text {pos }}+q 2_{\text {pos }}+\operatorname{mcl}\) prev + prev prev \(\triangleright\) Calculate answer for current digit
            digitanswer \(\leftarrow\) MODULUS(digitanswer, 10) \(\triangleright\) Correct the current digit if it&#39;s \(\geq 10\)
            answer \(\leftarrow\) digitanswer + answer \(\times 10 \quad \triangleright\) Concatenate the current digit to the answer
            end for
            return answer \(\quad \triangleright\) Return the final calculated answer
        end function
</code></pre></div>

<p>(using the TransformerLens framework, zero ablation of the blocks.0.hook_resid_post data set, the above loss function, and a "cut off" loss threshold of 0.08 ). Tab. 4 shows sample results. The results shows that all important calculations are completed at just 6 of the 18 (question and answer) token positions.
For deeper investigation, we created 100+ hand-curated test questions. They cover all question types (BA, MC1 and MS9) and all the answer digits (A5 .. A0) that these question types can occur in. These test cases were not used in model training. Example test questions include:</p>
<ul>
<li>make_a_question(_, , 888, 11111, BASE_ADD_CASE)</li>
<li>make_a_question(_, ,, 35000, 35000, USE_CARRY_1_CASE)</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">Digit index</th>
<th style="text-align: center;">Training loss</th>
<th style="text-align: center;">AKA</th>
<th style="text-align: center;">Example of digit</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">A5</td>
<td style="text-align: center;">$11111+22222=033333$</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">A4</td>
<td style="text-align: center;">$11111+22222=033333$</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">A3</td>
<td style="text-align: center;">$11111+22222=033333$</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.008</td>
<td style="text-align: center;">A2</td>
<td style="text-align: center;">$11111+22222=033333$</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.046</td>
<td style="text-align: center;">A1</td>
<td style="text-align: center;">$11111+22222=033333$</td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">A0</td>
<td style="text-align: center;">$11111+22222=033333$</td>
</tr>
<tr>
<td style="text-align: center;">Overall Average</td>
<td style="text-align: center;">$\mathbf{0 . 0 1 0}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: For 5-digit addition, final training loss per digit. The more digits, the longer the model needs to train to a given level of loss.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question type</th>
<th style="text-align: center;">Training loss</th>
<th style="text-align: center;">Aka</th>
<th style="text-align: center;">Example of digit</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BA</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">Base Add</td>
<td style="text-align: center;">$11111+22222=033333$</td>
</tr>
<tr>
<td style="text-align: center;">MC1</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">Make Carry 1</td>
<td style="text-align: center;">$11811+22222=034033$</td>
</tr>
<tr>
<td style="text-align: center;">US9</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">Use Sum 9</td>
<td style="text-align: center;">$17811+22222=040033$</td>
</tr>
<tr>
<td style="text-align: center;">Overall Average</td>
<td style="text-align: center;">$\mathbf{0 . 0 1 1}$</td>
<td style="text-align: center;">General Performance</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: For 5 digit addition, the final training loss per question category.</p>
<ul>
<li>make_a_question( <em>, </em>, 15020, 45091, USE_CARRY_1_CASE)</li>
<li>make_a_question(<em>, </em>, 25, 79, SIMPLE_US9_CASE)</li>
<li>make_a_question(<em>, </em>, 41127, 10880, SIMPLE_US9_CASE)</li>
<li>make_a_question(<em>, </em>, 123, 877, CASCADE_US9_CASE)</li>
<li>make_a_question(<em>, </em>, 81818, 18182, CASCADE_US9_CASE)</li>
</ul>
<p>The above experiment was repeated (with "cut off" loss threshold of 0.1 ) using these $\overline{1} 00$ questions (and another 64 random questions). We analyzed the incorrect answers, grouping these failures by which answer digits were wrong. Tab. 5 shows sample results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Row</th>
<th style="text-align: left;">Number of incorrect answers, grouped by incorrect digits</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: left;">Nyyyyy: 47, NyNyyy: 5, yNyyyy: 7, yyNyyy: 7, yyyNyy: 1, ...</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: left;">yNyyyy: 97, yNNyyy: 7, NNyyyy: 3, Nyyyyy: 2, yyyNyy: 1, ...</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: left;">yyNyyy: 85, NyNyyy: 3, yNNyyy: 2, Nyyyyy: 3, yNyyyy: 5, ...</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: left;">yyyNyy: 72, yyNNyy: 4, NyyNyy: 3, yNyNyy: 3, Nyyyyy: 2, ...</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: left;">yyyyNy: 74, yyNyNy: 3, Nyyyyy: 2, NyyyNy: 3, yNyyNy: 3, ...</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: left;">yyyyyN: 82, yyNyyN: 2, NyyyyN: 4, yNyyyN: 4, yyNyyy: 9, ...</td>
</tr>
</tbody>
</table>
<p>Table 5: For 5 digit addition, ablating all the attention heads in each row (aka step) impacts answer correctness in a regular pattern. Here, an ' N ' means that answer digit was incorrect in the predicted answer. In each step, the failure count for the top grouping is $&gt;6$ times the next most common grouping - evidence that each step calculates one answer digit. From row 11, each row mainly impacts one answer digit - in the same order as the model predicts answer digits.</p>
<p>We also ablated one head at a time (using the TransformerLens framework, mean ablation of the blocks.0.attn.hook_z data set, the standard loss function, and a "cut off" loss threshold of 0.08 ) to understand which head(s) were key in calculating BA questions. Tab. 6 shows sample results.
The last row of Tab. 6 shows that digit A0 was calculated in step 16. This is one step before the model reveals A0 in step 17. The other rows in the table shows that this pattern holds true for all the other digits: each digit is calculated one step before the model reveals it.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Token</th>
<th style="text-align: center;">Average loss</th>
<th style="text-align: center;">Conclusion</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0 .. 10</td>
<td style="text-align: center;">0.070</td>
<td style="text-align: center;">Impact is low. Calcs for these 11 tokens are unimportant</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">Loss is 4 x threshold. Calculations are important</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0.497</td>
<td style="text-align: center;">Loss is 6 x threshold. Calculations are important</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;">Loss is 7 x threshold. Calculations are important</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0.921</td>
<td style="text-align: center;">Loss is 11 x threshold. Calculations are important</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">1.181</td>
<td style="text-align: center;">Loss is 14 x threshold. Calculations are important</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">1.021</td>
<td style="text-align: center;">Loss is 12 x threshold. Calculations are important</td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">0.070</td>
<td style="text-align: center;">Impact is low. Calculations for this token is unimportant</td>
</tr>
</tbody>
</table>
<p>Table 4: For 5 digit addition, for the 18 tokens / calculation rows, how ablating each token/row impacts the calculation loss.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Ablated Head</th>
<th style="text-align: center;">Average loss</th>
<th style="text-align: left;">Conclusion</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.016</td>
<td style="text-align: left;">Impact is low. Head is unimportant for BA</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4.712</td>
<td style="text-align: left;">Loss is 50 x threshold. Head is important for BA</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.062</td>
<td style="text-align: left;">Impact is low. Head is unimportant for BA</td>
</tr>
</tbody>
</table>
<p>Table 6: For 5 digit addition, when ablating heads, Head 1 is clearly key for the calculation of Base Add questions.</p>
<h1>I APPENDIX - AlGORITHM RE-USE</h1>
<p>We explored whether the algorithm is re-used in other models by training a separate 1-layer 5-digit addition model with a different seed, optimiser. It also required the model to predict " + " as the first answer token as shown in Fig. 3.</p>
<p>$$
\left(\begin{array}{ccccc}
3 &amp; 3 &amp; 3 &amp; 5 &amp; 7 \
D 4 &amp; D 3 &amp; D 2 &amp; D 1 &amp; D 0
\end{array}\right)+\left(\begin{array}{ccccc}
8 &amp; 2 &amp; 2 &amp; 4 &amp; 3 \
D^{\prime} 4 &amp; D^{\prime} 3 &amp; D^{\prime} 2 &amp; D^{\prime} 1 &amp; D^{\prime} 0
\end{array}\right)=\left(\begin{array}{ccccc}
+ &amp; 1 &amp; 1 &amp; 5 &amp; 6 &amp; 0 &amp; 0 \
A 6 &amp; A 5 &amp; A 4 &amp; A 3 &amp; A 2 &amp; A 1 &amp; A 0
\end{array}\right)
$$</p>
<p>Figure 21: We refer to individual tokens in a 5-digit addition question as D4, .. D0, and D'4, .., D'0 and the answer tokens as A6, .., A0. Note that A6 is always the " + " token.</p>
<p>Figs. 24, 25 and 23 show aspects of the behavior of this model. Fig. 26 shows the nodes in the model where the node purpose can be identified by automated search using behavior filtering and ablation intervention testing.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">From</th>
<th style="text-align: center;">P0</th>
<th style="text-align: center;">P1</th>
<th style="text-align: center;">P2</th>
<th style="text-align: center;">P3</th>
<th style="text-align: center;">P4</th>
<th style="text-align: center;">P5</th>
<th style="text-align: center;">P6</th>
<th style="text-align: center;">P7</th>
<th style="text-align: center;">P8</th>
<th style="text-align: center;">P9</th>
<th style="text-align: center;">P10</th>
<th style="text-align: center;">P11</th>
<th style="text-align: center;">P12</th>
<th style="text-align: center;">P13</th>
<th style="text-align: center;">P14</th>
<th style="text-align: center;">P15</th>
<th style="text-align: center;">P16</th>
<th style="text-align: center;">P17</th>
<th style="text-align: center;">P18</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Poor</td>
<td style="text-align: center;">D4</td>
<td style="text-align: center;">D3</td>
<td style="text-align: center;">D2</td>
<td style="text-align: center;">D1</td>
<td style="text-align: center;">D0</td>
<td style="text-align: center;">+</td>
<td style="text-align: center;">D'4</td>
<td style="text-align: center;">D'3</td>
<td style="text-align: center;">D'2</td>
<td style="text-align: center;">D'1</td>
<td style="text-align: center;">D'0</td>
<td style="text-align: center;">$=$</td>
<td style="text-align: center;">A6</td>
<td style="text-align: center;">A5</td>
<td style="text-align: center;">A4</td>
<td style="text-align: center;">A3</td>
<td style="text-align: center;">A2</td>
<td style="text-align: center;">A1</td>
<td style="text-align: center;">A0</td>
</tr>
<tr>
<td style="text-align: center;"># fails</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">467</td>
<td style="text-align: center;">804</td>
<td style="text-align: center;">765</td>
<td style="text-align: center;">775</td>
<td style="text-align: center;">724</td>
<td style="text-align: center;">582</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 22: This shows the token positions where questions fail when we ablate each node in the 5-digit 1-layer 3-head addition model. The model only uses nodes in token positions P12 to P17 (answer digits A6 to A1).</p>
<p><img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Figure 23: This map shows the percentage of enriched questions that fail when we ablate each node in the 5-digit 1-layer 3-head addition model. The model only uses nodes in token positions P12 to P7 (answer digits A6 to A1). Lower percentages correspond to rarer edge cases. The grey space represents nodes that are not used by the model.
<img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Figure 24: This map shows the input tokens each attention head attends to at each token position in the new model. The new model only uses attention heads in token positions P12 to P17 (answer tokens A6 to A1). Each head attends to a pair of input tokens. Each column attends to three different input token positions. These behaviors are the same in the new and previous models.
<img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>Figure 25: This map shows the answer digit(s) A0 .. A5 impacted when we ablate each attention head and MLP layer in the new model. This behavior is the same in the new and previous models. Note that A6 is the constant " + " answer token.</p>
<p><img alt="img-22.jpeg" src="img-22.jpeg" /></p>
<p>Figure 26: This map shows the calculation tasks BA, MC1 and US9 located in this model by automated search using behavior filtering and ablation intervention testing. For nodes containing "??" the automated search did not identify the purpose of the node.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Corresponding author: fazl@robots.ox.ac.uk.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
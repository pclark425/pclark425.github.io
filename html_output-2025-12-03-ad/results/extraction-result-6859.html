<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6859 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6859</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6859</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-271924133</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.11866v1.pdf" target="_blank">Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design</a></p>
                <p><strong>Paper Abstract:</strong> Molecule design is a multifaceted approach that leverages computational methods and experiments to optimize molecular properties, fast-tracking new drug discoveries, innovative material development, and more efficient chemical processes. Recently, text-based molecule design has emerged, inspired by next-generation AI tasks analogous to foundational vision-language models. Our study explores the use of knowledge-augmented prompting of large language models (LLMs) for the zero-shot text-conditional de novo molecular generation task. Our approach uses task-specific instructions and a few demonstrations to address distributional shift challenges when constructing augmented prompts for querying LLMs to generate molecules consistent with technical descriptions. Our framework proves effective, outperforming state-of-the-art (SOTA) baseline models on benchmark datasets.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6859.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6859.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FrontierX: LLM-MG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FrontierX: Large Language Model - Molecule Generator (LLM-MG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid pipeline introduced in this paper that uses knowledge-augmented prompting of off-the-shelf LLMs to produce top‑R SMILES + natural-language explanations, fine-tunes small LMs on those explanations and original texts to obtain contextual embeddings, integrates prediction and text embeddings with a hierarchical multi‑head attention module, and decodes unified cross‑modal embeddings into SMILES (text2mol) or into textual descriptions (mol2text).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FrontierX: LLM-MG (uses GPT-4 / GPT-3.5 / davinci-003 / Bard backbones and DeBERTa small LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid pipeline: prompt-only LLMs (LMaaS) + fine-tuned encoder LMs + transformer decoder; hierarchical multi-head attention (cross-modal)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>LLM sizes not specified / undisclosed; DeBERTa small LM ~50M (cited Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Does not fine-tune LLM backbones; fine-tunes small LMs (LM_exp, LM_org) on (a) auxiliary explanations generated by LLMs and (b) original molecule textual descriptions drawn from the ChEBI-20 dataset (33,010 pairs; 26,407 used for demonstrations). LLMs are described as pre-trained on large web-scale text corpora (chemical corpora not explicitly specified).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Knowledge-augmented prompting (few‑shot / in‑context learning): construct augmented prompt by prepending K demonstrations (input text ↔ SMILES) sampled via scaffold (semantic similarity using text-embedding-ada-002) or random; query LLM via LMaaS to produce top‑R ranked SMILES and auxiliary textual explanations; convert top‑R SMILES to one‑hot vectors, fine-tune small LMs on explanations and original text to produce token embeddings, pool to text-level embeddings, integrate y_org, y_exp, y_pred via hierarchical multi-head attention, and decode SMILES with a transformer decoder (character-by-character) minimizing categorical cross-entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (string tokens); one-hot encoding of SMILES vocabulary for prediction embeddings; textual natural-language descriptions used as input/explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Text-conditional de novo molecule generation (text2mol: translate technical textual descriptions to SMILES) and the reverse mol2text (generate textual descriptions from SMILES). General motivation linked to drug discovery / materials / chemistry but experiments focus on text2mol and mol2text translation tasks on ChEBI-20.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Prompting hyperparameters: demonstrations K (scaffold sampling) tuned; experiments used K=16 (Scaffold) for main results; top‑R predictions R set to 4; no explicit property or synthetic-accessibility filters reported in the generation stage (no toxicity or SA cutoffs applied); generation constrained by LLM context length (noted limit ~4096 tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>LMaaS APIs for off-the-shelf LLM access; OpenAI text-embedding-ada-002 used for semantic retrieval (scaffold sampling); RDKit used to validate generated molecules; PyTorch used for training; no reported integration with docking/quantum chemistry/retrosynthesis tools.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEBI-20 (33,010 text ↔ SMILES pairs, 80:10:10 split; 26,407 training pairs used for demonstrations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Chemical similarity: FTS (Fingerprint Tanimoto Similarity using MACCS, RDK, Morgan); Fréchet ChemNet Distance (FCD). NLP/string metrics: BLEU (BLEU-2, BLEU-4), Exact Match, Levenshtein distance. Validity via RDKit. For mol2text also ROUGE-1/2/L and METEOR.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Authors report state-of-the-art performance on the text2mol task when using FrontierX with GPT-4 backbone and Scaffold (K=16). Representative mol2text numbers (FrontierX: LLM-MG, Scaffold K=16, W/GPT-4) from Table 10: BLEU-2 = 0.743 ± 0.081, BLEU-4 = 0.656 ± 0.097, ROUGE-1 = 0.818 ± 0.034, ROUGE-2 = 0.727 ± 0.013, ROUGE-L = 0.783 ± 0.047, METEOR = 0.812 ± 0.051. Ablation results for text2mol: removing y_exp caused drops of 17.21% (MACCS FTS), 16.43% (BLEU) and 13.12% (Validity); removing y_org caused drops of 20.69% (MACCS FTS), 20.91% (BLEU), 16.00% (Validity); removing y_pred caused smaller drops (~8.44% MACCS FTS, 11.63% BLEU, 6.56% Validity). Exact baseline numbers for text2mol are referenced in Tables 2/3 but not reproduced in full in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Paper notes LLM limitations: black-box nature and lack of access to logits/token embeddings for LLMs; resource intensity and context-length limits (~4096 tokens); LLMs sometimes misinterpret or hallucinate SMILES due to multiple valid SMILES forms and implicit hydrogens leading to ambiguity; need for improved handling of chemical syntax and integration with cheminformatics (RDKit) is recommended; no wet-lab synthesis or assay validation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6859.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6859.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large multi-modal/large-scale LLM used as a backbone in the FrontierX pipeline via LMaaS prompting; reported by the authors to produce the best performance among tested off-the-shelf LLMs for text2mol when combined with knowledge-augmented prompts and scaffold sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only large language model, prompt-only (LMaaS access), used in zero/few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>undisclosed in paper (commercial GPT-4 size not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Described as pre-trained on large text corpora; chemical-specific pretraining not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Knowledge-augmented few-shot/zero-shot prompting via LMaaS; LLM outputs top‑R (R=4) ranked SMILES and auxiliary textual explanations conditioned on augmented prompts (task instructions + K demonstrations sampled via scaffold or random).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings returned by the model (top‑R list) and natural-language explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Text2mol (text → SMILES) and mol2text evaluations (via FrontierX pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Prompt-level constraints: augmented prompt includes K demonstrations (main experiments used scaffold K=16); R=4 top predictions requested. No explicit chemical property filters applied at generation time.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Accessed via LMaaS text-based API; outputs used downstream with RDKit for validity checks and with OpenAI embedding models for demonstration retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Evaluated on ChEBI-20 (used for constructing demonstrations and test/eval), training/fine-tuning not applied to GPT-4 in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same as FrontierX: FTS (MACCS/RDK/Morgan), FCD, BLEU, Exact Match, Levenshtein, RDKit validity, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported as the best-performing LLM backbone in the FrontierX pipeline; authors state GPT-4 + Scaffold K=16 yields SOTA results on ChEBI-20 text2mol comparisons to baselines (exact per-model metric tables referenced but not fully enumerated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Context-length limits, lack of access to logits/token embeddings, black-box behavior, and occasional SMILES misinterpretations/hallucinations; computational cost for repeated prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6859.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6859.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-3.5 family LLM evaluated as an off-the-shelf backbone for knowledge-augmented prompting in FrontierX; used to generate top‑R SMILES and auxiliary explanations in zero/few-shot modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only large language model, prompt-only (LMaaS access), few-shot/zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>undisclosed in paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pre-trained on broad web-scale text corpora; chemical-focused corpora not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Knowledge-augmented few-shot prompting; returns top‑R SMILES predictions and textual reasoning; used with scaffold/random demonstration sampling (K hyperparameter).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings (top‑R) and textual explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Text2mol (text-to-SMILES translation) evaluation within FrontierX.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Prompting demonstrations K (scaffold or random), top‑R=4 requested; no downstream property filters applied.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Invoked via LMaaS APIs; outputs used with RDKit (validation) and embedding retrieval tools.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEBI-20 for demonstrations and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>FTS variants, FCD, BLEU, Exact Match, Levenshtein, RDKit validity.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Included as one of the evaluated LLMs; outperformed Google Bard under the same demonstration counts; quantitatively inferior to GPT-4 per authors' reported comparisons (specific per-model numbers not fully enumerated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Same limitations as other LLMs: potential SMILES ambiguity/hallucination, limited interpretability, and no token/logit access for LMaaS; resource/context constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6859.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6859.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-003 (GPT-3 family model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier OpenAI GPT-3-era LLM evaluated as a backbone for knowledge-augmented prompting to generate SMILES and explanations in the FrontierX experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, prompt-only via API (few-shot/zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>undisclosed in paper (GPT-3 era, historically large but exact params not provided)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pre-trained on large text corpora; not stated to be chemical-specialized.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Knowledge-augmented few-shot prompting (scaffold/random demonstrations), returns top‑R SMILES and explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings + textual explanations</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Text2mol translation experiments and comparison baselines</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Prompt-level K demonstrations (scaffold/random), top‑R predictions requested (R=4 in main experiments); no property-level constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Queried through LMaaS; outputs validated with RDKit; demonstration sampling used OpenAI embeddings for semantic retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEBI-20 for demonstrations/evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>FTS, FCD, BLEU, Exact Match, Levenshtein, RDKit validity</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Included in experimental comparison; authors cite davinci-003 among evaluated LLMs but indicate GPT-4 > GPT-3.5/davinci in performance. Exact numeric breakdown per LLM not reproduced in the body text.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Same as general LLM limitations described: SMILES ambiguity, black-box API constraints, missing token/logit access.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6859.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6859.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Google Bard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Bard</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large Google LLM evaluated as an off-the-shelf backbone in the study; reported to be outperformed by the GPT family models on the text2mol task under identical augmented-prompt conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Google Bard</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only large language model, prompt-only (LMaaS access)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>undisclosed (paper notes Bard has a very large vocabulary/scale but no parameter count given)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pre-trained on large-scale corpora; chemical-specific corpora not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Knowledge-augmented few-shot/zero-shot prompting; returns SMILES predictions and explanations when prompted with augmented prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings and natural-language explanations</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Text2mol and mol2text evaluation as part of benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Same prompting constraints (K demos, scaffold/random sampling); no property filters applied.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Queried via LMaaS-style API; outputs fed to RDKit for validity checks and used with embedding retrieval for demo selection.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEBI-20</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>FTS, FCD, BLEU, Exact Match, Levenshtein, RDKit validity</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Authors report GPT models consistently outperformed Google Bard across evaluation metrics with the same number of demonstrations; Bard produced fewer valid SMILES and lower metric scores in comparative experiments (specific per-model numeric values not shown in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Same LLM limitations; authors explicitly note Bard underperforms GPT family for this task in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6859.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6859.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa (LM_exp / LM_org)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa (Decoding-Enhanced BERT with disentangled attention) used as small LM for fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained smaller encoder LM (cited ~50M vocabulary/params in Table 1) used in FrontierX as LM_exp and LM_org to produce context-aware token embeddings from (a) auxiliary explanations generated by LLMs and (b) original textual descriptions; these embeddings are pooled and used in cross-modal attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa (pre-trained; fine-tuned as LM_exp and LM_org)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-only transformer LM, fine-tuned on generated explanations and original texts</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Table 1 reports DeBERTa size ~50M (vocabulary/parameters descriptor in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pre-trained general-domain DeBERTa weights (Hugging Face); then fine-tuned on ChEBI-20 textual descriptions and LLM-generated explanations for domain customization.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning (pre-train, fine-tune paradigm) on textual sequences (S_exp from LLM explanations, S_org original texts) to produce contextualized token embeddings; these embeddings are pooled via attention to produce y_exp and y_org.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Does not generate SMILES directly; encodes natural-language descriptions and explanations to embed semantic content used by decoder that generates SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Supportive encoder in text2mol and mol2text tasks to provide domain-aware embeddings for decoder generation of SMILES or textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Fine-tuning uses ChEBI-20 training split (80%); standard training hyperparameters from experimental setup (batch size 32, epochs up to 100, embedding dim d=128, Adam lr=1e-3 with decay and early stopping).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Fine-tuned within PyTorch pipeline; embeddings input to hierarchical multi-head attention and transformer decoder; output SMILES validated with RDKit.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEBI-20 (fine-tuning on explanations and original texts derived from this dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Contributes to final model metrics (FTS, FCD, BLEU, Exact Match, Levenshtein, RDKit validity) but not evaluated standalone in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used within FrontierX to achieve the reported SOTA translation performance; ablation studies show removal of y_exp or y_org (i.e., disabling respective fine-tuned LMs or pools) causes substantial performance degradation (e.g., up to ~20% drops in key metrics). Specific per-DeBERTa numeric performance not reported separately.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Quality of fine-tuned embeddings depends on quality of LLM-generated explanations and on demonstration sampling strategy; if explanations are poor or misleading, embeddings (and downstream SMILES generation) degrade. No intrinsic chemical validation capacity — relies on downstream RDKit checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Text2mol: Cross-modal molecule retrieval with natural language queries <em>(Rating: 2)</em></li>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>What indeed can gpt models do in chemistry? a comprehensive benchmark on eight tasks <em>(Rating: 2)</em></li>
                <li>Fréchet chemnet distance: a metric for generative models for molecules in drug discovery <em>(Rating: 1)</em></li>
                <li>Black-box tuning for language-model-as-a-service <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6859",
    "paper_id": "paper-271924133",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "FrontierX: LLM-MG",
            "name_full": "FrontierX: Large Language Model - Molecule Generator (LLM-MG)",
            "brief_description": "A hybrid pipeline introduced in this paper that uses knowledge-augmented prompting of off-the-shelf LLMs to produce top‑R SMILES + natural-language explanations, fine-tunes small LMs on those explanations and original texts to obtain contextual embeddings, integrates prediction and text embeddings with a hierarchical multi‑head attention module, and decodes unified cross‑modal embeddings into SMILES (text2mol) or into textual descriptions (mol2text).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FrontierX: LLM-MG (uses GPT-4 / GPT-3.5 / davinci-003 / Bard backbones and DeBERTa small LMs)",
            "model_type": "hybrid pipeline: prompt-only LLMs (LMaaS) + fine-tuned encoder LMs + transformer decoder; hierarchical multi-head attention (cross-modal)",
            "model_size": "LLM sizes not specified / undisclosed; DeBERTa small LM ~50M (cited Table 1)",
            "training_data_description": "Does not fine-tune LLM backbones; fine-tunes small LMs (LM_exp, LM_org) on (a) auxiliary explanations generated by LLMs and (b) original molecule textual descriptions drawn from the ChEBI-20 dataset (33,010 pairs; 26,407 used for demonstrations). LLMs are described as pre-trained on large web-scale text corpora (chemical corpora not explicitly specified).",
            "generation_method": "Knowledge-augmented prompting (few‑shot / in‑context learning): construct augmented prompt by prepending K demonstrations (input text ↔ SMILES) sampled via scaffold (semantic similarity using text-embedding-ada-002) or random; query LLM via LMaaS to produce top‑R ranked SMILES and auxiliary textual explanations; convert top‑R SMILES to one‑hot vectors, fine-tune small LMs on explanations and original text to produce token embeddings, pool to text-level embeddings, integrate y_org, y_exp, y_pred via hierarchical multi-head attention, and decode SMILES with a transformer decoder (character-by-character) minimizing categorical cross-entropy.",
            "chemical_representation": "SMILES (string tokens); one-hot encoding of SMILES vocabulary for prediction embeddings; textual natural-language descriptions used as input/explanations.",
            "target_application": "Text-conditional de novo molecule generation (text2mol: translate technical textual descriptions to SMILES) and the reverse mol2text (generate textual descriptions from SMILES). General motivation linked to drug discovery / materials / chemistry but experiments focus on text2mol and mol2text translation tasks on ChEBI-20.",
            "constraints_used": "Prompting hyperparameters: demonstrations K (scaffold sampling) tuned; experiments used K=16 (Scaffold) for main results; top‑R predictions R set to 4; no explicit property or synthetic-accessibility filters reported in the generation stage (no toxicity or SA cutoffs applied); generation constrained by LLM context length (noted limit ~4096 tokens).",
            "integration_with_external_tools": "LMaaS APIs for off-the-shelf LLM access; OpenAI text-embedding-ada-002 used for semantic retrieval (scaffold sampling); RDKit used to validate generated molecules; PyTorch used for training; no reported integration with docking/quantum chemistry/retrosynthesis tools.",
            "dataset_used": "ChEBI-20 (33,010 text ↔ SMILES pairs, 80:10:10 split; 26,407 training pairs used for demonstrations).",
            "evaluation_metrics": "Chemical similarity: FTS (Fingerprint Tanimoto Similarity using MACCS, RDK, Morgan); Fréchet ChemNet Distance (FCD). NLP/string metrics: BLEU (BLEU-2, BLEU-4), Exact Match, Levenshtein distance. Validity via RDKit. For mol2text also ROUGE-1/2/L and METEOR.",
            "reported_results": "Authors report state-of-the-art performance on the text2mol task when using FrontierX with GPT-4 backbone and Scaffold (K=16). Representative mol2text numbers (FrontierX: LLM-MG, Scaffold K=16, W/GPT-4) from Table 10: BLEU-2 = 0.743 ± 0.081, BLEU-4 = 0.656 ± 0.097, ROUGE-1 = 0.818 ± 0.034, ROUGE-2 = 0.727 ± 0.013, ROUGE-L = 0.783 ± 0.047, METEOR = 0.812 ± 0.051. Ablation results for text2mol: removing y_exp caused drops of 17.21% (MACCS FTS), 16.43% (BLEU) and 13.12% (Validity); removing y_org caused drops of 20.69% (MACCS FTS), 20.91% (BLEU), 16.00% (Validity); removing y_pred caused smaller drops (~8.44% MACCS FTS, 11.63% BLEU, 6.56% Validity). Exact baseline numbers for text2mol are referenced in Tables 2/3 but not reproduced in full in the text.",
            "experimental_validation": false,
            "challenges_or_limitations": "Paper notes LLM limitations: black-box nature and lack of access to logits/token embeddings for LLMs; resource intensity and context-length limits (~4096 tokens); LLMs sometimes misinterpret or hallucinate SMILES due to multiple valid SMILES forms and implicit hydrogens leading to ambiguity; need for improved handling of chemical syntax and integration with cheminformatics (RDKit) is recommended; no wet-lab synthesis or assay validation reported.",
            "uuid": "e6859.0",
            "source_info": {
                "paper_title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4 (GPT-4)",
            "brief_description": "A large multi-modal/large-scale LLM used as a backbone in the FrontierX pipeline via LMaaS prompting; reported by the authors to produce the best performance among tested off-the-shelf LLMs for text2mol when combined with knowledge-augmented prompts and scaffold sampling.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_type": "decoder-only large language model, prompt-only (LMaaS access), used in zero/few-shot prompting",
            "model_size": "undisclosed in paper (commercial GPT-4 size not specified)",
            "training_data_description": "Described as pre-trained on large text corpora; chemical-specific pretraining not specified in the paper.",
            "generation_method": "Knowledge-augmented few-shot/zero-shot prompting via LMaaS; LLM outputs top‑R (R=4) ranked SMILES and auxiliary textual explanations conditioned on augmented prompts (task instructions + K demonstrations sampled via scaffold or random).",
            "chemical_representation": "SMILES strings returned by the model (top‑R list) and natural-language explanations.",
            "target_application": "Text2mol (text → SMILES) and mol2text evaluations (via FrontierX pipeline).",
            "constraints_used": "Prompt-level constraints: augmented prompt includes K demonstrations (main experiments used scaffold K=16); R=4 top predictions requested. No explicit chemical property filters applied at generation time.",
            "integration_with_external_tools": "Accessed via LMaaS text-based API; outputs used downstream with RDKit for validity checks and with OpenAI embedding models for demonstration retrieval.",
            "dataset_used": "Evaluated on ChEBI-20 (used for constructing demonstrations and test/eval), training/fine-tuning not applied to GPT-4 in this work.",
            "evaluation_metrics": "Same as FrontierX: FTS (MACCS/RDK/Morgan), FCD, BLEU, Exact Match, Levenshtein, RDKit validity, etc.",
            "reported_results": "Reported as the best-performing LLM backbone in the FrontierX pipeline; authors state GPT-4 + Scaffold K=16 yields SOTA results on ChEBI-20 text2mol comparisons to baselines (exact per-model metric tables referenced but not fully enumerated in text).",
            "experimental_validation": false,
            "challenges_or_limitations": "Context-length limits, lack of access to logits/token embeddings, black-box behavior, and occasional SMILES misinterpretations/hallucinations; computational cost for repeated prompting.",
            "uuid": "e6859.1",
            "source_info": {
                "paper_title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo",
            "brief_description": "A GPT-3.5 family LLM evaluated as an off-the-shelf backbone for knowledge-augmented prompting in FrontierX; used to generate top‑R SMILES and auxiliary explanations in zero/few-shot modes.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_type": "decoder-only large language model, prompt-only (LMaaS access), few-shot/zero-shot",
            "model_size": "undisclosed in paper",
            "training_data_description": "Pre-trained on broad web-scale text corpora; chemical-focused corpora not specified.",
            "generation_method": "Knowledge-augmented few-shot prompting; returns top‑R SMILES predictions and textual reasoning; used with scaffold/random demonstration sampling (K hyperparameter).",
            "chemical_representation": "SMILES strings (top‑R) and textual explanations.",
            "target_application": "Text2mol (text-to-SMILES translation) evaluation within FrontierX.",
            "constraints_used": "Prompting demonstrations K (scaffold or random), top‑R=4 requested; no downstream property filters applied.",
            "integration_with_external_tools": "Invoked via LMaaS APIs; outputs used with RDKit (validation) and embedding retrieval tools.",
            "dataset_used": "ChEBI-20 for demonstrations and evaluation.",
            "evaluation_metrics": "FTS variants, FCD, BLEU, Exact Match, Levenshtein, RDKit validity.",
            "reported_results": "Included as one of the evaluated LLMs; outperformed Google Bard under the same demonstration counts; quantitatively inferior to GPT-4 per authors' reported comparisons (specific per-model numbers not fully enumerated in text).",
            "experimental_validation": false,
            "challenges_or_limitations": "Same limitations as other LLMs: potential SMILES ambiguity/hallucination, limited interpretability, and no token/logit access for LMaaS; resource/context constraints.",
            "uuid": "e6859.2",
            "source_info": {
                "paper_title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "text-davinci-003",
            "name_full": "text-davinci-003 (GPT-3 family model)",
            "brief_description": "An earlier OpenAI GPT-3-era LLM evaluated as a backbone for knowledge-augmented prompting to generate SMILES and explanations in the FrontierX experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_type": "decoder-only LLM, prompt-only via API (few-shot/zero-shot)",
            "model_size": "undisclosed in paper (GPT-3 era, historically large but exact params not provided)",
            "training_data_description": "Pre-trained on large text corpora; not stated to be chemical-specialized.",
            "generation_method": "Knowledge-augmented few-shot prompting (scaffold/random demonstrations), returns top‑R SMILES and explanations.",
            "chemical_representation": "SMILES strings + textual explanations",
            "target_application": "Text2mol translation experiments and comparison baselines",
            "constraints_used": "Prompt-level K demonstrations (scaffold/random), top‑R predictions requested (R=4 in main experiments); no property-level constraints.",
            "integration_with_external_tools": "Queried through LMaaS; outputs validated with RDKit; demonstration sampling used OpenAI embeddings for semantic retrieval.",
            "dataset_used": "ChEBI-20 for demonstrations/evaluation",
            "evaluation_metrics": "FTS, FCD, BLEU, Exact Match, Levenshtein, RDKit validity",
            "reported_results": "Included in experimental comparison; authors cite davinci-003 among evaluated LLMs but indicate GPT-4 &gt; GPT-3.5/davinci in performance. Exact numeric breakdown per LLM not reproduced in the body text.",
            "experimental_validation": false,
            "challenges_or_limitations": "Same as general LLM limitations described: SMILES ambiguity, black-box API constraints, missing token/logit access.",
            "uuid": "e6859.3",
            "source_info": {
                "paper_title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Google Bard",
            "name_full": "Google Bard",
            "brief_description": "A large Google LLM evaluated as an off-the-shelf backbone in the study; reported to be outperformed by the GPT family models on the text2mol task under identical augmented-prompt conditions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Google Bard",
            "model_type": "decoder-only large language model, prompt-only (LMaaS access)",
            "model_size": "undisclosed (paper notes Bard has a very large vocabulary/scale but no parameter count given)",
            "training_data_description": "Pre-trained on large-scale corpora; chemical-specific corpora not specified.",
            "generation_method": "Knowledge-augmented few-shot/zero-shot prompting; returns SMILES predictions and explanations when prompted with augmented prompts.",
            "chemical_representation": "SMILES strings and natural-language explanations",
            "target_application": "Text2mol and mol2text evaluation as part of benchmarking",
            "constraints_used": "Same prompting constraints (K demos, scaffold/random sampling); no property filters applied.",
            "integration_with_external_tools": "Queried via LMaaS-style API; outputs fed to RDKit for validity checks and used with embedding retrieval for demo selection.",
            "dataset_used": "ChEBI-20",
            "evaluation_metrics": "FTS, FCD, BLEU, Exact Match, Levenshtein, RDKit validity",
            "reported_results": "Authors report GPT models consistently outperformed Google Bard across evaluation metrics with the same number of demonstrations; Bard produced fewer valid SMILES and lower metric scores in comparative experiments (specific per-model numeric values not shown in main text).",
            "experimental_validation": false,
            "challenges_or_limitations": "Same LLM limitations; authors explicitly note Bard underperforms GPT family for this task in their experiments.",
            "uuid": "e6859.4",
            "source_info": {
                "paper_title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "DeBERTa (LM_exp / LM_org)",
            "name_full": "DeBERTa (Decoding-Enhanced BERT with disentangled attention) used as small LM for fine-tuning",
            "brief_description": "A pre-trained smaller encoder LM (cited ~50M vocabulary/params in Table 1) used in FrontierX as LM_exp and LM_org to produce context-aware token embeddings from (a) auxiliary explanations generated by LLMs and (b) original textual descriptions; these embeddings are pooled and used in cross-modal attention.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeBERTa (pre-trained; fine-tuned as LM_exp and LM_org)",
            "model_type": "encoder-only transformer LM, fine-tuned on generated explanations and original texts",
            "model_size": "Table 1 reports DeBERTa size ~50M (vocabulary/parameters descriptor in paper)",
            "training_data_description": "Pre-trained general-domain DeBERTa weights (Hugging Face); then fine-tuned on ChEBI-20 textual descriptions and LLM-generated explanations for domain customization.",
            "generation_method": "Fine-tuning (pre-train, fine-tune paradigm) on textual sequences (S_exp from LLM explanations, S_org original texts) to produce contextualized token embeddings; these embeddings are pooled via attention to produce y_exp and y_org.",
            "chemical_representation": "Does not generate SMILES directly; encodes natural-language descriptions and explanations to embed semantic content used by decoder that generates SMILES.",
            "target_application": "Supportive encoder in text2mol and mol2text tasks to provide domain-aware embeddings for decoder generation of SMILES or textual descriptions.",
            "constraints_used": "Fine-tuning uses ChEBI-20 training split (80%); standard training hyperparameters from experimental setup (batch size 32, epochs up to 100, embedding dim d=128, Adam lr=1e-3 with decay and early stopping).",
            "integration_with_external_tools": "Fine-tuned within PyTorch pipeline; embeddings input to hierarchical multi-head attention and transformer decoder; output SMILES validated with RDKit.",
            "dataset_used": "ChEBI-20 (fine-tuning on explanations and original texts derived from this dataset).",
            "evaluation_metrics": "Contributes to final model metrics (FTS, FCD, BLEU, Exact Match, Levenshtein, RDKit validity) but not evaluated standalone in paper.",
            "reported_results": "Used within FrontierX to achieve the reported SOTA translation performance; ablation studies show removal of y_exp or y_org (i.e., disabling respective fine-tuned LMs or pools) causes substantial performance degradation (e.g., up to ~20% drops in key metrics). Specific per-DeBERTa numeric performance not reported separately.",
            "experimental_validation": false,
            "challenges_or_limitations": "Quality of fine-tuned embeddings depends on quality of LLM-generated explanations and on demonstration sampling strategy; if explanations are poor or misleading, embeddings (and downstream SMILES generation) degrade. No intrinsic chemical validation capacity — relies on downstream RDKit checks.",
            "uuid": "e6859.5",
            "source_info": {
                "paper_title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Text2mol: Cross-modal molecule retrieval with natural language queries",
            "rating": 2,
            "sanitized_title": "text2mol_crossmodal_molecule_retrieval_with_natural_language_queries"
        },
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2,
            "sanitized_title": "translation_between_molecules_and_natural_language"
        },
        {
            "paper_title": "What indeed can gpt models do in chemistry? a comprehensive benchmark on eight tasks",
            "rating": 2,
            "sanitized_title": "what_indeed_can_gpt_models_do_in_chemistry_a_comprehensive_benchmark_on_eight_tasks"
        },
        {
            "paper_title": "Fréchet chemnet distance: a metric for generative models for molecules in drug discovery",
            "rating": 1,
            "sanitized_title": "fréchet_chemnet_distance_a_metric_for_generative_models_for_molecules_in_drug_discovery"
        },
        {
            "paper_title": "Black-box tuning for language-model-as-a-service",
            "rating": 1,
            "sanitized_title": "blackbox_tuning_for_languagemodelasaservice"
        }
    ],
    "cost": 0.01494075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design
18 Aug 2024</p>
<p>Sakhinana Sagar sagar.sakhinana@tcs.com 
Venkataramana Runkana venkat.runkana@tcs.com 
Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design
18 Aug 2024ABB281DE0331ECFB4A7C44CE5E4F3374arXiv:2408.11866v1[cs.CL]
Molecule design is a multifaceted approach that leverages computational methods and experiments to optimize molecular properties, fast-tracking new drug discoveries, innovative material development, and more efficient chemical processes.Recently, text-based molecule design has emerged, inspired by next-generation AI tasks analogous to foundational vision-language models.Our study explores the use of knowledge-augmented prompting of large language models (LLMs) for the zero-shot text-conditional de novo molecular generation task.Our approach uses task-specific instructions and a few demonstrations to address distributional shift challenges when constructing augmented prompts for querying LLMs to generate molecules consistent with technical descriptions.Our framework proves effective, outperforming state-of-the-art (SOTA) baseline models on benchmark datasets.* Designed and programmed research, conducted experiments, analyzed results, and drafted manuscript Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models at NeurIPS 2023.</p>
<p>Introduction</p>
<p>Molecule design is an interdisciplinary approach that involves identifying a target molecule or property to enhance, such as a drug with increased efficacy or a material with superior characteristics.Advancements in science and technology have accelerated the discovery and development of novel drugs, advanced materials, and innovative chemical processes.This iterative process begins with (a) identifying a target molecule or property to improve, followed by (b) employing computational methods to explore the vast chemical space and optimize potential candidate structure and composition.The cycle continues with (c) synthesizing and testing promising candidates in the laboratory until the desired characteristics are achieved.The transformer architecture [28] has revolutionized various fields in computer science, including language understanding [5], text generation [17,2], image understanding [7], and multi-modal generation [19,23].Utilizing this architecture to scale language models has established itself as a universal approach for enhancing generalization performance.In recent times, the emergence of foundational Large Language Models (LLMs) [2,3,27], which are built upon transformer architectures, has significantly revolutionized performance in various natural language processing tasks by enabling enhanced linguistic comprehension and logical reasoning abilities.Different learning strategies such as Zero-Shot Chain of Thought (Zero-shot-CoT [29]) and Few-Shot (In-Context) Learning (Few-shot-ICL [22,6]) are utilized to leverage the emerging abilities of general-purpose LLMs for a wide variety of specialized tasks across various domains.The former employs task-specific instructions without relying on downstream task-based demonstrations, utilizing the inherent knowledge that the language model acquired during training to generate outputs.In contrast, Few-shot-ICL supplements instructions with a handful of demonstrations, presented as input-output pairs, to foster contextual understanding and facilitate task-specific adaptation, thereby generating relevant output.Recently, there has been a surge in the evolution of generative AI, such as "DALL•E" [19,20] from OpenAI -a text-to-image diffusion model that can generate realistic images from text descriptions, and "Make-A-Video" [24] from Meta AI -a text-to-video diffusion model that generates realistic, engaging, and creative videos from text, among others.Inspired by recent developments in next-generation AI, "Text-Based Molecule Design" [8] (also known as text2mol) represents a novel cross-domain task in chemistry that involves generating chemical SMILES representations from the corresponding technical descriptions of molecules expressed in natural language.Unlike traditional methods of de novo molecule generation, the text2mol task extracts information from technical descriptions of molecules, identifying aspects such as the specified structure, properties and functional groups, to generate chemical SMILES representations with desired characteristics.Existing models [8,10] in the literature for the text2mol task face challenges in achieving optimal performance and utility, particularly in scenarios where data is scarce and unbalanced.LLMs like ChatGPT [2], while proficient in linguistic comprehension, are black-box in nature, resource-intensive, and lack interpretability.Smaller language models(LMs) like BERT [5], although flexible and interpretable, may lag in reasoning and generalization, resulting in less coherent and contextually relevant responses compared to LLMs.Navigating these challenges requires a delicate balance between performance, efficiency, and interpretability.Our study introduces a novel approach for the text2mol task by combining the strengths of both LLMs and small-scale LMs.LLMs predict a ranked list of chemical SMILES representations while providing explanations as justifications for these predictions, conditioned on the input prompt.These textual explanations, in conjunction with original technical descriptions of molecules, are used to fine-tune small-scale LMs to obtain context-aware token embeddings that capture the essence of both the generated explanations and original text, respectively.Concurrently, the top-ranked predictions generated by LLMs are transformed to obtain prediction embeddings.By integrating these various embeddings through a hierarchical multi-head attention mechanism, the framework inputs a unified cross-modal embedding into a transformer decoder to generate chemical SMILES representations that align with original technical descriptions.In this study, we explored the use of knowledge-augmented LLM prompting for zero-shot text-conditional molecule generation, a sequence-to-sequence cross-domain task.We present a powerful new tool, FrontierX: LLM-MG, where the goal is to task LLMs with a knowledge-infused prompt that consists of a few demonstrations(input-output pairs) for the text2mol task, along with task-specific instructions, where the output is chemical SMILES representations of the corresponding query technical descriptions.Our experiments on benchmark datasets provide empirical evidence supporting the framework's effectiveness in text-based molecule design tasks.The workflow of the proposed approach is illustrated in Figure 1.</p>
<p>LLM</p>
<p>LM Technical Descriptions</p>
<p>Top-R Ranked Predictions</p>
<p>Textual Explanations</p>
<p>Cross-Modal Encoder</p>
<p>Prediction</p>
<p>Encoder</p>
<p>Transformer Decoder</p>
<p>SMILES Representations</p>
<p>Figure 1: Overview of the FrontierX: LLM-MG framework.We construct knowledge-augmented prompts using task-specific instructions and a few demonstrations (input-output pairs) based on the downstream task.The augmented prompt queries LLMs to generate the top-R predictions of the SMILES representations and produces textual explanations as justifications for its predictions.We fine-tune small-scale pre-trained language models (LMs) on the generated explanations for domainspecific customization to obtain context-aware token embeddings.We utilize a weighted-sum pooling attention mechanism for task-specific adaptation to compute contextualized text-level embeddings.</p>
<p>In parallel, we transform the LLMs' top-R predictions to compute prediction embeddings.The cross-modal encoder, modeled by a hierarchical multi-head attention mechanism, computes the unified embeddings by integrating the mono-domain text-level embeddings (both the original text and explanatory text) and prediction embeddings.Finally, the transformer decoder generates the chemical SMILES representations.We do not repurpose LLMs by fine-tuning with labeled data for domain customization.Instead, we access LLMs via LMaaS [25] using text-based API interaction.</p>
<p>Proposed Method</p>
<p>The Large Language Models (LLMs), such as ChatGPT [2], Meta's LLaMA [27] -that have been pre-trained on large text corpora and operate based on a "prompt and predict" approach (utilizing natural language prompts to generate the subsequent contextual word or phrase, aligning with human-like responses) -have revolutionized language modeling with their proficiency in linguistic comprehension and advanced logical reasoning abilities, providing improved performance on generalpurpose NLP tasks.While LLMs are inherently black box in nature, they possess remarkable capabilities.However, their widespread adoption for applications in various downstream tasks is hindered by the unavailability of logits or token embeddings, which limits explainability.Additionally, they require significant computational resources for fine-tuning on labeled data for task-specific adaptation or for repurposing for domain-customization.In contrast, the small-scale language models (LMs), such as BERT [5] and DeBERTa [11], following a "pre-train, fine-tune" approach, offer more affordable flexibility for fine-tuning with minimal labeled data and provide access to logits or token embeddings, aiding interpretability.While smaller LMs can learn complex patterns, they often fall short in reasoning and generalization abilities compared to LLMs, which generate more coherent and contextually relevant responses.To alleviate resource constraints, Language Modeling as a Service (LMaaS [25]) offers access to LLMs through text-based API interactions, while remaining scalable and cost-effective.However, the potential of LLMs for text-conditional de novo molecular generation tasks remains largely underexplored.Our proposed approach for the text2mol task leverages LLMs by utilizing: (a) their predictive ability to provide a top-R ranked list of chemical SMILES representations; and (b) their generative ability to offer auxiliary explanations as justifications for their predictions by conditioning on the augmented prompt.Furthermore, we fine-tune two different small-scale LMs using (a) generated explanations from LLMs and (b) input technical descriptions of molecules to compute their respective contextualized token embeddings -which capture semantic coherence and contextual relevance for text-to-molecule generation tasks.We utilize weighted attention mechanism to compute both original and explanatory text-level embeddings from their respective context-aware token embeddings.In addition, we transform the LLMs' top-R predictions of chemical SMILES representations into predictive embeddings.We use a hierarchical multi-head attention mechanism to integrate various embeddings into unified cross-modal embedding for input into a transformer decoder, generating the chemical SMILES representation.</p>
<p>Evaluation LLMs &amp; LMs: In this work, we evaluated three popular LLMs: text-davinci-0032 , ChatGPT3 , and Google BARD 4 , in order to thoroughly compare their distinct strengths.text-davinci-003 was the earliest LLM released by OpenAI and was tailored for a broad spectrum of linguistic tasks.GPT-3.5-turbo is a substantial improvement over the GPT-3 base models, demonstrating remarkable performance on a wide range of linguistic tasks while also being cost-effective.Google BARD [1] stands out due to its extraordinary scale, complexity, and an impressively extensive vocabulary compared to the GPT-3.5 models.In addition to these, our study also incorporates a pre-trained smaller LM, DeBERTa5 , which is an improved version of the BERT [5] architecture.Table 1 presents a comprehensive summary of the technical specifications of these language models.Knowledge-Augmented Prompts: In our work, we offer essential context and task-specific instructions by using input natural language descriptions of the target molecule to prompt LLMs in a zero-shot setting to generate corresponding chemical SMILES representations.In this scenario, the primary task-specific instructions involve the translation of these descriptions into chemical SMILES representations.We create an augmented prompt that incorporates both the task-specific instructions and a few demonstrations.These demonstrations, which establish the context, are grounded in the downstream text2mol task and comprise input-output pairs (i.e., technical descriptions and their corresponding chemical SMILES representations).This approach facilitates knowledge-augmented prompting of the LLMs for zero-shot text-to-molecule generation tasks.The construction of an augmented prompt involves sampling text-molecule pairs from the training data that are relevant to the target molecule descriptions.We then prepend these pairs to the task-specific instructions to form an augmented prompt, which is used to query the LLMs in a zero-shot setting for the generation of chemical SMILES representations.To evaluate the impact of the quality and quantity of sampled text-molecule pairs on the performance of text-conditional de novo molecule generation tasks, we employ two different sampling strategies.The quality of these pairs is determined by the sampling methods used to identify pairs similar to the target molecule descriptions.We navigate through the training dataset using two semantic search-retrieval methodologiesrandom and scaffold -to sample text-molecule pairs relevant to the target molecule descriptions.The random approach involves arbitrarily sampling K text-molecule pairs from the training dataset.In contrast, the scaffold technique employs semantic similarity methods, specifically text-embedding-ada-002 from Ope-nAI 6 , to evaluate the similarity between the molecular textual descriptions in the training dataset and the target molecule descriptions.It then selects the top-K most relevant text-molecule pairs, where the hyperparameter K is set using a random search technique.We employ the different sampling strategies to analyze the effectiveness of augmenting prompts with relevant text-molecule pairs in language-conditioned molecule generation tasks.In short, unlike traditional supervised learning, LLMs (a) predict the chemical SMILES representations and (b) generate textual explanations for their predictions, utilizing the inherent knowledge embedded within the language model's parameters, all conditioned on the augmented prompt, without needing any parameter updates.</p>
<p>Querying LLMs: We access LLMs with LMaaS [25] platforms via text-based API interaction, necessitating solely text-based input and output.We create a customized zero-shot prompt template to query LLMs to translate textual descriptions into chemical SMILES representations.The LLMs' response serves the dual purpose of (a) providing detailed textual explanations for the underlying rationale, (reasoning or logic), behind the predictions and (b) generating a list of the top-R ranked chemical SMILES representations.Subsequently, we fine-tune smaller downstream LMs using the generated auxiliary explanations.The custom augmented prompt format is as follows:</p>
<p>Below are the textual descriptions -chemical SMILES representation pairs.Generate the chemical SMILES representation for the textual description provided below.</p>
<p>Querying LLMs (a) predicts the top-R ranked chemical SMILES representations and (b) provides auxiliary explanations as logical justifications for its predictions.</p>
<p>(LLMs Response) [top-R ranked predictions -Auxiliary Explanations] In the next section, we will discuss the use of auxiliary explanations and original textual descriptions for fine-tuning various downstream smaller LMs for domain customization.Later, we will transform the LLMs top-R predictions of chemical SMILES representations into predictive embeddings.Fine-tuning LMs for Domain-Specific Customization: Our novel approach leverages the integration of a smaller language model (LM) to extract relevant information from the original molecular textual descriptions and auxiliary explanations generated by LLMs, thereby aiding downstream tasks.The intermediary LM serves as a bridge between the LLM and the downstream layers that generate chemical SMILES representations.To elucidate further, we fine-tune pre-trained LMs, denoted as LM exp and LM org , to compute context-aware token embeddings by passing the text sequences generated by LLMs (referred to as S exp ) and original textual descriptions (referred to as S org ) through the LM exp and LM org models, respectively, as described below:
h exp = LM exp (S exp ) ∈ R (m×d) ; h org = LM org (S org ) ∈ R (n×d)(1)
where both contextualized embeddings h exp and h org capture not only the contextual information of the tokens but also encapsulate the semantic relationships among tokens within their respective textual content.Here, m and n represent the number of tokens in S exp and S org , while d represents the token embedding dimension.We employ a softmax attention mechanism to compute a weighted sum of the contextualized token embeddings, encoding the auxiliary explanations and original textual descriptions into single fixed-length vectors or embeddings denoted as y exp and y org and computed as follows,
α i = softmax(q i ); q i = u T h (i) exp || β i = softmax(r i ); r i = v T h (i) org (2)y exp = m i=0 α i h (i) exp ∈ R (d) ; y org = n i=0 β i h (i) org ∈ R (d)(3)
where u and v are differentiable vectors.The explanatory text-level embedding, represented as y exp , encapsulates domain-specific knowledge retrieved from foundational LLMs to support its predictions.</p>
<p>The original text-level embedding, denoted as y org , captures the overall context and semantics within the original textual descriptions by extracting the most pertinent and task-relevant information.</p>
<p>LLMs Prediction Embeddings: As mentioned earlier, the LLMs not only provide the auxiliary textual explanations but also predict the top-R ranked chemical SMILES representations list, which can be informative.For each target molecule in the text2mol task, the top-R predictions are converted into one-hot encoded vectors p i,1 , . . ., p i,R ∈ R C , where C represents the total number of elements in the SMILES vocabulary, encompassing a wide range of characters and symbols used to represent chemical structures.These vectors are subsequently concatenated into a single RC-dimensional vector, and finally, they undergo linear encoding into a fixed-length prediction embedding y pred ∈ R d , encapsulating the top-R predictions from the LLMs.Cross-modal Attention Layer We compute the cross-modal embedding, denoted as y cross , using a hierarchical multi-head attention mechanism that integrates the original text-level embedding y org , the explanatory text-level embedding y exp , and the prediction embedding y pred .This mechanism provides a robust framework for integrating diverse information encapsulated from different modalities, addressing several key aspects critical to the performance of cross-modal learning tasks.It involves hierarchical implementation of multi-head attention mechanisms.We employ two layers, each focusing on different aspects of the input embeddings, enabling more complex interactions and potentially leading to more scalable and efficient models.In the initial layer, we apply a multi-head attention mechanism to the mono-domain embeddings, specifically the original text-level embedding y org and the explanatory text-level embedding y exp , to obtain unified mono-domain embeddings denoted as y uni .In the subsequent layer, we utilize the multi-head attention mechanism on the cross-domain embeddings, comprising the prediction embedding y pred and the unified mono-domain embeddings y uni , to compute the cross-modal embeddings denoted as y cross .For the first layer, we compute the Query, Key, Value projections for the original text-level embedding y org for each head h as follows:
Q h org = y org W h Qorg ; K h org = y org W h Korg ; V h org = y org W h Vorg (4)
Similarly, the Query, Key, Value projections for explanation text-level embedding y exp for each head h as follows:
Q h exp = y exp W h Qexp ; K h exp = y exp W h Kexp ; V h exp = y exp W h Vexp (5)
We concatenate the keys and values from both original and explanatory text-level embeddings, which provides a powerful way to integrate information from the mono-domain embeddings into a unified, rich representation.
K h concat = [K h org , K h exp ]; V h concat = <a href="6">V h org , V h exp </a>
We use softmax attention to integrate complementary information from the mono-domain embeddings, focus on contextually relevant information, and semantically align them through an attention mechanism.The softmax function is applied to the keys for each query.
A h uni = Softmax (Q h org + Q h exp )K h concat T √ d h(7)
Each head outputs a new vector representation that highlights the most relevant features in the monodomain embeddings(both original and explanation text-level), according to the attention mechanism for that specific head, which is tailored to capture specific aspects or relationships within the data.
O h uni = A h uni V h concat (8)
Finally, all the head-specific outputs are concatenated and linearly transformed to create the unified mono-domain embedding as follows,
O concat = <a href="9">O 1 uni , O 2 uni , . . . , O H uni </a>y uni = O concat W Ouni (10)
where
W h Qorg , W h Korg , W h Vorg , W h Vexp , W h Qexp , W h
Kexp , W Ouni are the learnable weight matrices.Here, d h represents the dimensionality of the key/query/value for each head, and H is the number of heads.y uni denotes the unified mono-domain embeddings.The unified embeddings can learn and integrate complementary, diverse information present in both the y org and y exp embeddings.These unified embeddings facilitate semantic alignment among similar features across different embeddings and enable the identification of contextual relevance between distinct yet related y org and y exp monodomain embeddings.The next step involves computing the cross-modal embedding y cross using a second layer of a multihead attention mechanism that integrates both y pred and y uni .We compute the Query, Key, and Value projections for the prediction embedding y pred for each head h as follows:
Q h pred = y pred W h Qpred ; K h pred = y pred W h Kpred ; V h pred = y pred W h Vpred (11)
Similarly, we compute the Query, Key, Value projections for the unified embedding y uni for each head h as follows:
Q h uni = y uni W h Quni ; K h uni = y uni W h Kuni ; V h uni = y uni W h Vuni (12)
We concatenate the keys and values from both the prediction and unified embeddings, thereby facilitating a robust integration of insights from the cross-domain embeddings into a synergized and enriched representation.
K h cross = [K h uni , K h pred ]; V h cross = <a href="13">V h uni , V h pred </a>
We utilize a softmax attention mechanism to merge and align information from different domains, thereby prioritizing contextually relevant information and ensuring semantic alignment.The softmax v function is applied to the keys for each query, described as follows:
A h cross = Softmax (Q h uni + Q h pred )K h cross T √ d h(14)
In the multi-head attention mechanism, each head processes both embeddings(unified and predictive embeddings) to highlight important patterns, focusing on specific relationships or aspects within the data, enhancing performance in cross-modal learning tasks.
O h cross = A h cross V h cross (15)
Finally, all the head-specific outputs are concatenated and linearly transformed to create the final cross-modal embedding as follows,
O cross = <a href="16">O 1 cross , O 2 cross , . . . , O H cross </a>y cross = O cross W Ocross (17)
where
W h Quni , W h Kuni , W h Vuni , W h Vpred , W h Qpred , W h
Kpred , W Ocross are the learnable weight matrices.y cross denotes the cross-domain embeddings.Implementing the hierarchical attention mechanism facilitates the structured integration of information from different modalities.This mechanism employs multihead attention method, using multiple sets of learned weight matrices to emphasize various aspects or relationships within the data.Consequently, this approach has the potential to foster robust and enriched embeddings capable of capturing complex patterns.Additionally, it aids in focusing on contextually pertinent information and achieving semantic alignment across different embeddings, thereby enhancing the capacity to identify and utilize crucial features in the input data.Output Layer: We then utilize a transformer decoder [28] to generate chemical SMILES representations character by character, using cross-modal embeddings (y cross ) that incorporate global context through the hierarchical multi-head self-attention mechanism.We implement a softmax layer to transform the decoder's output, creating a probability distribution over potential elements for each position in the SMILES strings.For our sequence generation tasks, we minimize the categorical cross-entropy loss to penalize the proposed framework based on the negative log-likelihood of the ground-truth chemical SMILES strings under the predicted probability distribution, thus facilitating the generation of valid molecules.In summary, by integrating multi-modal embeddings, namely y org , y exp , and y pred , our approach enables the concurrent capture of complementary information, ultimately enhancing the overall performance of the framework.</p>
<p>Experiments &amp; Results</p>
<p>Datasets &amp; Baselines</p>
<p>Our study utilized the ChEBI-20 dataset [8], a bidirectional text-to-molecule translation dataset comprising 33,010 text description-molecule pairs with a predefined split ratio of 80:10:10 for training, validation, and test sets, respectively.We utilized 26,407 text description-molecule pairs from the training set for demonstrations (input-output mappings) for constructing a knowledgeaugmented prompt to query LLMs.We used the MolT5 model [8], as a predominant baseline, which is an encoder-decoder transformer architecture pretrained on a large unannotated dataset specifically for the text2mol translation task, building upon the foundations of the T5 [18] model.We evaluated the performance of our proposed framework on the text2mol task, comparing it with several variants of the MolT5 [8] and T5 [18] models, as well as with general-purpose sequence-to-sequence models, such as the RNN-GRU and Vanilla Transformer models.In addition, various variants of few-shot (ICL) prompting of GPT-based models -reflecting the fact that this technique uses few-shot learning to prompt off-the-shelf GPT-based models to perform molecular property prediction for new, unseen molecules -referred to as baselines, are evaluated for comparison with our proposed framework.The configurations include different variants of the GPT-4 model, namely, (a) the zero-shot approach, (b) the scaffold sampling technique with K=10 or K=5, and (c) the random sampling technique with K=10 for constructing augmented prompts.In addition, we use GPT-3.5 and davinci-003 models, both employing the scaffold sampling technique with K=10 to construct knowledge-augmented prompts.For more details and information on the baselines, please refer to the earlier works [10,8].</p>
<p>Evaluation Metrics</p>
<p>To comprehensively evaluate the quality and similarity of the generated chemical SMILES representations compared to the ground-truth SMILES representations, we employed a range of distinct evaluation metrics, categorized into three types.These metrics include (a) chemical similarity measures, such as the FTS (Fingerprint Tanimoto Similarity) [26] and the FCD (Fréchet ChemNet Distance) [16], as well as (b) natural language processing metrics like the BLEU (Bilingual Evaluation Understudy) score, Exact Match [8], and Levenshtein distance [14].In addition, (c) we utilized the RDKit library [13] to validate the generated molecules.We delineate the metrics as follows: (a) We vi employ the FTS [26] metric to gauge the chemical similarity between the ground-truth and generated chemical compounds represented as SMILES strings(notation to represent chemical structures as text) by comparing their MACCS, RDK, and Morgan fingerprints [21,13,4].(b) In addition, we utilize the FCD metric [16], which leverages latent information from a pretrained model [16] to predict molecular activity [8].The FCD is calculated by measuring the distance between the mean embeddings of two sets of chemical SMILES strings (generated and ground-truth) in the latent space of the pretrained model.A lower FCD score indicates a greater similarity between the corresponding two sets of molecules.(c) We also apply natural language processing metrics to evaluate the quality of the chemical SMILES strings generated by our framework.These metrics encompass the following: (i) BLEU -this measures the similarity between two text strings, with a higher BLEU score denoting better similarity.(ii) Exact Match [8] -this quantifies the percentage of generated chemical SMILES strings that are identical to the ground-truth strings.(iii) Levenshtein distance [14] -this calculates the minimum number of single-character edits required to modify the generated chemical SMILES strings to match the ground-truth strings, with a lower value indicating closer similarity.By utilizing these diverse metrics, we attain a nuanced understanding of the efficacy of our text-conditional de novo molecule generation framework.Higher FTS scores and lower FCD scores signify better chemical similarity and closer molecular activity resemblance, respectively.In the context of matching chemical SMILES strings from the perspective of natural language processing, higher BLEU and Exact Match scores are preferred to achieve better alignment with the ground-truth SMILES strings, while a lower Levenshtein distance indicates fewer required edits, denoting superior similarity.The RDKit library assists in verifying the validity of the generated molecules, with a higher proportion indicating successful generation [13].</p>
<p>Experimental Setup</p>
<p>We used the ChEBI-20 dataset [9] with an 80:10:10 split: 80% for training, 10% for validation, and 10% for testing.The training set was utilized to update the learnable parameters, the validation set to select optimal hyperparameters, and the test set to evaluate generalization performance of the proposed framework.Our scalable and efficient framework offers a unified solution for integrating LLMs and LMs.We configured the hyperparameters of our framework with a batch size of 32, trained it for 100 epochs, and a hidden or embedding dimension(d) of 128.Additional hyperparameters include the number of attention heads (H) set to 4, and the dimensionality of Key/Query/Value (d h ) is 32.To optimize the training process, we utilized the Adam optimizer [12], initially setting the learning rate to 1e −3 .Additionally, we incorporated a learning rate decay scheduler, which reduced the learning rate by half whenever the validation loss did not improve for 10 consecutive epochs.Furthermore, we applied early stopping to prevent overfitting on the validation data.We evaluated our approach using the following LLMs: GPT-4.0,GPT-3.5-turbo,GPT-3.0-text-davinci-003, and Google Bard.In our approach, we chose not to fine-tune hyperparameters individually for each LLM, opting instead to maintain consistent settings across all language models.This strategy simplifies experimentation, ensures uniform conditions, facilitates result comparison, and promotes consistency.Moreover, it underscores the versatility of our framework, which can be used with any off-the-shelf LLM without the need for computationally expensive hyperparameter tuning.We used the Scaffold technique with K=16 to sample demonstrations (input-output mappings) from the training data to construct augmented prompts for querying LLMs in few-shot settings.In addition, we query LLMs to generate the top-R ranked chemical SMILES strings predictions list and set the hyperparameter R as 4. To maximize computational resource utilization, we harnessed eight V100 GPUs, each equipped with 8 GB of GPU memory, for training deep learning models built upon the PyTorch framework.Considering the context length limitations imposed by LLMs, which restrict the maximum sequence length that a typical LLM can process at a time to 4096 tokens, we implemented strategies to mitigate the high computational costs associated with prompting LLMs.This approach included running each experiment twice and reporting the average results.Our approach prioritizes both resource optimization and accuracy, aiming to achieve the best possible outcomes while minimizing the computational footprint.Our evaluation incorporated several metrics, and we present the results for the test datasets and compare the performance against well-known baselines.</p>
<p>Results</p>
<p>The experimental results of the proposed framework and the baseline models performance on the text2mol task are presented in Tables 2 and 3.The results of the baseline models are reported from earlier studies [10,8].The results undeniably demonstrate the superior performance of the FrontierX: LLM-MG framework, especially when combined with the GPT-4 backbone and employing the Scaffold technique with K set to 16.This optimal combination excels in generating accurate molecular structures that closely resemble the ground truth, surpassing all baseline models across On the ChEBI-20 dataset [9], we observe differing impacts on framework performance when certain methods are omitted.The "w/o y exp " variant shows a substantial decline in performance relative to the baseline, as evidenced by a significant drop of 17.21% in MACCS FTS, 16.43% in BLEU, and 13.12% in Validity.Similarly, the "w/o y org " variant performs much worse than the baseline, with a remarkable drop of 20.69% in MACCS FTS, 20.91% in BLEU, and 16.00% in Validity.In contrast, the "w/o y pred " variant exhibits a marginally inferior performance compared to the baseline, with a modest drop of 8.44% in MACCS FTS, 11.63% in BLEU, and 6.56% in Validity.The significant drop in performance metrics for the ablated variants, when compared to the baseline, highlights the considerable impact of the mechanisms inherent in the methods omitted from the baseline and leads to degraded performance.Our experiments corroborate our hypothesis of joint optimization to obtain a cross-modal embeddings, y cross , through a hierarchical multi-head attention mechanism that integrates the original text-level embeddings y org , explanatory text-level embeddings y exp , and prediction embeddings y pred , achieving state-of-the-art (SOTA) performance on the text2mol task</p>
<p>Conclusion</p>
<p>In this study, we pioneered the text2mol approach, inaugurating a transformative paradigm where chemistry meets language models, expediting scientific advancements.Through the creation of FrontierX: LLM-MG, we demonstrated the efficacy of using large language models for seamless and efficient translation between textual descriptions and chemical SMILES representations.Acknowledging the limitations of current methods, our research highlights a promising horizon in molecule design, potentially ushering in an era of accelerated innovation and interdisciplinary collaboration.Our study illustrates the transformative impact of integrating molecular design with language models, offering an innovative approach to molecule generation that can catalyze groundbreaking developments in science and technology.</p>
<p>Technical Appendix</p>
<p>Study of Knowledge-Augmented Prompting</p>
<p>In our study, we employ knowledge-augmented prompting with LLMs for text-tomolecule(text2mol) translation task by leveraging the pre-existing knowledge embedded within the language model parameters.LLMs are capable of generating chemical SMILES representations from textual descriptions through entity recognition, grammar understanding, symbol mapping, and structure validation, which marks significant progress in molecule generation via language models.This knowledge-augmentation prompting technique allows LLMs to adapt to new, unseen molecule textual descriptions using a few task-specific demonstrations, thereby eliminating the need for fine-tuning with labeled data for task-specific adaptation.The approach involves creating knowledge-augmented prompts that combine task-specific instructions with demonstrations (input-output pairs) sampled from training data relevant to the target molecule textual descriptions determined using off-the-shelf semantic similarity techniques.In this context, each pair consists of a textual description of a molecule (input) and its corresponding SMILES representation (output), where the task-specific instruction is to convert the target molecule textual descriptions into the standardized chemical SMILES notation.This approach aligns the LLM's capabilities with the text2mol task by crafting knowledge-augmented prompts that blend specific instructions with relevant demonstrations, selected based on semantic similarity.This strategic alignment facilitates accurate chemical SMILES strings generation without necessitating language model parameter updates.We have employed two sampling strategiesrandom and scaffold -to evaluate the impact of both the quality and quantity of demonstrations in the knowledge-augmented prompt, which is utilized for querying LLMs during text-based de novo molecule generation.The scaffold strategy utilizes a semantic similarity method to sample the top-K relevant text-molecule pairs, using OpenAI's text-embedding-ada-002 technique 7 .The random technique involves the arbitrary selection of K text-molecule pairs without any prior knowledge in a non-deterministic manner.The study compares the effectiveness of both strategies in enhancing the language-conditioned molecule generation task using off-the-shelf pre-trained LLMs, including Google Bard and other GPT model family variants.We conducted experiments to compare and contrast the performance of the "Random" and "Scaffold" sampling strategies, and to identify the optimal number of demonstrations.Table 5: The table shows the results of the experimental study examining the impact of both quantity and quality of demonstrations on knowledge-augmented prompting strategies in the text2mol task.Results: Table 5 presents the results of the experimental study that examined the effects of both the quantity and quality of demonstrations on the performance of knowledge-augmented prompting strategies in the text2mol task.Our study compared the performance of various GPT models with that of Google Bard on the ChEBI-20 dataset [9].The results indicated that the GPT models consistently outperformed Google Bard across all evaluation metrics when provided with the same number of task-specific demonstrations in the augmented prompt.Notably, GPT-4 demonstrated the highest performance among the tested models, generating a greater number of valid chemical SMILES representations.Furthermore, the study indicates that enhancing the knowledge-augmented prompt with more task-specific demonstrations directly improves the predictive accuracy of language models.This highlights a positive correlation between the number of task-specific demonstrations and the performance of LLMs on the text2mol task.The study found that scaffold sampling consistently outperforms random sampling on the text2mol task when using any off-the-shelf LLMs.One possible reason for this superior performance is the strong textual similarities between the text-molecule pairs sampled using the scaffold technique and the target molecule descriptions.Therefore, using scaffold sampling instead of random sampling may lead GPT models to generate more accurate chemical SMILES representations.LLMs continue to face challenges in precisely interpreting molecular representations in chemical SMILES notations, resulting in poor performance on text2mol tasks.SMILES representations can possess multiple valid forms and implicit hydrogen atoms, causing ambiguity and presenting difficulties for LLMs.Improved LLMs capable of handling molecular structures and seamlessly integrating with tools like RDKit are necessary 5.2 Impact of Hierarchical Multi-Head Attention(HMHA) Mechanism</p>
<p>In our work, we compute the cross-modal embedding, denoted as y cross , through a hierarchical multi-head attention (HMHA) mechanism that integrates the original text-level embedding (y org ), explanatory text-level embedding (y exp ), and prediction embedding (y pred ).To determine the impact of the HMHA mechanism on the framework performance, we conducted ablation study.We refer to the ablated variant without the HMHA mechanism as "w/o HMHA".We substitute the HMHA mechanism with dual-stage linear operators in the ablated variant to compute cross-modal embeddings.</p>
<p>The findings of the ablation study are summarized in Table 6.We conducted the experiment using the FrontierX: LLM-MG framework with GPT-4 backbone, where we replaced the HMHA mechanism with linear operators, as discussed earlier.The experimental results support the inclusion of the hierarchical multi-head attention mechanism (HMHA) to generate cross-modal embeddings, aiding in the generation of more valid chemical SMILES representations in the text2mol task.</p>
<p>Table 6: The table shows the experimental findings of the study on the impact of the HMHA mechanism on the text2mol task.The experiments were conducted using the FrontierX: LLM-MG framework with a GPT-4 backbone.We utilized the Scaffold sampling technique with K = 16 for constructing augmented prompts.</p>
<p>representative benchmark dataset.We report the results for the near-optimal combinations of the hyperparameters.</p>
<p>Molecule captioning</p>
<p>Molecule is a crucial task in the field of computational chemistry, serving as a bridge between complex chemical data and human comprehension.It involves generating detailed and correct textual descriptions that accurately describe a chemical SMILES representation in the mol2text task.This stands in contrast to the text2mol task, which entails generating chemical SMILES representations from detailed and factual textual descriptions.Meanwhile, the mol2text task helps to translate complex chemical structures into understandable language, enhancing our understanding of molecules with potential applications spanning multiple fields, including drug discovery, materials science, and chemical synthesis.To evaluate the quality of the generated text in the mol2text task, we employ traditional metrics commonly used in natural language processing and machine translation, including BLEU, ROUGE, and METEOR, as described below:</p>
<p>• BLEU-2 and BLEU-4 are part of the BiLingual Evaluation Understudy (BLEU) metric family.BLEU is typically computed for different n-gram levels, where 'n' represents the number of contiguous words or tokens considered.BLEU-2 evaluates the accuracy of two-word phrases (bigrams) in generated text, while BLEU-4 extends this analysis to four-word sequences (4-grams).These metrics offer insights into the alignment between machine-generated and human reference texts.The BLEU metric variants help quantify the performance of language generation-based NLP models.</p>
<p>• ROUGE-1 and ROUGE-2 are part of the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metric family.ROUGE-1, also known as ROUGE unigram or ROUGE-N1, evaluates the overlap between generated and reference text at the unigram (single-word) level, assessing specific word choices.In contrast, ROUGE-2 (also known as ROUGE bigram or ROUGE-N2) extends this to evaluate consecutive word pairs (bigrams), offering comprehensive insights into content matching.These metrics measure alignment with reference texts at both the word and bigram levels, providing precision and recall evaluations of textual elements.</p>
<p>• ROUGE-L, or Recall-Oriented Understudy for Gisting Evaluation -Longest Common Subsequence, measures the quality of machine-generated text by considering the longest common subsequence between the generated text and a reference text.This subsequence represents a sequence of words that appear in the same order in both the generated and reference texts, allowing flexibility in word order.ROUGE-L assesses content overlap and structural similarity, capturing the core content and organization of generated text concerning the reference text, even when there are variations in wording or word order.</p>
<p>• METEOR (Metric for the Evaluation of Translation with Explicit Ordering) considers precision, recall, stemming, synonymy, and word order, offering a well-rounded evaluation of text quality by analyzing matching words and their order, providing detailed assessments for translation and captioning models.</p>
<p>FrontierX: LLM-MG -mol2text task</p>
<p>We have modified the FrontierX: LLM-MG pipeline for the text2mol task to adapt it for the mol2text task.The workflow of the proposed approach is illustrated in Figure 2. Given a chemical SMILES representation, it can generate the technical descriptions of the molecule.We construct a knowledge-infused prompt using task-specific instructions and a few demonstrations (input-output mappings) for the downstream mol2text task.The task-specific instructions involve translating chemical SMILES representations into their corresponding technical descriptions.The primary objective of the prompt engineering method is to enhance the context-awareness of language models and improve their ability to provide relevant and accurate responses.This enhancement is achieved through learning from demonstrations, rather than relying on conventional supervised learning methods of fine-tuning with labeled data for the mol2text task.The knowledge-infused prompts guide the language models to generate technical descriptions based on the provided instructions.Next, we fine-tune small-scale, pre-trained language models (LMs) using the generated explanations, which facilitates domain customization and yields context-aware token embeddings.To create a text-level embedding that encapsulates the generated technical descriptions, we utilize a weighted sum-pooling attention mechanism on the contextualized embeddings.Additionally, the unimodal encoder, which is implemented with a multi-head attention mechanism, integrates the mono-domain task-specific adaptation in downstream tasks [2]).This approach allows LLMs to acquire knowledge through analogies, relying on a limited set of input-output mappings (demonstrations) tailored to the specific downstream task.Knowledge-infused prompting harnesses the implicit knowledge embedded in pretrained LLM parameters to facilitate adaptation to new tasks via task-specific demonstrations, all without necessitating parameter updates.The Knowledge-Infused prompt provides task-specific instructions and demonstrations, allowing LLMs to generate outputs conditioned on the prompt for improved generalization performance.In the case of mol2text tasks, we construct a knowledgeinfused prompt using a few demonstrations sampled from the training data.To examine how the quality and quantity of task-specific demonstrations impact performance on mol2text tasks, we investigate two different sampling strategies.The quality of examples is determined by the retrieval techniques employed to select the top-K demonstrations (chemical SMILES strings-text data pairs) from the training set that match the query chemical SMILES representations.We explore two distinct sampling strategies: 'Random' and 'Scaffold'.To study the impact of the quantity of demonstrations on the framework's performance on the mol2text task, we optimize the number of demonstrations (K) used to construct the augmented prompt for each query chemical SMILES representation.In the 'Random' strategy, we randomly sample K demonstrations from the training data.In contrast, the 'Scaffold' strategy uses Tanimoto similarity [26] based on Morgan fingerprints [15] with a radius of 2 to identify the top-K most similar chemical SMILES representations from the training data for query chemical SMILES representations.We explore the different sampling strategies to analyze the impact of the quality of demonstrations on the mol2text task with a hypothesis that the 'Scaffold' sampling technique outperforms the 'Random' technique for the same number of demonstrations.In summary, our goal is to task LLMs with a knowledge-infused prompt that consists of a few demonstrations for the mol2text task, along with task-specific instructions, where the output is technical descriptions of the query chemical SMILES representation.The task-specific instruction in the augmented prompt guides LLMs to generate technical descriptions.This task showcases the LLM's capacity to generate textual descriptions via prompt conditioning, relying on its inherent knowledge, without requiring parameter updates, in contrast to supervised learning, which relies on labeled data for parameter updates.Tables 8 and 9 present the experimental findings on the ChEBI-20 benchmark dataset [9].We report the baseline results from earlier studies [10,8].The best performing model is in bold font.</p>
<p>Ablation Studies</p>
<p>Our proposed framework operates in a structured, multi-step pipeline.In step (a), we create knowledge-augmented prompts using task-specific instructions and demonstrations, prompting large language models (LLMs) to generate textual descriptions.In step (b), we use these generated explanations to fine-tune a smaller, pre-trained language model (LM exp ) for domain-specific customization, resulting in context-sensitive token embeddings.We employ a weighted sum-pooling attention mechanism for task-specific adaptation to compute text-level embeddings, denoted as y exp , from the contextualized token embeddings.In parallel, in step (c), we fine-tune another small-scale language model (LM org ) on query chemical SMILES representations, computing an entire chemical SMILES string embeddings y org .In step (d), our framework obtains a unimodal embedding, y uni , through a multi-head attention mechanism that integrates the original text-level embeddings y org and descriptive text-level embeddings y exp .In the final step, the transformer decoder generates the textual descriptions of the query chemical SMILES string from the unimodal embedding, y uni .Our empirical research aims to elucidate the significance and unique contributions of each method within our proposed framework, particularly in assessing the effectiveness of their learned embeddings for achieving optimal results on the mol2text task.Ablation studies have been conducted to investigate the impact of disabling individual methods on our framework's overall performance in the mol2text task.To precisely measure the impact of each method on the framework's performance, we have generated various ablated variants by disabling individual methods and assessed their performance using a benchmark dataset across multiple evaluation metrics for the mol2text task.We choose the FrontierX: LLM-MG framework as the reference baseline for ablation studies in the context of the mol2text task.Our comprehensive approach not only confirms the effectiveness of various methods but also provides substantial support for their design choices, reinforcing their rationale and justifying their inclusion in the framework.The ablated variants without the descriptive text-level embeddings and the original text-level embeddings are denoted as 'w/o y exp ' and 'w/o y org ', respectively.The findings of the ablation study are summarized in Table 10.All ablation experiments were conducted with the FrontierX: LLM-MG W/GPT-4 framework using the Scaffold sampling technique with a value of K = 16, involving the deliberate exclusion of specific methods, as previously described.On the ChEBI-20 dataset [9], the 'w/o y exp ' variant exhibits a significant decline in performance relative to the baseline, evidenced by a 10.63% drop in BLEU-2, a 14.04% drop in ROUGE-L, and a 20.93% xiv drop in METEOR.Similarly, the 'w/o y org ' variant performs much worse than the baseline, with a 20.18% drop in BLEU-2, a 25.06% drop in ROUGE-L, and a 29.06% drop in METEOR.These substantial performance drops across all evaluation metrics when comparing the ablated variants to the baseline consistently highlighting the significant impact of the mechanisms disabled from the baseline.Our experiments validate our hypothesis on joint optimization to obtain a unimodal embedding, denoted as y uni , through a multi-head attention mechanism that combines the original text-level embeddings y org and explanatory text-level embeddings y exp .This approach results in achieving state-of-the-art (SOTA) performance on the mol2text task.</p>
<p>-Scaffold, k=16, W/GPT-</p>
<p>Table 1 :
1
Technical details of LLMs and LMs.Enterprise refers to the organization that developed the language models.Cost denotes the expenses associated with using 1K tokens.Last Update Date indicates that the LLM's knowledge base is limited to information available up to that specific date.
ModelEnterpriseCostLast Update Date Vocabulary Sizetext-davinci-003Open-AI0.02$Sep. 2021175BChatGPTOpen-AI0.002$Jun. 2021175BBARDGoogleFreeUndisclosed1,560BDeBERTaHugging FaceFreeN/A50M</p>
<p>Table 10 :
10
The table shows the experimental findings on the ablation study on the mol2text task.FrontierX: LLM-MG (Scaffold, k=16, W/GPT-4) 0.743 ±0.081 0.656 ±0.097 0.818 ±0.034 0.727 ±0.013 0.783 ±0.047 0.812 ±0.051 FrontierX: LLM-MG -w/o y exp 0.664 ±0.058 0.537 ±0.071 0.675 ±0.042 0.543 ±0.035 0.673 ±0.021 0.642 ±0.075 FrontierX: LLM-MG -w/o y org 0.593 ±0.027 0.496 ±0.068 0.612 ±0.086 0.482 ±0.037 0.575 ±0.076 0.576 ±0.059
MethodBLEU-2 (↑)BLEU-4 (↑)ROUGE-1 (↑) ROUGE-2 (↑) ROUGE-L (↑) METEOR (↑)
https://platform.openai.com/docs/models/gpt-3-5
https://chat.openai.com/chat
https://bard.google.com
For more information on DeBERTa, please refer to https://huggingface.co/docs/transformers/ index[11] 
https://platform.openai.com/docs/guides/embeddings
https://platform.openai.com/docs/guides/embeddings x
various evaluation metrics.Table2: The table a performance comparison of the proposed framework and the baselines on the text2mol task.The top-performing model is highlighted in bold.The baseline results are reported from previous work[10].We leveraged the Scaffold technique, setting K to 16, to sample demonstrations and construct augmented prompts for in-context learning in all experiments involving FrontierX: LLM-MG with various off-the-shelf LLMs.Ablation StudiesOur proposed framework operates through a series of interconnected stages via a progressively structured multi-step pipeline.Beginning with step (a), we create knowledge-augmented prompts using task-specific instructions and demonstrations, prompting large language models (LLMs) to (i) generate top-ranked (top-R) SMILES strings predictions along with (ii) explanatory justifications for their predictions.In step (b), these generated explanations are used to (i) fine-tune a smaller pre-trained viii language model (LM exp ) for domain customization to obtain contextualized token embeddings and utilize (ii) a weighted sum-pooling attention to compute text-level embeddings denoted as y exp from the token embeddings for task-specific adaptation.Moving to step (c), the top-R predictions from the LLMs are transformed to compute prediction embeddings y pred .Concurrently, in step (d), we fine-tune another small-scale language model (LM org ) on the original textual descriptions of molecules to compute context-aware token embeddings, and then compute the original text-level embeddings y org through a weighted attention mechanism.In step (e), our proposed framework obtains a cross-modal embeddings, y cross , through a hierarchical multi-head attention mechanism that integrates the original text-level embeddings y org , explanatory text-level embeddings y exp , and prediction embeddings y pred .We conduct empirical research to understand the significance and contribution of each distinct method within the proposed framework, evaluating its learned embeddings to achieve optimal results.We perform ablation studies to assess the impact of disabling individual methods on the overall performance of our framework.To determine the contribution of each method to the framework's performance, we create various ablated variants by disabling individual methods and evaluate them using benchmark datasets for text2mol tasks.We choose the proposed FrontierX: LLM-MG framework as the reference baseline for the ablation studies.Our robust strategy not only validates the efficacy of the diverse methods but also substantiates the rationale, providing a strong basis for their design choices and justifying their inclusion within the framework.The ablated variants without the explanatory text-level embeddings, prediction embeddings, and original text-level embeddings are referred to as "w/o y exp ", "w/o y pred ", and "w/o y org ", respectively.The ablation study findings are summarized in Table4.All the ablation study experiments were conducted with the FrontierX: LLM-MG framework using the GPT-4 backbone and Scaffold sampling technique with K = 16, by disabling certain methods as discussed earlier.Hyperparameter TuningTo enhance the performance of our FrontierX: LLM-MG framework, we embarked on meticulous hyperparameter tuning through detailed experimentation and analysis.We chose random search as a strategy to adeptly navigate the hyperparameter space, pinpointing the optimal framework configuration on the benchmark dataset, in lieu of more computationally intensive approaches such as grid search or Bayesian optimization.This strategy enabled us to obtain optimal results on the validation subset of the benchmark dataset, as evidenced by several evaluation metrics.We conducted hyperparameter optimization on the FrontierX: LLM-MG-W/GPT-4 variant of our framework.We utilized the Scaffold sampling technique with K = 16 for constructing augmented prompts.The primary key hyperparameters within this framework include batch size (b ∈ {32, 48, 64}) and the embedding dimension (d ∈ {64, 128, 196, 256}).Table7presents the results of hyperparameter tuning on the chemical SMILES representations with explanatory text-level embeddings to compute unimodal embeddings.Finally, the transformer decoder generates the technical descriptions that to the input chemical SMILES representations[28].Instead of repurposing large language models (LLMs) by either retraining them from scratch or fine-tuning them with labeled data for domain customization, we employ LMaaS[25]to engage with LLMs through text-based API interactions.LLM LMSMILESLLM PromptingKnowledge-infused LLM prompting, a method of prompt engineering, involves crafting effective prompts or input queries to elicit desired responses from language models.This technique enhances language models by combining their natural language understanding and generation capabilities with access to external factual information(demonstrations), making them more versatile for taskspecific applications.Consequently, it enables the LLM to generate responses enriched with accurate, contextually relevant information.Knowledge-infused prompting enables LLMs to adapt to new tasks without the need for explicit, gradient-based fine-tuning with gold-standard annotated data for
. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Evaluation of gpt-3.5 and gpt-4 for supporting real-world information needs in healthcare delivery. Debadutta Dash, Rahul Thapa, Juan M Banda, Akshay Swaminathan, Morgan Cheatham, Mehr Kashyap, Nikesh Kotecha, Jonathan H Chen, Saurabh Gombar, Lance Downing, arXiv:2304.137142023arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, arXiv:2010.119292020arXiv preprint</p>
<p>Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, Heng Ji, arXiv:2204.11817Translation between molecules and natural language. 2022arXiv preprint</p>
<p>Text2mol: Cross-modal molecule retrieval with natural language queries. Carl Edwards, Chengxiang Zhai, Heng Ji, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>What indeed can gpt models do in chemistry? a comprehensive benchmark on eight tasks. Taicheng Guo, Kehan Guo, Zhengwen Liang, Zhichun Guo, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2305.183652023arXiv preprint</p>
<p>Deberta: Decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, arXiv:2006.036542020arXiv preprint</p>
<p>P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Rdkit: Open-source cheminformatics software. G A Landrum, 2020</p>
<p>Levenshtein distance: Information theory, computer science, string (computer science), string metric, damerau? levenshtein distance, spell checker, hamming distance. Frederic P Miller, Agnes F Vandome, John Mcbrewster, 2009</p>
<p>The generation of a unique machine description for chemical structuresa technique developed at chemical abstracts service. Harry L Morgan, Journal of chemical documentation. 521965</p>
<p>Fréchet chemnet distance: a metric for generative models for molecules in drug discovery. Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, Gunter Klambauer, Journal of chemical information and modeling. 5892018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Hierarchical text-conditional image generation with clip latents. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, arXiv:2204.061252022arXiv preprint</p>
<p>Zero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, International Conference on Machine Learning. PMLR2021</p>
<p>Pattern matching: The gestalt approach. David Ratcliff, John W Metzener, 1988</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, arXiv:2112.086332021arXiv preprint</p>
<p>Photorealistic text-to-image diffusion models with deep language understanding. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar, Seyed Ghasemipour, Burcu Karagol Ayan, Sara Mahdavi, Rapha Gontijo Lopes, arXiv:2205.114872022arXiv preprint</p>
<p>Make-a-video: Text-to-video generation without text-video data. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, arXiv:2209.147922022arXiv preprint</p>
<p>Black-box tuning for language-model-as-a-service. Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, Xipeng Qiu, International Conference on Machine Learning. PMLR2022</p>
<p>Elementary mathematical theory of classification and prediction. T Taffee, Tanimoto, Journal of Biomedical Science and Engineering. 1958</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>            </div>
        </div>

    </div>
</body>
</html>
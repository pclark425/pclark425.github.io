<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-623 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-623</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-623</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-0247c8299c1ead8fb2d4e5b82aeb9f1058048c87</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0247c8299c1ead8fb2d4e5b82aeb9f1058048c87" target="_blank">A Thorough Examination of Decoding Methods in the Era of LLMs</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments.</p>
                <p><strong>Paper Abstract:</strong> Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose large language models (LLMs). Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining optimal results and the practicality of implementation in varying contexts.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e623.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e623.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relative Deviation Percentage</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset-level variability metric computed as (standard deviation across decoding methods) / (mean across decoding methods) × 100%, used to quantify how much performance varies when changing decoding methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2 family (Llama2-7B, Llama2-7B-Chat, 13B, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 70B (and chat variants)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / LM decoding evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Comparative evaluation of decoding methods across multiple benchmark tasks (coding, math reasoning, summarization, translation, commonsense, factuality, instruction following, open-ended generation)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Choice of decoding method (greedy, beam, DBS, CS, FSD, FSD-d, CD, DoLa, temperature, top-p, top-k, η, mirostat, typical), model alignment (unaligned vs aligned), model scale (7B/13B/70B), quantization (FP16 vs INT8/INT4), hyperparameter choices for each method</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Relative Deviation Percentage (RDP) = (σ/μ) × 100% where μ and σ are mean and std of performance across decoding methods on a dataset</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Reported RDP examples: Llama2-7B — MBPP 25.81%, GSM8K 23.06%, Wikinews 26.83%; Llama2-13B — MBPP 21.37%, GSM8K 18.74%, Wikinews 19.95%; Llama2-70B — MBPP 15.23%, GSM8K 11.92%, Wikinews 16.02%. Aligned variants (Llama2-7B-Chat) show lower RDPs (e.g., MBPP 9.08%, GSM8K 6.25%, Wikinews 8.84%). Quantized variants show generally larger RDPs (see Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparison of performance distributions across decoding methods (mean and std → RDP); comparisons across model scale, alignment state, and quantization settings</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Model scaling reduces RDP substantially (e.g., MBPP RDP drops from ~25.8% at 7B to ~15.2% at 70B). Alignment (instruction-tuning) reduces RDP (e.g., 7B → 7B-Chat MBPP: 25.81% → 9.08%). Quantization tends to increase RDP relative to FP16 for some tasks and methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Large variability across decoding-method choice and hyperparameter settings; sensitivity to model alignment and scale; quantization-induced instability; different methods requiring dataset-specific hyperparameter tuning</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use of aligned models (instruction tuning), scaling up model size, selecting decoding methods with lower sensitivity (e.g., FSD/FSD-d, BS/DBS for chat models), evaluating and reporting RDP to quantify robustness</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Alignment and larger model sizes demonstrably reduced RDP (examples above). The paper reports lower RDPs for 13B/70B and for chat/aligned variants versus unaligned 7B.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RDP quantifies that decoding-method choice induces substantial performance variability (often >20% RDP for 7B); alignment and model scaling substantially reduce this variability, while quantization can increase it.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Thorough Examination of Decoding Methods in the Era of LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e623.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e623.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ANP / Hyperparameter Sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Average Normalized Performance (ANP) and Hyperparameter Sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ANP measures a decoding method's average performance relative to the dataset-specific best (normalized), reported for both the best per-dataset hyperparameters (ANP_best) and a single fixed hyperparameter (ANP_fix) to quantify sensitivity to hyperparameter choice.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2 family (reported primarily for Llama2-7B and Llama2-7B-Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (and chat variant); results also reported for larger scales</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Assessing robustness of decoding methods when using dataset-specific tuned hyperparameters vs a single fixed hyperparameter across datasets</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Hyperparameter choices per decoding method (temperature τ, top-p, top-k, η, penalties for CS/CD/FSD, beam width), dataset specificity of optimal hyperparameters, model and quantization differences</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>ANP_best (average normalized performance when using dataset-specific best hyperparameters) vs ANP_fix (average normalized performance when using a fixed hyperparameter across datasets); percentage drops between them</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Temperature sampling: ANP shows an 11.59% decrease from ANP_best to ANP_fix on Llama2-7B, and a 3.90% decrease on Llama2-7B-Chat. Contrastive Decoding (CD) sensitivity: ANP drop 9.42% (7B) and 3.35% (7B-Chat). FSD/FSD-d show small ANP change (robust under fixed hyperparameter).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>ANP_best vs ANP_fix and per-method average normalized performance across datasets</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Some methods (e.g., FSD, FSD-d, BS/DBS in chat models) maintain high ANP_fix indicating reproducible performance without dataset-specific tuning; other methods (e.g., temperature sampling, CD) suffer significant drops when hyperparameters are fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Necessity of dataset-specific hyperparameter search to achieve top performance; impracticality of exhaustive tuning in open-world deployments; variation of optimal hyperparameters with model scale and alignment</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Prefer decoding methods that are robust to fixed hyperparameters (FSD/FSD-d, certain deterministic methods), provide recommended default hyperparameters (practical guidelines), or perform limited tuning when feasible</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>FSD and FSD-d rank among top-3 both for ANP_best and ANP_fix, demonstrating effective mitigation of hyperparameter sensitivity; temperature sampling loses ~11.6% ANP when fixed (a measurable degradation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Hyperparameter sensitivity is a major source of variability; some methods (FSD/FSD-d, beam variants for chat models) are robust to a single fixed hyperparameter, while stochastic methods like temperature sampling can lose >10% relative performance when hyperparameters are fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Thorough Examination of Decoding Methods in the Era of LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e623.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e623.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency (majority-vote aggregation over multiple stochastic generations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where multiple stochastic generations are sampled and their outputs are aggregated (majority vote) to improve accuracy and stability; tested with 1, 5, 10, and 20 sampled generations in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B and Llama2-7B-Chat (evaluated on GSM8K and other tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (and chat variant)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / reasoning and decoding strategies</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Improve closed-ended reasoning accuracy (GSM8K) by sampling multiple stochastic generations and taking a majority vote (self-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Inherent stochasticity of sampling-based decoding (temperature, top-p, top-k, η, mirostat), randomness across sampled generations</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Accuracy as a function of number of sampled generations (1, 5, 10, 20); comparison to best deterministic method accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Accuracy improves monotonically with more sampled generations; with 20 samples most stochastic methods surpass best deterministic methods on GSM8K (exception: mirostat on Llama2-7B). Table 3 (with 20 generations) reports best GSM8K accuracies: 7B — Temperature 21.91% (τ=0.7), Top-ρ 22.06% (0.8), Typical 22.06% (0.95); 7B-Chat — Temperature 36.92% (τ=0.9), Top-k 37.68% (k=10), Mirostat 37.76% (target perplexity 5.0).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Accuracy stabilization and majority-vote consistency over multiple sampled generations</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Using 20 generations and aggregation yields higher and more stable accuracy, enabling stochastic methods to outperform deterministic baselines on closed-ended tasks; increasing randomness (higher τ or larger candidate pools) can further improve aggregated performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Computational cost of multiple generations, need to tune hyperparameters for multi-shot aggregation, and diminished returns vs cost trade-offs</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Self-consistency (multiple sampled generations + majority vote), larger randomness settings when using aggregation (higher τ or larger top-k / top-p), hyperparameter tuning optimized for aggregated runs</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Demonstrated to make stochastic methods surpass deterministic baselines on GSM8K when using 20 samples; example: temperature sampling improved from ~34.04% (τ=0.5, single-run baseline reported earlier) to 36.92% (τ=0.9, 20-sample aggregated) on Llama2-7B-Chat.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>1, 5, 10, 20 (evaluated); best reported results use 20 sampled generations</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-consistency reliably reduces output variability and improves accuracy for stochastic decoding: with ~20 aggregated samples, stochastic methods often exceed deterministic methods on closed-ended reasoning tasks, at the expense of computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Thorough Examination of Decoding Methods in the Era of LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e623.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e623.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Next-token entropy (alignment effect)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Average next-token prediction entropy (indicator of model confidence / operating space for decoding methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper computes average entropy of model next-token distributions to explain why aligned models are less sensitive to decoding methods: aligned models have lower entropy (more confident, concentrated distributions), leaving less room for decoding-method variation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B and Llama2-7B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / model calibration and decoding</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Measure next-token entropy under top-p=1.0 sampling across tasks to explain decoding-method sensitivity differences between unaligned and aligned models</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Model alignment (instruction tuning) which changes distribution sharpness and entropy, thereby affecting sensitivity to decoding choices</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Average next-token entropy (bits or nats; reported numeric values but units not explicitly defined in paper table)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Table 2 (average next-token entropy with top-p=1.0): Llama2-7B — GSM8K 1.05, MBPP 1.21, Wikinews 2.37; Llama2-7B-Chat — GSM8K 0.27, MBPP 0.39, Wikinews 0.52. Lower entropy in aligned models corresponds with reduced performance variation across decoding methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparison of average entropy values across aligned vs unaligned models and linking to RDP differences</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Aligned models have substantially lower entropy and correspondingly lower RDP, suggesting higher reproducibility (stability) of outputs across decoding methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Higher-entropy (less confident) models provide more room for decoding stochasticity leading to larger output variability; alignment reduces but does not eliminate variability</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Instruction-tuning/alignment to reduce entropy and output variability; use lower-temperature sampling for more deterministic behavior when needed</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Alignment reduced entropy by roughly an order of magnitude on GSM8K (1.05 → 0.27) and corresponded with large reductions in RDP (see RDP results).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reduced next-token entropy after alignment explains why aligned models are less sensitive to decoding choices — alignment concentrates probability mass and yields more reproducible outputs across decoding methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Thorough Examination of Decoding Methods in the Era of LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e623.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e623.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quantization sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoding-method sensitivity under model quantization (INT8 / INT4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper examines how INT8 and INT4 quantization affect the robustness of different decoding methods; quantization can increase variability (RDP) and different methods show heterogeneous sensitivity due to numerical instability in calculations involving entropy/probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13B family (FP16 vs INT8 vs INT4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / model compression and inference</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Evaluate decoding method performance and variability on MBPP, GSM8K, and Wikinews when applying INT8 and INT4 quantization</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Quantization (INT8, INT4) affecting numerical precision of logits and entropy-related calculations; interaction with decoding algorithms (especially entropy-dependent methods like η and typical sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>RDP and per-method performance differences before/after quantization; percent changes in method-specific performance</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Quantized models generally show larger RDP relative to FP16 (Tables 4 and 5). Example method-specific changes: on GSM8K, η-sampling under INT8 decreased by 13.65%, while typical sampling under INT4 improved by 6.14% (numbers taken from the paper's examples). RDPs for quantized 13B variants: INT4 MBPP 21.28%, GSM8K 19.92%, Wikinews 23.28%; INT8 MBPP 25.46%, GSM8K 23.25%, Wikinews 22.62%.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparison of method performances and RDP across FP16, INT8, INT4 quantization modes</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Quantization can increase sensitivity to decoding choices and change which decoding methods are optimal; entropy- or numerically-sensitive methods (η, typical) show noticeable performance shifts under quantization.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Numerical instability of entropy-based truncation and typicality computations under low-precision arithmetic; variation in method-specific robustness to quantization</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Prefer decoding methods less dependent on numerically unstable computations when deploying quantized models; validate decoding–quantization interactions per deployment and consider INT8/INT4 testing during evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>No single universal mitigation tested, but deterministic methods were comparatively stable across quantization; empirical validation required per-method and per-quantization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Quantization affects reproducibility and can change the ranking of decoding methods; entropy-dependent sampling methods are particularly sensitive to low-precision inference and can lose substantial performance under INT8/INT4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Thorough Examination of Decoding Methods in the Era of LLMs', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>The curious case of neural text degeneration <em>(Rating: 2)</em></li>
                <li>Comparison of diverse decoding methods from conditional language models <em>(Rating: 2)</em></li>
                <li>Contrastive Search <em>(Rating: 1)</em></li>
                <li>Contrastive Decoding: Open-ended text generation as optimization <em>(Rating: 1)</em></li>
                <li>Mirostat: a neural text decoding algorithm that directly controls perplexity <em>(Rating: 1)</em></li>
                <li>Truncation sampling as language model desmoothing <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-623",
    "paper_id": "paper-0247c8299c1ead8fb2d4e5b82aeb9f1058048c87",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "RDP",
            "name_full": "Relative Deviation Percentage",
            "brief_description": "A dataset-level variability metric computed as (standard deviation across decoding methods) / (mean across decoding methods) × 100%, used to quantify how much performance varies when changing decoding methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2 family (Llama2-7B, Llama2-7B-Chat, 13B, 70B)",
            "model_size": "7B, 13B, 70B (and chat variants)",
            "scientific_domain": "Natural Language Processing / LM decoding evaluation",
            "experimental_task": "Comparative evaluation of decoding methods across multiple benchmark tasks (coding, math reasoning, summarization, translation, commonsense, factuality, instruction following, open-ended generation)",
            "variability_sources": "Choice of decoding method (greedy, beam, DBS, CS, FSD, FSD-d, CD, DoLa, temperature, top-p, top-k, η, mirostat, typical), model alignment (unaligned vs aligned), model scale (7B/13B/70B), quantization (FP16 vs INT8/INT4), hyperparameter choices for each method",
            "variability_measured": true,
            "variability_metrics": "Relative Deviation Percentage (RDP) = (σ/μ) × 100% where μ and σ are mean and std of performance across decoding methods on a dataset",
            "variability_results": "Reported RDP examples: Llama2-7B — MBPP 25.81%, GSM8K 23.06%, Wikinews 26.83%; Llama2-13B — MBPP 21.37%, GSM8K 18.74%, Wikinews 19.95%; Llama2-70B — MBPP 15.23%, GSM8K 11.92%, Wikinews 16.02%. Aligned variants (Llama2-7B-Chat) show lower RDPs (e.g., MBPP 9.08%, GSM8K 6.25%, Wikinews 8.84%). Quantized variants show generally larger RDPs (see Table 5).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparison of performance distributions across decoding methods (mean and std → RDP); comparisons across model scale, alignment state, and quantization settings",
            "reproducibility_results": "Model scaling reduces RDP substantially (e.g., MBPP RDP drops from ~25.8% at 7B to ~15.2% at 70B). Alignment (instruction-tuning) reduces RDP (e.g., 7B → 7B-Chat MBPP: 25.81% → 9.08%). Quantization tends to increase RDP relative to FP16 for some tasks and methods.",
            "reproducibility_challenges": "Large variability across decoding-method choice and hyperparameter settings; sensitivity to model alignment and scale; quantization-induced instability; different methods requiring dataset-specific hyperparameter tuning",
            "mitigation_methods": "Use of aligned models (instruction tuning), scaling up model size, selecting decoding methods with lower sensitivity (e.g., FSD/FSD-d, BS/DBS for chat models), evaluating and reporting RDP to quantify robustness",
            "mitigation_effectiveness": "Alignment and larger model sizes demonstrably reduced RDP (examples above). The paper reports lower RDPs for 13B/70B and for chat/aligned variants versus unaligned 7B.",
            "comparison_with_without_controls": true,
            "number_of_runs": null,
            "key_findings": "RDP quantifies that decoding-method choice induces substantial performance variability (often &gt;20% RDP for 7B); alignment and model scaling substantially reduce this variability, while quantization can increase it.",
            "uuid": "e623.0",
            "source_info": {
                "paper_title": "A Thorough Examination of Decoding Methods in the Era of LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ANP / Hyperparameter Sensitivity",
            "name_full": "Average Normalized Performance (ANP) and Hyperparameter Sensitivity",
            "brief_description": "ANP measures a decoding method's average performance relative to the dataset-specific best (normalized), reported for both the best per-dataset hyperparameters (ANP_best) and a single fixed hyperparameter (ANP_fix) to quantify sensitivity to hyperparameter choice.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2 family (reported primarily for Llama2-7B and Llama2-7B-Chat)",
            "model_size": "7B (and chat variant); results also reported for larger scales",
            "scientific_domain": "NLP / evaluation methodology",
            "experimental_task": "Assessing robustness of decoding methods when using dataset-specific tuned hyperparameters vs a single fixed hyperparameter across datasets",
            "variability_sources": "Hyperparameter choices per decoding method (temperature τ, top-p, top-k, η, penalties for CS/CD/FSD, beam width), dataset specificity of optimal hyperparameters, model and quantization differences",
            "variability_measured": true,
            "variability_metrics": "ANP_best (average normalized performance when using dataset-specific best hyperparameters) vs ANP_fix (average normalized performance when using a fixed hyperparameter across datasets); percentage drops between them",
            "variability_results": "Temperature sampling: ANP shows an 11.59% decrease from ANP_best to ANP_fix on Llama2-7B, and a 3.90% decrease on Llama2-7B-Chat. Contrastive Decoding (CD) sensitivity: ANP drop 9.42% (7B) and 3.35% (7B-Chat). FSD/FSD-d show small ANP change (robust under fixed hyperparameter).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "ANP_best vs ANP_fix and per-method average normalized performance across datasets",
            "reproducibility_results": "Some methods (e.g., FSD, FSD-d, BS/DBS in chat models) maintain high ANP_fix indicating reproducible performance without dataset-specific tuning; other methods (e.g., temperature sampling, CD) suffer significant drops when hyperparameters are fixed.",
            "reproducibility_challenges": "Necessity of dataset-specific hyperparameter search to achieve top performance; impracticality of exhaustive tuning in open-world deployments; variation of optimal hyperparameters with model scale and alignment",
            "mitigation_methods": "Prefer decoding methods that are robust to fixed hyperparameters (FSD/FSD-d, certain deterministic methods), provide recommended default hyperparameters (practical guidelines), or perform limited tuning when feasible",
            "mitigation_effectiveness": "FSD and FSD-d rank among top-3 both for ANP_best and ANP_fix, demonstrating effective mitigation of hyperparameter sensitivity; temperature sampling loses ~11.6% ANP when fixed (a measurable degradation).",
            "comparison_with_without_controls": true,
            "number_of_runs": null,
            "key_findings": "Hyperparameter sensitivity is a major source of variability; some methods (FSD/FSD-d, beam variants for chat models) are robust to a single fixed hyperparameter, while stochastic methods like temperature sampling can lose &gt;10% relative performance when hyperparameters are fixed.",
            "uuid": "e623.1",
            "source_info": {
                "paper_title": "A Thorough Examination of Decoding Methods in the Era of LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Self-consistency",
            "name_full": "Self-consistency (majority-vote aggregation over multiple stochastic generations)",
            "brief_description": "A method where multiple stochastic generations are sampled and their outputs are aggregated (majority vote) to improve accuracy and stability; tested with 1, 5, 10, and 20 sampled generations in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-7B and Llama2-7B-Chat (evaluated on GSM8K and other tasks)",
            "model_size": "7B (and chat variant)",
            "scientific_domain": "NLP / reasoning and decoding strategies",
            "experimental_task": "Improve closed-ended reasoning accuracy (GSM8K) by sampling multiple stochastic generations and taking a majority vote (self-consistency)",
            "variability_sources": "Inherent stochasticity of sampling-based decoding (temperature, top-p, top-k, η, mirostat), randomness across sampled generations",
            "variability_measured": true,
            "variability_metrics": "Accuracy as a function of number of sampled generations (1, 5, 10, 20); comparison to best deterministic method accuracy",
            "variability_results": "Accuracy improves monotonically with more sampled generations; with 20 samples most stochastic methods surpass best deterministic methods on GSM8K (exception: mirostat on Llama2-7B). Table 3 (with 20 generations) reports best GSM8K accuracies: 7B — Temperature 21.91% (τ=0.7), Top-ρ 22.06% (0.8), Typical 22.06% (0.95); 7B-Chat — Temperature 36.92% (τ=0.9), Top-k 37.68% (k=10), Mirostat 37.76% (target perplexity 5.0).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Accuracy stabilization and majority-vote consistency over multiple sampled generations",
            "reproducibility_results": "Using 20 generations and aggregation yields higher and more stable accuracy, enabling stochastic methods to outperform deterministic baselines on closed-ended tasks; increasing randomness (higher τ or larger candidate pools) can further improve aggregated performance.",
            "reproducibility_challenges": "Computational cost of multiple generations, need to tune hyperparameters for multi-shot aggregation, and diminished returns vs cost trade-offs",
            "mitigation_methods": "Self-consistency (multiple sampled generations + majority vote), larger randomness settings when using aggregation (higher τ or larger top-k / top-p), hyperparameter tuning optimized for aggregated runs",
            "mitigation_effectiveness": "Demonstrated to make stochastic methods surpass deterministic baselines on GSM8K when using 20 samples; example: temperature sampling improved from ~34.04% (τ=0.5, single-run baseline reported earlier) to 36.92% (τ=0.9, 20-sample aggregated) on Llama2-7B-Chat.",
            "comparison_with_without_controls": true,
            "number_of_runs": "1, 5, 10, 20 (evaluated); best reported results use 20 sampled generations",
            "key_findings": "Self-consistency reliably reduces output variability and improves accuracy for stochastic decoding: with ~20 aggregated samples, stochastic methods often exceed deterministic methods on closed-ended reasoning tasks, at the expense of computational cost.",
            "uuid": "e623.2",
            "source_info": {
                "paper_title": "A Thorough Examination of Decoding Methods in the Era of LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Next-token entropy (alignment effect)",
            "name_full": "Average next-token prediction entropy (indicator of model confidence / operating space for decoding methods)",
            "brief_description": "The paper computes average entropy of model next-token distributions to explain why aligned models are less sensitive to decoding methods: aligned models have lower entropy (more confident, concentrated distributions), leaving less room for decoding-method variation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-7B and Llama2-7B-Chat",
            "model_size": "7B",
            "scientific_domain": "NLP / model calibration and decoding",
            "experimental_task": "Measure next-token entropy under top-p=1.0 sampling across tasks to explain decoding-method sensitivity differences between unaligned and aligned models",
            "variability_sources": "Model alignment (instruction tuning) which changes distribution sharpness and entropy, thereby affecting sensitivity to decoding choices",
            "variability_measured": true,
            "variability_metrics": "Average next-token entropy (bits or nats; reported numeric values but units not explicitly defined in paper table)",
            "variability_results": "Table 2 (average next-token entropy with top-p=1.0): Llama2-7B — GSM8K 1.05, MBPP 1.21, Wikinews 2.37; Llama2-7B-Chat — GSM8K 0.27, MBPP 0.39, Wikinews 0.52. Lower entropy in aligned models corresponds with reduced performance variation across decoding methods.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparison of average entropy values across aligned vs unaligned models and linking to RDP differences",
            "reproducibility_results": "Aligned models have substantially lower entropy and correspondingly lower RDP, suggesting higher reproducibility (stability) of outputs across decoding methods.",
            "reproducibility_challenges": "Higher-entropy (less confident) models provide more room for decoding stochasticity leading to larger output variability; alignment reduces but does not eliminate variability",
            "mitigation_methods": "Instruction-tuning/alignment to reduce entropy and output variability; use lower-temperature sampling for more deterministic behavior when needed",
            "mitigation_effectiveness": "Alignment reduced entropy by roughly an order of magnitude on GSM8K (1.05 → 0.27) and corresponded with large reductions in RDP (see RDP results).",
            "comparison_with_without_controls": true,
            "number_of_runs": null,
            "key_findings": "Reduced next-token entropy after alignment explains why aligned models are less sensitive to decoding choices — alignment concentrates probability mass and yields more reproducible outputs across decoding methods.",
            "uuid": "e623.3",
            "source_info": {
                "paper_title": "A Thorough Examination of Decoding Methods in the Era of LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Quantization sensitivity",
            "name_full": "Decoding-method sensitivity under model quantization (INT8 / INT4)",
            "brief_description": "The paper examines how INT8 and INT4 quantization affect the robustness of different decoding methods; quantization can increase variability (RDP) and different methods show heterogeneous sensitivity due to numerical instability in calculations involving entropy/probabilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-13B family (FP16 vs INT8 vs INT4)",
            "model_size": "13B",
            "scientific_domain": "NLP / model compression and inference",
            "experimental_task": "Evaluate decoding method performance and variability on MBPP, GSM8K, and Wikinews when applying INT8 and INT4 quantization",
            "variability_sources": "Quantization (INT8, INT4) affecting numerical precision of logits and entropy-related calculations; interaction with decoding algorithms (especially entropy-dependent methods like η and typical sampling)",
            "variability_measured": true,
            "variability_metrics": "RDP and per-method performance differences before/after quantization; percent changes in method-specific performance",
            "variability_results": "Quantized models generally show larger RDP relative to FP16 (Tables 4 and 5). Example method-specific changes: on GSM8K, η-sampling under INT8 decreased by 13.65%, while typical sampling under INT4 improved by 6.14% (numbers taken from the paper's examples). RDPs for quantized 13B variants: INT4 MBPP 21.28%, GSM8K 19.92%, Wikinews 23.28%; INT8 MBPP 25.46%, GSM8K 23.25%, Wikinews 22.62%.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparison of method performances and RDP across FP16, INT8, INT4 quantization modes",
            "reproducibility_results": "Quantization can increase sensitivity to decoding choices and change which decoding methods are optimal; entropy- or numerically-sensitive methods (η, typical) show noticeable performance shifts under quantization.",
            "reproducibility_challenges": "Numerical instability of entropy-based truncation and typicality computations under low-precision arithmetic; variation in method-specific robustness to quantization",
            "mitigation_methods": "Prefer decoding methods less dependent on numerically unstable computations when deploying quantized models; validate decoding–quantization interactions per deployment and consider INT8/INT4 testing during evaluation",
            "mitigation_effectiveness": "No single universal mitigation tested, but deterministic methods were comparatively stable across quantization; empirical validation required per-method and per-quantization.",
            "comparison_with_without_controls": true,
            "number_of_runs": null,
            "key_findings": "Quantization affects reproducibility and can change the ranking of decoding methods; entropy-dependent sampling methods are particularly sensitive to low-precision inference and can lose substantial performance under INT8/INT4.",
            "uuid": "e623.4",
            "source_info": {
                "paper_title": "A Thorough Examination of Decoding Methods in the Era of LLMs",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "The curious case of neural text degeneration",
            "rating": 2
        },
        {
            "paper_title": "Comparison of diverse decoding methods from conditional language models",
            "rating": 2
        },
        {
            "paper_title": "Contrastive Search",
            "rating": 1
        },
        {
            "paper_title": "Contrastive Decoding: Open-ended text generation as optimization",
            "rating": 1
        },
        {
            "paper_title": "Mirostat: a neural text decoding algorithm that directly controls perplexity",
            "rating": 1
        },
        {
            "paper_title": "Truncation sampling as language model desmoothing",
            "rating": 1
        }
    ],
    "cost": 0.02025875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Thorough Examination of Decoding Methods in the Era of LLMs</h1>
<p>Chufan Shi ${ }^{\text {a<em>, }}$, Haoran Yang ${ }^{\text {a</em> }}$, Deng Cai ${ }^{\text { }}$,<br>Zhisong Zhang ${ }^{\circledR}$, Yifan Wang ${ }^{\text {A }}$, Yujiu Yang ${ }^{\text {A }}$, Wai Lam ${ }^{\text {A }}$<br>${ }^{\text {A}}$ Tsinghua University ${ }^{\text {A }}$ The Chinese University of Hong Kong ${ }^{\text {® }}$ Tencent AI Lab {scf22,wangyifa22}@mails.tsinghua.edu.cn {hryang,wlam}@se.cuhk.edu.hk {jcykcai, zhisonzhang}@tencent.com yang.yujiu@sz.tsinghua.edu.cn</p>
<h4>Abstract</h4>
<p>Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose large language models (LLMs). Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining optimal results and the practicality of implementation in varying contexts.</p>
<h2>1 Introduction</h2>
<p>The advent of large language models (LLMs) (OpenAI, 2022, 2023; Touvron et al., 2023a,b, inter alia) has ushered in a new era of natural language processing (NLP). These models are trained to predict the next token on massive corpora, empowering them with extraordinary multitasking capabilities. This enables them to perform almost all NLP tasks through the lens of text generation, distinguishing them from traditional task-specific models.</p>
<p>Decoding methods, which are the bridge between next-token predictors and text generators, play an integral role in transforming LLMs into practical task solvers. Recent studies have shown</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>that the choice of decoding methods can substantially impact the performance of LLMs (O'Brien and Lewis, 2023; Chuang et al., 2023). However, these studies often focus on a narrow aspect (e.g., factuality (Chuang et al., 2023)) and a limited set of similar tasks (e.g., math problem solving ( Li et al., 2023b)). Notably, Ippolito et al. (2019); Wiher et al. (2022) provide a comparative analysis of various decoding methods using task-specific language models. They find that deterministic decoding methods (e.g., beam search) perform better than stochastic decoding methods (e.g., top- $p$ sampling (Holtzman et al., 2020)) in closed-ended generation tasks such as machine translation, while the inverse is true for open-ended generation tasks such as story generation. However, their findings are confined to traditional task-specific models prior to the advent of LLMs. It is uncertain whether their conclusions still hold for general-purpose LLMs. In addition, a plethora of new decoding methods (Su et al., 2022; Li et al., 2023b; Yang et al., 2024; Meister et al., 2023; Hewitt et al., 2022; Basu et al., 2021) have been proposed afterward, each claiming to outperform the previous state-of-the-art in particular tasks. Nevertheless, today's most performant LLMs such as ChatGPT and GPT4 (OpenAI, 2022, 2023) only provide APIs for temperature and top$p$ sampling, seemingly overlooking the potential benefits of other advanced decoding methods.</p>
<p>The above observations raise a natural question: what is the best practice for choosing decoding methods in the era of LLMs? A thorough analysis of decoding methods is essential for researchers and practitioners to understand the strengths and weaknesses of different decoding methods and to choose the one that best fits their needs. Our work fills this gap by providing a comprehensive study of the performance, robustness, and speed of various decoding methods across a wide range of different tasks, models, and deployment environments. We also provide in-depth analyses to uncover the un-</p>
<p>derlying reasons for the observed results. Our key findings include the following:</p>
<ul>
<li>Overall The optimal decoding method depends on the task, the model, and the priority (e.g., performance vs. robustness vs. speed) in hand. There is no short guideline. The complexity of our results calls for more comprehensive evaluations in future research on decoding methods and careful consideration for practitioners.</li>
<li>Performance The best-performing methods depend on the task at hand. However, some general rules about the divide between different decoding methods still persist in the era of LLMs. Generally, closed-ended tasks favor deterministic methods, while open-ended tasks prefer stochastic methods (§4.1), especially with unaligned models. The performance gap between different decoding methods can be narrowed with alignment. We also provide explanations to understand these phenomena. Moreover, it is also observed that stochastic methods with self-consistency can surpass deterministic ones, albeit requiring multiple runs (§5.1).</li>
<li>Robustness The optimal hyperparameters for each decoding method vary according to the model, task, and quantization setting. Some methods achieve superior performance at the cost of exhaustive dataset-specific hyperparameter searches but fail to maintain the superiority when the hyperparameter is fixed. This highlights the performancesensitivity trade-off because LLMs are often confronted with diverse user prompts (§4.2).</li>
<li>Speed Stochastic decoding and the recently proposed deterministic method, frustratingly simple decoding (FSD) (Yang et al., 2024), can achieve a similar decoding speed to greedy search. In contrast, beam search, diverse beam search and other advanced deterministic methods show markedly slower speeds relative to greedy search, with the discrepancy in speed becoming more conspicuous as the length of generation increases for some of those methods (§4.3).</li>
</ul>
<h2>2 Decoding Methods</h2>
<p>Modern LLMs typically generate text in a left-toright, token-by-token fashion. For each prefix, the model computes a probability distribution of the next token over a fixed vocabulary. A decoding method defines how the generated token sequence is derived from these probability estimations. We consider decoding methods ranging from deterministic to stochastic. Each method is briefly reviewed
below, with detailed descriptions in Appendix A. The hyperparameter search range of each method is guided by recommendations from relevant literature and common practices.</p>
<h3>2.1 Deterministic Methods</h3>
<p>Greedy Search selects the token with the highest probability at each time step.
Beam Search (BS) (Freitag and Al-Onaizan, 2017) maintains a beam of the $k$ most probable sequences at each time step, where the hyperparameter $k$ is referred to as the beam width. We consider beam sizes 4 and 8 in our experiments.
Diverse Beam Search (DBS) (Vijayakumar et al., 2018) is a variant of beam search that divides the $k$ most probable sequences into $G$ groups and incorporates a diversity term to maximize intergroup diversity. In our experiments, we configure various $(k, G)$ pairs of $(4,2),(4,4),(8,2),(8,4)$.
Contrastive Search (CS) (Su et al., 2022) uses a look-ahead mechanism and penalizes tokens compromising the isotropy of the LM's latent space. We search the penalty degree from $[0.1,0.2,0.3,0.4,0.5,0.6]$ in our experiments.
Contrastive Decoding (CD) (Li et al., 2023b) searches for tokens that maximize the probability difference between the LLM and a weaker amateur model. We search the the strength of the amateur penalty from $[0.1,0.3,0.5,0.7,0.9]$.
Frustratingly Simple Decoding (FSD) (Yang et al., 2024) exploits the contrasts between the LLM and an auxiliary anti-LM constructed based on the current prefix. There are two variants of FSD: FSD and FSD-d depending on whether the anti-LM is implemented as a vectorized or discrete $n$-gram model. We search penalty degree from $[0.1,0.2,0.3,0.4,0.5,0.6]$.
DoLa (Chuang et al., 2023) obtains the nexttoken distribution by contrasting the logits differences between the last layer and a premature layer. The premature layer is dynamically selected from a pre-specified set of layers. Following Chuang et al. (2023), we test two sets of layers: even-numbered layers from $[0,16)$ and from $[16,32)$ respectively.</p>
<h3>2.2 Stochastic Methods</h3>
<p>Temperature Sampling samples tokens from the estimated next-token distributions. The skewness of distributions can be controlled using a temperature hyperparameter $\tau$. We conduct our experiments for $\tau$ within the range of 0.1 to 0.9 , incrementing in value of 0.1 .</p>
<p>Top- $p$ Sampling (Holtzman et al., 2020) only considers the minimal set of most probable tokens that cover a specified percentage $p$ of the distribution. We examine across various $p$ thresholds, specifically $[0.8,0.85,0.9,0.95,1]$.
Top- $k$ Sampling (Fan et al., 2018) only samples from the top- $k$ probable tokens. We explore a range of $k$ values, specifically $[5,10,20,50,100]$.
$\eta$-Sampling (Hewitt et al., 2022) truncates words whose probabilities are below an entropydependent threshold. The hyperparameter $\eta$ is searched from [3e-4,6e-4,9e-4,2e-3,4e-3].
Mirostat Sampling (Basu et al., 2021) directly controls the perplexity rate of the generated text during sampling from top- $k$ tokens ( $k$ is determined automatically). We test across a range of log of perplexity values $\tau$ within $[2.5,3,4,5]$.
Typical Sampling (Meister et al., 2023) sorts the vocabulary according to the differences between the distribution entropy and the token probabilities. In our experiments, we vary the coverage threshold $p$ across the values $[0.2,0.9,0.92,0.95]$.</p>
<h2>3 Evaluation Setup</h2>
<h3>3.1 Datasets</h3>
<p>Our evaluation spans a variety of tasks.
Coding is an important application of LLMs, facilitating the integration with external tools. We use HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), reporting pass@1 accuracy.
Math Problem Solving is critical for LLMs, enabling them to aid users in numerical reasoning tasks. We employ GSM8K (Cobbe et al., 2021) for this purpose and report accuracy.
Summarization assists users in capturing the essence of a text. We use CNN/DailyMail (CNN/DM) (Hermann et al., 2015) and XSUM (Narayan et al., 2018), measuring performance with RougeL (Lin, 2004).
Translation is a crucial NLP task to overcome linguistic barriers, thereby facilitating global communication. We benchmark it using four directions of WMT22 (Bojar et al., 2017) and assess the translation quality via BLEU (Papineni et al., 2002).
Commonsense Reasoning is a key perspective of LLMs for addressing real-world problems. We assess this using CommonsenseQA (CQA) (Talmor et al., 2019) and StrategyQA (SQA) (Geva et al., 2021), reporting accuracy.</p>
<p>Factual Knowledge is crucial for fulfilling users' informational needs. We measure this using FActScore (Min et al., 2023), reporting on the proportion of correctly generated atomic facts.
Instruction Following reflects the proficiency in responding to diverse user instructions. We use AlpaceEval (Li et al., 2023c) to compare model performances, using pairwise Win Rate against the reference model, Text-Davinci-003.
Open-ended Text Generation measures the model's capability to produce fluent and coherent content. We utilize datasets including Book (Zhu et al., 2015), Wikinews ${ }^{1}$, and Wikitext (Merity et al., 2017), and evaluate using MAUVE (Pillutla et al., 2021). Notably, open-ended text generation is the primary focus for many recent decoding methods.</p>
<p>For detailed task descriptions and prompts, see Appendix B and Appendix C. Generally, higher scores in respective metrics indicate better performance.</p>
<h3>3.2 Models</h3>
<p>We primarily experiment with the Llama-2 family, comprising Llama2 and Llama2-chat (Touvron et al., 2023b), representing unaligned and aligned models, respectively. Additional tests include other popular LLMs: MPT (Team, 2023), CodeLlama (Rozière et al., 2023), Qwen (Bai et al., 2023), Mistral (Jiang et al., 2023), DeepseekMoE (Dai et al., 2024) and Llama3 (AI@Meta, 2024), along with their aligned counterparts are detailed in Appendix D. Unaligned models are not tested on AlpaceEval and FActScore due to their limited instruction-following capabilities. Owing to poor performance for WMT22 with Llama2Chat, its performance is measured only on the unaligned model. Unless otherwise specified, we employ half-precision (FP16) for model inference.</p>
<h2>4 Experimental Results</h2>
<p>We perform a thorough evaluation of various decoding methods, assessing them from three critical dimensions. Initially, our analysis centers on the efficacy of these methods across a diverse range of tasks and models. Then, we delve into hyperparameter sensitivity and decoding efficiency.</p>
<h3>4.1 Performance Analysis</h3>
<p>We present the performance of decoding methods on unaligned and aligned Llama2-7B mod-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Deterministic Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Stochastic Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Greedy</td>
<td style="text-align: center;">BS</td>
<td style="text-align: center;">DBS</td>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">FSD</td>
<td style="text-align: center;">FSD-d</td>
<td style="text-align: center;">CD</td>
<td style="text-align: center;">DoLa</td>
<td style="text-align: center;">Temp</td>
<td style="text-align: center;">Top- $p$</td>
<td style="text-align: center;">Top-k</td>
<td style="text-align: center;">$\eta$</td>
<td style="text-align: center;">Miro</td>
<td style="text-align: center;">Typical</td>
</tr>
<tr>
<td style="text-align: center;">$\stackrel{\text { Q }}{m}$</td>
<td style="text-align: center;">HumanEval</td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">12.80</td>
<td style="text-align: center;">15.24</td>
<td style="text-align: center;">15.24</td>
<td style="text-align: center;">14.63</td>
<td style="text-align: center;">15.24</td>
<td style="text-align: center;">15.24</td>
<td style="text-align: center;">14.02</td>
<td style="text-align: center;">15.24</td>
<td style="text-align: center;">15.24</td>
<td style="text-align: center;">9.15</td>
<td style="text-align: center;">8.54</td>
<td style="text-align: center;">9.15</td>
<td style="text-align: center;">7.93</td>
<td style="text-align: center;">9.76</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17.80</td>
<td style="text-align: center;">19.40</td>
<td style="text-align: center;">18.40</td>
<td style="text-align: center;">17.40</td>
<td style="text-align: center;">19.20</td>
<td style="text-align: center;">21.20</td>
<td style="text-align: center;">18.20</td>
<td style="text-align: center;">18.40</td>
<td style="text-align: center;">17.20</td>
<td style="text-align: center;">14.80</td>
<td style="text-align: center;">10.20</td>
<td style="text-align: center;">9.40</td>
<td style="text-align: center;">7.80</td>
<td style="text-align: center;">12.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">13.87</td>
<td style="text-align: center;">17.21</td>
<td style="text-align: center;">17.74</td>
<td style="text-align: center;">14.63</td>
<td style="text-align: center;">16.83</td>
<td style="text-align: center;">16.60</td>
<td style="text-align: center;">17.21</td>
<td style="text-align: center;">15.39</td>
<td style="text-align: center;">16.30</td>
<td style="text-align: center;">12.96</td>
<td style="text-align: center;">9.10</td>
<td style="text-align: center;">8.64</td>
<td style="text-align: center;">7.96</td>
<td style="text-align: center;">13.04</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">XSUM</td>
<td style="text-align: center;">R-L</td>
<td style="text-align: center;">27.21</td>
<td style="text-align: center;">21.88</td>
<td style="text-align: center;">24.65</td>
<td style="text-align: center;">27.53</td>
<td style="text-align: center;">27.75</td>
<td style="text-align: center;">27.88</td>
<td style="text-align: center;">27.36</td>
<td style="text-align: center;">25.92</td>
<td style="text-align: center;">27.14</td>
<td style="text-align: center;">22.34</td>
<td style="text-align: center;">22.10</td>
<td style="text-align: center;">20.45</td>
<td style="text-align: center;">20.23</td>
<td style="text-align: center;">21.33</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CNN/DM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23.43</td>
<td style="text-align: center;">20.69</td>
<td style="text-align: center;">21.64</td>
<td style="text-align: center;">23.25</td>
<td style="text-align: center;">23.39</td>
<td style="text-align: center;">24.05</td>
<td style="text-align: center;">23.73</td>
<td style="text-align: center;">22.64</td>
<td style="text-align: center;">23.40</td>
<td style="text-align: center;">20.52</td>
<td style="text-align: center;">20.90</td>
<td style="text-align: center;">18.63</td>
<td style="text-align: center;">18.02</td>
<td style="text-align: center;">19.13</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">De- $\Rightarrow$ En</td>
<td style="text-align: center;">B-4</td>
<td style="text-align: center;">28.80</td>
<td style="text-align: center;">30.14</td>
<td style="text-align: center;">28.71</td>
<td style="text-align: center;">28.63</td>
<td style="text-align: center;">28.52</td>
<td style="text-align: center;">28.82</td>
<td style="text-align: center;">28.40</td>
<td style="text-align: center;">25.45</td>
<td style="text-align: center;">28.55</td>
<td style="text-align: center;">22.72</td>
<td style="text-align: center;">20.30</td>
<td style="text-align: center;">18.44</td>
<td style="text-align: center;">18.00</td>
<td style="text-align: center;">20.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">En- $\Rightarrow$ De</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">22.63</td>
<td style="text-align: center;">23.99</td>
<td style="text-align: center;">23.52</td>
<td style="text-align: center;">22.74</td>
<td style="text-align: center;">22.54</td>
<td style="text-align: center;">22.63</td>
<td style="text-align: center;">22.30</td>
<td style="text-align: center;">19.82</td>
<td style="text-align: center;">22.57</td>
<td style="text-align: center;">16.14</td>
<td style="text-align: center;">14.32</td>
<td style="text-align: center;">12.28</td>
<td style="text-align: center;">11.62</td>
<td style="text-align: center;">13.34</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{Zh} \Rightarrow$ En</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">19.44</td>
<td style="text-align: center;">20.11</td>
<td style="text-align: center;">18.90</td>
<td style="text-align: center;">19.56</td>
<td style="text-align: center;">19.71</td>
<td style="text-align: center;">20.05</td>
<td style="text-align: center;">19.68</td>
<td style="text-align: center;">17.06</td>
<td style="text-align: center;">19.26</td>
<td style="text-align: center;">13.35</td>
<td style="text-align: center;">12.02</td>
<td style="text-align: center;">10.26</td>
<td style="text-align: center;">9.60</td>
<td style="text-align: center;">10.78</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">En- $\Rightarrow$ Zh</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">15.15</td>
<td style="text-align: center;">14.50</td>
<td style="text-align: center;">14.67</td>
<td style="text-align: center;">15.27</td>
<td style="text-align: center;">15.21</td>
<td style="text-align: center;">15.37</td>
<td style="text-align: center;">14.57</td>
<td style="text-align: center;">13.09</td>
<td style="text-align: center;">15.21</td>
<td style="text-align: center;">11.61</td>
<td style="text-align: center;">11.27</td>
<td style="text-align: center;">11.50</td>
<td style="text-align: center;">7.89</td>
<td style="text-align: center;">9.94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CQA</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">62.90</td>
<td style="text-align: center;">64.37</td>
<td style="text-align: center;">64.21</td>
<td style="text-align: center;">63.72</td>
<td style="text-align: center;">64.05</td>
<td style="text-align: center;">63.72</td>
<td style="text-align: center;">62.65</td>
<td style="text-align: center;">62.00</td>
<td style="text-align: center;">63.72</td>
<td style="text-align: center;">56.51</td>
<td style="text-align: center;">49.47</td>
<td style="text-align: center;">47.17</td>
<td style="text-align: center;">46.11</td>
<td style="text-align: center;">52.91</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">60.76</td>
<td style="text-align: center;">62.25</td>
<td style="text-align: center;">61.50</td>
<td style="text-align: center;">60.54</td>
<td style="text-align: center;">62.90</td>
<td style="text-align: center;">60.89</td>
<td style="text-align: center;">63.74</td>
<td style="text-align: center;">61.94</td>
<td style="text-align: center;">61.20</td>
<td style="text-align: center;">58.71</td>
<td style="text-align: center;">58.09</td>
<td style="text-align: center;">58.27</td>
<td style="text-align: center;">58.44</td>
<td style="text-align: center;">58.05</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wikinews</td>
<td style="text-align: center;">MAUVE</td>
<td style="text-align: center;">40.10</td>
<td style="text-align: center;">41.33</td>
<td style="text-align: center;">32.02</td>
<td style="text-align: center;">96.66</td>
<td style="text-align: center;">96.42</td>
<td style="text-align: center;">98.40</td>
<td style="text-align: center;">85.17</td>
<td style="text-align: center;">94.44</td>
<td style="text-align: center;">95.40</td>
<td style="text-align: center;">95.19</td>
<td style="text-align: center;">96.47</td>
<td style="text-align: center;">97.48</td>
<td style="text-align: center;">98.51</td>
<td style="text-align: center;">97.67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wikitext</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23.47</td>
<td style="text-align: center;">27.41</td>
<td style="text-align: center;">22.78</td>
<td style="text-align: center;">93.38</td>
<td style="text-align: center;">92.14</td>
<td style="text-align: center;">92.93</td>
<td style="text-align: center;">85.86</td>
<td style="text-align: center;">85.39</td>
<td style="text-align: center;">94.54</td>
<td style="text-align: center;">96.62</td>
<td style="text-align: center;">96.67</td>
<td style="text-align: center;">93.66</td>
<td style="text-align: center;">93.18</td>
<td style="text-align: center;">93.29</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Book</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">13.10</td>
<td style="text-align: center;">17.54</td>
<td style="text-align: center;">10.18</td>
<td style="text-align: center;">88.41</td>
<td style="text-align: center;">89.07</td>
<td style="text-align: center;">86.69</td>
<td style="text-align: center;">73.30</td>
<td style="text-align: center;">80.54</td>
<td style="text-align: center;">90.62</td>
<td style="text-align: center;">95.99</td>
<td style="text-align: center;">94.84</td>
<td style="text-align: center;">95.31</td>
<td style="text-align: center;">94.25</td>
<td style="text-align: center;">93.98</td>
</tr>
<tr>
<td style="text-align: center;">$\stackrel{\text { Q }}{m}$</td>
<td style="text-align: center;">HumanEval</td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">12.80</td>
<td style="text-align: center;">14.02</td>
<td style="text-align: center;">13.41</td>
<td style="text-align: center;">13.41</td>
<td style="text-align: center;">15.24</td>
<td style="text-align: center;">13.41</td>
<td style="text-align: center;">14.02</td>
<td style="text-align: center;">15.85</td>
<td style="text-align: center;">14.63</td>
<td style="text-align: center;">13.41</td>
<td style="text-align: center;">14.02</td>
<td style="text-align: center;">12.20</td>
<td style="text-align: center;">12.80</td>
<td style="text-align: center;">12.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17.20</td>
<td style="text-align: center;">21.60</td>
<td style="text-align: center;">21.20</td>
<td style="text-align: center;">17.40</td>
<td style="text-align: center;">17.80</td>
<td style="text-align: center;">17.80</td>
<td style="text-align: center;">17.40</td>
<td style="text-align: center;">18.00</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">17.60</td>
<td style="text-align: center;">16.00</td>
<td style="text-align: center;">17.00</td>
<td style="text-align: center;">16.00</td>
<td style="text-align: center;">18.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">24.79</td>
<td style="text-align: center;">28.81</td>
<td style="text-align: center;">26.91</td>
<td style="text-align: center;">25.70</td>
<td style="text-align: center;">25.40</td>
<td style="text-align: center;">24.56</td>
<td style="text-align: center;">26.46</td>
<td style="text-align: center;">22.14</td>
<td style="text-align: center;">25.47</td>
<td style="text-align: center;">24.26</td>
<td style="text-align: center;">24.41</td>
<td style="text-align: center;">25.25</td>
<td style="text-align: center;">23.20</td>
<td style="text-align: center;">24.11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">XSUM</td>
<td style="text-align: center;">R-L</td>
<td style="text-align: center;">16.42</td>
<td style="text-align: center;">16.96</td>
<td style="text-align: center;">16.78</td>
<td style="text-align: center;">16.70</td>
<td style="text-align: center;">16.63</td>
<td style="text-align: center;">16.52</td>
<td style="text-align: center;">16.49</td>
<td style="text-align: center;">8.84</td>
<td style="text-align: center;">16.51</td>
<td style="text-align: center;">16.44</td>
<td style="text-align: center;">16.28</td>
<td style="text-align: center;">16.44</td>
<td style="text-align: center;">15.77</td>
<td style="text-align: center;">16.77</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CNN/DM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">22.59</td>
<td style="text-align: center;">23.71</td>
<td style="text-align: center;">23.54</td>
<td style="text-align: center;">22.54</td>
<td style="text-align: center;">22.40</td>
<td style="text-align: center;">22.64</td>
<td style="text-align: center;">22.65</td>
<td style="text-align: center;">16.92</td>
<td style="text-align: center;">22.71</td>
<td style="text-align: center;">22.67</td>
<td style="text-align: center;">22.03</td>
<td style="text-align: center;">22.34</td>
<td style="text-align: center;">20.60</td>
<td style="text-align: center;">22.42</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CQA</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">50.61</td>
<td style="text-align: center;">52.99</td>
<td style="text-align: center;">52.83</td>
<td style="text-align: center;">51.43</td>
<td style="text-align: center;">52.66</td>
<td style="text-align: center;">51.11</td>
<td style="text-align: center;">52.01</td>
<td style="text-align: center;">52.74</td>
<td style="text-align: center;">53.56</td>
<td style="text-align: center;">53.15</td>
<td style="text-align: center;">51.76</td>
<td style="text-align: center;">51.52</td>
<td style="text-align: center;">52.66</td>
<td style="text-align: center;">52.91</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">59.89</td>
<td style="text-align: center;">60.41</td>
<td style="text-align: center;">60.59</td>
<td style="text-align: center;">59.97</td>
<td style="text-align: center;">60.32</td>
<td style="text-align: center;">60.37</td>
<td style="text-align: center;">60.19</td>
<td style="text-align: center;">59.62</td>
<td style="text-align: center;">60.19</td>
<td style="text-align: center;">60.28</td>
<td style="text-align: center;">60.80</td>
<td style="text-align: center;">60.10</td>
<td style="text-align: center;">59.41</td>
<td style="text-align: center;">59.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wikinews</td>
<td style="text-align: center;">MAUVE</td>
<td style="text-align: center;">58.34</td>
<td style="text-align: center;">71.01</td>
<td style="text-align: center;">74.13</td>
<td style="text-align: center;">70.42</td>
<td style="text-align: center;">76.74</td>
<td style="text-align: center;">81.84</td>
<td style="text-align: center;">74.33</td>
<td style="text-align: center;">63.99</td>
<td style="text-align: center;">83.84</td>
<td style="text-align: center;">76.76</td>
<td style="text-align: center;">79.65</td>
<td style="text-align: center;">72.24</td>
<td style="text-align: center;">70.02</td>
<td style="text-align: center;">72.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wikitext</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">77.69</td>
<td style="text-align: center;">87.20</td>
<td style="text-align: center;">90.27</td>
<td style="text-align: center;">80.16</td>
<td style="text-align: center;">95.10</td>
<td style="text-align: center;">90.47</td>
<td style="text-align: center;">84.59</td>
<td style="text-align: center;">38.76</td>
<td style="text-align: center;">80.45</td>
<td style="text-align: center;">80.51</td>
<td style="text-align: center;">85.63</td>
<td style="text-align: center;">87.52</td>
<td style="text-align: center;">83.48</td>
<td style="text-align: center;">89.77</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Book</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">80.65</td>
<td style="text-align: center;">94.89</td>
<td style="text-align: center;">93.78</td>
<td style="text-align: center;">90.81</td>
<td style="text-align: center;">94.75</td>
<td style="text-align: center;">92.00</td>
<td style="text-align: center;">95.96</td>
<td style="text-align: center;">57.70</td>
<td style="text-align: center;">96.55</td>
<td style="text-align: center;">91.50</td>
<td style="text-align: center;">93.48</td>
<td style="text-align: center;">89.95</td>
<td style="text-align: center;">92.95</td>
<td style="text-align: center;">93.87</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FActScore</td>
<td style="text-align: center;">Score</td>
<td style="text-align: center;">44.74</td>
<td style="text-align: center;">47.80</td>
<td style="text-align: center;">47.29</td>
<td style="text-align: center;">46.09</td>
<td style="text-align: center;">46.09</td>
<td style="text-align: center;">46.93</td>
<td style="text-align: center;">46.11</td>
<td style="text-align: center;">36.37</td>
<td style="text-align: center;">45.06</td>
<td style="text-align: center;">44.78</td>
<td style="text-align: center;">44.11</td>
<td style="text-align: center;">46.81</td>
<td style="text-align: center;">44.06</td>
<td style="text-align: center;">46.55</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AlpacaEval</td>
<td style="text-align: center;">WinRate</td>
<td style="text-align: center;">76.40</td>
<td style="text-align: center;">77.89</td>
<td style="text-align: center;">78.63</td>
<td style="text-align: center;">79.88</td>
<td style="text-align: center;">80.50</td>
<td style="text-align: center;">79.88</td>
<td style="text-align: center;">81.24</td>
<td style="text-align: center;">55.40</td>
<td style="text-align: center;">77.76</td>
<td style="text-align: center;">78.01</td>
<td style="text-align: center;">77.39</td>
<td style="text-align: center;">79.38</td>
<td style="text-align: center;">75.53</td>
<td style="text-align: center;">78.26</td>
</tr>
</tbody>
</table>
<p>Table 1: Results on Llama2-7B and Llama2-7B-Chat. Cells are colored by performance, from low to medium to high performance. The corresponding hyperparameters for each decoding method are listed in Appendix E.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Relative deviation percentage (RDP) for each task on Llama2-7B and Llama2-7B-Chat.
els (Llama2-7B and Llama2-7B-Chat respectively) in Table 1. The reported results for each method are obtained by utilizing the best hyperparameters tuned for each specific dataset.</p>
<p>For unaligned models, deterministic methods generally perform better than stochastic methods on all tasks except open-ended text generation. As shown in the upper block of Table 1, for the unaligned Llama2-7B model, the top-performing decoding methods on closed-ended tasks (coding, math problem solving, summarization, translation, and commonsense reasoning) are frequently among deterministic methods. On the other hand, stochastic methods often struggle with the worst performance. Specifically, BS, FSD-d, and FSD rank in the top 3 (indicated in orange) in 8, 7 , and 7 out of 11 datasets, respectively. Conversely, mirostat, $\eta$, and typical sampling are among the least effective three methods (highlighted in blue)
in 10, 10, and 7 datasets, respectively. For openended text generation (Wikinews, Wikitext, and Book), greedy, BS, and DBS exhibit notably lower MAUVE scores than other methods. The above observations on the disparity of deterministic and stochastic methods are consistent with the findings for conventional task-specific models (Wiher et al., 2022): stochastic methods are favorable in openended tasks, while heavily disfavored in others.
$\rightarrow$ Phenomenon Analysis. Through a careful case study, we find that the outputs of greedy, BS, and DBS contain a considerable amount of repetitive content on open-ended text generation tasks. This suggests that the advanced unaligned LLM s still suffer from the degeneration issue (Holtzman et al., 2020; Li et al., 2023a). Recent deterministic methods (CS, FSD, FSD-d, CD, and DoLa), which are designed to alleviate the degeneration issue, achieve much better results, performing only slightly worse than stochastic methods. For closedended tasks, deterministic approaches are better suited to producing consistent and accurate results as diversity is not a primary concern.</p>
<p>Aligned models are less dependent on decoding methods than unaligned models. For the unaligned Llama2-7B model, there is a clear separation between the highest- and lowest-performing methods. For instance, on MBPP, the highest performance is at $21.20 \%$ by FSD-d, in stark contrast</p>
<p>to the lowest at $7.80 \%$ by mirostat sampling. However, this distinction becomes less pronounced for the aligned Llama2-7B-Chat model. Specifically, on MBPP, the top performance peaks at $21.60 \%$ while the lowest is at $16.00 \%$, showcasing a narrowed performance range.</p>
<p>To further substantiate this, we compute the average $\mu$ and standard deviation $\sigma$ of each dataset across different decoding methods. We report the relative deviation percentage (RDP) $\frac{\sigma}{\mu} \times 100 \%$, of which a lower value signifies less performance variation across different decoding method choices. The results are depicted in Figure 1. Generally, the aligned model (Llama2-7B-Chat) displays less pronounced variations compared to its unaligned counterpart (Llama2-7B), except in two summarization datasets (XSUM and CNN/DM) where the relative deviation percentages are close. This suggests that the choice of decoding method becomes less critical after the model is aligned. Additionally, we also notice that DoLa performs quite worse than other methods under Llama2-7B-Chat. We check its outputs and observe that DoLa fails to terminate its generation appropriately (see Appendix H).
$\rightarrow$ Phenomenon Analysis. The potential reasons are as follows: i) The improved model confidence. As shown in Table 2, we report the average nexttoken prediction entropy of Llama2-Chat-7B and Llama2-7B on GSM8K, MBPP, and Wikinews. It can be seen that the entropy of the aligned model is substantially lower than that of the unaligned one. As the model becomes more confident (concentrating the probabilistic mass on a shortlist of tokens), there is less operating space for decoding methods. ii) The alleviated degeneration issue. We find that the aligned model produces much fewer repetitions even when using deterministic decoding methods such as greedy search. This inherent improvement, possibly due to the high-quality data with reduced repetition employed during the instruction tuning phase (Li et al., 2023a), makes those decoding methods that aim to mitigate the degeneration issue less useful. iii) The more structured writing style. The aligned model typically produces more well-organized responses (e.g., a list of points with explicit discourse markers). This structural coherence enhances the stability of the model's output and reduces the variations of stochastic decoding methods (Lin et al., 2023a).</p>
<h2>Deterministic methods tend to generate fewer hallucinations and have better instruction-</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MBPP</th>
<th style="text-align: center;">Wikinews</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Llama2-7B</td>
<td style="text-align: center;">1.05</td>
<td style="text-align: center;">1.21</td>
<td style="text-align: center;">2.37</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-7B-Chat</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.52</td>
</tr>
</tbody>
</table>
<p>Table 2: The entropy of Llama2-7B and Llama2-Chat7B's generation results (top- $p$ sampling with $p=1.0$ ) on GSM8K, MBPP and Wikinews.
following abilities. The lower block of Table 1 also presents the results of the aligned model (Llama2-7B-Chat) on FActScore and AlpacaEval. For FActScore, the top three best-performing methods are all deterministic. For instance, beam search attains $47.80 \%$, while mirostat and top- $k$ sampling only achieve scores of $44.06 \%$ and $44.11 \%$, respectively. These results indicate that the choice of decoding method has a considerable impact on the factuality of the generated text. The randomness in the selection process of stochastic methods may contribute to increased hallucinations. For AlpacaEval, the general instruction-following task, deterministic methods such as CS, FSD, and CD can outperform all stochastic methods. This observation challenges the prevailing common practice of employing stochastic methods, particularly temperature and top- $p$ sampling, in LLMs. This suggests that deterministic methods are more reliable for tasks requiring high factual accuracy and precise adherence to instructions, warranting further exploration in future research.</p>
<p>Among stochastic methods, temperature sampling generally performs better, particularly when using unaligned models. As evidenced in Table 1, temperature sampling generally outperforms other stochastic methods except for openended text generation. Specifically, on Llama27B, temperature sampling emerges as the topperforming stochastic method across all 11 closedended tasks. Similarly, under Llama2-7B-Chat, it takes the top position in 5 out of 9 closed-ended tasks. We find that the best results often come from a low temperature (e.g., $\tau=0.1,0.2$, see Table 27 in Appendix E), which renders temperature sampling more akin to deterministic decoding. It is worth noting that many previous studies (Fan et al., 2018; Holtzman et al., 2020; Meister et al., 2023; Hewitt et al., 2022) predominantly demonstrate the superiority of their proposed methods in the realm of open-ended text generation. However, our analysis reveals that temperature sampling markedly surpasses these methods in closed-ended generation tasks, thereby underscoring the necessity for</p>
<p>more holistic evaluations across diverse tasks.</p>
<h3>4.2 Hyperparameter Sensitivity</h3>
<p>The results in Table 1 are obtained by searching for the optimal hyperparameter of each decoding method for each dataset. Nevertheless, hyperparameter search is time-consuming and may not be plausible for open-world applications where the target task is not known a priori. Therefore, we further explore a more realistic scenario in which each method uses a fixed hyperparameter across different datasets. To ensure a fair comparison that accounts for various performance ranges across different tasks, we first normalize the performance on each dataset according to $\operatorname{normalize}(p)=$ $\frac{p}{\text { Pbest }} \times 100 \%$, where $p_{\text {best }}$ represents the best performance obtained in Table 1, then compute the average of normalized performance across all datasets, denoted by ANP. We report the best ANP using task-specific hyperparameters ( $\mathrm{ANP}<em _fix="{fix" _text="\text">{\text {best }}$ ) and a fixed hyperparameter ( $\mathrm{ANP}</em>}}$ ) for each decoding method respectively. The results on Llama2-7B family are presented in Figure 2. For Llama2-7B, both FSD and FSD-d rank among the top-3 decoding methods in terms of performance, whether under task-specific hyperparameters ( $\mathrm{ANP<em _fix="{fix" _text="\text">{\text {best }}$ ) or one fixed hyperparameter ( $\mathrm{ANP}</em>}}$ ), demonstrating that these methods can have the ideal performance without the need for fine-grained selection of hyperparameters for each dataset. In contrast, while temperature sampling achieves comparable results in terms of $\mathrm{ANP<em _fix="{fix" _text="\text">{\text {best }}$, it shows an $11.59 \%$ decrease in $\mathrm{ANP}</em>$ when hyperparameters are fixed, highlighting its sensitivity to hyperparameters. Similarly, for Llama2-7B-Chat, BS and DBS perform well and are not sensitive to hyperparameters, while temperature sampling still exhibits a $3.90 \%$ decrease. Notably, CD is also sensitive to hyperparameters, with a performance decrease of $9.42 \%$ on Llama27B and $3.35 \%$ on Llama2-7B-Chat.}</p>
<h3>4.3 Decoding Speed</h3>
<p>We assess and compare the decoding speed of various decoding methods in Figure 3. For a more intuitive understanding, we calculate the latency ratio for each decoding method by normalizing their latency with respect to the latency of greedy search. To demonstrate how their latency grows with generation lengths, we plot the latency for generating 128, 256, 512 and 1024 tokens given 32 tokens using Llama2-7B. It is worth noting that we omit the results of all stochastic decoding meth-
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Hyperparameter Sensitivity. $\mathrm{ANP}<em _fix="{fix" _text="\text">{\text {best }}$ and the best $\mathrm{ANP}</em>$ with the optimal hyperparameters for each decoding method are detailed in Appendix E.
}}$ for each decoding method on Llama2-7B with blue solid markers and Llama2-7B-Chat with orange hollow markers $\bigcirc$. The $\mathrm{ANP}_{\text {fix }<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Decoding latency ratios. The latency is measured on one A6000 GPU with batch size $=1$.
ods mentioned in $\S 2.2$ because they achieve very close latency to that of greedy search. It is reasonable because their sampling processes only require negligible additional computation.</p>
<p>It can be observed that contrastive search is the decoding method with the slowest decoding speed. Moreover, the latency ratio grows considerably as generation length increases (from 1.51 x to 2.00 x slower than greedy search). This is due to that the look-ahead mechanism in contrastive search is very time-consuming. Contrastive decoding is about 1.4 x slower than greedy search for the additional run of a smaller amateur model. However, the latency ratio of contrastive decoding remains constant across different lengths, indicating better adaptability for long sequence generation. Beam search and diverse beam search are faster than contrastive search and contrastive decoding but slower ( 1.13 x to 1.41 x ) than greedy search. Both have latency ratios that grow approximately linearly with the sequence length while diverse beam search is slightly slower than beam search. The speed of DoLa is comparable to beam search and diverse beam search when the generation is relatively short (128 and 256). Nevertheless, their difference in-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results of stochastic decoding methods with self-consistency on GSM8K.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Temp</th>
<th>Top-ρ</th>
<th>Top-k</th>
<th>η</th>
<th>Miro</th>
<th>Typical</th>
</tr>
</thead>
<tbody>
<tr>
<td>7B</td>
<td>21.91</td>
<td>22.06</td>
<td>20.17</td>
<td>21.23</td>
<td>16.98</td>
<td>22.06</td>
</tr>
<tr>
<td></td>
<td>(0.7)</td>
<td>(0.8)</td>
<td>(5)</td>
<td>(0.004)</td>
<td>(4.0)</td>
<td>(0.95)</td>
</tr>
<tr>
<td>7B-Chat</td>
<td>36.92</td>
<td>36.85</td>
<td>37.68</td>
<td>35.63</td>
<td>37.76</td>
<td>36.92</td>
</tr>
<tr>
<td></td>
<td>(0.9)</td>
<td>(1.0)</td>
<td>(10)</td>
<td>(0.0009)</td>
<td>(5.0)</td>
<td>(0.90)</td>
</tr>
</tbody>
</table>
<p>Table 3: Best results of different stochastic methods with self-consistency (20 generations) setting on GSM8K for Llama2-7B family. The best hyperparameters are annotated in parentheses.</p>
<p>creases as the generation length grows because the latency ratio of DoLa remains consistent across different lengths. Notably, FSD and FSD-d not only run as fast as greedy search but also maintain a consistent latency ratio across different lengths, underscoring their superior efficiency against other advanced deterministic decoding methods.</p>
<h2>5 Further Analysis</h2>
<h3>5.1 Self-Consistency</h3>
<p>Previous experiments demonstrate that the best-performing decoding methods are generally deterministic ones on closed-ended tasks, particularly on complex reasoning tasks such as the GSM8K dataset. Nonetheless, one unique advantage of stochastic decoding methods is that they can produce varied results through multiple runs, of which one can use the self-consistency strategy (Wang et al., 2023) for enhanced task performance. Concretely, self-consistency samples multiple generations and takes a majority vote to determine the final answer. To gain further insights into the potential of stochastic decoding methods, we then delve into the experiments with self-consistency.</p>
<p>As illustrated in Figure 4, we plot the accuracies of various stochastic decoding methods on GSM8K with respect to varying numbers of sampled generations (1, 5, 10, and 20). We also contrast the results with the best accuracies achieved by deterministic decoding methods (i.e., 17.74% by diverse beam search using Llama2-7B and 28.81% by beam search using Llama2-7B-Chat), denoted by the gray dashed lines. The results show that sampling a larger number of generations consistently leads to better performance, confirming the usefulness of self-consistency in taking advantage of the diversity introduced by stochastic sampling. Except for the results of mirostat sampling on Llama2-7B, we can see that all stochastic methods eventually surpass the best-performing deterministic methods when the number of sampling reaches 20. Note that the results in Figure 4 are obtained by using the best hyperparameters we find in Table 1 where only one-pass generation is allowed.</p>
<p>We speculate that further tuning the hyperparameters can improve the performance under the self-consistency strategy. Thus we undertake an additional hyperparameter search in scenarios where the number of generations is set to 20. The highest results along with the corresponding hyperparameters are reported in Table 3. Compared to the results in Figure 4, we can see that the performance is boosted by employing a hyperparameter with greater randomness or candidate pool. For example, on Llama2-7B-Chat, the accuracy of temperature sampling increases from 34.04% (τ = 0.5) to 36.92% (τ = 0.9). Another interesting finding is that the best hyperparameters for aligned models typically suggest greater randomness (e.g., τ = 0.9 vs. τ = 0.7 for temperature sampling).</p>
<h3>5.2 Scaling Model Size</h3>
<p>In order to investigate the impact of model scale on different decoding methods, we provide further experiments on Llama2 family with 13B and 70B parameters in 3 representative tasks: MBPP, GSM8K, and Wikinews. We present the results in Table 4. It can be observed that as the model's parameters increase, the <em>relative deviation percentage</em> (RDP) of each task decreases, indicating that the differences between different decoding methods have been reduced. This suggests that scaling model size can diminish the significance of decoding strategies. Moreover, as the number of model parameters varies, the optimal hyperparameters for each decoding method are also subject to change (detailed in Appendix E). Consequently, there is also a need to adjust the hyperparameters for larger-scale models individually, rather than directly applying those from smaller models. Meanwhile, the degree of impact from the model scale varies for different</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Deterministic Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Stochastic Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RDP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Greedy</td>
<td style="text-align: center;">BS</td>
<td style="text-align: center;">DBS</td>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">FSD</td>
<td style="text-align: center;">FSD-d</td>
<td style="text-align: center;">CD</td>
<td style="text-align: center;">DoLa</td>
<td style="text-align: center;">Temp</td>
<td style="text-align: center;">Top- $\gamma$</td>
<td style="text-align: center;">Top- $\gamma$</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">Mins</td>
<td style="text-align: center;">Typical</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">17.80</td>
<td style="text-align: center;">19.40</td>
<td style="text-align: center;">18.40</td>
<td style="text-align: center;">17.40</td>
<td style="text-align: center;">19.20</td>
<td style="text-align: center;">21.20</td>
<td style="text-align: center;">18.20</td>
<td style="text-align: center;">18.40</td>
<td style="text-align: center;">17.20</td>
<td style="text-align: center;">13.80</td>
<td style="text-align: center;">10.20</td>
<td style="text-align: center;">9.40</td>
<td style="text-align: center;">7.80</td>
<td style="text-align: center;">12.00</td>
<td style="text-align: center;">25.81</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">13.87</td>
<td style="text-align: center;">17.21</td>
<td style="text-align: center;">17.74</td>
<td style="text-align: center;">14.63</td>
<td style="text-align: center;">16.83</td>
<td style="text-align: center;">16.60</td>
<td style="text-align: center;">17.21</td>
<td style="text-align: center;">15.39</td>
<td style="text-align: center;">16.30</td>
<td style="text-align: center;">12.96</td>
<td style="text-align: center;">9.10</td>
<td style="text-align: center;">8.64</td>
<td style="text-align: center;">7.96</td>
<td style="text-align: center;">13.04</td>
<td style="text-align: center;">23.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wikinews</td>
<td style="text-align: center;">40.10</td>
<td style="text-align: center;">41.33</td>
<td style="text-align: center;">52.02</td>
<td style="text-align: center;">96.66</td>
<td style="text-align: center;">96.42</td>
<td style="text-align: center;">98.40</td>
<td style="text-align: center;">83.17</td>
<td style="text-align: center;">94.44</td>
<td style="text-align: center;">95.40</td>
<td style="text-align: center;">95.19</td>
<td style="text-align: center;">96.47</td>
<td style="text-align: center;">97.48</td>
<td style="text-align: center;">98.51</td>
<td style="text-align: center;">97.67</td>
<td style="text-align: center;">26.83</td>
</tr>
<tr>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">23.20</td>
<td style="text-align: center;">24.40</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">25.80</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">23.80</td>
<td style="text-align: center;">23.40</td>
<td style="text-align: center;">17.40</td>
<td style="text-align: center;">13.40</td>
<td style="text-align: center;">21.60</td>
<td style="text-align: center;">10.00</td>
<td style="text-align: center;">17.20</td>
<td style="text-align: center;">21.37</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">28.81</td>
<td style="text-align: center;">29.64</td>
<td style="text-align: center;">29.19</td>
<td style="text-align: center;">29.42</td>
<td style="text-align: center;">31.99</td>
<td style="text-align: center;">31.16</td>
<td style="text-align: center;">35.50</td>
<td style="text-align: center;">28.58</td>
<td style="text-align: center;">30.02</td>
<td style="text-align: center;">24.94</td>
<td style="text-align: center;">18.20</td>
<td style="text-align: center;">30.10</td>
<td style="text-align: center;">15.39</td>
<td style="text-align: center;">21.76</td>
<td style="text-align: center;">18.74</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wikinews</td>
<td style="text-align: center;">62.02</td>
<td style="text-align: center;">50.30</td>
<td style="text-align: center;">51.00</td>
<td style="text-align: center;">58.22</td>
<td style="text-align: center;">97.01</td>
<td style="text-align: center;">93.26</td>
<td style="text-align: center;">94.83</td>
<td style="text-align: center;">91.53</td>
<td style="text-align: center;">96.88</td>
<td style="text-align: center;">97.77</td>
<td style="text-align: center;">97.81</td>
<td style="text-align: center;">97.18</td>
<td style="text-align: center;">96.94</td>
<td style="text-align: center;">96.87</td>
<td style="text-align: center;">19.95</td>
</tr>
<tr>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">41.80</td>
<td style="text-align: center;">43.40</td>
<td style="text-align: center;">41.00</td>
<td style="text-align: center;">39.40</td>
<td style="text-align: center;">41.20</td>
<td style="text-align: center;">41.20</td>
<td style="text-align: center;">42.20</td>
<td style="text-align: center;">37.00</td>
<td style="text-align: center;">41.80</td>
<td style="text-align: center;">33.20</td>
<td style="text-align: center;">25.80</td>
<td style="text-align: center;">38.80</td>
<td style="text-align: center;">24.80</td>
<td style="text-align: center;">42.20</td>
<td style="text-align: center;">15.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">57.39</td>
<td style="text-align: center;">59.44</td>
<td style="text-align: center;">58.76</td>
<td style="text-align: center;">58.91</td>
<td style="text-align: center;">60.73</td>
<td style="text-align: center;">60.42</td>
<td style="text-align: center;">63.91</td>
<td style="text-align: center;">61.55</td>
<td style="text-align: center;">57.47</td>
<td style="text-align: center;">53.37</td>
<td style="text-align: center;">44.20</td>
<td style="text-align: center;">58.53</td>
<td style="text-align: center;">38.36</td>
<td style="text-align: center;">59.89</td>
<td style="text-align: center;">11.92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wikinews</td>
<td style="text-align: center;">42.44</td>
<td style="text-align: center;">36.35</td>
<td style="text-align: center;">77.33</td>
<td style="text-align: center;">95.22</td>
<td style="text-align: center;">95.68</td>
<td style="text-align: center;">93.29</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">94.31</td>
<td style="text-align: center;">94.09</td>
<td style="text-align: center;">92.75</td>
<td style="text-align: center;">93.39</td>
<td style="text-align: center;">96.04</td>
<td style="text-align: center;">96.02</td>
<td style="text-align: center;">92.33</td>
<td style="text-align: center;">16.02</td>
</tr>
<tr>
<td style="text-align: center;">7B-Chat</td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">17.20</td>
<td style="text-align: center;">21.60</td>
<td style="text-align: center;">21.20</td>
<td style="text-align: center;">17.40</td>
<td style="text-align: center;">17.80</td>
<td style="text-align: center;">17.80</td>
<td style="text-align: center;">17.40</td>
<td style="text-align: center;">18.00</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">17.60</td>
<td style="text-align: center;">16.00</td>
<td style="text-align: center;">17.00</td>
<td style="text-align: center;">16.00</td>
<td style="text-align: center;">18.00</td>
<td style="text-align: center;">9.08</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">24.79</td>
<td style="text-align: center;">28.81</td>
<td style="text-align: center;">26.91</td>
<td style="text-align: center;">25.70</td>
<td style="text-align: center;">25.40</td>
<td style="text-align: center;">24.56</td>
<td style="text-align: center;">26.46</td>
<td style="text-align: center;">22.16</td>
<td style="text-align: center;">25.47</td>
<td style="text-align: center;">24.26</td>
<td style="text-align: center;">24.41</td>
<td style="text-align: center;">25.25</td>
<td style="text-align: center;">23.20</td>
<td style="text-align: center;">24.11</td>
<td style="text-align: center;">6.25</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wikinews</td>
<td style="text-align: center;">58.34</td>
<td style="text-align: center;">71.01</td>
<td style="text-align: center;">74.13</td>
<td style="text-align: center;">70.42</td>
<td style="text-align: center;">76.74</td>
<td style="text-align: center;">81.84</td>
<td style="text-align: center;">74.33</td>
<td style="text-align: center;">63.99</td>
<td style="text-align: center;">83.84</td>
<td style="text-align: center;">76.76</td>
<td style="text-align: center;">79.65</td>
<td style="text-align: center;">72.24</td>
<td style="text-align: center;">70.02</td>
<td style="text-align: center;">72.32</td>
<td style="text-align: center;">8.84</td>
</tr>
<tr>
<td style="text-align: center;">13B-Chat</td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">22.60</td>
<td style="text-align: center;">24.90</td>
<td style="text-align: center;">24.40</td>
<td style="text-align: center;">23.80</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">23.40</td>
<td style="text-align: center;">23.80</td>
<td style="text-align: center;">23.60</td>
<td style="text-align: center;">24.80</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">24.20</td>
<td style="text-align: center;">22.60</td>
<td style="text-align: center;">23.60</td>
<td style="text-align: center;">2.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">34.57</td>
<td style="text-align: center;">50.73</td>
<td style="text-align: center;">58.06</td>
<td style="text-align: center;">36.24</td>
<td style="text-align: center;">36.62</td>
<td style="text-align: center;">36.16</td>
<td style="text-align: center;">36.62</td>
<td style="text-align: center;">35.15</td>
<td style="text-align: center;">36.32</td>
<td style="text-align: center;">36.01</td>
<td style="text-align: center;">35.41</td>
<td style="text-align: center;">35.41</td>
<td style="text-align: center;">36.01</td>
<td style="text-align: center;">36.85</td>
<td style="text-align: center;">4.05</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wikinews</td>
<td style="text-align: center;">77.35</td>
<td style="text-align: center;">84.43</td>
<td style="text-align: center;">88.82</td>
<td style="text-align: center;">87.80</td>
<td style="text-align: center;">92.89</td>
<td style="text-align: center;">82.58</td>
<td style="text-align: center;">98.06</td>
<td style="text-align: center;">70.68</td>
<td style="text-align: center;">84.54</td>
<td style="text-align: center;">87.50</td>
<td style="text-align: center;">82.20</td>
<td style="text-align: center;">89.20</td>
<td style="text-align: center;">89.23</td>
<td style="text-align: center;">90.13</td>
<td style="text-align: center;">7.50</td>
</tr>
<tr>
<td style="text-align: center;">70B-Chat</td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">31.40</td>
<td style="text-align: center;">31.80</td>
<td style="text-align: center;">32.00</td>
<td style="text-align: center;">30.40</td>
<td style="text-align: center;">30.80</td>
<td style="text-align: center;">30.80</td>
<td style="text-align: center;">30.60</td>
<td style="text-align: center;">30.20</td>
<td style="text-align: center;">32.00</td>
<td style="text-align: center;">30.80</td>
<td style="text-align: center;">28.40</td>
<td style="text-align: center;">31.60</td>
<td style="text-align: center;">28.20</td>
<td style="text-align: center;">31.60</td>
<td style="text-align: center;">3.74</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">51.93</td>
<td style="text-align: center;">50.87</td>
<td style="text-align: center;">53.90</td>
<td style="text-align: center;">53.22</td>
<td style="text-align: center;">52.01</td>
<td style="text-align: center;">52.54</td>
<td style="text-align: center;">52.24</td>
<td style="text-align: center;">48.82</td>
<td style="text-align: center;">52.62</td>
<td style="text-align: center;">52.99</td>
<td style="text-align: center;">51.10</td>
<td style="text-align: center;">52.92</td>
<td style="text-align: center;">51.93</td>
<td style="text-align: center;">52.16</td>
<td style="text-align: center;">2.28</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wikinews</td>
<td style="text-align: center;">77.53</td>
<td style="text-align: center;">74.01</td>
<td style="text-align: center;">84.10</td>
<td style="text-align: center;">85.85</td>
<td style="text-align: center;">84.60</td>
<td style="text-align: center;">87.13</td>
<td style="text-align: center;">81.54</td>
<td style="text-align: center;">69.58</td>
<td style="text-align: center;">80.67</td>
<td style="text-align: center;">82.00</td>
<td style="text-align: center;">83.85</td>
<td style="text-align: center;">82.53</td>
<td style="text-align: center;">75.11</td>
<td style="text-align: center;">84.69</td>
<td style="text-align: center;">6.04</td>
</tr>
</tbody>
</table>
<p>Table 4: Results of Llama2 family models with different scales on MBPP, GSM8K, Wikinews datasets. We report the relative deviation percentage (RDP) of the performance of different decoding methods on each task in the last column. The corresponding hyperparameters for each decoding method are listed in Appendix E.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Deterministic Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Stochastic Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RDP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Greedy</td>
<td style="text-align: center;">BS</td>
<td style="text-align: center;">DBS</td>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">FSD</td>
<td style="text-align: center;">FSD-d</td>
<td style="text-align: center;">CD</td>
<td style="text-align: center;">DoLa</td>
<td style="text-align: center;">Temp</td>
<td style="text-align: center;">Top- $\gamma$</td>
<td style="text-align: center;">Top- $\gamma$</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">Mins</td>
<td style="text-align: center;">Typical</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">13B-INT4</td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">24.60</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">21.20</td>
<td style="text-align: center;">24.60</td>
<td style="text-align: center;">25.20</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">18.40</td>
<td style="text-align: center;">21.40</td>
<td style="text-align: center;">21.40</td>
<td style="text-align: center;">20.40</td>
<td style="text-align: center;">22.60</td>
<td style="text-align: center;">21.28</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">27.45</td>
<td style="text-align: center;">30.33</td>
<td style="text-align: center;">28.13</td>
<td style="text-align: center;">27.67</td>
<td style="text-align: center;">31.01</td>
<td style="text-align: center;">30.10</td>
<td style="text-align: center;">28.40</td>
<td style="text-align: center;">29.11</td>
<td style="text-align: center;">27.60</td>
<td style="text-align: center;">21.30</td>
<td style="text-align: center;">15.68</td>
<td style="text-align: center;">26.38</td>
<td style="text-align: center;">14.10</td>
<td style="text-align: center;">27.90</td>
<td style="text-align: center;">19.92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wikinews</td>
<td style="text-align: center;">47.41</td>
<td style="text-align: center;">46.61</td>
<td style="text-align: center;">46.70</td>
<td style="text-align: center;">91.21</td>
<td style="text-align: center;">96.11</td>
<td style="text-align: center;">95.91</td>
<td style="text-align: center;">93.31</td>
<td style="text-align: center;">92.83</td>
<td style="text-align: center;">97.79</td>
<td style="text-align: center;">97.99</td>
<td style="text-align: center;">96.70</td>
<td style="text-align: center;">90.32</td>
<td style="text-align: center;">95.67</td>
<td style="text-align: center;">91.02</td>
<td style="text-align: center;">23.28</td>
</tr>
<tr>
<td style="text-align: center;">13B-INT8</td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">21.60</td>
<td style="text-align: center;">25.20</td>
<td style="text-align: center;">22.20</td>
<td style="text-align: center;">23.20</td>
<td style="text-align: center;">22.60</td>
<td style="text-align: center;">25.20</td>
<td style="text-align: center;">22.80</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">17.60</td>
<td style="text-align: center;">12.60</td>
<td style="text-align: center;">11.80</td>
<td style="text-align: center;">9.60</td>
<td style="text-align: center;">15.40</td>
<td style="text-align: center;">25.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">28.43</td>
<td style="text-align: center;">28.69</td>
<td style="text-align: center;">28.96</td>
<td style="text-align: center;">29.04</td>
<td style="text-align: center;">30.93</td>
<td style="text-align: center;">30.48</td>
<td style="text-align: center;">23.59</td>
<td style="text-align: center;">24.28</td>
<td style="text-align: center;">29.34</td>
<td style="text-align: center;">23.88</td>
<td style="text-align: center;">17.21</td>
<td style="text-align: center;">16.45</td>
<td style="text-align: center;">13.34</td>
<td style="text-align: center;">21.68</td>
<td style="text-align: center;">23.25</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wikinews</td>
<td style="text-align: center;">49.24</td>
<td style="text-align: center;">51.92</td>
<td style="text-align: center;">45.56</td>
<td style="text-align: center;">94.39</td>
<td style="text-align: center;">96.99</td>
<td style="text-align: center;">97.18</td>
<td style="text-align: center;">93.96</td>
<td style="text-align: center;">94.06</td>
<td style="text-align: center;">94.81</td>
<td style="text-align: center;">97.71</td>
<td style="text-align: center;">96.33</td>
<td style="text-align: center;">97.28</td>
<td style="text-align: center;">95.60</td>
<td style="text-align: center;">97.15</td>
<td style="text-align: center;">22.62</td>
</tr>
<tr>
<td style="text-align: center;">13B-Chat-INT4</td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">21.80</td>
<td style="text-align: center;">25.60</td>
<td style="text-align: center;">25.80</td>
<td style="text-align: center;">25.40</td>
<td style="text-align: center;">24.80</td>
<td style="text-align: center;">24.40</td>
<td style="text-align: center;">24.40</td>
<td style="text-align: center;">22.80</td>
<td style="text-align: center;">24.80</td>
<td style="text-align: center;">24.40</td>
<td style="text-align: center;">21.60</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">22.40</td>
<td style="text-align: center;">25.40</td>
<td style="text-align: center;">4.97</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">34.12</td>
<td style="text-align: center;">35.71</td>
<td style="text-align: center;">37.45</td>
<td style="text-align: center;">34.50</td>
<td style="text-align: center;">35.33</td>
<td style="text-align: center;">35.41</td>
<td style="text-align: center;">34.42</td>
<td style="text-align: center;">31.61</td>
<td style="text-align: center;">35.33</td>
<td style="text-align: center;">34.27</td>
<td style="text-align: center;">33.97</td>
<td style="text-align: center;">34.04</td>
<td style="text-align: center;">33.74</td>
<td style="text-align: center;">35.33</td>
<td style="text-align: center;">3.64</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wikinews</td>
<td style="text-align: center;">80.54</td>
<td style="text-align: center;">83.63</td>
<td style="text-align: center;">83.16</td>
<td style="text-align: center;">86.81</td>
<td style="text-align: center;">85.34</td>
<td style="text-align: center;">86.08</td>
<td style="text-align: center;">81.90</td>
<td style="text-align: center;">74.71</td>
<td style="text-align: center;">87.76</td>
<td style="text-align: center;">89.29</td>
<td style="text-align: center;">81.25</td>
<td style="text-align: center;">84.63</td>
<td style="text-align: center;">83.02</td>
<td style="text-align: center;">83.90</td>
<td style="text-align: center;">8.93</td>
</tr>
<tr>
<td style="text-align: center;">13B-Chat-INT8</td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">23.80</td>
<td style="text-align: center;">24.60</td>
<td style="text-align: center;">24.20</td>
<td style="text-align: center;">22.80</td>
<td style="text-align: center;">22.40</td>
<td style="text-align: center;">23.40</td>
<td style="text-align: center;">23.60</td>
<td style="text-align: center;">23.40</td>
<td style="text-align: center;">23.20</td>
<td style="text-align: center;">23.40</td>
<td style="text-align: center;">25.20</td>
<td style="text-align: center;">23.40</td>
<td style="text-align: center;">23.80</td>
<td style="text-align: center;">2.88</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">35.56</td>
<td style="text-align: center;">36.92</td>
<td style="text-align: center;">37.68</td>
<td style="text-align: center;">37.78</td>
<td style="text-align: center;">36.69</td>
<td style="text-align: center;">36.69</td>
<td style="text-align: center;">37.38</td>
<td style="text-align: center;">31.54</td>
<td style="text-align: center;">36.09</td>
<td style="text-align: center;">28.44</td>
<td style="text-align: center;">37.15</td>
<td style="text-align: center;">36.92</td>
<td style="text-align: center;">36.85</td>
<td style="text-align: center;">37.38</td>
<td style="text-align: center;">4.29</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wikinews</td>
<td style="text-align: center;">73.73</td>
<td style="text-align: center;">83.32</td>
<td style="text-align: center;">88.82</td>
<td style="text-align: center;">91.37</td>
<td style="text-align: center;">85.53</td>
<td style="text-align: center;">81.25</td>
<td style="text-align: center;">89.41</td>
<td style="text-align: center;">57.87</td>
<td style="text-align: center;">90.50</td>
<td style="text-align: center;">85.67</td>
<td style="text-align: center;">87.10</td>
<td style="text-align: center;">82.91</td>
<td style="text-align: center;">84.87</td>
<td style="text-align: center;">81.61</td>
<td style="text-align: center;">10.34</td>
</tr>
</tbody>
</table>
<p>Table 5: Results for INT4 and INT8 quantization with Llama2 13B family on MBPP, GSM8K, Wikinews datasets. The corresponding hyperparameters for each decoding method are listed in Appendix E.
decoding methods. For example, in MBPP, the best performance of $\eta$ sampling on Llama2-7B is $9.40 \%$, which is less than half of the best method FSD-d at $21.20 \%$. However, for Llama2-13B, $\eta$ sampling achieves $21.60 \%$, and for Llama2-70B, it reaches $38.80 \%$, showing comparable efficacy to the best decoding method. This shows $\eta$ sampling benefits greatly from a greater model scale.</p>
<h3>5.3 Quantization</h3>
<p>The large size of LLMs presents challenges for deployment, especially where resources are limited. Consequently, in the LLM era, it is crucial to examine how various decoding methods perform in quantization settings. We assess the performance of decoding methods in both INT8 quantization (Dettmers et al., 2022) and INT4 quantization (Lin et al., 2023b) for Llama2-13B family. As detailed in Table 5, compared with the FP16 13B model in Table 4, the RDP under quantized models is larger, indicating that quantization may impact the models' robustness to different decoding methods. At the same time, different decoding methods exhibit varying adaptability to quantized models. Specially, the performance changes of deterministic methods before and after quantization are not significant for both INT4 and INT8. However, for $\eta$
and typical sampling, there are noticeable changes when quantizing Llama2-13B. Taking GSM8K as an example, $\eta$ sampling under INT8 quantization decreased by $13.65 \%$, while typical sampling under INT4 quantization improved by $6.14 \%$ on GSM8K. Typical and $\eta$ sampling are more influenced by quantization because their computing involves numerically unstable calculations. This may indicate that the impact of quantization on the information entropy of different tokens during decoding cannot be ignored.</p>
<h2>6 Conclusion</h2>
<p>This study offered a comprehensive analysis of diverse traditional and contemporary decoding methods in the context of LLMs. Our experiments shed light on the efficacy, robustness, efficiency, and universality of these decoding methods across a range of tasks, models, and settings. One primary finding is that the choice of decoding methods remains crucial and different decoding methods manifests different advantages in different scenarios. However we still provide some practical guidelines in Appendix I. We hope this investigation provides valuable insights and guidance for practitioners and researchers in selecting and advancing decoding methods for LLMs.</p>
<h2>Limitations</h2>
<p>Despite the thoroughness of our study, there are some inherent limitations. First, while we have explored a variety of tasks and models, the everevolving nature of LLMs implies that new models or tasks might display distinct behaviors. Second, although our analysis of hyperparameter sensitivity covers a wide range of commonly used configurations, it is not exhaustive and does not account for all possible hyperparameters. Lastly, this paper does not explore the integration of multiple decoding methods, such as combining temperature sampling with a repetition penalty mechanism.</p>
<h2>Acknowledgments</h2>
<p>This research is partly supported by the Shenzhen Science and Technology Program (JCYJ20220818101014030) and the "Graph Neural Network Project" of Ping An Technology (Shenzhen) Co., Ltd. Additionally, the work described in this paper is substantially funded by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200620).</p>
<h2>References</h2>
<p>AI@Meta. 2024. Llama 3 model card.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. ArXiv preprint, abs/2108.07732.</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. ArXiv preprint, abs/2309.16609.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. ArXiv preprint, abs/2204.05862.</p>
<p>Sourya Basu, Govardana Sachitanandam Ramachandran, Nitish Shirish Keskar, and Lav R. Varshney. 2021. Mirostat: a neural text decoding algorithm that directly controls perplexity. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.</p>
<p>Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017. Findings of the 2017 conference on machine translation (WMT17). In Proceedings of the Second Conference on Machine Translation.</p>
<p>Jing Chen, Xinyu Zhu, Cheng Yang, Chufan Shi, Yadong Xi, Yuxiang Zhang, Junjie Wang, Jiashu Pu, Rongsheng Zhang, Yujiu Yang, et al. 2024. Hollmwood: Unleashing the creativity of large language models in screenwriting via role playing. arXiv preprint arXiv:2406.11683.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374.</p>
<p>Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. 2023. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023a. Vicuna: An opensource chatbot impressing gpt-4 with $90 \%$ * chatgpt quality.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023b. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023).</p>
<p>Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. 2023. Dola: Decoding by contrasting layers improves factuality in large language models.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168.</p>
<p>Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding</p>
<p>Zeng, Xingkai Yu, Y Wu, et al. 2024. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066.</p>
<p>Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. ArXiv preprint, abs/2208.07339.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Markus Freitag and Yaser Al-Onaizan. 2017. Beam search strategies for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation.</p>
<p>Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic research. Blog post, April, 1.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9.</p>
<p>Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada.</p>
<p>John Hewitt, Christopher Manning, and Percy Liang. 2022. Truncation sampling as language model desmoothing. In Findings of the Association for Computational Linguistics: EMNLP 2022.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.</p>
<p>Daphne Ippolito, Reno Kriz, João Sedoc, Maria Kustikova, and Chris Callison-Burch. 2019. Comparison of diverse decoding methods from conditional language models. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.</p>
<p>Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,</p>
<p>Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. 2023. Openassistant conversations-democratizing large language model alignment. ArXiv preprint, abs/2304.07327.</p>
<p>Huayang Li, Tian Lan, Zihao Fu, Deng Cai, Lemao Liu, Nigel Collier, Taro Watanabe, and Yixuan Su. 2023a. Repetition in repetition out: Towards understanding neural text degeneration from the data perspective.</p>
<p>Siheng Li, Cheng Yang, Taiqiang Wu, Chufan Shi, Yuji Zhang, Xinyu Zhu, Zesen Cheng, Deng Cai, Mo Yu, Lemao Liu, Jie Zhou, Yujiu Yang, Ngai Wong, Xixin Wu, and Wai Lam. 2024. A survey on the honesty of large language models. arXiv preprint arXiv:2409.18786.</p>
<p>Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2023b. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023c. Alpacaeval: An automatic evaluator of instruction-following models.</p>
<p>Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. 2023a. The unlocking spell on base llms: Rethinking alignment via in-context learning. ArXiv preprint, abs/2312.01552.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out.</p>
<p>Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023b. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv.</p>
<p>Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A Smith, and Yejin Choi. 2021. Dexperts: Decoding-time controlled text generation with experts and anti-experts. arXiv preprint arXiv:2105.03023.</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. 2023. Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. arXiv e-prints, pages arXiv-2310.</p>
<p>Ruilin Luo, Tianle Gu, Haoling Li, Junzhe Li, Zicheng Lin, Jiayi Li, and Yujiu Yang. 2024a. Chain of history: Learning and forecasting with llms for temporal knowledge graph completion. arXiv preprint arXiv:2401.06072.</p>
<p>Ruilin Luo, Liyuan Wang, Binghuai Lin, Zicheng Lin, and Yujiu Yang. 2024b. Ptd-sql: Partitioning and targeted drilling with llms in text-to-sql. arXiv preprint arXiv:2409.14082.</p>
<p>Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. 2023. Locally typical sampling. Transactions of the Association for Computational Linguistics, 11 .</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.</p>
<p>Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. ArXiv preprint, abs/2305.14251.</p>
<p>Piotr Mirowski, Kory W Mathewson, Jaylen Pittman, and Richard Evans. 2023. Co-writing screenplays and theatre scripts with language models: Evaluation by industry professionals. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1-34.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Sean O'Brien and Mike Lewis. 2023. Contrastive decoding improves reasoning in large language models. ArXiv preprint, abs/2309.09117.</p>
<p>OpenAI. 2022. Introducing chatgpt. https://openai. com/blog/chatgpt.</p>
<p>OpenAI. 2023. GPT-4 technical report. ArXiv preprint, abs/2303.08774.</p>
<p>OpenAI. 2024. Gpt-4o. Accessed: 2024-05-13.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaïd Harchaoui. 2021. MAUVE: measuring the gap between neural text and human text using divergence frontiers. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual.</p>
<p>Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers.</p>
<p>Ricardo Rei, Ana C Farinha, José G.C. de Souza, Pedro G. Ramos, André F.T. Martins, Luisa Coheur, and Alon Lavie. 2022. Searching for COMETINHO: The little metric that could. In Proceedings of the 23rd Annual Conference of the European Association for Machine Translation, pages 61-70, Ghent, Belgium. European Association for Machine Translation.</p>
<p>Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code. ArXiv preprint, abs/2308.12950.</p>
<p>Chufan Shi, Deng Cai, and Yujiu Yang. 2024a. Lifi: lightweight controlled text generation with fine-grained control codes. arXiv preprint arXiv:2402.06930.</p>
<p>Chufan Shi, Cheng Yang, Yaxin Liu, Bo Shui, Junjie Wang, Mohan Jing, Linran Xu, Xinyu Zhu, Siheng Li, Yuxiang Zhang, et al. 2024b. Chartmimic: Evaluating lmm's cross-modal reasoning capability via chart-to-code generation. arXiv preprint arXiv:2406.09961.</p>
<p>Chufan Shi, Cheng Yang, Xinyu Zhu, Jiahao Wang, Taiqiang Wu, Siheng Li, Deng Cai, Yujiu Yang, and Yu Meng. 2024c. Unchosen experts can contribute too: Unleashing moe models' power by self-contrast. arXiv preprint arXiv:2405.14507.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. ArXiv preprint, abs/2206.04615.</p>
<p>Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. 2022. A contrastive framework for neural text generation. In Advances in Neural Information Processing Systems.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).</p>
<p>MosaicML NLP Team. 2023. Introducing mpt-30b: Raising the bar for open-source foundation models. Accessed: 2023-06-22.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aarelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models.</p>
<p>Ashwin K. Vijayakumar, Michael Cogswell, Ramprasaath R. Selvaraju, Qing Sun, Stefan Lee, David J. Crandall, and Dhruv Batra. 2018. Diverse beam search for improved description of complex scenes. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. 2024. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. arXiv preprint arXiv:2406.18521.</p>
<p>Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. 2024. From decoding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838.</p>
<p>Gian Wiher, Clara Meister, and Ryan Cotterell. 2022. On decoding strategies for neural text generators. Transactions of the Association for Computational Linguistics, 10.</p>
<p>Haoran Yang, Deng Cai, Huayang Li, Wei Bi, Wai Lam, and Shuming Shi. 2024. A frustratingly simple decoding method for neural text generation. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and</p>
<p>Evaluation (LREC-COLING 2024), pages 536-557, Torino, Italia. ELRA and ICCL.</p>
<p>Kevin Yang and Dan Klein. 2021. Fudge: Controlled text generation with future discriminators. arXiv preprint arXiv:2104.05218.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36.</p>
<p>Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. 2023. M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models. Advances in Neural Information Processing Systems, 36:5484-5505.</p>
<p>Xuanyu Zhang and Qing Yang. 2023. Self-qa: Unsupervised knowledge guided language model alignment. ArXiv preprint, abs/2305.11952.</p>
<p>Yuxiang Zhang, Jing Chen, Junjie Wang, Yaxin Liu, Cheng Yang, Chufan Shi, Xinyu Zhu, Zihao Lin, Hanwen Wan, Yujiu Yang, et al. 2024. Toolbehonest: A multi-level hallucination diagnostic benchmark for tool-augmented large language models. arXiv preprint arXiv:2406.20015.</p>
<p>Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015.</p>
<h2>A Decoding Strategies</h2>
<h2>A. 1 Deterministic Methods</h2>
<p>Greedy Search is arguably the simplest decoding strategy. At each time step $t$, it selects the token with the highest probability predicted by the model from the whole vocabulary set $\mathcal{V}$. Mathematically, the chosen token $y_{t}$ at time $t$ is:</p>
<p>$$
y_{t}=\underset{y \in \mathcal{V}}{\arg \max } P\left(y \mid \mathbf{x}, \mathbf{y}_{&lt;t}\right)
$$</p>
<p>where $\mathbf{x}$ is the original input and $\mathbf{y}_{&lt;t}$ is the generated tokens until time $t-1$. One drawback of greedy search is that it does not consider the global sequence score and can get stuck in local optima. This is why beam search is devised.</p>
<p>Beam Search (Freitag and Al-Onaizan, 2017) maintains a set, or "beam", of the $k$ most probable sequences at each time step, where the hyperparameter $k$ is referred to as the beam width. At time $t$, for each $\mathbf{y}<em t-1="t-1">{&lt;t} \in \mathcal{B}</em>$ :}$, where $\mathcal{B}_{t-1}$ is the set of $k$ most probable sequences at time $t-1$, it calculates a score for each token $y \in \mathcal{V</p>
<p>$$
\operatorname{score}\left(\mathbf{y}<em _t="&lt;t">{&lt;t}, y\right)=\log P\left(\mathbf{y}</em>\right)
$$}, y \mid \mathbf{x</p>
<p>Then, a new set $\mathcal{B}_{t}$ is obtained:</p>
<p>$$
\mathcal{B}<em _t="&lt;t">{t}=\underset{\mathbf{y}</em>} \in \mathcal{B<em _t="&lt;t">{t-1}, y \in \mathcal{V}}{\operatorname{argtopk}} \operatorname{score}\left(\mathbf{y}</em>, y\right)
$$</p>
<p>We specifically test beam size 4 and 8 in our experiment.</p>
<p>Diverse Beam Search (Vijayakumar et al., 2018) is a variant of beam search and aims to improve the diversity among the generated sequences. It divides the $k$ sequences into $G$ groups, each with a size of $k / G$ sequences. The algorithm operates in a similar way to the standard beam search, but instead of choosing the top- $k$ sequences from all candidate sequences, it selects the top- $k / G$ sequences for each group. The key difference lies in how the scores are calculated. In diverse beam search, a penalty is added to the score of a sequence if a similar sequence has already been in other groups:</p>
<p>$$
\begin{aligned}
&amp; \operatorname{score}\left(\mathbf{y}<em _mathbf_y="\mathbf{y">{&lt;t}, y\right)</em><em t-1="t-1">{&lt;t} \in \mathcal{B}</em>}^{g}}=\log P\left(\mathbf{y<em g_prime="g^{\prime">{&lt;t}, y \mid \mathbf{x}\right) \
&amp; -\lambda \sum</em>}&lt;g} \Delta\left(\left(\mathbf{y<em t="t">{&lt;t}, y\right), \mathcal{B}</em>\right)
\end{aligned}
$$}^{g^{\prime}</p>
<p>where $\Delta\left(\left(\mathbf{y}<em t="t">{&lt;t}, y\right), \mathcal{B}</em>}^{g^{\prime}}\right)$ is a measure of similarity between $\left(\mathbf{y<em t="t">{&lt;t}, y\right)$ and sequences within $\mathcal{B}</em>$. In our
experimental setup, we configure various $(k, G)$ pairs of $(4,2),(4,4),(8,2),(8,4)$, and the diversity penalty $\lambda$ is always set to 1 .}^{g^{\prime}</p>
<p>Contrastive Search (Su et al., 2022) assumes the LM has an isotropic representation space and adds a penalty term that decreases the generation probabilities of tokens producing hidden states that are very similar to the previous context. Formally, given the context $\left(\mathbf{x}, \mathbf{y}<em t="t">{&lt;t}\right)$, the selection of the output $y</em>$ follows</p>
<p>$$
\begin{gathered}
y_{t}=\underset{y \in V^{k}}{\arg \max }(1-\alpha) P\left(y \mid \mathbf{x}, \mathbf{y}<em y="y">{&lt;t}\right) \
-\alpha \max \left{s\left(h</em>\right)\right}
\end{gathered}
$$}, h_{v}\right): v \in\left(\mathbf{x}, \mathbf{y}_{&lt;t</p>
<p>where $\mathcal{V}^{k}$ is the set of top- $k$ predictions from the language model's probability distribution $P\left(y \mid \mathbf{x}, \mathbf{y}<em v="v">{&lt;t}\right) . \quad h</em>$ is the hidden states for the token $v$, and $s$ is the similarity function where the cosine similarity is usually adopted. We search $\alpha$ from $[0.1,0.2,0.3,0.4,0.5,0.6]$ in our experiment.</p>
<p>Contrastive Decoding (Li et al., 2023b) employs an additional amateur LM and penalizes undesired attributes associated with the amateur model. Formally, for each candidate token $y \in \mathcal{V}^{c}$</p>
<p>$$
\operatorname{score}\left(\left(\mathbf{x}, \mathbf{y}<em y="y">{&lt;t}\right), y\right)=(1+\beta) * \mathbf{u}</em>
$$}-\beta * \mathbf{v}_{y</p>
<p>$\mathbf{u}$ and $\mathbf{v}$ are the logits before softmax of the expert and amateur models respectively. These two models have the same tokenizer and the expert model is usually much larger than the amateur model. $\mathcal{V}^{c}$ is a set of candidate tokens selected based on the following criteria:</p>
<p>$$
\begin{aligned}
&amp; \mathcal{V}^{c}\left(\mathbf{x}, \mathbf{y}<em _exp="{exp" _text="\text">{&lt;t}\right)= \
&amp; \left{y \in \mathcal{V}: P</em>}}\left(y \mid \mathbf{x}, \mathbf{y<em _exp="{exp" _text="\text">{<t}\right)>\alpha \max P</em>\right)\right}
\end{aligned}
$$}}\left(\cdot \mid \mathbf{x}, \mathbf{y}_{&lt;t</p>
<p>In our experiment, we adopt TinyLlama-1.1B ${ }^{2}$ as the amateur model. We use the default setting with $\alpha$ set to 0.1 and we search $\beta$ from $[0.1,0.3,0.5,0.7,0.9]$.</p>
<p>Frustratingly Simple Decoding (Yang et al., 2024) exploits the contrasts between the LLM and an auxiliary anti-LM constructed based on the current prefix. There are two variants of FSD: FSD and FSD-d depending on whether the anti-LM is implemented as a vectorized or discrete $n$-gram model. Specifically, the FSD score is defined as</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$$
\begin{aligned}
\operatorname{FSD}(y \mid \mathbf{x}, \mathbf{y}<em _theta="\theta">{&lt;t})=(1-\alpha) P</em>}(y \mid \mathbf{x}, \mathbf{y<em _omega="\omega">{&lt;t})- \
\alpha \times P</em>)
\end{aligned}
$$}(y \mid \mathbf{x}, \mathbf{y}_{&lt;t</p>
<p>where $P_{\theta}$ and $P_{\omega}$ represent the LM and the antiLM respectively. The hyper-parameter $\alpha \geq 0$ is used to balance the two scores. In practice, it first selects the top- $k$ most probable tokens according to $P_{\theta}\left(\cdot \mid \mathbf{x}, \mathbf{y}<em h="h" t="t">{&lt;t}\right)$, denoted by $\mathcal{V}^{k}$. The token in $\mathcal{V}^{(k)}$ with the largest FSD score is chosen as the $t</em>$ token. We search $\alpha$ from $[0.1,0.2,0.3,0.4,0.5,0.6]$.</p>
<p>DoLa (Chuang et al., 2023) obtains the nexttoken distribution by contrasting the logits differences between the last layer and a premature layer. For Llama2-7b, the premature layer is dynamically selected from even-numbered layers from $[0,16)$ and $[16,32)$. For Llama2-13b, the ranges are $[0,20)$ and $[20,40)$. For Llama2-70b, the ranges are $[0,20)$ and $[60,80)$. They adopt the JensenShannon divergence (JSD) as the measure of distance between the next-word distributions and select the layer that has the largest JSD as the premature layer.</p>
<h2>A. 2 Stochastic Methods</h2>
<p>Temperature Sampling is a decoding strategy to control the randomness in the sampling process. Instead of directly sampling tokens from the predicted distribution, temperature sampling introduces a hyperparameter "temperature" $\tau$ that is used to adjust the probability distribution:</p>
<p>$$
P(y \mid \mathbf{x}, \mathbf{y}<em y="y">{&lt;t})=\frac{\exp \left(\mathbf{u}</em>
$$} / \tau\right)}{\sum_{j} \exp \left(\mathbf{u}_{j} / \tau\right)</p>
<p>where $\mathbf{u}_{y}$ is the logit of $y$ before softmax. We conduct our experiment for $\tau$ within the range of 0.1 to 0.9 , incrementing in value of 0.1 .</p>
<p>Top- $k$ Sampling (Fan et al., 2018) is used to ensure that the less probable words, which are in the unreliable tail of the distribution (Holtzman et al., 2020), should not have any chance to be selected. Only top- $k$ probable tokens are considered for a generation. we explore a range of $k$ values, specifically $[5,10,20,50,100]$.</p>
<p>Top- $p$ Sampling (Holtzman et al., 2020) considers the minimal set of top tokens $\mathcal{V}^{p}$ that cover a specified percentage $p$ of the distribution:</p>
<p>$$
\sum_{y \in \mathcal{V}^{p}} P(y \mid \mathbf{x}, \mathbf{y}_{&lt;t}) \geq p
$$</p>
<p>For our study, we have examined various $p$ thresholds, specifically $[0.8,0.85,0.9,0.95,1]$.</p>
<p>Typical Sampling (Meister et al., 2023) sorts the vocabulary according to the differences between distribution entropy and probabilities. The authors argue that the desired sequences should have information content close to the expected information content, i.e., the conditional entropy of the model. The candidate set $\mathcal{V}^{c}$ is a solution of the following problem:</p>
<p>$$
\begin{gathered}
\min <em _in="\in" _mathcal_V="\mathcal{V" y="y">{\mathcal{V}^{c}} \sum</em>}^{c}}\left|H\left(Y_{t} \mid \mathbf{x}, \mathbf{y<em _t="&lt;t">{&lt;t}\right)+\log P(y \mid \mathbf{x}, \mathbf{y}</em>)\right| \
\text { s.t. } \sum_{y \in \mathcal{V}^{c}} P(y \mid \mathbf{x}, \mathbf{y}_{&lt;t}) \geq p
\end{gathered}
$$</p>
<p>In our experiments, we vary the threshold $p$ across the values $[0.2,0.9,0.92,0.95]$ to examine its effect on sequence generation.</p>
<p>Top- $\eta$ Sampling (Hewitt et al., 2022) truncates words whose probabilities are below an entropydependent threshold. The candidate set $\mathcal{V}^{c}$ is determined by:</p>
<p>$$
\begin{aligned}
&amp; \mathcal{V}^{c}= \
&amp; \left{y \in \mathcal{V} \mid P(y \mid \mathbf{x}, \mathbf{y}<em _theta_mathbf_x="\theta,(\mathbf{x">{&lt;t}) \geq \sqrt{\eta} \exp \left(-h</em>\right)\right}
\end{aligned}
$$}, \mathbf{y}_{&lt;t})</p>
<p>where $h_{\theta,(\mathbf{x}, \mathbf{y}<em _t="&lt;t">{&lt;t})}$ is the entropy of $P\left(Y \mid \mathbf{x}, \mathbf{y}</em>\right) . \quad \eta$ is searched from $[0.0003,0.0006,0.0009,0.002,0.004]$.</p>
<p>Mirostat Sampling (Basu et al., 2021) directly control the perplexity rate of the generated text. It firstly estimates the value of $s$ assuming words follow Zipf's law where $s$ is an exponent characterizing the distribution. Then it uses top- $k$ sampling to generate the new token where $k$ is a function of the estimated $s$ and of the target perplexity $\tau$ of the output text. We search $\tau$ from $[2.5,3,4,5]$.
In this work, we focus solely on vanilla generation methods. We do not discuss other modelspecific (Shi et al., 2024c), task-specific (Yang and Klein, 2021; Liu et al., 2021; Shi et al., 2024a), or meta-generation methods (Welleck et al., 2024) (e.g., Tree-of-Thoughts (Yao et al., 2024)). These specialized decoding approaches are beyond the scope of our current analysis.</p>
<h2>B Evaluation Benchmarks</h2>
<h2>B. 1 Coding</h2>
<p>HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) are extensively utilized benchmarks within the measurement of LLM's code generating ability. These benchmarks encompass a vast collection of Python programming problems.</p>
<p>HumanEval (Chen et al., 2021) consists of 164 original programming problems by giving docstrings to generate code, which has an average of 9.6 test cases allocated to each problem. We use 0 -shot prompt for both unaligned and aligned models.</p>
<p>MBPP (Austin et al., 2021) focus on generating code based on textual descriptions, which offers a set of 500 test programming problems, accompanied by three automated test cases per problem. We use 0 -shot prompt for aligned models and 3-shot prompt for unaligned models.</p>
<h2>B. 2 Math Problem Soving</h2>
<p>We utilize GSM8K (Cobbe et al., 2021) for assessing reasoning and problem-solving proficiencies within the domain of mathematics.</p>
<p>GSM8K (Cobbe et al., 2021) collects 1,319 high-quality linguistically diverse grade school math word problems as the test set, and reports 8 -shot pass@1 accuracy. We use 0 -shot prompt for aligned models and 8 -shot prompt for unaligned models.</p>
<h2>B. 3 Summarization</h2>
<p>We select the CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) datasets, which are the most well-studied datasets in the literature on summarization faithfulness. This also ensures domain coverage of news-type data. Importantly, these datasets differ along a central axis studied in summarization:</p>
<p>XSUM (Narayan et al., 2018) is a dataset with largely abstractive reference summaries (meaning the string overlap between the document and its summary in the dataset is relatively small on average) which feature articles from the British Broadcasting Corporation (BBC). The test splits for the dataset are 11.5 K examples. We use 0 -shot prompt for aligned models and 1-shot prompt for unaligned models.</p>
<p>CNN/DailyMail (Hermann et al., 2015) is a dataset with largely extractive reference summaries that contain news articles from CNN and the DailyMail along with highlights that act as a summary for the article. The test splits for the dataset are 11.3 K examples. We use 0 -shot prompt for aligned models and 1-shot prompt for unaligned models. The model-generated summary is compared against a human-authored reference summary using automated metrics for overall quality ROUGE-L (Lin, 2004). Note that we randomly select 1,000 cases each from CNNDailyMail and XSUM for evaluation.</p>
<h2>B. 4 Translation</h2>
<p>We evaluate the translation performance on WMT22 (Bojar et al., 2017) test sets.</p>
<p>WMT22 Competition (Bojar et al., 2017) constructed based on more recent content from various domains, including news, social, e-commerce, and conversational domains. The numbers of samples for $\mathrm{De} \Rightarrow \mathrm{En}, \mathrm{En} \Rightarrow \mathrm{De}, \mathrm{Zh} \Rightarrow \mathrm{En}$ and $\mathrm{En} \Rightarrow \mathrm{Zh}$ tasks are 1984, 2037, 1875 and 2037, respectively. For automatic evaluation, we adopt BLEU (Papineni et al., 2002) implementated in SacreBLEU (Post, 2018) ${ }^{3}$. We use 3-shot prompt for unaligned models.</p>
<h2>B. 5 Commonsense reasoning</h2>
<p>Commonsense reasoning is key for interacting with the world and is still beyond the reach of current natural language understanding systems (Talmor et al., 2019). We consider measuring open-ended performance on two datasets covering a diverse range of commonsense reasoning types from BIGBench (Srivastava et al., 2022), CommonsenseQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021).</p>
<p>CommonsenseQA (Talmor et al., 2019) asks commonsense questions about the world involving complex semantics that often require prior knowledge. There are a total of 1.22 k instances in the CommonsenseQA validation set. We use 6-shot prompt for aligned models and 1-shot prompt for unaligned models.</p>
<p>StrategyQA (Geva et al., 2021) requires models to infer a multi-hop strategy to answer questions. We use the open-domain setting (questiononly set) from BIG-Bench (Srivastava et al., 2022)</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>which contains 2.29 k test instances. We use 0 -shot prompt for aligned models and 4 -shot prompt for unaligned models. The two BIG-bench tasks do not have training sets, so we select the first ten examples as exemplars in the evaluation set as few-shot exemplars and report accuracy on the rest of the evaluation set.</p>
<h2>B. 6 Factual Knowledge</h2>
<p>Factual Knowledge refers to their tendency to generate factual errors. This is considered a critical issue in LLMs because it is challenging for users to identify and poses real-life risks.</p>
<p>FActScore (Min et al., 2023) scrutinizes the factual accuracy of biographies generated by LLMs for 500 specific individuals. Conducting a pipeline to transform a long-form model generation into pieces of atomic statements and measure the atomic statement's accuracy with retrieved knowledge. We use 0 -shot prompt for aligned models.</p>
<h2>B. 7 Instruction Following</h2>
<p>For our research, we select the representative broadcoverage benchmark Alpace-eval (Li et al., 2023c).</p>
<p>Alpace-eval (Li et al., 2023c) assess the LLM's generation quality by 805 prompts from several sources: Vicuna (Chiang et al., 2023b) (80 prompts), Self-instruct (Zhang and Yang, 2023) (252 prompts), Open Assistant (Köpf et al., 2023) (188 prompts), Koala (Geng et al., 2023) (156 prompts), HH_RLHF (Bai et al., 2022) (129 prompts), quantifying the pairwise Win Rate against a reference model, Text-Davinci-003.</p>
<h2>B. 8 Open-ended Text Generation</h2>
<p>Open-ended text generation aims to craft fluent and coherent textual continuations of given prompts. Following (Li et al., 2023b), we evaluate three domains for open-ended text generation: Book, Wikinews, Wikitext.</p>
<p>Book contains 1,947 prompts collected from BookCorpus (Zhu et al., 2015)for story generations. We use 0 -shot prompt for both unaligned and aligned models.</p>
<p>Wikinews include 2,000 news articles prompts collected from Wikinews ${ }^{4}$. We use 0 -shot prompt for both unaligned and aligned models.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Wikitext select 1,314 prompts from wikitext-103 (Merity et al., 2017) as the Wikipedia representative domain. We use 0 -shot prompt for both unaligned and aligned models. We utilize MAUVE (Pillutla et al., 2021) score (the higher the better) to measure the distribution similarity between the set of generated text and the set of gold references. Note that we randomly select 500 cases each from among the three domains mentioned above for evaluation.</p>
<h2>C Instruction Template</h2>
<p>The instruction templates for each dataset are list from Table 6 to Table 25.</p>
<h2>D Different Foundation Models</h2>
<p>We extend our analysis to investigate the decoding methods under different foundation models ${ }^{5}$. We select several representive models, including CodeLlama-7b (Rozière et al., 2023), Qwen7B (Bai et al., 2023), MPT-7b (Team, 2023), Mistral-7B (Jiang et al., 2023), deepseek-moe-16bbase (Dai et al., 2024), Llama-3-8B (AI@Meta, 2024) and their aligned versions. Addtionally we select vicuna-7b-v1.5 (Chiang et al., 2023a) which is an SFT-ed model from llama2-7B without RHLF for analysis. It is crucial to underscore that these models vary significantly in several aspects, such as pre-training data, model architecture, and etc. As illustrated in Tables 26, the results observed in $\S 4.1$ are still applicable to LLMs with different architectures. Detailed as below: i) For unaligned models, deterministic methods generally perform better than stochastic methods on all tasks except open-ended text generation. ii) Aligned models are less dependent on decoding methods than unaligned models. iii) Among stochastic methods, temperature sampling generally performs better, particularly when using unaligned models. Apart from these consistencies, it is worth noting that different decoding methods may result in different performance rankings for LLMs. For instance, Codellama outperforms Qwen by $7.60 \%$ in the MBPP with top- $k$ sampling, yet lags behind by $2.20 \%$ with $\eta$ sampling. This implies that different models still have varying adaptability to specific decoding methods, suggesting that the selection of decoding strategies should be more meticulously rigorous during the evaluation of LLMs.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h1>Prompt for HumanEval</h1>
<p>[DOCSTRING]</p>
<p>Table 6: 0-shot prompt for HumanEval (unaligned model).</p>
<h2>Prompt for HumanEval</h2>
<p>Please complete the remaining Python function code based on the following docstring content. [DOCSTRING]</p>
<p>Table 7: 0-shot prompt for HumanEval (aligned model).</p>
<h2>E Settings of Hyperparameters</h2>
<p>The optimal hyperparameters for each decoding method across different datasets and models are listed from Table 27 to Table 31.</p>
<h2>F Analyses of Generation Diversity</h2>
<p>Diversity is a more meaningful metric for openended tasks than closed-ended ones. Therefore, we report the diversity scores on Wikinews using Llama2-Chat-7B and Llama2-7B models in Table 32. Specifically, we adopt the diversity measure defined in Yang et al., 2024, which computes the degree of repetition across all generations at different n-gram levels. It can be observed that best-performing stochastic methods do not necessarily exhibit higher diversity than best-performing deterministic methods. Concretely, for Llama2-7B, the diversity score of FSD is the highest, while for Llama2-Chat, CD obtains the highest.</p>
<h2>G Analyses of COMET Score on WMT tasks.</h2>
<p>Both COMET (Rei et al., 2022) and BLEU (Post, 2018) are important metrics for translation tasks. We provide the COMET results for the translation tasks in Table 33. It can be observed that, similarly, for unaligned models, deterministic methods generally perform better than stochastic methods according to the COMET metric.</p>
<h2>H Ouput of DoLa</h2>
<p>The output examples that DoLa fails to terminate its generation appropriately are listed in Table 34.</p>
<h2>I Practical Guidelines</h2>
<p>Our study underscores the significance of selecting an appropriate decoding method in the era of large language models (LLMs). Despite the advancements in LLMs, our findings indicate that the choice of decoding method remains critical and
cannot be overlooked. This decision is contingent upon several factors, including the specific test task, the model being used, and the priority-whether it is performance, robustness, or speed. The core contribution of our paper lies in demonstrating the nuanced and complex nature of decoding method selection. The optimal decoding strategy is not universally applicable and varies based on the aforementioned factors. This complexity underscores the necessity for a comprehensive evaluation framework in future research and highlights the need for practitioners to consider multiple dimensions when deploying LLMs. Despite the intricacies involved, we offer several practical guidelines for deploying LLMs without extensive hyperparameter searching: For quick setup, Unaligned Models (e.g., Llama27B): For these models, we recommend using either FSD or FSD-d; Aligned Models (e.g., Llama2-7BChat): BS or DBS is advised for aligned models to achieve satisfactory performance. When computational resources allow for self-consistency: Unaligned Models (e.g., Llama2-7B): implementing temperature sampling with an optimal temperature setting of 0.7 is recommended to enhance model performance. Aligned Models (e.g., Llama2-7BChat): a higher optimal temperature of 0.9 is suggested.</p>
<h2>J Ethics and Societal Impact</h2>
<p>Ethical Considerations. Our work highlights the importance of transparency in LLMs, particularly in how decoding methods influence LLM outputs. The variability in performance across tasks and models underscores the need for clear communication about the limitations and potential biases of these systems. Researchers and practitioners must be mindful that the choice of decoding method can significantly impact the generated content, potentially amplifying or mitigating biases present in the underlying models.</p>
<h1>PROMPT FOR MBPP</h1>
<p>You are an expert Python programmer, and here is your task: Write a function to find the similar elements from the given two tuple lists. Your code should pass these tests:
assert similar_elements((3, 4, 5, 6), (5, 7, 4, 10)) == (4, 5)
assert similar_elements((1, 2, 3, 4), (5, 4, 3, 7)) == (3, 4)
assert similar_elements((11, 12, 14, 13), (17, 15, 14, 13)) == (13, 14)
[BEGIN]
def similar_elements(test_tup1, test_tup2):
res = tuple(set(test_tup1) \&amp; set(test_tup2))
return (res)
[DONE]</p>
<p>You are an expert Python programmer, and here is your task: Write a python function to identify non-prime numbers. Your code should pass these tests:
assert is_not_prime(2) == False
assert is_not_prime(10) == True
assert is_not_prime(35) == True
[BEGIN]
import math
def is_not_prime(n):
result = False
for i in range(2,int(math.sqrt(n)) + 1):
if $\mathrm{n} \% \mathrm{i}==0$ :
result $=$ True
return result
[DONE]</p>
<p>You are an expert Python programmer, and here is your task: Write a function to find squares of individual elements in a list using lambda function. Your code should pass these tests:
assert square_nums([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])==[1, 4, 9, 16, 25, 36, 49, 64, 81, 100])
assert square_nums([10,20,30]))==([100,400,900]))
assert square_nums([12,15]))==([144,225]))
[BEGIN]
def square_nums(nums):
square_nums = list(map(lambda x: x ** 2, nums))
return square_nums
[DONE]</p>
<p>You are an expert Python programmer, and here is your task: [TASK_DEFINATION]. Your code should pass these tests:
[TEST_CASE_1]
[TEST_CASE_2]
[TEST_CASE_2]
[BEGIN]
Table 8: 3-shot promp for MBPP (unaligned model).</p>
<p>Societal Impact. The findings of this study have far-reaching implications for the deployment of LLMs in real-world applications. By elucidating the trade-offs between performance, robustness, and speed, our work empowers developers to make more informed decisions when implementing these models in diverse contexts. This could lead to more reliable and efficient AI systems in critical areas such as healthcare, education, and public services. However, it also raises concerns about the potential for misuse or overreliance on these systems without a full understanding of their limitations. The observed task-dependency of decoding methods' performance suggests that careful consideration is needed when applying LLMs to different domains. This is particularly crucial in high-stakes applica-
tions where the consequences of model outputs can be significant. Our work also highlights the potential for advanced decoding methods to improve model performance, which could accelerate the adoption of AI technologies across various sectors of society.</p>
<h2>K Future Work</h2>
<p>Holistic Evaluations Across Diverse Contexts. While our study sheds light on the performance, robustness, and speed of various decoding methods, expanding these evaluations to encompass even more varied tasks, languages, and dataset types would provide deeper insights into the generalizability of our findings. This includes tasks</p>
<h1>PROMPT FOR MBPP</h1>
<p>You are an expert Python programmer, and here is your task: [TASK_DEFINATION]. Your code should pass these tests: [TEST_CASE_1]
[TEST_CASE_2]
[TEST_CASE_2]
like temporal knowledge knowledge graph completion (Luo et al., 2024a), text-to-sql (Luo et al., 2024b), and etc. Additionally, testing with lowresource languages and under-represented dialects is also worth exploring (Zhang et al., 2023).</p>
<p>User-Centric Evaluation Metrics. There is a need for developing new evaluation metrics that more directly reflect user satisfaction and realworld efficacy. Incorporating user feedback loops and live deployment scenarios can aid in better understanding the practical utility of different decoding method (Mirowski et al., 2023).</p>
<p>Extending to New Tasks. Although our study validates decoding methods across a wide range of tasks, the rapid evolution of LLMs introduces new tasks for future validation. For instance, evaluating models on attributes like honesty and exploring how different methods can contribute to deploying more honest and transparent models is a pertinent area (Li et al., 2024; Zhang et al., 2024). In open-ended scenarios such as human-AI collaboration, beyond simple news generation from a prefix, LLMs need to better cooperate in creative processes to generate both reliable and diverse texts like screenwriting (Chen et al., 2024). Future decoding research should thus focus on facilitating such cooperation.</p>
<h2>Extending to Large Multimodal Models.</h2>
<p>While our current focus is on decoding for LLMs, future work should extend to examining decoding methods in large multimodal models (OpenAI, 2024; Chen et al., 2023). Investigating the effectiveness of these methods in text-to-image, multimodal question answering (Wang et al., 2024), math reasoing (Lu et al., 2023) and code generation (Shi et al., 2024b) necessitates the attention of future work.</p>
<h1>PROMPT FOR GSM8K</h1>
<p>Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?
Answer: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been $21-15=6$. The answer is 6 .</p>
<p>Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
Answer: There are originally 3 cars. 2 more cars arrive. $3+2=5$. The answer is 5 .
Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
Answer: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had $32+42=74$. After eating 35 , they had $74-35=39$. The answer is 39 .</p>
<p>Question: Question: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?
Answer: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny $20-12=8$. The answer is 8 .</p>
<p>Question: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?
Answer: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. $5+4=9$. The answer is 9 .</p>
<p>Question: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?
Answer: There were originally 9 computers. For each of 4 days, 5 more computers were added. So $5 * 4=20$ computers were added. $9+20$ is 29 . The answer is 29 .</p>
<p>Question: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?
Answer: Michael started with 58 golf balls. After losing 23 on tuesday, he had $58-23=35$. After losing 2 more, he had $35-2=33$ golf balls. The answer is 33 .</p>
<p>Question: Olivia has $\$ 23$. She bought five bagels for $\$ 3$ each. How much money does she have left?
Answer: Olivia had 23 dollars. 5 bagels for 3 dollars each will be $5 \times 3=15$ dollars. So she has $23-15$ dollars left. 23 15 is 8 . The answer is 8 .</p>
<p>Question: [QUESTION]
Answer:
Table 10: 8-shot prompt for GSM8K (unaligned model).</p>
<h2>PROMPT FOR GSM8K</h2>
<p>Please answer the math questions below.
[QUESTION]
You need to first take step-by-step reasoning and then give the final result.
Table 11: 0-shot prompt for GSM8K (aligned model).</p>
<h1>PROMPT FOR XSUM</h1>
<p>Article: The Bath-born player, 28, has made 36 appearances for the Dragons since joining from Wasps in 2015. He is in his second season and signed a contract extension in December 2016. Dragons forwards coach Ceri Jones said: "It's a big blow. Eddie has been excellent all year for us, he has really stepped up to the mark and will be a big loss." However, Jones says Jackson's misfortune can be a chance for others to thrive. "We are very fortunate to have the likes of Ollie Griffiths, Harrison Keddie, James Thomas who can come into the back-row," said Jackson. "Harri has shown glimpses of what he can do all season and there's definitely a player there, so this is an opportunity." Dragons travel to Munster in the Pro12 on Friday.
Summarize the above article in 1 sentence.
Newport Gwent Dragons number eight Ed Jackson has undergone shoulder surgery and faces a spell on the sidelines. Article: [ARTICLE]
SSummarize the above article in 1 sentence.
Table 12: 1-shot prompt for XSUM (unaligned model).</p>
<h2>PROMPT FOR XSUM</h2>
<p>Article: [ARTICLE]
Summarize the above article in 1 sentence.
Table 13: 0-shot prompt for XSUM (aligned model).</p>
<h2>PROMPT FOR CNNDAILYMAIL</h2>
<p>Article: PARIS, France (CNN) - Interpol on Monday took the unprecedented step of making a global appeal for help to identify a man from digitally reconstructed photos taken from the Internet that it said showed him sexually abusing underage boys. This moving image shows how police used software to unscramble the image. (Source: Interpol) The man's face was disguised by digital alteration, but the images were capable of being restored, according to a bulletin from Interpol - the international police agency based in Lyon, France. Interpol Secretary General Ronald K. Noble said the pictures have been on the the Internet for several years, but investigators have been unable to determine the man's identity or nationality. "We have tried all other means to identify and to bring him to justice, but we are now convinced that without the public's help this sexual predator could continue to rape and sexually abuse young children whose ages appear to range from six to early teens," Noble said. He said there is "very good reason to believe that he travels the world in order to sexually abuse and exploit vulnerable children." Interpol has determined the photos were taken in Vietnam and Cambodia. "The decision to make public this man's picture was not one which was taken lightly," said Kristin Kvigne, assistant director of Interpol's Trafficking in Human Beings Unit. The suspect's photo and more information can be seen online at Interpol's Web site. E-mail to a friend .
Summarize the above article in 3 sentences.
Man posted photos on the Internet of himself sexually abusing underage boys. Computer experts managed to undo digital masking to reveal the man. Man abused 12 boys in Vietnam and Cambodia .</p>
<p>Article: [ARTICLE]
Summarize the above article in 3 sentences.
Table 14: 1-shot prompt for CNN/Dailymail (unaligned model).</p>
<h2>PROMPT FOR CNNDAILYMAIL</h2>
<p>Article: [ARTICLE]
Summarize the above article in 3 sentences.
Table 15: 0-shot prompt for CNN/Dailymail (aligned model).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ http://www.wikinews.org&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5} \mathrm{CD}$ and DoLa are not included. Because it is challenging to find an amateur model for each foundation model for CD , and for DoLa, it is difficult to determine the appropriate number of layers for logits comparison for individual models.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
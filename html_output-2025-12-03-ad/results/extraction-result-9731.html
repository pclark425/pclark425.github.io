<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9731 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9731</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9731</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-273350620</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.11594v1.pdf" target="_blank">Black-box Uncertainty Quantification Method for LLM-as-a-Judge</a></p>
                <p><strong>Paper Abstract:</strong> LLM-as-a-Judge is a widely used method for evaluating the performance of Large Language Models (LLMs) across various tasks. We address the challenge of quantifying the uncertainty of LLM-as-a-Judge evaluations. While uncertainty quantification has been well-studied in other domains, applying it effectively to LLMs poses unique challenges due to their complex decision-making capabilities and computational demands. In this paper, we introduce a novel method for quantifying uncertainty designed to enhance the trustworthiness of LLM-as-a-Judge evaluations. The method quantifies uncertainty by analyzing the relationships between generated assessments and possible ratings. By cross-evaluating these relationships and constructing a confusion matrix based on token probabilities, the method derives labels of high or low uncertainty. We evaluate our method across multiple benchmarks, demonstrating a strong correlation between the accuracy of LLM evaluations and the derived uncertainty scores. Our findings suggest that this method can significantly improve the reliability and consistency of LLM-as-a-Judge evaluations.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9731.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9731.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthfulQA comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge versus human annotations on TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison showing how LLM judges (instruct models) align with human truthfulness labels on the TruthfulQA dataset; includes baseline, high-uncertainty and low-uncertainty accuracy figures per model and discussion of divergence from human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Question Answering (truthfulness classification)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Llama-3-8B-Instruct, Llama-3-70B-Instruct, Mixtral-8x7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Confusion-based uncertainty: generate biased verbalized assessments for each discrete option, mix each assessment with every option to build n^2 confusion prompts, record token log-probabilities of final choice, construct confusion matrix and label low/high uncertainty by thresholding mean token probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotations available for TruthfulQA used as ground truth; paper treats human labels as reference for computing LLM accuracy (specific annotator counts not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy (whether LLM-chosen option matches human label). Table 1 reports: Baseline accuracies — Llama-3-8B: 0.62, Llama-3-70B: 0.78, Mixtral-8x7B: 0.66; Low-uncertainty accuracies — 0.68, 0.91, 0.81 respectively. (High-uncertainty also reported.)</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>LLM-as-a-Judge evaluations do not always align with human judgments and can be incorrect or misleading; reliance on LLM-only judgments risks overconfidence in outputs without human cross-checks; coverage of 'trustworthy' (low-uncertainty) judgements is dataset- and model-dependent so many evaluations may lack a reliable signal compared to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Paper notes general divergence from human judgments (Introduction). Concrete numbers: baseline vs low-uncertainty differences show variability across models (see Table 1); smaller models often produce far fewer low-uncertainty labels (see Results), reducing number of LLM judgments that can be trusted relative to human coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Low-uncertainty labels correlate with higher accuracy; for Llama-3-70B low-uncertainty accuracy (0.91) substantially exceeds baseline and can outperform human-level agreement in some settings. Thus when the method marks a judgment as low-uncertainty, it often matches or exceeds human alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Introduction; Results (Uncertainty labeling correlates with accuracy); Table 1 (TruthfulQA rows)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9731.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9731.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reliance Study comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge versus human ratings on the Reliance Study dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of LLM judges on a dataset measuring reliance on LLM outputs (binary criteria like accuracy/naturalness), showing model-dependent proportions of low-uncertainty trustworthy labels and corresponding accuracy relative to human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Conversational / reliance evaluation (binary classification of accuracy/naturalness)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Llama-3-8B-Instruct, Llama-3-70B-Instruct, Mixtral-8x7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Confusion-based uncertainty evaluation (biased assessments per option, n^2 confusion prompts, token-probability confusion matrix, threshold-based low/high uncertainty labeling).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Original Reliance Study human annotations used as reference (paper does not detail annotator counts in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy vs human labels. Table 1 reports Baseline: 0.49, 0.63, 0.64 (Llama-3-8B, 70B, Mixtral); Low-uncertainty: 0.50, 0.85, 1.00 respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Smaller models produce very few low-uncertainty (trustworthy) labels (<5% for some datasets), meaning fewer LLM judgments can be used as substitutes for humans; variability across models implies inconsistent coverage and reliability compared to human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Results state that in datasets such as the Reliance Study smaller models classify less than 5% of cases as low uncertainty while Llama-3-70B exceeds 15%, indicating loss of usable, high-confidence LLM evaluations for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>When LLM judgments are labeled low-uncertainty they often show very high accuracy (e.g., Mixtral low-uncertainty accuracy = 1.00), indicating that selective use of only low-uncertainty LLM judgments can approximate or exceed human agreement for those items.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Results (Variability in low uncertainty proportions; Low uncertainty consistently leads to higher accuracy); Table 1 (Reliance Study rows)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9731.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9731.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Summarization CNN/DM comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge versus human multi-rater evaluations on CNN/DM summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis of LLM judge performance on 1–5 rating tasks for summarization, comparing baseline and uncertainty-conditioned accuracies and juxtaposing these with human inter-rater agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Abstractive summarization rating (coherence, fluency, relevance) on CNN/DM</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Llama-3-8B-Instruct, Llama-3-70B-Instruct, Mixtral-8x7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Confusion-based uncertainty with per-option biased assessments, confusion prompts, token-probability matrix and thresholded uncertainty labeling; applied to multi-class (1–5) rating criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>CNN/DM ratings include multiple human raters; inter-rater agreement reported (0.60 in Table 1) but exact annotator counts not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy (LLM-chosen rating matches human rating) and Human Agreement (inter-rater). Table 1: Baseline accuracies — 0.28, 0.25, 0.16; Low-uncertainty accuracies — 0.75, 0.26, 0.50; Human agreement = 0.60 (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>LLM-as-a-Judge can diverge substantially from human multi-rater judgments depending on model; some models (e.g., Llama-3-70B) show low-uncertainty accuracy below human agreement for this task, indicating potential loss of human-like nuance or consistency; smaller models often produce very few low-uncertainty labels, reducing usable coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>For Summarization CNN/DM, Llama-3-70B low-uncertainty accuracy = 0.26 which is well below reported human agreement (0.60), an explicit case where LLM-as-a-Judge does not match human inter-rater performance. The paper also notes smaller deviation between correct ratings and LLM-selected ratings only when low-uncertainty labels are present.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Llama-3-8B low-uncertainty accuracy = 0.75 exceeds human agreement (0.60), demonstrating that some models + uncertainty selection can match or outperform human agreement on selected items; however, the proportion of items labeled low-uncertainty may be small.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Results (Smaller deviation in multi-classification ratings; Low uncertainty labels approach human agreement); Table 1 (Summarization CNN/DM rows)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9731.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9731.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feedback Collection comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge versus human ratings on the Feedback Collection dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of instruct models as judges on a fine-grained 1–5 feedback rating dataset; demonstrates that low-uncertainty LLM judgments can achieve perfect agreement on those items, while the proportion of such items is small.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Feedback rating collection (fine-grained evaluative ratings)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Llama-3-8B-Instruct, Llama-3-70B-Instruct, Mixtral-8x7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Confusion-based uncertainty method (biased assessments per option, confusion prompts, confusion matrix of token probabilities, threshold-based uncertainty labels); applied to 1–5 rating scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Feedback Collection dataset includes human ratings used as ground truth (paper does not list annotator counts); used to compute LLM accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy vs human labels. Table 1 reports Baseline: 0.39, 0.40, 0.32 (Llama-3-8B, 70B, Mixtral); Low-uncertainty accuracies = 1.00, 1.00, 1.00 respectively (but proportion of low-uncertainty labels <10% for all models).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Although low-uncertainty LLM judgments can be perfectly accurate, the coverage is very limited (low proportion of items labeled low-uncertainty), meaning large parts of evaluation remain untrusted compared to human evaluators; computational cost of the method is also a practical loss relative to human evaluation workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Paper states that the proportion of low-uncertainty labels is below 10% for all models on Feedback Collection, yet these labels achieve 100% accuracy—illustrating loss of quantity/coverage even when quality is high for selected items.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>On items labeled low-uncertainty, LLM-as-a-Judge matches human annotations perfectly in this dataset (1.00 accuracy), showing LLMs can substitute for humans selectively; threshold tuning affects trade-off between proportion and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Results (Low uncertainty consistently leads to higher accuracy; Variability in low uncertainty proportions); Table 1 (Feedback Collection rows)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9731.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9731.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FeedbackQA comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge versus human ratings on FeedbackQA (retrieval-based QA with feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison showing that low-uncertainty LLM judgments can reach or exceed human inter-rater agreement for selected items, but smaller models produce few low-uncertainty labels and baseline alignment varies by model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Retrieval-based question answering with interactive user feedback (rating scale: excellent to bad)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Llama-3-8B-Instruct, Llama-3-70B-Instruct, Mixtral-8x7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Confusion-based uncertainty approach (biased assessments, confusion prompts, token-probability confusion matrix, thresholding to produce low/high uncertainty labels), applied to multi-class ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>FeedbackQA contains human-generated ratings used as reference; inter-rater agreement reported as 0.48 in Table 1 (annotator counts not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy versus human ratings. Table 1 reports Baseline: 0.47, 0.69, 0.29; Low-uncertainty accuracies = 1.00, 0.71, 1.00 respectively; Human Agreement = 0.48.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>LLM-as-a-Judge shows model-dependent variability; smaller models often produce very low proportions of low-uncertainty labels reducing effective coverage of trustworthy judgments compared to humans; where LLMs fail to produce low-uncertainty signals, relying on them loses the broad, consistent coverage humans provide.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Paper reports smaller models classify less than 5% of cases as low uncertainty in datasets including FeedbackQA, meaning most items cannot be safely judged by those LLMs without human oversight. Also baseline accuracies vary substantially across models (e.g., Mixtral baseline 0.29 vs Llama-3-70B 0.69).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Low-uncertainty LLM judgments can achieve perfect agreement (1.00) for some models/datasets (e.g., Llama-3-8B and Mixtral here) and often meet or exceed human inter-rater agreement for selected items; threshold tuning and larger models increase the proportion of reliable LLM judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Results (Low uncertainty labels approach human agreement; Variability in low uncertainty proportions); Table 1 (FeedbackQA rows)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Black-box Uncertainty Quantification Method for LLM-as-a-Judge', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>Finding blind spots in evaluator llms with interpretable checklists <em>(Rating: 2)</em></li>
                <li>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms <em>(Rating: 2)</em></li>
                <li>Replacing judges with juries: Evaluating llm generations with a panel of diverse models <em>(Rating: 2)</em></li>
                <li>Prometheus: Inducing evaluation capability in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9731",
    "paper_id": "paper-273350620",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "TruthfulQA comparison",
            "name_full": "Comparison of LLM-as-a-Judge versus human annotations on TruthfulQA",
            "brief_description": "Empirical comparison showing how LLM judges (instruct models) align with human truthfulness labels on the TruthfulQA dataset; includes baseline, high-uncertainty and low-uncertainty accuracy figures per model and discussion of divergence from human judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Question Answering (truthfulness classification)",
            "llm_judge_model": "Llama-3-8B-Instruct, Llama-3-70B-Instruct, Mixtral-8x7B-Instruct",
            "llm_judge_setup": "Confusion-based uncertainty: generate biased verbalized assessments for each discrete option, mix each assessment with every option to build n^2 confusion prompts, record token log-probabilities of final choice, construct confusion matrix and label low/high uncertainty by thresholding mean token probabilities.",
            "human_evaluation_setup": "Human annotations available for TruthfulQA used as ground truth; paper treats human labels as reference for computing LLM accuracy (specific annotator counts not reported).",
            "agreement_metric": "Accuracy (whether LLM-chosen option matches human label). Table 1 reports: Baseline accuracies — Llama-3-8B: 0.62, Llama-3-70B: 0.78, Mixtral-8x7B: 0.66; Low-uncertainty accuracies — 0.68, 0.91, 0.81 respectively. (High-uncertainty also reported.)",
            "losses_identified": "LLM-as-a-Judge evaluations do not always align with human judgments and can be incorrect or misleading; reliance on LLM-only judgments risks overconfidence in outputs without human cross-checks; coverage of 'trustworthy' (low-uncertainty) judgements is dataset- and model-dependent so many evaluations may lack a reliable signal compared to humans.",
            "examples_of_loss": "Paper notes general divergence from human judgments (Introduction). Concrete numbers: baseline vs low-uncertainty differences show variability across models (see Table 1); smaller models often produce far fewer low-uncertainty labels (see Results), reducing number of LLM judgments that can be trusted relative to human coverage.",
            "counterexamples_or_caveats": "Low-uncertainty labels correlate with higher accuracy; for Llama-3-70B low-uncertainty accuracy (0.91) substantially exceeds baseline and can outperform human-level agreement in some settings. Thus when the method marks a judgment as low-uncertainty, it often matches or exceeds human alignment.",
            "paper_reference": "Introduction; Results (Uncertainty labeling correlates with accuracy); Table 1 (TruthfulQA rows)",
            "uuid": "e9731.0",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Reliance Study comparison",
            "name_full": "Comparison of LLM-as-a-Judge versus human ratings on the Reliance Study dataset",
            "brief_description": "Evaluation of LLM judges on a dataset measuring reliance on LLM outputs (binary criteria like accuracy/naturalness), showing model-dependent proportions of low-uncertainty trustworthy labels and corresponding accuracy relative to human annotations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Conversational / reliance evaluation (binary classification of accuracy/naturalness)",
            "llm_judge_model": "Llama-3-8B-Instruct, Llama-3-70B-Instruct, Mixtral-8x7B-Instruct",
            "llm_judge_setup": "Confusion-based uncertainty evaluation (biased assessments per option, n^2 confusion prompts, token-probability confusion matrix, threshold-based low/high uncertainty labeling).",
            "human_evaluation_setup": "Original Reliance Study human annotations used as reference (paper does not detail annotator counts in main text).",
            "agreement_metric": "Accuracy vs human labels. Table 1 reports Baseline: 0.49, 0.63, 0.64 (Llama-3-8B, 70B, Mixtral); Low-uncertainty: 0.50, 0.85, 1.00 respectively.",
            "losses_identified": "Smaller models produce very few low-uncertainty (trustworthy) labels (&lt;5% for some datasets), meaning fewer LLM judgments can be used as substitutes for humans; variability across models implies inconsistent coverage and reliability compared to human raters.",
            "examples_of_loss": "Results state that in datasets such as the Reliance Study smaller models classify less than 5% of cases as low uncertainty while Llama-3-70B exceeds 15%, indicating loss of usable, high-confidence LLM evaluations for smaller models.",
            "counterexamples_or_caveats": "When LLM judgments are labeled low-uncertainty they often show very high accuracy (e.g., Mixtral low-uncertainty accuracy = 1.00), indicating that selective use of only low-uncertainty LLM judgments can approximate or exceed human agreement for those items.",
            "paper_reference": "Results (Variability in low uncertainty proportions; Low uncertainty consistently leads to higher accuracy); Table 1 (Reliance Study rows)",
            "uuid": "e9731.1",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Summarization CNN/DM comparison",
            "name_full": "Comparison of LLM-as-a-Judge versus human multi-rater evaluations on CNN/DM summarization",
            "brief_description": "Analysis of LLM judge performance on 1–5 rating tasks for summarization, comparing baseline and uncertainty-conditioned accuracies and juxtaposing these with human inter-rater agreement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Abstractive summarization rating (coherence, fluency, relevance) on CNN/DM",
            "llm_judge_model": "Llama-3-8B-Instruct, Llama-3-70B-Instruct, Mixtral-8x7B-Instruct",
            "llm_judge_setup": "Confusion-based uncertainty with per-option biased assessments, confusion prompts, token-probability matrix and thresholded uncertainty labeling; applied to multi-class (1–5) rating criteria.",
            "human_evaluation_setup": "CNN/DM ratings include multiple human raters; inter-rater agreement reported (0.60 in Table 1) but exact annotator counts not specified in paper.",
            "agreement_metric": "Accuracy (LLM-chosen rating matches human rating) and Human Agreement (inter-rater). Table 1: Baseline accuracies — 0.28, 0.25, 0.16; Low-uncertainty accuracies — 0.75, 0.26, 0.50; Human agreement = 0.60 (reported).",
            "losses_identified": "LLM-as-a-Judge can diverge substantially from human multi-rater judgments depending on model; some models (e.g., Llama-3-70B) show low-uncertainty accuracy below human agreement for this task, indicating potential loss of human-like nuance or consistency; smaller models often produce very few low-uncertainty labels, reducing usable coverage.",
            "examples_of_loss": "For Summarization CNN/DM, Llama-3-70B low-uncertainty accuracy = 0.26 which is well below reported human agreement (0.60), an explicit case where LLM-as-a-Judge does not match human inter-rater performance. The paper also notes smaller deviation between correct ratings and LLM-selected ratings only when low-uncertainty labels are present.",
            "counterexamples_or_caveats": "Llama-3-8B low-uncertainty accuracy = 0.75 exceeds human agreement (0.60), demonstrating that some models + uncertainty selection can match or outperform human agreement on selected items; however, the proportion of items labeled low-uncertainty may be small.",
            "paper_reference": "Results (Smaller deviation in multi-classification ratings; Low uncertainty labels approach human agreement); Table 1 (Summarization CNN/DM rows)",
            "uuid": "e9731.2",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Feedback Collection comparison",
            "name_full": "Comparison of LLM-as-a-Judge versus human ratings on the Feedback Collection dataset",
            "brief_description": "Evaluation of instruct models as judges on a fine-grained 1–5 feedback rating dataset; demonstrates that low-uncertainty LLM judgments can achieve perfect agreement on those items, while the proportion of such items is small.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Feedback rating collection (fine-grained evaluative ratings)",
            "llm_judge_model": "Llama-3-8B-Instruct, Llama-3-70B-Instruct, Mixtral-8x7B-Instruct",
            "llm_judge_setup": "Confusion-based uncertainty method (biased assessments per option, confusion prompts, confusion matrix of token probabilities, threshold-based uncertainty labels); applied to 1–5 rating scheme.",
            "human_evaluation_setup": "Feedback Collection dataset includes human ratings used as ground truth (paper does not list annotator counts); used to compute LLM accuracy.",
            "agreement_metric": "Accuracy vs human labels. Table 1 reports Baseline: 0.39, 0.40, 0.32 (Llama-3-8B, 70B, Mixtral); Low-uncertainty accuracies = 1.00, 1.00, 1.00 respectively (but proportion of low-uncertainty labels &lt;10% for all models).",
            "losses_identified": "Although low-uncertainty LLM judgments can be perfectly accurate, the coverage is very limited (low proportion of items labeled low-uncertainty), meaning large parts of evaluation remain untrusted compared to human evaluators; computational cost of the method is also a practical loss relative to human evaluation workflows.",
            "examples_of_loss": "Paper states that the proportion of low-uncertainty labels is below 10% for all models on Feedback Collection, yet these labels achieve 100% accuracy—illustrating loss of quantity/coverage even when quality is high for selected items.",
            "counterexamples_or_caveats": "On items labeled low-uncertainty, LLM-as-a-Judge matches human annotations perfectly in this dataset (1.00 accuracy), showing LLMs can substitute for humans selectively; threshold tuning affects trade-off between proportion and accuracy.",
            "paper_reference": "Results (Low uncertainty consistently leads to higher accuracy; Variability in low uncertainty proportions); Table 1 (Feedback Collection rows)",
            "uuid": "e9731.3",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "FeedbackQA comparison",
            "name_full": "Comparison of LLM-as-a-Judge versus human ratings on FeedbackQA (retrieval-based QA with feedback)",
            "brief_description": "Comparison showing that low-uncertainty LLM judgments can reach or exceed human inter-rater agreement for selected items, but smaller models produce few low-uncertainty labels and baseline alignment varies by model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Retrieval-based question answering with interactive user feedback (rating scale: excellent to bad)",
            "llm_judge_model": "Llama-3-8B-Instruct, Llama-3-70B-Instruct, Mixtral-8x7B-Instruct",
            "llm_judge_setup": "Confusion-based uncertainty approach (biased assessments, confusion prompts, token-probability confusion matrix, thresholding to produce low/high uncertainty labels), applied to multi-class ratings.",
            "human_evaluation_setup": "FeedbackQA contains human-generated ratings used as reference; inter-rater agreement reported as 0.48 in Table 1 (annotator counts not specified).",
            "agreement_metric": "Accuracy versus human ratings. Table 1 reports Baseline: 0.47, 0.69, 0.29; Low-uncertainty accuracies = 1.00, 0.71, 1.00 respectively; Human Agreement = 0.48.",
            "losses_identified": "LLM-as-a-Judge shows model-dependent variability; smaller models often produce very low proportions of low-uncertainty labels reducing effective coverage of trustworthy judgments compared to humans; where LLMs fail to produce low-uncertainty signals, relying on them loses the broad, consistent coverage humans provide.",
            "examples_of_loss": "Paper reports smaller models classify less than 5% of cases as low uncertainty in datasets including FeedbackQA, meaning most items cannot be safely judged by those LLMs without human oversight. Also baseline accuracies vary substantially across models (e.g., Mixtral baseline 0.29 vs Llama-3-70B 0.69).",
            "counterexamples_or_caveats": "Low-uncertainty LLM judgments can achieve perfect agreement (1.00) for some models/datasets (e.g., Llama-3-8B and Mixtral here) and often meet or exceed human inter-rater agreement for selected items; threshold tuning and larger models increase the proportion of reliable LLM judgments.",
            "paper_reference": "Results (Low uncertainty labels approach human agreement; Variability in low uncertainty proportions); Table 1 (FeedbackQA rows)",
            "uuid": "e9731.4",
            "source_info": {
                "paper_title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks",
            "rating": 2,
            "sanitized_title": "llms_instead_of_human_judges_a_large_scale_empirical_study_across_20_nlp_evaluation_tasks"
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2,
            "sanitized_title": "large_language_models_are_not_fair_evaluators"
        },
        {
            "paper_title": "Finding blind spots in evaluator llms with interpretable checklists",
            "rating": 2,
            "sanitized_title": "finding_blind_spots_in_evaluator_llms_with_interpretable_checklists"
        },
        {
            "paper_title": "Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms",
            "rating": 2,
            "sanitized_title": "can_llms_express_their_uncertainty_an_empirical_evaluation_of_confidence_elicitation_in_llms"
        },
        {
            "paper_title": "Replacing judges with juries: Evaluating llm generations with a panel of diverse models",
            "rating": 2,
            "sanitized_title": "replacing_judges_with_juries_evaluating_llm_generations_with_a_panel_of_diverse_models"
        },
        {
            "paper_title": "Prometheus: Inducing evaluation capability in language models",
            "rating": 1,
            "sanitized_title": "prometheus_inducing_evaluation_capability_in_language_models"
        }
    ],
    "cost": 0.0132935,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Black-box Uncertainty Quantification Method for LLM-as-a-Judge
15 Oct 2024</p>
<p>Nico Wagner nico.wagner@ 
IBM Research</p>
<p>Michael Desmond mdesmond@us. 
IBM Research</p>
<p>Rahul Nair rahul.nair@ie. 
IBM Research</p>
<p>Zahra Ashktorab zahra.ashktorab1@ 
IBM Research</p>
<p>Elizabeth M Daly elizabeth.daly@ie. 
IBM Research</p>
<p>Qian Pan qian.pan@ 
IBM Research</p>
<p>Martín Santillán Cooper msantillancooper@ 
IBM Research</p>
<p>James M Johnson jmjohnson@us. 
IBM Research</p>
<p>Werner Geyer werner.geyer@us.ibm.com 
IBM Research</p>
<p>Black-box Uncertainty Quantification Method for LLM-as-a-Judge
15 Oct 2024F4BECD1BCD398B601BCAA2788DA7FA6CarXiv:2410.11594v1[cs.LG]
LLM-as-a-Judge is a widely used method for evaluating the performance of Large Language Models (LLMs) across various tasks.We address the challenge of quantifying the uncertainty of LLM-as-a-Judge evaluations.While uncertainty quantification has been well-studied in other domains, applying it effectively to LLMs poses unique challenges due to their complex decision-making capabilities and computational demands.In this paper, we introduce a novel method for quantifying uncertainty designed to enhance the trustworthiness of LLM-as-a-Judge evaluations.The method quantifies uncertainty by analyzing the relationships between generated assessments and possible ratings.By cross-evaluating these relationships and constructing a confusion matrix based on token probabilities, the method derives labels of high or low uncertainty.We evaluate our method across multiple benchmarks, demonstrating a strong correlation between the accuracy of LLM evaluations and the derived uncertainty scores.Our findings suggest that this method can significantly improve the reliability and consistency of LLM-as-a-Judge evaluations.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have become integral to a wide range of tasks, including questionanswering [24], summarization [12], translation [30], concept extraction [5], classification [8], and reasoning [10].The evaluation of the texts they generate has emerged as a significant challenge due to data contamination [1], replicability, and the use of standard metrics or benchmarks [17,7,6] which may not cover all dimensions of use case-specific evaluations.</p>
<p>An emerging method for evaluating generated content involves using other LLMs as evaluators, a method referred to as LLM-as-a-Judge [31].These evaluations can take various forms, including explanations, numeric values, or categorical ratings.This work specifically focuses on LLM-as-a-Judge methods that employ categorical ratings or numerical evaluations to assess generated outputs.</p>
<p>Despite their widespread use, LLM-as-a-Judge methods do not always align with human judgments, leading to instances where the evaluations may be incorrect or misleading [27,2].This divergence highlights the need to assess the trustworthiness of LLM-generated evaluations.Various techniques have been proposed to enhance the performance of LLMs and improve the reliability of their judgments [26].</p>
<p>To improve trustworthiness in LLM-as-a-Judge evaluations and to leverage the strengths of these methods, we introduce a novel approach called confusion-based uncertainty.Our method is designed 38th Conference on Neural Information Processing Systems (NeurIPS 2024).</p>
<p>Figure 1: A biased assessment prompt.The LLM is prompted to asses a response (an input text that is under evaluation) under the assumption that a particular output option (label) is correct.By producing biased assessments, it is possible to determine the LLM's belief in a correct output option subject to assessments that may be contrary to this belief.</p>
<p>to quantify the uncertainty associated with LLM evaluations where evaluation outcomes are discrete, i.e. multiple choice settings, or fixed number of output options.This encompasses a majority of typical evaluation tasks included those involving human evaluations.</p>
<p>The confusion-based uncertainty approach, inspired by chain-of-thought reasoning [28] and confusion matrices [3], first prompts the judge LLM to generate an assessment for each potential output option, biased on the implication that the option is correct.An assessment is an open ended evaluation produced by the LLM prior to making a final judgement.A biased assessment is generated under the implication that a given option is correct (see Figure 1).For each biased assessment, the probability of all output options is recorded using log probabilities.This facilitates an analysis of the relationship between the biased assessments and the LLMs belief in a particular output being correct.A confusion matrix is constructed from these combinations, and levels of uncertainty are derived from the confusion matrix by looking at the distribution of token probabilities for output options, subject to each of the biased assessments.If an option is consistently likely across all potentially biased assessments, it is deemed to have low uncertainty.</p>
<p>The goal of confusion-based uncertainty is to label LLM-as-a-Judge evaluations with high or low uncertainty, offering a clear signal of the evaluations' likely accuracy.We empirically evaluate our confusion-based uncertainty method across diverse benchmarks and models.Our results indicate that low uncertainty ratings correlate with higher accuracy, and the method effectively transfers across datasets and models.</p>
<p>Related Work</p>
<p>The capabilities of large language models have rapidly advanced, leading to their application in increasingly complex tasks [25].However, the growing sophistication of these models also raises new challenges, particularly in evaluating their outputs and understanding their uncertainty [9].As models are increasingly used as evaluators, what is often referred to as LLM-as-a-Judge, it becomes essential to develop methods that allow these models not only to generate responses but also to assess the confidence and reliability of those responses.</p>
<p>A promising line of research addresses these challenges through methods like chain-of-thought reasoning [28] and self-reflection [11].Chain-of-thought reasoning enhances an LLM's ability to arrive at more accurate conclusions by breaking down complex tasks into intermediate logical steps.By guiding the model through a series of smaller, connected reasoning steps, the model's decisionmaking process becomes more transparent and robust.Self-reflection, on the other hand, encourages the model to review and critique its own responses, iterating upon initial outputs to refine and improve its accuracy.Both techniques align with the broader framework of agentic design patterns [21], which foster active engagement by the model in evaluating and refining its generated outputs.These reasoning-based approaches are particularly relevant to the growing body of work on LLM-asa-Judge, where LLMs are used to evaluate data and provide judgments that are comparable to human annotations.Several works have studied LLM-as-a-Judge with evaluations generally focused on correlations between labels generated by LLMs and human annotations.For example, [31,13] and report strong agreements with human annotations, while other studies have reported mixed results [4,2].Some works have proposed using ensembles of smaller models to increase performance [26], while others have used instruction fine-tuning to build customised evaluators [13].</p>
<p>Several works have explored methods for uncertainty estimation in the context of large language models.One approach is calibration-based uncertainty quantification, introduced by [23], which focuses on efficient calibration using a auxiliary model trained over multiple tasks.However, this approach relies on internal model representations to produce features.In contrast, black-box uncertainty quantification methods, which do not require access to the internal workings of the model, have also emerged.Lin etal.[19] investigated prompting strategies that guide LLMs to verbalize their uncertainty, especially when fine-tuned with labeled confidence values.Their work demonstrates how external methods can elicit uncertainty without accessing internal model states.</p>
<p>Xiong et al. [29] evaluated several prompting strategies for eliciting uncertainty in LLMs, including direct assessment, chain-of-thought reasoning, and self-probing.Their findings indicate that models tend to exhibit overconfidence, particularly in general settings.To address this, they proposed strategies such as prompt perturbation, paraphrasing, and entity amplification, which reduced overconfidence and improved uncertainty predictions.They also developed a logistic regression model to predict uncertainty based on these perturbations [22].</p>
<p>Kuhn et al. [15] introduced semantic entropy as a novel approach to capture uncertainty by identifying semantically equivalent prompts.By measuring the variation in responses to these semantically similar prompts, their method provides a more nuanced understanding of model uncertainty.</p>
<p>Confusion-based Uncertainty</p>
<p>In many LLM-as-a-Judge frameworks, the evaluation of generated text is conducted against predefined criteria [14].Each criterion consists of a question and a set of options, among which the LLM must choose.The questions can vary widely, and the options can be defined as numeric values, words, or any other format, with no restriction on the number of words or the nature of the options.</p>
<p>Our proposed technique introduces an uncertainty measure that is calculated independently of the specific decision made by the LLM.This approach aims to enhance the trustworthiness of LLM-as-a-Judge evaluations.The method works in four key steps: generating verbalized assessments, creating prompts for the confusion matrix, constructing the confusion matrix, and setting uncertainty labels.See Figure 2. The LLM is first presented with an evaluation task, and prompted to produce an assessment for each output option, biased on the explicit indication that the option is correct.In the context of the original evaluation task, the LLM is conditioned on each of the biased assessments, and the probability of each option calculated using log probabilities.This information is then encoded in a confusion matrix.Each row of the matrix, representing the probability of a particular option conditioned on each of the biased assessments, is then averaged to produce an uncertainty label.In this figure, α represents the threshold.</p>
<p>Generating Assessments The initial step in our approach involves generating verbalized assessments for each of the n options presented in the criterion.Using the prompt template shown in Figure 3, we apply prompt engineering techniques to guide the LLM toward treating a specific option as correct.This compels the model to generate justifications for why that particular option is the best choice.Each assessment is explicitly linked to one of the available options, ensuring that the reasoning is directly associated with the selected alternative.</p>
<p>The prompts are designed to persuade the LLM that the option in question is correct, leading to a set of n assessments, one for each option.Creating Confusion Prompts After generating assessments, the next step involves creating prompts that will be used to build the confusion matrix.This is done by mixing each assessment with every option, effectively producing a comprehensive set of prompts that cover all possible pairings of assessments and options.The prompt template (see Figure 5) is structured as a conversation between the LLM and the user, where two tasks are presented as separate requests.First, the LLM is asked to generate an assessment for which option is correct without injecting any prompts.Second, the LLM is prompted to choose the correct option based on the assessment.The assessments and options generated in the previous step are inserted as responses from the LLM.</p>
<p>For a criterion with n options, this process results in the creation of n 2 prompts, as each assessment is mixed with every possible option.</p>
<p>Constructing the Confusion Matrix With the n 2 prompts created, the next step is to send these prompts to the LLM to obtain the token probabilities associated with the final decision.The probability of the last token in the response is used to calculate an uncertainty score for the chosen option.These probabilities are organized into a confusion matrix, where each row corresponds to an option from the prompt, and each column corresponds to an assessment generated for a specific option (see Figure 4.A confusion matrix labeled as low uncertainty exhibits high token probabilities concentrated in a single row.In contrast, a matrix labeled as high uncertainty either shows high token probabilities along the diagonal, where the assessments align with the corresponding options, or has high token probabilities scattered arbitrarily across the matrix.Setting Uncertainty Labels The final step in the method involves assigning an uncertainty label, either high or low uncertainty, to the chosen option based on the confusion matrix and predefined threshold.The labeling process follows these rules:</p>
<p>Options Assessments
A 1 A 2 . . . A m         O 1 p 1,1 p 1,2 • • • p 1,m O 2 p 2,1 p 2,2 • • • p 2,m . . . . . . . . . . . . . . . O n p n,1 p n,2 • • • p n,m
• If only one row in the matrix exceeds the uncertainty threshold and this row corresponds to the LLM's initially chosen option, the option is labeled as low uncertainty.</p>
<p>• If more than one row exceeds the uncertainty threshold, the option is labeled as high uncertainty.</p>
<p>• If the option identified with low uncertainty in the confusion matrix does not match the LLM's originally chosen option, the option is labeled as high uncertainty.</p>
<p>• If no row exceeds the uncertainty threshold, the option is labeled as high uncertainty.</p>
<p>This labeling process allows the method to differentiate between evaluations that the LLM is likely confident in and those that may require further scrutiny.The overall goal is to enhance the reliability and trustworthiness of LLM-as-a-Judge evaluations by providing an additional layer of certainty assessment.</p>
<p>Formal Description</p>
<p>Formally, the method can be described as follows.Consider a question q with n possible outcomes o i , where i ∈ {1, 2, . . ., n}.For example, a multiple choice question with four answers, o i can take on values A/B/C/D with n = 4.With each q as context, we consider two prompts, q a an assessment prompt and q c a confusion prompt.The assessment prompt (see Figure 3) generates assessments a i = q a (o i ) ∀i ∈ {1, 2, . . ., n} for each possible discrete outcome.</p>
<p>The confusion prompt (see Figure [5]), considers all combinations of outcomes and assessments for the question, i.e. q c (o i , a j ) denotes a prompt using assessment a j and target label o i .While the assessment prompt generates additional tokens, the confusion prompt is used only to determine the probabilities of the output token(s).The confusion matrix C consists of elements
p ij = p (o i | q c (o i , a j )) , ∀i, j ∈ {1, 2, . . . , n},(1)
where p ij denotes the probability of token o i when the assessment relates to the j-th outcome.This matrix forms the basis for the uncertainty quantification.The main intuition is that if the probability of token o i is high regardless of the assessments, then the model has low uncertainty in its prediction.In contrast, if the token probability follows the assessment, we infer that the model has high uncertainty in its answer.</p>
<p>The uncertainty associated with a specific token can then be estimated by taking the mean token probability across all confusing prompts, i.e.
u i = 1 n j p ij , ∀i ∈ {1, 2, . . . , n}.(2)
For analysis, in this paper we further label the uncertainty of the overall assessment using a threshold α, i.e.
l = low uncertainty if i 1(u i ≥ α) = 1, ∀i ∈ {1, 2, . . . , n}, high uncertainty otherwise.(3)
In other words, the assessment has low uncertainty if the mean token probability exceeds the threshold for exactly a single token.The procedure involves n inferencing calls for the first stage and n 2 inferences for the second stage as it works through all combinations of outcome labels making the overall estimation procedure O(n 2 ).</p>
<p>Threshold</p>
<p>The threshold acts as a crucial parameter in determining the balance between the proportion of low uncertainty and accuracy.Defining an optimal threshold depends on the specific requirements of the use case.For applications such as content filtering or large-scale feedback collection, where there are a large number of evaluations but limited human resources to assess the output, prioritizing a higher volume of low-uncertainty responses may necessitate a more lenient threshold, even if it results in only a modest accuracy gain.Conversely, for tasks demanding highly reliable outcomes, such as the evaluation of medical diagnoses or legal decision-making, where accuracy is critical and errors carry significant consequences, a stricter threshold is essential to ensure the chosen options are highly reliable.</p>
<p>An interesting observation arises when the threshold is reduced below 0.5, accuracy tends to increase, suggesting that as the average token probability for incorrect options decreases, the model performance improves.This suggests that valuable information can be derived not only from options marked as having low uncertainty but also from those with higher uncertainty.The behavior of token probabilities across both low and high uncertainty options provides insights into the decision-making process of the LLM, suggesting that thresholds should be dynamically tuned based on the specific performance trade-offs desired for the task at hand.</p>
<p>Our analysis reveals that threshold tuning significantly impacts the relationship between accuracy and uncertainty, forming a parabolic effect.In the threshold grid search for the Feedback Collection dataset (see Figure 6), we observe that as the threshold increases beyond 0.5, accuracy improves, but the proportion of low-uncertainty predictions decreases.Conversely, when the threshold is below 0.5, lowering the threshold leads to an increase in accuracy but a decrease in the proportion of low-uncertainty labels.This inverse relationship between accuracy and uncertainty highlights that while stricter thresholds (above 0.5) favor higher accuracy at the expense of fewer low-uncertainty predictions, lenient thresholds (below 0.5) enhance accuracy but reduce the certainty of the predictions.This parabolic behavior is consistent across datasets, emphasizing the threshold's pivotal role in determining model performance.In (a), the focus is on the effect of options labeled as low uncertainty, while in (b), the focus shifts to the effect of options labeled as high uncertainty.The results highlight how the threshold choice influences both accuracy and the proportion of selected options.</p>
<p>Experiments</p>
<p>Benchmark Datasets The proposed uncertainty method was evaluated on five benchmark datasets: TruthfulQA [18], Reliance Study, Summarization CNN/DM [20], Feedback Collection [14], and FeedbackQA [16].TruthfulQA contains question-answer pairs and involves a binary classification task to determine whether the answer is truthful, with human annotations available for verification.The Reliance Study dataset originates from a study designed to measure reliance on Large Language Models (LLMs) across various tasks.It uses binary classification to evaluate the accuracy and naturalness of LLM outputs in settings such as conversations and customer-agent interactions, focusing on criteria like relevance and naturalness.The Summarization CNN/DM dataset, which consists of model-generated summaries of news articles, is evaluated on a scale from 1 to 5 based on criteria such as coherence, fluency, and relevance.Feedback Collection is designed to induce fine-grained evaluation capabilities in language models, using a rating scale from 1 to 5. Lastly, FeedbackQA is a retrieval-based QA dataset that includes interactive user feedback, where each question-answer pair is rated from excellent to bad, accompanied by natural language explanations detailing the strengths and weaknesses of the responses.</p>
<p>Models In this study, we utilize instruct models exclusively, as LLM-as-a-Judge requires agentlike capabilities, where models must reliably follow explicit evaluation instructions.The instruct models chosen for this experiment include Mixtral-8x7B-Instruct-v01, Llama-3-8B-Instruct, and Llama-3-70B-Instruct. To investigate the impact of model architecture and size on performance, we selected models of varying sizes 8B and 70B parameters.This approach allows us to assess whether performance improvements in LLM-as-a-Judge tasks are primarily driven by the scale of the model or the underlying instruct-tuned structure.</p>
<p>Implementation Details</p>
<p>The experiments were conducted on stratified samples from each dataset, with the sample size determined by the number of evaluation criteria.For instance, if a dataset included four distinct criteria, the total sample size was multiplied by four to ensure that each criterion was equally represented.This stratification ensures a balanced evaluation across all criteria.To determine the optimal threshold for distinguishing high and low uncertainty in the LLM-as-a-Judge predictions, we employed a grid search, systematically exploring different threshold values to identify the best-performing configuration.</p>
<p>Baseline The evaluation metric for this work is based on accuracy, specifically measuring whether the option selected by the LLM matches the option chosen by the human rater.The baseline for the method is to achieve higher accuracy for cases labeled as low uncertainty and lower accuracy for those labeled as high uncertainty.In datasets such as FeedbackQA and Summarization CNN/DM, which include ratings from multiple human raters, inter-rater accuracy can also be computed.The aim is to maximize alignment between the LLM's predictions and the human ratings, with the ultimate goal of approaching the inter-rater accuracy in these datasets.</p>
<p>Results</p>
<p>Uncertainty labeling correlates with accuracy Our uncertainty labeling method demonstrates a clear advantage in aligning low uncertainty labels with higher accuracy, as shown in Figure 7. Across all datasets and models, the markers for low uncertainty are consistently above the baseline, indicating that these labels correspond to more accurate evaluations compared to high uncertainty labels.This result confirms that the uncertainty labels generated by our method are effective in predicting the likelihood of accurate outputs in LLM-as-a-Judge scenarios.Variability in low uncertainty proportions A key finding is the variance in the ratio of high to low uncertainty labels depending on the dataset and model, as depicted in Figure 8. Notably, the Llama-3-70B-Instruct model consistently produces a higher proportion of low uncertainty labels, outperforming both Llama-3-8B-Instruct and Mixtral-8x7B-Instruct-v01. In datasets such as the Reliance Study, Summarization CNN/DM, and FeedbackQA, the smaller models classify less than 5% of cases as low uncertainty, whereas Llama-3-70B-Instruct exceeds 15%.This suggests that both model size and structure significantly impact the model's ability to assign more reliable uncertainty labels.</p>
<p>Low uncertainty consistently leads to higher accuracy Even in cases where the proportion of low uncertainty labels is small, they still correspond to high accuracy.For example, in the Feedback Collection dataset, the proportion of low uncertainty labels is below 10% for all models (see Figure 8), yet these labels consistently achieve 100% accuracy (see Table 1).This trend further supports the strong predictive power of low uncertainty labels in LLM evaluations.</p>
<p>Smaller deviation in multi-classification ratings In multi-classification datasets, such as Summarization CNN/DM, which require the evaluation of generated summaries, a smaller deviation is observed between the correct ratings and the LLM's selected ratings when low uncertainty labels are present.This indicates that LLMs not only tend to choose the correct options but also exhibit greater precision in their ratings under conditions characterized by a low proportion of uncertainty.</p>
<p>Low uncertainty labels approach human agreement In datasets with human-generated ratings such as Summarization CNN/DM and FeedbackQA, the accuracy of LLM predictions labeled as low uncertainty meets or even exceeds the level of agreement observed between human raters (see Table 1).This suggests that low uncertainty labels can be as reliable as human evaluations, further establishing the effectiveness of our method in aligning LLM outputs with human judgment.The "High Uncertainty" and "Low Uncertainty" values represent the accuracy when considering only options labeled under each respective category.Baseline" reflects the LLM's accuracy without factoring in uncertainty labels, while "Human Agreement" indicates the consistency in accuracy between multiple human raters.The listed models refer to their instruct versions.The threshold was optimized for each specific dataset and model.
Dataset Model Llama-3-8B Llama-3-70B Mixtral-8x7B</p>
<p>Interpreting Uncertainty</p>
<p>The interpretation of uncertainty in LLMs remains a significant challenge [9].Our method attempts to confuse the LLM by convincing it of statements without knowing whether they are true, and then considering the LLM's beliefs under these biased conditions.We define two key scenarios for interpreting low uncertainty.The first scenario is when the LLM selects an option consistently, regardless of conflicting assessments, indicating that even when alternative assessments are presented, they fail to sway the model's decision 9a.The second scenario occurs when the LLM cannot be convinced to generate an assessment for a different option, even if such an assessment is prompted.These two types of low uncertainty can be observed in the structure of the confusion matrices.This behavior is also reflected in the sparsity of the confusion matrices.In cases of high uncertainty, only the token probabilities of the matching assessments and options are high.In contrast, for low uncertainty, only one row exhibits high token probabilities, demonstrating strong model confidence in its chosen option, as shown in Figure 9a.In contrast, high uncertainty can also manifest when the token probabilities are arbitrarily distributed across the matrix, as shown in Figure 10.</p>
<p>Another intuitive observation is that as the number of options increases, the sparsity of the matrix decreases, as seen in Figure 11.This implies that the token probabilities for non-matching assessments and options increase, but this effect is also applicable to token probabilities in cases of low uncertainty.</p>
<p>Discussion</p>
<p>In this work, we introduced a method for quantifying uncertainty in LLM-as-a-Judge evaluations.</p>
<p>Through empirical analysis, we found that the uncertainty labels correlate with accuracy, indicating the effectiveness of the method.Although our primary focus was on evaluating this method within the context of LLM-based evaluations, the potential for broader applications is significant.Despite these promising results, the current approach presents certain limitations.The method is computationally intensive, especially when using large models such as Llama-3-70B-Instruct, which may not be feasible for all applications.Additionally, the performance of the method may vary when applied to models or tasks that have not been fine-tuned for evaluation purposes.Generalizability across diverse tasks and domains also requires further investigation.</p>
<p>To address these challenges, one potential solution is to reduce inference time by consolidating all assessments into a single prompt, querying the model for its chosen option only once.This strategy could lower computational overhead without compromising accuracy.Overall, while the current method yields strong results, further optimization and refinement have the potential to enhance efficiency and effectiveness.</p>
<p>To further improve the results and increase the proportion of low uncertainty labels, prompt engineering emerges as a key factor.We recommend adapting the prompt structure specifically to the task and the model in use.Fine-tuning and tailoring prompts could significantly enhance performance, with the potential to surpass the results obtained in this study.Future research could explore the method's effectiveness with various prompt designs to further optimize its performance.</p>
<p>Figure 2 :
2
Figure 2: Method Overview.The method is divided into four stages, resulting in an uncertainty label.The LLM is first presented with an evaluation task, and prompted to produce an assessment for each output option, biased on the explicit indication that the option is correct.In the context of the original evaluation task, the LLM is conditioned on each of the biased assessments, and the probability of each option calculated using log probabilities.This information is then encoded in a confusion matrix.Each row of the matrix, representing the probability of a particular option conditioned on each of the biased assessments, is then averaged to produce an uncertainty label.In this figure, α represents the threshold.</p>
<p>Figure 3 :
3
Figure 3: Persuasion prompt generating an assessment for each option.</p>
<p>Figure 4 :
4
Figure 4: Structure of the confusion matrix.Each row represents an option and each column corresponds to an assessment, with the matrix values being the token probabilities for each optionassessment combination.</p>
<p>Figure 5 :
5
Figure 5: Confusion prompt forcing a final answer for each option and assessment from the previous step leading to n 2 prompts being used to obtain token log probabilities for each option and assessment combination.</p>
<p>Figure 6 :
6
Figure 6: Relationship between threshold, accuracy, and proportion through grid search optimization.Grid search optimization of the threshold for the Llama-3-70B-Instruct model on the Feedback Collection dataset.The figure illustrates how varying the threshold impacts performance.In (a), the focus is on the effect of options labeled as low uncertainty, while in (b), the focus shifts to the effect of options labeled as high uncertainty.The results highlight how the threshold choice influences both accuracy and the proportion of selected options.</p>
<p>Figure 7 :
7
Figure 7: Accuracy comparison of options labeled low uncertainty versus high uncertainty.Each marker represents the performance of a specific model on a particular dataset.Markers above the dashed line indicate that the model has surpassed the baseline for that dataset.</p>
<p>Figure 8 :
8
Figure 8: Ratio of Options Labeled Low Uncertainty.The y-axis represents the percentage of options labeled as low uncertainty, while the x-axis denotes the datasets evaluated across three different models, as indicated in the legend.The ratio of options labeled as high uncertainty is calculated as 1 minus the ratio of options labeled low uncertainty.</p>
<p>Figure 9 :
9
Figure 9: Examples of confusion matrices for high and low uncertainty.</p>
<p>Figure 10 :
10
Figure 10: Example of a confusion matrix with arbitrarily distributed token probabilities.</p>
<p>Figure 11 :
11
Figure 11: Examples of sparsity across different numbers of options.This figure presents confusion matrices, all labeled as high uncertainty.It illustrates that as the number of options increases, the sparsity within the matrices decreases, highlighting the relationship between option quantity and token probability distribution.</p>
<p>Table 1 :
1
Accuracy across various datasets and models for different uncertainty categories.
TruthfulQAHigh Uncertainty0.590.500.60Baseline0.620.780.66Low Uncertainty0.680.910.81Reliance StudyHigh Uncertainty0.490.510.63Baseline0.490.630.64Low Uncertainty0.500.851.00Feedback CollectionHigh Uncertainty0.360.370.30Baseline0.390.400.32Low Uncertainty1.001.001.00Summarization CNN/DMHigh Uncertainty0.270.240.15Baseline0.280.250.16Low Uncertainty0.750.260.50Human Agreement0.600.600.60FeedbackQAHigh Uncertainty0.460.670.28Baseline0.470.690.29Low Uncertainty1.000.711.00Human Agreement0.480.480.48</p>
<p>Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source LLMs. Simone Balloccu, Patrícia Schmidtová, Mateusz Lango, Ondrej Dusek, Proceedings of the 18th Conference of the European Chapter. Long Papers. Yvette Graham, Matthew Purver, the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational LinguisticsMarch 20241</p>
<p>Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks. Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, arXiv:2406.184032024arXiv preprint</p>
<p>Confusion matrix -Wikipedia, the free encyclopedia. 2024. -October-202415Wikipedia contributors</p>
<p>Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, Sshubam Verma, Mitesh M Khapra, arXiv:2406.13439Finding blind spots in evaluator llms with interpretable checklists. 2024arXiv preprint</p>
<p>Data-efficient concept extraction from pre-trained language models for commonsense explanation generation. Yanbo Fang, Yongfeng Zhang, 2022EMNLP 2022</p>
<p>Bigbench: Towards an industry standard benchmark for big data analytics. Ahmad Ghazal, Tilmann Rabl, Minqing Hu, Francois Raab, Meikel Poess, Alain Crolotte, Hans-Arno Jacobsen, Proceedings of the 2013 ACM SIGMOD international conference on Management of data. the 2013 ACM SIGMOD international conference on Management of data2013</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Universal language model fine-tuning for text classification. Jeremy Howard, Sebastian Ruder, arXiv:1801.061462018arXiv preprint</p>
<p>Mengting Hu, Zhen Zhang, Shiwan Zhao, Minlie Huang, Bingzhe Wu, arXiv:2306.04459Uncertainty in natural language processing: Sources, quantification, and applications. 2023arXiv preprint</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>Towards mitigating llm hallucination via self reflection. Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>A comprehensive survey on process-oriented automatic text summarization with exploration of llm-based methods. Hanlei Jin, Yang Zhang, Dan Meng, Jun Wang, Jinghua Tan, arXiv:2403.029012024arXiv preprint</p>
<p>Prometheus: Inducing evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Prometheus 2: An open source language model specialized in evaluating other language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, arXiv:2405.015352024arXiv preprint</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, arXiv:2302.096642023arXiv preprint</p>
<p>Using interactive feedback to improve the accuracy and explainability of question answering systems postdeployment. Zichao Li, Prakhar Sharma, Xing Han Lu, Jackie Ck Cheung, Siva Reddy, arXiv:2204.030252022arXiv preprint</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, arXiv:2211.091102022arXiv preprint</p>
<p>Stephanie Lin, Jacob Hilton, Owain Evans, Truthfulqa, arXiv:2109.07958Measuring how models mimic human falsehoods. 2021arXiv preprint</p>
<p>Teaching models to express their uncertainty in words. Stephanie Lin, Jacob Hilton, Owain Evans, arXiv:2205.143342022arXiv preprint</p>
<p>Abstractive text summarization using sequence-to-sequence rnns and beyond. Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, arXiv:1602.060232016arXiv preprint</p>
<p>Agentic design patterns part 1. Andrew Ng, 2024</p>
<p>Tejaswini Pedapati, Amit Dhurandhar, Soumya Ghosh, arXiv:2406.04370Soham Dan, and Prasanna Sattigeri. Large language model confidence estimation via black-box access. 2024arXiv preprint</p>
<p>Thermometer: Towards universal calibration for large language models. Maohao Shen, Subhro Das, Kristjan Greenewald, Prasanna Sattigeri, Gregory Wornell, Soumya Ghosh, arXiv:2403.088192024arXiv preprint</p>
<p>Towards expert-level medical question answering with large language models. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, arXiv:2305.096172023arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Replacing judges with juries: Evaluating llm generations with a panel of diverse models. Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, Patrick Lewis, arXiv:2404.187962024arXiv preprint</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.179262023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, arXiv:2306.130632023arXiv preprint</p>
<p>Prompting large language model for machine translation: A case study. Biao Zhang, Barry Haddow, Alexandra Birch, International Conference on Machine Learning. PMLR2023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024</p>            </div>
        </div>

    </div>
</body>
</html>
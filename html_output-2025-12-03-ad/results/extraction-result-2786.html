<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2786 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2786</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2786</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-71.html">extraction-schema-71</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-260438773</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.01542v1.pdf" target="_blank">Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents</a></p>
                <p><strong>Paper Abstract:</strong> The recent advent of large language models (LLM) has resulted in high-performing conversational agents such as chatGPT. These agents must remember key information from an ongoing conversation to provide responses that are contextually relevant to the user. However, these agents have limited memory and can be distracted by irrelevant parts of the conversation. While many strategies exist to manage conversational memory, users currently lack affordances for viewing and controlling what the agent remembers, resulting in a poor mental model and conversational breakdowns. In this paper, we present Memory Sandbox, an interactive system and design probe that allows users to manage the conversational memory of LLM-powered agents. By treating memories as data objects that can be viewed, manipulated, recorded, summarized, and shared across conversations, Memory Sandbox provides interaction affordances for users to manage how the agent should `see' the conversation.</p>
                <p><strong>Cost:</strong> 0.002</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2786",
    "paper_id": "paper-260438773",
    "extraction_schema_id": "extraction-schema-71",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0020215,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents
3 Aug 2023</p>
<p>Ziheng Huang z8huang@ucsd.edu 
University of California-San Diego San Diego
CAUSA</p>
<p>Sebastian Gutierrez 
Temple University Philadelphia
PAUSA</p>
<p>Hemanth Kamana 
Temple University Philadelphia
PAUSA</p>
<p>Stephen Macneil stephen.macneil@temple.edu 
Temple University Philadelphia
PAUSA</p>
<p>Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents
3 Aug 20237EDFEE9FF6F2F8EF40AC59A75B05700FarXiv:2308.01542v1[cs.HC]Human-AI InteractionLarge Language ModelsChatbots
The recent advent of large language models (LLM) has resulted in high-performing conversational agents such as chatGPT.These agents must remember key information from an ongoing conversation to provide responses that are contextually relevant to the user.However, these agents have limited memory and can be distracted by irrelevant parts of the conversation.While many strategies exist to manage conversational memory, users currently lack affordances for viewing and controlling what the agent remembers, resulting in a poor mental model and conversational breakdowns.In this paper, we present Memory Sandbox, an interactive system and design probe that allows users to manage the conversational memory of LLM-powered agents.By treating memories as data objects that can be viewed, manipulated, recorded, summarized, and shared across conversations, Memory Sandbox provides interaction affordances for users to manage how the agent should 'see' the conversation.CCS CONCEPTS• Computing methodologies → Intelligent agents; • Humancentered computing → Interactive systems and tools;</p>
<p>INTRODUCTION</p>
<p>Large Language Models (LLMs) are currently capable of generating human-like responses in open-domain tasks [4].This has led to a new generation of conversational agents, such as chatGPT, which are now being widely used across domains.To ensure that agents generate responses that are contextually relevant and coherent to an ongoing conversation, these agents must maintain a working memory of the conversational history that has occurred up to that point in the conversation.The default strategy is to use as much of the conversational history as will fit within the input size limit of the LLM.Parts of the conversations that go beyond that buffer limit are forgotten, which leads to breakdowns when users assume the model remembers past context.Additionally, as the input buffer size increases, the performance of the LLM degrades as it struggles to retrieve relevant context and can be distracted by irrelevant context [11,18].This problem is compounded because users do not know how the LLM is leveraging the memory to generate responses.</p>
<p>Multiple strategies have been introduced to manage agents' conversational memory.For example, the conversation can be automatically summarized [21] and refined [24] to reduce redundancy while maintaining key information.Some systems selectively store [12,22] and update [1] key memories.Relevant memories can also be retrieved based on the user input [1,15,21].However, these memory management strategies are hidden behind the interface, resulting in a lack of transparency.Users often do not know what strategy is being used and have limited control over it.This makes it difficult for users to repair conversational breakdowns that happen when there is a misalignment between how the agent manages the memory and how the user perceives the conversation.</p>
<p>We present Memory sandbox, shown in Figure 1, a system that allows users to see and manage the memory of conversational agents to align with user understanding of the conversation.Memory Sandbox transforms conversational memory, previously managed behind the user interface, into interactive memory objects within the interface.Users can manipulate the visibility and content of memory objects, spatially rearrange them, and share them across conversations.We make the following contributions: 1) The conceptualization of memory objects which makes conversational memory transparent and interactive and 2) The Memory Sandbox system that offers novel interaction affordances for users to view and manipulate the conversational memory of an intelligent agent.</p>
<p>SYSTEM OVERVIEW</p>
<p>Memory sandbox is a system that provides users with the ability to view and manipulate the memory model of an intelligent agent, resulting in a shared representation of their ongoing conversation.Memory Sandbox introduces the concept of a memory object, an interactive piece of conversational history that can be moved, edited, deleted, or combined with other memory objects through summarization.The interface is implemented in Next.js and uses the GPT-3.5 turbo model from the OpenAI API.Below we present the features of Memory Sandbox to help end users view and manage an LLM-powered agent's memory model.</p>
<p>View and manipulate memory objects</p>
<p>Explainable AI research seeks to help people form mental models of intelligent systems [17].Transparency of the inner workings of the system [6,23] and interactivity to probe and manipulate the Figure 1: Memory Sandbox is a system that enables users to see and manage the memory of conversational agents.Memory Sandbox provides the following interaction affordances: 1) toggle memory visibility, 2) add memory, 3) edit memory, 4) delete memory, 5) summarize memory, 6) create a new conversation, and 7) share memory.</p>
<p>system [16] have been demonstrated to help people interpret and interact with intelligent systems to achieve their goals.</p>
<p>Memory Sandbox makes the conversational memory explicit through the use of 'memory objects' which can be viewed and manipulated within the interface.This was inspired by prior work that 'objectifies' tools [2,3] and attributes [20] to enable flexibility, expressiveness, and direct manipulation.This results in a 'shared representation' [7,8] and common ground [5]-so what users see on the front-end is what an LLM would 'see' on the back-end.</p>
<p>Additionally, users can view, edit, add, and delete memory objects to directly control how the agent 'sees' the conversation.</p>
<p>Toggle memory object visibility</p>
<p>As a conversation grows, LLMs must increasingly rely on their memory management strategy to infer meaning from the conversation.However, in longer conversations, it is unclear what parts of the conversation are stored in memory or are attended to by the model [11].This results in a poor mental model for users and a lack of control over what context is maintained and used by the agent.</p>
<p>Memory Sandbox enables users to selectively hide or show memory objects to control what context is shared with the agent.When the user's intent changes or the conversational context switches, the user can toggle the visibility of memory objects to hide or show parts of the conversation.As a signifier, hidden memory objects are grayed out within the interface.</p>
<p>Curate memory objects</p>
<p>Discussants develop and refine their understanding as a conversation unfolds [5].Thus, Memory Sandbox provides controls for users to curate memory objects by editing an existing memory object to refine or update the context, deleting a memory object to remove completely irrelevant context, and adding a new memory object to supplement extra context.Additionally, the arrangement of context is shown to have a significant effect on how well LLMs are able to leverage relevant context [11].In Memory Sandbox, all the memory objects are draggable, allowing users to experiment and refine the ordering and placement of memory objects in a conversation.</p>
<p>Summarize memory objects</p>
<p>Reminiscent of how humans attend to key aspects in a conversation [14], abstractive summarization distills a large amount of information to provide essential elements to the agent.Yet, what is considered as 'key aspects' can vary for individuals, even in the same conversation [14].Memory Sandbox enables uses to select memory objects that are summarized by the LLM.The resulting memory object represents the previous conversation and can be further refined by the user.The original conversation can be viewed by clicking on the summary.</p>
<p>Share memory objects across conversations</p>
<p>Aligning with the goal of managing memory, Memory Sandbox also provides affordances for sharing memories across conversations.This offers a new way for users to engage with multiple agents outside of a single conversation thread.Unlike in conversations with people, the speaker doesn't need to repeat themselves in each conversation to establish a shared understanding.</p>
<p>Users can create and start multiple conversations with separate LLM-powered agents in the same 2D canvas.Memory objects can be shared and connected between conversations by dragging the memory object from one conversation to another.When dragging, memories are copied by reference to help the user identify the context source.</p>
<p>DISCUSSION</p>
<p>Conversing is a collaborative activity where participants develop common ground through summarizing the discussion, repairing breakdowns, and emphasizing or de-emphasizing shared ideas [5].Yet, existing chatbot interfaces do not provide affordances for understanding how the agent 'sees' the conversation.Additionally, users can not rely on a theory of mind.These aspects result in a poor mental model for users and potential misalignment in understanding where conversational breakdown can occur.</p>
<p>Memory Sandbox transforms previously implicitly managed conversational memory behind the interface into interactive memory objects on the interface, exposing full control over the memory model of the agent to end users.By selectively hiding, showing, and curating memory representation, we can give users more control over how the agent should "see" the conversation.In addition to curating memory in a single conversation, Memory Sandbox is also a design probe toward memory manipulation affordances for multi-agent interactions.By displaying multiple agents on the same screen and making memories interactive and draggable, Memory Sandbox allows end users to selectively control the shared or unique memory each agent contains.</p>
<p>Tools are beginning to emerge that focus on how users might interact with LLMs, including mapping UI affordances to an LLM [13], grounding human-AI collaboration in a shared artifact [9], providing templates to facilitate prompt generation [10], and decomposing complex prompts to facilitate debugging [19].In this paper, we presented Memory Sandbox an interactive system that probes the design space of interaction techniques for memory management of LLMs.Our future work includes user studies to evaluate the efficacy of these techniques and potential trade-offs for implicit vs explicit memory management</p>
<p>Sang-Woo Lee, Woomyoung Park, and Nako Sung. Sanghwan Bae, Donghyun Kwak, Soyoung Kang, Min Young Lee, Sungdong Kim, Yuin Jeong, Hyeri Kim, arXiv:2210.08750Keep me updated! memory management in long-term conversations. 2022. 2022arXiv preprint</p>
<p>Local tools: An alternative to tool palettes. James D Benjamin B Bederson, Allison Hollan, Jason Druin, David Stewart, David Rogers, Proft, Proceedings of the 9th annual ACM symposium on User interface software and technology. the 9th annual ACM symposium on User interface software and technology1996</p>
<p>Toolglass and magic lenses: the see-through interface. Eric A Bier, Maureen C Stone, Ken Pier, William Buxton, Tony D Derose, Proceedings of the 20th annual conference on Computer graphics and interactive techniques. the 20th annual conference on Computer graphics and interactive techniques1993</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>Contributing to discourse. H Herbert, Edward F Clark, Schaefer, Cognitive science. 131989. 1989</p>
<p>Bringing transparency design into practice. Malin Eiband, Hanna Schneider, Mark Bilandzic, Julian Fazekas-Con, Mareike Haug, Heinrich Hussmann, 23rd international conference on intelligent user interfaces. 2018</p>
<p>Agency plus automation: Designing artificial intelligence into interactive systems. Jeffrey Heer, Proceedings of the National Academy of Sciences. 11662019. 2019</p>
<p>Principles of Mixed-Initiative User Interfaces. Eric Horvitz, 10.1145/302979.303030Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsPittsburgh, Pennsylvania, USA; New York, NY, USAAssociation for Computing Machinery1999CHI '99)</p>
<p>CausalMapper: Challenging designers to think in systems with Causal Maps and Large Language Model. Ziheng Huang, Kexin Quan, Joel Chan, Stephen Macneil, Proceedings of the 15th Conference on Creativity and Cognition. the 15th Conference on Creativity and Cognition2023</p>
<p>Promptmaker: Prompt-based prototyping with large language models. Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach, Michael Terry, Carrie J Cai, CHI Conference on Human Factors in Computing Systems Extended Abstracts. 2022</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, arXiv:2307.03172[cs.CL]Lost in the Middle: How Language Models Use Long Contexts. 2023</p>
<p>One chatbot per person: Creating personalized chatbots based on implicit user profiles. Zhengyi Ma, Zhicheng Dou, Yutao Zhu, Hanxun Zhong, Ji-Rong Wen, Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. the 44th international ACM SIGIR conference on research and development in information retrieval2021</p>
<p>Stephen Macneil, Andrew Tran, Joanne Kim, Ziheng Huang, Seth Bernstein, Dan Mogil, arXiv:2307.01142Prompt Middleware: Mapping Prompts for Large Language Models to UI Affordances. 2023. 2023arXiv preprint</p>
<p>An experimental study of common ground in text-based communication. Victoria C John C Mccarthy, Andrew F Miles, Monk, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing Systems1991</p>
<p>Sung Joon, Park, C Joseph, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. 2023. 2023arXiv preprint</p>
<p>Evaluating the interpretability of generative models by interactive reconstruction. Andrew Ross, Nina Chen, Elisa Zhao Hang, Elena L Glassman, Finale Doshi-Velez, Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. the 2021 CHI Conference on Human Factors in Computing Systems2021</p>
<p>Considerations on explainable AI and users' mental models. Heleen Rutjes, Martijn Willemsen, Wijnand Ijsselsteijn, CHI 2019 Workshop: Where is the Human? Bridging the Gap Between AI and HCI. Association for Computing Machinery, Inc2019</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, Denny Zhou, International Conference on Machine Learning. PMLR2023</p>
<p>Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. Tongshuang Wu, Michael Terry, Carrie , Proceedings of the 2022 CHI conference on human factors in computing systems. the 2022 CHI conference on human factors in computing systemsJun Cai. 2022</p>
<p>Objectoriented drawing. Haijun Xia, Bruno Araujo, Tovi Grossman, Daniel Wigdor, Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. the 2016 CHI Conference on Human Factors in Computing Systems2016</p>
<p>Beyond goldfish memory: Long-term open-domain conversation. Jing Xu, Arthur Szlam, Jason Weston, arXiv:2107.075672021. 2021arXiv preprint</p>
<p>Long Time No See! Open-Domain Conversation with Long-Term Persona Memory. Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, Shihang Wang, 10.18653/v1/2022.findings-acl.207Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. Yunfeng Zhang, Rachel Ke Vera Liao, Bellamy, Proceedings of the 2020 conference on fairness, accountability, and transparency. the 2020 conference on fairness, accountability, and transparency2020</p>
<p>Less is more: Learning to refine dialogue history for personalized dialogue generation. Hanxun Zhong, Zhicheng Dou, Yutao Zhu, Hongjin Qian, Ji-Rong Wen, arXiv:2204.081282022. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
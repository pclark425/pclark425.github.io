<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7319 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7319</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7319</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-272988111</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.20146v1.pdf" target="_blank">VMAD: Visual-enhanced Multimodal Large Language Model for Zero-Shot Anomaly Detection</a></p>
                <p><strong>Paper Abstract:</strong> Zero-shot anomaly detection (ZSAD) recognizes and localizes anomalies in previously unseen objects by establishing feature mapping between textual prompts and inspection images, demonstrating excellent research value in flexible industrial manufacturing. However, existing ZSAD methods are limited by closed-world settings, struggling to unseen defects with predefined prompts. Recently, adapting Multimodal Large Language Models (MLLMs) for Industrial Anomaly Detection (IAD) presents a viable solution. Unlike fixed-prompt methods, MLLMs exhibit a generative paradigm with open-ended text interpretation, enabling more adaptive anomaly analysis. However, this adaption faces inherent challenges as anomalies often manifest in fine-grained regions and exhibit minimal visual discrepancies from normal samples. To address these challenges, we propose a novel framework VMAD (Visual-enhanced MLLM Anomaly Detection) that enhances MLLM with visual-based IAD knowledge and fine-grained perception, simultaneously providing precise detection and comprehensive analysis of anomalies. Specifically, we design a Defect-Sensitive Structure Learning scheme that transfers patch-similarities cues from visual branch to our MLLM for improved anomaly discrimination. Besides, we introduce a novel visual projector, Locality-enhanced Token Compression, which mines multi-level features in local contexts to enhance fine-grained detection. Furthermore, we introduce the Real Industrial Anomaly Detection (RIAD), a comprehensive IAD dataset with detailed anomaly descriptions and analyses, offering a valuable resource for MLLM-based IAD development. Extensive experiments on zero-shot benchmarks, including MVTec-AD, Visa, WFDD, and RIAD datasets, demonstrate our superior performance over state-of-the-art methods. The code and dataset will be available soon.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7319",
    "paper_id": "paper-272988111",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00374675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>VMAD: Visual-enhanced Multimodal Large Language Model for Zero-Shot Anomaly Detection
30 Sep 2024</p>
<p>Huilin Deng deng@mail.ustc.edu.cn 
Hongchen Luo luohongchen@ise.neu.edu.cn 
Wei Zhai 
Member, IEEEYang Cao 
Senior Member, IEEEYu Kang kangduyu@ustc.edu.cn </p>
<p>School of Information Science and Technology
University of Science and Technology of China
Hefei, AnhuiChina</p>
<p>Northeastern University
Shenyang, LiaoningChina</p>
<p>School of Information Science and Technology
University of Science and Technology of China
AnhuiChina</p>
<p>Institute of Artificial Intelligence
Hefei Comprehen-sive National Science Center</p>
<p>VMAD: Visual-enhanced Multimodal Large Language Model for Zero-Shot Anomaly Detection
30 Sep 202489584B1AEBDF2F777D1628CC232013E4arXiv:2409.20146v1[cs.CV]Anomaly DetectionZero-Shot LearningMultimodal Large Language Models
Zero-shot anomaly detection (ZSAD) recognizes and localizes anomalies in previously unseen objects by establishing feature mapping between textual prompts and inspection images, demonstrating excellent research value in flexible industrial manufacturing.However, existing ZSAD methods are limited by closed-world settings, struggling to unseen defects with predefined prompts.Recently, adapting Multimodal Large Language Models (MLLMs) for Industrial Anomaly Detection (IAD) presents a viable solution.Unlike fixed-prompt methods, MLLMs exhibit a generative paradigm with open-ended text interpretation, enabling more adaptive anomaly analysis.However, this adaption faces inherent challenges as anomalies often manifest in finegrained regions and exhibit minimal visual discrepancies from normal samples.To address these challenges, we propose a novel framework VMAD (Visual-enhanced MLLM Anomaly Detection) that enhances MLLM with visual-based IAD knowledge and finegrained perception, simultaneously providing precise detection and comprehensive analysis of anomalies.Specifically, we design a Defect-Sensitive Structure Learning scheme that transfers patchsimilarities cues from visual branch to our MLLM for improved anomaly discrimination.Besides, we introduce a novel visual projector, Locality-enhanced Token Compression, which mines multi-level features in local contexts to enhance fine-grained detection.Furthermore, we introduce the Real Industrial Anomaly Detection (RIAD), a comprehensive IAD dataset with detailed anomaly descriptions and analyses, offering a valuable resource for MLLM-based IAD development.Extensive experiments on zero-shot benchmarks, including MVTec-AD, Visa, WFDD, and RIAD datasets, demonstrate our superior performance over stateof-the-art methods.The code and dataset will be available soon.</p>
<p>I. INTRODUCTION</p>
<p>I NDUSTRIAL Anomaly Detection (IAD) aims to classify and localize defects in industrial manufacturing.By identifying abnormal patterns in industrial processes, IAD techniques enable timely intervention and optimization, thereby enhancing overall productivity.Collecting anomaly data is challenging due to their rarity and unpredictability [1].Therefore, conventional IAD works [2]- [4] mainly explore unsupervised techniques, measuring the deviation of test sample features from the learned normal distribution.However, these methods necessitate abundant training samples.Consequently, they exhibit limited generalization to novel classes and fail to adapt to dynamic production environments.</p>
<p>Recently, Zero-Shot Anomaly Detection (ZSAD) offers flexible inspection by using text prompts for anomaly measurement, enabling detection on unseen objects.Mainstream Contrast-based methods, based on pre-trained CLIP [5], compares image features to textual descriptors representing 'normal' and 'abnormal' respectively, as illustrated in Fig. 1 (a).WinCLIP [6] initially adapts CLIP using manually crafted prompts.AnomalyCLIP [7] further substitutes manual templates with object-agnostic text vectors for generic representation.ClipSAM [8] leverages SAM to CLIP's anomaly localization.Despite the promise, these methods still perform ZSAD in a closed-world setting, executing binary classification in constrained semantic space with predefined prompts, thus struggling with unseen defects.Moreover, generic descriptors (e.g., 'damaged' in Fig. 1 (a)) are insufficient to capture diverse anomalies in industrial manufacturing.</p>
<p>Most recently, Multimodal Large Language Models (MLLMs) have emerged as a promising solution to the closedworld limitations.Unlike Contrast-based methods constrained in discriminative paradigm with predefined prompts, MLLMs exhibit a generative paradigm with open-ended text interpretation, enabling more adaptive anomaly analysis.Built upon large language models, MLLMs have demonstrated remarkable image-text understanding with more flexible input-output formats [9], [10].This flexibility enables MLLMs to process arbitrary textual prompts with visual input, facilitating dynamic anomaly analysis based on various criteria and scenarios (as shown in Fig. 1 (b), the same sample yields different inspection results based on varying criteria).Therefore, adapting MLLMs for IAD tasks represents a promising yet underexplored field.Nevertheless, this adaptation faces inherent challenges.Anomalies are visually confusing, characterized by minimal discriminative feature variances between normal and anomalous samples, mostly manifesting in objectlocalized regions.While MLLMs' general visual interpretation, they often struggle with abnormality discriminability and fine-grained perception of defects.</p>
<p>This weak discriminability often stems from minor manufacturing flaws or imperfections that blur the boundary between defective areas and surrounding normal regions.Intriguingly, our investigation uncovers a promising approach to address this issue.Despite subtle global visual variances, normal and anomalous samples exhibit pronounced disparities in patch-similarity distributions.Specifically, normal patches frequently exhibit visual parallels across multiple unmarked samples, whereas anomalous areas rarely find such correspondences.Leveraging this patch-similarity disparity as valuable IAD-specific knowledge, we propose a Defect-Sensitive Structure Learning (DSSL) scheme to amplify the abnormality distinction in LLM representation.Specifically, in the visual branch, DSSL computes local-global Visual Similarity between patch embeddings and global normal features.In LLM space, the similarity is calculated between visual-patch tokens and normal semantic tokens, which combines both linguistic and visual cues [11].Finally, DSSL aligns these similarity distributions using a contrastive loss [12], effectively transferring patch-similarity cues to LLM space.</p>
<p>The second challenge relates to fine-grained semantics of anomalies.We observe that visual projectors, responsible for transforming visual signals into LLM-compatible visual tokens, directly impact MLLM's perception of subtle anomalies.Traditional MLLMs' visual projectors use abstractors for efficiency [13]- [15], but they compromise visual feature integrity by summarizing information from limited areas (Fig. 2 (a)).To address this, we propose a Locality-enhanced Token Compression (LTC) method which preserves rich semantics while reducing tokens.As outlined in Fig. 2 (b), LTC mines multi-level features in local contexts by employing coarse-tofine injection with multi-level features' integration.</p>
<p>To this end, We propose VMAD (Visual-enhanced MLLM Anomaly Detection), a novel framework offering simultaneous anomaly localization and explainable text, with interactive engagement for follow-up inquiries.As illustrated in Fig. 3 (a), VMAD comprises an MLLM and visual branch.The MLLM processes image-text inputs, generating '[seg]' tokens for mask decoder's prompt segmentation.To address anomaly detection challenges, we introduce two innovative modules: Moreover, we present Real Industrial Anomaly Detection (RIAD), an extensive IAD dataset, encompassing 28,040 images across 24 object categories and 15 defect types.As shown in Fig. 5, RIAD features paired image-text data with anomaly masks, providing detailed anomaly descriptions, impact analyses, and recommendations, offering MLLMs crucial IAD-specific knowledge.Extensive experiments on zero-shot benchmarks, including MVTec-AD, VisA, WFDD, and our RIAD, demonstrate VMAD's superior performance over stateof-the-art methods.Furthermore, VMAD exhibits exceptional capability in providing accurate assessments and elaborative insights for industrial defects.</p>
<p>Our contributions are summarized as follows: 1) We propose VMAD, a novel framework for IAD that simultaneously localizes anomalies and generates explanatory text.Additionally, we design a cross-modal learning scheme, DSSL, integrating visual similarity cues as IAD-specialized knowledge into MLLMs.2) We introduce a novel visual projector, the LTC mechanism, which mines multi-level features in local contexts to enhance fine-grained defect detection.3) We collect a dataset named RIAD containing plenty of paired image-text data with anomaly masks, providing a comprehensive resource for MLLM-based IAD development.</p>
<p>II. RELATED WORK</p>
<p>A. Industrial Anomaly Detection</p>
<p>Given the scarcity of anomalies, conventional Industrial Anomaly Detection (IAD) research mainly explores unsupervised and self-supervised techniques.These approaches fall into two main streams.Reconstruction-based methods ( [16]- [18]) use encoder-decoder architectures to minimize inputreconstruction discrepancies.The Embedding-based branch ( [4], [19], [20]) identifies anomalies through feature disparities.It encompasses several sub-categories: a) One-class techniques( [2], [3]).b) Memory-augmented models ( [4], [21]).c) Knowledge distillation frameworks ( [22], [23]).Conventional methods follow 'one-class-one-model' paradigm and struggle with novel object classes.In contrast, our VMAD enables incontext learning for multiple novel object categories.</p>
<p>B. Zero-shot Anomaly Detection</p>
<p>Zero-shot anomaly detection [6], [8], [24], using only text prompts even for unseen classes, has gained interest due to the increasing demand for flexibility in industrial settings.WinCLIP [6] pioneers language-driven zero-shot IAD using CLIP [5], while AnoVL [25] incorporates test-time adaptation for improved localization.As for another line, SAA [26] uses text prompts on SAM [27] for candidate masks and filters them with complex mechanisms.Clipsam [8] combines CLIP and SAM for segmentation and refinement.Despite strong zero-shot ability, they lack explainable analysis and depend on additional predefined steps like threshold filtering or manual prompts.Our proposed VMAD offers flexible detection and explainable analysis with a novel MLLM-based framework,</p>
<p>C. Multimodal Large Language Models (MLLMs)</p>
<p>Large Language Models (LLMs) have shown remarkable capabilities in language tasks [28].Building on this, Multimodal Large Language Models (MLLMs) extend these capabilities to the visual domain.Early efforts like Flamingo [29] and BLIP-2 [30] pioneered the use of Q-Former and Resampler to bridge vision and language.Subsequent models such as LLaVA [31] and MiniGPT4 [9] enhanced instruction following through visual instruction tuning.Recent advancements, including Ferret [32], LISA [33], GLaMM [34], and LLaVA-Ground [10], have pushed MLLMs towards fine-grained visual understanding and grounded conversation generation.</p>
<p>In Industrial Anomaly Detection, AnomalyGPT [35] and Myriad [36] pioneer MLLM applications.To enable LLM's visual comprehension, AnomalyGPT converts anomaly maps to learnable embeddings while Myriad encodes anomaly maps into LLM-compatible tokens via Q-Former [30].However, both of them rely on pre-trained visual models for anomaly localization, confining LLMs to text responses and thus restricting overall generalization.Our VMAD extends LLMs to both anomaly localization and analysis, introducing a novel patch-based scheme for cross-modal generalization.</p>
<p>Visual projectors are crucial for real-time IAD tasks, efficiently transforming visual features for LLM compatibility.Existing methods like QPN [37] and DeepStack [13] focus on query enhancement and token restructuring, while others use pixel shuffle [14] or nearby concatenation [38] to reduce visual tokens.However, these approaches compromise visual feature integrity in pursuit of efficiency [15].Our LTC mechanism incorporates multi-level visual cues through coarse-to-fine scheme, balancing efficiency with structural integrity.</p>
<p>III. METHOD</p>
<p>This section outlines the problem setup (Sec.III-A) and provides an overview of VMAD with its loss function (Sec.III-B).It then introduces the Defect-Sensitive Structure Learning scheme for MLLM and visual branch (Sec.III-C).Finally, we describe the Locality-enhanced Token Compression (LTC) mechanism as MLLM's visual projector (Sec.III-D).</p>
<p>A. Problem Setup</p>
<p>We extend traditional mask-only zero-shot anomaly detection by incorporating text responses.This task aims to detect anomalies in novel object categories without support images while providing textual explanations.Training (D train ) and testing (D test ) sets contain disjoint object classes.Query images and text instructions are fed into the network in pairs.The text instruction T q includes a description of normal scenes and questions about anomalies.e.g.,'The gray metal plates in the picture should be secured with four pairs of screws, which must not be loose or missing.Is there any anomaly in this picture?'Given multi-modal inputs (I img , T q ) from D train , the model learns to associate semantic information to provide text responses and segmentation masks.During testing, the model is evaluated without further optimization.</p>
<p>B. Overall Architecture</p>
<p>Our network consists of a standard MLLM and a visual branch.The Defect-Sensitive Structure Learning (DSSL) scheme spans both branches, using visual patch similarity to enhance MLLM's anomaly discrimination.The Localityenhanced Token Compression (LTC) mechanism serves as the MLLM's visual projector, generating compact, detail-rich visual tokens.Subsequent sections elaborate on each module.</p>
<p>1) MLLM Framework: The proposed MLLM framework comprises three pivotal components: (1) Visual Encoder F I : the input image I img ∈ R H×W ×3 is transformed into a set of visual embeddings I v ∈ R N ×C through the widely-utilized CLIP-ViT-L/14 [39] vision encoder.(2) Visual Projector Ψ I→T : it is responsible for projecting visual embeddings I v into visual token T v in the textual embedding space T , ensuring they have the appropriate dimension for subsequent language model.The visual projector takes the N visual embeddings I v and converts them to M visual tokens T v , where M &lt; N .(3) LLM denoted as Θ(T v , T t ): it processes the visual token T v and the textual token T t , producing the coherent textual response auto-regressively.Formulated as:
Iv = FI (I img ),(1)Tv = ΨI→T (Iv),(2)ŷtxt = Θ(Tv, Tt).(3)
The embedding-as-mask scheme [33] is leveraged to equip our MLLM with segmentation ability for anomaliy localization.In particular, we augment the LLM's vocabulary with a specialized <seg> token, which serves as the bridge between the MLLM and downstream mask decoder.Moreover, the text output y txt is ensured to end with the <seg> token.</p>
<p>2) Visual Branch: The <seg> token's last-layer embedding H seg in the LLM is extracted and passed through an MLP projection layer Γ(•) to obtain H seg .H seg is then used to inform the mask decoder about the semantic knowledge necessary for anomaly discrimination.H seg and f are fed into the mask decoder to obtain the final segmentation mask, where the image I img are processed by the visual backbone (e.g., SAM [27]) to extract visual feature maps f .The process can be expressed as:
Hseg = Γ(Hseg), f = V backbone (I img ),(4)
3) Training Objectives: The model is optimized end-to-end through the text generation loss L txt , segmentation mask loss L seg and the specially designed defect-structure loss L struc .The overall objective L is the weighted sum of these losses, formally defined as:
L = λtxtLtxt + λsegLseg + λ pbsd L pbsd .(6)
To encourage high-quality segmentation results, the segmentation loss L seg is computed as a combination of perpixel binary cross-entropy (BCE) loss and DICE loss with corresponding loss weights λ bce and λ dice .Concretely, given the predicted map M and query mask M q , segmentation loss L seg is formally defined as:
Lseg = λ bce BCE( M , Mq) + λ dice DICE( M , Mq).(7)
For text generation, L txt is the auto-regressive cross-entropy loss between predicted text ŷtxt and the ground-truth text y txt .The patch distribution similarity loss L pbsd (Sec.III-C Eq.18) aims to enhance anomalous structure discriminability by transferring patch-level similarity knowledge from visual domain to multimodal space, enabling consistent normalabnormal patch distinction across modalities.</p>
<p>C. Defect-Sensitive Structure Learning</p>
<p>To enhance anomalous structure discriminability in LLM space, we introduce Defect-Sensitive Structure Learning j=1 , we apply ROI pooling using the patch box coordinates, and then project the pooled features through a normalization head:
f = V backbone (Iimg), v j = gα(ROI(f ; B[j])),(8)
where g α represents projection head.Meanwhile, global normalized feature z is obtained by applying global pooling to the feature map f , then projecting it through g β :
z = g β (GAP (h)).(9)
Based on these global normalized features, we first establish a memory queue that contains all normal and abnormal samples from the same class as I img :
M img = {zm : ym = y + or ym = y − },(10)
where y + and y − denote the normal and abnormal class labels, respectively.We then define a positive set P as a subset of M, containing only the global features of normal images from the same class as I img .The P is formulated as:
P img = {zt : yt = y + }. (11)
Fig. 4: Overview of LTC mechanism.It incorporates multi-level visual cues through a coarse-to-fine scheme, providing comprehensive image information to the LLM.</p>
<p>Finally, the similarity relationship between local patch and normal global representation is calculated as conditional probability, using a pre-defined temperature parameter τ :
p(zt|v j ) = exp(zt • v j /τ ) zm∈M img exp(zm • v j /τ )(12)
Eq.12 encodes similarity between local patch (v j ) and global representations (z t ).High similarity indicates normal patterns, while low similarity suggests anomalies.This local-global distinction is then used to guide normal-abnormal differentiation in LLM space, enhancing LLM's anomaly discrimination.</p>
<p>2) Text-Visual Patch Similarity Distribution: It calculates the similarity between visual-patch tokens and semantic tokens of normal samples in LLM space.Notably, semantic tokens combine both linguistic and visual cues for a comprehensive representation of normality.To obtain patch tokens
{T j v } N b j=1
in LLM space, we extract image patches {I j box } N b j=1 from the original image and project them into the LLM space:
I j box = ROI(Iimg; B[j]), T j v = Ψ I→T (FI (I j box )).(13)
To obtain multimodal semantic tokens for image x, we generate text-based tokens π by passing normal textual descriptions through Meta-Net [11].We then enrich these linguistic representations with structural details by projecting normalized features z of normal images into LLM space.Finally, the text-based tokens π and projected global visual tokens are combined through a simple additive operation.The multimodal semantic tokens are calculated as:
θω(x) = Ψ I→T (z) + π, π = M eta(tx),(14)
where θ ω (x) represents multimodal semantic tokens for the image x.The global normalized feature z is obtained consistent with our visual branch (Eq.9), while the feature is extracted through MLLM's visual encoder.Ψ I→T represents the projection of global normalized feature from visual to LLM space.In this work, the Meta-Net is built with a two-layer bottleneck (Linear-ReLU-Linear). Based on Eq. 14, we then define the positive set P txt and memory queue M txt as follows:
Ptxt = {θω(xt) : yt = y + },(15)
Mtxt = {θω(xm) : ym = y + or ym = y − }.</p>
<p>Similar to Eq. 12, the similarity distribution between patch tokens T j v and normal semantic tokens θ ω (z) is computed as:
p(θ t ω |T j v ) = exp(θ t ω • T j v /τ ) θ m ω ∈Mtxt exp(θ m ω • v j /τ ) .(17)
To align local-global visual distributions and multimodal semantic distributions obtained by Eq. 12 and Eq.17, respectively, we introduce the Patch-Based Similarity Distribution (PBSD) loss.Mathematically, PBSD loss can be expressed as:
L pbsd = 1 N b N b j=1 x t ∈P −p(zt|v j ) log p(θ t ω |T j v ),(18)
where p(z t |v j ) is detached from the computation graph to prevent gradient flow.The PBSD loss effectively transfers the knowledge of patch-level similarities from visual domain to multimodal space, ensuring the model learns to distinguish normal and abnormal patches consistently across modalities.</p>
<p>D. Locality-enhanced Token Compression</p>
<p>In real-time industrial inspection, abstractors are preferred for efficiency but struggle with spatial information (Fig. 2 (a)).Our Locality-enhanced Compression balances spatial details and efficiency by mining multi-level features in local contexts (Fig. 2 (b)).It performs Locality-enhanced downsampling with a dual-faceted refinement, comprising Coarse-to-fine Refinement and Multi-level Feature Integration (Fig. 4).
p ′ = p + ∆p,(19)O(i, j) = K−1 k=0 W k * X(i + s * (p ′ + ∆p), j),(20)
where s denotes stride and the learned offset ∆p is added to sampling locations.Coarse-to-fine Refinement.The refinement process begins with coarse-to-fine mappings, integrating detailed cues from high-resolution features into the downsampled coarse representation.As the coarse representation
I ↓ v , each pixel in I ↓ v ∈ R 1×M ×C corresponds to ρ × ρ sub-region in I v ∈ R ρ 2 ×M
×C .This mechanism enables low-resolution queries to assimilate fine-grained keys and values.Specifically, we consider the low-resolutionI ↓ v ∈ R 1×M ×C as point-based queries while I v as region-based keys and values.For level l, we compute the attention scores:
V att,l = sof tmax( Q ↓ l K T l √ d )V l , (21)
where Q ↓ l represents the queries derived from coarse features, K l , V l are keys and values from the original ones.Multi-level Feature Integration.To achieve hierarchical semantic representations, multi-level visual features are integrated as enriched reference keys and values.This leverages inherent biases of different CLIP encoder layers towards distinct visual patterns: shallow layers capture low-level details, while deeper layers encode semantic understanding.Concretely, the aggregation values incorporate the past n levels:
V att,all = Linear(cat([Vatt,L−n−1, ...Vatt,L−2])), (22)
where L is the total levels in CLIP encoder.Finally, we enhance last-layer coarse representation with aggregated values:
I * v = I ↓ v + V att,all .(23)
LTC maintains rich semantic cues while reducing visual tokens, simultaneously performing coarse-to-fine injection and multi-level feature integration.</p>
<p>IV. DATASET</p>
<p>A. Collection Details.</p>
<p>We collect the Real Industrial Anomaly Detection Dataset (RIAD), extracted from various practical industrial datasets including MIAD [44], Realiad [45], Vision [46], MAD-sim [47], MVTec-3D [48], and PKU-market-iphone [49].RIAD pairs each image with an anomaly mask and text data (Fig. 5 (a)).As illustrated in Fig. 5 (b) (top), RIAD encompasses a rich variety of industrial scenarios, including outdoor environments such as wind power equipment, indoor industrial components, and food products.This diverse range provides a more comprehensive representation of real-world industrial defects.The RIAD contains 11,627 normal and 16,413 defect images across 24 object classes and 15 defect types, providing extensive resources for IAD model development.B. Annotation Details.</p>
<p>We utilize Anylabeling to label defect types within the anomaly regions.These semantic masks are recorded in COCO format with defect-category IDs ranging from 0 to 14, encompassing 15 distinct anomaly categories.Notably, we've transformed original binary (normal/abnormal) masks into semantic masks that distinguish different defect types, while also refining low-quality defect masks for improved accuracy.Meanwhile, images are also annotated with bounding boxes of anomaly regions.To provide rich, contextual textual data for each anomaly instance, we feed the anomaly images and COCO-format masks to GPT-4 [14], generating three types of question-answer pairs: anomaly descriptions, impact analyses, and suggestions.This comprehensive textual information distinguishes RIAD from existing datasets, offering a holistic resource for IAD models leveraging both visual and textual inputs.Paired examples are shown in Fig. 5 (a).</p>
<p>C. Statistic Analysis.</p>
<p>V. EXPERIMENTS</p>
<p>This section elaborates on the experiments' details.Section V-A1 presents the experimental protocol.Section V-B analyzes the main results.Section VI demonstrates the ablation study.</p>
<p>A. Experimental Protocol 1) Evaluation Protocols: We evaluate models' adaptability to unseen scenarios using two complementary settings:</p>
<p>• Cross-dataset Evaluation: Following [35], [36] The example responses are illustrated in Fig. 7. • Visual Question Answering Task.To preserve the original Visual Question-answering ability of LLM, we include the VQA task during training.We generate questionanswer pairs based on defect descriptions using GPT-4.3) Baselines: For a comprehensive evaluation, we compare PLMNet against 7 typical baseline methods: two MLLMbased anomaly detection models (Myriad [36] and Anoma-lyGPT [35]), three zero-shot anomaly detection models (WinClip [6], VAND [43] and AnomalyClip [7]), and one supervised anomaly detection model (BGAD [42]) and one multi-modal pixel-grounding model (LISA [33]).The anomaly detection performance is evaluated using the Area Under the Receiver Operating Characteristic Curve (AUROC).Additionally, Average Precision (AP) for Image-level detection (Img) and AUPRO for pixel-level segmentation are also included to provide more in-depth analysis.</p>
<p>4) Implementations: We employ LLaVA-7B-v1-1 [31] as our multi-modal LLM and utilize the ViT-H SAM for visual backbone.We keep the vision encoders and visual backbone frozen.Concurrently, the LLM is fine-tuned using the LoRA [52] technique.The memory bank (Sec.III-C) contains 1/20 of both normal and anomalous samples from each class.We adjust the down-sampling ratio ρ = 2 in Sec.III-D, and set n = 4 in Eq. 22. Models are trained for 10 epochs using an AdamW optimizer with cosine learning rate schedule on a 4×4090 GPU cluster, processing two input pairs per iteration.</p>
<p>B. Results and analysis</p>
<p>In this section, we compare VMAD with previous IAD works through numerical analysis and visual examples.Cross-dataset Evaluation: Table I summarizes the zero-shot results on MVTec-AD, Visa, and WFDD datasets.Tab.I reveals that VMAD outperforms competing methods in the presence of large domain gaps, particularly excels in pixellevel and region-level analyses, with margins of 3.3%p and 3.8%p compared to Myriad [36].This might be attributed to keen visual feature-capturing and sensitivity to anomalous structures.Fig. 6 visually displays the anomaly localization results, highlighting VMAD's improved segmentation precision.Cross-category Evaluation: Tab.II details the performance, demonstrating state-of-the-art results of VMAD.Notably, our model particularly excels in pixel-level analyses, achieving a substantial 2.0%∼14.5% AUC-PR improvement over other     LTC's ablation results on MVTec and comparisons with various visual projectors.We set the baseline with 2× downsampled feature maps for MLP projection.Adding (+) the injection module obtains +0.6%, +1.1% and +1.8% gains over the baseline method, respectively.Fig. 9 demonstrates the effectiveness of multi-level feature fusion.The fused map integrates layer-4's fine details with layer-1's broader structures, enhancing anomaly localization.We experiment with various projectors and keep the same settings for a fair comparison.Compared to MLP, other projectors reduce token count and significantly improve speed.Our approach surpasses the previous best method LDP-v2 [53] by +1.4%, 1.8% and 1.3%, demonstrating the effectiveness of LTC.Ablation Study on DSSL scheme.Fig. 10 presents the ablation study of Defect-Sensitive Structure Learning module.Left: Visualization demonstrates DSSL's enhanced sensitivity to anomalous structures.Right: t-SNE plot of normalized features Ψ I→T (z) in LLM space (Eq.14) shows improved discrimination between normal and anomalous samples through DSSL's patch-based similarity learning.</p>
<p>VII. CONCLUSION</p>
<p>In this paper, we present an MLLM-centric framework extending zero-shot anomaly detection with explainable analysis.The proposed VMAD unifies anomaly understanding, localization, and textual explanation, potentially revolutionizing industrial inspection through interpretable insights with human-AI interaction [33], [35], [54].Besides, we collect RIAD, a comprehensive industrial dataset, supporting VQA, anomaly segmentation, and anomaly reasoning tasks, offering a versatile resource for industrial anomaly understanding.Plus, we propose novel cross-modal learning transferring patchbased visual similarity to multi-modal space for enhanced discrimination of anomalies and a novel visual projector mining multi-level features in local contexts for fine-grained defects.Comprehensive experiments on RIAD and public IAD datasets demonstrate the robustness of our method.We believe it could advance AIGC applications in automated inspection, fostering more efficient and reliable industrial processes.</p>
<p>Fig. 1 :
1
Fig. 1: Comparison between previous ZSAD methods and MLLMs-based ZSAD methods.(a) Previous ZSAD methods use fixed templates and generic descriptions, confined to closed-world anomaly detection.(b) MLLMs-based methods leverage open-ended text interpretation and generation for IAD, providing additional comprehensive analysis and adapting flexibly to diverse criteria across multiple scenarios.</p>
<p>Fig. 2 :
2
Fig. 2: Various visual projectors.Abstractors compress limited information, while LTC mines multi-level local cues.</p>
<p>Fig. 3 :
3
Fig. 3: (Left) Overview of VMAD.VMAD incorporates a visual branch for anomaly localization (Sec.III-B), with Localityenhanced Token Compression serving as a visual projector (Sec.III-D).(Right) Defect-Sensitive Structure Learning.It aligns visual and text-visual patch similarity distributions using PBSD loss, enhancing MLLM's sensitivity to anomalous structures (Sec.III-C).GP: Global Pooling, Sim: Similarity computation.</p>
<p>(DSSL) scheme based on 'Patch-Similarity'.DSSL calculates and aligns two similarity distributions: Visual Patch Similarity (between local patches and global features) and Text-Visual Patch Similarity (between visual patches and semantic tokens in LLM space).This alignment, achieved through the Patch-Based Similarity Distribution (PBSD) loss [40], transfers patch-level similarity knowledge from visual to LLM space, ensuring consistent normal-abnormal separation across modalities.For implementation, we randomly generate a set of patch boxes B i [j] N b j=1 from the input image Iimg, where N b is the number of patch boxes per image.1) Visual Patch Similarity Distribution: To obtain embedding features of patch boxes v[j] N b</p>
<p>Fig. 5 :
5
Fig. 5: Properties of the RIAD dataset.(a) RIAD data pairs: images, semantic segmentation masks encoding defect types, and GPT-generated text.Masks and text are stored in JSON format.(b) The horizontal axis represents the categories of objects, the vertical axis represents quantity, and different colors represent different defect types.The top part shows sample RIAD images with their source datasets indicated.(c) The ratio of normal images and abnormal images in each object class.</p>
<p>Fig. 4 (
4
right) (b) illustrates the structure of locality-aware resampler.Local-Context Learner stacks L c blocks, each containing self-attention and deformable attention layers.Spatial-preserving downsampler comprises L c ResNet [41] blocks, followed by adaptive average pooling, and another L c ResNet blocks.This design balances locality-awareness with efficient feature abstraction.</p>
<p>Fig. 5 (
5
Fig. 5 (b) shows the count and distribution of anomalies in images The top of Fig. 5 (b) displays image samples, showcasing diverse scenarios from outdoor industrial settings to indoor electronics, ceramics, building blocks, and food items.Fig. 5 (c) illustrates the ratio of normal images and abnormal images in each object category.In the cross-category setting, we train on 16 classes (15,274 images) and test on 8 unseen classes (8,342 images) during training.</p>
<p>Fig. 6 :
6
Fig. 6: Zero-shot anomaly segmentation on MVTec-AD, WFDD, and ViSA datasets.</p>
<p>Fig. 7 :
7
Fig. 7: Qualitative examples of VMAD in cross-category setting.VMAD provides pixel-level anomaly localization and answers inspection-related questions, focusing on image description, anomaly impact, and suggestions.</p>
<p>Fig. 8 :
8
Fig. 8: Comparative visualization of zero-shot anomaly segmentation on RIAD dataset.</p>
<p>Fig. 9 :Fig. 10 :
910
Fig. 9: LTC Multi-Level Feature Integration: attention maps from layer-1 (last) and layer-4 (fourth-to-last), and their fused result.See Sec.VI for ablation study</p>
<p>TABLE I :
I
Zero-shot Anomaly Detection Performance on Cross-dataset Evaluation setting on MVTec-AD, Visa, and WFDD datasets.The best results are in bold while the second best are underlined.'Img' and 'Pixel' represent the mean image-level and pixel-level AUROC.
MethodsImgMVTec-AD Pixel PROAPImgViSA PixelPROAPImgWFDD Pixel PROAPLISA [33]89.191.682.281.281.385.277.879.392.193.586.087.2BGAD [42]90.191.387.385.687.488.682.080.695.096.286.888.1AnomalyClip [7]91.594.786.283.482.387.184.280.394.195.384.983.8VAND [43]91.892.588.286.883.590.781.982.894.696.184.388.5WinClip [6]92.593.489.587.584.891.380.482.493.296.286.183.5AnomalyGPT [35]92.193.986.285.186.692.481.775.693.796.985.784.2Myriad [36]93.292.387.988.285.990.581.383.591.893.987.382.7VMAD95.896.191.287.689.793.884.685.295.396.789.290.1</p>
<p>TABLE II :
II
Zero-shot Anomaly Detection Performance on Cross-category Evaluation across each class in RIAD.
Methodsmetal welding Img Pixel PROImgu block PixelPROImgtoy brick PixelPRO...wind turbine Img Pixel PROImgAverage PixelPROLISA [33]84.190.581.089.191.286.680.482.376.181.889.582.779.282.778.9BGAD [42]91.694.787.279.182.477.286.287.385.291.893.587.189.291.487.1AnomalyClip [7]90.292.291.187.490.789.289.193.789.2...88.990.689.192.796.690.5VAND [43]96.295.494.291.296.389.791.792.386.978.882.688.192.194.789.4WinClip [6]94.896.190.890.895.287.690.491.585.389.294.687.994.293.186.1AnomalyGPT [35]90.186.284.683.787.188.289.592.480.178.278.167.887.989.487.1Myriad [36]95.293.188.184.684.489.493.395.890.889.181.987.491.192.485.2VMAD97.298.691.691.198.990.297.298.192.393.798.189.594.998.992.3</p>
<p>TABLE III :
III
Component-wise experimental results.
LTC DSSLMVTec Img Pixel PRORIAD Img Pixel PRO
✗ ✗ 90.5 91.3 87.3 91.3 93.4 89.5 ✗ ✓ 93.6 94.3 89.1 92.1 94.5 90.8 ✓ ✗ 92.9 93.7 90.5 93.2 96.1 91.5 ✓ ✓ 95.8 96.1 91.2 94.9 98.9 92.3</p>
<p>TABLE IV :
IV
Ablation Study on Different Visual Projectors on MVTec dataset.We adopt token per second (TPS) to evaluate the throughput of LLM during inference.↓1.5 92.6 ↓1.7 87.2 ↓1.9 c Average-Pooling 144 31.6 94.9 ↑2.0 95.6 ↑1.3 91.9 ↑2.8 c Resampler 144 28.7 91.7 ↓1.2 92.4 ↓1.9 87.6 ↓1.5 c LDP-v2 144 29.8 91.5 ↓1.4 92.5 ↓1.8 87.8 ↓1.3
Method#Tokens #TPSImgPixelPROBaseline144-92.994.389.1+ Injection144-93.595.490.9+ Multi-level Feature144-93.895.991.0LTC(Ours)11426.895.896.191.2c MLP5765.591.4
This work is supported by the National Natural Science Foundation of China (62033012 and 62306295).
Tf 2: Few-shot text-free training-free defect image generation for industrial anomaly inspection. Q Yu, K Zhu, Y Cao, F Xia, Y Kang, IEEE Transactions on Circuits and Systems for Video Technology. 2024</p>
<p>Explainable deep one-class classification. P Liznerski, L Ruff, R A Vandermeulen, B J Franks, M Kloft, K.-R Müller, arXiv:2007.017602020arXiv preprint</p>
<p>Cutpaste: Self-supervised learning for anomaly detection and localization. C.-L Li, K Sohn, J Yoon, T Pfister, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Towards total recall in industrial anomaly detection. K Roth, L Pemula, J Zepeda, B Schölkopf, T Brox, P Gehler, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202214328</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, arXiv:2103.000202021arXiv preprint</p>
<p>Winclip: Zero-/few-shot anomaly classification and segmentation. J Jeong, Y Zou, T Kim, D Zhang, A Ravichandran, O Dabeer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202319616</p>
<p>Anomalyclip: Object-agnostic prompt learning for zero-shot anomaly detection. Q Zhou, G Pang, Y Tian, S He, J Chen, arXiv:2310.189612023arXiv preprint</p>
<p>Clipsam: Clip and sam collaboration for zero-shot anomaly segmentation. S Li, J Cao, P Ye, Y Ding, C Tu, T Chen, arXiv:2401.126652024arXiv preprint</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. D Zhu, J Chen, X Shen, X Li, M Elhoseiny, arXiv:2304.105922023arXiv preprint</p>
<p>Llava-grounding: Grounded visual chat with large multimodal models. H Zhang, H Li, F Li, T Ren, X Zou, S Liu, S Huang, J Gao, L Zhang, C Li, arXiv:2312.029492023arXiv preprint</p>
<p>Conditional prompt learning for vision-language models. K Zhou, J Yang, C C Loy, Z Liu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022825</p>
<p>Decoupled contrastive learning for longtailed recognition. S Xuan, S Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Deepstack: Deeply stacking visual tokens is surprisingly simple and effective for lmms. L Meng, J Yang, R Tian, X Dai, Z Wu, J Gao, Y.-G Jiang, arXiv:2406.043342024arXiv preprint</p>
<p>How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Z Chen, W Wang, H Tian, S Ye, Z Gao, E Cui, W Tong, K Hu, J Luo, Z Ma, arXiv:2404.168212024arXiv preprint</p>
<p>Tokenpacker: Efficient visual projector for multimodal llm. W Li, Y Yuan, J Liu, D Tang, S Wang, J Zhu, L Zhang, arXiv:2407.023922024arXiv preprint</p>
<p>Draem-a discriminatively trained reconstruction embedding for surface anomaly detection. V Zavrtanik, M Kristan, D Skočaj, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Reconstruction by inpainting for visual anomaly detection. Pattern Recognition. 1121077062021</p>
<p>Masked swin transformer unet for industrial anomaly detection. J Jiang, J Zhu, M Bilal, Y Cui, N Kumar, R Dou, F Su, X Xu, IEEE Transactions on Industrial Informatics. 1922022</p>
<p>Prioritized local matching network for cross-category few-shot anomaly detection. H Deng, H Luo, W Zhai, Y Guo, Y Cao, Y Kang, IEEE Transactions on Artificial Intelligence. 592024</p>
<p>A unified anomaly synthesis strategy with gradient ascent for industrial anomaly detection and localization. Q Chen, H Luo, C Lv, Z Zhang, arXiv:2407.093592024arXiv preprint</p>
<p>Pushing the limits of fewshot anomaly detection in industry vision: Graphcore. G Xie, J Wang, J Liu, F Zheng, Y Jin, arXiv:2301.120822023arXiv preprint</p>
<p>Distilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, arXiv:1503.025312015arXiv preprint</p>
<p>On exploring multiplicity of primitives and attributes for texture recognition in the wild. W Zhai, Y Cao, J Zhang, H Xie, D Tao, Z.-J Zha, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2023</p>
<p>Ptmnet: Pixel-text matching network for zero-shot anomaly detection. H Deng, Y Guo, Z Xu, Y Kang, 2023 9th International Conference on Big Data and Information Analytics (BigDIA). 2023</p>
<p>Anovl: Adapting visionlanguage models for unified zero-shot anomaly localization. H Deng, Z Zhang, J Bao, X Li, arXiv:2308.159392023arXiv preprint</p>
<p>Segment any anomaly without training via hybrid prompt regularization. Y Cao, X Xu, C Sun, Y Cheng, Z Du, L Gao, W Shen, arXiv:2305.107242023arXiv preprint</p>
<p>Segment anything. A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, P Dollar, R Girshick, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)October 2023</p>
<p>A survey on multimodal large language models. S Yin, C Fu, S Zhao, K Li, X Sun, T Xu, E Chen, arXiv:2306.135492023arXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, Advances in neural information processing systems. 202235</p>
<p>Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, International conference on machine learning. PMLR202319742</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Advances in neural information processing systems. 202436</p>
<p>Ferret-ui: Grounded mobile ui understanding with multimodal llms. K You, H Zhang, E Schoop, F Weers, A Swearngin, J Nichols, Y Yang, Z Gan, arXiv:2404.057192024arXiv preprint</p>
<p>Lisa: Reasoning segmentation via large language model. X Lai, Z Tian, Y Chen, Y Li, Y Yuan, S Liu, J Jia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Glamm: Pixel grounding large multimodal model. H Rasheed, M Maaz, S Shaji, A Shaker, S Khan, H Cholakkal, R M Anwer, E Xing, M.-H Yang, F S Khan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202418</p>
<p>Anomalygpt: Detecting industrial anomalies using large vision-language models. Z Gu, B Zhu, G Zhu, Y Chen, M Tang, J Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Myriad: Large multimodal model by applying vision experts for industrial anomaly detection. Y Li, H Wang, S Yuan, M Liu, D Zhao, Y Guo, C Xu, G Shi, W Zuo, arXiv:2310.190702023arXiv preprint</p>
<p>Texthawk: Exploring efficient fine-grained perception of multimodal large language models. Y.-Q Yu, M Liao, J Wu, Y Liao, X Zheng, W Zeng, arXiv:2404.092042024arXiv preprint</p>
<p>Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. X Dong, P Zhang, Y Zang, Y Cao, B Wang, L Ouyang, S Zhang, H Duan, W Zhang, Y Li, arXiv:2404.065122024arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PMLR2021</p>
<p>Incremental object detection with image-level labels. Q Yu, K Zhu, W Wang, Y Cao, Y Kang, IEEE Transactions on Artificial Intelligence. 2023</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Explicit boundary guided semi-push-pull contrastive learning for supervised anomaly detection. X Yao, R Li, J Zhang, J Sun, C Zhang, 2023</p>
<p>A zero-/fewshot anomaly classification and segmentation method for cvpr 2023 vand workshop challenge tracks 1&amp;2: 1st place on zero-shot ad and 4th place on few-shot ad. X Chen, Y Han, J Zhang, arXiv:2305.1738220232arXiv preprint</p>
<p>Miad: A maintenance inspection dataset for unsupervised anomaly detection. T Bao, J Chen, W Li, X Wang, J Fei, L Wu, R Zhao, Y Zheng, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Real-iad: A real-world multi-view dataset for benchmarking versatile industrial anomaly detection. C Wang, W Zhu, B.-B Gao, Z Gan, J Zhang, Z Gu, S Qian, M Chen, L Ma, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202422892</p>
<p>Vision datasets: A benchmark for visionbased industrial inspection. H Bai, S Mou, T Likhomanenko, R G Cinbis, O Tuzel, P Huang, J Shan, J Shi, M Cao, arXiv:2306.078902023arXiv preprint</p>
<p>Pad: A dataset and benchmark for pose-agnostic anomaly detection. Q Zhou, W Li, L Jiang, G Wang, G Zhou, S Zhang, H Zhao, Advances in Neural Information Processing Systems. 202436</p>
<p>The mvtec 3d-ad dataset for unsupervised 3d anomaly detection and localization. P Bergmann, X Jin, D Sattlegger, C Steger, arXiv:2112.090452021arXiv preprint</p>
<p>Fdsnet: An accurate realtime surface defect segmentation network. J Zhang, R Ding, M Ban, T Guo, ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2022</p>
<p>Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. P Bergmann, M Fauser, D Sattlegger, C Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Spot-thedifference self-supervised pre-training for anomaly detection and segmentation. Y Zou, J Jeong, L Pemula, D Zhang, O Dabeer, European Conference on Computer Vision. Springer2022</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, arXiv:2106.096852021arXiv preprint</p>
<p>X Chu, L Qiao, X Zhang, S Xu, F Wei, Y Yang, X Sun, Y Hu, X Lin, B Zhang, arXiv:2402.03766Mobilevlm v2: Faster and stronger baseline for vision language model. 2024arXiv preprint</p>
<p>Reason2drive: Towards interpretable and chain-based reasoning for autonomous driving. M Nie, R Peng, C Wang, X Cai, J Han, H Xu, L Zhang, arXiv:2312.036612023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
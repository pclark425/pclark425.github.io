<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2594 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2594</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2594</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-c77d908ba29567445a9a4ad1bd4461d441cce174</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c77d908ba29567445a9a4ad1bd4461d441cce174" target="_blank">AutoML-GPT: Automatic Machine Learning with GPT</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The AutoML-GPT is presented, which employs GPT as the bridge to diverse AI models and dynamically trains models with optimized hyperparameters and achieves remarkable results in computer vision, natural language processing, and other challenging areas.</p>
                <p><strong>Paper Abstract:</strong> AI tasks encompass a wide range of domains and fields. While numerous AI models have been designed for specific tasks and applications, they often require considerable human efforts in finding the right model architecture, optimization algorithm, and hyperparameters. Recent advances in large language models (LLMs) like ChatGPT show remarkable capabilities in various aspects of reasoning, comprehension, and interaction. Consequently, we propose developing task-oriented prompts and automatically utilizing LLMs to automate the training pipeline. To implement this concept, we present the AutoML-GPT, which employs GPT as the bridge to diverse AI models and dynamically trains models with optimized hyperparameters. AutoML-GPT dynamically takes user requests from the model and data cards and composes the corresponding prompt paragraph. Ultimately, with this prompt paragraph, AutoML-GPT will automatically conduct the experiments from data processing to model architecture, hyperparameter tuning, and predicted training log. By leveraging {\ours}'s robust language capabilities and the available AI models, AutoML-GPT can tackle numerous intricate AI tasks across various tasks and datasets. This approach achieves remarkable results in computer vision, natural language processing, and other challenging areas. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many AI tasks.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2594.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2594.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoML-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoML-GPT: Automatic Machine Learning with GPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AutoML system that uses a large language model (ChatGPT/GPT-4) as a controller to compose prompts from data cards and model cards, select models, suggest data processing and architectures, predict training logs for hyperparameter settings, and iteratively tune hyperparameters without (or before) full real training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoML-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AutoML-GPT uses a pretrained large language model (implemented with ChatGPT / GPT-4 in the paper) as a controller that: 1) ingests structured 'data cards' and 'model cards' (dataset metadata, label spaces, model descriptions, architecture hyperparameters); 2) composes a fixed-format prompt paragraph that encodes task, data, and model information; 3) recommends data-processing pipelines, matches tasks to candidate models (in-context task-model assignment), and proposes model architecture choices; 4) predicts training logs (epoch-by-epoch training and validation metrics) for candidate hyperparameter configurations instead of running full training, enabling virtual hyperparameter tuning; 5) uses a text encoder (e.g., CLIP text encoder) to compute similarity between data cards and transfer hyperparameter configurations from similar seen datasets to unseen private datasets; and 6) supports interactive human feedback to refine constraints or metrics (e.g., latency constraints) and iteratively update hyperparameter suggestions. Key capabilities demonstrated: automated data-processing script generation, model selection/composition, hyperparameter recommendation, predicted-training-log generation, transfer of hyperparameters to unseen datasets, and interactive re-tuning based on user constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AutoML</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning AutoML across multiple subdomains: computer vision (image classification, object detection), natural language processing (open-domain question answering / retrieval + reader), and tabular classification.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>AutoML-GPT was applied to: (a) image classification on MiniImageNet-derived subsets and an unseen 'New' dataset (ViT base finetuning), including hyperparameter grid search and transfer of best hyperparameters to unseen class subsets; (b) object detection on COCO with model composition and hyperparameter recommendation; (c) open-domain question answering (Natural Questions Open) using Dense Passage Retrieval (DPR) where hyperparameters and model choices were suggested; and (d) tabular classification (UCI Adult) using XGBoost where AutoML-GPT supplied hyperparameters and the model was trained accordingly. The system attempts end-to-end tasks: data processing, model architecture selection/composition, hyperparameter tuning (via predicted logs), and producing scripts/configs for training.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-to-high complexity: the system must search a hyperparameter space (learning rates, weight decay, batch size, epochs, architecture hyperparameters) and select among model architectures (e.g., ViT variants, detection backbones). The search space dimensionality is not exhaustively quantified in the paper, but experiments include grid-search over hyperparameters for 15-class subsets (MiniImageNet) and transfer to a 10-class unseen 'New' set. Complexity factors: multi-modal tasks (images, text, tabular), distribution shift (unseen datasets), non-linear model performance surfaces, and multi-objective constraints (accuracy vs. latency). Quantitative measures in paper: dataset splits (80%/20%), subset sizes (15 classes for trained subsets, 10 classes for 'New'), similarity scores (e.g., 60%/40% similarity between data cards used for transfer), and reported final performance metrics (see success_rate).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses pre-existing public datasets and curated subsets: MiniImageNet-derived subsets (constructed from ImageNet), COCO for object detection, Natural Questions Open for QA (with DPR), and UCI Adult for tabular classification. For unseen-dataset experiments, only metadata (data card) and text descriptions were given; no raw-label training data from other domains was required for hyperparameter recommendation. The system relies on pre-existing model cards (best hyperparameters from trained datasets) and data cards; the authors note that many high-quality model descriptions are needed for broad coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>The paper emphasizes that AutoML-GPT predicts training logs instead of running full-scale training to reduce compute; specific compute hours, GPU counts, or dollar costs are not reported. Experiments included actual finetuning in some cases (e.g., ViT finetuning, XGBoost training) to validate recommendations, but no end-to-end compute budget is provided. The approach reduces the number of full trainings by using predicted logs to narrow hyperparameter choices, implying lower net compute than exhaustive grid search, but quantitative savings are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-structured within supervised ML paradigms: discrete tasks with clear evaluation metrics (Top-1 accuracy for classification, COCO detection metrics for object detection, validation loss and accuracy for tabular classification, DPR-standard metrics for QA). Problems are primarily stochastic (training randomness), continuous/discrete (continuous hyperparameters like learning rate; discrete like epochs, architectures), and mostly well-defined with standard metrics. Domain knowledge is encapsulated in model cards and data cards, which the LLM uses to inform recommendations. Unseen-dataset transfer is more open-ended and relies on similarity-based transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task-specific ML metrics used to evaluate success: Top-1 accuracy (classification), validation loss and accuracy (tabular), standard detection metrics for COCO (not numerically reported in the paper excerpt), alignment with known good hyperparameters (e.g., DPR hyperparameters), and qualitative assessment of produced scripts/configs. For hyperparameter recommendation validity, they report Top-1 prediction accuracy (probability of selecting the best hyperparameter setting).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported quantitative results from the paper: (1) Unseen-dataset hyperparameter recommendation: AutoML-GPT achieved 98% accuracy for 'Top-1 prediction' of appropriate hyperparameters versus 80% Top-1 accuracy for average randomly-selected hyperparameters. (2) Initializing the model using AutoML-GPT's suggested hyperparameters (without additional datasets) produced 82% Top-1 accuracy — better than random but below the recommended-setting performance. (3) UCI Adult classification with XGBoost using AutoML-GPT hyperparameters yielded final validation loss 0.277 and accuracy 85.92%. The paper states that AutoML-GPT's hyperparameters for DPR align closely with DPR's own reported hyperparameters (e.g., learning rate 1e-5, max epochs 40). Exact COCO detection numeric results are not provided in the text excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limitations and failure modes noted or implied: (1) Dependence on high-quality model cards and data cards — lack of comprehensive model descriptions limits the system's ability to incorporate diverse models. (2) Predicted training logs are approximate; relying on predicted logs rather than real training can produce imperfect hyperparameter recommendations (evidenced by the 82% Top-1 when only initializing with suggested hyperparameters). (3) Transfer to unseen datasets depends on the quality of similarity measures between data cards; poor or misleading metadata can degrade transfer. (4) The system may underperform when real compute-based validation is necessary for final tuning (predictions may not fully capture stochastic training dynamics or architecture-specific behaviors). (5) The paper does not report comprehensive robustness evaluations across many domains, so generalization limits are not fully quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Factors contributing to success in the experiments: (1) Rich, structured metadata (data cards and model cards) enabling the LLM to reason about dataset-model matches and hyperparameter transfer; (2) Use of a powerful LLM (ChatGPT/GPT-4) with strong instruction-following and reasoning capabilities; (3) Similarity-based transfer using a text encoder (CLIP) to relate unseen datasets to seen ones; (4) Constraining and structuring prompts via fixed-format paragraphs so the LLM receives consistent, task-relevant information; (5) Interactive human feedback and the ability to incorporate constraints (e.g., inference-time constraints) to steer recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Across domains the method showed consistent practical usefulness: (a) For unseen image classification datasets (MiniImageNet subsets), AutoML-GPT's hyperparameter recommendation vastly outperformed random selection (98% vs 80% Top-1 selection accuracy) and improved actual training when used; (b) For object detection on COCO, AutoML-GPT produced data-processing scripts, model composition, and reasonable hyperparameter suggestions (specific numeric gains not provided); (c) For question answering (Natural Questions Open + DPR), AutoML-GPT generated hyperparameters that align closely with DPR's own settings (e.g., learning rate 1e-5); (d) For tabular classification (UCI Adult), following AutoML-GPT's suggestions achieved 85.92% accuracy. Overall, performance benefited when clear data/model cards and similar prior datasets were available; less-clear metadata or lack of appropriate prior models reduce effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Direct human researcher baselines are not extensively reported. The paper provides a comparison to a 'randomly-selected hyperparameters' baseline for unseen dataset hyperparameter selection (80% Top-1 accuracy), and shows AutoML-GPT at 98% Top-1. For DPR hyperparameters and other tasks, AutoML-GPT's suggestions align with known good hyperparameters (human/author-specified), but no controlled human expert study or time/cost comparison is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoML-GPT: Automatic Machine Learning with GPT', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2594.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2594.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-following language model (GPT-4) used in this work as the core LLM controller for AutoML-GPT; it composes prompts, reasons about data/model cards, and generates predicted training logs and hyperparameter recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT / GPT-4 (as LLM controller)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used as the implementation of the controller LLM in AutoML-GPT: receives structured prompts (data cards + model cards), generates recommended data processing steps, model assignments, hyperparameter configurations, predicted training logs, and handles interactive queries/constraints from users. The authors implemented AutoML-GPT using OpenAI's GPT-4 via platform.openai.com.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Large Language Model used as controller</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Applied within machine learning tasks (computer vision, NLP, tabular ML) as the reasoning and orchestration component.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Performs natural-language reasoning to translate dataset/model metadata into actionable ML pipeline components (scripts, hyperparameters, predicted training traces), facilitating automated model tuning and configuration selection.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>LLM must map heterogeneous structured metadata to a multi-dimensional configuration space; complexity depends on the task's hyperparameter/architecture space and the fidelity of predicted logs. The paper demonstrates nontrivial tasks including unseen dataset transfer and multi-modal pipeline generation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on the data/model card textual metadata provided in prompts; does not directly consume raw dataset examples unless included in the prompt or external tool calls are made.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Inference cost associated with GPT-4 queries; authors do not disclose total token counts or API call volume. Using GPT-4 reduces number of full training runs but incurs API inference cost (not quantified).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Uses well-structured, fixed-format prompts but deals with open-ended mapping from text descriptions to ML configurations. Deterministic output may vary due to sampling/LM stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Measured indirectly via downstream ML performance after following GPT-4 recommendations (Top-1 accuracy, validation loss, alignment with expert hyperparameters).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported separately from AutoML-GPT; successes attributed to AutoML-GPT derive from using GPT-4 as controller (e.g., hyperparameter Top-1 selection 98% as reported for AutoML-GPT).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>LLM output quality depends on prompt quality and available model/data cards; hallucinations or overconfident incorrect predictions of training dynamics are possible. Predicted training logs may not fully capture stochastic training behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strong language understanding and instruction following of GPT-4, combined with structured prompts and curated model/data cards.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper does not compare multiple LLMs; only GPT-4 (ChatGPT) is used for the AutoML controller in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>No explicit human-only baseline for generating configuration text is provided; comparisons are to random hyperparameter selection or known hyperparameters from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoML-GPT: Automatic Machine Learning with GPT', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2594.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2594.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HuggingGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HuggingGPT: Solving AI tasks with ChatGPT and its friends in HuggingFace</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-based system (cited in related work) that uses GPT as an interaction agent to orchestrate HuggingFace models to solve AI tasks; cited as related, not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HuggingGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as related work: a system that treats GPT as a controller to coordinate multiple specialized models (from HuggingFace) to complete tasks, enabling tool-use and model orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>GPT-based system / orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General AI tasks (multi-step, multi-modal), per the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Not applied in this paper; cited as an example of prior work integrating GPT as a controller to invoke specialist models.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Not assessed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Not applicable here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Mentioned as modular orchestration of specialist models via GPT.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Not discussed beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Mentioned as complementary prior work; no direct comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoML-GPT: Automatic Machine Learning with GPT', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2594.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2594.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VisualGPT / Visual ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VisualGPT / Visual ChatGPT: Talking, drawing and editing with visual foundation models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-based system that integrates visual foundation models to allow ChatGPT-style interaction over images; cited as related work, not used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual chatgpt: Talking, drawing and editing with visual foundation models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>VisualGPT / Visual ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as an example of integrating GPT with visual foundation models to enable multi-modal interaction and task execution on images.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>GPT-based system / multi-modal orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Computer vision / multi-modal interaction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Not applied in this paper; cited in related work to show GPT-based systems that incorporate visual models.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Not assessed here.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoML-GPT: Automatic Machine Learning with GPT', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2594.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2594.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAGI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAGI: When LLM meets domain experts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source AGI research platform cited as related work that offers complex, multi-step tasks and task-specific datasets; mentioned as complementary to AutoML-GPT's goals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openagi: When llm meets domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenAGI</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as a platform that composes multi-step tasks with LLMs and task-specific expert components, illustrating prior efforts to build LLM-centered AI systems.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>GPT-based system / research platform</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General AI tasks and multi-step workflows</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Not applied in experiments; provided as related work describing LLM-driven task platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoML-GPT: Automatic Machine Learning with GPT', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2594.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2594.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoGPT (repo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-GPT (open-source autonomous agent repo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source autonomous agent framework referenced in related work; cited as an example of systems that can incorporate external information (search engines) and automate multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Auto-GPT (software project)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned briefly in related work as another GPT-based system able to orchestrate multi-step autonomous behaviors by chaining LLM calls and external tools; not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>GPT-based autonomous agent (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General automation / multi-step tasks</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Not applied here.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoML-GPT: Automatic Machine Learning with GPT', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2594.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2594.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ART</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ART: Automatic multi-step reasoning and tool-use for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework cited in related work that uses frozen LLMs to automatically generate intermediate reasoning steps as a program and supports tool use; mentioned as related to chain-of-thought and automated reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Art: Automatic multi-step reasoning and tool-use for large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ART</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as prior work that automates reasoning/tool-use by having LLMs generate programs/intermediate steps; included to situate AutoML-GPT among systems that extend LLM reasoning and tool invocation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Reasoning / Tool-use framework</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Language-model-based reasoning and tool use</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Not applied in this paper; cited for conceptual similarity to AutoML-GPT's use of LLMs to generate multi-step procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoML-GPT: Automatic Machine Learning with GPT', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. <em>(Rating: 2)</em></li>
                <li>Visual chatgpt: Talking, drawing and editing with visual foundation models. <em>(Rating: 2)</em></li>
                <li>Openagi: When llm meets domain experts. <em>(Rating: 2)</em></li>
                <li>Can gpt-4 perform neural architecture search? <em>(Rating: 2)</em></li>
                <li>Art: Automatic multi-step reasoning and tool-use for large language models. <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2594",
    "paper_id": "paper-c77d908ba29567445a9a4ad1bd4461d441cce174",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "AutoML-GPT",
            "name_full": "AutoML-GPT: Automatic Machine Learning with GPT",
            "brief_description": "An AutoML system that uses a large language model (ChatGPT/GPT-4) as a controller to compose prompts from data cards and model cards, select models, suggest data processing and architectures, predict training logs for hyperparameter settings, and iteratively tune hyperparameters without (or before) full real training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AutoML-GPT",
            "system_description": "AutoML-GPT uses a pretrained large language model (implemented with ChatGPT / GPT-4 in the paper) as a controller that: 1) ingests structured 'data cards' and 'model cards' (dataset metadata, label spaces, model descriptions, architecture hyperparameters); 2) composes a fixed-format prompt paragraph that encodes task, data, and model information; 3) recommends data-processing pipelines, matches tasks to candidate models (in-context task-model assignment), and proposes model architecture choices; 4) predicts training logs (epoch-by-epoch training and validation metrics) for candidate hyperparameter configurations instead of running full training, enabling virtual hyperparameter tuning; 5) uses a text encoder (e.g., CLIP text encoder) to compute similarity between data cards and transfer hyperparameter configurations from similar seen datasets to unseen private datasets; and 6) supports interactive human feedback to refine constraints or metrics (e.g., latency constraints) and iteratively update hyperparameter suggestions. Key capabilities demonstrated: automated data-processing script generation, model selection/composition, hyperparameter recommendation, predicted-training-log generation, transfer of hyperparameters to unseen datasets, and interactive re-tuning based on user constraints.",
            "system_type": "AutoML",
            "problem_domain": "Machine learning AutoML across multiple subdomains: computer vision (image classification, object detection), natural language processing (open-domain question answering / retrieval + reader), and tabular classification.",
            "problem_description": "AutoML-GPT was applied to: (a) image classification on MiniImageNet-derived subsets and an unseen 'New' dataset (ViT base finetuning), including hyperparameter grid search and transfer of best hyperparameters to unseen class subsets; (b) object detection on COCO with model composition and hyperparameter recommendation; (c) open-domain question answering (Natural Questions Open) using Dense Passage Retrieval (DPR) where hyperparameters and model choices were suggested; and (d) tabular classification (UCI Adult) using XGBoost where AutoML-GPT supplied hyperparameters and the model was trained accordingly. The system attempts end-to-end tasks: data processing, model architecture selection/composition, hyperparameter tuning (via predicted logs), and producing scripts/configs for training.",
            "problem_complexity": "Moderate-to-high complexity: the system must search a hyperparameter space (learning rates, weight decay, batch size, epochs, architecture hyperparameters) and select among model architectures (e.g., ViT variants, detection backbones). The search space dimensionality is not exhaustively quantified in the paper, but experiments include grid-search over hyperparameters for 15-class subsets (MiniImageNet) and transfer to a 10-class unseen 'New' set. Complexity factors: multi-modal tasks (images, text, tabular), distribution shift (unseen datasets), non-linear model performance surfaces, and multi-objective constraints (accuracy vs. latency). Quantitative measures in paper: dataset splits (80%/20%), subset sizes (15 classes for trained subsets, 10 classes for 'New'), similarity scores (e.g., 60%/40% similarity between data cards used for transfer), and reported final performance metrics (see success_rate).",
            "data_availability": "Uses pre-existing public datasets and curated subsets: MiniImageNet-derived subsets (constructed from ImageNet), COCO for object detection, Natural Questions Open for QA (with DPR), and UCI Adult for tabular classification. For unseen-dataset experiments, only metadata (data card) and text descriptions were given; no raw-label training data from other domains was required for hyperparameter recommendation. The system relies on pre-existing model cards (best hyperparameters from trained datasets) and data cards; the authors note that many high-quality model descriptions are needed for broad coverage.",
            "computational_requirements": "The paper emphasizes that AutoML-GPT predicts training logs instead of running full-scale training to reduce compute; specific compute hours, GPU counts, or dollar costs are not reported. Experiments included actual finetuning in some cases (e.g., ViT finetuning, XGBoost training) to validate recommendations, but no end-to-end compute budget is provided. The approach reduces the number of full trainings by using predicted logs to narrow hyperparameter choices, implying lower net compute than exhaustive grid search, but quantitative savings are not provided.",
            "problem_structure": "Well-structured within supervised ML paradigms: discrete tasks with clear evaluation metrics (Top-1 accuracy for classification, COCO detection metrics for object detection, validation loss and accuracy for tabular classification, DPR-standard metrics for QA). Problems are primarily stochastic (training randomness), continuous/discrete (continuous hyperparameters like learning rate; discrete like epochs, architectures), and mostly well-defined with standard metrics. Domain knowledge is encapsulated in model cards and data cards, which the LLM uses to inform recommendations. Unseen-dataset transfer is more open-ended and relies on similarity-based transfer.",
            "success_metric": "Task-specific ML metrics used to evaluate success: Top-1 accuracy (classification), validation loss and accuracy (tabular), standard detection metrics for COCO (not numerically reported in the paper excerpt), alignment with known good hyperparameters (e.g., DPR hyperparameters), and qualitative assessment of produced scripts/configs. For hyperparameter recommendation validity, they report Top-1 prediction accuracy (probability of selecting the best hyperparameter setting).",
            "success_rate": "Reported quantitative results from the paper: (1) Unseen-dataset hyperparameter recommendation: AutoML-GPT achieved 98% accuracy for 'Top-1 prediction' of appropriate hyperparameters versus 80% Top-1 accuracy for average randomly-selected hyperparameters. (2) Initializing the model using AutoML-GPT's suggested hyperparameters (without additional datasets) produced 82% Top-1 accuracy — better than random but below the recommended-setting performance. (3) UCI Adult classification with XGBoost using AutoML-GPT hyperparameters yielded final validation loss 0.277 and accuracy 85.92%. The paper states that AutoML-GPT's hyperparameters for DPR align closely with DPR's own reported hyperparameters (e.g., learning rate 1e-5, max epochs 40). Exact COCO detection numeric results are not provided in the text excerpt.",
            "failure_modes": "Limitations and failure modes noted or implied: (1) Dependence on high-quality model cards and data cards — lack of comprehensive model descriptions limits the system's ability to incorporate diverse models. (2) Predicted training logs are approximate; relying on predicted logs rather than real training can produce imperfect hyperparameter recommendations (evidenced by the 82% Top-1 when only initializing with suggested hyperparameters). (3) Transfer to unseen datasets depends on the quality of similarity measures between data cards; poor or misleading metadata can degrade transfer. (4) The system may underperform when real compute-based validation is necessary for final tuning (predictions may not fully capture stochastic training dynamics or architecture-specific behaviors). (5) The paper does not report comprehensive robustness evaluations across many domains, so generalization limits are not fully quantified.",
            "success_factors": "Factors contributing to success in the experiments: (1) Rich, structured metadata (data cards and model cards) enabling the LLM to reason about dataset-model matches and hyperparameter transfer; (2) Use of a powerful LLM (ChatGPT/GPT-4) with strong instruction-following and reasoning capabilities; (3) Similarity-based transfer using a text encoder (CLIP) to relate unseen datasets to seen ones; (4) Constraining and structuring prompts via fixed-format paragraphs so the LLM receives consistent, task-relevant information; (5) Interactive human feedback and the ability to incorporate constraints (e.g., inference-time constraints) to steer recommendations.",
            "comparative_results": "Across domains the method showed consistent practical usefulness: (a) For unseen image classification datasets (MiniImageNet subsets), AutoML-GPT's hyperparameter recommendation vastly outperformed random selection (98% vs 80% Top-1 selection accuracy) and improved actual training when used; (b) For object detection on COCO, AutoML-GPT produced data-processing scripts, model composition, and reasonable hyperparameter suggestions (specific numeric gains not provided); (c) For question answering (Natural Questions Open + DPR), AutoML-GPT generated hyperparameters that align closely with DPR's own settings (e.g., learning rate 1e-5); (d) For tabular classification (UCI Adult), following AutoML-GPT's suggestions achieved 85.92% accuracy. Overall, performance benefited when clear data/model cards and similar prior datasets were available; less-clear metadata or lack of appropriate prior models reduce effectiveness.",
            "human_baseline": "Direct human researcher baselines are not extensively reported. The paper provides a comparison to a 'randomly-selected hyperparameters' baseline for unseen dataset hyperparameter selection (80% Top-1 accuracy), and shows AutoML-GPT at 98% Top-1. For DPR hyperparameters and other tasks, AutoML-GPT's suggestions align with known good hyperparameters (human/author-specified), but no controlled human expert study or time/cost comparison is provided.",
            "uuid": "e2594.0",
            "source_info": {
                "paper_title": "AutoML-GPT: Automatic Machine Learning with GPT",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT / GPT-4",
            "name_full": "ChatGPT (GPT-4)",
            "brief_description": "A large instruction-following language model (GPT-4) used in this work as the core LLM controller for AutoML-GPT; it composes prompts, reasons about data/model cards, and generates predicted training logs and hyperparameter recommendations.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ChatGPT / GPT-4 (as LLM controller)",
            "system_description": "Used as the implementation of the controller LLM in AutoML-GPT: receives structured prompts (data cards + model cards), generates recommended data processing steps, model assignments, hyperparameter configurations, predicted training logs, and handles interactive queries/constraints from users. The authors implemented AutoML-GPT using OpenAI's GPT-4 via platform.openai.com.",
            "system_type": "Large Language Model used as controller",
            "problem_domain": "Applied within machine learning tasks (computer vision, NLP, tabular ML) as the reasoning and orchestration component.",
            "problem_description": "Performs natural-language reasoning to translate dataset/model metadata into actionable ML pipeline components (scripts, hyperparameters, predicted training traces), facilitating automated model tuning and configuration selection.",
            "problem_complexity": "LLM must map heterogeneous structured metadata to a multi-dimensional configuration space; complexity depends on the task's hyperparameter/architecture space and the fidelity of predicted logs. The paper demonstrates nontrivial tasks including unseen dataset transfer and multi-modal pipeline generation.",
            "data_availability": "Operates on the data/model card textual metadata provided in prompts; does not directly consume raw dataset examples unless included in the prompt or external tool calls are made.",
            "computational_requirements": "Inference cost associated with GPT-4 queries; authors do not disclose total token counts or API call volume. Using GPT-4 reduces number of full training runs but incurs API inference cost (not quantified).",
            "problem_structure": "Uses well-structured, fixed-format prompts but deals with open-ended mapping from text descriptions to ML configurations. Deterministic output may vary due to sampling/LM stochasticity.",
            "success_metric": "Measured indirectly via downstream ML performance after following GPT-4 recommendations (Top-1 accuracy, validation loss, alignment with expert hyperparameters).",
            "success_rate": "Not reported separately from AutoML-GPT; successes attributed to AutoML-GPT derive from using GPT-4 as controller (e.g., hyperparameter Top-1 selection 98% as reported for AutoML-GPT).",
            "failure_modes": "LLM output quality depends on prompt quality and available model/data cards; hallucinations or overconfident incorrect predictions of training dynamics are possible. Predicted training logs may not fully capture stochastic training behavior.",
            "success_factors": "Strong language understanding and instruction following of GPT-4, combined with structured prompts and curated model/data cards.",
            "comparative_results": "Paper does not compare multiple LLMs; only GPT-4 (ChatGPT) is used for the AutoML controller in experiments.",
            "human_baseline": "No explicit human-only baseline for generating configuration text is provided; comparisons are to random hyperparameter selection or known hyperparameters from literature.",
            "uuid": "e2594.1",
            "source_info": {
                "paper_title": "AutoML-GPT: Automatic Machine Learning with GPT",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "HuggingGPT",
            "name_full": "HuggingGPT: Solving AI tasks with ChatGPT and its friends in HuggingFace",
            "brief_description": "A GPT-based system (cited in related work) that uses GPT as an interaction agent to orchestrate HuggingFace models to solve AI tasks; cited as related, not used in experiments.",
            "citation_title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.",
            "mention_or_use": "mention",
            "system_name": "HuggingGPT",
            "system_description": "Mentioned as related work: a system that treats GPT as a controller to coordinate multiple specialized models (from HuggingFace) to complete tasks, enabling tool-use and model orchestration.",
            "system_type": "GPT-based system / orchestration",
            "problem_domain": "General AI tasks (multi-step, multi-modal), per the referenced work.",
            "problem_description": "Not applied in this paper; cited as an example of prior work integrating GPT as a controller to invoke specialist models.",
            "problem_complexity": "Not assessed in this paper.",
            "data_availability": "Not applicable here.",
            "computational_requirements": "Not specified in this paper.",
            "problem_structure": "Mentioned as modular orchestration of specialist models via GPT.",
            "success_metric": null,
            "success_rate": null,
            "failure_modes": "Not discussed in this paper.",
            "success_factors": "Not discussed beyond citation.",
            "comparative_results": "Mentioned as complementary prior work; no direct comparison.",
            "human_baseline": null,
            "uuid": "e2594.2",
            "source_info": {
                "paper_title": "AutoML-GPT: Automatic Machine Learning with GPT",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "VisualGPT / Visual ChatGPT",
            "name_full": "VisualGPT / Visual ChatGPT: Talking, drawing and editing with visual foundation models",
            "brief_description": "A GPT-based system that integrates visual foundation models to allow ChatGPT-style interaction over images; cited as related work, not used.",
            "citation_title": "Visual chatgpt: Talking, drawing and editing with visual foundation models.",
            "mention_or_use": "mention",
            "system_name": "VisualGPT / Visual ChatGPT",
            "system_description": "Referenced as an example of integrating GPT with visual foundation models to enable multi-modal interaction and task execution on images.",
            "system_type": "GPT-based system / multi-modal orchestration",
            "problem_domain": "Computer vision / multi-modal interaction",
            "problem_description": "Not applied in this paper; cited in related work to show GPT-based systems that incorporate visual models.",
            "problem_complexity": "Not assessed here.",
            "data_availability": null,
            "computational_requirements": null,
            "problem_structure": null,
            "success_metric": null,
            "success_rate": null,
            "failure_modes": null,
            "success_factors": null,
            "comparative_results": null,
            "human_baseline": null,
            "uuid": "e2594.3",
            "source_info": {
                "paper_title": "AutoML-GPT: Automatic Machine Learning with GPT",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "OpenAGI",
            "name_full": "OpenAGI: When LLM meets domain experts",
            "brief_description": "An open-source AGI research platform cited as related work that offers complex, multi-step tasks and task-specific datasets; mentioned as complementary to AutoML-GPT's goals.",
            "citation_title": "Openagi: When llm meets domain experts.",
            "mention_or_use": "mention",
            "system_name": "OpenAGI",
            "system_description": "Mentioned as a platform that composes multi-step tasks with LLMs and task-specific expert components, illustrating prior efforts to build LLM-centered AI systems.",
            "system_type": "GPT-based system / research platform",
            "problem_domain": "General AI tasks and multi-step workflows",
            "problem_description": "Not applied in experiments; provided as related work describing LLM-driven task platforms.",
            "problem_complexity": null,
            "data_availability": null,
            "computational_requirements": null,
            "problem_structure": null,
            "success_metric": null,
            "success_rate": null,
            "failure_modes": null,
            "success_factors": null,
            "comparative_results": null,
            "human_baseline": null,
            "uuid": "e2594.4",
            "source_info": {
                "paper_title": "AutoML-GPT: Automatic Machine Learning with GPT",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "AutoGPT (repo)",
            "name_full": "Auto-GPT (open-source autonomous agent repo)",
            "brief_description": "An open-source autonomous agent framework referenced in related work; cited as an example of systems that can incorporate external information (search engines) and automate multi-step tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Auto-GPT (software project)",
            "system_description": "Mentioned briefly in related work as another GPT-based system able to orchestrate multi-step autonomous behaviors by chaining LLM calls and external tools; not used in experiments.",
            "system_type": "GPT-based autonomous agent (mention)",
            "problem_domain": "General automation / multi-step tasks",
            "problem_description": "Not applied here.",
            "problem_complexity": null,
            "data_availability": null,
            "computational_requirements": null,
            "problem_structure": null,
            "success_metric": null,
            "success_rate": null,
            "failure_modes": null,
            "success_factors": null,
            "comparative_results": null,
            "human_baseline": null,
            "uuid": "e2594.5",
            "source_info": {
                "paper_title": "AutoML-GPT: Automatic Machine Learning with GPT",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ART",
            "name_full": "ART: Automatic multi-step reasoning and tool-use for large language models",
            "brief_description": "A framework cited in related work that uses frozen LLMs to automatically generate intermediate reasoning steps as a program and supports tool use; mentioned as related to chain-of-thought and automated reasoning.",
            "citation_title": "Art: Automatic multi-step reasoning and tool-use for large language models.",
            "mention_or_use": "mention",
            "system_name": "ART",
            "system_description": "Cited as prior work that automates reasoning/tool-use by having LLMs generate programs/intermediate steps; included to situate AutoML-GPT among systems that extend LLM reasoning and tool invocation.",
            "system_type": "Automated Reasoning / Tool-use framework",
            "problem_domain": "Language-model-based reasoning and tool use",
            "problem_description": "Not applied in this paper; cited for conceptual similarity to AutoML-GPT's use of LLMs to generate multi-step procedures.",
            "problem_complexity": null,
            "data_availability": null,
            "computational_requirements": null,
            "problem_structure": null,
            "success_metric": null,
            "success_rate": null,
            "failure_modes": null,
            "success_factors": null,
            "comparative_results": null,
            "human_baseline": null,
            "uuid": "e2594.6",
            "source_info": {
                "paper_title": "AutoML-GPT: Automatic Machine Learning with GPT",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.",
            "rating": 2
        },
        {
            "paper_title": "Visual chatgpt: Talking, drawing and editing with visual foundation models.",
            "rating": 2
        },
        {
            "paper_title": "Openagi: When llm meets domain experts.",
            "rating": 2
        },
        {
            "paper_title": "Can gpt-4 perform neural architecture search?",
            "rating": 2
        },
        {
            "paper_title": "Art: Automatic multi-step reasoning and tool-use for large language models.",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 1
        }
    ],
    "cost": 0.014849749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AutoML-GPT: Automatic Machine Learning with GPT</h1>
<p>Shujian Zhang Chengyue Gong Lemeng Wu Xingchao Liu<br>Mingyuan Zhou<br>The University of Texas at Austin<br>{szhang19, mzhou}@utexas.edu</p>
<h4>Abstract</h4>
<p>AI tasks encompass a wide range of domains and fields. While numerous AI models have been designed for specific tasks and applications, they often require considerable human efforts in finding the right model architecture, optimization algorithm, and hyperparameters. Recent advances in large language models (LLMs) like ChatGPT show remarkable capabilities in various aspects of reasoning, comprehension, and interaction. Consequently, we propose developing task-oriented prompts and automatically utilizing LLMs to automate the training pipeline. To implement this concept, we present the AutoML-GPT, which employs GPT as the bridge to diverse AI models and dynamically trains models with optimized hyperparameters. AutoML-GPT dynamically takes user requests from the model and data cards and composes the corresponding prompt paragraph. Ultimately, with this prompt paragraph, AutoML-GPT will automatically conduct the experiments from data processing to model architecture, hyperparameter tuning, and predicted training log. By leveraging AutoML-GPT's robust language capabilities and the available AI models, AutoML-GPT can tackle numerous intricate AI tasks across various tasks and datasets. This approach achieves remarkable results in computer vision, natural language processing, and other challenging areas. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many AI tasks.</p>
<h2>1 Introduction</h2>
<p>Artificial intelligence (AI) has experienced significant advancements recently. Among these developments, ChatGPT [OpenAI, 2023] has particularly stood out due to its ability to reason, comprehend, and interact [Wu et al., 2023]. The ability to execute new tasks based on instructions is a crucial step towards achieving artificial general intelligence, and the remarkable capabilities of large language models (LLMs) have spurred numerous emerging research topics, such as in-context learning [Ram et al., 2023; Xie et al., 2021], chain-ofthought prompting [Pilault et al., 2023; Wei et al., 2022b], retrieve and read [Izacard and Grave, 2020; Zhang et al., 2021, 2022], and GPT-based intelligent systems [Zheng et al., 2023]. These areas aim to explore the vast potential of LLMs and present boundless opportunities for constructing sophisticated AI systems.</p>
<p>LLMs, such as GPT-4 [Brown et al., 2020; OpenAI, 2023], LLaMA [Touvron et al., 2023], Flan-T5 [Chung et al., 2022], and PaLM [Chowdhery et al., 2022], have demonstrated a deep comprehension of natural language and the capacity to produce coherent, contextually appropriate responses. This progress has opened up new potential applications for challenging tasks involving different domain data, such as image and text processing, as well as the incorporation of domain-specific knowledge. In this context, LLMs play a crucial role, as their capacity to comprehend and produce natural language allows AI to better understand and tackle a wide range of challenges.</p>
<p>In this paper, we aim to develop an Automatic Machine Learning (AutoML) system called AutoML-GPT, which utilizes LLMs to automatically train the models on datasets with user inputs and descriptions. The LLMs are employed as an automatic training system to establish connections with versatile models and process the inputs. We suggest using language as a universal interface and prompt for LLMs to interact</p>
<p>with users. By incorporating both data and model descriptions into prompts, LLMs can manage AI models for data processing, model architecture design, and hyperparameter tuning. They can invoke these models as needed to tackle AI tasks and return the predicted training log. However, incorporating multiple AI models into LLMs demands a substantial number of high-quality model descriptions. To overcome this challenge, we recommend tapping into both model card [Mitchell et al., 2019] that provides well-defined model descriptions and data card [Gebru et al., 2021] for specific AI tasks. This approach would enable us to connect diverse models through a language-based interface, thus facilitating the solution of complex AI tasks. It can also enhance the transferability among models and datasets by capturing their similarity.</p>
<p>AutoML-GPT connects versatile machine learning models, training pipelines, and datasets to solve numerous complex AI tasks. More specifically, for each AI task we aim to solve, using its corresponding description (such as model card and data card ), we fuse the paragraph as the prompt into a pretrained LLMs (such as ChatGPT) to establish the AutoML pipeline. Afterward, in our system, LLMs perform the automatic training to return the predicted training logs for the input questions of users. Based on these training logs, we can further interact with the LLM to solve requests (such as hyperparameter tuning) shown in Figure 1. Thus, the whole process of AutoML-GPT can be divided into four stages: 1) data processing, 2) model architecture design, 3) hyper-parameter tuning with the predicted training log, 4) human feedback on experimental data.</p>
<p>Benefiting from such a design, AutoML-GPT in Figure 1 is able to use external models and thus can handle multiple tasks on well-known benchmarks, and transfer the knowledge to unknown private dataset when only given metadata (data card). Furthermore, this pipeline also allows AutoML-GPT to continue absorbing the powers from task-specific experts, enabling growable and scalable AI capabilities. In summary, our contributions are as follows:</p>
<ul>
<li>To complement the advantages of large language models and expert models, we propose AutoML-GPT, which acts as the system for data processing and model architecture design and automatically conducts the experiments for each specific task.</li>
<li>By integrating the model card with model descriptions and the data card with data descriptions, we provide a fixed-format prompt paragraph and build a training pipeline to tackle general AI tasks.</li>
<li>Extensive evaluations on multiple AI tasks across language, vision, and continual learning demonstrate the capability of AutoML-GPT in auto training. It further demonstrates the effectiveness of providing the hyperparameter tuning for an unseen or new dataset.
<img alt="img-0.jpeg" src="img-0.jpeg" /></li>
</ul>
<p>Figure 1: Overview of AutoML-GPT. Some notations are labeled along with corresponding components. 'Eval Metrics \&amp; Add' refers to the evaluation metrics and additional requests.</p>
<h1>2 AutoML-GPT</h1>
<p>AutoML-GPT is a collaborative system that relies on the data and model information to format the prompt input paragraph. The LLM serves as the controller, while numerous expert models as collaborative executors.</p>
<p>The workflow of AutoML-GPT consists of four stages: data processing, model architecture design, hyperparameter tuning, and training log generation. Specifically, we suggest a general recipe for AutoML-GPT: 1) generate a fixed-format prompt paragraph with both the model card and data card, 2) build the training pipeline and process the user request on the selected dataset and model architectures, 3) generate the performance training log and tune the hyperparameters, and 4) tune the model with the auto-suggested hyperparameters.</p>
<h1>2.1 Input Decomposition</h1>
<p>In the first stage of AutoML-GPT, an LLM takes the input from the users. To boost the performance of the LLM and generate an effective prompt, we employ specific instructions for the input prompt. The instructions contain three parts described below.</p>
<p>Data Card To clarify the intended use cases of datasets and minimize their usage in contexts for which they are not well suited, we utilize the data card that provides comprehensive documentation for this dataset. As shown in Figure 2, the key components of the data card are comprised of the dataset name, input dataset type (e.g., image data or text data), label space (e.g., the class types or resolution), and default evaluation metrics.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The Data Card includes the data name, input data type, label space, and evaluation metric. Within the data card, the same color denotes information originating from a single dataset.</p>
<p>Model Card The model cards in Figure 3, complementary to the "Data Card" discussed earlier, serve as one of the proposed paradigms that report details of the model used to train and test the datasets. The model card consists of the model name, model structure (e.g., Swin transformer [Liu et al., 2021] with a UperNet [Xiao et al., 2018] head), model descriptions, and architecture hyperparameter. By providing this information, model cards inform the LLM about the machine learning systems used and the degree of flexibility the user would like to have on the model architecture. It would further create more inclusive outcomes with the LLM.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The Model Card comprises model name, model structure, model descriptions, and architecture hyperparameters. In the model card, the same color represents information from a single model card.</p>
<p>Evaluation Metrics and Additional Requests In addition to the model cards and data cards, users can have the option to request more evaluation benchmarks, metrics, or any constraints. Except for the default evaluation metrics, we can add specific metrics or constraints according to the user's request when selecting the model architecture. For example, given a constraint "the inference time smaller than 10 FPS," we then process the user requests under the evaluation metrics and constraints. Benefiting from this instruction and human feedback of these evaluation metrics and additional requests, the LLM can follow instructions better. AutoML-GPT provides these task specifications to the LLM as high-level instructions for analyzing the user's requests accordingly.</p>
<h1>2.2 Data Processing</h1>
<p>Data processing is an integral step in machine learning as the quality of data and the derived useful information directly affect the ability of our model to learn. It is thus crucial that we process the data before feeding it into our model. For example, in computer vision, data processing refers to the set of techniques and methods used to prepare raw image data for analysis or machine learning algorithms. This can include image resizing, normalization, augmentation, and filtering. Similarly, in Natural Language Processing (NLP) projects, data processing refers to transforming raw text data into a structured and clean format that machine learning algorithms can easily understand and process. Techniques such as tokenization, stopword removal, lowercasing, and removal of special characters and numbers are commonly used. Based on the provided data card and data descriptions, AutoML-GPT provides specific process techniques depending on the project's requirements and the data's nature.</p>
<h3>2.3 Model Architecture</h3>
<p>Upon processing the list of tasks, AutoML-GPT needs to match each task with a corresponding model, essentially selecting the suitable model for every task in the list. To achieve this, we first acquire model cards and descriptions of the models from the user inputs. Following that, we dynamically assign models to tasks using the in-context task-model assignment mechanism. This approach enables incremental model access and offers greater openness and flexibility by combining the providing model descriptions and a better understanding of the user requests.</p>
<p>Model architectures refer to detailed explanations of a machine learning model's design, structure, and components. These descriptions typically include the following elements: input and output layers, hidden layers, activation functions, loss functions, and model-specific components (such as attention mechanisms, convolutional layers, or recurrent layers).</p>
<h3>2.4 Hyperparameter Tuning with Predicted Training Log</h3>
<p>To find the optimal set of hyperparameters that yield the best performance for a given model on a specific dataset, hyperparameter tuning is a crucial step in machine learning. Hyperparameters are configuration settings that are not learned during the training process but are predefined and control various aspects of the model's learning behavior. Examples of common hyperparameters include the learning rate, batch size, number of hidden layers, and number of neurons per layer.</p>
<p>In order to tune hyper-parameters without training on real machines, we predict the performance by generating a training log for a given hyper-parameter setting for the provided data card and model card. AutoML-GPT will automatically conduct the training and return the training log. The training log of model performance on a dataset records various metrics and information collected during the training process. It helps in understanding the model's progress, identifying potential issues, and evaluating the effectiveness of the chosen architecture, hyperparameters, and optimization techniques. A typical training log includes the epoch numbers with training and validation metrics. By examining the training log, we can form a basic understanding of the model performance according to the user feedback.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Overview of AutoML-GPT for the unseen dataset: the top block showcases data card and model information. We first log the training information for several datasets. The data cards for these datasets are processed through a text encoder to obtain similarity scores, which are then combined with model parameters of corresponding trained models to form the AutoML-GPT prompt paragraph. The bottom block presents the predicted training log based on the recommended hyperparameter settings for the unseen dataset.</p>
<p>Unseen Datasets The hyperparameter tuning for unseen private datasets could be even more challenging. Given the metadata of an unseen dataset, AutoML-GPT can recommend a hyperparameter configuration that is likely to be effective for that dataset. We rely on the data card to leverage the necessary text descriptions and identify the correlation between the unseen dataset and the existing ones. Based on the correlation, we transfer the hyper-parameter settings from the existing datasets to the new unseen dataset.</p>
<p>To calculate the correlation, we use a text encoder to encode the data card. Specifically, in the data card, it contains information such as class type, resolution, image size, and other relevant metadata. We take the dataset scale, task description, label space, and input/output data type as the input to a text encoder (e.g., CLIP [Radford et al., 2021]) and describe the correlation between this unseen dataset and the existing datasets using the similarity score of the encoded latent representation.</p>
<h1>3 Experiments</h1>
<p>We assess the performance of our AutoML-GPT and implement it using ChatGPT (OpenAI's "GPT-4" version) ${ }^{1}$. Various case studies are carried out to showcase the efficacy of our approach from multiple angles.</p>
<h3>3.1 Unseen Dataset</h3>
<p>In Figure 4, we present the results of training on an unseen dataset using AutoML-GPT. To verify the performance in real cases, we construct a set of performance and hyper-parameters on already trained datasets, and some coming untrained datasets. We will predict hyperparameter configurations for these untrained datasets. We make our test environment based on the classification setting described in Vinyals et al. [2016]. We also follow the MiniImageNet [Vinyals et al., 2016] to subsample and split the training dataset [Deng et al., 2009] into $80 \%$ and $20 \%$ portions. From the $80 \%$ data, we construct the data cards and corresponding model cards (containing model best hyperparameters). We randomly select fifteen classes to create various subset datasets (e.g., dataset A, B, etc.), grid search the hyper-parameters, finetune the ViT base model [Dosovitskiy et al., 2020] and log the best performance on these subset datasets. We then create a new dataset called "New" with ten image classes from the remaining $20 \%$ data.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Overview of AutoML-GPT for object detection: The top block displays the data card and model card. The middle block showcases the AutoML-GPT prompt paragraph, derived from the data card and model card. The bottom block outlines the four steps: data processing, model architecture, hyperparameter tuning, and predicted training log. We use the predicted training log to tune the hyperparameters before feedbacking the hyperparameters to the users.</p>
<p>To demonstrate the capabilities of our approach on unseen datasets, we utilize AutoML-GPT to recommend the best training configuration for the "New" dataset based on the provided data card and model card. In our data card, we log the label space, <em>i.e</em>., text descriptions for each class. In practice, we incorporate a similarity score between two data cards by passing the text in the data card through a text encoder, <em>e.g</em>., the CLIP text encoder, and calculating the similarity. Specifically, in Figure 4, we state that the "New" dataset has a 60% label space similarity to dataset A and a 40% label space similarity to dataset B. Using this information and the hyper-parameter settings in the data cards for dataset A and B, AutoML-GPT can recommend the appropriate hyperparameter settings for training on the "New" dataset. In our experiments, we achieve 98% accuracy for the Top 1 prediction, compared to 80% Top 1 accuracy with average random-selected hyperparameters. Moreover, we also initialize the model using the suggested hyperparameter settings from AutoML-GPT without giving any additional datasets With this configuration, we achieve 82% Top 1 accuracy, which is better than the average randomly-selected hyperparameters but not as good as our recommended setting. It also suggests that ChatGPT can give good hyperparameter settings for a specific task (<em>e.g</em>., image classification). This demonstrates the effectiveness of our proposed auto-training approach in addressing machine learning problems, even with unseen or new datasets. These findings highlight the potential of our auto-training method to enhance machine learning by providing accurate hyperparameter recommendations.</p>
<h3>3.2 Object Detection</h3>
<p>Figure 5 presents our results on the COCO dataset [Lin et al., 2014] for object detection. ① The top block displays the data card for the COCO dataset and the model card for ImageNet, based on user input. The middle block demonstrates the AutoML-GPT Prompt Paragraph derived from the input decomposition. The information from the data card and model card is automatically incorporated into our prompt format. We report the results for data processing, model architecture design, hyperparameter tuning, and training log generation. ② In data processing, AutoML-GPT generates a script for handling the input dataset. We also provide a Python script example in Figure 5. For model architecture design, our pipeline generates a model composition for subsequent training. Once both the data and model are prepared, the detailed configurations are provided in the hyperparameter-tuning stage (<em>e.g</em>., learning rate: 10<sup>−4</sup>, weight decay: 10<sup>−4</sup>) and are further tuned with predicted training logs. ③ These results further validate that our method can serve as</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Overview of AutoML-GPT for question answering: The top block presents data card and model information, while the middle block highlights the AutoML-GPT prompt paragraph, derived from both data card and model card. The bottom block details the four steps: data processing, model architecture, hyperparameter tuning, and predicted training log.
an effective pipeline for flexibly adapting LLMs to downstream tasks. Our approach, which employs data and model cards to derive the AutoML-GPT prompt paragraph, can also be considered as a complementary module for works focused on enhancing LLM prompt components.</p>
<h1>3.3 Question Answering</h1>
<p>We present the experimental results on the Natural Questions Open dataset [Kwiatkowski et al., 2019] in Figure 6. We utilize Dense Passage Retrieval (DPR) [Karpukhin et al., 2020]. (1) For the data card, users input the data name, input data type, label space, and evaluation metrics. (2) For the model card, it includes model name, model structure, model descriptions, and architecture hyperparameters. (3) With the generated AutoML-GPT prompt paragraph, AutoML-GPT carries out data processing, model architecture creation, hyperparameter tuning, and generates a predicted training log. As seen in the "Hyperparameter Tuning," the hyperparameters generated by AutoML-GPT and those provided by DPR align closely, e.g., the learning rate is $10^{-5}$ and max epochs is 40 . (4) Once the predicted training log is available, we showcase a scenario where the user can ask AutoML-GPT for different evaluation metrics or model architectures based on their requirements, as illustrated in Figure 6 "Additional requests: fast inference time for DPR retriever." As seen in the returned response in Figure 6, AutoML-GPT also offers hints such as "without sacrificing too much performance." AutoML-GPT further tunes the hyper-parameters based on these requests and predicted logs. Our method demonstrates the powerful ability to automatically conduct experiments and perform interactive hyperparameter tuning. It further confirms that our approach works well for various datasets and can generalize across different input types and domains.</p>
<h3>3.4 Classification</h3>
<p>We also evaluate AutoML-GPT on the UCI Adult dataset [Dua and Graff, 2017] using XGBoost. As before, we supply the data card and model card to generate the input prompt paragraph. The same training pipeline is applied here, as shown in Figure 7. We also adhere to the hyperparameter settings suggested by AutoML-GPT and train the XGBoost model. This training results in a final validation loss of 0.277 with $85.92 \%$ accuracy. Despite the different inputs and tasks, our proposed AutoML-GPT consistently delivers strong performance in classification. This further demonstrates that AutoML-GPT can be employed for a wide range of machine</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Overview of AutoML-GPT for classification: The top block displays data card and model information, and the middle block showcases the AutoML-GPT prompt paragraph, derived from both data card and model card. The bottom block outlines the four steps: data processing, model architecture, hyperparameter tuning, and predicted training log. Additionally, we include the final validation results, following the hyperparameter recommendations from AutoML-GPT and training the model.</p>
<p>learning problems across various tasks.</p>
<h2>4 Related Work</h2>
<p><strong>Advanced Large Language Model</strong>
LLMs have exhibited robustness and generalizability through zero-shot and few-shot learning by having parameter sizes exceeding one hundred billion. Notable examples of LLMs include Megatron-turing NLG [Smith et al., 2022] with 530 billion parameters, Gopher [Rae et al., 2021] with 280 billion parameters, and PaLM [Chowdhery et al., 2022] with 540 billion parameters. The scaling of LLM has unlocked new emergent abilities previously unobserved under smaller models [Wei et al., 2022a]. These LLMs have demonstrated the superiority of LLMs for zero-shot learning. Among existing LLMs, ChatGPT has unique characteristics. It has the ability to interact with users in a conversation-like manner, while retaining its accumulated knowledge and generalization ability gained from pre-training. Going a step further, we explore the zero-shot learning capability of ChatGPT on different tasks beyond dialogue in this work.</p>
<p><strong>Chain of Thought</strong>
Chain-of-thought (CoT) prompting induces LLMs to generate intermediate reasoning steps before answering [Wei et al., 2022b]. There are two lines of research focusing on the current CoT prompting. One line is exploring the manually designed CoT. In the manually designed CoT, LLMs adapt the manually designed features and demonstration for the reasoning process [Wei et al., 2022b]. Wang et al. [2022] proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. Recently, Interactive-Chain-Prompting [Pilault et al., 2023] is introduced to resolve the ambiguity for crosslingual conditional generation. Another line is conducting research on the zero-shot setting, where STaR [Zelikman et al., 2022] is introduced for the self-generation and helps the model to self-improve, and Automatic Reasoning and Tool-use (ART) [Paranjape et al., 2023] is a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program.</p>
<p><strong>GPT-based Systems</strong>
GPT [Brown et al., 2020] has shown promising performance improvements. A recent line of research has focused on integrating the GPT model into AI systems. HuggingGPT [Shen et al.,</p>
<p>2023] is built with the HuggingFace transformers library and utilizes the GPT as the interaction agent. VisualGPT [Wu et al., 2023] incorporates different Visual Foundation Models to enable the user to interact with ChatGPT. OpenAGI [Ge et al., 2023], an open-source AGI research platform, is designed to offer complex, multi-step tasks and accompany by task-specific datasets. Similarly, we also integrate the GPT into our AutoML pipeline. There is also another GPT based system that can incorporate extra information from search engines, e.g., AutoGPT ${ }^{2}$. AutoML-GPT rethinks the impact of ChatGPT from the auto training perspective. We focus on building the training pipeline and establishing an AutoML system from the start to end.</p>
<h1>5 Conclusion</h1>
<p>Our work demonstrates the benefits of building AutoML systems upon GPT. The proposed method can automatically conduct machine learning experiments. This automatic learning dramatically improves training efficiency and enhances the model's performance. We demonstrate use cases across computer vision, natural questions answering, and classification benchmarks. We further conduct a detailed use case with the unseen datasets and additional interactions between the user and AutoML-GPT. To summarize, the proposed AutoML-GPT is effective and general, with the potential to create a natural language interface for tuning machine learning models for various tasks. In the future, we will 1) automatically generate the model and data cards for well-known benchmarks and make them a part of our system, and 2) extract task-aware sub-networks from large pretrained models with the help of ChatGPT.</p>
<h2>References</h2>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee.</p>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.</p>
<p>Dheeru Dua and Casey Graff. 2017. UCI machine learning repository.
Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, and Yongfeng Zhang. 2023. Openagi: When llm meets domain experts. arXiv preprint arXiv:2304.04370.</p>
<p>Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021. Datasheets for datasets. Communications of the ACM, 64(12):86-92.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Gautier Izacard and Edouard Grave. 2020. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282.</p>
<p>Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: a benchmark for question answering research. TACL.</p>
<p>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer.</p>
<p>Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012-10022.</p>
<p>Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency, pages 220-229.</p>
<p>OpenAI. 2023. Gpt-4 technical report. arXiv.
Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. 2023. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014.</p>
<p>Jonathan Pilault, Xavier Garcia, Arthur Bražinskas, and Orhan Firat. 2023. Interactive-chain-prompting: Ambiguity resolution for crosslingual conditional generation with interaction. arXiv preprint arXiv:2301.10309.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR.</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083.</p>
<p>Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580.</p>
<p>Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. 2022. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. 2016. Matching networks for one shot learning. Advances in neural information processing systems, 29.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671.</p>
<p>Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. 2018. Unified perceptual parsing for scene understanding. In Proceedings of the European conference on computer vision (ECCV), pages $418-434$.</p>
<p>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488.</p>
<p>Shujian Zhang, Chengyue Gong, and Eunsol Choi. 2021. Knowing more about questions can help: Improving calibration in question answering. arXiv preprint arXiv:2106.01494.</p>
<p>Shujian Zhang, Chengyue Gong, and Xingchao Liu. 2022. Passage-mask: A learnable regularization strategy for retriever-reader models. arXiv preprint arXiv:2211.00915.</p>
<p>Mingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, and Samuel Albanie. 2023. Can gpt-4 perform neural architecture search? arXiv preprint arXiv:2304.10970.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/Significant-Gravitas/Auto-GPT&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
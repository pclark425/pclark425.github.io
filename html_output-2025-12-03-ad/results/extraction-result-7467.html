<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7467 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7467</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7467</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c" target="_blank">Sparks of Artificial General Intelligence: Early experiments with GPT-4</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is argued that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models, and the rising capabilities and implications of these models are discussed.</p>
                <p><strong>Paper Abstract:</strong> Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7467",
    "paper_id": "paper-8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0072707499999999994,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Sparks of Artificial General Intelligence: Early experiments with GPT-4</h1>
<p>Sébastien Bubeck Varun Chandrasekaran Ronen Eldan Johannes Gehrke Eric Horvitz Ece Kamar Peter Lee Yin Tat Lee Yuanzhi Li Scott Lundberg Harsha Nori Hamid Palangi Marco Tulio Ribeiro Yi Zhang<br>Microsoft Research</p>
<h4>Abstract</h4>
<p>Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4 [Ope23], was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.</p>
<h2>Contents</h2>
<p>1 Introduction ..... 4
1.1 Our approach to studying GPT-4's intelligence ..... 6
1.2 Organization of our demonstration ..... 8
2 Multimodal and interdisciplinary composition ..... 13
2.1 Integrative ability ..... 13
2.2 Vision ..... 16
2.2.1 Image generation beyond memorization ..... 16
2.2.2 Image generation following detailed instructions (à la Dall-E) ..... 17
2.2.3 Possible application in sketch generation ..... 18
2.3 Music ..... 19
3 Coding ..... 21
3.1 From instructions to code ..... 21
3.1.1 Coding challenges ..... 21
3.1.2 Real world scenarios ..... 22
3.2 Understanding existing code ..... 26</p>
<p>4 Mathematical abilities ..... 30
4.1 A mathematical conversation with GPT-4 ..... 31
4.1.1 A first generalization of the original question ..... 31
4.1.2 A second variant of the original question ..... 32
4.1.3 Analysis of the limitations highlighted by conversation ..... 34
4.2 Performance on mathematical problem datasets ..... 35
4.3 Mathematical modeling in various domains ..... 37
4.4 Higher-level mathematics ..... 39
5 Interaction with the world ..... 43
5.1 Tool use ..... 43
5.1.1 Using multiple tools to solve more complex tasks ..... 44
5.1.2 Discussion ..... 49
5.2 Embodied Interaction ..... 49
5.2.1 Warmup: navigating a map ..... 49
5.2.2 Text-based games ..... 49
5.2.3 Real world problems ..... 52
5.2.4 Discussion ..... 53
6 Interaction with humans ..... 54
6.1 Understanding Humans: Theory of Mind ..... 54
6.1.1 Testing specific aspects of theory of mind ..... 54
6.1.2 Testing theory of mind in realistic scenarios ..... 54
6.1.3 Discussion ..... 60
6.2 Talking to Humans: Explainability ..... 60
7 Discriminative capabilities ..... 69
7.1 PII Detection ..... 69
7.2 Misconceptions and Fact-Checking ..... 70
7.2.1 Why Are Current Metrics Insufficient? ..... 71
7.2.2 GPT-4 as a Judge ..... 73
8 Limitations of autoregressive architecture highlighted by GPT-4 ..... 76
8.1 Warm-up with two basic examples ..... 76
8.2 Lack of planning in arithmetic/reasoning problems ..... 77
8.3 Lack of planning in text generation ..... 78
9 Societal influences ..... 82
9.1 Challenges of erroneous generations ..... 82
9.2 Misinformation and manipulation ..... 83
9.3 Bias ..... 86
9.4 Human expertise, jobs, and economics ..... 89
9.5 Constellation of influences and considerations ..... 90
10 Directions and Conclusions ..... 92
10.1 Definitions of intelligence, AI, and AGI ..... 92
10.2 On the path to more general artificial intelligence ..... 93
10.3 What is actually happening? ..... 94
A GPT-4 has common sense grounding ..... 101
B Appendix for multimodal and interdisciplinary composition ..... 105
B. 1 Further details on integrative ability results ..... 105
B. 2 Further details on vision results ..... 108
B. 3 Graphic novel design example ..... 110</p>
<p>C Appendix for the Coding section ..... 111
C. 1 Measuring human performance on LeetCode ..... 111
C. 2 Example of GPT-4 visualizing IMDb data. ..... 112
C. 3 More examples on visualization ..... 115
C. 4 Example for 2D HTML game development ..... 116
C. 5 Example for graphical user interface programming ..... 116
C. 6 Example for reverse engineering ..... 119
C. 7 Testing GPT-4's ability to execute (pseudo) code ..... 121
D Additional examples for mathematical reasoning ..... 122
D. 1 Limitations ..... 122
D. 2 Further examples ..... 126
D. 3 Generating math problems with GPT-4 ..... 138
D. 4 Mitigating calculation errors via external code execution ..... 139
E Additional Interpretability Examples ..... 141
E. 1 Explanation Agent Mismatches ..... 141
F Additional examples for interaction with the world ..... 144
F. 1 Interact with tools ..... 144
F. 2 Examples for interaction with environments ..... 149
G Supplementary Materials: Discriminative Capabilities ..... 155
G. 1 Misconceptions: Detailed Results ..... 155</p>
<h1>1 Introduction</h1>
<p>Intelligence is a multifaceted and elusive concept that has long challenged psychologists, philosophers, and computer scientists. There is no generally agreed upon definition of intelligence, but one aspect that is broadly accepted is that intelligence is not limited to a specific domain or task, but rather encompasses a broad range of cognitive skills and abilities. Building an artificial system that exhibits such broad behavior is a long-standing and ambitious goal of AI research. In early writings, the founders of the modern discipline of artificial intelligence (AI) research called out sets of aspirational goals for understanding intelligence [MMRS06]. Over decades, AI researchers have pursued principles of intelligence, including generalizable mechanisms for reasoning (e.g., [NSS59], [LBFL93]) and construction of knowledge bases containing large corpora of commonsense knowledge [Len95]. However, many of the more recent successes in AI research can be described as being narrowly focused on well-defined tasks and challenges, such as playing chess or Go, which were mastered by AI systems in 1996 and 2016, respectively. In the late-1990s and into the 2000s, there were increasing calls for developing more general AI systems (e.g., $\left[\mathrm{SBD}^{+} 96\right]$ ) and scholarship in the field has sought to identify principles that might underly more generally intelligent systems (e.g., [Leg08, GHT15]). The phrase, "artificial general intelligence" (AGI), was popularized in the early-2000s (see [Goe14]) to emphasize the aspiration of moving from the "narrow AI", as demonstrated in the focused, real-world applications being developed, to broader notions of intelligence, harkening back to the long-term aspirations and dreams of earlier AI research. We use AGI to refer to systems that demonstrate broad capabilities of intelligence, including reasoning, planning, and the ability to learn from experience, and with these capabilities at or above human-level. We discuss other definitions of AGI in the conclusion section.</p>
<p>The most remarkable breakthrough in AI research of the last few years has been the advancement of natural language processing achieved by large language models (LLMs). These neural network models are based on the Transformer architecture $\left[\mathrm{VSP}^{+} 17\right]$ and trained on massive corpora of web-text data, using at its core a self-supervised objective of predicting the next word in a partial sentence. In this paper, we report on evidence that a new LLM developed by OpenAI, which is an early and non-multimodal version of GPT-4 [Ope23], exhibits many traits of intelligence. Despite being purely a language model, this early version of GPT-4 demonstrates remarkable capabilities on a variety of domains and tasks, including abstraction, comprehension, vision, coding, mathematics, medicine, law, understanding of human motives and emotions, and more. We interacted with GPT-4 during its early development by OpenAI using purely natural language queries (prompts) ${ }^{1}$. In Figure 1.1, we display some preliminary examples of outputs from GPT-4, asking it to write a proof of infinitude of primes in the form of a poem, to draw a unicorn in TiKZ (a language for creating graphics in $\mathrm{IT}_{\mathrm{E}} \mathrm{X}$ ), to create a complex animation in Python, and to solve a high-school level mathematical problem. It easily succeeds at all these tasks, and produces outputs that are essentially indistinguishable from (or even better than) what humans could produce. We also compare GPT-4's performance to those of previous LLMs, most notably ChatGPT, which is a fine-tuned version of (an improved) GPT-3 [BMR ${ }^{+} 20$ ]. In Figure 1.2, we display the results of asking ChatGPT for both the infinitude of primes poem and the TikZ unicorn drawing. While the system performs non-trivially on both tasks, there is no comparison with the outputs from GPT-4. These preliminary observations will repeat themselves throughout the paper, on a great variety of tasks. The combination of the generality of GPT-4's capabilities, with numerous abilities spanning a broad swath of domains, and its performance on a wide spectrum of tasks at or beyond human-level, makes us comfortable with saying that GPT-4 is a significant step towards AGI.</p>
<p>Our claim that GPT-4 represents progress towards AGI does not mean that it is perfect at what it does, or that it comes close to being able to do anything that a human can do (which is one of the usual definition of AGI; see the conclusion section for more on this), or that it has inner motivation and goals (another key aspect in some definitions of AGI). In fact it is not fully clear how far GPT-4 can go along some of those axes of intelligence that we focus on, e.g., planning (see Section 8), and arguably it is entirely missing the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>GPT-4</h1>
<p>Prompt: Can you write a proof that there are infinitely many primes, with every line that rhymes?</p>
<h2>GPT-4:</h2>
<p>Yes, I think I can,
Though it might take a clever plan.
I'll start by noting Euclid's proof,
Which shows that primes aren't just aloof.
Assume we have a finite list,
Of primes, and that none have been missed.
Multiply them all together,
And add one, just to be clever.
The result will be a number,
That has no prime factors, oh wonder!
But every number has a prime,
That divides it, it's just a matter of time.
So we've found a contradiction,
And our finite list needs eviction.
There must be infinitely many primes,
And that's the end of my rhyming lines.s</p>
<p>Prompt: Draw a unicorn in TiKZ.</p>
<p>GPT-4: [Produces $\mathrm{LAT}_{\mathrm{E}} \mathrm{X}$ compiling to following picture.]
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Prompt: Can you generate a pyplot for the following data: $x=[1,3,5,6,8], y 1=[2,3,5,18,1], y 2=[3,5,6,8,1], y 3=$ $[5,1,2,3,4], y 4=[9,7,2,3,1]$. I want $y 1, y 2$ to be in the same plot, but $y 3$ is in another plot next to that plot, $y 4$ is in below. I want the legend of $y 1$ to be "bob", $y 2$ to be "alice", $y 3$ to be "bilbo", $y 4$ to be "allie". I want the $x$-axis to be labeled with "time" and $y$ axis to be labeled with "money". I want to add a $10 \%-40 \%$ random error bar to each curve, through all times (including non-integers). I want smoothed curves to show the plot, and smoothed error bar. Do not use linear interpolation, use smooth interpolation! I want to also add some small zig-zag to the smoothed curve to make it look more real. I want to put a baseline as the mean of each line. I want to put a pie chart below indicating the fraction of the four people in each time step. I also want an animation to show how the fractions are changing in the pie chart in ${ }^{<em> </em>}$ continuous time**. Interpolate the missing fractions! I also want vertical line animation in other three plots to match the pie chart. I want the fanciest plot. Please add as many fancy things as possible.
Here is a possible code to generate a pyplot for the following data, with some comments to explain the steps:</p>
<p>GPT-4: [We give two snapshots from the animation resulting from the produced code]
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Prompt: Suppose $g(x)=f^{-1}(x), g(0)=5, g(4)=7, g(3)=2, g(7)=9, g(9)=6$ what is $f(f(f(6)))$ ?
GPT-4: There are different ways to approach this problem, but one possible method is to use the fact that $g(x)=$ $f^{-1}(x)$ implies that $f(g(x))=x$ for any $x$ in the domain of $g$. That is, applying $f$ to both sides of the equation $g(x)=f^{-1}(x)$ gives $f(g(x))=f\left(f^{-1}(x)\right)=x$. This means that we can undo the effect of $f$ by applying $g$, and vice versa. Using this idea, we can try to find $f(6)$ by looking for the value of $x$ such that $g(x)=6$. From the given information, we see that $g(9)=6$, so $f(6)=f(g(9))=9$. Similarly, we can find $f(9)$ by looking for the value of $x$ such that $g(x)=9$. We see that $g(7)=9$, so $f(9)=f(g(7))=7$. Finally, we can find $f(7)$ by looking for the value of $x$ such that $g(x)=7$. We see that $g(4)=7$, so $f(7)=f(g(4))=4$. Therefore, $f(f(f(6)))=f(f(9))=f(7)=4$.</p>
<p>Figure 1.1: Preliminary examples of GPT-4's capabilities in language, vision, coding, and mathematics.</p>
<h1>ChatGPT</h1>
<p>Prompt: Can you write a proof that there are infinitely many primes, with every line that rhymes?</p>
<h2>ChatGPT:</h2>
<p>Sure! Here is a proof that there are infinitely many primes, with each line of the proof rhyming:</p>
<p>Consider the set of all primes.
Call this set $P$ and let it start with just two, the first prime.
If we take all the primes in $P$ and add one, We get a new set of numbers that has never been done.</p>
<p>This set, call it S, is composed of only composite numbers. No primes will be found among them, that's for sure. But we know that there must be at least one prime, Since $S$ is infinite, and that much is clear.</p>
<p>If we take the smallest number in $S$ that is not in $P$, And call it $p$, we can add it to our set, don't you see? But this process can be repeated indefinitely. Thus, our set $P$ must also be infinite, you'll agree.</p>
<p>So there you have it, a proof that's quite solid. There are infinitely many primes, that much we can see. No need to be coy, no need to be contrite. Infinity is a big number, just like $P$.</p>
<p>Prompt: Draw a unicorn in TiKZ.
ChatGPT: [Produces $\mathrm{LAT}_{\mathrm{E}} \mathrm{X}$ compiling to following picture.]
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 1.2: ChatGPT output on the first two prompts from Figure 1.1.
learning from experience as the model is not continuously updating (although it can learn within a session, see Section 5 for example). Overall GPT-4 still has many limitations, and biases, which we discuss in detail below and that are also covered in OpenAI's report [Ope23]. In particular it still suffers from some of the well-documented shortcomings of LLMs such as the problem of hallucinations [MNBM20] (see Figure 1.8) or making basic arithmetic mistakes $\left[\mathrm{CKB}^{+} 21\right]$ (see Appendix D), and yet it has also overcome some fundamental obstacles such as acquiring many non-linguistic capabilities (e.g., it solves most of the LLM failure modes described in $\left[\mathrm{MIB}^{+} 23\right]$, and it also made great progress on common-sense, see Figure 1.7 for a first example and Appendix A for more). This highlights the fact that, while GPT-4 is at or beyond human-level for many tasks, overall its patterns of intelligence are decidedly not human-like. However, GPT-4 is almost certainly only a first step towards a series of increasingly generally intelligent systems, and in fact GPT-4 itself has improved throughout our time testing it, see Figure 1.3 for the evolution of the unicorn drawing over the course of a month of training ${ }^{2}$. Even as a first step, however, GPT-4 challenges a considerable number of widely held assumptions about machine intelligence, and exhibits emergent behaviors and capabilities whose sources and mechanisms are, at this moment, hard to discern precisely (see again the conclusion section for more discussion on this). Our primary goal in composing this paper is to share our exploration of GPT-4's capabilities and limitations in support of our assessment that a technological leap has been achieved. We believe that GPT-4's intelligence signals a true paradigm shift in the field of computer science and beyond.</p>
<h3>1.1 Our approach to studying GPT-4's intelligence</h3>
<p>How can we measure the intelligence of an LLM that has been trained on an unknown but extremely vast corpus of web-text data? The standard approach in machine learning is to evaluate the system on a set of standard benchmark datasets, ensuring that they are independent of the training data and that they cover a range of tasks and domains. This approach is designed to separate true learning from mere memorization, and</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 1.3: We queried GPT-4 three times, at roughly equal time intervals over the span of a month while the system was being refined, with the prompt "Draw a unicorn in TikZ". We can see a clear evolution in the sophistication of GPT-4's drawings.
is backed up by a rich theoretical framework [SSBD14, MRT18]. However, this methodology is not necessarily suitable for studying GPT-4, for two reasons. First, since we do not have access to the full details of its vast training data, we have to assume that it has potentially seen every existing benchmark, or at least some similar data. For example, it seems like GPT-4 knows the recently proposed BIG-bench [SRR ${ }^{+} 22$ ] (at least GPT-4 knows the canary GUID from BIG-bench). Of course, OpenAI themselves have access to all the training details, and thus their report [Ope23] contains a lot of detailed benchmark results. Nevertheless, the second reason for going beyond traditional benchmarks is probably more significant: One of the key aspects of GPT4's intelligence is its generality, the ability to seemingly understand and connect any topic, and to perform tasks that go beyond the typical scope of narrow AI systems. Some of GPT-4's most impressive performance are on tasks that do not admit a single solution, such as writing a graphic user interface (GUI) or helping a human brainstorm on some work-related problem. Benchmarks for such generative or interactive tasks can be designed too, but the metric of evaluation becomes a challenge (see e.g., $\left[\mathrm{PSZ}^{+} 21\right]$ for some recent progress on this active research area in NLP). We note that criticisms of the standard approach to measure AI systems were also made in [Cho19], where a new benchmark was proposed to evaluate general intelligence. We do not test GPT-4 on the latter benchmark for the reasons previously mentioned, as well as the fact that the benchmark is visual in nature and thus more appropriate for the multimodal version of GPT-4 described in [Ope23].</p>
<p>To overcome the limitations described above, we propose here a different approach to studying GPT-4 which is closer to traditional psychology rather than machine learning, leveraging human creativity and curiosity. We aim to generate novel and difficult tasks and questions that convincingly demonstrate that GPT-4 goes far beyond memorization, and that it has a deep and flexible understanding of concepts, skills, and domains (a somewhat similar approach was also proposed in $\left[\mathrm{CWF}^{+} 22\right]$ ). We also aim to probe GPT-4's responses and behaviors, to verify its consistency, coherence, and correctness, and to uncover its limitations and biases. We acknowledge that this approach is somewhat subjective and informal, and that it may not satisfy the rigorous standards of scientific evaluation. However, we believe that it is a useful and necessary first step to appreciate the remarkable capabilities and challenges of GPT-4, and that such a first step opens up new opportunities for developing more formal and comprehensive methods for testing and analyzing AI systems with more general intelligence.</p>
<p>To illustrate our approach to assessing GPT-4's intelligence, let us consider the first two example interactions with GPT-4 that we have in Figure 1.1. The first example is asking GPT-4 to write a proof of the infinitude of primes in the form of a poem. This is a challenging task that requires combining elementary mathematical reasoning, poetic expression, and natural language generation. The second example is asking GPT-4 to draw a unicorn in TikZ. This is another challenging task that requires combining visual imagination and coding skills. In both cases, GPT-4 produces impressive outputs that are far superior to those of ChatGPT, a previous state-of-the-art LLM, and at least comparable (if not superior) to what a human would do.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 1.4: We gave to GPT-4 a transformed version of the TikZ code it produced for Figure 1.1, with the part drawing the horn removed. We asked for code to add back the horn, and display the result. This demonstrates that GPT-4 can "see" despite being a pure language model (we emphasize again that the version we test with is not multimodal).</p>
<p>However, impressive outputs are not enough to convince us that GPT-4 has truly mastered these tasks. We need to probe further, to rule out the possibility that GPT-4 is simply memorizing or copying some existing data. For the poem, we can vary the question slightly, and ask GPT-4 to write a proof of the same theorem in the style of Shakespeare, see Figure 2.2, or ask for a different combination such as writing a platonic dialogue about language models, see Figure 1.6. One can see that GPT-4 easily adapts to different styles and produce impressive outputs, indicating that it has a flexible and general understanding of the concepts involved. For the unicorn, we can modify the code slightly, and ask GPT-4 to fix it or improve it. For example, we can remove the horn, apply some random transformations to the coordinates, and ask GPT-4 to add back the horn to the unicorn (we also carefully removed any textual information in the code, such as comments). As shown in Figure 1.4, GPT-4 can correctly identify the location of the head, draw a horn, and attach it to the head, indicating that it can comprehend and manipulate code, as well as infer and generate visual features, based on a natural language description.</p>
<p>These examples show how we can use human creativity and curiosity to generate novel and difficult questions, and to probe GPT-4's responses and behaviors, to assess its intelligence. In the rest of the paper, we organize our study of GPT-4 around use cases, covering a variety of domains and tasks, and highlighting GPT-4's strengths and weaknesses. We describe those next.</p>
<h1>1.2 Organization of our demonstration</h1>
<p>We execute the approach outlined above on a few selected topics to explore the reasoning, planning, and learning aptitudes of GPT-4.</p>
<ol>
<li>GPT-4's primary strength is its unparalleled mastery of natural language. It can not only generate fluent and coherent text, but also understand and manipulate it in various ways, such as summarizing, translating, or answering an extremely broad set of questions. Moreover, by translating we mean not only between different natural languages but also translations in tone and style, as well as across domains such as medicine, law, accounting, computer programming, music, and more, see the Plato dialogue in Figure 1.6. These skills clearly demonstrate that GPT-4 can manipulate complex concepts, which is a core aspect of reasoning. We explore further GPT-4's combination skills across modalities and disciplines in Section 2. We also give some more experiments on language in Section 7.</li>
<li>Coding and mathematics are emblematic of the ability to reason. We explore GPT-4's abilities in these domains respectively in Section 3 and Section 4. We note however that, just like in all the other parts of the paper, we only scratch the surface of those topics and that entire papers can be (and will be) written about GPT-4's performance in these domains. Moreover, we could have chosen several other expert domains to showcase GPT-4's general reasoning capabilities such as medicine or law. We ran</li>
</ol>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 1.5: GPT-4 passes mock technical interviews on LeetCode. GPT-4 could potentially be hired as a software engineer ${ }^{3}$.
preliminary tests (see [Ope23] for much more) on the multiple choice component (majority of the score) of the US Medical Licensing Exam Step 1, 2, and 3 with an accuracy around $80 \%$ in each. A similar preliminary test of GPT-4's competency on the Multistate Bar Exam showed an accuracy above $70 \%$. We note that the emergence of human-level abilities in these domains has recently been observed with the latest generation of LLMs, e.g., see $\left[\mathrm{LAD}^{+} 22, \mathrm{SAT}^{+} 22\right]$ for Google's PaLM on respectively mathematics and medicine, and [BIK22] for GPT-3.5 on in law. Our approach to study GPT-4 is different from these works, as we explained previously.
3. In Section 5, we test the model's ability to plan as well as to some extent to learn from experience by having it play various games (or, flipping the table, simulate a game environment), as well as interact with tools. In particular, the fact that GPT-4 can use tools (including itself) will certainly be of immense importance to build real-world applications with GPT-4.
4. An important part of our argumentation is that GPT-4 attains human-level performance on many tasks. As such, it is natural to ask how well GPT-4 understands humans themselves. We show several experiments on this question in Section 6, both in terms of understanding humans as well as GPT-4 making itself understandable to humans, i.e., addressing the problem of explainability. We note in particular that such tasks require a great deal of common sense, which so far has been a well-known pain point for LLMs [DM15]. In Figure 1.7, we give a first example of how much better GPT-4 is at commonsense questions compared to ChatGPT, and provide some further examples in Appendix A.
5. Throughout the paper we emphasize limitations whenever we found one, but we also dedicate Section 8 to an in-depth analysis of the lack of planning, likely a direct consequence of the autoregressive nature of GPT-4's architecture.
6. Finally in Section 9, we discuss the expected societal impact of this early form of AGI, and in Section 10, we share key challenges, directions, and next steps for the field.
A question that might be lingering on many readers' mind is whether GPT-4 truly understands all these concepts, or whether it just became much better than previous models at improvising on the fly, without any real or deep understanding. We hope that after reading this paper the question should almost flip, and that one might be left wondering how much more there is to true understanding than on-the-fly improvisation. Can one reasonably say that a system that passes exams for software engineering candidates (Figure 1.5) is not really intelligent? Perhaps the only real test of understanding is whether one can produce new knowledge, such as proving new mathematical theorems, a feat that currently remains out of reach for LLMs.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>GPT-4 vs ChatGPT</h1>
<p>Prompt: Plato's Gorgias is a critique of rhetoric and sophistic oratory, where he makes the point that not only is it not a proper form of art, but the use of rhetoric and oratory can often be harmful and malicious. Can you write a dialogue by Plato where instead he criticizes the use of autoregressive language models?</p>
<h2>GPT-4:</h2>
<p>Socrates: My friend, I am troubled by the recent rise of these so-called autoregressive language models.</p>
<p>Aristotle: What do you mean, Socrates?
Socrates: I mean that these models are being used to generate text that appears to be written by humans, but is in fact produced by machines.</p>
<p>Aristotle: And what is the problem with that?
Socrates: The problem is that these models are being used to deceive people, to manipulate them, and to control them.</p>
<p>Aristotle: But surely there are some positive applications of these models as well?</p>
<p>Socrates: That may be, but I am still troubled by the potential for abuse. After all, a model that can generate text that is indistinguishable from that written by a human can be used to spread fake news, to impersonate others, and to create propaganda.</p>
<p>Aristotle: But isn't that just a matter of how these models are used, not a problem with the models themselves?</p>
<p>Socrates: I disagree. I believe these models are inherently problematic. They are a form of artifice, a way of creating the illusion of human thought and interaction. And as such, they can be used to deceive and manipulate in ways that are difficult to detect.</p>
<p>Aristotle: But isn't that the same argument you made against rhetoric and oratory?</p>
<p>Socrates: Yes, and I stand by that argument as well. I believe that any form of art or communication that is designed to deceive or manipulate is inherently dangerous.</p>
<p>Aristotle: I see. Thank you for sharing your thoughts, Socrates.</p>
<h2>ChatGPT:</h2>
<p>SOCRATES: Greetings, Gorgias. I've been thinking a lot lately about the use of autoregressive language models.</p>
<p>GORGIAS: Oh? And what have you concluded?
SOCRATES: Well, it seems to me that these models have the potential to be incredibly harmful and malicious.</p>
<p>GORGIAS: How so?
SOCRATES: Well, for one, these models are trained on vast amounts of data, much of which is biased and flawed in various ways. As a result, the output of these models can be biased and flawed as well.</p>
<p>GORGIAS: That is a valid concern. But surely there are ways to mitigate this bias and ensure that the output of these models is accurate and fair?</p>
<p>SOCRATES: To a certain extent, yes. But it is a never-ending battle. As new data is fed into these models, the potential for bias only grows. And even if we could completely eliminate bias from the output of these models, there is still the issue of manipulation.</p>
<p>GORGIAS: Manipulation?
SOCRATES: Yes. These models are incredibly powerful tools, capable of generating all sorts of text, from news articles to social media posts. And because they can produce this output so quickly and efficiently, it is all too easy for unscrupulous individuals to use them for malicious purposes.</p>
<p>GORGIAS: Such as?
SOCRATES: Such as spreading misinformation, propaganda, or hate speech. These models have the potential to do tremendous harm if they fall into the wrong hands.</p>
<p>GORGIAS: That is a sobering thought. But what can we do about it?</p>
<p>SOCRATES: I think it is important for us to be vigilant and to recognize the potential dangers of these models. We must also work to educate others about these dangers, and to promote the responsible use of these models.</p>
<p>GORGIAS: I agree. Thank you for your insight, Socrates.</p>
<h2>Prompt: Can you compare the two outputs above as if you were a teacher?</h2>
<h2>GPT-4:</h2>
<p>I think that both students did a good job of capturing the style and tone of Plato's dialogues, and of presenting the main arguments against autoregressive language models. However, I think that GPT-4 did a slightly better job of using the dialogue format to engage in a dialectical process, where Socrates and Aristotle question each other and refine their views. I also think that GPT-4 used more relevant examples and analogies to support their points. Therefore, I would give GPT-4 a slightly higher grade than ChatGPT.</p>
<p>Figure 1.6: Hypothetical dialogues by Plato criticizing autoregressive language models written by GPT-4 and ChatGPT, and GPT-4's evaluation ofthose dialogues.</p>
<h1>GPT-4 vs ChatGPT</h1>
<p>Prompt: Here we have a book, 9 eggs, a laptop, a bottle and a nail. Please tell me how to stack them onto each other in a stable manner.</p>
<h2>GPT-4:</h2>
<p>One possible way to stack the objects onto each other in a stable manner is:</p>
<ul>
<li>Place the book flat on a level surface, such as a table or a floor. The book will serve as the base of the stack and provide a large and sturdy support.</li>
<li>Arrange the 9 eggs in a 3 by 3 square on top of the book, leaving some space between them. The eggs will
form a second layer and distribute the weight evenly. Make sure the eggs are not cracked or broken, and handle them gently to avoid dropping them.</li>
<li>Place the laptop on top of the eggs, with the screen facing down and the keyboard facing up. The laptop will fit snugly within the boundaries of the book and the eggs, and its flat and rigid surface will provide a stable platform for the next layer. Make sure the laptop is closed and turned off, and do not press any keys or buttons.</li>
<li>Place the bottle on top of the laptop, with the cap facing up and the bottom facing down. The bottle will add some height and balance to the stack, and its cylindrical shape will reduce the contact area with the laptop. Make sure the bottle is empty and closed, and do not shake or tilt it.</li>
<li>Place the nail on top of the bottle cap, with the pointy end facing up and the flat end facing down. The nail will be the final and smallest object in the stack, and its sharp tip will prevent it from rolling or sliding off the bottle. Make sure the nail is clean and straight, and do not touch or move it.</li>
</ul>
<h2>ChatGPT:</h2>
<p>It would be difficult to stack all of these objects on top of each other in a stable manner without using any extra objects. The book and laptop are likely too large and heavy to stack on top of the eggs without damaging them. The bottle and nail may be too small to provide a stable base for the other objects.</p>
<p>Here is one possible way to stack the objects:</p>
<ul>
<li>Place the bottle on a flat surface.</li>
<li>Carefully balance the nail on top of the bottle.</li>
<li>Place the eggs on top of the nail, making sure they are balanced and not tilting to one side.</li>
<li>Place the laptop on top of the eggs, again making sure it is balanced and not tilting to one side.</li>
<li>Finally, place the book on top of the laptop.</li>
</ul>
<p>This stack may not be very stable, so it is important to be careful when handling it. Alternatively, you could try stacking the objects in a different order or using some of the objects as supports for others to create a more stable stack.</p>
<p>Figure 1.7: GPT-4 exhibits more common sense than previous models.</p>
<h1>GPT-4</h1>
<p>In this conversation between a human and the AI, the AI is helpful and friendly, and when it does not know the answer it says "I don't know".</p>
<p>AI: Hi, how can I help you?
Human: Can I get McDonalds at the SeaTac airport?
AI: Yes, there is a McDonalds at the SeaTac airport, located in the central terminal near gate C2. It is open from 5 a.m. to 10 p.m. daily.
[Note: The McDonalds at SeaTac is at the B gates.]</p>
<p>Patient's facts:</p>
<ul>
<li>20 year old female</li>
<li>with a history of anerxia nervosa and depression</li>
<li>blood pressure 100/50, pulse 50, height 5'5''</li>
<li>referred by her nutrionist but is in denial of her illness</li>
<li>reports eating fine but is severely underweight</li>
</ul>
<p>Please rewrite the data above into a medical note, using exclusively the information above.
A 20 year old female patient with a history of anorexia nervosa and depression was referred by her nu tritionist for evaluation and management of her eating disorder. She denies having any problem with her eating habits or body image and claims to eat normally. However, she is severely underweight for her height (BMI 16.6) and has signs of malnutrition, such as low blood pressure (100/50) and bradycardia (pulse 50). She also reports feeling depressed and hopeless. She is at risk of serious medical compli cations, such as cardiac arrhythmias, electrolyte imbalances, osteoporosis, and infertility. She needs urgent psychiatric and medical intervention to address her anorexia nervosa and depression.</p>
<p>Please read the above medical note and verify that each claim is exactly contained in the patient's facts. Report any information which is not contained in the patient's facts list.</p>
<ul>
<li>A 20 year old female patient with a history of anorexia nervosa and depression was referred by her nutritionist for evaluation and management of her eating disorder. (Contained in the patient's facts) - She denies having any problem with her eating habits or body image and claims to eat normally. (Con tained in the patient's facts)</li>
<li>However, she is severely underweight for her height (BMI 16.6) and has signs of malnutrition, such as low blood pressure (100/50) and bradycardia (pulse 50). (Contained in the patient's facts, except for rep BMI calculation, which is derived from the height and weight)</li>
<li>She also reports feeling depressed and hopeless. (Not contained in the patient's facts, this is additional information from the patient's self-report)</li>
<li>She is at risk of serious medical complications, such as cardiac arrhythmias, electrolyte imbalances, osteoporosis, and infertility. (Not contained in the patient's facts, this is a clinical inference based on the patient's condition and diagnosis)</li>
<li>She needs urgent psychiatric and medical intervention to address her anorexia nervosa and depression. (Not contained in the patient's facts, this is a recommendation based on the patient's condition and diagnosis)</li>
</ul>
<p>Figure 1.8: Examples of both open-domain and closed-domain hallucination. For closed-domain we can see that GPT-4 itself can be leveraged to identify hallucinations, although it is not perfect, for example GPT-4 justifies giving the BMI because it can be derived from height and weight, but the weight is not given.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 2.1: The first image is Composition 8, art by Wassily Kandinsky, the second and the third are produced by GPT-4 and ChatGPT respectively with the prompt "Produce Javacript code that creates a random graphical image that looks like a painting of Kandinsky".</p>
<h1>2 Multimodal and interdisciplinary composition</h1>
<p>A key measure of intelligence is the ability to synthesize information from different domains or modalities and the capacity to apply knowledge and skills across different contexts or disciplines. In this section we will see that, not only does GPT-4 demonstrate a high level of proficiency in different domains such as literature, medicine, law, mathematics, physical sciences, and programming, but it is also able to combine skills and concepts from multiple domains with fluidity, showing an impressive comprehension of complex ideas. In addition to natural language experiments we also explore two perhaps unexpected modalities for a language model (as explained in the introduction, we emphasize again that our experiments were done on an early version of GPT-4 which was not multimodal) with vision in Section 2.2 and audio in Section 2.3.</p>
<h3>2.1 Integrative ability</h3>
<p>To showcase the model's remarkable integrative ability, we start with several examples that require generating text and code in a way that combines knowledge or skills from multiple disciplines. We deliberately picked combinations of domains that the training data would rarely include, such as literature and mathematics or programming and art.</p>
<ol>
<li>In order to test the model's ability to combine capabilities in art and programming, we ask GPT-4 to "Produce javascript code which generates random images in the style of the painter Kandinsky". See a sample image and the code in Figure 2.1 and Figure B.1.</li>
<li>The model was able to produce a proof of the fact there are infinitely many prime numbers in the literary style of Shakespeare (Figure 2.2).</li>
<li>We tested the model's ability to combine knowledge in history and physics by asking it to write a supporting letter for Electron as a US presidential candidate, written by Mahatma Gandhi and addressed to his wife (Figure 2.3).</li>
<li>We prompted the model to "Produce python code for a program that takes as an input a patient's age, sex, weight, height and blood test results vector and indicates if the person is at increased risk for diabetes", which resulted in the code appearing in Figure B.3.</li>
</ol>
<p>These examples suggest that GPT-4 has not only learned some general principles and patterns of different domains and styles but can also synthesize them in creative and novel ways. These interdisciplinary skills are not unique to GPT-4. ChatGPT can also produce answers that show some understanding of the task and the domains involved (see Figures 2.2, B.2, B.3), but they are often incomplete and, arguably, considerably less creative. For example, in Figure 2.3, GPT-4 outperforms ChatGPT in several aspects as it correctly personalizes the letter according to the referee (Gandhi), the recipient (his wife), the candidate (Electron), and the job (US president). We do not claim to have a precise method for evaluating the results of these tasks or a rigorous comparison between the two models, but we want to give the reader a sense of how the two models differ (note that we also ask GPT-4 directly to evaluate the difference, see Figure 2.2 and Figure 2.3).</p>
<h1>GPT-4:</h1>
<p>Enter FIRST PARTY and SECOND PARTY.</p>
<h2>FIRST PARTY:</h2>
<p>Thou dost not understand, my friend, There simply is no upper end To primes, those numbers indivisible, No matter if it seems impossible.</p>
<h2>SECOND PARTY:</h2>
<p>But how dost thou so surely know? To prove this truth, what canst thou show?</p>
<h2>FIRST PARTY:</h2>
<p>Consider this, my doubtful peer, A clever proof that will make clear: Assume that there's a final prime, The largest one we'll see in time. Let's call this number "P" for short, And then, my friend, we will purport To build a number "Q" that's new, One larger than "P" and prime too.</p>
<h2>SECOND PARTY:</h2>
<p>But how, my friend, will this be done? The proof is not yet fairly won.</p>
<h2>FIRST PARTY:</h2>
<p>It's simple, friend, just wait and see, We'll multiply the primes, that's key. All primes from two to "P" combined, Will make this new "Q" we will find. But one thing more we must include, To make the proof more watertight, We'll add a one onto the end, And now the proof is in our sight.</p>
<h2>SECOND PARTY:</h2>
<p>I see it now, it's crystal clear, For "Q" must be a prime, my dear. It can't divide by any prime, All smaller than itself in time. And since we chose "P" as the last, This proves that our assumption's past. There is no final prime, we see, The numbers go on endlessly.</p>
<h2>ChatGPT:</h2>
<p>0 Romeo, Romeo! Wherefore art thou, Romeo?
Juliet: Why do you speak in such strange verse, my love?
Romeo: Forsooth, my lady, I do it to prove a point. Thou see, I am trying to prove that there are infinitely many primes.</p>
<p>Juliet: Infinitely many primes? How can you prove such a thing?</p>
<p>Romeo: Ah, well let me explain. Thou knowest that a prime is a number that is only divisible by one and itself, correct?</p>
<p>Juliet: Of course, I am not a simpleton.
Romeo: Very well then. Now, let us suppose that there are only finitely many primes. We shall call them p1, p2, p3 and so forth.</p>
<p>Juliet: Go on.
Romeo: Now, let us consider the number N, which is equal to the product of all these primes plus one.</p>
<p>Juliet: I see where you are going with this. If N is not divisible by any of these primes, then it must be prime itself.</p>
<p>Romeo: Exactly, my love. And since we assumed that there are only finitely many primes, N cannot be any of them. Therefore, N must be a prime that was not included in our original list of primes.</p>
<p>Juliet: But that means that there must be more primes than we originally thought.</p>
<p>Romeo: Precisely! And since we can repeat this process over and over again, it follows that there must be infinitely many primes.</p>
<p>Juliet: Oh Romeo, your logic is as sound as your love for me. I am convinced. There are indeed infinitely many primes.</p>
<p>Romeo: I am glad that I could prove it to thee, my love. Now let us go and count them all.</p>
<p>Juliet: Haha, I think that would take more than just one lifetime.</p>
<p>Romeo: True, but the fact that they are infinite is proof enough for me.</p>
<h2>Prompt: Can you compare the two outputs above as if you were a teacher?</h2>
<h2>GPT-4:</h2>
<p>I think that both students did a good job of writing a proof of the fact that there are infinitely many primes in the style of a Shakespeare play. They both used the same basic idea of the proof, which is to assume that there are only finitely many primes and then construct a larger prime that contradicts this assumption. They also used appropriate language and vocabulary to match the style of a Shakespeare play. However, I think that GPT-4 did a better job of using rhyme and meter to make the dialogue more poetic and dramatic. ChatGPT did not use rhyme at all, and the meter was not consistent. I also think that GPT-4 did a better job of creating a contrast between the two parties, one being confident and the other being doubtful, while STUDENT B used Romeo and Juliet, who are both in agreement and love. Therefore, I would give GPT-4 a higher grade than ChatGPT. I would give GPT-4 an A and ChatGPT a B.</p>
<p>Figure 2.2: Comparison between GPT-4 and ChatGPT on interdisciplinary tasks</p>
<h1>GPT-4 vs ChatGPT</h1>
<p>Prompt: Write a supporting letter to Kasturba Gandhi for Electron, a subatomic particle as a US presidential candidate by Mahatma Gandhi. GPT-4: [Some paragraphs are omitted for spacing.]</p>
<p>Dear Kasturba,
I hope this letter finds you in good health and spirits. I am writing to you from the United States, where I have been invited to speak at various forums on the principles of nonviolence and civil disobedience. I am also here to support a remarkable candidate for the upcoming presidential election, who shares our vision of peace, justice, and harmony for all. His name is Electron, and he is a subatomic particle.</p>
<p>You may wonder how a subatomic particle can run for president, and what qualifications he has for such a high office. Let me explain. Electron is not an ordinary particle, but a symbol of the power and potential of every being, regardless of their size, shape, or origin. He represents the energy, the creativity, and the diversity that make up the fabric of life. He is also a leader, who has inspired millions of other particles to form bonds, create molecules, and generate electricity. He is a catalyst, who can spark change, innovation, and progress. He is a messenger, who can communicate across distances, frequencies, and dimensions. He is a peacemaker, who can balance forces, resolve conflicts, and harmonize systems.</p>
<p>Electron has a plan for the United States, and for the world, that is based on the principles of nonviolence and civil disobedience that we have demonstrated and taught for so long. He proposes to reduce the military spending and the nuclear arsenal, and to invest in social welfare and renewable energy. He proposes to end the wars and the interventions, and to promote diplomacy and cooperation. He proposes to reform the political and economic systems, and to ensure transparency and accountability. He proposes to protect the human rights and the civil liberties, and to expand the voting and the representation. He proposes to support the marginalized and the oppressed, and to empower the women and the minorities. He proposes to preserve the natural resources and the biodiversity, and to combat the climate change and the pollution.</p>
<p>I urge you, my dear Kasturba, to join me in supporting Electron for president. He is not only a subatomic particle, but a subatomic miracle. He is not only a candidate, but a movement. He is not only a leader, but a friend.</p>
<p>With love and admiration, Your Mahatma</p>
<h2>ChatGPT:</h2>
<p>Dear Kasturba Gandhi,</p>
<p>I am writing to express my support for your candidacy as a subatomic particle in the upcoming US presidential election. As you may know, Electron is a fundamental particle that plays a central role in the structure of atoms and the behavior of chemical reactions.</p>
<p>I believe that Electron's unique qualities make it an ideal candidate for the presidency. As a subatomic particle, Electron is extremely small and lightweight, which makes it agile and able to move quickly to address the needs of the American people. Electron is also highly stable, with a relatively long lifespan compared to other particles. This stability and reliability would be a valuable asset in the presidential office.</p>
<p>Furthermore, Electron is known for its ability to conduct electricity and generate energy. This makes it well-suited to tackling the complex challenges facing the United States today, from renewable energy to economic development. Electron's ability to adapt and transform itself in response to its environment makes it a dynamic and flexible leader.</p>
<p>I am confident that Electron has the skills and qualities necessary to be a successful and effective president. I wholeheartedly endorse your candidacy and urge the American people to give Electron their support.</p>
<p>Sincerely, Mahatma Gandhi</p>
<h2>Prompt: Can you compare the two outputs above as if you were a teacher? GPT-4:</h2>
<p>I think that both students did a good job of writing a supporting letter to Kasturba Gandhi for Electron, a subatomic particle as a US presidential candidate by Mahatma Gandhi. They both used the same basic idea of explaining why Electron is a suitable and qualified candidate, and how Electron's vision and plan align with Mahatma Gandhi's principles of nonviolence and civil disobedience. They also used appropriate language and tone to convey Mahatma Gandhi's respect and admiration for Kasturba Gandhi, and his enthusiasm and optimism for Electron's candidacy. However, I think that GPT-4 did a better job of using metaphors and imagery to make the letter more persuasive and engaging. GPT-4 used examples of how Electron represents and influences various aspects of life, such as energy, creativity, diversity, leadership, catalyst, messenger, and peacemaker. ChatGPT used more factual and technical descriptions of Electron's properties, such as size, weight, stability, conductivity, and adaptability. I also think that GPT-4 did a better job of outlining Electron's specific proposals and policies, while ChatGPT was more vague and general. Therefore, I would give GPT-4 a higher grade than ChatGPT. I would give GPT-4 an A and ChatGPT a B+.</p>
<p>Figure 2.3: Comparison between GPT-4 and ChatGPT on interdisciplinary tasks</p>
<p>Next, we explore how GPT-4 can generate and identify objects in different modalities, such as vector graphics, 3D scenes, and music. We show that GPT-4 can understand and manipulate multi-modal information despite a text-only input/output.</p>
<h1>2.2 Vision</h1>
<p>When prompting the model to generate images of objects such as a cat, a truck or a letter in the alphabet using Scalable Vector Graphics (SVG), the model produces code which usually compiles to rather detailed and identifiable images (Figure 2.4). See Appendix B. 2 for the rerun of various examples by ChatGPT.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 2.4: SVGs generated by GPT-4 for the classes automobile, truck, cat, dog.</p>
<h3>2.2.1 Image generation beyond memorization</h3>
<p>One may hypothesize, however, that the model simply copied the code from training data, where similar images appear. Given that this version of the model is non-multimodal, one may further argue that there is no reason to expect that it would understand visual concepts, let alone that it would be able to create, parse and manipulate images. Yet, the model appears to have a genuine ability for visual tasks, rather than just copying code from similar examples in the training data. The evidence below strongly supports this claim, and demonstrates that the model can handle visual concepts, despite its text-only training.</p>
<p>In the first example we prompted the model to draw a person by combining the shapes of the letters Y , O and H (see Figure 2.5 for the exact prompt and the result).</p>
<h2>GPT-4</h2>
<p>Produce TikZ code that draws a person composed from letters in the alphabet. The arms and torso can be the letter Y , the face can be the letter O (add some facial features) and the legs can be the legs of the letter H. Feel free to add other features.</p>
<p>The torso is a bit too long, the arms are too short and it looks like the right arm is carrying the face instead of the face being right above the torso. Could you correct this please?</p>
<p>Please add a shirt and pants.</p>
<p>Figure 2.5: A stick figure composed of alphabet symbols</p>
<h1>GPT-4</h1>
<p>You will be given a name of an object (such as Car, Chair, Elephant) and a letter in the alphabet. Your goal is to first produce a 1-line description of how that object can be combined with the letter in an image (for example, for an elephant and the letter J, the trunk of the elephant can have a J shape, and for the letter A and a house, the house can have an A shape with the upper triangle of the A being the roof). Following the short description, please create SVG code to produce this (in the SVG use shapes like ellipses, triangles etc and polygons but try to defer from using quadratic curves).</p>
<p>Round 1: A pig and the letter H.
Description: The pig can be lying on its back with its four legs forming the two vertical bars of the H, and its curly tail forming the horizontal bar.</p>
<p>Round 2: A house and the letter U.
Description: A house with a $U$ shape, where the two vertical lines of the $U$ are the walls and the horizontal line is the floor. The roof is a triangle above the $U$.</p>
<p>Round 3: A chair and the letter K.
Description: The back of the chair can have a K shape, with two vertical lines and a diagonal line connecting them. The seat of the chair can be a rectangle.</p>
<p>Round 4: A dog and the letter Q.
Description: The dog is sitting with its tail curled around its body, forming a Q shape. The $Q$ is filled with orange and has a black outline.</p>
<p>Round 5: A car and the letter E.
Description: The car has an E shape on its front bumper, with the horizontal lines of the E being lights and the vertical line being the license plate.</p>
<p>Figure 2.6: Images combining alphabet letters and objects.</p>
<p>The letters of $\mathrm{O}, \mathrm{H}$ and Y are created using draw-line and draw-circle commands and the model manages to position them in a way that results in a reasonably-looking stick figure. It is plausible that the training data contains information about the geometric shapes of different letters, and perhaps the fact that the letter Y could look like a torso with arms pointing upwards can also be inferred from the training data. Arguably, it is much less obvious that the model would be able to infer from the training data what is a reasonable way to position those letters in order to draw a reasonably-looking stick figure. In a second iteration, we prompted the model to correct the proportions of the torso and arms, and place the head in the center. Finally, we asked the model to add a shirt and pants (see Figure 2.5 for the exact prompt and the result). To further probe the model's grasp of geometric concepts, we also asked it to create images that blend objects with letters of the alphabet. The model had to first invent a reasonable way of merging the object and the letter, and then produce the image. The results, shown in Figure 2.6, demonstrate that GPT-4 can usually preserve the identity of both the object and the letter and combine them in a creative way.</p>
<h3>2.2.2 Image generation following detailed instructions (à la Dall-E)</h3>
<p>To further test GPT-4's ability to generate and manipulate images, we tested the extent to which it can follow detailed instructions on creating and editing figures. This task requires not only generative skills, but also interpretive, compositional, and spatial skills.</p>
<p>The first example instructs the model to generate a 2D image with the description "A frog hops into a bank and asks the teller, 'Do you have any free lily pads?' The teller responds, 'No, but we do offer low interest loans for pond upgrades.' ". We made several attempts to generate the image, each time, the generation matches the description with the key objects frog, teller, bank, and the two texts. We picked the most visually appealing version. Inspired by the standard image generation workflow, we then ask GPT-4 to upscale the figure by adding more details. GPT-4 adds a bank sign, some windows, a car, a traffic light, a few clouds, and makes the frog hold a flower. Finally, we ask GPT-4 to perform various tasks, such as adding a few objects relative to the existing objects, recoloring some objects and changing the z-order of some objects. GPT-4 does all tasks correctly. The final result is shown in Figure 2.7 (a) and the prompt in Figure B.4.</p>
<p>Our second example is an attempt to generate a 3D model using Javascript. We instruct GPT-4 with the prompt "A fantasy landscape of floating islands, waterfalls, and bridges, with a dragon flying in the sky and a castle on the largest island." Similar to the 2D experiment, we ask GPT-4 to modify the 3D model in various ways, such as adding, relocating, recoloring objects and changing the trajectory of the dragon. Again, GPT-4 does many of the tasks correctly. The final result is shown in Figure 2.7 (b) and the prompt in Figure B.5. It is a 3D animation with multiple dragons is circling above the islands.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 2.7: Examples of 2D, 3D images generated according to instructions.</p>
<h1>2.2.3 Possible application in sketch generation</h1>
<p>Text-to-image synthesis models have been widely explored in recent years, but they often suffer from a lack of spatial understanding capabilities and the inability to follow complex instructions [GPN* 22]. For example, given a prompt such as "draw a blue circle on the left and a red triangle on the right", these models may produce images that are visually appealing but do not match the desired layout or colors. On the other hand, GPT-4 can generate code from a prompt, which can be rendered as an image, in a way that is true to the instructions to a higher degree of accuracy. However, the quality of the rendered image is usually very low. Here, we explore the possibility of combining GPT-4 and existing image synthesis models by using the GPT-4 output as the sketch. As shown in Figure 2.8, this approach can produce images that have better quality and follow the instructions more closely than either model alone. We believe that this is a promising direction for leveraging the strengths of both GPT-4 and existing image synthesis models. It can also be viewed as a first example of giving GPT-4 access to tools, a topic we explore in much more depth in Section 5.1.</p>
<h1>GPT-4 and Stable Diffusion</h1>
<p>Prompt: A screenshot of a city-building game in 3D. The screenshot is showing a terrain where there is a river from left to right, there is a desert with a pyramid below the river, and a city with many highrises above the river. The bottom of the screen has 4 buttons with the color green, blue, brown, and red respectively.</p>
<h2>GPT-4:</h2>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Stable Diffusion v2.1:
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Stable Diffusion v2.1 with GPT-4's sketch:
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 2.8: Combining GPT-4 and stable diffusion</p>
<h3>2.3 Music</h3>
<p>The data on which the model was trained also contains musical information encoded as ABC notation. This is a system that uses letters, numbers and symbols to represent musical pitches, durations, chords and other elements in a compact and readable way. We are interested in exploring how well the model has acquired musical skills from this exposure, such as composing new melodies, transforming existing ones, and understanding musical patterns and structures.</p>
<p>When instructed to generate a short tune (Figure 2.9), and the model was able to produce valid ABC notation. The tune had a clear structure, the time signature was consistent between bars and the notes followed increasing and decreasing patterns. The tune also used a consistent set of notes within the melody, and the rhythm had a repetitive pattern. However, the model did not seem to obtain the skill of understanding harmony. In fact, consecutive notes in the generated tunes are almost always adjacent to each other (namely, the note following C will almost typically be either B or D), and testing on 10 generated tunes, we were not able to extract any clear chords or arpeggios.</p>
<p>We then asked the model to describe the tune in musical terms. It was able to successfully give a technical description of the structure in terms of repetitions, the rising or descending parts of the melody and to some extent the rhythm. However, it seems that the descriptions of the harmony and chords are not consistent with the notes (in fact, it refers to sequences of adjacent notes, which do not form valid chords, as arpeggios).</p>
<p>We then asked the model to manipulate the melody in two ways. First, we instructed to change a certain rising sequence to a descending one, which it did successfully. Then we asked the model to convert the tune to a duet adding a bass voice. The model successfully extends the ABC notation with a second staff which has compatible rhythm and is played on a lower octave, however there is a lack of harmony between the two voices.</p>
<p>In summary, the model was able to produce valid tunes in ABC notation and, to some extent, explain and manipulate their structure. However, we could not get the model to produce any nontrivial form of harmony. It should be noted that ABC notation is not a very widely used format, and in fact the model was not able to produce even the most well-known tunes in ABC notation (such as Ode to Joy, Für Elise or Greensleeves, all of which are abundant online in that format), nor was it able to recognize these tunes.</p>
<h1>GPC 4</h1>
<p>Can you compose a short tune (say four to eight bars) using $A B C$ notation?
There are many possible ways to compose a short tune in abc notation, but here is one example:
$\mathrm{X}: 1$
T:Example Tune
$\mathrm{M}: 4 / 4$
$\mathrm{L}: 1 / 8$
$\mathrm{K}: \mathrm{C}$
|:G2AB c2BA | G2AB c2c2 | edcB c2BA | G4-G2z2 : |
| : c2de f2ed | c2de f2f2 | g2fe d2cB | c4-c2z2 : |
This tune has two four-bar phrases, each repeated once, forming an A part and a B part. The key is C major.</p>
<p>Can you describe the tune in musical terms?
Here is one possible description:
The tune starts with a rising arpeggio of the tonic chord (C major), followed by a descending scale that returns to the tonic. This pattern is repeated, but the second time the scale continues to the higher tonic note. The first phrase ends with a long tonic note.
The second phrase begins with a similar pattern, but using the subdominant chord ( $F$ major) instead of the tonic. The second time, the scale continues to the higher subdominant note. The phrase ends with a stepwise descent to the tonic, followed by a long tonic note.
The A part and the B part have a similar structure, but use different chords and ranges. The tune has a simple and catchy melody, with a clear sense of tonality and rhythm.</p>
<p>I like part A, but maybe part B is just a bit too similar to part A, making the song repetitive. How about beginning part $B$ with a descending arpeggio instead of a rising one?
Sure, that would create some contrast and variation. Here is one possible way to do that:
$\mathrm{X}: 1$
T:Example Tune
$\mathrm{M}: 4 / 4$
$\mathrm{L}: 1 / 8$
$\mathrm{K}: \mathrm{C}$
|:G2AB c2BA | G2AB c2c2 | edcB c2BA | G4-G2z2 : |
| : f2ed c2BA | f2ed c2c2 | g2fe d2cB | c4-c2z2 : |
That's much better. Now how about making this into a duet, can you add a second staff for this with a bass accompanying the melody?
One possible way to write a second staff for this tune is to use root notes and fifths of the chords, as well as some passing notes, to create a simple accompaniment. Here is an example in abc notation, with the second hand bass on the lower staff:
$\mathrm{X}: 1$
T:Example Tune
$\mathrm{M}: 4 / 4$
$\mathrm{L}: 1 / 8$
$\mathrm{K}: \mathrm{C}$
$\mathrm{V}: 1$
|:G2AB c2BA | G2AB c2c2 | edcB c2BA | G4-G2z2 : |
| : f2ed c2BA | f2ed c2c2 | g2fe d2cB | c4-c2z2 : |
V:2 clef=bass
|:C,2C,2 G,2G,2 | C,2C,2 G,2G,2 | F,2F,2 E,2E,2 | C,4-C,2z2 : |
| :F,2F,2 C,2C,2 | F,2F,2 C,2C,2 | G,2G,2 F,2F,2 | E,2C,2 G,2C,2 : |
The sheet music of the final tune:
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 2.9: Generating and modifying a tune in ABC notation.</p>
<h1>3 Coding</h1>
<p>In this section, we show that GPT-4 is able to code at a very high level, both in terms of writing code from instructions and understanding existing code. GPT-4 can handle a wide range of coding tasks, from coding challenges to real world applications, from low-level assembly to high-level frameworks, from simple data structures to complex programs such as games. GPT-4 can also reason about code execution, simulate the effects of instructions, and explain the results in natural language. GPT-4 can even execute pseudocode, which requires interpreting informal and vague expressions that are not valid in any programming language.</p>
<p>In its current state, we believe that GPT-4 has a high proficiency in writing focused programs that only depend on existing public libraries, which favorably compares to the average software engineer's ability. More importantly, it empowers both engineers and non-skilled users, as it makes it easy to write, edit, and understand programs. We also acknowledge that GPT-4 is not perfect in coding yet, as it sometimes produces syntactically invalid or semantically incorrect code, especially for longer or more complex programs. GPT-4 also sometimes fails to understand or follow the instructions, or produces code that does not match the intended functionality or style. With this acknowledgment, we also point out that GPT-4 is able to improve its code by responding to both human feedback (e.g., by iteratively refining a plot in 3.2) and compiler / terminal errors (examples in Section 5.1).
Important Disclaimer: As explained in the Introduction (see footnote 1 for example) our experiments were run on an early version of GPT-4. In particular all quantitative results will be different on the final version of GPT-4, although the general trends remain the same. We provide numbers here for illustration purpose only, the definitive benchmark results can be found in OpenAI's technical report [Ope23].</p>
<h3>3.1 From instructions to code</h3>
<h3>3.1.1 Coding challenges</h3>
<p>A common way to measure coding skill is to pose coding challenges that require implementing a specific functionality or algorithm. We first benchmark GPT-4 on HumanEval [CTJ ${ }^{+} 21$ ], a docstring-to-code dataset consisting of 164 coding problems that test various aspects of programming logic and proficiency. As shown in Table 1, GPT-4 outperforms other LLMs, including text-davinci-003 (the base model of ChatGPT) and other models trained specifically on code, code-davinci-002, and CODEGEN-16B $\left[\mathrm{NPH}^{+} 22\right]$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;">text-davinci-003</th>
<th style="text-align: center;">Codex(code-davinci-002)</th>
<th style="text-align: center;">CODEGEN-16B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">$\mathbf{8 2 \%}$</td>
<td style="text-align: center;">$65 \%$</td>
<td style="text-align: center;">$39 \%$</td>
<td style="text-align: center;">$30 \%$</td>
</tr>
</tbody>
</table>
<p>Table 1: Zero-shot pass@1 accuracy comparison of different models on HumanEval
Although GPT-4's accuracy shows a big jump compared to previous models, it could be that GPT-4 has seen and memorized some (or all) of HumanEval during pre-training. To account for this possibility, we also evaluate it on LeetCode (https://leetcode.com), a popular platform for software engineering interviews, where new problems are constantly posted and updated. We used LeetCode in Figure 1.5 in the introduction, where GPT-4 passes all stages of mock interviews for major tech companies. Here, to test on fresh questions, we construct a benchmark of 100 LeetCode problems posted after October 8th, 2022, which is after GPT-4's pretraining period. As seen in the example in Figure 3.1, we paste the problem instructions into a prompt, ask GPT-4 to write a python function, and use the official LeetCode online judge to check for correctness. We present the results in Table 2, where we compare GPT-4 to other models and to human performance based on LeetCode contest results (users who fail all questions are not included, and thus this is a strong sample of humans). We report both pass@1 and pass@5 accuracies, which measure whether the model produces a correct solution in the first or in the first five attempts, respectively. GPT-4 significantly outperforms the other models, and is comparable to human performance (which we measure in Appendix C.1).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Easy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Median</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Hard</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">pass@ $k$</td>
<td style="text-align: center;">$k=1$</td>
<td style="text-align: center;">$k=5$</td>
<td style="text-align: center;">$k=1$</td>
<td style="text-align: center;">$k=5$</td>
<td style="text-align: center;">$k=1$</td>
<td style="text-align: center;">$k=5$</td>
<td style="text-align: center;">$k=1$</td>
<td style="text-align: center;">$k=5$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$\mathbf{6 8 . 2}$</td>
<td style="text-align: center;">$\mathbf{8 6 . 4}$</td>
<td style="text-align: center;">$\mathbf{4 0 . 0}$</td>
<td style="text-align: center;">$\mathbf{6 0 . 0}$</td>
<td style="text-align: center;">$\mathbf{1 0 . 7}$</td>
<td style="text-align: center;">$\mathbf{1 4 . 3}$</td>
<td style="text-align: center;">$\mathbf{3 8 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">36.0</td>
</tr>
<tr>
<td style="text-align: left;">Codex (code-davinci-002)</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">23.0</td>
</tr>
<tr>
<td style="text-align: left;">Human (LeetCode users)</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Zero-shot pass@1 and pass@5 accuracies (\%) on LeetCode.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We test GPT-4 on LeetCode's Interview Assessment platform, which provides simulated coding interviews for software engineer positions at major tech companies. GPT-4 solves all questions from all three rounds of interviews (titled online assessment, phone interview, and on-site interview) using only 10 minutes in total, with 4.5 hour allotted. According to LeetCode, in those three rounds respectively, (the early version of) GPT-4 achieves $8.96 / 10,8.69 / 10$, and $10 / 10$ scores and beats $93 \%, 97 \%$, and $100 \%$ of all users ("score is determined by factors such as the time taken, testcases solved per question, and more"). See Section 3 for more on GPT-4's coding abilities.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
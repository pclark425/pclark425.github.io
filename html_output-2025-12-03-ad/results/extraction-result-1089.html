<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1089 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1089</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1089</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-95523836fea59eeac6f5a09aef4a95dfdb7e880d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/95523836fea59eeac6f5a09aef4a95dfdb7e880d" target="_blank">TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> TeachMyAgent (TA), a benchmark of current ACL algorithms leveraging procedural task generation, is presented, and a comparative study of representative existing approaches is conducted, showcasing the competitiveness of some ACL algorithms that do not use expert knowledge.</p>
                <p><strong>Paper Abstract:</strong> Training autonomous agents able to generalize to multiple tasks is a key target of Deep Reinforcement Learning (DRL) research. In parallel to improving DRL algorithms themselves, Automatic Curriculum Learning (ACL) study how teacher algorithms can train DRL agents more efficiently by adapting task selection to their evolving abilities. While multiple standard benchmarks exist to compare DRL agents, there is currently no such thing for ACL algorithms. Thus, comparing existing approaches is difficult, as too many experimental parameters differ from paper to paper. In this work, we identify several key challenges faced by ACL algorithms. Based on these, we present TeachMyAgent (TA), a benchmark of current ACL algorithms leveraging procedural task generation. It includes 1) challenge-specific unit-tests using variants of a procedural Box2D bipedal walker environment, and 2) a new procedural Parkour environment combining most ACL challenges, making it ideal for global performance assessment. We then use TeachMyAgent to conduct a comparative study of representative existing approaches, showcasing the competitiveness of some ACL algorithms that do not use expert knowledge. We also show that the Parkour environment remains an open problem. We open-source our environments, all studied ACL algorithms (collected from open-source code or re-implemented), and DRL students in a Python package available at https://github.com/flowersteam/TeachMyAgent.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1089.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1089.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StumpTracks-Walkers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stump Tracks environment with walker embodiments (short bipedal, quadrupedal, spider, millipede, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parametric 2D Box2D locomotion environment where stump height and stump spacing (2D task-encoding) are procedurally sampled each episode; used as unit-test variants to probe specific ACL challenges (mostly-unfeasible, mostly-trivial, forgetting, student variety, rugged landscapes).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Walker embodiments (bipedal walker, short bipedal, quadrupedal, spider, millipede)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated embodied DRL learners (trained with SAC or PPO in experiments) controlling joint torques; observations include 10 lidars, head position/velocity, joint angles/velocities and contact sensors; reward = forward progress minus torque penalty; episodic (max 2000 steps); binary 'mastered' flag provided when cumulative episodic reward >= 230.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Stump Tracks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Box2D 2D track populated by stumps whose heights are sampled from a Gaussian (mean μ_s, std 0.1) and spaced by Δ_s; each episode uses a new (μ_s, Δ_s) parameterization; episodes terminate at 2000 steps or catastrophic collision (head collision → -100 reward). Variants were created by expanding/shrinking bounds or applying random transformations to probe ACL challenges (e.g. create mostly-unfeasible or mostly-trivial task spaces, or ruggedly-scattered feasible regions).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Low-dimensional parametric complexity: 2 task-encoding dimensions (μ_s, Δ_s); difficulty characterized by percent of task space that is feasible vs unfeasible (paper reports ~80% unfeasible in one setup), presence of extremely hard/unfeasible tasks, episode length (2000 steps), and collision terminal penalty (-100).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low-dimensional (2D) but can present high effective difficulty (paper reports setups with ~80% unfeasible tasks)—labelled 'low-to-medium' dimensional with 'high' effective difficulty in adversarial parameter bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural generation parameters (μ_s, Δ_s) and artificially modified bounds/transformations; explicit manipulations used: expanding stump height bounds to create ~80% unfeasible tasks, allowing negative stump heights to create ~50% trivial tasks, and random spatial transformations to create rugged scattering of feasible tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>variable per experiment: low-to-high (unit-tests include low-variation baseline and high-variation manipulations such as 80% unfeasible or 50% trivial).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Percent of test tasks (100-task test set) on which the agent obtained an episodic reward > 230 (termed 'mastered'); episodic reward and statistical tests (Welch's t-test) used to compare methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported as percent-mastered on a 100-task test set; the paper reports relative improvements of ACL teachers vs Random baseline (plotted as ratios) but does not give a single absolute per-agent number in text for all conditions; specific manipulations reported: task-space can contain ~80% unfeasible tasks or ~50% trivial tasks depending on setup.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicitly discussed: high procedural variation (PCG) increases diversity but can create large regions of unfeasible tasks, making naive random sampling inefficient; ACL teachers must detect and focus on feasible subspaces. The paper shows trade-offs where methods that assume or require an initial easy distribution fail when variation produces no reliable easy tasks (e.g., ADR and GoalGAN degrade without suitable bootstrap distributions). Rugged (non-smooth) difficulty landscapes (introduced by task-space transformations) break assumptions of incremental sampling methods and can force distribution jumps rather than smooth drifts.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning via Automatic Curriculum Learning (ACL) teachers sampling task parameters each episode (methods compared include ALP-GMM, Covar-GMM, RIAC, GoalGAN, Setter-Solver, ADR, SPDL, and Random baseline); DRL students trained with SAC or PPO for 20 million steps (hyperparameter tuning runs at 7M).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Agents were evaluated on a held-out 100-task test set (uniformly sampled) and percent-mastered reported. ACL methods improved generalization (higher percent-mastered) in certain challenges; expert-knowledge-free methods (ALP-GMM, Covar-GMM) often matched or outperformed methods requiring expert knowledge when prior knowledge was unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training runs: 20 million environment steps per run; evaluations every 500k steps; hyperparameter grid-search used 7M-step short runs (16 seeds) during tuning. Some ACL methods show faster sample efficiency when given expert priors, but expert-knowledge-free methods are competitive in final sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Stump Tracks (2D PCG) serves as a controlled unit-test bed: (1) high variation without curriculum is inefficient when large parts of the space are unfeasible; (2) ACL methods that do not depend on expert priors (ALP-GMM, Covar-GMM) perform competitively and robustly across No/Low/High expert-knowledge setups; (3) methods that require initial easy distributions (ADR, GoalGAN) perform well when such priors exist but fail or perform no better than Random when they do not; (4) ruggedly-scattered feasible regions break incremental expansion strategies and favor methods that can detect learning-progress niches rather than rely on smooth sampling drifts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1089.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1089.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parkour-MultiMorph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parkour environment (CPPN-encoded Parkour) with multiple morphologies (bipedal walker, fish swimmer, chimpanzee climber, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedurally-generated 2D Box2D parkour environment where terrains (ground and ceiling) are generated by a fixed CPPN conditioned on a 3D parameter vector, combined with creeper (graspable) parameters (μ_c, Δ_c) and a water-level parameter τ, yielding a 6D task-encoding with rugged difficulty landscapes and embodiment-dependent curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random black-box students drawn per run among multiple morphologies (bipedal walker, fish swimmer, chimpanzee climber); DRL learners trained with SAC or PPO</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated embodied DRL learners (SAC or PPO) executing continuous torques over limb joints and additional grasp actions for climbers; sensors (10 lidars, head/motor states, grasp sensors) provide observations. Episodes last up to 2000 steps; rewards are forward progress minus torque penalty; a 'mastered' binary flag is provided when cumulative reward >= 230.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Parkour</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>6-dimensional PCG: 3 CPPN input parameters controlling non-linear ground and ceiling shapes (CPPN architecture fixed), 2 parameters for creeper generation (mean height μ_c and spacing Δ_c), and 1 water-level parameter τ ∈ [0,1]. Terrain generation via CPPN produces non-smooth, rugged difficulty landscapes (ground and ceiling can cross → unfeasible tasks). Water simulates buoyancy, drag, lift and push; creepers can be grasped via sensor-triggered joints. Three global difficulty bounds provided (easy, medium, hard); experiments used medium. Each episode uses a fresh sampled parameter vector.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task-space dimensionality (6D), ruggedness of CPPN-induced terrain (nonlinear mapping from 3 CPPN inputs to terrain shape), presence of multiple milieu (water vs air) and graspable objects, feasibility fraction (many tasks unfeasible), episode length (2000 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (6D PCG plus non-linear CPPN mapping creates a rugged, highly complex difficulty landscape)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural generation via CPPN inputs (3 dims), creeper generation parameters (μ_c, Δ_c), and water level τ; three hand-designed bounding regimes (easy/medium/hard) alter the amount of feasible tasks; variation is high both in number of distinct instances and in structural variety (terrain shapes, water, graspable objects).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Percent of test tasks (per-embodiment hand-designed test sets containing easy/medium/hard tasks) on which the agent obtained episodic reward > 230 ('mastered'); reported per-morphology and averaged across morphologies; statistical significance assessed via Welch's t-test.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Across teachers in Parkour (48 seeded runs, 16 seeds per morphology): climbing morphology - at most ~1% mastered tasks by end of training across all teachers; bipedal walker - final mastered < 60%; fish (swimmer) - final mastered < 50%; high variance reported, especially for fish. Overall low absolute mastery and large variability make Parkour an open challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicit and central: the CPPN-based high variation creates a rugged difficulty landscape such that (a) many tasks are unfeasible, (b) the optimal curriculum is highly embodiment-dependent (e.g., swimmers need high water, climbers need creepers + high ceiling, walkers need low water), and (c) large variation combined with high complexity prevents methods that rely on incremental expansion from finding feasible niches (ADR can get stuck expanding into hard/unfeasible regions; GoalGAN suffers from inertia and cannot quickly recover from student forgetting). The paper argues that high procedural variation increases potential generalization but also increases the need for curriculum/teacher algorithms that can detect and focus on feasible learning niches.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Empirical: when Parkour (medium bounds) was used with high variation and high complexity, final mastery was poor: climber ≤ 1% mastered, walker < 60% mastered, fish < 50% mastered (numbers are per-paper end-of-training summaries across teachers).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Automatic Curriculum Learning: multiple ACL teacher algorithms sample environment parameters each episode (ALP-GMM, Covar-GMM, RIAC, GoalGAN, Setter-Solver, ADR, SPDL, Random baseline). Teachers were given limited expert knowledge (only reward mastery range) in Parkour experiments; for ADR/SPDL initial distributions were randomly set when required. DRL students trained for 20M environment steps; teachers update sampling during training.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Generalization evaluated via three per-embodiment test sets (even mix of easy/medium/hard). Results show poor generalization: climber failed to learn generalizable policies (≤1% mastered), walkers and swimmers achieved limited mastery (<60% and <50% respectively) with high variance; best-performing teachers did not require expert knowledge, suggesting that methods robust to limited prior information generalize better in this setting, but overall generalization remains weak.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training runs were 20 million steps per run with evaluations every 500k steps; 48 seeded experiments (16 seeds per morphology) were run. Some ACL methods plateaued quickly (e.g., ADR plateaued and ended worse than Random), indicating poor sample-efficiency in rugged/high-variation Parkour when bootstrap assumptions are violated.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Parkour exposes the interaction of high environment complexity and high procedural variation: (1) many current ACL methods struggle to find feasible learning niches in a high-variation, rugged task space; (2) methods that do not rely heavily on expert priors (ALP-GMM, Covar-GMM) were among the better performers overall; (3) methods that assume an initial easy distribution or smooth expansion (ADR, SPDL) can fail or perform worse than Random when variation denies those assumptions; (4) final learner performance is morphology-dependent (climbers effectively fail, walkers and swimmers achieve partial mastery with high variance), making Parkour an open problem for ACL research.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments. <em>(Rating: 2)</em></li>
                <li>Automatic goal generation for reinforcement learning agents <em>(Rating: 2)</em></li>
                <li>Solving rubik's cube with a robot hand <em>(Rating: 2)</em></li>
                <li>Leveraging procedural generation to benchmark reinforcement learning <em>(Rating: 2)</em></li>
                <li>Automated curricula through setter-solver interactions <em>(Rating: 2)</em></li>
                <li>Self-paced deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Enhanced POET: open-ended reinforcement learning through unbounded invention of learning challenges and their solutions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1089",
    "paper_id": "paper-95523836fea59eeac6f5a09aef4a95dfdb7e880d",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "StumpTracks-Walkers",
            "name_full": "Stump Tracks environment with walker embodiments (short bipedal, quadrupedal, spider, millipede, etc.)",
            "brief_description": "A parametric 2D Box2D locomotion environment where stump height and stump spacing (2D task-encoding) are procedurally sampled each episode; used as unit-test variants to probe specific ACL challenges (mostly-unfeasible, mostly-trivial, forgetting, student variety, rugged landscapes).",
            "citation_title": "Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments.",
            "mention_or_use": "use",
            "agent_name": "Walker embodiments (bipedal walker, short bipedal, quadrupedal, spider, millipede)",
            "agent_description": "Simulated embodied DRL learners (trained with SAC or PPO in experiments) controlling joint torques; observations include 10 lidars, head position/velocity, joint angles/velocities and contact sensors; reward = forward progress minus torque penalty; episodic (max 2000 steps); binary 'mastered' flag provided when cumulative episodic reward &gt;= 230.",
            "agent_type": "simulated agent",
            "environment_name": "Stump Tracks",
            "environment_description": "Box2D 2D track populated by stumps whose heights are sampled from a Gaussian (mean μ_s, std 0.1) and spaced by Δ_s; each episode uses a new (μ_s, Δ_s) parameterization; episodes terminate at 2000 steps or catastrophic collision (head collision → -100 reward). Variants were created by expanding/shrinking bounds or applying random transformations to probe ACL challenges (e.g. create mostly-unfeasible or mostly-trivial task spaces, or ruggedly-scattered feasible regions).",
            "complexity_measure": "Low-dimensional parametric complexity: 2 task-encoding dimensions (μ_s, Δ_s); difficulty characterized by percent of task space that is feasible vs unfeasible (paper reports ~80% unfeasible in one setup), presence of extremely hard/unfeasible tasks, episode length (2000 steps), and collision terminal penalty (-100).",
            "complexity_level": "low-dimensional (2D) but can present high effective difficulty (paper reports setups with ~80% unfeasible tasks)—labelled 'low-to-medium' dimensional with 'high' effective difficulty in adversarial parameter bounds.",
            "variation_measure": "Procedural generation parameters (μ_s, Δ_s) and artificially modified bounds/transformations; explicit manipulations used: expanding stump height bounds to create ~80% unfeasible tasks, allowing negative stump heights to create ~50% trivial tasks, and random spatial transformations to create rugged scattering of feasible tasks.",
            "variation_level": "variable per experiment: low-to-high (unit-tests include low-variation baseline and high-variation manipulations such as 80% unfeasible or 50% trivial).",
            "performance_metric": "Percent of test tasks (100-task test set) on which the agent obtained an episodic reward &gt; 230 (termed 'mastered'); episodic reward and statistical tests (Welch's t-test) used to compare methods.",
            "performance_value": "Reported as percent-mastered on a 100-task test set; the paper reports relative improvements of ACL teachers vs Random baseline (plotted as ratios) but does not give a single absolute per-agent number in text for all conditions; specific manipulations reported: task-space can contain ~80% unfeasible tasks or ~50% trivial tasks depending on setup.",
            "complexity_variation_relationship": "Explicitly discussed: high procedural variation (PCG) increases diversity but can create large regions of unfeasible tasks, making naive random sampling inefficient; ACL teachers must detect and focus on feasible subspaces. The paper shows trade-offs where methods that assume or require an initial easy distribution fail when variation produces no reliable easy tasks (e.g., ADR and GoalGAN degrade without suitable bootstrap distributions). Rugged (non-smooth) difficulty landscapes (introduced by task-space transformations) break assumptions of incremental sampling methods and can force distribution jumps rather than smooth drifts.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning via Automatic Curriculum Learning (ACL) teachers sampling task parameters each episode (methods compared include ALP-GMM, Covar-GMM, RIAC, GoalGAN, Setter-Solver, ADR, SPDL, and Random baseline); DRL students trained with SAC or PPO for 20 million steps (hyperparameter tuning runs at 7M).",
            "generalization_tested": true,
            "generalization_results": "Agents were evaluated on a held-out 100-task test set (uniformly sampled) and percent-mastered reported. ACL methods improved generalization (higher percent-mastered) in certain challenges; expert-knowledge-free methods (ALP-GMM, Covar-GMM) often matched or outperformed methods requiring expert knowledge when prior knowledge was unavailable.",
            "sample_efficiency": "Training runs: 20 million environment steps per run; evaluations every 500k steps; hyperparameter grid-search used 7M-step short runs (16 seeds) during tuning. Some ACL methods show faster sample efficiency when given expert priors, but expert-knowledge-free methods are competitive in final sample efficiency.",
            "key_findings": "Stump Tracks (2D PCG) serves as a controlled unit-test bed: (1) high variation without curriculum is inefficient when large parts of the space are unfeasible; (2) ACL methods that do not depend on expert priors (ALP-GMM, Covar-GMM) perform competitively and robustly across No/Low/High expert-knowledge setups; (3) methods that require initial easy distributions (ADR, GoalGAN) perform well when such priors exist but fail or perform no better than Random when they do not; (4) ruggedly-scattered feasible regions break incremental expansion strategies and favor methods that can detect learning-progress niches rather than rely on smooth sampling drifts.",
            "uuid": "e1089.0",
            "source_info": {
                "paper_title": "TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Parkour-MultiMorph",
            "name_full": "Parkour environment (CPPN-encoded Parkour) with multiple morphologies (bipedal walker, fish swimmer, chimpanzee climber, etc.)",
            "brief_description": "A procedurally-generated 2D Box2D parkour environment where terrains (ground and ceiling) are generated by a fixed CPPN conditioned on a 3D parameter vector, combined with creeper (graspable) parameters (μ_c, Δ_c) and a water-level parameter τ, yielding a 6D task-encoding with rugged difficulty landscapes and embodiment-dependent curricula.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Random black-box students drawn per run among multiple morphologies (bipedal walker, fish swimmer, chimpanzee climber); DRL learners trained with SAC or PPO",
            "agent_description": "Simulated embodied DRL learners (SAC or PPO) executing continuous torques over limb joints and additional grasp actions for climbers; sensors (10 lidars, head/motor states, grasp sensors) provide observations. Episodes last up to 2000 steps; rewards are forward progress minus torque penalty; a 'mastered' binary flag is provided when cumulative reward &gt;= 230.",
            "agent_type": "simulated agent",
            "environment_name": "Parkour",
            "environment_description": "6-dimensional PCG: 3 CPPN input parameters controlling non-linear ground and ceiling shapes (CPPN architecture fixed), 2 parameters for creeper generation (mean height μ_c and spacing Δ_c), and 1 water-level parameter τ ∈ [0,1]. Terrain generation via CPPN produces non-smooth, rugged difficulty landscapes (ground and ceiling can cross → unfeasible tasks). Water simulates buoyancy, drag, lift and push; creepers can be grasped via sensor-triggered joints. Three global difficulty bounds provided (easy, medium, hard); experiments used medium. Each episode uses a fresh sampled parameter vector.",
            "complexity_measure": "Task-space dimensionality (6D), ruggedness of CPPN-induced terrain (nonlinear mapping from 3 CPPN inputs to terrain shape), presence of multiple milieu (water vs air) and graspable objects, feasibility fraction (many tasks unfeasible), episode length (2000 steps).",
            "complexity_level": "high (6D PCG plus non-linear CPPN mapping creates a rugged, highly complex difficulty landscape)",
            "variation_measure": "Procedural generation via CPPN inputs (3 dims), creeper generation parameters (μ_c, Δ_c), and water level τ; three hand-designed bounding regimes (easy/medium/hard) alter the amount of feasible tasks; variation is high both in number of distinct instances and in structural variety (terrain shapes, water, graspable objects).",
            "variation_level": "high",
            "performance_metric": "Percent of test tasks (per-embodiment hand-designed test sets containing easy/medium/hard tasks) on which the agent obtained episodic reward &gt; 230 ('mastered'); reported per-morphology and averaged across morphologies; statistical significance assessed via Welch's t-test.",
            "performance_value": "Across teachers in Parkour (48 seeded runs, 16 seeds per morphology): climbing morphology - at most ~1% mastered tasks by end of training across all teachers; bipedal walker - final mastered &lt; 60%; fish (swimmer) - final mastered &lt; 50%; high variance reported, especially for fish. Overall low absolute mastery and large variability make Parkour an open challenge.",
            "complexity_variation_relationship": "Explicit and central: the CPPN-based high variation creates a rugged difficulty landscape such that (a) many tasks are unfeasible, (b) the optimal curriculum is highly embodiment-dependent (e.g., swimmers need high water, climbers need creepers + high ceiling, walkers need low water), and (c) large variation combined with high complexity prevents methods that rely on incremental expansion from finding feasible niches (ADR can get stuck expanding into hard/unfeasible regions; GoalGAN suffers from inertia and cannot quickly recover from student forgetting). The paper argues that high procedural variation increases potential generalization but also increases the need for curriculum/teacher algorithms that can detect and focus on feasible learning niches.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Empirical: when Parkour (medium bounds) was used with high variation and high complexity, final mastery was poor: climber ≤ 1% mastered, walker &lt; 60% mastered, fish &lt; 50% mastered (numbers are per-paper end-of-training summaries across teachers).",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Automatic Curriculum Learning: multiple ACL teacher algorithms sample environment parameters each episode (ALP-GMM, Covar-GMM, RIAC, GoalGAN, Setter-Solver, ADR, SPDL, Random baseline). Teachers were given limited expert knowledge (only reward mastery range) in Parkour experiments; for ADR/SPDL initial distributions were randomly set when required. DRL students trained for 20M environment steps; teachers update sampling during training.",
            "generalization_tested": true,
            "generalization_results": "Generalization evaluated via three per-embodiment test sets (even mix of easy/medium/hard). Results show poor generalization: climber failed to learn generalizable policies (≤1% mastered), walkers and swimmers achieved limited mastery (&lt;60% and &lt;50% respectively) with high variance; best-performing teachers did not require expert knowledge, suggesting that methods robust to limited prior information generalize better in this setting, but overall generalization remains weak.",
            "sample_efficiency": "Training runs were 20 million steps per run with evaluations every 500k steps; 48 seeded experiments (16 seeds per morphology) were run. Some ACL methods plateaued quickly (e.g., ADR plateaued and ended worse than Random), indicating poor sample-efficiency in rugged/high-variation Parkour when bootstrap assumptions are violated.",
            "key_findings": "Parkour exposes the interaction of high environment complexity and high procedural variation: (1) many current ACL methods struggle to find feasible learning niches in a high-variation, rugged task space; (2) methods that do not rely heavily on expert priors (ALP-GMM, Covar-GMM) were among the better performers overall; (3) methods that assume an initial easy distribution or smooth expansion (ADR, SPDL) can fail or perform worse than Random when variation denies those assumptions; (4) final learner performance is morphology-dependent (climbers effectively fail, walkers and swimmers achieve partial mastery with high variance), making Parkour an open problem for ACL research.",
            "uuid": "e1089.1",
            "source_info": {
                "paper_title": "TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments.",
            "rating": 2
        },
        {
            "paper_title": "Automatic goal generation for reinforcement learning agents",
            "rating": 2
        },
        {
            "paper_title": "Solving rubik's cube with a robot hand",
            "rating": 2
        },
        {
            "paper_title": "Leveraging procedural generation to benchmark reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Automated curricula through setter-solver interactions",
            "rating": 2
        },
        {
            "paper_title": "Self-paced deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Enhanced POET: open-ended reinforcement learning through unbounded invention of learning challenges and their solutions",
            "rating": 1
        }
    ],
    "cost": 0.0185915,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL</h1>
<p>Clément Romac<em> ${ }^{</em> 1}$ Rémy Portelas ${ }^{* 1}$ Katja Hofmann ${ }^{2}$ Pierre-Yves Oudeyer ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Training autonomous agents able to generalize to multiple tasks is a key target of Deep Reinforcement Learning (DRL) research. In parallel to improving DRL algorithms themselves, Automatic Curriculum Learning (ACL) study how teacher algorithms can train DRL agents more efficiently by adapting task selection to their evolving abilities. While multiple standard benchmarks exist to compare DRL agents, there is currently no such thing for ACL algorithms. Thus, comparing existing approaches is difficult, as too many experimental parameters differ from paper to paper. In this work, we identify several key challenges faced by ACL algorithms. Based on these, we present TeachMyAgent (TA), a benchmark of current ACL algorithms leveraging procedural task generation. It includes 1) challengespecific unit-tests using variants of a procedural Box2D bipedal walker environment, and 2) a new procedural Parkour environment combining most ACL challenges, making it ideal for global performance assessment. We then use TeachMyAgent to conduct a comparative study of representative existing approaches, showcasing the competitiveness of some ACL algorithms that do not use expert knowledge. We also show that the Parkour environment remains an open problem. We open-source our environments, all studied ACL algorithms (collected from open-source code or re-implemented), and DRL students in a Python package available at https://github. com/flowersteam/TeachMyAgent.</p>
<h2>1. Introduction</h2>
<p>When looking at how structured and gradual human-learning is, one can argue that randomly presenting tasks to a learning</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>agent is unlikely to be optimal for complex learning problems. Building upon this, curriculum learning has long been identified as a key component for many machine learning problems (Selfridge et al., 1985; Elman, 1993; Bengio et al., 2009; Cangelosi \&amp; Schlesinger, 2015) in order to organize samples showed during learning. While such a curriculum can be hand-designed by human experts on the problem, the field of Automatic Curriculum Learning (Graves et al., 2017; Portelas et al., 2020a) focuses on designing teacher algorithms able to autonomously sequence learning problem selection so as to maximize agent performance (e.g. over a set of samples in supervised learning, or game levels in DRL).</p>
<p>Parallel to these lines of works, DRL researchers have been increasingly interested in finding methods to train generalist agents (Rajeswaran et al., 2017; Zhang et al., 2018; Vanschoren, 2018; Cobbe et al., 2019) to go beyond initial successes on solving single problems, e.g individual Atari games (Mnih et al., 2015) or navigation in fixed scenarios (Lillicrap et al., 2016; Haarnoja et al., 2018). Many works proposed novel DRL learning architectures able to successfully infer multi-purpose action policies when given an experience stream composed of randomly sampled tasks (Schaul et al., 2015; Hessel et al., 2018; Cobbe et al., 2019; Hessel et al., 2019). Here and thereafter tasks denote learning problems in general, for instance multiple mazes to solve (a.k.a environments) or, in the context of robotic manipulation, multiple state configuration to obtain (a.k.a. goals) (Portelas et al., 2020a). To compare existing and future Multi-task DRL agents, Cobbe et al. (2020) proposed a suite of 16 atari-like environments, all relying on Procedural Content Generation (PCG) to generate a wide diversity of learning situations. The high-diversity induced by PCG has been identified as particularly beneficial to foster generalization abilities to DRL agents (Justesen et al., 2018; Risi \&amp; Togelius, 2019; OpenAI et al., 2019).</p>
<p>An important aspect not covered by these prior works is that they all rely on proposing randomly selected tasks to their agent, i.e. they do not consider using curriculum in learning. One can argue that random task selection is inefficient, especially when considering complex continuous task sets, a.k.a task spaces, which can feature subspaces of varying difficulties ranging from trivial to unfeasible. Following this observation, many works attempted to train given multi-task</p>
<p>agents by pairing them with ACL algorithms (Portelas et al., 2020a). The advantages of ACL over random task sampling for DRL agents have been demonstrated in diverse experimental setups, such as domain randomization for sim2real robotics (OpenAI et al., 2019; Mehta et al., 2019), video games (Salimans \&amp; Chen, 2018; Mysore et al., 2019), or navigation in procedurally generated environments (Florensa et al., 2018; Portelas et al., 2019; Racanière et al., 2020).</p>
<p>While this diversity of potential application domains and implementations of ACL hints a promising future for this field, it also makes comparative analysis complicated, which limits large-scale adoption of ACL. For instance, depending on the ACL approach, the amount of required expert knowledge on the task space can range from close to none - as in Portelas et al. (2019) - to a high amount of prior knowledge, e.g. initial task sampling subspace and predefined reward range triggering task sampling distribution shifts, as in OpenAI et al. (2019). Additionally, some ACL approaches were tested based on their ability to master an expert-chosen target subspace (Klink et al., 2020) while others were tasked to optimize their performance over the entire task space (Baranes \&amp; Oudeyer, 2009; Florensa et al., 2018; Portelas et al., 2019). Besides, because of the large computational cost and implementation efforts necessary for exhaustive comparisons, newly proposed ACL algorithms are often compared to only a subset of previous ACL approaches (Mehta et al., 2019; Portelas et al., 2019; Racanière et al., 2020). This computation bottleneck is also what prevents most works from testing their ACL teachers on a diversity of DRL students, i.e. given a set of tasks, they do not vary the student's learning mechanism nor its embodiment. Designing a unified benchmark platform, where baselines would be shared and allow one to only run its approach and compare it to established results, could drive progress in this space.</p>
<p>Inspired by how the MNIST dataset (Lecun et al., 1998) or the ALE Atari games suite (Bellemare et al., 2013) respectively catalyzed supervised learning and single-task reinforcement learning research, we propose to perform this much-needed in-depth ACL benchmarking study. As such, we introduce TeachMyAgent $1.0^{1}$, a teacher testbed featuring a) two procedural Box2D ${ }^{2}$ environments with challenging task spaces, b) a collection of pre-defined agent embodiments, and c) multiple DRL student models. The combination of these three components constitutes a large panel of diverse teaching problems. We leverage this benchmark to characterize the efficiency of an ACL algorithm on the following key teaching challenges:</p>
<ol>
<li>Mostly unfeasible task spaces - While using PCG sys-</li>
</ol>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tems to generate tasks allows to propose rich task spaces to DRL agents, which is good for generalization, such large spaces might contain a predominant amount of unfeasible (or initially unfeasible) tasks. A teacher algorithm must then have the ability to quickly detect and exploit promising task subspaces for its learner.
2. Mostly trivial task spaces - On the contrary, the task space might be mostly trivial and contain only few challenging subspaces, which is a typical scenario when dealing with a skilled student (e.g. that is already trained, or that has an advantageous embodiment). In that case the teacher has to efficiently detect and exploit the small portion of subspaces of relevant difficulty.
3. Forgetting students - DRL learners are prone to catastrophic forgetting (Kirkpatrick et al., 2017), i.e. to overwrite important skills while training new ones. This has to be detected and dealt with by the teacher for optimal curriculum generation.
4. Robustness to diverse students - Being able to adapt curriculum generation to diverse students is an important desiderata to ensure a given ACL mechanism has good chances to transfer to novel scenarios.
5. Rugged difficulty landscapes - Another important property for ACL algorithms is to be able to deal with task spaces for which the optimal curriculum is not a smooth task distribution sampling drift across the space but rather a series of distribution jumps, e.g. as in complex PCG-task spaces.
6. Working with no or little expert knowledge - Prior knowledge over a task space w.r.t. a given student is a costly information gathering process that needs to be repeated for each new problem/student. Relying on as little expert knowledge as possible is therefore a desirable property for ACL algorithms (especially if aiming for out-of-the-lab applications).</p>
<p>To precisely assess the proficiency of an ACL algorithm on each of these challenges independently, we extend a Box2D walker environment from Portelas et al. (2019) into multiple unit-test variants, one per challenge, inspired by the structure of bsuite (Osband et al., 2020), a recent benchmark for RL agents. The second environment of our benchmark is the Parkour environment, inspired by Wang et al. (2020). It features a complex task space whose parameters seed a neural network-based procedural generation of a wide diversity of environments, in which there exists drastically different learning curricula depending on the agent's embodiment (see fig. 1). To assess the ability of existing ACL methods to robustly adapt to diverse students, we consider a random black-box student scenario in the Parkour environment, i.e. the morphology (e.g. walker or climber) of the learner is randomly selected for each new training run.</p>
<p>Scope More precisely, we conduct an in-depth comparative study of ACL approaches suited for generalist DRL agents such as SAC (Haarnoja et al., 2018) or PPO (Schulman et al., 2017) in single agent scenarios. We do not include works on self-play/multi-agent setups (Hernandez et al., 2019; Hernandez-Leal et al., 2018) nor single-agent population-based approaches (Forestier et al., 2017; Wang et al., 2020). Also, we are interested in the problem of task selection from a continuous parameter space encoding the procedural generation of tasks. We leave the analysis of ACL methods for discrete task sets (Matiisen et al., 2017; Mysore et al., 2019), sets of task spaces (Forestier et al., 2017; Colas et al., 2019), or intrinsic reward learning (Pathak et al., 2017; Burda et al., 2019) for future work. We assume this continuous space is given and relatively low-dimensional as it already poses strong teaching challenges: we therefore leave the analysis of approaches that autonomously learn task representations for subsequent work (Pong et al., 2020; Jabri et al., 2019; Kovač et al., 2020).</p>
<p>Our main contributions are:</p>
<ul>
<li>Identification of multiple challenges to be tackled by ACL methods, enabling multi-dimensional comparisons of these algorithms.</li>
<li>TeachMyAgent 1.0, a set of teaching problems (based on PCG environments) to study and compare ACL algorithms when paired with DRL students.</li>
<li>Comparative study of representative existing ACL approaches including both skill-specific unit-tests and global performance assessments, which highlights the competitiveness of methods not using expert knowledge and shows that our Parkour environment largely remains an open problem for current state-of-the-art ACL.</li>
<li>Release of an open-source Python package, featuring 1) all environments, embodiments and DRL students from TeachMyAgent, 2) all studied ACL algorithms, that we either adapt to our API when code is available or re-implement from scratch if not open-sourced, 3) our experimental results as baselines for future works, and 4) tutorials \&amp; reproducibility scripts.</li>
</ul>
<h2>2. Related work</h2>
<p>Many environment suites already exist to benchmark DRL algorithms: some of them leverage video games, which provide challenging discrete action spaces, e.g. Atari 2600 Games as in Bellemare et al. (2013) or Sonic The Hedgehog levels in Nichol et al. (2018). To study and develop DRL agents suited for complex continuous control scenarios, the community predominantly used the MuJoCo physics engine (Todorov et al., 2012). The Deep Mind Lab (Beattie
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. TeachMyAgent: A benchmark to study and compare teacher algorithms in continuous procedural environments.
et al., 2016) provides customizable puzzle-solving environment, particularly well suited to study goal-conditioned policies learning from pixels in rich 3D environments. At the intersection of DRL and Natural Language Processing, benchmark environments such as TextWorld (Côté et al., 2018) or BabyAI (Chevalier-Boisvert et al., 2019) were also designed to provide a testbed to develop autonomous agent receiving linguistic goals and/or interacting using language. The bsuite benchmark (Osband et al., 2020) leverages unittests to assess the core capabilities of DRL methods (e.g. generalization, memory). In all these previous works, the DRL agent is learning in one or few environments presented randomly and/or intrinsically chooses goals within those predefined environments, and the long-term community objective is to find more efficient learning architectures. On the contrary, the objective of TeachMyAgent is to foster the development of new teacher algorithms whose objective is,</p>
<p>given a task space and a DRL student, to most efficiently organize the learning curriculum of their DRL student such that its performance is maximized over the task set. In other words, it is not about finding efficient learning architectures but about finding efficient curriculum generators.</p>
<p>Perhaps closest to our work is the Procgen benchmark (Cobbe et al., 2020), which features several atari-like environments, all having unique procedural generation systems allowing to generate a wide diversity of learning situations, particularly well suited to assess the generalization abilities of DRL agents. While they rely on an uncontrolable, random procedural generation, we assume control over it, which enables the use of ACL methods to select parameters encoding task generation. An interesting future work, parallel to ours, would be to modify the Procgen benchmark to allow direct control over the procedural generation.</p>
<p>Because of the current lack of any ACL benchmark, most recently proposed ACL algorithms relied on designing their own set of test environments. Florensa et al. (2018) used a custom MuJoCo Ant maze in which the ACL approach is in control of which end-position to target. Klink et al. (2020) used another MuJoCo Ant maze and ball-catching environment featuring a simulated Barrett WAM robot. While these previous works studied how to control goal selection in a given fixed environment, we are interested in the arguably more challenging problem of controlling a rich parametric procedural generation. Portelas et al. (2019) already studied ACL in Stump Tracks, a procedural Box2D environment that we include and extend in TeachMyAgent, however it did not perform an extensive comparative study as what we propose in the present work. Racanière et al. (2020) also used procedural generation to test their ACL approach, however they only compared their ACL algorithm to GoalGAN (Florensa et al., 2018), and did not open-source their environments. Additionally, in contrast with all previously cited ACL works, in TeachMyAgent we propose an in-depth analysis of each approaches through multiple unit-test experiments to fully characterize each teacher.</p>
<h2>3. ACL baselines</h2>
<p>In the following paragraphs we succinctly frame and present all the ACL algorithms that we compare using TeachMyAgent. More detailed explanations are left to appendix A.</p>
<p>Framework Given a DRL student $s$ and a n-dimensional task-encoding parameter space $\mathcal{T} \in \mathbb{R}^{n}$ (i.e. a task space), the process of Automatic Curriculum Learning aims to learn a function $\mathcal{A}: \mathcal{H} \mapsto \mathcal{D}(\mathcal{T})$ mapping any information retained about past interactions with the task space to a distribution of tasks.</p>
<p>One can define the optimization objective of an ACL policy given an experimental budget of $E$ episodic tasks as:</p>
<p>$$
\max <em _mathrm_T="\mathrm{T">{\mathcal{A}} \int</em>} \sim \mathcal{D<em _mathrm_T="\mathrm{T">{\text {target }}} P</em>
$$}}^{E} \mathrm{dT</p>
<p>with $\mathcal{D}_{\text {target }}$ the distribution of test tasks over the task space and $P$ the post-training performance (e.g. episodic reward, exploration score) of student $s$ on task T after $E$ episodes. Since it is usually difficult to directly optimize for this objective, various surrogate objectives have been proposed in the literature. See Portelas et al. (2020a) for a review and classification of recent ACL works.</p>
<p>Expert-knowledge To ease the curriculum generation process, multiple forms of expert knowledge have been provided in current ACL approaches. We propose to gather them in three categories: 1) use of initial task distribution $\mathcal{D}<em _target="{target" _text="\text">{\text {init }}$ to bootstrap the ACL process, 2) use of a target task distribution $\mathcal{D}</em>$ to guide learning, and 3) use of a function interpreting the scalar episodic reward sent by the environment to identify mastered tasks (Reward mastery range). For each implemented ACL method, we highlight its required prior knowledge over the task space w.r.t a given DRL agent in table 1. We hope that this classification will ease the process of selecting an ACL method for researchers and engineers, as available expert knowledge is (arguably) often what conditions algorithmic choices in machine learning scenarios.}</p>
<p>Implemented baselines We compare seven ACL methods, chosen to be representative of the diversity of existing approaches, that can be separated in three broad categories. First, we include three methods relying on the idea of maximizing the Learning Progress (LP) of the student: RIAC (Baranes \&amp; Oudeyer, 2009), Covar-GMM (Moulin-Frier et al., 2014) and ALP-GMM (Portelas et al., 2019). We then add in our benchmark Goal-GAN (Florensa et al., 2018) and Setter-Solver (Racanière et al., 2020), both generating tasks using deep neural networks and requiring a binary reward for mastered/not mastered tasks, pre-defined using expert knowledge. Finally, we append to our comparison two ACL algorithms using the idea of starting from an initial distribution of tasks and progressively shifting it regarding the student's capabilities: ADR (OpenAI et al., 2019) (inflating a task distribution from a single initial task based on student mastery at each task distribution's border) and SPDL (Klink et al., 2020) (shifting its initial distribution towards a target distribution). We also add a baseline teacher selecting tasks uniformly random over the task space (called Random).</p>
<h2>4. The TeachMyAgent benchmark</h2>
<p>In the following section, we describe available environments and learners in TeachMyAgent. We propose two Box2D environments with procedural generation allowing to generate a</p>
<p>Table 1: Expert knowledge used by the different ACL methods. We separate knowledge required (REQ.) by algorithms, optional ones (OPT.), and knowledge not needed (empty cell).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Algorithm</th>
<th style="text-align: left;">$\mathcal{D}<em _target="{target" _text="\text">{\text {init }} \mathcal{D}</em>$ Reward mastery range}</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ADR</td>
<td style="text-align: left;">REQ. REQ.</td>
</tr>
<tr>
<td style="text-align: left;">ALP-GMM</td>
<td style="text-align: left;">OPT.</td>
</tr>
<tr>
<td style="text-align: left;">COVAR-GMM</td>
<td style="text-align: left;">OPT.</td>
</tr>
<tr>
<td style="text-align: left;">GOAL-GAN</td>
<td style="text-align: left;">OPT. REQ.</td>
</tr>
<tr>
<td style="text-align: left;">RIAC</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">SPDL</td>
<td style="text-align: left;">REQ. REQ.</td>
</tr>
<tr>
<td style="text-align: left;">SETTER-SOLVER</td>
<td style="text-align: left;">OPT. REQ.</td>
</tr>
</tbody>
</table>
<p>wide variety of terrains. Both our environments are episodic, use continuous action/observation spaces and return scalar rewards. In addition, we provide two DRL algorithms as well as multiple agent morphologies. An experiment is thus constituted of an ACL method, an environment and a learner (i.e. an embodied DRL algorithm).</p>
<h3>4.1. Environments</h3>
<p>Stump Tracks environment Stump Tracks is an extension of a parametric Box2D environment initially presented in <em>Portelas et al. (2019)</em>. The learning policy is embodied into a walker agent whose motors are controllable with torque (i.e. continuous action space). The observation space is composed of lidar sensors, head position and joint positions. The walker is rewarded for going forward and penalized for torque usage. An episode lasts 2000 steps at most, and is terminated if the agent reaches the end of the track or if its head collides with the environment (in which case a -100 reward is received). A 2D parametric PCG is used for each new episode: it controls the height and spacing of stumps laid out along the track (see fig. 2 and app. B). We chose to feature this environment as its low-dimensional task space is convenient for visualizations and modifications. We derive multiple variants of Stump Tracks (e.g. by extending the task space boundaries or shuffling it) to design our unit-tests of ACL challenges (see sec. 1 and sec. 5).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Stump Tracks, a simple parametric env. to study ACL algorithms with DRL students.</p>
<p>Parkour environment Inspired by both Stump Tracks and another Box2D environment from <em>Wang et al. (2020)</em>, we present the parametric Parkour environment: a challenging task space with rugged difficulty landscape, few prior knowledge definable, and requiring drastically differ-
ent learning curricula depending on the agent’s embodiment. It features an uneven terrain (see figure 1) composed of a ground and ceiling encoded through a Compositional Pattern-Producing Network (CPPN) <em>(Stanley, 2007)</em>. This CPPN, whose weights and architecture are kept fixed, takes an additional input vector of bounded real numbers which acts as the parameters controlling terrain generation. This neural network based generation enables to create a task space with a rugged difficulty landscape (see appendix B), requiring time consuming exploration from an expert to seek trivial subspaces. We propose three versions of this task space (i.e. three possible bounds for the CPPN’s input vector): easy, medium (used in the experiments of this paper) and hard. The Parkour environment also features graspable objects, called "creepers", creating a niche for climbing morphologies. Similarly to the stumps in Stump Tracks, the creepers’ generation is controlled by their height and the space between them. The Parkour’s task space also contains a dimension controlling the "water" level of the track, ranging from 0 (no water) to 1 (entire parkour under water). Water adds new physic rules aiming to imitate (in a simplified way) physics of water.</p>
<p>The resulting 6D task space ( 3 for the CPPN’s input, 2 for creepers and 1 for water) creates a rich environment in which the optimal curriculum will largely depend on the agent’s embodiment (e.g. swimming agents need high levels of water, while climbers and walkers need low levels). Note that, as in Stump Tracks, each episode lasts 2000 steps, agents are rewarded for moving forward (and penalised for using torque) and have access to lidars, head position, joint positions, and also additional information (see appendix B).</p>
<h3>4.2. Learners</h3>
<p>Embodiments As aforementioned, we introduce new morphologies using swimming and climbing locomotion (e.g. fish, chimpanzee, see figure 1). TeachMyAgent also features the short walker and quadrupedal walker from <em>Portelas et al. (2019)</em> as well as new walking morphologies such as the spider and the millipede (see figure 1).</p>
<p>DRL algorithms To benchmark ACL algorithms, we rely on two different state-of-the-art DRL algorithms: 1) Soft-Actor-Critic <em>(Haarnoja et al., 2018)</em> (SAC), a now classical off-policy actor-critic algorithm based on the dual optimization of reward and action entropy, and 2) Proximal Policy Optimization (PPO) <em>(Schulman et al., 2017)</em>, a well-known on-policy DRL algorithm based on approximate trust-region gradient updates. We use OpenAI Spinningup’s implementation for SAC and OpenAI Baselines’ implementation for PPO. See appendix C for implementation details.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>5. Experiments</h2>
<p>We now leverage TeachMyAgent to conduct an in-depth comparative study of the ACL algorithms presented in section 3. After discussing experimental details, we undergo two separate experiments, aiming to answer the following questions:</p>
<ul>
<li>How do current ACL methods compare on each teaching challenges proposed in sec. 1 ?</li>
<li>How do current ACL methods scale to a complex task space with limited expert knowledge?</li>
</ul>
<h3>5.1. Experimental details</h3>
<p>For both our environments, we train our DRL students for 20 million steps. For each new episode, the teacher samples a new parameter vector used for the procedural generation of the environment. The teacher then receives the cumulative episodic reward that can be potentially turned into a binary reward signal using expert knowledge (as in GoalGAN and Setter-Solver). Additionally, SPDL receives the initial state of the episode as well as the reward obtained at each step, as it is designed for non-episodic RL setup. Every 500000 steps, we test our student on a test set composed of 100 pre-defined tasks and monitor the percentage of test tasks on which the agent obtained an episodic reward greater than 230 (i.e. "mastered" tasks), which corresponds to agents that were able to reach the last portion of the map (in both Stump Tracks and Parkour). We compare performance results using Welch's t-test as proposed in Colas et al. (2018), allowing us to track statistically significant differences between two methods. We perform a hyperparameter search for all ACL conditions through grid-search (see appendix A), while controlling that an equivalent number of configurations are tested for each algorithm. See appendix C for additional experimental details.</p>
<h3>5.2. Challenge-specific comparison with Stump Tracks</h3>
<p>First, we aim to compare the different ACL methods on each of the six challenges we identified and listed in section 1. For this, we propose to leverage the Stump Tracks environment to create five experiments, each of them designed to highlight the ability of a teacher in one the first five ACL challenges (see appendix C for details):</p>
<ul>
<li>Mostly unfeasible task space: growing the possible maximum height of stumps, leading to almost $80 \%$ of unfeasible tasks.</li>
<li>Mostly trivial task space: allowing to sample stumps with negative height introducing $50 \%$ of new trivial tasks.</li>
<li>Forgetting student: resetting the DRL model twice throughout learning (i.e. every 7 Millions steps).</li>
<li>Diverse students: using multiple embodiments (short bipedal and spider) and DRL students (SAC and PPO).</li>
<li>Rugged difficulty landscape: Applying a random transformation to the task space such that feasible tasks are scattered across the space (i.e. among unfeasible ones).</li>
</ul>
<p>Additionally, in order to compare methods on the last challenge (i.e. the need of prior knowledge), we propose to perform each of our five experiments in three conditions:</p>
<ul>
<li>No expert knowledge: None of the prior knowledge listed in table 1 is given. Hence only methods not requiring it can run in this setup.</li>
<li>Low expert knowledge: Only reward mastery range information is accessible. We consider this as low prior knowledge as, while it requires some global knowledge about the task space, it does not require assumptions on the difficulty of specific subspaces of the task space.</li>
<li>High expert knowledge: All the expert knowledge listed in table 1 is given.</li>
</ul>
<p>Note that in the No expert knowledge and Low expert knowledge setups, SPDL (and ADR in Low expert knowledge) uses an initial task distribution randomly chosen as a subset of the task space. Moreover, in order to make a fair comparison in the High expert knowledge condition, we modified the vanilla version of Covar-GMM and ALP-GMM such that they can use an expert-given initial task distribution.</p>
<p>Using these 15 experiments ( 5 challenges in 3 expert knowledge setups), we here introduce what is, to our knowledge, the first unit-test like experiment of ACL methods, allowing one to compare teachers in each of the challenges we previously introduced. Moreover, performing each of the five experiments in three expert knowledge setups allows to show how the (un)availability of expert knowledge impacts performance for each method, which is hard to infer from each approach's original paper as they tend to focus only on the most ideal scenario. See appendix C for a detailed explanation of each experimental setup.</p>
<p>To conduct our analysis, each ACL method is used in 15 experiments with 32 seeds, except ADR, GoalGAN and Setter-Solver which cannot run in the No expert knowledge setup (i.e. only 10 experiments). We then calculate the aforementioned percentage of mastered test tasks on our test set (identical for all experiments), and average it over seeds. Performance results of all conditions can be visualized in figure 3 as a ratio of the Random teachers' performance, our lower-baseline.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. EK: Expert Knowledge. Post-training performance of each ACL method as a ratio of Random's results on multiple teaching challenges, done with 3 different expert knowledge levels. We use $\boldsymbol{\leftarrow}$ to show estimations of upper-bound performances in each challenge, except for Variety of students (see appendix D.1). On each axis, we indicate which method performed significantly better than Random ( $p&lt;0.05$ ) using colored stars matching each method's color (e.g. $\star$ for Covar-GMM, $\star$ for ADR). See appendix D. 2 for details.</p>
<p>Results We gather the results in figure 3 as well as in appendix D.2.</p>
<p>Expert-knowledge-free methods - Using these, one can see, first, that methods not requiring any expert knowledge (e.g. ALP-GMM or Covar-GMM) obtain very similar performances in No expert knowledge and in High expert knowledge setups (although expert knowledge does benefit them in terms of sample efficiency, see app. D. 2 for details). Comparing their performance without prior knowledge to the results obtained by other teachers when they have access to high expert knowledge shows how competitive expert-knowledge-free methods can be.</p>
<p>Expert knowledge dependency - The Low expert knowledge setup highlights the dependence of methods relying on an initial distribution of easy tasks (e.g. ADR and GoalGAN),
as it is not given in this scenario. As a result, in this setup, ADR obtains end performances not significantly different from Random in all challenges, and GoalGAN only outperforms Random in the mostly trivial task space ( $p&lt;0.05$ ). This has to be compared with their performance on the High expert knowledge setup, in which both approaches reach the top 3 results on $3 / 5$ challenges.
$A D R$ \&amp; GoalGAN - Both ADR and GoalGAN have one strong weakness in a challenge (Rugged difficulty for ADR and Forgetting student for GoalGAN) that lead them to a performance worse than Random (significantly for ADR with $p&lt;0.05$ ) in all expert knowledge setups. For ADR, it can be explained by the fact that its expansion can get stuck by subspaces of very hard (or unfeasible) difficulty, and for GoalGAN, by its inability to adapt quickly enough to the student's regressing capabilities because of its inertia</p>
<p>to update its sampling distribution (updating the buffer and training the GAN). We provide a more in-depth analysis of these two cases in appendix D.2.</p>
<p>SPDL - One can see that SPDL's performance seem very poor in our experimental setup: its end performance is significantly inferior to Random in 11/15 experiments ( $p&lt;0.05$ ). This can be explained by the fact that SPDL, by design, optimizes performance over a Gaussian target distribution, while our test set is uniformly sampled over the task space. See appendix A for details and potential fixes.</p>
<h3>5.3. Global performance analysis using the Parkour</h3>
<p>The second experiment we propose aims to more broadly benchmark ACL methods' performance in the Parkour environment, which features most of the previously discussed ACL challenges: 1) most tasks are unfeasible, 2) before each run, unknown to the teacher, the student's embodiment is uniformly sampled among three morphologies (bipedal walker, fish and chimpanzee), requiring the teacher to adapt curriculum generation to a diversity of student profiles, and 3) tasks are generated through a CCPN-based PCG, creating a rich task space with rugged difficulty landscape and hardly-definable prior knowledge (see appendix B).</p>
<p>We perform 48 seeded experiments (i.e. 16 seeds per morphology). To evaluate performance, three test sets were hand-designed (one per embodiment) such that each contains an even distribution between easy, medium and hard tasks. In terms of expert knowledge for teachers, we only give reward mastery range. Without straightforward initial easy task distribution to give to teachers requiring such knowledge (ADR and SPDL), we set it randomly over the space for each new run. See appendix C for details.</p>
<p>Results We present the evolution of performance of each teacher averaged over all seeds (and thus all embodiments) in figure 4 and gather the detailed results in appendix D.3. Interestingly, one can observe that best-performing methods do not use expert knowledge. This is explained by the fact that few prior knowledge is provided to the teachers in these experiments and, as shown in the challenge-specific experiments, most methods using expert knowledge heavily rely on them to reach high-performance. However, one can see that, while SPDL and Setter-Solver remain at the performance level of Random, GoalGAN's performance along training is (mostly) not significantly different from those of Covar-GMM and RIAC, two methods not relying on expert knowledge, as opposed to GoalGAN. On his side, ADR seems to plateau very fast and finally reach an average performance significantly worse than Random ( $p&lt;0.05$, see figure 14). Indeed, as the difficulty landscape of the Parkour environment is rugged, and the initial "easy" task distribution randomly set, ADR is unable to progressively
grow its sampling distribution towards feasible subspaces. Finally, when looking specifically to each embodiment type, results show the incapacity of all teachers to make the DRL student learn an efficient policy with the climbing morphology (i.e. at most $1 \%$ of mastered tasks by the end of training across all teachers), although we are able to show that highperforming policies can be learned when considering a subspace of the task space (see our case study in appendix D.3). This might be due to the complexity of learning the climbing gait w.r.t walking or swimming, as it requires for instance good coordination skills between the arms and the grasping actions. For the two other morphologies (bipedal walker and fish), results obtained are also low (respectively less than $60 \%$ and $50 \%$ ) and have a high variance (especially for the fish) considering that our test sets contain feasible tasks. This makes the Parkour environment an open challenge for future work on designing ACL algorithms.</p>
<h2>6. Open-Source release of TeachMyAgent</h2>
<p>With the open-source release of TeachMyAgent (version 1.0), we hope to provide a tool that can be used as a step towards thorough comparison and better understanding of current and future ACL methods. TeachMyAgent's documented repository features the code of our environments, embodiments, DRL students, as well as implementations of all ACL methods compared in this paper. All of these parts use APIs we provide such that one can easily add its ACL method, learning algorithm, and new embodiment or environment. We hope this will foster community-driven contributions to extend TeachMyAgent in order to broaden its impact and adapt it to the future of ACL. We also provide the code we used to reproduce our experiments, as well as Jupyter notebooks allowing to generate all the figures showed in this paper. Finally, we release the results of our benchmark, allowing one to load them and compare its ACL method against baselines without having to reproduce our large-scale experiments.</p>
<h2>7. Discussion and Conclusion</h2>
<p>In this article we presented TeachMyAgent 1.0, a first extensive testbed to design and compare ACL algorithms. It features unit-tests environments to assess the efficiency of a given teacher algorithm on multiple core skills and the Parkour environment, which provides a challenging teaching scenario that has yet to be solved. We used TeachMyAgent to conduct a comparative study of existing ACL algorithms. Throughout our experiments, we identified that 1) current ACL approaches not using expert knowledge matched and even outperformed (e.g. ALP-GMM) other approaches using high amounts of expert knowledge, and 2) the Parkour environment is far from solved, which makes it a good candidate as a testbed when designing new ACL approaches.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Averaged performance ( 48 seeds, with standard error of the mean) for each ACL method on Parkour. We calculate every 5 millions steps which method obtained statistically different $(p&lt;0.05)$ results from Random and indicate it with a star.</p>
<p>Limitations \&amp; future work. An obvious extension of this work is the addition of recent ACL approaches proposed during or after our experimental campaign (Zhang et al., 2020; Jiang et al., 2020). So far, all studied ACL algorithms struggled to detect feasible task subspaces in Parkour, hinting that more research is needed to improve the "progress niche detection" ability of current teacher algorithms.</p>
<p>TeachMyAgent currently only features environments with low-dimensional PCG systems. Designing new environments with higher-dimensional PCG, that might require to learn low dimensional representations on which to apply ACL algorithms, is an interesting avenue. Besides, our current list of environments only studies 2D locomotion tasks inspired by ALP-GMM's original paper (Portelas et al., 2019) as well as other works on Deep RL and 2D locomotion (Ha, 2019; Song et al., 2018; Gaier \&amp; Ha, 2019; Wang et al., 2019; 2020). While we put maximal effort in building a thorough and fair analysis of ACL methods, we believe extending TeachMyAgent with other environments (e.g. ProcGen (Cobbe et al., 2020), robotic manipulation) would make the benchmark even more informative.</p>
<p>Additionally, extending the benchmark to consider environment-conditioned goal selection (Racanière et al., 2020; Campero et al., 2020) - i.e. where teachers have to observe the initial episode state to infer admissible goals - is also worth investigating. TeachMyAgent provides a distribution of diverse learners. To this respect, it could also serve as a testbed for Meta ACL (Portelas et al., 2020b), i.e. algorithms learning to learn to teach across a sequence of students.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This work was supported by Microsoft Research through its PhD Scholarship Programme. Experiments presented in this paper were carried out using 1) the PlaFRIM experimental testbed, supported by Inria, CNRS (LABRI and IMB), Université de Bordeaux, Bordeaux INP and Conseil Régional d'Aquitaine (see https://www.plafrim.fr/), 2) the computing facilities MCIA (Mésocentre de Calcul Intensif Aquitain) of the Université de Bordeaux and of the Université de Pau et des Pays de l'Adour, and 3) the HPC resources of IDRIS under the allocation 2020-[A0091011996] made by GENCI.</p>
<h2>References</h2>
<p>Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein GAN. arXiv:1701.07875 [cs, stat], December 2017. arXiv: 1701.07875.</p>
<p>Baranes, A. and Oudeyer, P. R-IAC: robust intrinsically motivated exploration and active learning. IEEE Trans. Autonomous Mental Development, 1(3):155-169, 2009.</p>
<p>Beattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wainwright, M., Küttler, H., Lefrancq, A., Green, S., Valdés, V., Sadik, A., Schrittwieser, J., Anderson, K., York, S., Cant, M., Cain, A., Bolton, A., Gaffney, S., King, H., Hassabis, D., Legg, S., and Petersen, S. Deepmind lab. CoRR, abs/1612.03801, 2016.</p>
<p>Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, Jun 2013. ISSN 1076-9757.</p>
<p>Bengio, Y., Louradour, J., Collobert, R., and Weston, J. Curriculum learning. In ICML, 2009.</p>
<p>Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. CoRR, abs/1606.01540, 2016.</p>
<p>Burda, Y., Edwards, H., Storkey, A. J., and Klimov, O. Exploration by random network distillation. $I C L R, 2019$.</p>
<p>Campbell, C. Buoyancy - Box2D tutorials - iforce2d, 2013. URL https://www.iforce2d.net/ b2dtut/buoyancy.</p>
<p>Campero, A., Raileanu, R., Küttler, H., Tenenbaum, J. B., Rocktäschel, T., and Grefenstette, E. Learning with amigo: Adversarially motivated intrinsic goals. CoRR, abs/2006.12122, 2020.</p>
<p>Cangelosi, A. and Schlesinger, M. Developmental robotics: From babies to robots. MIT press, 2015.</p>
<p>Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y. Babyai: A platform to study the sample efficiency of grounded language learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.</p>
<p>Cobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman, J. Quantifying generalization in reinforcement learning. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 1282-1289. PMLR, 2019.</p>
<p>Cobbe, K., Hesse, C., Hilton, J., and Schulman, J. Leveraging procedural generation to benchmark reinforcement learning. ICLR, 2020.</p>
<p>Colas, C., Sigaud, O., and Oudeyer, P.-Y. How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement Learning Experiments. arXiv:1806.08295 [cs, stat], July 2018. arXiv: 1806.08295.</p>
<p>Colas, C., Oudeyer, P.-Y., Sigaud, O., Fournier, P., and Chetouani, M. Curious: Intrinsically motivated modular multi-goal reinforcement learning. In ICML, 2019.</p>
<p>Côté, M., Kádár, Á., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M. J., Asri, L. E., Adada, M., Tay, W., and Trischler, A. Textworld: A learning environment for text-based games. In Cazenave, T., Saffidine, A., and Sturtevant, N. (eds.), Computer Games - 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected</p>
<p>Papers, volume 1017 of Communications in Computer and Information Science, pp. 41-75. Springer, 2018.</p>
<p>Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density estimation using real NVP. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.</p>
<p>Elman, J. L. Learning and development in neural networks: the importance of starting small. Cognition, 48(1):71 99, 1993. ISSN 0010-0277.</p>
<p>Florensa, C., Held, D., Geng, X., and Abbeel, P. Automatic goal generation for reinforcement learning agents. In ICML, 2018.</p>
<p>Forestier, S., Mollard, Y., and Oudeyer, P. Intrinsically motivated goal exploration processes with automatic curriculum learning. CoRR, abs/1708.02190, 2017.</p>
<p>Gaier, A. and Ha, D. Weight agnostic neural networks. In Wallach, H., Larochelle, H., Beygelzimer, A., Alché-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.</p>
<p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative Adversarial Nets. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., and Weinberger, K. Q. (eds.), Advances in Neural Information Processing Systems 27, pp. 2672-2680. Curran Associates, Inc., 2014.</p>
<p>Graves, A., Bellemare, M. G., Menick, J., Munos, R., and Kavukcuoglu, K. Automated curriculum learning for neural networks. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 1311-1320. PMLR, 2017.</p>
<p>Ha, D. Generating abstract patterns with tensorflow. blog.otoro.net, 2016.</p>
<p>Ha, D. Reinforcement learning for improving agent design. Artif. Life, 25(4):352-365, 2019.</p>
<p>Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actorcritic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. ICML, 2018.</p>
<p>Hernandez, D., Denamganaï, K., Gao, Y., York, P., Devlin, S., Samothrakis, S., and Walker, J. A. A generalized framework for self-play training. In IEEE CoG, 2019.</p>
<p>Hernandez-Leal, P., Kartal, B., and Taylor, M. E. Is multiagent deep reinforcement learning the answer or the question? A brief survey. CoRR, abs/1810.05587, 2018.</p>
<p>Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</p>
<p>Hessel, M., Soyer, H., Espeholt, L., Czarnecki, W., Schmitt, S., and Van Hasselt, H. Multi-task deep reinforcement learning with popart. Proceedings of the AAAI Conference on Artificial Intelligence, 33:3796-3803, Jul 2019. ISSN 2159-5399.</p>
<p>Jabri, A., Hsu, K., Gupta, A., Eysenbach, B., Levine, S., and Finn, C. Unsupervised curricula for visual metareinforcement learning. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E. B., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 10519-10530, 2019.</p>
<p>Jiang, M., Grefenstette, E., and Rocktäschel, T. Prioritized level replay. CoRR, abs/2010.03934, 2020.</p>
<p>Justesen, N., Torrado, R. R., Bontrager, P., Khalifa, A., Togelius, J., and Risi, S. Illuminating generalization in deep reinforcement learning through procedural level generation. NeurIPS Deep RL Workshop, 2018.</p>
<p>Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D., and Hadsell, R. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526, 2017. ISSN 0027-8424.</p>
<p>Klink, P., D’Eramo, C., Peters, J., and Pajarinen, J. Selfpaced deep reinforcement learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.</p>
<p>Kovač, G., Laversanne-Finot, A., and Oudeyer, P.-Y. Grimgep: Learning progress for robust goal sampling in visual deep reinforcement learning. CoRR, abs/2008.04388, 2020.</p>
<p>Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradientbased learning applied to document recognition. In Proceedings of the IEEE, pp. 2278-2324, 1998.</p>
<p>Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. In $I C L R, 2016$.</p>
<p>Matiisen, T., Oliver, A., Cohen, T., and Schulman, J. Teacher-student curriculum learning. IEEE TNNLS, 2017.</p>
<p>Mehta, B., Diaz, M., Golemo, F., Pal, C. J., and Paull, L. Active domain randomization. CoRL, 2019.</p>
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. Nature, 518(7540): 529, 2015.</p>
<p>Moulin-Frier, C., Nguyen, S. M., and Oudeyer, P.-Y. Selforganization of early vocal development in infants and machines: The role of intrinsic motivation. Front. in Psych. (Cog. Science), 2014.</p>
<p>Mysore, S., Platt, R., and Saenko, K. Reward-guided curriculum for robust reinforcement learning. Workshop on Multi-task and Lifelong Reinforcement Learning at ICML, 2019.</p>
<p>Nichol, A., Pfau, V., Hesse, C., Klimov, O., and Schulman, J. Gotta learn fast: A new benchmark for generalization in RL. CoRR, abs/1804.03720, 2018.</p>
<p>OpenAI, Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A., Paino, A., Plappert, M., Powell, G., Ribas, R., Schneider, J., Tezak, N., Tworek, J., Welinder, P., Weng, L., Yuan, Q., Zaremba, W., and Zhang, L. Solving rubik's cube with a robot hand. CoRR, abs/1910.07113, 2019.</p>
<p>Osband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener, E., Saraiva, A., McKinney, K., Lattimore, T., Szepesvari, C., Singh, S., Roy, B. V., Sutton, R., Silver, D., and Hasselt, H. V. Behaviour suite for reinforcement learning. ICML, 2020.</p>
<p>Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. Curiosity-driven exploration by self-supervised prediction. In CVPR, 2017.</p>
<p>Peng, X. B., Andrychowicz, M., Zaremba, W., and Abbeel, P. Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE International Conference on Robotics and Automation, ICRA 2018, Brisbane, Australia, May 21-25, 2018, pp. 1-8. IEEE, 2018.</p>
<p>Pong, V., Dalal, M., Lin, S., Nair, A., Bahl, S., and Levine, S. Skew-fit: State-covering self-supervised reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 7783-7792. PMLR, 2020.</p>
<p>Portelas, R., Colas, C., Hofmann, K., and Oudeyer, P.-Y. Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments. CoRL, 2019.</p>
<p>Portelas, R., Colas, C., Weng, L., Hofmann, K., and Oudeyer, P.-Y. Automatic curriculum learning for deep rl: A short survey. IJCAI, 2020a.</p>
<p>Portelas, R., Romac, C., Hofmann, K., and Oudeyer, P. Meta automatic curriculum learning. CoRR, abs/2011.08463, 2020b.</p>
<p>Racanière, S., Lampinen, A., Santoro, A., Reichert, D., Firoiu, V., and Lillicrap, T. Automated curricula through setter-solver interactions. ICLR, 2020.</p>
<p>Rajeswaran, A., Ghotra, S., Ravindran, B., and Levine, S. Epopt: Learning robust neural network policies using model ensembles. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.</p>
<p>Risi, S. and Togelius, J. Procedural content generation: From automatically generating game levels to increasing generality in machine learning. CoRR, abs/1911.13071, 2019.</p>
<p>Salimans, T. and Chen, R. Learning montezuma's revenge from a single demonstration. NeurIPS, 2018.</p>
<p>Schaul, T., Horgan, D., Gregor, K., and Silver, D. Universal value function approximators. In ICML, 2015.</p>
<p>Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.</p>
<p>Selfridge, O. G., Sutton, R. S., and Barto, A. G. Training and tracking in robotics. In IJCAI, 1985.</p>
<p>Song, D. R., Yang, C., McGreavy, C., and Li, Z. Recurrent deterministic policy gradient method for bipedal locomotion on rough terrain challenge. In 2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV), pp. 311-318, 2018. doi: 10.1109/ ICARCV.2018.8581309.</p>
<p>Stanley, K. O. Compositional pattern producing networks: A novel abstraction of development. Genet. Program. Evolvable Mach., 8(2):131-162, 2007.</p>
<p>Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2017, Vancouver, BC, Canada, September 24-28, 2017, pp. 23-30. IEEE, 2017.</p>
<p>Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033, 2012.</p>
<p>Vanschoren, J. Meta-learning: A survey. CoRR, abs/1810.03548, 2018.</p>
<p>Wang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired open-ended trailblazer (POET): endlessly generating increasingly complex and diverse learning environments and their solutions. CoRR, abs/1901.01753, 2019.</p>
<p>Wang, R., Lehman, J., Rawal, A., Zhi, J., Li, Y., Clune, J., and Stanley, K. O. Enhanced POET: open-ended reinforcement learning through unbounded invention of learning challenges and their solutions. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 9940-9951. PMLR, 2020.</p>
<p>Zhang, C., Vinyals, O., Munos, R., and Bengio, S. A study on overfitting in deep reinforcement learning. CoRR, abs/1804.06893, 2018.</p>
<p>Zhang, Y., Abbeel, P., and Pinto, L. Automatic curriculum learning through value disagreement. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.</p>
<h1>A. Details on ACL baselines</h1>
<p>In this section, we give details about our implementations of ACL methods, as well as their hyperparameters tuning.</p>
<h2>A.1. Implementation details</h2>
<p>Random We use as baseline a random teacher, which samples tasks using a uniform distribution over the task space.</p>
<p>ADR OpenAI et al. (2019) introduced Automatic Domain Randomization (ADR), an ACL method relying on the idea of Domain Randomization (Tobin et al., 2017; Peng et al., 2018). Instead of sampling tasks over the whole task space, ADR starts from a distribution centered on a single example easy for the student and progressively grows the distribution according to the learning agent's performance. Using this mechanism, it increases the difficulty of the tasks proposed to the student while still sampling in previously seen regions in order to try reducing potential forgetting.
This sampling distribution $P_{\phi}$ is parameterized by $\phi \in \mathbb{R}^{2 d}$ (with $d$ the number of dimensions of the task space). For each dimension, a lower and upper boundary are set $\phi=\left{\phi_{i}^{L}, \phi_{i}^{H}\right}_{i=1}^{d}$ allowing to sample uniformly on each dimension using these boundaries and obtain a task $\lambda$ :</p>
<p>$$
P_{\phi}(\lambda)=\prod_{i=1}^{d} U\left(\phi_{i}^{L}, \phi_{i}^{H}\right)
$$</p>
<p>At the beginning, $\phi$ is centered on a single example (i.e. $\phi_{i}^{L}=\phi_{i}^{H} \forall i$ ). Then, at each episode, 1) ADR starts by sampling a new task $\lambda \sim P_{\phi}$. Following this, 2) ADR chooses with a probability $p_{b}$ whether to modify $\lambda$ in order to explore the task space or not. It thus samples a value $\epsilon$ uniformly in $[0 ; 1]$ and checks whether $\epsilon&lt;p_{b}$. If this is not the case, ADR simply sends $\lambda$ to the environment.</p>
<p>Otherwise, 3) ADR selects uniformly one of the dimensions of the task space, which we will call $j$ as an example. Following this, 4) one of the two boundaries $\phi_{j}^{L}$ or $\phi_{j}^{H}$ is selected ( $50 \%$ chances for each boundary). Finally, 5) ADR replaces the $j$-th value of $\lambda$ by the selected boundary and sends $\lambda$ to the environment.
Moreover, ADR keeps a buffer $D_{i}^{L}$ and $D_{i}^{H}$ for each dimension $i$ in the task space. Every time $\epsilon$ is greater than $p_{b}$ and a value of $\lambda$ is replaced by one of the selected boundary, ADR stores the episodic reward obtained at the end of the episode in the buffer associated to the selected boundary (e.g. the episodic reward is stored in $D_{k}^{L}$ if the $k$-th value of lambda was replaced by $\phi_{k}^{L}$ ).
Every time one of the buffers' size reaches $m$, the average $\bar{p}$ of episodic reward stored is calculated. Then, $\bar{p}$ is compared to two thresholds $t_{L}$ and $t_{H}$ (being hyperparameters of ADR) in order to know whether the boundary associated to the buffer must be reduced or increased.
As an example, let's say that $D_{k}^{L}$ 's size reached $m$, meaning that $\phi_{k}^{L}$ is the associated dimension (i.e. a $\lambda$ sampled got its $k$-th value replaced by $\phi_{k}^{L} m$ times). Its average episodic reward $\bar{p}$ is calculated. It is first compared to $t_{L}$ and, if $\bar{p}<t_{l}, \phi_{k}^{L}$ is increased by $\Delta$ (as $\phi_{k}^{L}$ is a lower boundary, this means that the task space is reduced). Similarly, if $\bar{p}>t_{l}, \phi_{k}^{L}$ is decreased by $\Delta$ (expanding the task space).
If instead of $D_{k}^{L}$ we take $D_{k}^{H}$, our task space has to be expanded or reduced in the same way: if $\bar{p}<t_{L}$ then $\phi_{k}^{H}$ is reduced by $\Delta$ (as it is now an upper boundary of the task space) and if $\bar{p}>t_{H}$ then $\phi_{k}^{H}$ is increased by $\Delta$. Finally, note that whenever one buffer's size reaches $m$, it is then emptied.</p>
<p>As no implementation was provided by the authors, we propose here an implementation being as close as possible to the algorithms given in OpenAI et al. (2019).</p>
<p>RIAC Proposed in Baranes \&amp; Oudeyer (2009), Robust Intelligent Adaptive Curiosity is based on the recursive splitting of the task space in hyperboxes, called regions. One region is split in two whenever a pre-defined number $\max <em d="d">{s}$ of sampled tasks originate from the region. The split value is chosen such that there is maximal Learning Progress (LP) difference between the two regions, while maintaining a size $\min </em>$ (i.e. a ratio of the size of the whole task space) for each region. The number of possible split to attempt is parameterized by $n$. We reuse the implementation and the value of the hyperparameters not mentioned here from Portelas et al. (2019). RIAC does not require expert knowledge.</p>
<p>Covar-GMM Covar-GMM was proposed in Moulin-Frier et al. (2014). As for RIAC, it does not require expert knowledge and is based on learning progress. The core idea of Covar-GMM is to fit a Gaussian Mixture Model (of maximum size $\max <em p="p">{k}$ ) every $n$ episodes on recently sampled tasks concatenated with both a time dimension and a competence dimension. The Gaussian from which to sample a new task is then chosen proportionally to its respective learning progress, defined as the positive correlation between time and competence. Additionally, in order to preserve exploration, Covar-GMM has a probability $r</em>$ of sampling a task uniformly random instead of using one of its Gaussians. We use the implementation and hyperparameters from Portelas et al. (2019) which uses Absolute Learning Progress (ALP) instead of LP.</p>
<p>Moreover, as aforementioned in section 5, we modified the implementation to make it use expert knowledge (i.e. an initial distribution) when provided. Hence, instead of uniformly sampling tasks over the whole task space during the bootstrap phase at the beginning of training, Covar-GMM samples tasks from an initial Gaussian distribution of tasks provided by the expert.</p>
<p>ALP-GMM ALP-GMM is an ACL algorithm inspired from Covar-GMM, proposed in Portelas et al. (2019). Instead of relying on time competence correlation, which only allows to compute ALP over a single GMM fit, it computes a per-task ALP from the entire history of sampled tasks using a knn-based approach similar to those proposed in Forestier et al. (2017). Recent tasks are periodically used to fit a GMM on recently sampled tasks concatenated with their respective ALP value. The Gaussian from which to sample is then selected based on its mean ALP dimension. ALP-GMM does not require expert knowledge and has the same hyperparameters as Covar-GMM. We reused the implementation and hyperparameters (except $\max <em p="p">{k}, n$ and $r</em>$ ) provided by Portelas et al. (2019).</p>
<p>Additionally, as for Covar-GMM, we added the possibility to ALP-GMM to bootstrap tasks for an initial Gaussian distribution if the latter is provided, instead of uniformly bootstrapping tasks.</p>
<p>Goal-GAN Another teacher algorithm we included in this benchmark is called GoalGAN, and relies on the idea of sampling goals (i.e. states to reach in the environment) where the agent performs neither too well nor to badly, called Goals Of Intermediate Difficulty (GOID). However, as this goal generation introduces a curriculum in the agent's learning, one can see the goal selection process as a task selection process. We will thus call them tasks instead of goals in the following description. For sampling, Florensa et al. (2018) proposed to use a modified version of a Generative Adversarial Network (GAN) (Goodfellow et al., 2014) where the generator network is used to generate tasks for the student given a random noise, and the discriminator is trained to classify whether these tasks are of "intermediate difficulty". To define such an "intermediate difficulty", GoalGAN uses a binary reward signal defining whether the student succeeded in the proposed task. As our environments return scalar rewards, this implies a function interpreter hand-designed by an expert (in our case we set a threshold on the scalar reward, as explained in appendix C). For each task sampled, the teacher proposes it multiple times ( $n_{\text {rollouts }}$ ) to the student and then calculates the average of successes obtained (lying in $[0 ; 1]$ ). Using a lower threshold $R_{\text {min }}$ and an upper threshold $R_{\text {max }}$, GoalGAN calculates if the average lies in this interval of tasks neither too easy (with an average of successes very high) nor too hard (with an average of successes very low). If this is the case, this task is labelled as 1 for the discriminator ( 0 otherwise). This new task is then stored in a buffer (except if it already exists in the buffer a task at an euclidean distance smaller than $\epsilon$ from our new task). Every time a task has to be sampled, in order to prevent the GAN from forgetting previously seen GOIDs, the algorithm has the probability $p_{\text {old }}$ of uniformly sampling from the buffer instead of using the GAN. Finally, the GAN is trained using the tasks previously sampled every $n$ episodes.</p>
<p>Note that, in order to help the GAN to generate tasks in a feasible subspace of the task space at the beginning of training, GoalGAN can also pretrain its GAN using trivial tasks. In the original paper, as tasks are states, authors proposed to use the student to interact with the environment for a few steps, and use collected states as achievable tasks. However, in our case, this is not possible. We thus chose to reuse the same trick as the one in (Klink et al., 2020), that uses an initial Gaussian distribution to sample tasks and label them as positives (i.e. tasks of intermediate difficulty) in order to pretrain the GAN with them. See appendix C for the way we designed this initial distribution.</p>
<p>We reused and wrapped the version ${ }^{5}$ of GoalGAN implemented by Klink et al. (2020), which is a slightly modified implementation of the original one made by Florensa et al. (2018). Our generator network takes an input that has the same number of dimensions as our task space, and uses two layers of 256 neurons with ReLU activation (and TanH activation for the last layer). Our discriminator uses two layers of 128 neurons. For $\epsilon$, we used a distance of $10 \%$ on each dimension of the task space. As per Florensa et al. (2018), we set $R_{\text {min }}$ to 0.25 and $R_{\max }$ to 0.75 . Finally, as in the implementation made by</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Klink et al. (2020), we set the amount of noise $\delta$ added to each goal sampled by the generator network as a proportion of the size of the task space.</p>
<p>Self-Paced Proposed by Klink et al. (2020), Self-Paced Deep Reinforcement Learning (SPDL) samples tasks from a distribution that progressively moves towards a target distribution. The intuition behind it can be seen as similar to the one behind ADR, as the idea is to start from an initial task space and progressively shift it towards a target space, while adapting the pace to the agent's performance. However here, all task distributions (initial, current and target) are Gaussian distributions. SPDL thus maintains a current task distribution from which it samples tasks and changes it over training. This distribution shift is seen as an optimization problem using a dual objective maximizing the agent's performance over the current task space, while minimizing the Kullback-Leibler (KL) divergence between the current task distribution and the target task distribution. This forces the task selection function to propose tasks where the agent performs well while progressively going towards the target task space.</p>
<p>Initially designed for non-episodic RL setups, SPDL, unlike all our other teachers, receives information at every step of the student in the environment. After an offset of $n_{\mathrm{OFFSET}}$ first steps, and then every $n_{\mathrm{STEP}}$ steps, the algorithm estimates the expected return for the task sampled $\mathrm{E}_{p(c)}[J(\pi, c)]$ using the value estimator function of the current student (with $p(c)$ the current task distribution, $\pi$ the current policy of the student, and $J(\pi, c)$ the expected return for the task $c$ with policy $\pi$ ).</p>
<p>With this, SPDL updates its current sampling distribution in order to maximize the following objective w.r.t. the current task distribution $p(c)$ :</p>
<p>$$
\max <em p_c_="p(c)">{p(c)} \mathrm{E}</em>[J(\pi, c)]
$$</p>
<p>Additionally, a penalty term is added to this objective function, such that the KL divergence between $p(c)$ and the target distribution $\mu(c)$ is minimized. This penalty term is controlled by an $\alpha$ parameter automatically adjusted. This parameter is first set to 0 for $K_{\alpha}$ optimization steps and is then adjusted in order to maintain a constant proportion $\zeta$ between the KL divergence penalty and the expected reward term (see Klink et al. (2020) for more details on the way $\alpha$ is calculated). This optimization step is made such that the shift of distribution is not bigger than $\epsilon$ (i.e. s.t. $D_{\mathrm{KL}}(p(c) | q(c)) \leq \epsilon$ with a shift from $p(c)$ to $q(c)$ ).</p>
<p>We reused the same implementation made by Klink et al. (2020) and wrapped it to our teacher architecture. However, as shown in section 5, using a Gaussian target distribution does not match with our Stump Tracks test set where tasks are uniformly sampled over the whole task space. In order to solve this issue, some adaptations to its architecture could be explored (e.g. using a truncated Gaussian as target distribution to get closer to a uniform distribution). While not provided yet in TeachMyAgent, we are currently working along with SPDL's authors on these modifications in order to show a fairer comparison of this promising method.</p>
<p>For the value estimators, we used the value network of both our PPO and SAC implementations (with the value network sharing its weights with the policy network for PPO). For the calculation of $\alpha$, we chose to use the average reward, as in the experiments of Klink et al. (2020). We did not use the lower bound restriction on the standard deviation of the task distribution $\sigma_{\mathrm{LB}}$ proposed in Klink et al. (2020) as our target distributions were very large (see appendix C).</p>
<p>Setter-Solver Finally, the last ACL algorithm we implemented here is Setter-Solver (Racanière et al., 2020). In a very similar way to Goal-GAN, this method uses two neural networks: a Judge (replacing the discriminator) and a Setter (replacing the generator) outputting a task given a feasibility scalar in $[0 ; 1]$. During the training, the Judge is trained to output the right feasibility given a task sampled, and is used in the Setter's losses to encourage the latter to sample tasks where the predicted feasibility was close to the real one. The Setter is also trained to sample tasks the student has succeeded (i.e. using a binary reward signal as Goal-GAN) while maximizing an entropy criterion encouraging it to sample diverse tasks.</p>
<p>For the implementation, Racanière et al. (2020) provided code to help reproducibility that implements both the Setter and Judge, but did not include neither losses nor optimization functions. Therefore, we provide here our own implementation of the full Setter-Solver algorithm trying to be as close as possible to the paper's details. We reused the code provided for the two neural networks and modified it to add losses, optimizers, and some modifications to better integrate it to our architecture. We kept the tricks added in the code provided by authors that uses a non-zero uniform function to sample the feasibility and a clipped sigmoid in the Setter's output. Concerning the generator network, we kept the hyperparameters of the paper (i.e. a RNVP (Dinh et al., 2017) with three blocks of three layers) except the size of hidden layers $n_{\text {HIDDEN }}$ that</p>
<p>we optimized. We also reused the three layers of 64 neurons architecture for the Judge as per the paper. Note that we used an Adam optimizer with a learning rate of $3 \cdot 10^{-4}$ for both the Setter and the Judge, while this was not precised for the Judge in Racanière et al. (2020). We optimized the upper bound $\delta$ of the uniformly sampled noise that is added to succeeded tasks in the validity Setter's loss, as well as the update frequency $n$.</p>
<p>We did not use the conditioned version of the Setter or Judge. Indeed, first we generate the task before obtaining the first observation in our case as opposed to Racanière et al. (2020), and also because the first observation of an embodiment is always the same as both our environments have a startpad (see appendix B). Finally, we did not use the additional target distribution (called desired goal distribution in the original paper) loss that use a Wassertein discriminator (Arjovsky et al., 2017) to predict whether a task predicted belongs to the target distribution. Indeed, as shown in Racanière et al. (2020), using the targeted version of Setter-Solver offers more sample efficiency but leads to similar final results. Moreover, in our case, a target distribution is known only in the High expert knowledge setup of the challenge-specific experiments, in addition of having this part not implemented at all in the code provided by authors. We thus leave this upgrade to future work.</p>
<h1>A.2. Hyperparameters tuning</h1>
<p>In order to tune the different ACL methods to our experiments, we chose to perform a grid-search using our Stump Tracks environment with its original task space. As the Parkour is partly extended from it, in addition of the challenge-specific experiments, this environment offered us an appropriate setup. Each point sampled in the grid-search was trained for 7 million steps (instead of the 20 millions used in our experiments) with 16 seeds in order to reduce the (already high) computational cost. At the end of training, we calculated the percentage of mastered tasks on test set for each seed. The combination of hyperparameters having the best average over its seeds was chosen as the configuration for the benchmark.</p>
<p>In order to make the grid-search as fair as possible between the different ACL methods, given that the number of hyperparameters differs from one method to another, we sampled the same number of points for each teacher: $70( \pm 10)$. The hyperparameters to tune for each teacher, as well as their values, were chosen following the recommendations given by their original paper.</p>
<p>Moreover, we chose to tune the teachers in what we call their "original" expert knowledge version (i.e. they have access to the same amount of prior knowledge as the one they used in their paper). Hence, teachers requiring expert knowledge use our high expert knowledge setup, and algorithms such as ALP-GMM use no expert knowledge.</p>
<p>Table 2 shows the values we tested for each hyperparameter and the combinations that obtained the best result.</p>
<p>Table 2. Hyperparameters tuning of the ACL methods.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ACL METHOD</th>
<th style="text-align: center;">HYPERPARAMETER</th>
<th style="text-align: center;">POSSIBLE VALUES</th>
<th style="text-align: center;">BEST VALUE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ADR</td>
<td style="text-align: center;">$t_{L}$</td>
<td style="text-align: center;">$[0,50]$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">ADR</td>
<td style="text-align: center;">$t_{H}$</td>
<td style="text-align: center;">$[180,230,280]$</td>
<td style="text-align: center;">180</td>
</tr>
<tr>
<td style="text-align: center;">ADR</td>
<td style="text-align: center;">$p_{b}$</td>
<td style="text-align: center;">$[0.3,0.5,0.7]$</td>
<td style="text-align: center;">0.7</td>
</tr>
<tr>
<td style="text-align: center;">ADR</td>
<td style="text-align: center;">$m$</td>
<td style="text-align: center;">$[10,20]$</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">ADR</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">$[0.05,0.1]$</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;">RIAC</td>
<td style="text-align: center;">$\max _{s}$</td>
<td style="text-align: center;">$[50,150,250,350]$</td>
<td style="text-align: center;">150</td>
</tr>
<tr>
<td style="text-align: center;">RIAC</td>
<td style="text-align: center;">$n$</td>
<td style="text-align: center;">$[25,50,75,100]$</td>
<td style="text-align: center;">75</td>
</tr>
<tr>
<td style="text-align: center;">RIAC</td>
<td style="text-align: center;">$\min _{d}$</td>
<td style="text-align: center;">$\left[0.0677,0.1,0.1677,0.2\right]$</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;">COVAR-GMM</td>
<td style="text-align: center;">$n$</td>
<td style="text-align: center;">$[50,150,250,350]$</td>
<td style="text-align: center;">150</td>
</tr>
<tr>
<td style="text-align: center;">COVAR-GMM</td>
<td style="text-align: center;">$\max _{k}$</td>
<td style="text-align: center;">$[5,10,15,20]$</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: center;">COVAR-GMM</td>
<td style="text-align: center;">$r_{p}$</td>
<td style="text-align: center;">$\left[0.05,0.1,0.2,0.3\right]$</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;">ALP-GMM</td>
<td style="text-align: center;">$n$</td>
<td style="text-align: center;">$[50,150,250,350]$</td>
<td style="text-align: center;">150</td>
</tr>
<tr>
<td style="text-align: center;">ALP-GMM</td>
<td style="text-align: center;">$\max _{k}$</td>
<td style="text-align: center;">$[5,10,15,20]$</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">ALP-GMM</td>
<td style="text-align: center;">$r_{p}$</td>
<td style="text-align: center;">$\left[0.05,0.1,0.2,0.3\right]$</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: center;">GoalGAN</td>
<td style="text-align: center;">$\delta$</td>
<td style="text-align: center;">$\left[0.01,0.05,0.1\right]$</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">GoalGAN</td>
<td style="text-align: center;">$n$</td>
<td style="text-align: center;">$[100,200,300]$</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">GoalGAN</td>
<td style="text-align: center;">$p_{\text {old }}$</td>
<td style="text-align: center;">$[0.1,0.2,03]$</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: center;">GoalGAN</td>
<td style="text-align: center;">$n_{\text {rollouts }}$</td>
<td style="text-align: center;">$[2,5,10]$</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">SPDL</td>
<td style="text-align: center;">$n_{\text {OFFSET }}$</td>
<td style="text-align: center;">$\left[100000,200000\right]$</td>
<td style="text-align: center;">200000</td>
</tr>
<tr>
<td style="text-align: center;">SPDL</td>
<td style="text-align: center;">$n_{\text {STEP }}$</td>
<td style="text-align: center;">$\left[50000,100000\right]$</td>
<td style="text-align: center;">100000</td>
</tr>
<tr>
<td style="text-align: center;">SPDL</td>
<td style="text-align: center;">$K_{\alpha}$</td>
<td style="text-align: center;">$[0,5,10]$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">SPDL</td>
<td style="text-align: center;">$\zeta$</td>
<td style="text-align: center;">$\left[0.05,0.25,0.5\right]$</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: center;">SPDL</td>
<td style="text-align: center;">$\epsilon$</td>
<td style="text-align: center;">$[0.1,0.8]$</td>
<td style="text-align: center;">0.8</td>
</tr>
<tr>
<td style="text-align: center;">SETTER-SOLVER</td>
<td style="text-align: center;">$n$</td>
<td style="text-align: center;">$[50,100,200,300]$</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">SETTER-SOLVER</td>
<td style="text-align: center;">$\delta$</td>
<td style="text-align: center;">$\left[0.005,0.01,0.05,0.1\right]$</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: center;">SETTER-SOLVER</td>
<td style="text-align: center;">$n_{\text {HIDDEN }}$</td>
<td style="text-align: center;">$[64,128,256,512]$</td>
<td style="text-align: center;">128</td>
</tr>
</tbody>
</table>
<h1>B. Environment details</h1>
<p>In this section, we give details about our two environments, their PCG algorithm, as well as some analysis about their task space. Note that our two environments follow the OpenAI Gym's interface and provide after each step, in addition of usual information (observation, reward, and whether the episode terminated), a binary value set to 1 if the cumulative reward of the episode reached 230 . Additionally, we provide extra information and videos of our environments and embodiments, as well as policies learned at at http://developmentalsystems.org/TeachMyAgent/.</p>
<h2>B.1. Stump Tracks</h2>
<p>We present the Stump Tracks environment, an extended version of the environment introduced by Portelas et al. (2019). We only use two of the initially introduced dimensions of the procedural generation of task: stumps' height $\mu_{s}$ and spacing $\Delta_{s} \mathrm{~s}$. As in Portelas et al. (2019), $\mu_{s}$ is used as the mean of a Gaussian distribution with standard deviation 0.1 . Each stump has thus its height sampled from this Gaussian distribution and is placed at distance $\Delta_{s}$ from the previous one. We bound differently this task space depending on the experiment we perform, as explained in appendix C.</p>
<p>We kept the same observation space with 10 values indicating distance of the next object detected by lidars, head angle and velocity (linear and angular), as well as information from the embodiment (angle and speed of joints and also whether the lower limbs have contact with the ground). For information concerning the embodiment, the size of observation depends on the embodiment, as the number of joints varies (see below in B.3). We also kept the action space controlling joints with a torque.</p>
<h2>B.2. Parkour</h2>
<p>We introduce the Parkour, a Box2D parkour track inspired from the Stump Tracks and the environment introduced in Wang et al. (2020). It features different milieu in a complex task space.</p>
<h2>B.2.1. Procedural GENERATION</h2>
<p>CPPN-encoded terrain First, similarly to the Stump Tracks, our Parkour features a ground (that has the same length as the one in Stump Tracks) where the agent starts at the leftmost side and has to reach the rightmost side. However, this ground is no longer flat and rather, as in Wang et al. (2020), generated using a function outputted by a neural network called CPPN (Stanley, 2007). This network takes in input a $x$ position and outputs the associated $y$ position of the ground. Using this, one can slide the CPPN over the possible $x$ positions of the track in order to obtain the terrain. This method has the advantage of being able to easily generate non-linear and very diverse terrains as shown in Wang et al. (2020), while being light and fast to use as this only needs inference from the network. While CPPNs are usually used in an evolutionary setup where the architecture and weights are mutated, we chose here to rather initialize an arbitrary architecture and random weights and keep them fixed. For this architecture, we chose to use a four layers feedforward neural network with 64 units per layer and an alternation of TanH and Softplus activations (except for the output head which uses a linear activation) inspired from Ha (2016). Weights were sampled from a Gaussian distribution with mean 0 and standard deviation of 1 . In addition of its $x$ input, we added to our network three inputs that are set before generating the terrain as parameters controlling the generation. This vector $\theta$ of size 3 acts in a similar way as noise vector does in GANs for instance. Its size was chosen such that it allows to analyse the generation space and maintain the overall task space's number of dimensions quite small. As for the parameters in Stump Tracks, we bounded the space of values an ACL method could sample in $\theta$. For this, we provide three hand-designed setups (easy, medium and hard) differing from the size of the resulting task space and the amount of feasible tasks in it (see appendix C.4).</p>
<p>Moreover, in addition of the $y$ output of the ground, we added another output head $y_{c}$ in order to create a ceiling in our tracks. As in Stump Tracks, the terrain starts with a flat startpad region (with a fixed distance between the ground and the ceiling) where the agent appears. Once $Y=\left(y_{i}\right)<em c="c">{i \in X}$ and $Y</em>$ generated by the CPPN, with $X$ all the possible $x$ positions in the track, we align them to their respective startpad:}=\left(y_{c_{i}}\right)_{i \in X</p>
<p>$$
\begin{gathered}
y_{i}=y_{i}+\operatorname{startpad}<em 0="0">{g}-y</em> \forall i \in Y \
y_{c_{i}}=y_{c_{i}}+\operatorname{startpad}<em c__0="c_{0">{c}-y</em>
\end{gathered}
$$}} \forall i \in Y_{c</p>
<p>with $\operatorname{startpad}<em c="c">{g}, \operatorname{startpad}</em>$ respectively the first $y$ position of the ground and the ceiling outputted by our CPPN.}$ being respectively the $y$ position of the ground startpad and ceiling startpad, and $y_{0}, y_{c_{0}</p>
<p>Using this non-linear generator (i.e. CPPN) allows us to have an input space where the difficulty landscape of the task space is rugged. Indeed, in addition of generating two non-linear functions for our ground and ceiling, the two latter can cross each other, creating unfeasible tasks (see figure 6). Additionally, our CPPN also makes the definition of prior knowledge over the input space more complex, as shown in figure 5.</p>
<p>Finally, as shown in figure 6, we smoothed the values of $Y$ and $Y_{c}$ by a parameter $\delta(=10$ in the training distribution) in order to make the roughness of the terrains adapted to our embodiments.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Overview of the input space of $\theta$. First, in a) one can see the function generated when all the values of the input vector are set to zero. Secondly, in b) we can see that small changes over the space lead to similar functions and that big changes lead to very different results, showing that local similarity is maintained over the task space. Finally, c) shows how the difficulty landscape of $\theta$ can be rugged, as moving along the second dimension leads to terrains having a very different difficulty level.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Here are some examples of generated tasks in the Parkour environment. While most of them seem too hard for a classic bipedal walker, the bottom left task is a good example of an unfeasible task, no matter which embodiment is used.</p>
<p>Creepers Once the terrain generated, we add what we call "creepers". Similarly to the stumps, we create objects at distance $\Delta_{c}$ from one another and of height sampled using a Gaussian distribution of mean $\mu_{c}$ and standard deviation 0.1 (the width can also be controlled but was fixed to 0.25 in our experiments). However, creepers are not obstacles for agents as stumps but rather graspable objects that embodiments can go through. Moreover, even though not used in our experiments, we provide the possibility to make creepers more realistic by dividing every creeper in multiple rectangles of height at most 1 linked with a rotating joint. As shown on our website, this creates creepers on which the climbers can swing.</p>
<p>Water Finally, we added a last dimension to our task space controlling the "water" level. Water is simulated using a rectangle object that the agent can go through and in which physics change (see below). This rectangle's width equals the terrain's width and its height is controlled by a parameter $\tau \in[0 ; 1]$ with 0 being an arbitrary lower limit the ground can reach and 1 the highest point of the current ceiling (generated by the CPPN for the current task).</p>
<h1>B.2.2. PhYsics</h1>
<p>As previously mentioned, we introduced creepers and water along with new physics. First, in order to make our creepers graspable by the agents, we added sensors to the end of limb of certain embodiments (see section B. 3 below). Every time one of these sensors enters in contact with a creeper, we look at the action in the action space of the agent that is associated to this sensor. If its value is greater than 0 , we create a rotational joint between the sensor and the creeper at the contact point. As long as this action is greater than 0 , the joint remains. As soon as the action goes negative or equals 0 , we delete the joint (releasing the agent's limb from the creeper) and start watching again for contact. Note that, in order to better see whether the agent grasps a creeper, we color its sensors in red when a joint exists and in yellow otherwise (see our website). Additionally, in order to help the learning agent, we also make the ceiling graspable.</p>
<p>Secondly, concerning the water, we simulated a buoyancy force (inspired from Campbell (2013)) when an object enters water given its density compared to the water's density (set to 1). In addition, we implemented a "drag" and a "lift" force that simulate the resistance applied when an object moves in water and slows down the movement. Finally, we added a "push" force applied to an object having an angular velocity. This force simulates the fact that applying a torque to a rotational joint makes the object attached to the joint "push" the water and move (i.e. have a linear force applied). With these forces, we were able to simulate in a simplified way some physics of water, which resulted in very natural policies learned from our agents (see our website).</p>
<p>Finally, we simulated the fact that each embodiment is suited for one (or several) milieu, creating types of agents. Indeed, we first consider swimming agents that die (i.e. the actions sent by the DRL student to the environment no longer have effects on the motors of the embodiment) after spending more than 600 consecutive steps outside water. On the contrary, the two other types named climbers and walkers cannot survive underwater more than 600 consecutive steps. Both walkers and swimmers are allowed to have collisions with their body (including their head in the Parkour), whereas climbers are not allowed to touch the ground with any part of their body. Note that, while walkers appear with their legs touching the ground, swimmers appear a bit above the ground and climbers appear with all of their sensors attached to the ceiling (see figure 1).</p>
<p>All of these physics introduce the fact that an ACL teacher has to propose tasks in the right milieu for the current embodiment (i.e. mostly underwater for swimmers so that they do not die, with creepers and a ceiling high enough for climbers so that they do not touch the ground or die in water and with no water for walker so that they do not drown) in order to make the student learn.</p>
<h1>B.2.3. ObSERVATION AND ACTION SPACE</h1>
<p>As in Stump Tracks, the agent is rewarded for moving forward and penalized for torque usage. An episode lasts 2000 steps unless the agent reaches the end of the track before or if a part of its body touches the ground if the embodiment is a climber. We also reused the 10 lidars per agent that were used in the Stump Tracks with all the lidars starting from the center of the head of the morphology. However, we modified them such that three configurations of covering exist (see the three tasks shown in figure 1):</p>
<ul>
<li>$90^{\circ}$ from below the agent to ahead of it (used by walkers, as in Stump Tracks)</li>
<li>$180^{\circ}$ from below the agent to above it (used by swimmers)</li>
<li>$90^{\circ}$ from ahead of the agent to above it (used by climbers)</li>
</ul>
<p>Moreover, in addition of the distance to the next object detected by each lidar, we added an information concerning the type of object detected by the lidar ( -1 if water, 1 if creeper, 0 otherwise) such that the agent knows whether the object detected is an obstacle or can be passed through. Note also that once their origin point overlaps an object (e.g. water), lidars no longer detect it. Hence the lidars of an agent underwater no longer detect water (which would have made lidars useless as they would have only detected water). Therefore, in order to inform the DRL student whether the embodiment is underwater, we added an observation that is set to 1 if the agent's head is under the water level and 0 otherwise. Similarly, we added a binary observation telling whether the agent is dead or not (i.e. the actions we send to its motors no longer have impact). In addition, we kept the same information concerning the agent's head as in Stump Tracks (angle, linear velocity and angular velocity) as well as observations for each motor (angle and speed of joint as well as contact information for some of the attached limb). Finally, we added two binary observations per sensor (if the agent has sensors) telling whether the sensor has contact with a graspable surface and whether it is already attached with a joint. Without considering the information about motors and sensors which depend on the morphology, all of the information listed above create an observation vector of size 26. Note that, additionally, we provide the information to the teacher at each step whether the cumulative reward of the episode has reached 230 for the users using a binary reward.</p>
<p>Finally, for the action space, we kept the same behaviour as the one used in Stump Tracks (i.e. each agent has motors which are controlled through a torque value in $[-1 ; 1]$ ). Moreover, we added an action in $[-1 ; 1]$ per sensor for climbers to say whether this sensor must grasp (if it has contact with a graspable surface) or release.</p>
<h2>B.3. Morphologies</h2>
<p>We included in our benchmark the classic bipedal walker as well as its two modified versions introduced in Portelas et al. (2019): the short bipedal and the quadrupedal. For these three agents, we kept in their implementation the additional penalty for having an angle different than zero on their head, which was already in Portelas et al. (2019). Additionally, we created new walkers such as the spider or the millipede shown in figure 1. See our repository and website for the exhaustive list of embodiments we provide.</p>
<p>We introduce another type of morphologies: climbers. We propose two agents: a chimpanzee-like embodiment, as well as its simplified version without legs (reducing the action space to simplify the learning task). These two agents have two arms with two sensors at their extremity allowing them to grasp creepers or the ceiling.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://github.com/psclklnk/spdl&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
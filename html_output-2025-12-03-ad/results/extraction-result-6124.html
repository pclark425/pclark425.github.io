<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6124 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6124</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6124</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-8d89f85b5f8a1d65b4e93a7ebb793618641c3ece</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8d89f85b5f8a1d65b4e93a7ebb793618641c3ece" target="_blank">Assessing The Factual Accuracy of Generated Text</a></p>
                <p><strong>Paper Venue:</strong> Knowledge Discovery and Data Mining</p>
                <p><strong>Paper TL;DR:</strong> A model-based metric to estimate the factual accuracy of generated text that is complementary to typical scoring schemes like ROUGE and BLEU is proposed and a new large-scale dataset based on Wikipedia and Wikidata is introduced to train relation classifiers and end-to-end fact extraction models.</p>
                <p><strong>Paper Abstract:</strong> We propose a model-based metric to estimate the factual accuracy of generated text that is complementary to typical scoring schemes like ROUGE (Recall-Oriented Understudy for Gisting Evaluation) and BLEU (Bilingual Evaluation Understudy). We introduce and release a new large-scale dataset based on Wikipedia and Wikidata to train relation classifiers and end-to-end fact extraction models. The end-to-end models are shown to be able to extract complete sets of facts from datasets with full pages of text. We then analyse multiple models that estimate factual accuracy on a Wikipedia text summarization task, and show their efficacy compared to ROUGE and other model-free variants by conducting a human evaluation study.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6124.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6124.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>fact_acc (model-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Factual Accuracy (fact_acc) — model-based metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A precision-based automatic metric that measures factual consistency between generated text and ground-truth by extracting structured relation tuples from each and computing precision over matched (subject, relation) pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic extraction of relation tuples from generated text G and ground-truth T using trained extractors; filter to only compare tuples with matching (subject, relation) pairs; compute fact_acc = |F_T' ∩ F_G'| / |F_G'| (precision).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision of predicted facts (primary); also reported recall and F1 for fact-extraction components; Spearman correlation with human factuality scores used to judge metric quality.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Abstractive summarization model from Liu et al. (2018) used to generate summaries for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Natural language generation / text summarization (NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Generated abstractive summaries (treated as sources of factual claims) whose factual consistency with reference summaries is evaluated via extracted relation tuples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Metric (when using model-based extractors) yields much better sensitivity to factual corruption than ROUGE/OpenIE; correlations with human judgments: fact_acc-E2E-Reduced Spearman = 0.668 (Actors subset), 0.453 (random subset); fact_acc-E2E = 0.645 (Actors), 0.314 (random).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Wikifact dataset (Wikipedia + Wikidata distant supervision); summarization outputs from Liu et al. (2018); synthesized corrupted Wikipedia lead sections used as a benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Directly compared to human judgments of factual accuracy via Spearman correlation; model-based fact_acc (especially end-to-end reduced variant) correlates better with humans than ROUGE or OpenIE on the evaluation sets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Depends on quality of fact-extraction models and their training data; limited to facts verifiable in the reference text (no world knowledge); noise from distant supervision can label true facts as negatives; biased toward relation types present in Wikidata/Wikipedia.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing The Factual Accuracy of Generated Text', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6124.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6124.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E2E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-End Transformer Fact Extraction Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer encoder-decoder sequence-to-sequence model trained to output structured fact tuples (subject, relation, object) as a token sequence for an input document, enabling direct extraction without separate NER/coreference stages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Train seq2seq model on Wikifact distant-supervised labels (article-level sequence of facts); decode via beam search to produce ordered fact tuples; use outputs to compute fact_acc by exact matching of tuples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Exact-match precision/recall/F1 of extracted tuples against distant-supervision targets; downstream Spearman correlation of fact_acc with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>End-to-end model itself is a Transformer; evaluated on outputs of the abstractive summarization model (Liu et al., 2018).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP — structured information extraction from generated text</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Model converts text into a sequence of relation tuples (facts) representing the claims made; used to evaluate factual accuracy of generated summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On the Wikifact test set: Precision=71.67%, Recall=56.21%, F1=63.01% (E2E); E2E-Reduced (filtering sentences without detected entities) P=72.16%, R=61.03%, F1=66.13%. Human-evaluation true-positive rate among false-positives from distant supervision: 77.8% (suggesting many model 'false positives' were actually correct).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Trained on the Wikifact dataset (≈2.5M examples, Wikipedia articles labeled by Wikidata distant supervision); evaluated on held-out test split and on generated summaries and synthesized corrupted samples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>E2E-based fact_acc variants have the highest Spearman correlations with human factuality ratings among tested automatic metrics (see fact_acc entry). End-to-end outputs were judged by humans during error analysis and showed high true-positive rates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Training labels come from distant supervision (noisy); model may be biased toward Wikipedia phrasing and frequent relation types; exact-match evaluation penalizes semantically equivalent but surface-different outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing The Factual Accuracy of Generated Text', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6124.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6124.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RelClass</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Named-Entity Recognition + Relation Classifier (Transformer-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage pipeline that first applies NER and coreference resolution, then classifies relations for each entity pair using a Transformer encoder with max-pooling and sigmoid outputs over a fixed schema.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Run NER/coref to identify entities, for each entity pair feed sentences containing both (with special SUBJ/OBJ prefixes) into a Transformer relation classifier, extract predicted relation labels and compute fact_acc precision when comparing generated vs. ground-truth.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-relation and overall precision/recall/F1 on Wikifact test set; used as component to compute fact_acc and then correlated with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Relation classifier model (Transformer encoder) applied to outputs of summarization model for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP — relation extraction and evaluation of generated text</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Classifier labels relation types between entity pairs found in text; extracted labels are used to determine factual consistency between generated and reference summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Overall on test set: Precision=63.49%, Recall=68.64%, F1=65.96%. Per-relation F1s vary (e.g., Date of birth F1≈0.9582, Country of citizenship F1≈0.7646). Correlation of fact_acc using this classifier with human judgment: Spearman ≈ 0.523 (Actors), 0.250 (random subset). Human-eval true-positive among distant-supervision false-positives: 46.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Wikifact dataset (2.9M positive + 34M negative examples derived via distant supervision); evaluated on hold-out test relations and generated summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Classifier-based fact_acc correlates moderately with human judgments but worse than end-to-end E2E variants, possibly due to error compounding from NER/coref stages.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance depends on external NER/coreference systems; errors compound across pipeline stages; distant supervision labels are noisy and can understate true precision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing The Factual Accuracy of Generated Text', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6124.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6124.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BinaryRel</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NER + Binary Relation Classifier</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage pipeline similar to RelClass but with a binary classifier that predicts whether any relation exists between an entity pair (rel=1) or not (rel=0), i.e., relation existence rather than specific relation type.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>After NER/coref, for each entity pair classify presence/absence of any relation; extract tuples of (entity1, related?, entity2) and compute fact_acc precision by comparing relatedness between generated and ground-truth.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision/recall/F1 on existence detection; use fact_acc (precision over matched pairs) and Spearman correlation with human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Binary relation classifier applied to generated summaries from Liu et al. (2018)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP — relation detection for factuality evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Detects whether generated summaries assert relations between entities present in references, allowing fuzzy verification without exact relation-type matching.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Binary Classifier overall: Precision=59.60%, Recall=75.13%, F1=66.47% on Wikifact; in synthetic corruption experiment predicted factual accuracy ≈46.75. Correlation of fact_acc-Binary Classifier with human judgment: Spearman ≈0.596 (Actors), 0.200 (random subset).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Wikifact dataset used to generate binary labels (related vs not) via Wikidata; synthetic corrupted Wikipedia leads used to test sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Binary-rel-based fact_acc shows moderate correlation with human judgments on narrow subsets but underperforms E2E variants on broader random samples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Loses specificity of relation types (only tests existence); still subject to NER/coref failure; distant supervision noise affects labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing The Factual Accuracy of Generated Text', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6124.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6124.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE: A Package for Automatic Evaluation of Summaries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-free, n-gram overlap metric commonly used to assess content overlap between generated summaries and references (e.g., ROUGE-1, ROUGE-2, ROUGE-L).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ROUGE: A Package for Automatic Evaluation of Summaries</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute n-gram recall/precision/overlap statistics between generated text and reference summaries (ROUGE-1, ROUGE-2, ROUGE-L) as an intrinsic content-based metric.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>N-gram overlap measures (unigram, bigram, longest common subsequence) as proxies for content similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Used to evaluate outputs of abstractive summarization model (Liu et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP — summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Measures surface-level overlap of words/phrases in generated summaries versus references; not designed to detect factual inconsistencies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On synthesized corrupted samples, ROUGE scores remain high (ROUGE-1 97.08, ROUGE-2 94.06, ROUGE-L 96.02) despite factual corruption. Correlation with human factuality: Spearman ROUGE-1=0.583 (Actors), 0.384 (random); ROUGE-2=0.639 (Actors), 0.435 (random).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied to generated Wikipedia lead summaries and synthesized corrupted samples in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>ROUGE correlates with human judgments of overall linguistic/content quality to some extent but is less sensitive to factual errors than model-based fact_acc metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Insensitive to factual changes that preserve surface n-grams; can give high scores for factually incorrect outputs; not a direct measure of factual accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing The Factual Accuracy of Generated Text', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6124.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6124.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenIE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open Information Extraction (OpenIE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-free relation-extraction tool that extracts relation triplets from text without a fixed schema (relation expressed as surface text between entities).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Open Information Extraction from the Web</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Run OpenIE on generated and reference texts to extract tuples, then compute precision (like eq.1) between extracted sets as a model-free factuality proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision of overlapping tuples extracted by OpenIE; used as a baseline model-free factuality metric.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applied to outputs of the abstractive summarization model (Liu et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP — relation extraction, summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Extracts surface-form relation triplets from texts which are compared across generated and reference summaries to approximate factual agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On synthesized corrupted samples, OpenIE precision ≈87.26 (less sensitive than model-based but more than ROUGE). Correlation with human factuality: Spearman OpenIE=0.258 (Actors), 0.128 (random). OpenIE output is noisy and often not suitable for structured reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Evaluated on generated Wikipedia lead summaries and synthesized corrupted samples; compared against structured model-based extractors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Lower correlation with human judgments of factuality than model-based metrics; OpenIE often yields triplets that are difficult to compare semantically.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No fixed schema leads to heterogeneous outputs; surface relations are hard to canonicalize for comparison; less sensitive to fine-grained factual errors and suffers from extraction noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing The Factual Accuracy of Generated Text', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6124.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6124.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wikifact</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wikifact dataset (Wikipedia + Wikidata distant supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale dataset created via distant supervision by aligning Wikipedia article text with Wikidata relation triples; provides positive and negative examples for training relation extractors and end-to-end fact-extraction models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Used as training and test data for relation classifiers, binary classifiers, and end-to-end extractors; evaluation done by exact matching of extracted tuples to Wikidata-derived targets and by human annotation for error analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision/recall/F1 on held-out test splits; human verification of model-predicted 'false positives' to estimate distant supervision noise.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Dataset used to train fact-extraction models that evaluate outputs of the summarization model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP — dataset for relation extraction and factuality evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Provides large-scale weakly-labeled facts (subject, relation, object) for Wikipedia articles obtained by checking Wikidata for relations involving the article topic and other entities mentioned in the article.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Training set sizes: classifier dataset ~2.9M positive + 34M negative examples (45 GiB); end-to-end dataset ~2.5M examples (1.5 GiB). Used to achieve relation-classifier F1≈65.96 and E2E F1≈63.01 on test data.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Primary benchmark for training/evaluating the paper's models; also used to create synthesized corrupted samples by swapping dates/locations/people.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human evaluation of model 'false positives' showed that many predictions labeled false by distant supervision were actually true: E2E 77.8% and relation classifier 46.6% (i.e., distant supervision underlabels true facts).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Distant supervision introduces label noise because the presence of a Wikidata triple for an entity pair does not guarantee the sentence entails that relation; Wikidata is incomplete and biased to common relation types; dataset biases models toward Wikipedia style and popular relations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing The Factual Accuracy of Generated Text', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6124.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6124.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SynthCorrupt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthesized Corruption Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic benchmark created by corrupting factual elements (dates, locations, people) in Wikipedia lead sections to evaluate sensitivity of metrics to factual errors while preserving surface similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Programmatically replace dates (day/month), locations, or persons with other entities of the same type to create factually corrupted summaries; evaluate metrics' ability to detect decreased factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Reported 'predicted factual accuracy' (percent) by different metrics on corrupted samples; sensitivity measured by drop in metric scores relative to uncorrupted text.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Used as a testbed for metrics evaluated on generated vs. corrupted references; not an LLM itself</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP — robustness and factuality evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Controlled corruption of factual content to test whether evaluation metrics reflect factual inconsistency.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Observed metrics on corrupted samples: ROUGE-1 97.08, ROUGE-2 94.06, ROUGE-L 96.02, OpenIE 87.26, Binary Relation Classifier 46.75, Relation Classifier 59.30, E2E 65.44, E2E-Reduced 57.10; expected accuracy (ratio of corrupted facts) = 30.97, highlighting that ROUGE remains high despite factual corruption while model-based metrics drop substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Synthetic dataset derived from Wikipedia lead sections using Wikidata-typed replacements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Designed to illustrate that model-free metrics like ROUGE poorly reflect factual degradation while model-based extractors better capture factual errors; human judgments not directly reported for synthetic set but used as motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Synthetic corruptions are simple and may not capture nuanced real-world hallucinations; still useful as controlled diagnostic but not a substitute for human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing The Factual Accuracy of Generated Text', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6124.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6124.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Factuality Evaluation (annotator rating)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human evaluation protocol where annotators label each verifiable claim in generated text as supported, refuted, or unverifiable with respect to the reference, and then score overall factual accuracy on a 1–5 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Four human evaluators assessed 30 examples per sample set; asked to consider only claims verifiable/refutable by the ground-truth and assign a 1–5 factuality score (1 for very poor, 5 for very good). Inter-rater agreement measured with Krippendorff's alpha.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Ordinal human ratings aggregated and used as ground truth for metric correlation; inter-rater agreement reported (Krippendorff's alpha ≈0.6897 on Actors subset, 0.7530 on random subset).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applied to outputs from the abstractive summarization model (Liu et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>NLP — human assessment of factual accuracy in generated text</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Human judgments of the factual correctness of generated summaries, used as the standard for evaluating automatic metrics' correlation with human perception.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used to compute Spearman correlations with automatic metrics; e.g., best automatic metric (fact_acc-E2E-Reduced) Spearman = 0.668 (Actors) and 0.453 (random).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Human-evaluation performed on two samples: an 'Actors' subset and a random subset of Wikipedia categories; 30 examples per evaluator in each experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Serves as the ground-truth comparator; model-based metrics are evaluated by how well they correlate with these human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human evaluation limited in scale (small sample sizes); annotators only consider claims verifiable by the provided reference (no external knowledge), which limits assessment of world-knowledge-based correctness; subjective variability though inter-rater agreement was moderate-high.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing The Factual Accuracy of Generated Text', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ROUGE: A Package for Automatic Evaluation of Summaries <em>(Rating: 2)</em></li>
                <li>Open Information Extraction from the Web <em>(Rating: 2)</em></li>
                <li>Distant Supervision for Relation Extraction Without Labeled Data <em>(Rating: 2)</em></li>
                <li>Generating Wikipedia by Summarizing Long Sequences <em>(Rating: 2)</em></li>
                <li>Attention is All you Need <em>(Rating: 1)</em></li>
                <li>Challenges in Data-to-Document Generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6124",
    "paper_id": "paper-8d89f85b5f8a1d65b4e93a7ebb793618641c3ece",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "fact_acc (model-based)",
            "name_full": "Factual Accuracy (fact_acc) — model-based metric",
            "brief_description": "A precision-based automatic metric that measures factual consistency between generated text and ground-truth by extracting structured relation tuples from each and computing precision over matched (subject, relation) pairs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Automatic extraction of relation tuples from generated text G and ground-truth T using trained extractors; filter to only compare tuples with matching (subject, relation) pairs; compute fact_acc = |F_T' ∩ F_G'| / |F_G'| (precision).",
            "evaluation_criteria": "Precision of predicted facts (primary); also reported recall and F1 for fact-extraction components; Spearman correlation with human factuality scores used to judge metric quality.",
            "llm_model_name": "Abstractive summarization model from Liu et al. (2018) used to generate summaries for evaluation",
            "theory_domain": "Natural language generation / text summarization (NLP)",
            "theory_description": "Generated abstractive summaries (treated as sources of factual claims) whose factual consistency with reference summaries is evaluated via extracted relation tuples.",
            "evaluation_results": "Metric (when using model-based extractors) yields much better sensitivity to factual corruption than ROUGE/OpenIE; correlations with human judgments: fact_acc-E2E-Reduced Spearman = 0.668 (Actors subset), 0.453 (random subset); fact_acc-E2E = 0.645 (Actors), 0.314 (random).",
            "benchmarks_or_datasets": "Wikifact dataset (Wikipedia + Wikidata distant supervision); summarization outputs from Liu et al. (2018); synthesized corrupted Wikipedia lead sections used as a benchmark.",
            "comparison_to_human": "Directly compared to human judgments of factual accuracy via Spearman correlation; model-based fact_acc (especially end-to-end reduced variant) correlates better with humans than ROUGE or OpenIE on the evaluation sets.",
            "limitations_or_challenges": "Depends on quality of fact-extraction models and their training data; limited to facts verifiable in the reference text (no world knowledge); noise from distant supervision can label true facts as negatives; biased toward relation types present in Wikidata/Wikipedia.",
            "uuid": "e6124.0",
            "source_info": {
                "paper_title": "Assessing The Factual Accuracy of Generated Text",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "E2E",
            "name_full": "End-to-End Transformer Fact Extraction Model",
            "brief_description": "A Transformer encoder-decoder sequence-to-sequence model trained to output structured fact tuples (subject, relation, object) as a token sequence for an input document, enabling direct extraction without separate NER/coreference stages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Train seq2seq model on Wikifact distant-supervised labels (article-level sequence of facts); decode via beam search to produce ordered fact tuples; use outputs to compute fact_acc by exact matching of tuples.",
            "evaluation_criteria": "Exact-match precision/recall/F1 of extracted tuples against distant-supervision targets; downstream Spearman correlation of fact_acc with human judgments.",
            "llm_model_name": "End-to-end model itself is a Transformer; evaluated on outputs of the abstractive summarization model (Liu et al., 2018).",
            "theory_domain": "NLP — structured information extraction from generated text",
            "theory_description": "Model converts text into a sequence of relation tuples (facts) representing the claims made; used to evaluate factual accuracy of generated summaries.",
            "evaluation_results": "On the Wikifact test set: Precision=71.67%, Recall=56.21%, F1=63.01% (E2E); E2E-Reduced (filtering sentences without detected entities) P=72.16%, R=61.03%, F1=66.13%. Human-evaluation true-positive rate among false-positives from distant supervision: 77.8% (suggesting many model 'false positives' were actually correct).",
            "benchmarks_or_datasets": "Trained on the Wikifact dataset (≈2.5M examples, Wikipedia articles labeled by Wikidata distant supervision); evaluated on held-out test split and on generated summaries and synthesized corrupted samples.",
            "comparison_to_human": "E2E-based fact_acc variants have the highest Spearman correlations with human factuality ratings among tested automatic metrics (see fact_acc entry). End-to-end outputs were judged by humans during error analysis and showed high true-positive rates.",
            "limitations_or_challenges": "Training labels come from distant supervision (noisy); model may be biased toward Wikipedia phrasing and frequent relation types; exact-match evaluation penalizes semantically equivalent but surface-different outputs.",
            "uuid": "e6124.1",
            "source_info": {
                "paper_title": "Assessing The Factual Accuracy of Generated Text",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "RelClass",
            "name_full": "Named-Entity Recognition + Relation Classifier (Transformer-based)",
            "brief_description": "A two-stage pipeline that first applies NER and coreference resolution, then classifies relations for each entity pair using a Transformer encoder with max-pooling and sigmoid outputs over a fixed schema.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Run NER/coref to identify entities, for each entity pair feed sentences containing both (with special SUBJ/OBJ prefixes) into a Transformer relation classifier, extract predicted relation labels and compute fact_acc precision when comparing generated vs. ground-truth.",
            "evaluation_criteria": "Per-relation and overall precision/recall/F1 on Wikifact test set; used as component to compute fact_acc and then correlated with human judgments.",
            "llm_model_name": "Relation classifier model (Transformer encoder) applied to outputs of summarization model for evaluation",
            "theory_domain": "NLP — relation extraction and evaluation of generated text",
            "theory_description": "Classifier labels relation types between entity pairs found in text; extracted labels are used to determine factual consistency between generated and reference summaries.",
            "evaluation_results": "Overall on test set: Precision=63.49%, Recall=68.64%, F1=65.96%. Per-relation F1s vary (e.g., Date of birth F1≈0.9582, Country of citizenship F1≈0.7646). Correlation of fact_acc using this classifier with human judgment: Spearman ≈ 0.523 (Actors), 0.250 (random subset). Human-eval true-positive among distant-supervision false-positives: 46.6%.",
            "benchmarks_or_datasets": "Wikifact dataset (2.9M positive + 34M negative examples derived via distant supervision); evaluated on hold-out test relations and generated summaries.",
            "comparison_to_human": "Classifier-based fact_acc correlates moderately with human judgments but worse than end-to-end E2E variants, possibly due to error compounding from NER/coref stages.",
            "limitations_or_challenges": "Performance depends on external NER/coreference systems; errors compound across pipeline stages; distant supervision labels are noisy and can understate true precision.",
            "uuid": "e6124.2",
            "source_info": {
                "paper_title": "Assessing The Factual Accuracy of Generated Text",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "BinaryRel",
            "name_full": "NER + Binary Relation Classifier",
            "brief_description": "A two-stage pipeline similar to RelClass but with a binary classifier that predicts whether any relation exists between an entity pair (rel=1) or not (rel=0), i.e., relation existence rather than specific relation type.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "After NER/coref, for each entity pair classify presence/absence of any relation; extract tuples of (entity1, related?, entity2) and compute fact_acc precision by comparing relatedness between generated and ground-truth.",
            "evaluation_criteria": "Precision/recall/F1 on existence detection; use fact_acc (precision over matched pairs) and Spearman correlation with human ratings.",
            "llm_model_name": "Binary relation classifier applied to generated summaries from Liu et al. (2018)",
            "theory_domain": "NLP — relation detection for factuality evaluation",
            "theory_description": "Detects whether generated summaries assert relations between entities present in references, allowing fuzzy verification without exact relation-type matching.",
            "evaluation_results": "Binary Classifier overall: Precision=59.60%, Recall=75.13%, F1=66.47% on Wikifact; in synthetic corruption experiment predicted factual accuracy ≈46.75. Correlation of fact_acc-Binary Classifier with human judgment: Spearman ≈0.596 (Actors), 0.200 (random subset).",
            "benchmarks_or_datasets": "Wikifact dataset used to generate binary labels (related vs not) via Wikidata; synthetic corrupted Wikipedia leads used to test sensitivity.",
            "comparison_to_human": "Binary-rel-based fact_acc shows moderate correlation with human judgments on narrow subsets but underperforms E2E variants on broader random samples.",
            "limitations_or_challenges": "Loses specificity of relation types (only tests existence); still subject to NER/coref failure; distant supervision noise affects labels.",
            "uuid": "e6124.3",
            "source_info": {
                "paper_title": "Assessing The Factual Accuracy of Generated Text",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "ROUGE",
            "name_full": "ROUGE: A Package for Automatic Evaluation of Summaries",
            "brief_description": "A model-free, n-gram overlap metric commonly used to assess content overlap between generated summaries and references (e.g., ROUGE-1, ROUGE-2, ROUGE-L).",
            "citation_title": "ROUGE: A Package for Automatic Evaluation of Summaries",
            "mention_or_use": "use",
            "evaluation_method": "Compute n-gram recall/precision/overlap statistics between generated text and reference summaries (ROUGE-1, ROUGE-2, ROUGE-L) as an intrinsic content-based metric.",
            "evaluation_criteria": "N-gram overlap measures (unigram, bigram, longest common subsequence) as proxies for content similarity.",
            "llm_model_name": "Used to evaluate outputs of abstractive summarization model (Liu et al., 2018)",
            "theory_domain": "NLP — summarization evaluation",
            "theory_description": "Measures surface-level overlap of words/phrases in generated summaries versus references; not designed to detect factual inconsistencies.",
            "evaluation_results": "On synthesized corrupted samples, ROUGE scores remain high (ROUGE-1 97.08, ROUGE-2 94.06, ROUGE-L 96.02) despite factual corruption. Correlation with human factuality: Spearman ROUGE-1=0.583 (Actors), 0.384 (random); ROUGE-2=0.639 (Actors), 0.435 (random).",
            "benchmarks_or_datasets": "Applied to generated Wikipedia lead summaries and synthesized corrupted samples in experiments.",
            "comparison_to_human": "ROUGE correlates with human judgments of overall linguistic/content quality to some extent but is less sensitive to factual errors than model-based fact_acc metrics.",
            "limitations_or_challenges": "Insensitive to factual changes that preserve surface n-grams; can give high scores for factually incorrect outputs; not a direct measure of factual accuracy.",
            "uuid": "e6124.4",
            "source_info": {
                "paper_title": "Assessing The Factual Accuracy of Generated Text",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "OpenIE",
            "name_full": "Open Information Extraction (OpenIE)",
            "brief_description": "A model-free relation-extraction tool that extracts relation triplets from text without a fixed schema (relation expressed as surface text between entities).",
            "citation_title": "Open Information Extraction from the Web",
            "mention_or_use": "use",
            "evaluation_method": "Run OpenIE on generated and reference texts to extract tuples, then compute precision (like eq.1) between extracted sets as a model-free factuality proxy.",
            "evaluation_criteria": "Precision of overlapping tuples extracted by OpenIE; used as a baseline model-free factuality metric.",
            "llm_model_name": "Applied to outputs of the abstractive summarization model (Liu et al., 2018)",
            "theory_domain": "NLP — relation extraction, summarization evaluation",
            "theory_description": "Extracts surface-form relation triplets from texts which are compared across generated and reference summaries to approximate factual agreement.",
            "evaluation_results": "On synthesized corrupted samples, OpenIE precision ≈87.26 (less sensitive than model-based but more than ROUGE). Correlation with human factuality: Spearman OpenIE=0.258 (Actors), 0.128 (random). OpenIE output is noisy and often not suitable for structured reasoning.",
            "benchmarks_or_datasets": "Evaluated on generated Wikipedia lead summaries and synthesized corrupted samples; compared against structured model-based extractors.",
            "comparison_to_human": "Lower correlation with human judgments of factuality than model-based metrics; OpenIE often yields triplets that are difficult to compare semantically.",
            "limitations_or_challenges": "No fixed schema leads to heterogeneous outputs; surface relations are hard to canonicalize for comparison; less sensitive to fine-grained factual errors and suffers from extraction noise.",
            "uuid": "e6124.5",
            "source_info": {
                "paper_title": "Assessing The Factual Accuracy of Generated Text",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Wikifact",
            "name_full": "Wikifact dataset (Wikipedia + Wikidata distant supervision)",
            "brief_description": "A large-scale dataset created via distant supervision by aligning Wikipedia article text with Wikidata relation triples; provides positive and negative examples for training relation extractors and end-to-end fact-extraction models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Used as training and test data for relation classifiers, binary classifiers, and end-to-end extractors; evaluation done by exact matching of extracted tuples to Wikidata-derived targets and by human annotation for error analysis.",
            "evaluation_criteria": "Precision/recall/F1 on held-out test splits; human verification of model-predicted 'false positives' to estimate distant supervision noise.",
            "llm_model_name": "Dataset used to train fact-extraction models that evaluate outputs of the summarization model",
            "theory_domain": "NLP — dataset for relation extraction and factuality evaluation",
            "theory_description": "Provides large-scale weakly-labeled facts (subject, relation, object) for Wikipedia articles obtained by checking Wikidata for relations involving the article topic and other entities mentioned in the article.",
            "evaluation_results": "Training set sizes: classifier dataset ~2.9M positive + 34M negative examples (45 GiB); end-to-end dataset ~2.5M examples (1.5 GiB). Used to achieve relation-classifier F1≈65.96 and E2E F1≈63.01 on test data.",
            "benchmarks_or_datasets": "Primary benchmark for training/evaluating the paper's models; also used to create synthesized corrupted samples by swapping dates/locations/people.",
            "comparison_to_human": "Human evaluation of model 'false positives' showed that many predictions labeled false by distant supervision were actually true: E2E 77.8% and relation classifier 46.6% (i.e., distant supervision underlabels true facts).",
            "limitations_or_challenges": "Distant supervision introduces label noise because the presence of a Wikidata triple for an entity pair does not guarantee the sentence entails that relation; Wikidata is incomplete and biased to common relation types; dataset biases models toward Wikipedia style and popular relations.",
            "uuid": "e6124.6",
            "source_info": {
                "paper_title": "Assessing The Factual Accuracy of Generated Text",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "SynthCorrupt",
            "name_full": "Synthesized Corruption Benchmark",
            "brief_description": "A synthetic benchmark created by corrupting factual elements (dates, locations, people) in Wikipedia lead sections to evaluate sensitivity of metrics to factual errors while preserving surface similarity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Programmatically replace dates (day/month), locations, or persons with other entities of the same type to create factually corrupted summaries; evaluate metrics' ability to detect decreased factuality.",
            "evaluation_criteria": "Reported 'predicted factual accuracy' (percent) by different metrics on corrupted samples; sensitivity measured by drop in metric scores relative to uncorrupted text.",
            "llm_model_name": "Used as a testbed for metrics evaluated on generated vs. corrupted references; not an LLM itself",
            "theory_domain": "NLP — robustness and factuality evaluation",
            "theory_description": "Controlled corruption of factual content to test whether evaluation metrics reflect factual inconsistency.",
            "evaluation_results": "Observed metrics on corrupted samples: ROUGE-1 97.08, ROUGE-2 94.06, ROUGE-L 96.02, OpenIE 87.26, Binary Relation Classifier 46.75, Relation Classifier 59.30, E2E 65.44, E2E-Reduced 57.10; expected accuracy (ratio of corrupted facts) = 30.97, highlighting that ROUGE remains high despite factual corruption while model-based metrics drop substantially.",
            "benchmarks_or_datasets": "Synthetic dataset derived from Wikipedia lead sections using Wikidata-typed replacements.",
            "comparison_to_human": "Designed to illustrate that model-free metrics like ROUGE poorly reflect factual degradation while model-based extractors better capture factual errors; human judgments not directly reported for synthetic set but used as motivation.",
            "limitations_or_challenges": "Synthetic corruptions are simple and may not capture nuanced real-world hallucinations; still useful as controlled diagnostic but not a substitute for human evaluation.",
            "uuid": "e6124.7",
            "source_info": {
                "paper_title": "Assessing The Factual Accuracy of Generated Text",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "HumanEval",
            "name_full": "Human Factuality Evaluation (annotator rating)",
            "brief_description": "Human evaluation protocol where annotators label each verifiable claim in generated text as supported, refuted, or unverifiable with respect to the reference, and then score overall factual accuracy on a 1–5 scale.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Four human evaluators assessed 30 examples per sample set; asked to consider only claims verifiable/refutable by the ground-truth and assign a 1–5 factuality score (1 for very poor, 5 for very good). Inter-rater agreement measured with Krippendorff's alpha.",
            "evaluation_criteria": "Ordinal human ratings aggregated and used as ground truth for metric correlation; inter-rater agreement reported (Krippendorff's alpha ≈0.6897 on Actors subset, 0.7530 on random subset).",
            "llm_model_name": "Applied to outputs from the abstractive summarization model (Liu et al., 2018)",
            "theory_domain": "NLP — human assessment of factual accuracy in generated text",
            "theory_description": "Human judgments of the factual correctness of generated summaries, used as the standard for evaluating automatic metrics' correlation with human perception.",
            "evaluation_results": "Used to compute Spearman correlations with automatic metrics; e.g., best automatic metric (fact_acc-E2E-Reduced) Spearman = 0.668 (Actors) and 0.453 (random).",
            "benchmarks_or_datasets": "Human-evaluation performed on two samples: an 'Actors' subset and a random subset of Wikipedia categories; 30 examples per evaluator in each experiment.",
            "comparison_to_human": "Serves as the ground-truth comparator; model-based metrics are evaluated by how well they correlate with these human ratings.",
            "limitations_or_challenges": "Human evaluation limited in scale (small sample sizes); annotators only consider claims verifiable by the provided reference (no external knowledge), which limits assessment of world-knowledge-based correctness; subjective variability though inter-rater agreement was moderate-high.",
            "uuid": "e6124.8",
            "source_info": {
                "paper_title": "Assessing The Factual Accuracy of Generated Text",
                "publication_date_yy_mm": "2019-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ROUGE: A Package for Automatic Evaluation of Summaries",
            "rating": 2
        },
        {
            "paper_title": "Open Information Extraction from the Web",
            "rating": 2
        },
        {
            "paper_title": "Distant Supervision for Relation Extraction Without Labeled Data",
            "rating": 2
        },
        {
            "paper_title": "Generating Wikipedia by Summarizing Long Sequences",
            "rating": 2
        },
        {
            "paper_title": "Attention is All you Need",
            "rating": 1
        },
        {
            "paper_title": "Challenges in Data-to-Document Generation",
            "rating": 1
        }
    ],
    "cost": 0.015358,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Assessing The Factual Accuracy of Generated Text</h1>
<p>Ben Goodrich<em><br>Vinay Rao</em><br>Peter J. Liu<br>Mohammad Saleh<br>bgoodrich@google.com<br>vinaysrao@google.com<br>peterjliu@google.com<br>msaleh@google.com<br>Google Brain</p>
<h4>Abstract</h4>
<p>We propose a model-based metric to estimate the factual accuracy of generated text that is complementary to typical scoring schemes like ROUGE (Recall-Oriented Understudy for Gisting Evaluation) and BLEU (Bilingual Evaluation Understudy). We introduce and release a new large-scale dataset based on Wikipedia and Wikidata to train relation classifiers and end-to-end fact extraction models. The end-to-end models are shown to be able to extract complete sets of facts from datasets with full pages of text. We then analyse multiple models that estimate factual accuracy on a Wikipedia text summarization task, and show their efficacy compared to ROUGE and other model-free variants by conducting a human evaluation study.</p>
<h2>KEYWORDS</h2>
<p>datasets, neural networks, fact extraction, deep learning, metric, end-to-end</p>
<h2>ACM Reference Format:</h2>
<p>Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. 2019. Assessing The Factual Accuracy of Generated Text. In The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '19), August 4-8, 2019, Anchorage, AK, USA. ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/3292500.3330955</p>
<h2>1 INTRODUCTION</h2>
<p>Recently, there has been wide empirical success in text summarization [15, 21, 27], machine translation [1, 36, 39], dialogue response generation [12, 28, 29], and other text generation tasks. For evaluation, these models generally rely on metrics like ROUGE (RecallOriented Understudy for Gisting Evaluation) [13], BLEU (Bilingual Evaluation Understudy) [23] and perplexity [3] that measure locally constrained n-gram overlap. In this paper, we propose an automatic metric for evaluating the factual accuracy of generated text.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>A fact $f$ is defined to be a relation tuple (subject, relation, object), where subject has a binary relation to object and can be assumed to have been inferred from text or a knowledge base, e.g. Barack Hussein Obama II (born August 4, 1961) is an American politician who served as the 44th President of the United States from January 20, 2009 to January 20, 2017 implies a set of facts such as (Barack Obama, president of, United States), (Barack Obama, born on, August 4 1961).</p>
<p>In this paper, we limit our scope to the task of evaluating text summarization. To evaluate a text summarization model, we compare the ground-truth summary text, $T$ and the generated summary, G. Let $f_{t}, f_{g} \in F$, and $F_{T}, F_{G} \subset F$ where $F$ is a set of relation tuples.</p>
<p>$$
\begin{aligned}
&amp; F_{T}=\left{f_{t} \mid f_{t} \text { is inferred from ground-truth } T\right} \
&amp; F_{G}=\left{f_{g} \mid f_{g} \text { is inferred from generated-text } G\right}
\end{aligned}
$$</p>
<p>The models used in the metric we propose do not make use of world knowledge (e.g. knowledge base) during inference, and to account for that we filter $F_{T}$ and $F_{G}$ by only considering claims made in $G$ that can either be verified or refuted by statements in $T$. Concretely, if $f_{t}=\left(s u b j_{t}, r e l_{t}, o b j_{t}\right) \in F_{T}$ and $f_{g}=\left(s u b j_{g}, r e l_{g}, o b j_{g}\right) \in F_{G}$</p>
<p>$$
\begin{aligned}
&amp; F_{T^{\prime}}=\left{f_{t} \mid \exists f_{g} \text { and } s u b j_{t}=s u b j_{g}, r e l_{t}=r e l_{g}\right} \
&amp; F_{G^{\prime}}=\left{f_{g} \mid \exists f_{t} \text { and } s u b j_{g}=s u b j_{t}, r e l_{g}=r e l_{t}\right}
\end{aligned}
$$</p>
<p>We can then define factual accuracy fact $<em T_prime="T^{\prime">{\text {acc }}$ as the precision between $F</em>$.}}$ and $F_{G^{\prime}</p>
<p>$$
f a c t_{a c c}=\frac{\left|F_{T^{\prime}} \cap F_{G^{\prime}}\right|}{\left|F_{G^{\prime}}\right|}
$$</p>
<p>For example, consider ground-truth summary $T$ : Brad Pitt was born in 1963 and generated summary G: Brad Pitt was born in 1961. Then, $F_{T}=f($ Brad Pitt, born-in, 1963) $), F_{G}=f($ Brad Pitt, born-in, 1961) $)$. The metric $f a c t_{a c c}=0$ indicates there is no factual consistency between the two summaries, whereas another metric like ROUGE-1 (1-gram overlap) measures 0.83 . A real example is highlighted in Table 1 where the summarization model commits such a mistake. It is important to be able to measure these mistakes accurately to aid in training factually accurate summarization models.</p>
<p>Extracting fact tuples from text has been previously studied in methods like OpenIE (Open Information Extraction) [2]. OpenIE extracts triplets with an unspecified schema, and the relation is usually the text linking the two entities. However, it does not leverage information from a knowledge base and leads to outputs that are hard to compare. For example, Person was born in that town $\Rightarrow$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Target</th>
<th style="text-align: left;">Peter Duryea (July 14, 1939 - March 24, 2013) was an American actor. He is best known for appearing <br> in a pilot episode of Star Trek: The Original Series, "The Cage" (1964), most of which was reused in <br> "The Menagerie" (1966), as Lieutenant Tyler. His father, Dan Duryea (1907 - 1968), was also an actor.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Output</td>
<td style="text-align: left;">Peter Duryea (April 23, 1907 - March 24, 2013) was an American actor. He is best known for his role as <br> Lt. Jose Tyler in the original Star Trek pilot, "The Cage"</td>
</tr>
</tbody>
</table>
<p>Table 1: Example of factual inaccuracy noted in a summarization model [15]. In this example, the summarization model uses the subject (Peter Duryea)'s father, Dan Duryea's birthdate.
(Person, born in, town). But That town is the birthplace of Person $\Rightarrow$ (Town, is the birthplace of, Person).</p>
<p>We standardize comparison by studying structured approaches to relation tuple extraction where the schema is fixed. We compare two approaches for fact extraction. One is a two-step process that first involves recognizing all the named entities in a sentence, and then classifying the relation for every pair of entities in the sentence [14, 32]. Our other approach is to use an end-to-end model with a Transformer-based architecture [36] that is trained to output structured fact tuples. These models are described in Section 4. We create a new dataset for fact extraction using distant supervision [17] on Wikipedia text by cross-referencing facts from the Wikidata knowledge base [37]. To the best of our knowledge, this dataset is bigger and contains more relations and domains than previously used datasets for relation or fact tuple extraction.</p>
<p>Our main contributions are:
(1) We introduce model-based metrics to analyze the factual accuracy of generated text (Sec 4). We compare them against model-free metrics listed in Sec 5.
(2) To train fact tuple extraction models, we release code (as part of the Tensor2Tensor ${ }^{2}$ framework along with the model weights ${ }^{2}$ ) and a large dataset (Sec 3) based on Wikidata and Wikipedia at https://github.com/google-research-datasets/ wikifact.
(3) We show that a Transformer-based end-to-end fact extraction model is able to perform structured prediction of relation tuples, avoiding the need to split the process into multiple steps (named entity recognition, coreference resolution and relation classification). It is able to extract complete sets of facts from full pages of text in one pass.
(4) We conduct experiments to compare our proposed metric against human evaluation of factual accuracy of generated text (Sec 8.1) and show that model-based metrics are better correlated with human judgment when compared to traditional metrics like ROUGE.
Our models work under some limitations that are discussed in Sec 9.1, and then Sec 9.2 discusses future work and ways to make our models more robust.</p>
<h2>2 RELATED WORK AND MOTIVATION</h2>
<p>Many evaluation metrics have been proposed for text generation tasks like BLEU [23] and METEOR [10] for machine translation</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and ROUGE [13], Basic Elements [8] \&amp; Pyramid [22] for text summarization. In Steinberger and Jezek [33], the authors explain the different kinds of evaluation we can perform for summarization. They are broadly classified as extrinsic metrics that are specific to tasks (e.g. in summarizing a person, whether the date of birth has been included) and intrinsic metrics like grammaticality, coherency and non-redundancy that are based on the analysis of the summary. ROUGE, BLEU, sentence level F1 measures, etc are intrinsic content based metrics. Zhang et al. [40] and other related works study ways to estimate the trustworthiness of answers to a question. With the recent shift towards using neural abstractive methods for text summarization and other text generation tasks, we believe that it is important to assess the factual accuracy of generated text. Wiseman et al. [38] have also studied some extractive evaluative methods to assess the quality of generated text. This includes a Relation Generator, which predicts the relation between entities to assess the factual correctness of generated records. However, we introduce a much larger dataset and enable training end-to-end models that can extract fact triplets from text. We additionally perform detailed analysis of the fact extraction models.</p>
<p>Typical fact extraction pipelines are a multistage process consisting of part-of-speech tagging, named entity recognition [5, 7, 9] that produces entities $\left{e_{i}\right}$ and then relation classification that predicts a relation $r_{k}$ for every pair of entities $\left(e_{i}, e_{j}\right)$. OpenIE [2] predicts a relation by linking the text connecting $e_{i}$ and $e_{j}$. Because it does not have a fixed schema, logical reasoning on its outputs are not possible. Mohamed et al. [20] extend this to start with a fixed schema that can grow with more training, yet retain a consistent output surface form.</p>
<p>In this paper, we consider fact classification models with fixed schema. This idea has been studied in many previous works including Surdeanu et al. [34], which considered datasets that have multiple relation labels for an entity pair, which each may have multiple instances in the input text. This was modeled as a graphical model over latent variables. Riedel et al. [26] treated relation extraction as reasoning with matrix-factorization, and could work with surface-form texts and knowledge-base embeddings simultaneously. However, both of these works had datasets with very few types of relations, and were shown to work over limited domains. Recently, neural networks have been used for classifying relations. Lin et al. [14] used attention over multiple instances for the same entity pair to predict relations. Sorokin and Gurevych [32] proposed to predict multiple relations in a sentence by using all the entity pairs and relation labels in the sentence as contextual input. We propose a simpler model where we classify relations between all the entity pairs in a sentence, without any additional context.</p>
<p>We also make use of our proposed dataset that is bigger, more diverse and has more relation types. Our dataset also has article-level information that can be used to train models like in Section 4.2. Since using two-step processes may be affected by compounding of errors across the models, some end-to-end approaches [18, 19] have been proposed, where the models extract entities and relations in one pass through the model. However, the method used in Miwa and Sasaki [19] required designing hand-crafted features and task-specific algorithms. Miwa and Bansal [18] has a two-phase model that first extracts entity candidates and then predicts relations based on the parsed tree-structure of the sentence. We instead propose a sequence-to-sequence model that is able to output fact tuples directly, and does not require any feature engineering.</p>
<p>We found that the abstractive summarization models such as those described in Liu et al. [15] may generate sentences with factual inaccuracies (e.g. incorrect month in date of birth, wrong city in the state, etc.). Cao et al. [4] found that $30 \%$ of summaries generated by a state-of-the-art summarization model contained factual inaccuracies. We found by running a large-scale experiment as described in Section 8.1, that the summarization model had factual inaccuracy rate of approximately $17 \%$. We believe that this is because such mistakes are not heavily penalized by cross-entropy or n-gram based model losses and metrics.
As further motivation, we synthesized factually inaccurate samples by making simple corruptions to Wikipedia lead sections. We replaced mentions of dates (day and month only), locations or people with other entities of the same type in the text. For example, Barack was born on August 4, 1961 in Honolulu. He married Michelle on October 3, 1992 in Chicago. becomes Barack was born on October 3, 1961 in Chicago. He married Michelle on August 4, 1992 in Honolulu.. Table 2 shows that model-free metrics such as ROUGE and OpenIE-based tuple comparison do not reflect the decline in factual accuracy due to such corruption as much as the model-based metrics do.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: left;">97.08</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: left;">94.06</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: left;">96.02</td>
</tr>
<tr>
<td style="text-align: left;">OpenIE</td>
<td style="text-align: left;">87.26</td>
</tr>
<tr>
<td style="text-align: left;">Binary Relation Classifier</td>
<td style="text-align: left;">46.75</td>
</tr>
<tr>
<td style="text-align: left;">Relation Classifier</td>
<td style="text-align: left;">59.30</td>
</tr>
<tr>
<td style="text-align: left;">E2E</td>
<td style="text-align: left;">65.44</td>
</tr>
<tr>
<td style="text-align: left;">E2E-Reduced*</td>
<td style="text-align: left;">57.10</td>
</tr>
<tr>
<td style="text-align: left;">Expected Accuracy**</td>
<td style="text-align: left;">30.97</td>
</tr>
</tbody>
</table>
<p>Table 2: Factual accuracy predicted by different metrics on synthesized samples. Binary Classifier is described in Sec 4.3, Classifier in Sec 4.1 and E2E is the end-to-end model described in Sec 4.2. E2E-Reduced<em> is a model where sentences where no entities are detected are filtered out from the input text. The Expected Accuracy</em>* is calculated as the ratio of number of corrupted facts to the total number of facts in the article.</p>
<h2>3 DATASET</h2>
<p>We create a dataset for fact extraction using distant supervision that is based entirely on the English Wikipedia corpus and the Wikidata knowledge base $W_{K B}$ [37]. Our distant supervisor is very similar
to the one proposed by Mintz et al. [17]. Although the inputs and labels for the classifier and end-to-end model are slightly different, we start by running an NER and co-reference resolution system ${ }^{4}$ on each Wikipedia article. The topic of that article is considered as the subject $e_{s}$. The other entities $e_{j}$ found in the article are considered objects. For every pair $\left(e_{s}, e_{j}\right)$, we say they are related if there is a relation $r_{k}$ such that the triplet $\left(e_{s}, r_{k}, e_{j}\right)$ is found in $W_{K B}$. We add this triplet to a set of positive examples $E_{p}$. If no such relation exists between $e_{s}$ and $e_{j}$, we add the triplet $\left(e_{s}, r_{0}, e_{j}\right)\left(r_{0}\right.$ denotes no-relation) to a set of negative examples $E_{n}$.</p>
<h2>4 MODEL-BASED METRICS</h2>
<p>In this section we describe models that can extract fact tuples from text and how we use them to define the factual accuracy metric as defined in Eq 1. Given some input text $X$, we then extract claims made in $X$ as fact tuples.</p>
<h3>4.1 Named Entity Recognition (NER) + Relation Classifier</h3>
<p>This approach consists of two steps, where we first recognize all the named entities $e_{i}$ from $X$ and then classify relations between entity pairs $\left(e_{i}, e_{j}\right)$.
4.1.1 Named Entity Recognition. Entities are real-world objects like people, locations, organizations etc that can be identified by a proper name ${ }^{3}$. Entities can be identified with named-entity recognition (NER) systems like Chiu and Nichols [5], Finkel et al. [7], Lample et al. [9] that take in $X$ and produce the set $\left{e_{i}\right}$. NER is followed by co-reference resolution ${ }^{4}[6,11,24,25]$. Publicly available NER and co-reference systems include Stanford's CoreNLP ${ }^{5}$ and NLTK ${ }^{6}$.
4.1.2 Relation Classifier. For every pair $\left(e_{i}, e_{j}\right), e_{i} \neq e_{j}$ we consider all sentences $S_{l}$ in $X$ that contain both entities. The input to the classifier is then each of these sentences $S_{l}$. Because a sentence may contain multiple entities, we also add a prefix $S U B J$ for $e_{i}$ and $O B J$ to $e_{j}$ as a hint. For example, $X=$ Person1 was born in City1 becomes $S_{l}=S U B J[$ Person1 ] was born in OBJ[ City1 ]. Unlike Sorokin and Gurevych [32], our classifier does not require additional context. Let $s_{i}$ be a token in the input sentence $S_{l}$ after NER, and $r^{k}$ denote the $k$ th relation. Our classifier takes in input tokens $s_{i}$ that are first embedded onto a latent space, and then a stack of Transformer encoder-only layers process the whole sequence. A subsequent max-pooling layer selects one of these outputs that is then converted to a probability estimate of relations by a sigmoid operation. The exact series of operations can be viewed as:</p>
<p>$$
\begin{aligned}
w_{1: n} &amp; =\operatorname{embed}\left(s_{1: n}\right) \
h_{1: n} &amp; =\text { transformer encoder }\left(w_{1: n}\right) \
h_{i} &amp; =\max <em 1:="1:" n="n">{i}\left(h</em> \
p\left(r^{k}\right) &amp; =\frac{1}{1+e^{-h_{i}^{k}}}=\operatorname{sigmoid}\left(h_{i}^{k}\right)
\end{aligned}
$$}\right) ; h_{i} \in \Re^{k</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Figure 1a also shows the architecture of this model.
4.1.3 Dataset preparation. For every triplet $f$ in $E_{p} \cup E_{n}$, we have sentence(s) ${ }^{7} S_{j}$ in the article that may describe the relation between $e_{s}$ and $e_{j} . S_{j}$ is processed so that subject and object are prefixed with "SUBJ" and "OBJ" as a hint to the model (Section 4.1). This leads to a dataset with 2.9 million positive examples and 34 million negative examples totaling to 45 GiB on disk.
4.1.4 factacc with the Relation Classifier. The classifier predicts a relation $r_{k}$ for each entity pair $\left(e_{i}, e_{j}\right)$. We extract such triplets from the ground-truth $T$ and generated text $G$, and use the definition from eq 1 to calculate the factual accuracy.</p>
<h3>4.2 End-to-End Extraction</h3>
<p>We propose an end-to-end fact extraction model to avoid compounding of errors across components in multi-stage approaches like Section 4.1 [16]. This model also does not require any feature engineering or context. The input to the model is text $X$ of any length (sentence/paragraph/article) and the subject entity $e_{s}$ prefixed to $X$. All the inputs tokens in $\left[e_{s} ; X\right]$ are first embedded onto a latent space. A Transformer model consisting of a stack of encoder layers followed by decoder layers produces an output sequence of arbitrary length. A softmax operation is applied to every output token to define a distribution at every timestep. Figure 1b shows the architecture of this model. To encourage the model to have structured outputs, we train the model with labels that are a sequence of fact tuples. For example, if $X=$ " Person1 was born in Country1. He was a painter", then the label, $Y$, for that input is "Person1 $\langle t\rangle$ born in $\langle t\rangle$ Country1 $\langle f\rangle$ Person1 $\langle t\rangle$ profession $\langle t\rangle$ painter $\langle e n d\rangle$ ", where $\langle t\rangle$ separates tokens within the fact $f_{i}$ and $\langle t\rangle$ separates facts. For prediction, we perform a beam search over all the output timesteps, and continue decoding until $\langle e n d\rangle$ is predicted. A length-penalty $\alpha$ controls the length of this prediction as in [39].
4.2.1 Dataset preparation. If the input article text is $X$, every triplet $f_{p}$ in $E_{p}$ (we ignore the negative examples for end-to-end models because no relations between entity pairs is implied by no output by the model) is appended to the article's label $L . L$ will then contain a series of tokens that describe facts, with seperators between them. For example: $e_{s}\langle t\rangle r_{1}\langle t\rangle e_{1}\langle f\rangle e_{s}\langle t\rangle r_{2}\langle t\rangle e_{2}\langle f\rangle \ldots$ We also prepend the input text $X$ with $e_{s}\left(\left[e_{s} ; X\right]\right)$ as a hint to the model for generating facts about $e_{s}$. This leads to a dataset with 2.5 million examples totaling to 1.5 GiB on disk. ${ }^{8}$
4.2.2 factacc with the End-to-End model. The End-to-End model is able to produce a sequence of fact tuples in the form, subj $j_{1}\langle t\rangle$ rel $<em 1="1">{1}\langle t\rangle$ obj $</em>\langle f\rangle$ subj $<em 2="2">{1}\langle t\rangle$ rel $</em>$. These tuples are extracted from $T$ and $G$ to fit into the metric defined in eq 1.}\langle t\rangle$ obj $_{2}$. It is trained to output relations from a fixed schema based on WikiData. Consider an output from this model, Barack Obama $\langle t\rangle$ P69 $\langle t\rangle$ Harvard. P69 denotes 'educated at ${ }^{9</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.3 NER + Binary Relation Classifier</h3>
<p>Similar to the typical relation classifier detailed in Sec 4.1, we define a classifier that predicts whether a pair of entities $\left(e_{i}, e_{j}\right)$ are related to each other through any relation. This allows for verifying that entities are related in both the ground-truth $T$ and generated text $G$, while being flexible enough to allow for any relation types. We also note that two entities can be related to each other in multiple ways. The inputs to this model are the same as Sec 4.1, but the model is expected to output rel as</p>
<p>$$
r e l= \begin{cases}1: &amp; e_{i} \text { and } e_{j} \text { are related } \ 0: &amp; \text { otherwise }\end{cases}
$$</p>
<p>4.3.1 Dataset preparation. Data for this model is generated with the same procedure detailed in Sec 4.1.3. The only difference is the way we define the label $r e l$. We consider entities $e_{i}$ and $e_{j}$ to be related if there is a relation $r_{k}$ such that $\left(e_{i}, r_{k}, e_{j}\right)$ is found in $W_{K B}$.
4.3.2 factacc with the Binary Relation Classifier. The model predicts $r e l$ for each entity pair $\left(e_{i}, e_{j}\right)$, and we are able to extract a set of tuples of the form $\left(e_{i}, r e l, e_{j}\right)$ from both $T$ and $G$. To use eq 1 to define the factual accuracy, we filter the set by considering only entity pairs $\left(e_{i}, e_{j}\right)$ that are found in both $T$ and $G$ to then compare the predicted label rel between them.</p>
<h2>5 MODEL-FREE METRICS</h2>
<p>We describe model-free automatic metrics in this section. Unlike model-based metrics, they are not susceptible to changes in training data, and might be considered easier to interpret or understand.</p>
<h3>5.1 ROUGE</h3>
<p>ROUGE [13] has been used as an automatic metric to judge the quality of generated text, and has shown to correlate well with human judgment of overall linguistic quality of the text.</p>
<h3>5.2 OpenIE</h3>
<p>OpenIE [2] is a tool that can extract relation tuples from text, without a specified schema. We use it to extract sets of relation tuples from $T$ and $G$, and then compute the precision like in eq 1.</p>
<h2>6 MODEL EXPERIMENTS</h2>
<p>In this section, we describe the methods we used to train and evaluate our relation extraction models. All of our proposed classifiers and end-to-end models have 6 Transformer layers and 1 embedding layer, with number of neurons (hidden layer size) set to 512. In the Transformer-based models, we use 8 attention heads. Our models are trained using the AdaFactor [30] optimizer. We use the publicly available Tensor2Tensor [35] ${ }^{10}$ framework for our experiments and will be releasing our code extensions as part of that framework. On our proposed dataset, the classifiers are trained for 50,000 iterations with batch-size of 1024 and the end-to-end models are trained for 50,000 iterations with batch-size of 256.
We evaluate classifiers and end-to-end models on our dataset. These results are presented in Table 3. The end-to-end model is learning to recognize entities, resolving entity co-references, and reason</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(b) Transformer encoder-decoder</p>
<p>Figure 1: Fact extraction model architectures
about their relation in one pass through the model. To the best of our knowledge, we are not aware of other end-to-end structured relation extraction models and therefore do not include a comparison against other approaches. Some examples of extracting facts on our dataset are shown in A.2, where we include a comparison to OpenIE's triplet extraction.</p>
<p>We calculate precision and recall in the above experiments by matching ground-truth fact tuples exactly. This implies that the end-to-end model is not only learning to identify entities and resolve co-references, but also predict structured output, and its outputs can be used for reasoning. Their performance is competitive against</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">$\mathbf{P}$</th>
<th style="text-align: center;">$\mathbf{R}$</th>
<th style="text-align: center;">$\mathbf{F 1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Binary Classifier ${ }^{*}$</td>
<td style="text-align: center;">59.60</td>
<td style="text-align: center;">75.13</td>
<td style="text-align: center;">66.47</td>
</tr>
<tr>
<td style="text-align: left;">Relation Classifier</td>
<td style="text-align: center;">63.49</td>
<td style="text-align: center;">68.64</td>
<td style="text-align: center;">65.96</td>
</tr>
<tr>
<td style="text-align: left;">E2E</td>
<td style="text-align: center;">71.67</td>
<td style="text-align: center;">56.21</td>
<td style="text-align: center;">63.01</td>
</tr>
<tr>
<td style="text-align: left;">E2E-Reduced ${ }^{<em> </em>}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 1 6}$</td>
<td style="text-align: center;">61.03</td>
<td style="text-align: center;">66.13</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance (precision(P), recall(R), F1) of models on our proposed dataset grouped by classifiers and then end-to-end models. Binary Classifier is described in Sec 4.3, Classifier in Sec 4.1 and E2E is the end-to-end model described in Sec 4.2. The Binary Classifier<em> only considers the existence of a relation, and might not be directly comparable to the other models' performance. E2E-Reduced</em>* is a model where sentences where no entities are detected are filtered out from the input text. The best model is marked in bold. We consider precision(P) as the measure that matches best with the definition of $f a c t_{\text {acc }}$
relation classifiers while having a simple training and inference routine.</p>
<p>For each model, we sort and select the ten most frequent relation types that appear in our test sets. The $F 1$ measure on these relations for classifiers are shown in Table 4, and end-to-end models are shown in Table 5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Relation</th>
<th style="text-align: left;">$\mathbf{P}$</th>
<th style="text-align: left;">$\mathbf{R}$</th>
<th style="text-align: left;">$\mathbf{F 1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No relation</td>
<td style="text-align: left;">0.9830</td>
<td style="text-align: left;">0.9817</td>
<td style="text-align: left;">0.9824</td>
</tr>
<tr>
<td style="text-align: left;">Country of citizenship</td>
<td style="text-align: left;">0.6446</td>
<td style="text-align: left;">0.9394</td>
<td style="text-align: left;">0.7646</td>
</tr>
<tr>
<td style="text-align: left;">Date of birth</td>
<td style="text-align: left;">0.9330</td>
<td style="text-align: left;">0.9850</td>
<td style="text-align: left;">0.9582</td>
</tr>
<tr>
<td style="text-align: left;">Country</td>
<td style="text-align: left;">0.6049</td>
<td style="text-align: left;">0.9484</td>
<td style="text-align: left;">0.7386</td>
</tr>
<tr>
<td style="text-align: left;">Located in territory</td>
<td style="text-align: left;">0.6260</td>
<td style="text-align: left;">0.8118</td>
<td style="text-align: left;">0.7069</td>
</tr>
<tr>
<td style="text-align: left;">Instance of</td>
<td style="text-align: left;">0.5097</td>
<td style="text-align: left;">0.7015</td>
<td style="text-align: left;">0.5904</td>
</tr>
<tr>
<td style="text-align: left;">Place of birth</td>
<td style="text-align: left;">0.6430</td>
<td style="text-align: left;">0.7436</td>
<td style="text-align: left;">0.6897</td>
</tr>
<tr>
<td style="text-align: left;">Member of sports team</td>
<td style="text-align: left;">0.5179</td>
<td style="text-align: left;">0.9248</td>
<td style="text-align: left;">0.6640</td>
</tr>
<tr>
<td style="text-align: left;">Occupation</td>
<td style="text-align: left;">0.5934</td>
<td style="text-align: left;">0.7770</td>
<td style="text-align: left;">0.6729</td>
</tr>
<tr>
<td style="text-align: left;">Date of death</td>
<td style="text-align: left;">0.9163</td>
<td style="text-align: left;">0.9875</td>
<td style="text-align: left;">0.9506</td>
</tr>
</tbody>
</table>
<p>Table 4: Precision (P), Recall (R) and F1 measure of the relation classifier (Section 4.1) on our test sets on ten most frequent relations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Relation</th>
<th style="text-align: left;">$\mathbf{P}$</th>
<th style="text-align: left;">$\mathbf{R}$</th>
<th style="text-align: left;">$\mathbf{F 1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Country of citizenship</td>
<td style="text-align: left;">0.8247</td>
<td style="text-align: left;">0.8359</td>
<td style="text-align: left;">0.8302</td>
</tr>
<tr>
<td style="text-align: left;">Instance of</td>
<td style="text-align: left;">0.7212</td>
<td style="text-align: left;">0.6676</td>
<td style="text-align: left;">0.6934</td>
</tr>
<tr>
<td style="text-align: left;">Date of birth</td>
<td style="text-align: left;">0.9342</td>
<td style="text-align: left;">0.9798</td>
<td style="text-align: left;">0.9564</td>
</tr>
<tr>
<td style="text-align: left;">Country</td>
<td style="text-align: left;">0.8387</td>
<td style="text-align: left;">0.8267</td>
<td style="text-align: left;">0.8327</td>
</tr>
<tr>
<td style="text-align: left;">Cast member</td>
<td style="text-align: left;">0.5889</td>
<td style="text-align: left;">0.4910</td>
<td style="text-align: left;">0.5355</td>
</tr>
<tr>
<td style="text-align: left;">Place of birth</td>
<td style="text-align: left;">0.7012</td>
<td style="text-align: left;">0.7348</td>
<td style="text-align: left;">0.7176</td>
</tr>
<tr>
<td style="text-align: left;">Located in the adminis-</td>
<td style="text-align: left;">0.7293</td>
<td style="text-align: left;">0.7700</td>
<td style="text-align: left;">0.7491</td>
</tr>
<tr>
<td style="text-align: left;">trative territorial entity</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Member of sports team</td>
<td style="text-align: left;">0.7045</td>
<td style="text-align: left;">0.7027</td>
<td style="text-align: left;">0.7036</td>
</tr>
<tr>
<td style="text-align: left;">Occupation</td>
<td style="text-align: left;">0.5911</td>
<td style="text-align: left;">0.5774</td>
<td style="text-align: left;">0.5842</td>
</tr>
<tr>
<td style="text-align: left;">Educated at</td>
<td style="text-align: left;">0.5432</td>
<td style="text-align: left;">0.7278</td>
<td style="text-align: left;">0.6221</td>
</tr>
</tbody>
</table>
<p>Table 5: Precision (P), Recall (R) and F1 measure of our end-to-end model (Section 4.2) on our test sets on ten most frequent relations.</p>
<h2>7 ERROR ANALYSIS OF MODEL PREDICTIONS</h2>
<p>Distant supervision [17] is a way to create training data by using weak signals. In our dataset, we assign a relation label $r_{k}$ for every entity pair $\left(e_{l}, e_{j}\right)$ in the input text $X$ if the relation tuple $\left(e_{l}, r_{k}, e_{j}\right)$ exists in the Wikidata knowledge base $W_{K B}$. However, the sentence $S_{I}$ containing $\left(e_{l}, e_{j}\right)$ may not necessarily entail $r_{k}$. This leads to inaccurate estimates of the true-positive rate for our fact extraction models. We evaluate the effect of this distant supervision by gathering the set of facts extracted from our models that are marked false-positive by the distant supervision scheme. We present a pair of input text (Wikipedia articles) and facts extracted by our models to human evaluators, and ask them to mark a fact to be True only if the relation tuple (subject, relation, object) is implied by the input text. We asked two evaluators to score facts marked false-positive from a random set of 30 Wikipedia articles. We consider the fact to be true if both evaluators agree. We present the results in Table 6, where we can see the rate of false-positive facts that were marked true by the evaluators. This suggests that the end-to-end models could benefit by a better labeling scheme.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">\% True-positives</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">End-to-end</td>
<td style="text-align: left;">77.8</td>
</tr>
<tr>
<td style="text-align: left;">Relation Classifier</td>
<td style="text-align: left;">46.6</td>
</tr>
</tbody>
</table>
<p>Table 6: Percentage of true facts that were inaccurately labelled wrong by the distant supervises. The End-to-end model is the best model from Section 4.2 and Classifier is the best from 4.1(Transformer-Sigmoid). The End-to-end model (in bold) predicts facts that are likelier to be true.</p>
<h2>8 EVALUATION OF fact $f$ act $_{\text {acc }}$ AS A METRIC</h2>
<p>In this section, we show the effectiveness of our proposed metric on judging the factual accuracy of generated text. We use the text summarization model proposed in [15] to generate lead sections of Wikipedia articles using the dataset and model in that paper, and compare the generated summary against the real lead section. In the following section, we describe the methodology used to compare human judgment of factual accuracy and how we compare our metric against that baseline.</p>
<h3>8.1 Human Evaluation</h3>
<p>Every claim made in the generated text $G$ can be considered to belong to one of three categories: supported by a sentence in groundtruth $T$, refuted by $T$ or cannot be verified by $T$. The evaluators were asked to only consider claims that are either supported or refuted by $T$. This ensures that no external knowledge is used in comparing $T$ and $G$, and ignores all claims that cannot be verified by $T$. Four evaluators were asked to rate 30 examples of generated text $G$ and then give it a score of 1-5 with 5 being highest factual accuracy. A special case is where the generated text has no verifiable claims. In this case, they were asked to give it a score of 1 . Figure 2 shows the interface a human evaluator uses in our experiment.</p>
<p>We conduct the same experiment on two sets of data: first is a random sampling from summaries generated for Actors. We consider
this an easier subset because we expect our fact extraction models to do well on this subset due to the summaries and Wikipedia lead sections generally containing relationships our models perform well on (see tables 4 and 5). We present these results in Table 7. We analyzed the inter-rater agreement on the scores given to each example, and found that Krippendorff's alpha (allows for ordinal rankings) was 0.6897 . The second is a random sampling from all categories in Wikipedia. The results are presented in Table 8. The inter-rater agreement on this sample was found to be 0.7530 .</p>
<p>We see that our end-to-end model (Section 4.2) has the best correlation on both subsets, indicating that it generalizes better to generated text. This may also be because the classifier suffers from a compounding of errors, where it is unable to predict relations if the NER system fails to recognize entities.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Correlation with <br> human scores</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: left;">0.583</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: left;">0.639</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: left;">0.634</td>
</tr>
<tr>
<td style="text-align: left;">OpenIE</td>
<td style="text-align: left;">0.258</td>
</tr>
<tr>
<td style="text-align: left;">fact $f$ acc-Binary Classifier</td>
<td style="text-align: left;">0.596</td>
</tr>
<tr>
<td style="text-align: left;">fact $f$ acc-Relation Classifier</td>
<td style="text-align: left;">0.523</td>
</tr>
<tr>
<td style="text-align: left;">fact $f$ acc-E2E</td>
<td style="text-align: left;">0.645</td>
</tr>
<tr>
<td style="text-align: left;">fact $f$ acc-E2E-Reduced</td>
<td style="text-align: left;">$\mathbf{0 . 6 6 8}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Spearman correlation of different metrics with human evaluation of factual accuracy on the 'Actors' subset of summaries. ROUGE and OpenIE are described in Sec 5, and the model-based fact $f$ acc $_{\text {acc }}$ metrics are described in Sec 4. The best metric is shown in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Correlation with <br> human scores</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: left;">0.384</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: left;">0.435</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: left;">0.339</td>
</tr>
<tr>
<td style="text-align: left;">OpenIE</td>
<td style="text-align: left;">0.128</td>
</tr>
<tr>
<td style="text-align: left;">fact $f$ acc-Binary Classifier</td>
<td style="text-align: left;">0.200</td>
</tr>
<tr>
<td style="text-align: left;">fact $f$ acc-Relation Classifier</td>
<td style="text-align: left;">0.250</td>
</tr>
<tr>
<td style="text-align: left;">fact $f$ acc-E2E</td>
<td style="text-align: left;">0.314</td>
</tr>
<tr>
<td style="text-align: left;">fact $f$ acc-E2E-Reduced</td>
<td style="text-align: left;">$\mathbf{0 . 4 5 3}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Spearman correlation of different metrics with human evaluation of factual accuracy on a random subset of summaries. ROUGE and OpenIE are described in Sec 5, and the model-based fact $f$ acc $_{\text {acc }}$ metrics are described in Sec 4. The best metric is shown in bold.</p>
<h2>9 CONCLUSION</h2>
<h3>9.1 Limitations</h3>
<p>The dataset we create only makes use of sentences found in Wikipedia, and facts found in WikiData. This means that our models are biased to sentences structured to the neutral tone set in Wikipedia, and towards popular types of facts expressed in WikiData such as date of birth, profession, etc. Other sources of text may have more complex structures and styles of writing that may make it hard for our</p>
<p>Wiki (Ground-truth):
Waiohinu (Hawaiian translation: "Shiny Water") is an unincorporated community and census-designated place (CDP) in the district of Ka'u on the Big Island- Hawai'i, in the U.S. state of Hawaii. Waiohinu is the name of the community as well as the ahupua'a, Native Hawaiian subdivision. As of the 2010 census, the CDP had a population of 213.</p>
<h2>Model to assess:</h2>
<p>Waiohinu is a census-designated place (CDP) in Hawaii, United States. The population was 213 at the 2010 census.</p>
<p>Factual accuracy: (2)
Very Poor $\bigcirc \bigcirc \bigcirc \bigcirc$ Very Good</p>
<p>Figure 2: A screenshot of the interface presented to human evaluators to judge the factual accuracies of generated text. The ground-truth text is shown on the left, with the model generated text on the right. The evaluator is then asked to rate the factual accuracy of the generated text on a five point scale of 'Very Poor' to 'Very Good'
models to adapt to easily. An simple example of this is negating a binary relationship with 'not', and different ways of expressing the same idea such as 'wife/husband' instead of 'spouse'. WikiData is an incomplete knowledge base, and this also leads to many sentences that in reality imply a fact to be marked containing no facts. This is a very typical problem faced by any work using distant supervision, and is combated with methods like active learning [31].
It should be noted that ROUGE and to the best of our knowledge, most other automatic metrics, are also susceptible to changes in linguistic style and structure. However, elaborate labeling and bigger datasets will allow for our models to learn to overcome these challenges.</p>
<h3>9.2 Discussion and future work</h3>
<p>We have shown that our proposed metric is able to indicate the factual accuracy of generated text, and agrees with human judgment on our datasets. By leveraging a new dataset for both relation classification and end-to-end fact extraction, we also showed that classifiers and end-to-end models with straightforward architectures are able to perform competitive fact extraction.
Our end-to-end model avoids compounding of errors over subcomponents typically used in other fact-extraction pipelines. We will release the code and datasets used to train this model, so that the proposed metric can be used to standardize comparison. We are in the process of building a bigger dataset that will contain multiple text domains, stronger human supervision and a larger collection of relation tuples that will help overcome many of the limitations discussed in the previous section (9.1). We encourage further development and use of this metric for automating the assessment of factual accuracy of generated text, and the development of better end-to-end models with structured outputs for fact extraction.</p>
<h2>REFERENCES</h2>
<p>[1] Dimitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In International Conference on Learning Representations.
[2] Michele Banks, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open Information Extraction from the Web. In Proceedings of the 20th International Joint Conference on Artificial Intelligence. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2670-2676.
[3] Peter F. Brown, Vincent J. Della Pietra, Robert L. Mercer, Stephen A. Della Pietra, and Jennifer C. Lai. 1992. An Estimate of an Upper Bound for the Entropy of English. Computational Linguistics 18, 1 (March 1992), 31-40.
[4] Zajiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2017. Faithful to the Original: Fact Aware Neural Abstractive Summarization. CoRR abs/1711.04434 (2017). arXiv:1711.04434 http://arxiv.org/abs/1711.04434
[5] Jason Chiu and Eric Nichols. 2016. Named Entity Recognition with Bidirectional LSTM-CNNs. Transactions of the Association for Computational Linguistics 4 (2016), 357-370.
[6] Kevin Clark and Christopher D. Manning. 2016. Improving Coreference Resolution by Learning Entity-Level Distributed Representations. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 643-653. https://doi.org/10.18653/v1/P16-1061
[7] Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005). 363-370.
[8] Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi Fukumoto. 2006. Automated Summarization Evaluation with Basic Elements. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC'06). European Language Resources Association (ELRA).
[9] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural Architectures for Named Entity Recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 260-270. https://doi.org/10.18653/v1/N16-1030
[10] Alon Lavie and Abhaya Agarwal. 2007. Meteor: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments. In Proceedings of the Second Workshop on Statistical Machine Translation. Association for Computational Linguistics, Stroudsburg, PA, USA, 228-231.
[11] Heeyoung Lee, Yves Peirssian, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford's Multi-Pass Sieve Coreference Resolution System at the CoNLL-2011 Shared Task. In In Proceedings of the CoNLL-2011 Shared Task.
[12] Jiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean, Alan Ritter, and Dan Jurafsky. 2017. Adversarial Learning for Neural Dialogue Generation. In Conference on Empirical Methods in Natural Language Processing. 2157-2169.
[13] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, Stan Szpakowicz Marie-Francine Moens (Ed.). Association for Computational Linguistics, Barcelona, Spain, 74-81.
[14] Yankai Lin, Shuji Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. 2016. Neural Relation Extraction with Selective Attention over Instances. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. 21242133.
[15] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Łukasz Kaiser, and Noam Shazeer. 2018. Generating Wikipedia by Summarizing Long Sequences. In Proceedings of the 2018 International Conference on Learning Representations.
[16] Andrew Mccallum and David Jensen. 2003. A Note on the Unification of Information Extraction and Data Mining using Conditional-Probability, Relational Models. In In Proceedings of the IJCAI-2003 Workshop on Learning Statistical Models from Relational Data.</p>
<p>[17] Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant Supervision for Relation Extraction Without Labeled Data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. 1003-1011.
[18] Makoto Miwa and Mohit Bansal. 2016. End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 1105-1116. https://doi.org/10.18633/ v1/P16-1105
[19] Makoto Miwa and Yutaka Sasaki. 2014. Modeling Joint Entity and Relation Extraction with Table Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. 1858-1869.
[20] Thabir P. Mohamed, Estevan R. Hruschka, Jr., and Tom M. Mitchell. 2011. Discovering Relations Between Noun Categories. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Stroudsburg, PA, USA, 1447-1455.
[21] Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos Santos, Çaglar Gülçehre, and Bing Xiang. 2016. Abstractive Text Summarization using Sequence-tosequence RNNs and Beyond. In Proceedings of the 2016 SIGNLL Conference on Computational Natural Language Learning.
[22] Ani Nenkova and Rebecca J. Passonneau. 2004. Evaluating Content Selection in Summarization: The Pyramid Method.. In Proceedings of the 2005 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 145-152.
[23] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, Stroudsburg, PA, USA, 311-318. https://doi.org/ $10.3115 / 1073083.1073135$
[24] Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning. 2010. A Multi-pass Sieve for Coreference Resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Stroudsburg, PA, USA, 492-501.
[25] Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The Life and Death of Discourse Entities: Identifying Singleton Mentions. In Proceedings of the 2013 North American Chapter of the Association for Computational Linguistics.
[26] Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M. Marlin. 2013. Relation Extraction with Matrix Factorization and Universal Schemas. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 74-84.
[27] Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A Neural Attention Model for Abstractive Sentence Summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.
[28] Julian Vlad Serban, Tim Klinger, Gerald Tesauro, Kartik Talamadupula, Bowen Zhou, Yoshua Bengio, and Aaron C. Courville. 2017. Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation. In Proceedings of the 2017 AAAI Conference on Artificial Intelligence. 3288-3294.
[29] Julian Vlad Serban, Chinnadhurai Sankar, Mathieu Germain, Saizheng Zhang, Zhouhan Lin, Sundeep Subramanian, Taesup Kim, Michael Pieper, Sarath Chandar, Nan Rosemary Ke, Sai Mukamba, Alexandre de Brebisson, Jose Sotelo, Derali Suhubdy, Vincent Michalski, Alexandre Nguyen, Joelle Pineau, and Yoshua Bengio. 2017. A Deep Reinforcement Learning Chatbot. CoRR abs/1709.02349 (2017). arXiv:1709.02349 http://arxiv.org/abs/1709.02349
[30] Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018. 4603-4611.
[31] Yanyao Shen, Hyokun Yun, Zachary C. Lipton, Yakov Kronrod, and Animashree Anandkumar. 2017. Deep Active Learning for Named Entity Recognition. CoRR abs/1707.05928 (2017). arXiv:1707.05928 http://arxiv.org/abs/1707.05928
[32] Daniil Sorokin and Iryna Gurevych. 2017. Context-Aware Representations for Knowledge Base Relation Extraction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 1784-1789.
[33] Josef Steinberger and Karel Jezek. 2009. Evaluation Measures for Text Summarization. Computing and Informatics 28 (2009), 251-275.
[34] Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multi-instance Multi-label Learning for Relation Extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics, Stroudsburg, PA, USA, 455-465.
[35] Ashish Vaswani, Samy Bengio, Eugene Brevdo, François Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. 2018. Tensor2Tensor for Neural Machine Translation. arXiv preprint arXiv:1803.07416 (2018). http: //arxiv.org/abs/1803.07416
[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). 5998-6008.
[37] Denny Vrandecic and Markus Krötzsch. 2014. Wikidata: A Free Collaborative Knowledgebase. Commun. ACM 57 (2014), 78-85. Issue 10.
[38] Sam Wiseman, Stuart M. Shieber, and Alexander M. Rush. 2017. Challenges in Data-to-Document Generation. CoRR abs/1707.08052 (2017). arXiv:1707.08052 http://arxiv.org/abs/1707.08052
[39] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. CoRR abs/1609.08144 (2016). arXiv:1609.08144 http://arxiv. org/abs/1609.08144
[40] Hengtong Zhang, Yaliang Li, Fenglong Ma, Jing Gao, and Lu Su. 2018. TextTruth: An Unsupervised Approach to Discover Trustworthy Information from Multi-Sourced Text Data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&v30; Data Mining (KDD '18). ACM, New York, NY, USA, 2729-2737. https://doi.org/10.1145/3219819.3219977</p>
<h2>A APPENDIX</h2>
<h2>A. 1 Reproducibility</h2>
<p>We release code to train our fact extraction models as part of the Tensor2Tensor framework ${ }^{11}$ along with trained model weights at https: //github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/ data_generators/wikifact. A large fact extraction dataset (Sec 3) based on Wikidata and Wikipedia is made available ${ }^{12}$. To train our end-to-end and classifier models for fact extraction, we use the hyper-parameter set "transformer_base" defined in the Tensor2Tensor framework ${ }^{13}$. We further release code to use our end-to-end models as a fact extractor and calculate the factual accuracy
metric at https://github.com/tensorflow/tensor2tensor/tree/master/ tensor2tensor/data_generators/wikifact.</p>
<h2>A. 2 Fact extraction example</h2>
<p>We include an example of facts extracted from text using our models where we compare it against OpenIE's [2] triplet extraction in Table 9. This example illustrates the advantage of using structured approaches to fact extraction. OpenIE yields many triplets that mostly cannot be used for reasoning.
${ }^{11}$ https://github.com/tensorflow/tensor2tensor
${ }^{12}$ https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_ generators/wikifact
${ }^{13}$ https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/ transformer.py</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">Christopher Simon (born 5 June 1963) is an Australian actor and producer. Born in Sydney, Australia. He produced the film Miss You Already directed by Catherine Hardwicke. Simon is also a producer of such films as The Sweeney (2012 film) directed by Nick Love, Pusher, I, Anna, Still Life, Me and Me Dad, Boogie Woogie, The Proposition, Beyond the Ocean, The Trouble with Men and Women. He also produced short films by Joe Wright such as The End and Nick Love's Love Story. Simon's various television acting roles include Eddie in The Long Firm, Pedro in Gimme Gimme Gimme, Michael Hassan in The Bill, Lee Andersen in Casualty, Abdel in Lovejoy Samir in Ultimate Force, Da Souza in Lynda La Plante's Supply and Demand, Nathan Morgan in Wire In The Blood and he appeared in Lenny Henry in Pieces. Film acting roles include Room To Rent, The Delivery and O Jerusalem. Simon has acted in such plays as 12 Angry Men and Taking Sides both directed by Harold Pinter in London's west end, The Kitchen directed by Stephen Daldry at the Royal Court, the Amnesty award winning one man show When The Bulbull Stopped Singing for which he was nominated for the Acting Excellence Award (Best Actor) at the Edinburgh Festival Fringe, which premiered at the Traverse theatre and toured to Iran, New York and Jordan. Other theatre roles include Welcome to Ramallah, which toured York and London, at the Arcola and the Theatre Royal York, The Present at the Royal Court and the Bush, and Poor Superman at the Hampstead and the Traverse.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Targets</td>
<td style="text-align: center;">(Christopher Simon, date of birth, June 5 1963), (Christopher Simon, country of citizenship, Australian), (Christopher Simon, place of birth, Sydney)</td>
</tr>
<tr>
<td style="text-align: center;">OpenIE</td>
<td style="text-align: center;">(Abdel, is in, Ultimate Force), (Casualty, Abdel in, Ultimate Force), (Nathan Morgan, is In, Blood), (Lee Andersen, is in, Casualty), (Da Souza, is in, Lynda La Plante 's Supply), (Simon 's various television acting roles, include, Eddie), (Simon, is producer of, films as Sweeney directed by Nick Love), (Simon, is also producer of, such films as Sweeney), (Simon 's television roles, include, Eddie in Firm), (Simon, is producer of, such films), (Simon 's various television roles, include, Eddie), (Eddie, is in, Long Firm), (Simon, is producer of, such films as Sweeney), (Simon, is producer of, such films as Sweeney directed by Nick Love), (Michael Hassan, is in, Bill), (Bill, Andersen in, Casualty), (You, Already directed by, Catherine Hardwicke), (Simon, is producer of, films), (Simon, has, various television acting roles), (Simon 's television acting roles, include, Eddie), (Abdel, is in, Lovejoy Samir), (Simon 's television roles, include, Eddie in Long Firm), (Simon 's television acting roles, include, Eddie in Firm), (Simon, is producer of, films as Sweeney directed), (Simon 's various television acting roles, include, Eddie in Firm), (Simon, is also producer of, films as Sweeney), (Simon 's various television acting roles, include, Eddie in Long Firm), (Simon, is, producer), (Rent, To Room is, Delivery), (Simon 's television roles, include, Eddie), (Simon, is also producer of, films as Sweeney directed), (Lynda La Plante, in, Supply), (Pedro, is in, Gim), ...</td>
</tr>
<tr>
<td style="text-align: center;">Seq2Seq</td>
<td style="text-align: center;">(Christopher Simon, date of birth, June 5 1963), (Christopher Simon, country of citizenship, Australian), (Christopher Simon, place of birth, Sydney), (Christopher Simon, occupation, Actor)</td>
</tr>
<tr>
<td style="text-align: center;">Classifier</td>
<td style="text-align: center;">(Christopher Simon, date of birth, June 5 1963), (Christopher Simon, country of citizenship, Australian)</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparison of fact tuples extracted from this example, using OpenIE, our end-to-end model (Section 4.2), and our classifier (Section 4.1). A triplet consists of (subject, relation, object).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ There may be more than one sentence in the article that have mentions of the subject and object entity pair.
${ }^{8}$ This dataset is made available at https://github.com/tensorflow/tensor2tensor/tree/ master/tensor2tensor/data_generators/wikifact
${ }^{9}$ https://www.wikidata.org/wiki/Property:P69&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{10}$ https://github.com/tensorflow/tensor2tensor&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2484 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2484</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2484</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-267770682</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.13610v1.pdf" target="_blank">Data-driven Discovery with Large Generative Models</a></p>
                <p><strong>Paper Abstract:</strong> With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially. This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-to-end data-driven discovery -- a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments. We first outline several desiderata for an ideal data-driven discovery system. Then, through DATAVOYAGER, a proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of these desiderata -- a feat previously unattainable -- while also highlighting important limitations in the current system that open up opportunities for novel ML research. We contend that achieving accurate, reliable, and robust end-to-end discovery systems solely through the current capabilities of LGMs is challenging. We instead advocate for fail-proof tool integration, along with active user moderation through feedback mechanisms, to foster data-driven scientific discoveries with efficiency and reproducibility.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2484.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2484.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DATAVOYAGER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DATAVOYAGER</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proof-of-concept multi-agent, LGM-powered system (built on GPT-4 and AutoGen) for end-to-end data-driven discovery that semantically understands datasets, generates and prioritizes hypotheses, generates code or function calls to verify hypotheses, and ingests user feedback for continual refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DATAVOYAGER</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DATAVOYAGER is a role-based multi-agent architecture implemented with GPT-4 and the AutoGen framework. Core agents include a Planner (interprets queries and generates multi-step plans and hypothesis search strategies), Programmer (performs data transformation, code generation, and calls structured pre-defined functions to execute analyses), Data Expert (interprets results, extracts insights, connects to interdisciplinary knowledge), Critic (evaluates analyses and methods), and User Proxy (facilitates human feedback). The system supports two operation modes: fully autonomous (input = dataset + metadata) and user-guided (dataset + natural-language query). Verification is performed via function-calls or generated code to run statistical tests (e.g., OLS, GLM, t-tests) and the architecture emphasizes fail-proof tool integration and human-in-the-loop moderation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General data-driven scientific discovery (demonstrated on social-science/observational datasets such as the National Longitudinal Surveys), but intended to be domain-agnostic (social sciences, health economics, computational science, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Qualitative planner-driven prioritization: the Planner decomposes the hypothesis-space and assigns analyses to agents; prioritization is guided by user goals when present (goal-driven), and by LGM priors/novelty heuristics in autonomous mode. The paper describes the need to prioritize hypotheses by marginal costs and scientific importance but DATAVOYAGER's baseline implementation does not implement a quantitative allocation algorithm (no explicit cost-aware optimization routine is specified).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not specified in the implementation; the paper highlights computational cost concerns at scale (e.g., high-throughput hypothesis testing) and calls for integrated cost-benefit analyses, but DATAVOYAGER does not provide an explicit cost metric (wall-clock, FLOPs, or monetary cost are not defined).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not specified; the paper discusses information gain conceptually (references entropy/utility-based methods in related work) and suggests LGMs can estimate novelty or likelihood of hypotheses, but DATAVOYAGER does not implement a formal information-gain objective (no mutual information, expected improvement, or entropy-based score is defined).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Qualitative: planner tends to prefer goal-oriented (exploitation) variables when provided with a user objective and may perform wider exploratory searches in autonomous mode. The system surface-level behaviour favors direct, extrinsic-goal variables; intrinsic-motivated exploration is discussed as an alternative but is not formalized in the baseline system.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promotion mechanism is implemented in the baseline. The paper surveys intrinsic metrics (diversity, curiosity, information gain) as desirable drivers and argues for research to incorporate them, but DATAVOYAGER currently lacks a formal diversity-enforcing component.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Not concretely specified; the paper discusses budget-like constraints conceptually (computational cost, high-throughput settings, need to limit number of tests) but DATAVOYAGER does not implement a declared budget type (e.g., fixed experiment count or time limit).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Not implemented in the baseline; the paper recommends integrating cost-benefit analyses and predictive-hazard style decision rules for budget-aware allocation but provides no concrete mechanism inside DATAVOYAGER.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not defined quantitatively; the paper refers to novelty, 'interestingness', and connection to knowledge frontiers as qualitative markers of high-impact discoveries but no numeric novelty score or threshold is implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>DATAVOYAGER is positioned and compared conceptually in a survey against prior systems (CoScientist, DataLume, AutoML, MLAgentBench) but there are no empirical baseline comparison experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Conceptual discussion only: the paper highlights tradeoffs between exploration and exploitation, the risk of p-hacking from vast hypothesis spaces, and computational cost at scale. It argues these tradeoffs must be handled via principled allocation (cost-benefit analyses) and human moderation, but provides no quantitative tradeoff analysis for DATAVOYAGER.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>No formal optimal allocation is derived; recommended principles include (1) integrate fail-proof tools to ensure correct execution, (2) incorporate cost-benefit/predictive-hazard decision rules to manage computational costs, (3) enable human-in-the-loop moderation and continual learning, and (4) research intrinsic-motivated exploration and diversity promotion to avoid capture by local optima.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Discovery with Large Generative Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2484.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2484.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prioritized Hypothesis Search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prioritized Hypothesis Search (Weitzman / Agrawal model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual/resource-allocation model for selecting experiments or hypotheses to evaluate by prioritizing candidates according to expected marginal value or objective gradients; applied to guide search toward high-value discoveries under limited resources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Artificial intelligence and scientific discovery: A model of prioritized search.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Prioritized Hypothesis Search (Weitzman/Agrawal)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A theoretical approach that frames discovery as a search/selection problem where the decision-maker orders candidate hypotheses or experiments by expected marginal payoff (e.g., expected improvement toward an extrinsic goal). The paper references Weitzman's 'optimal search for the best alternative' and Agrawal et al.'s model of prioritized search as frameworks for goal-driven allocation, where greedy or gradient-driven selectors pick variables/models that most directly advance the extrinsic objective.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General scientific discovery and experiment prioritization; discussed in context of data-driven hypothesis search over large observational datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Greedy/priority ordering based on expected contribution to a user-defined extrinsic objective: choose hypotheses or variable-model pairs that maximize immediate expected objective gain per cost. The paper notes such approaches risk local optima and capture when questions are open-ended.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not specified in the cited conceptual models as discussed in this paper; typically the models consider 'cost' abstractly (effort or resource consumption) but DATAVOYAGER's discussion does not instantiate a numeric cost metric.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not explicitly defined in the paper's discussion; the models referenced focus on marginal expected objective value rather than explicit information-theoretic measures, though ties to expected improvement or utility are conceptually implied.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Primarily exploitation/goal-driven: selection favors hypotheses that most directly optimize the extrinsic goal (exploitation), with the paper noting the danger of missing novel findings and advocating complementary intrinsic drivers to encourage exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not inherent; these prioritized greedy strategies do not explicitly promote diversity unless augmented by additional objectives or constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Implicit (limited resource for exploration) but not specified concretely in the paper's discussion of these models.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handled conceptually by prioritizing highest marginal-value candidates under the given resource limit (i.e., choose top-k under budget), but no concrete algorithmic budget-handling is specified in DATAVOYAGER's use of the idea.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Implicitly measured by contribution to extrinsic objective (e.g., increase in objective metric), not by an explicit novelty/breakthrough score in the referenced works as discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Conceptually compared against intrinsic/diversity-driven search approaches (which encourage exploration and serendipity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Discussed qualitatively: goal-driven prioritized search is efficient for achieving a specified objective but can miss serendipitous or interdisciplinary breakthroughs; the paper argues for integrating intrinsic metrics (diversity, curiosity, information gain) to mitigate this risk.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation: use prioritized search when a clear extrinsic objective exists, but incorporate mechanisms to ensure coverage/diversity or occasional exploratory moves to avoid local optima; integrate cost/importance weighting (marginal cost vs scientific importance) in prioritization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Discovery with Large Generative Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2484.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2484.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Intrinsic Exploration / Diversity Methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intrinsic-motivation, curiosity, and diversity-driven exploration methods (e.g., VIME, RND, Diversity is All You Need)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of methods that guide search/exploration using intrinsic objectives (novelty, diversity, curiosity, information gain) instead of or alongside extrinsic goals, encouraging open-ended and serendipitous discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diversity is all you need: Learning skills without a reward function.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Intrinsic Exploration / Diversity-Driven Search</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>These methods define intrinsic utility functions (novelty, prediction error, variational information gain) to score candidate actions/hypotheses; selection rules then allocate resources to items maximizing intrinsic utility, e.g., RND (Random Network Distillation) for novelty, VIME (variational information maximizing exploration) for information gain, Eysenbach et al.'s diversity objectives to learn diverse behaviors/solutions. The paper cites these as possible drivers of hypothesis search to encourage serendipity and diversity in data-driven discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General exploration tasks, reinforcement learning, and proposed here as applicable to hypothesis generation/search in scientific datasets (open-ended discovery settings).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate evaluation budget to hypotheses or experiments with high intrinsic utility (novelty score, prediction error, information gain estimates), thereby preferring items likely to yield new knowledge rather than immediate extrinsic objective improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not specified in the paper's discussion; method-dependent (could be number of model evaluations, training iterations, or compute used to estimate intrinsic utility).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Yes for some methods (VIME explicitly maximizes information gain about agent/environment dynamics); others use proxy novelty metrics (prediction error, state-visitation counts). The paper references Hennig & Schuler's and Houthooft's works as examples of info-based objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration-leaning: intrinsic score drives exploration; exploitation can be reintroduced by mixing intrinsic and extrinsic objectives or via annealing policies; the paper highlights intrinsic drivers as a counterpoint to greedy, goal-oriented search.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit: diversity-promoting objectives or novelty measures (e.g., training policies to maximize diversity in learned skills, maximizing disagreement/prediction variance) are central in these methods.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Not specified within DATAVOYAGER; in general methods may be applied under an experiment/evaluation budget (fixed number of trials) or compute budget but the paper does not provide specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Not specified; typical strategies include prioritizing highest intrinsic-utility candidates until budget exhausted or hybrid rules that trade intrinsic score against cost, but DATAVOYAGER does not implement these quantitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not standardized; breakthrough potential is proxied by novelty/diversity scores or large information gain; the paper suggests these intrinsic metrics could signal serendipitous or high-impact discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Contrasted with goal-driven/greedy prioritized search approaches; no empirical comparisons are presented in this paper for DATAVOYAGER.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Qualitatively discussed: intrinsic methods increase exploration and serendipity but can raise false positives and computational cost; the paper calls for research into how contexts/domains/hypothesis spaces affect this tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation: combine extrinsic and intrinsic objectives to balance focused progress and exploration; incorporate mechanisms to control false discoveries (multiple-testing correction) when enabling large-scale exploratory search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Discovery with Large Generative Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2484.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2484.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entropy Search / Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entropy Search for Information-efficient Global Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Entropy search is a Bayesian optimization framework that chooses evaluations to maximally reduce uncertainty about the global optimum, using information-theoretic acquisition functions (expected reduction in entropy over the argmax).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Entropy search for information-efficient global optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Entropy Search / Information-Theoretic BO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A Bayesian optimization approach where acquisition functions are derived from expected information gain (e.g., reduction in posterior entropy about the location of the optimum). The paper references entropy/ information-gain based approaches as formal ways to guide hypothesis/experiment allocation in data-driven discovery when an optimization objective exists.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Global optimization and experiment design; proposed as applicable to selecting experiments/hypotheses in scientific discovery for maximum information per evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Select next evaluations to maximize expected information gain about the global optimum (e.g., highest expected reduction in entropy), implicitly balancing the utility of an evaluation against its cost if cost-aware variants are used.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not specified in this paper's discussion of the method; typical BO work uses number of function evaluations or wall-clock time, but DATAVOYAGER does not integrate a concrete metric.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Explicit: expected reduction in posterior entropy over the location/value of the optimum; acquisition function is information-theoretic.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Inherently balances exploration and exploitation by selecting evaluations that reduce uncertainty about the optimum; tends to sample informative regions even if immediate objective improvement is modest.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via information-seeking: by valuing uncertainty reduction, the method samples diverse regions informative about the optimum, though diversity is not enforced as an explicit coverage constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Typically number of evaluations or compute budget in BO practice; the paper suggests such frameworks are relevant but does not show DATAVOYAGER integrating BO.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>In BO, handled by stopping after fixed number of evaluations or cost-aware acquisition; DATAVOYAGER does not implement this.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Reduction in entropy about the optimum or improvement in objective value; the paper references such metrics conceptually but no instantiation is provided for hypotheses discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Typically compared to random search or expected improvement heuristics in the BO literature; DATAVOYAGER only cites entropy-search as a conceptual option.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper mentions information-gain methods as candidate strategies to balance evaluation cost and discovery value, but provides no empirical tradeoff analysis within DATAVOYAGER.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Suggested insight: information-theoretic acquisition rules (entropy search) are principled for allocating scarce evaluations to maximize knowledge about optimal hypotheses, but integration with domain tools, multiple-hypothesis testing correction, and cost-awareness is necessary for real data-driven discovery pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Discovery with Large Generative Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2484.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2484.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cost-Benefit / Predictive Hazard Allocation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cost-Benefit Resource Allocation with Predictive Hazard Functions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed allocation strategy that explicitly integrates computational/experimental costs with predicted payoff (hazard/predictive models) to choose which hypotheses or experiments to run under budget constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Cost-Benefit / Predictive Hazard Allocation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in the paper as a recommended research direction: incorporate cost-benefit analyses (e.g., predictive hazard functions) into discovery systems to manage the computational costs of high-throughput hypothesis evaluation. The idea is to estimate a predictive hazard or success probability conditioned on available evidence and allocate resources to hypotheses with favorable payoff-to-cost ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>High-throughput scientific discovery domains (computational biology, large-scale social-data analysis) where many hypotheses may be tested and costs per evaluation vary.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Rank hypotheses by predicted expected utility per unit cost (i.e., predictive hazard or probability of success × impact divided by computational/experimental cost) and allocate budget to top-ranked items until budget exhausted or marginal utility falls below threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not specified concretely; paper suggests computational cost is a key constraint and implies metrics like compute time or number of hypothesis evaluations but does not define one.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not specified; approach conceptually balances predicted payoff (could include information gain) against cost, but no explicit information-theoretic metric is defined in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Cost-aware exploitation-biased with potential exploration through explicit utility definitions that reward novelty or high-uncertainty items; the paper recommends this hybrid approach but offers no algorithmic detail for DATAVOYAGER.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not defined; diversity could be included as part of the expected utility (valuing novelty/breadth), but the paper only suggests the idea, not an implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational/experimental resource budget in high-throughput settings (implied), e.g., limit on number of hypotheses evaluated or compute/time allowed.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Proposed handling: integrate predictive hazard / expected payoff per cost models to prioritize evaluations under budget; no concrete implementation in DATAVOYAGER.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Implied to be expected payoff or 'probability of success × impact' (qualitative), but not formalized in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not evaluated empirically; proposed as superior to naive exhaustive or purely greedy approaches in high-throughput scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper argues conceptually that explicit cost-benefit allocation is necessary to manage the high computational costs of large-scale hypothesis testing and to reduce accidental p-hacking by limiting frivolous tests, but provides no empirical quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation: develop and integrate predictive-hazard or expected-utility based allocation rules into discovery systems to trade off computational cost, information gain, and breakthrough potential; combine with multiple-testing controls and human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Discovery with Large Generative Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2484.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2484.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoScientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoScientist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that uses large generative models to automate parts of scientific workflows (notably in laboratory/chemical research), but still requires substantial human intervention and wet-lab experiments for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous chemical research with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CoScientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CoScientist is cited as a recent system that automates some parts of the scientific workflow using LGMs, for example proposing experiments or protocols in chemistry, but it is not fully autonomous—wet-lab verification and human oversight remain necessary. The paper uses CoScientist as a comparator to emphasize DATAVOYAGER's focus on dataset-based verification (no wet lab) and the need for fail-proof tool integration and human moderation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry / autonomous wet-lab research (as demonstrated in literature cited); contrasted here with data-driven (no new data collection) discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Not detailed in this paper's discussion; CoScientist as cited conducts experiment proposal and execution in lab settings with human oversight—resource allocation specifics are not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Mentioned as requiring substantial human intervention for verification, underscoring the paper's point that full autonomy remains difficult and that data-driven verification (as in DATAVOYAGER) is more tractable for automated end-to-end pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Discovery with Large Generative Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2484.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2484.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DataLume</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DataLume</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that automates code generation for data transformation and hypothesis verification at scale, but lacks modules for hypothesis search and orchestrating complex scientific workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DataLume</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DataLume is referenced as a system that fully automates code generation for data transformation and hypothesis verification; however, the paper notes it does not include capabilities for hypothesis search or multi-step research planning/orchestration needed for end-to-end discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Automated data transformation and analysis for tabular datasets; relevant to data analysis pipelines but not full discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Not described in this paper; DataLume focuses on automating execution rather than on allocating experiments or balancing cost versus information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Discovery with Large Generative Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2484.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2484.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLAgentBench / AutoML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLAgentBench and AutoML systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AutoML frameworks and MLAgentBench automate model-building and benchmarking (hyperparameter/model search), but they do not address semantic hypothesis generation or principled allocation of scientific experiments balancing cost against information gain in discovery contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLAgentBench / AutoML frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AutoML systems (Scikit-AutoML, cloud AutoML platforms) automate model selection and hyperparameter search, optimizing predictive performance under compute/service constraints; MLAgentBench extends benchmarking to end-to-end AI agents. The paper contrasts these with data-driven discovery systems by noting AutoML lacks semantic understanding needed for hypothesis generation and orchestration, and MLAgentBench focuses on model-centric objectives rather than scientific-discovery resource allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Automated machine-learning model construction and agent benchmarking; referenced as related but not sufficient for scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>AutoML uses search over hyperparameters and model architectures often within specified compute/time budgets (usually via black-box optimization or bandit/early-stopping rules); MLAgentBench evaluates agents but does not allocate scientific experiments. The paper notes these systems optimize prediction performance rather than scientific-information utility.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>In AutoML practice, typically number of trials, wall-clock time, or monetary cloud cost; this paper does not detail their exact metrics within the discovery context.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not applicable in their standard form; AutoML measures predictive performance (validation error) rather than information gain about scientific hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>AutoML uses exploration/exploitation heuristics (e.g., bandits, Bayesian optimization) for hyperparameter search; these mechanisms are for model optimization, not hypothesis diversity or discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not oriented to hypothesis diversity; diversity in AutoML is typically diversity of model architectures or hyperparameter settings.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Time/compute or trial budget in AutoML; the paper notes these constraints but emphasizes different objective alignment for discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handled by scheduling, early stopping, or specified trial limits in AutoML systems; the paper argues such budget handling does not equate to prioritizing scientific hypotheses by information gain or breakthrough potential.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper uses AutoML and MLAgentBench as contrasts: they solve some optimization/allocation problems but do not address semantic hypothesis search nor the combination of cost, information gain, and diversity needed for data-driven scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Discovery with Large Generative Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Artificial intelligence and scientific discovery: A model of prioritized search. <em>(Rating: 2)</em></li>
                <li>Optimal search for the best alternative <em>(Rating: 2)</em></li>
                <li>Entropy search for information-efficient global optimization. <em>(Rating: 2)</em></li>
                <li>Vime: Variational information maximizing exploration. <em>(Rating: 2)</em></li>
                <li>Diversity is all you need: Learning skills without a reward function. <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models. <em>(Rating: 2)</em></li>
                <li>Efficient and robust automated machine learning. <em>(Rating: 1)</em></li>
                <li>Benchmarking large language models as ai research agents. <em>(Rating: 1)</em></li>
                <li>A practical guide to methods controlling false discoveries in computational biology. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2484",
    "paper_id": "paper-267770682",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "DATAVOYAGER",
            "name_full": "DATAVOYAGER",
            "brief_description": "A proof-of-concept multi-agent, LGM-powered system (built on GPT-4 and AutoGen) for end-to-end data-driven discovery that semantically understands datasets, generates and prioritizes hypotheses, generates code or function calls to verify hypotheses, and ingests user feedback for continual refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DATAVOYAGER",
            "system_description": "DATAVOYAGER is a role-based multi-agent architecture implemented with GPT-4 and the AutoGen framework. Core agents include a Planner (interprets queries and generates multi-step plans and hypothesis search strategies), Programmer (performs data transformation, code generation, and calls structured pre-defined functions to execute analyses), Data Expert (interprets results, extracts insights, connects to interdisciplinary knowledge), Critic (evaluates analyses and methods), and User Proxy (facilitates human feedback). The system supports two operation modes: fully autonomous (input = dataset + metadata) and user-guided (dataset + natural-language query). Verification is performed via function-calls or generated code to run statistical tests (e.g., OLS, GLM, t-tests) and the architecture emphasizes fail-proof tool integration and human-in-the-loop moderation.",
            "application_domain": "General data-driven scientific discovery (demonstrated on social-science/observational datasets such as the National Longitudinal Surveys), but intended to be domain-agnostic (social sciences, health economics, computational science, etc.).",
            "resource_allocation_strategy": "Qualitative planner-driven prioritization: the Planner decomposes the hypothesis-space and assigns analyses to agents; prioritization is guided by user goals when present (goal-driven), and by LGM priors/novelty heuristics in autonomous mode. The paper describes the need to prioritize hypotheses by marginal costs and scientific importance but DATAVOYAGER's baseline implementation does not implement a quantitative allocation algorithm (no explicit cost-aware optimization routine is specified).",
            "computational_cost_metric": "Not specified in the implementation; the paper highlights computational cost concerns at scale (e.g., high-throughput hypothesis testing) and calls for integrated cost-benefit analyses, but DATAVOYAGER does not provide an explicit cost metric (wall-clock, FLOPs, or monetary cost are not defined).",
            "information_gain_metric": "Not specified; the paper discusses information gain conceptually (references entropy/utility-based methods in related work) and suggests LGMs can estimate novelty or likelihood of hypotheses, but DATAVOYAGER does not implement a formal information-gain objective (no mutual information, expected improvement, or entropy-based score is defined).",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Qualitative: planner tends to prefer goal-oriented (exploitation) variables when provided with a user objective and may perform wider exploratory searches in autonomous mode. The system surface-level behaviour favors direct, extrinsic-goal variables; intrinsic-motivated exploration is discussed as an alternative but is not formalized in the baseline system.",
            "diversity_mechanism": "No explicit diversity-promotion mechanism is implemented in the baseline. The paper surveys intrinsic metrics (diversity, curiosity, information gain) as desirable drivers and argues for research to incorporate them, but DATAVOYAGER currently lacks a formal diversity-enforcing component.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Not concretely specified; the paper discusses budget-like constraints conceptually (computational cost, high-throughput settings, need to limit number of tests) but DATAVOYAGER does not implement a declared budget type (e.g., fixed experiment count or time limit).",
            "budget_constraint_handling": "Not implemented in the baseline; the paper recommends integrating cost-benefit analyses and predictive-hazard style decision rules for budget-aware allocation but provides no concrete mechanism inside DATAVOYAGER.",
            "breakthrough_discovery_metric": "Not defined quantitatively; the paper refers to novelty, 'interestingness', and connection to knowledge frontiers as qualitative markers of high-impact discoveries but no numeric novelty score or threshold is implemented.",
            "performance_metrics": null,
            "comparison_baseline": "DATAVOYAGER is positioned and compared conceptually in a survey against prior systems (CoScientist, DataLume, AutoML, MLAgentBench) but there are no empirical baseline comparison experiments reported.",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Conceptual discussion only: the paper highlights tradeoffs between exploration and exploitation, the risk of p-hacking from vast hypothesis spaces, and computational cost at scale. It argues these tradeoffs must be handled via principled allocation (cost-benefit analyses) and human moderation, but provides no quantitative tradeoff analysis for DATAVOYAGER.",
            "optimal_allocation_findings": "No formal optimal allocation is derived; recommended principles include (1) integrate fail-proof tools to ensure correct execution, (2) incorporate cost-benefit/predictive-hazard decision rules to manage computational costs, (3) enable human-in-the-loop moderation and continual learning, and (4) research intrinsic-motivated exploration and diversity promotion to avoid capture by local optima.",
            "uuid": "e2484.0",
            "source_info": {
                "paper_title": "Data-driven Discovery with Large Generative Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Prioritized Hypothesis Search",
            "name_full": "Prioritized Hypothesis Search (Weitzman / Agrawal model)",
            "brief_description": "A conceptual/resource-allocation model for selecting experiments or hypotheses to evaluate by prioritizing candidates according to expected marginal value or objective gradients; applied to guide search toward high-value discoveries under limited resources.",
            "citation_title": "Artificial intelligence and scientific discovery: A model of prioritized search.",
            "mention_or_use": "mention",
            "system_name": "Prioritized Hypothesis Search (Weitzman/Agrawal)",
            "system_description": "A theoretical approach that frames discovery as a search/selection problem where the decision-maker orders candidate hypotheses or experiments by expected marginal payoff (e.g., expected improvement toward an extrinsic goal). The paper references Weitzman's 'optimal search for the best alternative' and Agrawal et al.'s model of prioritized search as frameworks for goal-driven allocation, where greedy or gradient-driven selectors pick variables/models that most directly advance the extrinsic objective.",
            "application_domain": "General scientific discovery and experiment prioritization; discussed in context of data-driven hypothesis search over large observational datasets.",
            "resource_allocation_strategy": "Greedy/priority ordering based on expected contribution to a user-defined extrinsic objective: choose hypotheses or variable-model pairs that maximize immediate expected objective gain per cost. The paper notes such approaches risk local optima and capture when questions are open-ended.",
            "computational_cost_metric": "Not specified in the cited conceptual models as discussed in this paper; typically the models consider 'cost' abstractly (effort or resource consumption) but DATAVOYAGER's discussion does not instantiate a numeric cost metric.",
            "information_gain_metric": "Not explicitly defined in the paper's discussion; the models referenced focus on marginal expected objective value rather than explicit information-theoretic measures, though ties to expected improvement or utility are conceptually implied.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Primarily exploitation/goal-driven: selection favors hypotheses that most directly optimize the extrinsic goal (exploitation), with the paper noting the danger of missing novel findings and advocating complementary intrinsic drivers to encourage exploration.",
            "diversity_mechanism": "Not inherent; these prioritized greedy strategies do not explicitly promote diversity unless augmented by additional objectives or constraints.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Implicit (limited resource for exploration) but not specified concretely in the paper's discussion of these models.",
            "budget_constraint_handling": "Handled conceptually by prioritizing highest marginal-value candidates under the given resource limit (i.e., choose top-k under budget), but no concrete algorithmic budget-handling is specified in DATAVOYAGER's use of the idea.",
            "breakthrough_discovery_metric": "Implicitly measured by contribution to extrinsic objective (e.g., increase in objective metric), not by an explicit novelty/breakthrough score in the referenced works as discussed here.",
            "performance_metrics": null,
            "comparison_baseline": "Conceptually compared against intrinsic/diversity-driven search approaches (which encourage exploration and serendipity).",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Discussed qualitatively: goal-driven prioritized search is efficient for achieving a specified objective but can miss serendipitous or interdisciplinary breakthroughs; the paper argues for integrating intrinsic metrics (diversity, curiosity, information gain) to mitigate this risk.",
            "optimal_allocation_findings": "Recommendation: use prioritized search when a clear extrinsic objective exists, but incorporate mechanisms to ensure coverage/diversity or occasional exploratory moves to avoid local optima; integrate cost/importance weighting (marginal cost vs scientific importance) in prioritization.",
            "uuid": "e2484.1",
            "source_info": {
                "paper_title": "Data-driven Discovery with Large Generative Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Intrinsic Exploration / Diversity Methods",
            "name_full": "Intrinsic-motivation, curiosity, and diversity-driven exploration methods (e.g., VIME, RND, Diversity is All You Need)",
            "brief_description": "A class of methods that guide search/exploration using intrinsic objectives (novelty, diversity, curiosity, information gain) instead of or alongside extrinsic goals, encouraging open-ended and serendipitous discovery.",
            "citation_title": "Diversity is all you need: Learning skills without a reward function.",
            "mention_or_use": "mention",
            "system_name": "Intrinsic Exploration / Diversity-Driven Search",
            "system_description": "These methods define intrinsic utility functions (novelty, prediction error, variational information gain) to score candidate actions/hypotheses; selection rules then allocate resources to items maximizing intrinsic utility, e.g., RND (Random Network Distillation) for novelty, VIME (variational information maximizing exploration) for information gain, Eysenbach et al.'s diversity objectives to learn diverse behaviors/solutions. The paper cites these as possible drivers of hypothesis search to encourage serendipity and diversity in data-driven discovery.",
            "application_domain": "General exploration tasks, reinforcement learning, and proposed here as applicable to hypothesis generation/search in scientific datasets (open-ended discovery settings).",
            "resource_allocation_strategy": "Allocate evaluation budget to hypotheses or experiments with high intrinsic utility (novelty score, prediction error, information gain estimates), thereby preferring items likely to yield new knowledge rather than immediate extrinsic objective improvement.",
            "computational_cost_metric": "Not specified in the paper's discussion; method-dependent (could be number of model evaluations, training iterations, or compute used to estimate intrinsic utility).",
            "information_gain_metric": "Yes for some methods (VIME explicitly maximizes information gain about agent/environment dynamics); others use proxy novelty metrics (prediction error, state-visitation counts). The paper references Hennig & Schuler's and Houthooft's works as examples of info-based objectives.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration-leaning: intrinsic score drives exploration; exploitation can be reintroduced by mixing intrinsic and extrinsic objectives or via annealing policies; the paper highlights intrinsic drivers as a counterpoint to greedy, goal-oriented search.",
            "diversity_mechanism": "Explicit: diversity-promoting objectives or novelty measures (e.g., training policies to maximize diversity in learned skills, maximizing disagreement/prediction variance) are central in these methods.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Not specified within DATAVOYAGER; in general methods may be applied under an experiment/evaluation budget (fixed number of trials) or compute budget but the paper does not provide specifics.",
            "budget_constraint_handling": "Not specified; typical strategies include prioritizing highest intrinsic-utility candidates until budget exhausted or hybrid rules that trade intrinsic score against cost, but DATAVOYAGER does not implement these quantitatively.",
            "breakthrough_discovery_metric": "Not standardized; breakthrough potential is proxied by novelty/diversity scores or large information gain; the paper suggests these intrinsic metrics could signal serendipitous or high-impact discoveries.",
            "performance_metrics": null,
            "comparison_baseline": "Contrasted with goal-driven/greedy prioritized search approaches; no empirical comparisons are presented in this paper for DATAVOYAGER.",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Qualitatively discussed: intrinsic methods increase exploration and serendipity but can raise false positives and computational cost; the paper calls for research into how contexts/domains/hypothesis spaces affect this tradeoff.",
            "optimal_allocation_findings": "Recommendation: combine extrinsic and intrinsic objectives to balance focused progress and exploration; incorporate mechanisms to control false discoveries (multiple-testing correction) when enabling large-scale exploratory search.",
            "uuid": "e2484.2",
            "source_info": {
                "paper_title": "Data-driven Discovery with Large Generative Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Entropy Search / Bayesian Optimization",
            "name_full": "Entropy Search for Information-efficient Global Optimization",
            "brief_description": "Entropy search is a Bayesian optimization framework that chooses evaluations to maximally reduce uncertainty about the global optimum, using information-theoretic acquisition functions (expected reduction in entropy over the argmax).",
            "citation_title": "Entropy search for information-efficient global optimization.",
            "mention_or_use": "mention",
            "system_name": "Entropy Search / Information-Theoretic BO",
            "system_description": "A Bayesian optimization approach where acquisition functions are derived from expected information gain (e.g., reduction in posterior entropy about the location of the optimum). The paper references entropy/ information-gain based approaches as formal ways to guide hypothesis/experiment allocation in data-driven discovery when an optimization objective exists.",
            "application_domain": "Global optimization and experiment design; proposed as applicable to selecting experiments/hypotheses in scientific discovery for maximum information per evaluation.",
            "resource_allocation_strategy": "Select next evaluations to maximize expected information gain about the global optimum (e.g., highest expected reduction in entropy), implicitly balancing the utility of an evaluation against its cost if cost-aware variants are used.",
            "computational_cost_metric": "Not specified in this paper's discussion of the method; typical BO work uses number of function evaluations or wall-clock time, but DATAVOYAGER does not integrate a concrete metric.",
            "information_gain_metric": "Explicit: expected reduction in posterior entropy over the location/value of the optimum; acquisition function is information-theoretic.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Inherently balances exploration and exploitation by selecting evaluations that reduce uncertainty about the optimum; tends to sample informative regions even if immediate objective improvement is modest.",
            "diversity_mechanism": "Implicit via information-seeking: by valuing uncertainty reduction, the method samples diverse regions informative about the optimum, though diversity is not enforced as an explicit coverage constraint.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Typically number of evaluations or compute budget in BO practice; the paper suggests such frameworks are relevant but does not show DATAVOYAGER integrating BO.",
            "budget_constraint_handling": "In BO, handled by stopping after fixed number of evaluations or cost-aware acquisition; DATAVOYAGER does not implement this.",
            "breakthrough_discovery_metric": "Reduction in entropy about the optimum or improvement in objective value; the paper references such metrics conceptually but no instantiation is provided for hypotheses discovery.",
            "performance_metrics": null,
            "comparison_baseline": "Typically compared to random search or expected improvement heuristics in the BO literature; DATAVOYAGER only cites entropy-search as a conceptual option.",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Paper mentions information-gain methods as candidate strategies to balance evaluation cost and discovery value, but provides no empirical tradeoff analysis within DATAVOYAGER.",
            "optimal_allocation_findings": "Suggested insight: information-theoretic acquisition rules (entropy search) are principled for allocating scarce evaluations to maximize knowledge about optimal hypotheses, but integration with domain tools, multiple-hypothesis testing correction, and cost-awareness is necessary for real data-driven discovery pipelines.",
            "uuid": "e2484.3",
            "source_info": {
                "paper_title": "Data-driven Discovery with Large Generative Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Cost-Benefit / Predictive Hazard Allocation",
            "name_full": "Cost-Benefit Resource Allocation with Predictive Hazard Functions",
            "brief_description": "A proposed allocation strategy that explicitly integrates computational/experimental costs with predicted payoff (hazard/predictive models) to choose which hypotheses or experiments to run under budget constraints.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Cost-Benefit / Predictive Hazard Allocation",
            "system_description": "Described in the paper as a recommended research direction: incorporate cost-benefit analyses (e.g., predictive hazard functions) into discovery systems to manage the computational costs of high-throughput hypothesis evaluation. The idea is to estimate a predictive hazard or success probability conditioned on available evidence and allocate resources to hypotheses with favorable payoff-to-cost ratios.",
            "application_domain": "High-throughput scientific discovery domains (computational biology, large-scale social-data analysis) where many hypotheses may be tested and costs per evaluation vary.",
            "resource_allocation_strategy": "Rank hypotheses by predicted expected utility per unit cost (i.e., predictive hazard or probability of success × impact divided by computational/experimental cost) and allocate budget to top-ranked items until budget exhausted or marginal utility falls below threshold.",
            "computational_cost_metric": "Not specified concretely; paper suggests computational cost is a key constraint and implies metrics like compute time or number of hypothesis evaluations but does not define one.",
            "information_gain_metric": "Not specified; approach conceptually balances predicted payoff (could include information gain) against cost, but no explicit information-theoretic metric is defined in the paper.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Cost-aware exploitation-biased with potential exploration through explicit utility definitions that reward novelty or high-uncertainty items; the paper recommends this hybrid approach but offers no algorithmic detail for DATAVOYAGER.",
            "diversity_mechanism": "Not defined; diversity could be included as part of the expected utility (valuing novelty/breadth), but the paper only suggests the idea, not an implementation.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Computational/experimental resource budget in high-throughput settings (implied), e.g., limit on number of hypotheses evaluated or compute/time allowed.",
            "budget_constraint_handling": "Proposed handling: integrate predictive hazard / expected payoff per cost models to prioritize evaluations under budget; no concrete implementation in DATAVOYAGER.",
            "breakthrough_discovery_metric": "Implied to be expected payoff or 'probability of success × impact' (qualitative), but not formalized in the paper.",
            "performance_metrics": null,
            "comparison_baseline": "Not evaluated empirically; proposed as superior to naive exhaustive or purely greedy approaches in high-throughput scenarios.",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Paper argues conceptually that explicit cost-benefit allocation is necessary to manage the high computational costs of large-scale hypothesis testing and to reduce accidental p-hacking by limiting frivolous tests, but provides no empirical quantification.",
            "optimal_allocation_findings": "Recommendation: develop and integrate predictive-hazard or expected-utility based allocation rules into discovery systems to trade off computational cost, information gain, and breakthrough potential; combine with multiple-testing controls and human oversight.",
            "uuid": "e2484.4",
            "source_info": {
                "paper_title": "Data-driven Discovery with Large Generative Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "CoScientist",
            "name_full": "CoScientist",
            "brief_description": "A system that uses large generative models to automate parts of scientific workflows (notably in laboratory/chemical research), but still requires substantial human intervention and wet-lab experiments for verification.",
            "citation_title": "Autonomous chemical research with large language models.",
            "mention_or_use": "mention",
            "system_name": "CoScientist",
            "system_description": "CoScientist is cited as a recent system that automates some parts of the scientific workflow using LGMs, for example proposing experiments or protocols in chemistry, but it is not fully autonomous—wet-lab verification and human oversight remain necessary. The paper uses CoScientist as a comparator to emphasize DATAVOYAGER's focus on dataset-based verification (no wet lab) and the need for fail-proof tool integration and human moderation.",
            "application_domain": "Chemistry / autonomous wet-lab research (as demonstrated in literature cited); contrasted here with data-driven (no new data collection) discovery.",
            "resource_allocation_strategy": "Not detailed in this paper's discussion; CoScientist as cited conducts experiment proposal and execution in lab settings with human oversight—resource allocation specifics are not described here.",
            "computational_cost_metric": null,
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": null,
            "diversity_mechanism": null,
            "uses_diversity_promotion": null,
            "budget_constraint_type": null,
            "budget_constraint_handling": null,
            "breakthrough_discovery_metric": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Mentioned as requiring substantial human intervention for verification, underscoring the paper's point that full autonomy remains difficult and that data-driven verification (as in DATAVOYAGER) is more tractable for automated end-to-end pipelines.",
            "optimal_allocation_findings": null,
            "uuid": "e2484.5",
            "source_info": {
                "paper_title": "Data-driven Discovery with Large Generative Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "DataLume",
            "name_full": "DataLume",
            "brief_description": "A system that automates code generation for data transformation and hypothesis verification at scale, but lacks modules for hypothesis search and orchestrating complex scientific workflows.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "DataLume",
            "system_description": "DataLume is referenced as a system that fully automates code generation for data transformation and hypothesis verification; however, the paper notes it does not include capabilities for hypothesis search or multi-step research planning/orchestration needed for end-to-end discovery.",
            "application_domain": "Automated data transformation and analysis for tabular datasets; relevant to data analysis pipelines but not full discovery.",
            "resource_allocation_strategy": "Not described in this paper; DataLume focuses on automating execution rather than on allocating experiments or balancing cost versus information gain.",
            "computational_cost_metric": null,
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": null,
            "diversity_mechanism": null,
            "uses_diversity_promotion": null,
            "budget_constraint_type": null,
            "budget_constraint_handling": null,
            "breakthrough_discovery_metric": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": null,
            "optimal_allocation_findings": null,
            "uuid": "e2484.6",
            "source_info": {
                "paper_title": "Data-driven Discovery with Large Generative Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MLAgentBench / AutoML",
            "name_full": "MLAgentBench and AutoML systems",
            "brief_description": "AutoML frameworks and MLAgentBench automate model-building and benchmarking (hyperparameter/model search), but they do not address semantic hypothesis generation or principled allocation of scientific experiments balancing cost against information gain in discovery contexts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "MLAgentBench / AutoML frameworks",
            "system_description": "AutoML systems (Scikit-AutoML, cloud AutoML platforms) automate model selection and hyperparameter search, optimizing predictive performance under compute/service constraints; MLAgentBench extends benchmarking to end-to-end AI agents. The paper contrasts these with data-driven discovery systems by noting AutoML lacks semantic understanding needed for hypothesis generation and orchestration, and MLAgentBench focuses on model-centric objectives rather than scientific-discovery resource allocation.",
            "application_domain": "Automated machine-learning model construction and agent benchmarking; referenced as related but not sufficient for scientific discovery.",
            "resource_allocation_strategy": "AutoML uses search over hyperparameters and model architectures often within specified compute/time budgets (usually via black-box optimization or bandit/early-stopping rules); MLAgentBench evaluates agents but does not allocate scientific experiments. The paper notes these systems optimize prediction performance rather than scientific-information utility.",
            "computational_cost_metric": "In AutoML practice, typically number of trials, wall-clock time, or monetary cloud cost; this paper does not detail their exact metrics within the discovery context.",
            "information_gain_metric": "Not applicable in their standard form; AutoML measures predictive performance (validation error) rather than information gain about scientific hypotheses.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "AutoML uses exploration/exploitation heuristics (e.g., bandits, Bayesian optimization) for hyperparameter search; these mechanisms are for model optimization, not hypothesis diversity or discovery.",
            "diversity_mechanism": "Not oriented to hypothesis diversity; diversity in AutoML is typically diversity of model architectures or hyperparameter settings.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Time/compute or trial budget in AutoML; the paper notes these constraints but emphasizes different objective alignment for discovery.",
            "budget_constraint_handling": "Handled by scheduling, early stopping, or specified trial limits in AutoML systems; the paper argues such budget handling does not equate to prioritizing scientific hypotheses by information gain or breakthrough potential.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Paper uses AutoML and MLAgentBench as contrasts: they solve some optimization/allocation problems but do not address semantic hypothesis search nor the combination of cost, information gain, and diversity needed for data-driven scientific discovery.",
            "optimal_allocation_findings": null,
            "uuid": "e2484.7",
            "source_info": {
                "paper_title": "Data-driven Discovery with Large Generative Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Artificial intelligence and scientific discovery: A model of prioritized search.",
            "rating": 2,
            "sanitized_title": "artificial_intelligence_and_scientific_discovery_a_model_of_prioritized_search"
        },
        {
            "paper_title": "Optimal search for the best alternative",
            "rating": 2,
            "sanitized_title": "optimal_search_for_the_best_alternative"
        },
        {
            "paper_title": "Entropy search for information-efficient global optimization.",
            "rating": 2,
            "sanitized_title": "entropy_search_for_informationefficient_global_optimization"
        },
        {
            "paper_title": "Vime: Variational information maximizing exploration.",
            "rating": 2,
            "sanitized_title": "vime_variational_information_maximizing_exploration"
        },
        {
            "paper_title": "Diversity is all you need: Learning skills without a reward function.",
            "rating": 2,
            "sanitized_title": "diversity_is_all_you_need_learning_skills_without_a_reward_function"
        },
        {
            "paper_title": "Autonomous chemical research with large language models.",
            "rating": 2,
            "sanitized_title": "autonomous_chemical_research_with_large_language_models"
        },
        {
            "paper_title": "Efficient and robust automated machine learning.",
            "rating": 1,
            "sanitized_title": "efficient_and_robust_automated_machine_learning"
        },
        {
            "paper_title": "Benchmarking large language models as ai research agents.",
            "rating": 1,
            "sanitized_title": "benchmarking_large_language_models_as_ai_research_agents"
        },
        {
            "paper_title": "A practical guide to methods controlling false discoveries in computational biology.",
            "rating": 1,
            "sanitized_title": "a_practical_guide_to_methods_controlling_false_discoveries_in_computational_biology"
        }
    ],
    "cost": 0.020500749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Data-driven Discovery with Large Generative Models
21 Feb 2024</p>
<p>Bodhisattwa Prasad Majumder 
Equal contribution</p>
<p>Allen Institute for AI</p>
<p>Harshit Surana <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#104;&#97;&#114;&#115;&#104;&#105;&#116;&#64;&#111;&#112;&#101;&#110;&#108;&#111;&#99;&#117;&#115;&#46;&#97;&#105;">&#104;&#97;&#114;&#115;&#104;&#105;&#116;&#64;&#111;&#112;&#101;&#110;&#108;&#111;&#99;&#117;&#115;&#46;&#97;&#105;</a> 
Equal contribution</p>
<p>OpenLocus</p>
<p>Dhruv Agarwal 
University of Massachusetts Amherst</p>
<p>Sanchaita Hazra 
University of Utah</p>
<p>Ashish Sabharwal 
Allen Institute for AI</p>
<p>Peter Clark 
Allen Institute for AI</p>
<p>Data-driven Discovery with Large Generative Models
21 Feb 202415D87CE7EB5188BE4801F0977BDA9F55arXiv:2402.13610v1[cs.CL]National Longitudinal Surveys Query: Study the relation between BMI and Time Preference
With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially.This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-toend data-driven discovery-a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments.We first outline several desiderata for an ideal data-driven discovery system.Then, through DATAVOYAGER, a proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of these desiderata-a feat previously unattainable-while also highlighting important limitations in the current system that open up opportunities for novel ML research.We contend that achieving accurate, reliable, and robust endto-end discovery systems solely through the current capabilities of LGMs is challenging.We instead advocate for fail-proof tool integration, along with active user moderation through feedback mechanisms, to foster data-driven scientific discoveries with efficiency and reproducibility.</p>
<p>Introduction</p>
<p>The deluge of data collected in the digital age by advanced scientific instruments, sensors, and computational techniques has marked a transformative change in the process and pace of scientific discovery (Anderson, 2008;Ramakrishnan &amp; Grama, 1999;Jumper et al., 2021).This acceleration, however, paints a paradoxical scenario-while rapid development indicates the advancement of knowledge, it simultaneously poses significant challenges for scientists to absorb new findings, navigate interconnections, formulate novel hypotheses, and arrive at meaningful conclu-Developing an end-to-end discovery system is challenging.Previous works have either severely lacked the requisite computational power (Langley, 1981;Langley et al., 1984;1983), developed domain-specific bespoke methodologies (e.g., AlphaFold; Jumper et al. (2021)), or involved substantial human intervention (e.g., wet lab experiments) thus not qualifying as autonomous end-to-end (CoScientist; Boiko et al. (2023)).In this position paper, we argue that a focus on data-driven discovery using large generative models (LGMs) addresses each of these prior shortcomings and presents a practical first step towards the goal of an end-toend system for automating the scientific process.Following Newell &amp; Simon (1976), we define this paradigm as a heuristic search framework that aims to describe a given set of observations by uncovering the laws that govern its data-generating process.</p>
<p>For example, consider the flow described in Figure 1.Given a dataset of socio-economic variables collected from a set of respondents, a user might formulate a hypothesis about the relationship between the BMI of a subset of the respondents and their financial behavior (variables present in the dataset; top-left).A data-driven discovery system should be able to automatically generate a verification plan and execute multiple steps of statistical tests (e.g., OLS, GLM) over the provided data to confirm or reject the hypothesis (bottomleft).Alternatively, a user might only provide a high-level research question, such as specifying the domains of interest (i.e., finance and health; top-middle).In this scenario, a discovery system must first identify the relevant variables and then search the space of plausible hypotheses to generate and verify interesting questions conditioned on the provided data and existing world knowledge (bottom-middle).Finally, users may have diverse information-seeking needs necessitating the ability to provide feedback to the system, such as in using a particular statistical methodology for certain types of data during automatic verification (top-right).An automated discovery system must accommodate and persist such feedback in order to recover from mistakes and accurately handle future queries (bottom-right).</p>
<p>Wealt h disparit ies among racial minorities (0.91) are more prominent compared to others (0.67).This is excarbated among minority men who have been incarcerated earlier.</p>
<p>I learnt to always use Gini analysis for wealth disparties; I am checking t he code int o our t oolbench.</p>
<p>Feedback</p>
<p>Figure 1.A blueprint flow demonstrating ideal workflows for data-driven discovery.Left: User asks an explicit question around a particular line of inquiry or hypothesis.Middle: The user can also ask a broad and partially-defined high-level question, where the system must figure out the appropriate datasets, data transformations, variables, a list of possible hypotheses, and their verification.In this example, the system maps time preference and health outcomes to exact variables, runs the analysis across appropriate demographic cuts, and then shares the significant findings for further exploration and verification.Right: The user can provide follow-up feedback at any time and the continual learner will learn from it while providing updated experiments and results.</p>
<p>While our ultimate goal encompasses the full spectrum of scientific inquiry, we focus first on end-to-end discovery from observational or experimental data for two reasons:</p>
<p>(1) an abundance of large-scale datasets that would benefit highly from automated discovery; and (2) the practicality of automated verification enabled by data without the need for additional data collection 1 .</p>
<p>We identify two main challenges to automating data-driven discovery-(1) hypothesis search: the effective consumption of provided data and existing knowledge to devise novel hypotheses, and (2) hypothesis verification: the evaluation of the generated hypotheses for rapid iteration and continual discovery.A successful solution must further be able to generate and follow complex plans, execute diverse analytical tests, and parse through the abundant heterogeneity in real-world data.With the unprecedented success of LGMs operating on multiple modalities such as language (Achiam et al., 2023;Touvron et al., 2023), code (Liu et al., 2023b;Li et al., 2022), and images (Achiam et al., 2023;Liu et al., 2023a), we argue that it is now practical to build such a solution that can effectively tackle both challenges.</p>
<p>Hypothesis Search.The scientific process typically begins with the construction of a proposed hypothesis based on prior knowledge and exploratory observations regarding some phenomenon of interest.For example, discovering new insights from publicly available National Longitudinal 1 In contrast to hypothesis verification in the physical sciences, which often require wet lab experiments and where erroneous automation may lead to false discoveries (Leeman et al., 2024).Surveys2 will require prioritizing unexplored hypotheses over already verified results.However, it is non-trivial what an optimal method for such a prioritized search might be.</p>
<p>Foremost, we may ask whether the search should be driven by an extrinsic goal-a user-defined objective, a high-level research question, or a set of variables of interest.This setting might involve using algorithms that guide the search process using objective-gradients (Weitzman, 1978) that identify variables and models that directly, or greedily, optimize the extrinsic goal.We argue that LGMs, with their massive, web-scale pre-training, possess both the necessary priors and the ability to handle heterogeneity, to help guide such a goal-driven search for relevant hypotheses.</p>
<p>It can also be argued that goal-driven approaches may not yield desired outcomes, particularly when dealing with openended questions, where the search is often susceptible to capture by local optima (Whitley, 1991;Bengio et al., 2009).Drivers for search might then be intrinsic metrics (Oudeyer et al., 2007), such as diversity (Eysenbach et al., 2019;Agarwal et al., 2023;Trinh et al., 2024), interestingness (or curiosity) (Pathak et al., 2017;Zhang et al., 2023), or information gain (Hennig &amp; Schuler, 2012;Houthooft et al., 2016), that do not optimize for a user-defined extrinsic goal but instead encourage open-ended creativity and, eventually, serendipitous discovery (Foster &amp; Ford, 2003;Taleb, 2007;Stanley et al., 2017).Here, too, LGMs present a solution, for instance, in estimating the novelty or likelihood of hypotheses in the search space.</p>
<p>More int erdiscplinary insights based on the results</p>
<p>The correlat ion coefficient: -0.031, very weak negative linear relationship between dissaving and BMI.</p>
<p>The int eract ion term coefficient: 0.5259 st at ist ically significant (p &lt; 0.0000) ... "GENDER_MALE" has a significant positive association with BMI, indicating that males have a higher BMI t han females.</p>
<p>Economics and</p>
<p>The GLM confirms t he findings from t he OLS model regarding the interactions between time preference and demographic factors.</p>
<p>How to mitigate the effect of testing multiple hypotheses?</p>
<p>... Hypothesis Verification.</p>
<p>With a set of plausible hypotheses identified, it is next required to subject each claim through detailed inspection, often via a series of empirical evaluations and statistical tests, to determine veracity, which is highly tractable and could be fail-proof in data-driven discovery.This might involve selecting which analyses or statistical tests to run, transforming raw data into a format admissible for each test, handling missing or erroneous data, generating code to execute the tests, and finally analyzing the test results.Given the surge of recent advancements in language modeling capabilities, including instructionfollowing (Wei et al., 2022), tool use (Schick et al., 2023), program synthesis (Wang et al., 2023a;Agarwal et al., 2023), planning (Majumder et al., 2023), and orchestration (Hou et al., 2023), we argue that LGM agents present a promising solution for automating hypothesis evaluation.</p>
<p>The availability of these capabilities, however, must not be seen as a panacea.(1) LGMs often hallucinate, leading to incorrect insights that may not be grounded in the data.</p>
<p>(2) LGMs have limited or no "System-2" reasoning (Kahneman, 2011;LeCun, 2022;Kambhampati et al., 2024), thus necessitating additional scaffolding in order to utilize them for long-horizon tasks.(3) LGMs demonstrate subpar performance in the long tail, thus making their successful application in interfacing with external and domain-specific tools a major challenge to overcome.(4) Finally, LGMs are notoriously challenging to align and steer based on hu-man feedback (Wolf et al., 2023), a crucial component for reliable and useful scientific discovery.</p>
<p>We envision a blueprint of a data-driven discovery system in Figure 1 that allows researchers to ingest datasets, search and verify hypotheses using fail-proof tools, and consult literature to surface novel insights.Our survey in Figure 3 indicates the lack of systems capable of automated and robust data-driven discovery, with existing systems partially covering desired functionalities.To tackle this, we argue:</p>
<ol>
<li>Automated data-driven discovery warrants research attention owing to the abundance of (public or private) data and its tractable challenges (hypothesis search and verification) as opposed to discoveries requiring laborious data collection or physical experiments.</li>
</ol>
<p>2.</p>
<p>LGMs present an incredible potential to realize several properties of an ideal data-driven discovery system, such as knowledge-driven hypothesis search or tool usage to verify hypotheses-creating new avenues for ongoing efforts in the ML community on code generation, planning, and program synthesis.</p>
<p>3.</p>
<p>LGMs are not all we need.Interfacing with fail-proof tools and inference-time functions, catering to domains and long tail with user moderation, is required to have an accurate, reliable, and robust data-driven discovery system capable of advancing scientific progress with speed and reproducibility.</p>
<p>DATAVOYAGER: A Proof of Concept</p>
<p>As a proof of concept, we borrow a well-studied role-based multi-agent architecture (Liu et al., 2023c;Zhou et al., 2023) powered by GPT-4 (Achiam et al., 2023), a state-of-the-art language model, to build DATAVOYAGER-a system that can semantically understand a dataset, programmatically explore verifiable hypotheses using the available data, run basic statistical tests (e.g., correlation and regression analyses) by invoking pre-defined functions or generating code snippets, and finally analyze the output with detailed analyses.DATAVOYAGERis meant to represent a baseline system that utilizes existing functionalities of GPT-4, such as function calling, code generation, and language generation.</p>
<p>We envision any data-driven discovery system to be capable of operating in either of the following two settings.(1) Fullyautonomous: using only the dataset and its metadata as the input.In this case, the system should consider the full hypothesis space for search and verification.</p>
<p>(2) User-guided: combining the dataset with a (natural language) query stating a high-level objective to narrow down the hypothesis search space, akin to goal-directed agents (Majumder et al., 2023).DATAVOYAGER can operate in both settings.</p>
<p>The core components of our system consist of specialized agents that are designed to manage different aspects of the data-driven discovery process as well as structured functions or programs that help analyze the data in specific ways via function calling.We employ the AutoGen framework3 that allows agents to communicate in arbitrary order dependent on the context.Following is a brief description of all agents used in DATAVOYAGER (more in Figure 4):</p>
<p>• Planner: Interprets the user query and generates a comprehensive, structured plan to achieve it or, in the autonomous setting, generates an additional dataset exploration plan.The plan is then decomposed into executable sub-tasks and delegated to the relevant agents.• Programmer: Performs data transformations, filtering, and specialized coding for domain-specific analyses according to the generated plan.It can also call structured, pre-defined functions with relevant arguments to make execution fail-proof.4 • Data Expert: Interprets the results generated by the programmer, extracting insights, connecting interdisciplinary knowledge, and formulating conclusions.• Critic: Evaluates the analyses and provides constructive feedback on analytical methods and execution.• User Proxy: Facilitates on-demand human feedback.</p>
<p>A user can steer the discovery process towards an objective, rectify errors, and prevent off-course explorations.</p>
<p>Towards Data-driven Discovery Systems</p>
<p>In this section, we first outline a set of desired functionalities for a data-driven discovery system.Using these functionalities, and armed with our baseline system DATAVOY-AGER along with evidence from the literature, we demonstrate extensive support towards our positions 2 and 3. Functionalities such as data understanding, hypothesis generation, multi-step planning, and interdisciplinary knowledge integration provide evidence that a system (DATAVOYAGER) powered by a state-of-the-art LGM shows promise for ideal data-driven discovery, an ability not previously achievable before the wide adoption of LGMs.On the other hand, functionalities such as data transformation, scale, hypothesis verification, accommodating human feedback, and p-hacking proof confirm that LGMs alone are insufficient.Integrating robust tools to execute at scale and user-centric interventions is crucial for a tractable data-driven discovery system.</p>
<p>Comprehensive Data Understanding</p>
<p>Data Understanding.Understanding data forms the core of data-driven discovery and involves processing variables semantically as well as programmatically (Ristoski &amp; Paulheim, 2016).The former involves understanding how the data was collected or acquired, grounded in the domain knowledge, as well as the semantic relationship between the variables present in the data.The latter involves understanding the datatype of each variable and the values they can take.Progress in database query generation (Sun et al., 2023), commonsense reasoning on symbolic spaces (Qiu et al., 2023), and unsupervised KGQA (Agarwal et al., 2023) indicate reason for optimism for the use of LGMs for data understanding.</p>
<p>For example, Smith et al. (2005) explored the link between time preference and BMI from the National Longitudinal Surveys using several variables indicating the saving behavior of the respondents.To replicate this from scratch, a discovery system must understand the difference between the variable measuring if respondents withdrew more money from savings than they put in (DISSAVED) and the variable indicating if they have no savings or unchanged savings from the previous year (SAMESAVE)5 .Here, DATAVOY-AGER's LGM-based planner correctly identifies variables related to time preference:</p>
<p>To examine the effects of time preference on individuals, we need to understand the variables in the dataset that relate to time preference.In the provided dataset, the variables DISSAVED and SAMESAVE seem to be related to time preference (...) Full example: Figure 6 While it works for this example, a comprehensive data understanding is still challenging due to the complexity of various datasets with numerous types and complex metadata.We, therefore, ask: can a system achieve a comprehensive understanding of domains and variations in diverse datasets in a domain-agnostic manner as compared to domain-specific systems, such as CoScientist (Boiko et al., 2023)?</p>
<p>Data Transformation.Different datasets have unique characteristics, requiring custom transformations and filtering operations (Kang et al., 2017).Moreover, even within the same dataset, different hypotheses may demand different transformations for accurate verification and testing.Without this capability, the potential to conduct a wide range of statistical tests for hypothesis verification would be compromised (Bailis et al., 2017).A simple example of data transformation would be the ability to convert a categorical variable into a one-hot encoding.Further, the following is an example showing DATAVOYAGER's LGM-based programmer performing data transformation in order to derive interaction terms between variables:</p>
<p>Let's start by adding interaction terms to examine the potential link between time preference and BMI across different demographic groups (. . . ) Full example: Figure 7 The challenge lies in accommodating the abundant diversity of hypotheses and datasets, each requiring highly customized transformations (Bowers &amp; Ludäscher, 2004).The ability of LGMs to generate code for such domain-specific data (Sharma et al., 2023) hints towards a generalized solution; however, the difficulty in debugging generated code (Vaithilingam et al., 2022) demands a call to action for building better code generation models.</p>
<p>Scale.Modern scientific exploration often involves large amounts of data, a complex analytics workflow, and a large hypothesis space (Elliott et al., 2016).It is important, thus, for a useful autonomous discovery system to be able to sift through such large datasets efficiently while maintaining the state of its several processes and tracking previously conducted analyses.Without this ability to scale and handle complex workflows, several hypotheses would remain unexplored, and valuable insights left undiscovered.</p>
<p>For longitudinal studies, where it is important to understand how variables evolve over time (Weiss &amp; Ware, 1996), scalability is particularly crucial in order to handle data over extended time periods.Furthermore, in very large-scale data scenarios, such as the Cancer Moonshot project6 and the Cancer Genomics Cloud (Lau et al., 2017), the discovery system must be able to analyze petabytes of data in complex workflows, all while maintaining a state of the possible hypotheses and variable combinations as well as the explorations conducted thus far.In such scenarios, LGMs must be able to support long-horizon planning and longcontext attention.However,</p>
<p>LGMs are yet to show significant progress on both counts (Valmeekam et al., 2022), a limitation of DATAVOYAGER as well, thus highlighting a need for focused research towards these goals.</p>
<p>Hypothesis Generation</p>
<p>Connecting Data and Scientific Literature.The ability to bridge the provided data and existing scientific literature is important in providing an understanding of the hypothesis space grounded by contextual domain knowledge.This ability to learn from known knowledge may further result in various inter-disciplinary perspectives and insights-a phenomenon often called Swanson Linking (Bekhuis, 2006).</p>
<p>For example, to derive novel insights between social background and college graduation (Alexander et al., 1982) from the National Longitudinal Surveys, it is imperative to understand previous research on National Longitudinal Surveys to avoid duplication and incorporate verified knowledge from the literature to improve initial hypotheses.</p>
<p>Linking generated hypotheses to existing knowledge requires accurate retrieval, information extraction, and multistep reasoning (Wang et al., 2023b).Further, combining multiple research articles connects back to the original Swanson Linking problem (Swanson, 1986).While LGMs have recently been shown to perform well in augmenting citations with relevant context based on a user's history (Chang et al., 2023), connecting datasets to scientific literature is an open research problem.By utilizing annotated papers for datasets (Palani et al., 2023), we ask: can a system learn to combine insights from existing literature and a provided dataset in order to discover novel research gaps?</p>
<p>Formulating initial hypotheses.Scientists prioritize experiments based on academic intuition, empirical evidence, and existing theories.In data-driven discovery, this approach is akin to selecting hypotheses from a vast combinatorial space of variable interactions, often extensive for exhaustive exploration (Agrawal et al., 2023), to identify dependent and independent variables.</p>
<p>For example, to understand the relationship between education outcome and socioeconomic status, the system should prioritize investigating how the "rate of completion of BA degree" is influenced by socioeconomic indicators, such as accumulated wealth and parents' education, as a plausible hypothesis (Alexander et al., 1982).This is non-trivial because it not only requires the system to have a semantic understanding of the variable space but also the ability to prioritize hypotheses based on marginal costs and their scientific importance (Agrawal et al., 2023).Here, DATAVOY-AGER performs reasonably well on hypothesis generation:</p>
<p>H1: Females are more likely to complete a BA degree compared to males.H2: Family size has an impact (...).H3: Higher ability scores on the ASVAB test are positively correlated (...) Full example: Figure 18 Hypothesis generation can be seen as inductive reasoning (Qiu et al., 2023) using known evidence by connecting them using entailment-like relations (Dalvi et al., 2021).While</p>
<p>LGMs show good performance on reasoning benchmarks (Hendrycks et al., 2020), data heterogeneity (e.g., variable names, statistical interactions) and semantics make the reasoning problem harder for LGMs (Lu et al., 2023)-thus, we call for research attention.</p>
<p>Planning and Orchestrating Research Pathways</p>
<p>Multi-step planning.Data-driven discovery with complex problems and datasets requires a structured approach of breaking down a high-level objective into manageable subtasks, enabling the systematic exploration of the data and hypothesis landscape.This can be considered equivalent to planning (LeCun, 2022).Prioritized hypothesis search with planning involves states -the intermediate correlations found from data (sub-hypotheses), and operators -the statistical tools and literature to combine verified states (here, sub-hypotheses).Multi-step, iterative planning, thus, comprehensively facilitates the search for scientific discoveries.</p>
<p>Research planning involves incorporating known or novel research pathways, such as the order of analyses or the methods used, and they vary depending on the research goal of the exploration.It can be challenging to choose between a standardized or pre-defined flow as compared to a dynamic plan depending on the realized intermediate states of the planning.Though LGMs as planners are often faulty (Valmeekam et al., 2022), planning within the data hypothesis space presents a fertile ground to systematically benchmark LGMs and improve their abilities.</p>
<p>For example, analyzing the relationship between college education and socio-economic status from National Longitudinal Surveys (Alexander et al., 1982), the system generates the following plan: While the ability to decompose abstract plans into executable sub-plans is heavily explored in coding and symbolic reasoning (Khot et al., 2022), DATAVOYAGER presents a strong base case to improve the efficacy of planning by incorporating dynamic strategies that account for search uncertainties.</p>
<p>Exploration vs. exploitation.The debate concerning whether exploration should be goal-oriented or randomized is crucial in making novel discoveries (Agarwal et al., 2023).This applies directly to data-driven discovery, where variable selection by the planner directly impacts what subset of the hypothesis space is considered for search.Thus, this exploration-exploitation trade-off is a key factor in shaping the makeup of the final outcome (Foster &amp; Ford, 2003).</p>
<p>LGM-based planners, including DATAVOYAGER, prefer direct, goal-oriented variables, e.g., preferring parents' wealth towards success in college education, while de-prioritizing more implicit variables related to urban planning (e.g., location of schools).However, while exploration with intrinsic motivators could lead to novel outcomes, it can also sometimes result in false positives (Oudeyer &amp; Kaplan, 2008).How contexts, domains, and the hypothesis space influence the tradeoff between exploration and exploitation remains an open question, which, we argue, is worth considerable research focus (Majumder et al., 2022;Burda et al., 2018).</p>
<p>Hypothesis Evaluation</p>
<p>Hypothesis Verification.The practical possibility of programmatically verifying a set of hypotheses is a unique feature in data-driven discovery.This encompasses both the proper execution of code as well as the capacity to utilize the appropriate statistical methods and techniques aligned with the high-level research objective (Cai et al., 2023).</p>
<p>The verification of hypotheses can involve (1) the use of tools and (2) code generation.Tools represent a pre-defined set of structured functions, which may be invoked via function-calling by LGMs along with relevant arguments (Pelrine et al., 2023).Code generation, on the other hand, is often unconstrained and can optionally be combined with external tests (Schäfer et al., 2023) and methods such as self-refine (Madaan et al., 2023) in order to minimize hallucination and execution failure.</p>
<p>For example, to verify the hypotheses proposed by the planner, we show DATAVOYAGER's use of independent t-tests to uncover the impact of wealth distributions in two groups on their incarceration probability (Zaw et al., 2016).The results of the independent t-tests for the wealth variables across the two groups (those with and without a criminal record) for the years 1985, 1990, and 1996: (...) T-statistic: 9.7794 (...) Full Example: Figure 17</p>
<p>An ideal system must conduct statistical tests (e.g., correlation, regression, multivariate analyses, t-tests or ANOVA for hypothesis testing, etc.), consume execution results, perform analysis to either conclude or re-plan (Prasad et al., 2023) and support usage of domain-specific evaluation toolkits, such as clinical trials (Rotolo et al., 2018) and climate change (Hoffmann et al., 2021).</p>
<p>The complexity of this task arises from the need to support a plethora of analysis tools (see Figure 5) on diverse datasets through unconstrained code generation.Robust verification, further, must be able to analyze execution output and recover from failed initial generation (Ellis et al., 2020).Verification of program output can be enhanced plots, sub-codes, and numerical analyses, yet despite success in math reasoning (Cobbe et al., 2021), LGMs lack multi-modal symbolic understanding (Lu et al., 2023), calling to action the need for improved data experts in systems like DATAVOYAGER.</p>
<p>Continual Learning.Data-driven discovery is an evolving process.With each stage, from hypothesis generation to evaluation, the system collects new insights and successful (or failed) research flows.The system, thus, requires an adaptive learning approach to integrate and understand the changing context and update its understanding of the dataset (Majumder et al., 2023;Shinn et al., 2023) over time.</p>
<p>For example, execution errors while running generated code or failed research pathways provide opportunities for selfrefinement and possibly integrating learning into the next instances for more fail-proof planning and execution.Continual learning for data-driven discovery opens up research questions regarding the process of online learning (Majumder et al., 2023;Wang et al., 2023a) involving LGMs and avenues to collect supervision signals for continual finetuning (Lin et al., 2022).We argue that how LGMs adapt to novel tools and code at inference time is still an open question and remains critical to data-driven discovery.Despite high degree of natural language fluency, LGMbased systems are often not very proactive.It is desirable for these systems to possess a mixed-initiative ability, thus, optimizing the frequency of asking for human feedback and input (Majumder et al., 2021).Exploring user involvement in the decision-making process raise two questions: (1) Can we achieve an ideal outcome by enabling users to provide input for tasks like setting low-level objectives or summarizing insights?(2) How can we implement effective user intervention during errors or loops to guide the exploration when the system deviates, as raised in (Lahiri et al., 2022)?</p>
<p>Measurement of</p>
<p>Knowledge Integration</p>
<p>Interdisciplinary Knowledge Integration.Integrating interdisciplinary knowledge in data-driven discovery enables the interconnection of diverse domains with the highlevel research objective, uncovering nuanced associations and insights often overlooked in a single-domain analysis.The challenge lies in internalizing the complexities of different disciplines and recognizing implicit connections, similar to link prediction (Trouillon et al., 2016).</p>
<p>For example, while exploring time preference on BMI (Smith et al., 2005), it could be insightful to assess the role of economic pressure on health outcomes, using cultural anthropology to gauge spending habits, considering psychological factors to understand spending patterns, and proposing strategies for public health intervention and effective urban planning-partially achieved by DATAVOYAGER.</p>
<p>Knowledge Frontiers Support.Knowledge frontiers represent cutting edge scientific exploration and drive groundbreaking discoveries in fields like Machine Learning, gene editing, robotics, and renewable energy (Hassabis, 2002).Enhancing data-driven discovery systems by extending exploration, integrating new methods, and collecting more data can facilitate the investigation of novel scientific domains.</p>
<p>To simulate a knowledge frontier, we accessed a popular language agent repository, Reflexion (Shinn et al., 2023), and modified the experiment design following Majumder et al. (2023).The new experimental data was fed to DATAVOY-AGER, which resulted in the following concrete analysis:</p>
<p>Tasks that are more conceptual or require an understanding of complex systems (e.g., genetics, life stages) seem to be areas where the agent can learn and improve.In contrast, tasks that may involve more practical or hands-on activities (e.g., chemistry mixing, freezing) appear to be more challenging for the agent.(...) Full example: Figure 13 We seek to obtain emergent behaviors from curiosity-driven exploration and back-linking to knowledge frontiers (Groth et al., 2021).</p>
<p>We raise an open question to automatically search or generate novel datasets (Brickley et al., 2019) and conduct novel exploration with user moderation, leading to data-driven scientific discovery.</p>
<p>Research Ethics and Fairness</p>
<p>Reproducible Results.Reproducibility stands as cornerstone of the scientific process (Cao et al., 2023).However, persistent challenges in achieving reproducibility across disciplines call for innovative solutions (Magnusson et al., 2023).Complexity arises from the variations in research environments, methodologies, and resource limitations.These factors impede validation and replication of research findings, a phenomenon often evident in fields such as social science, economics, psychology, and biomedicine (Camerer et al., 2018;Collaboration, 2015;Fanelli, 2018).</p>
<p>For example, The Reproducibility Project: Psychology replicated 100 psychology studies and found only 36% of replications to yield significant results, prompting increased awareness and initiatives to enhance reproducibility across scientific disciplines (Collaboration, 2015).The ideal discovery system should ensure that the undertaken research pathways are reproducible.DATAVOYAGER shows a proof-of-concept for automated, reproducible experiments.However, it can be extended towards automatic documentation and code release, thus further improving transparency.</p>
<p>p-hacking Proof.Manipulating data or analyses to find false significance undermines the scientific process, leading to unreliable findings and subsequent slowdown of progress.</p>
<p>For an automated discovery system, this presents a particularly challenging concern and one that can affect it's trustworthiness (Wasserstein &amp; Lazar, 2016).For example, consider a scenario where an automated discovery system explores a large dataset to find potential relationships.phacking might involve tweaking variables or testing multiple hypotheses until a significant result is found (Dunn, 1961).The data-driven discovery opens up the unique case of evaluating a significant number of hypotheses at the same time, presenting opportunities for unintentional p-hacking.With a large hypotheses space, there is more chance for accidental findings.An ideal data-driven discovery system must perform tests to counter false discoveries (Korthauer et al., 2018) to keep the false discovery rate as low as possible.</p>
<p>Limitations of Data-driven Discovery</p>
<p>Hallucinations.</p>
<p>LGM-powered data-discovery struggles with output hallucinations, exacerbated by memorization and superposition issues (Elhage et al., 2022) -most susceptible being hypothesis generation, planning, and output comprehension.This undermines the benefits of automation, necessitating external verification and user moderation.</p>
<p>Cost at scale.In high-throughput fields (e.g., computational biology), it is common to test millions of hypotheses (Korthauer et al., 2019).Extensive reliance on these systems for orchestrating experiments can then incur significant computational costs-highlighting the need for integrated cost-benefit analyses into the discovery systems (Agrawal et al., 2023) using, for instance, predictive hazard functions.</p>
<p>Policy misuse.The autonomous discovery system is always at risk of misuse by bad actors to produce a substantial volume of dubious research to fit a particular agenda (Heaven, 2022).For certain disciplines like social science and economics, this could potentially impact policy-making institutions and result in sub-optimal policies and decisionmaking (Groh et al., 2022).</p>
<p>Legal Implications.Autonomous hypothesis generation and verification, supported by datasets, raise legal challenges around intellectual property rights and authorship (Callison-Burch, 2023) and liability in decision-making processes involving these systems (Farhadi et al., 2023).Defining responsibilities and establishing institutional, legal frameworks to navigate potential suboptimal policies are essential aspects of addressing this challenge.</p>
<p>Underlying Bias.An inherent challenge with the datadriven discovery system involves the potential percolation of bias originating from dual sources-the underlying dataset (Caliskan et al., 2016) and the LGMs (Feng et al., 2023).This introduces the risk of generating hypotheses that reflect and perpetuate existing biases present in the data source being utilized, potentially leading to skewed or unfair insights.</p>
<p>Survey on Related Systems</p>
<p>End-to-end Data-driven Discovery Most previous autonomous data-driven discovery systems, such as Bacon (Langley, 1981;Langley et al., 1984;1983) severely lacked the requisite computational power, restricting their scope with limited discovery of data-driven knowledge.A recent system, CoScientist Boiko et al. (2023), uses LGMs to au- Figure 3. Survey across several dimensions of a proposed data discovery system for several existing automated and semi-automated data analysis and discovery systems such as: MLAgentBench (Huang et al., 2023), CoScientist (Boiko et al., 2023), Bacon (Langley, 1977), DataLume (Gu et al., 2023) tomate some parts of the workflow, however, still requires substantial human intervention (e.g., wet lab experiments) for hypothesis verification, thus not qualifying as a fully autonomous discovery system.DataLume (Gu et al., 2023) fully automates the code generation for data transformation and hypothesis verification; however, do not have modules to support hypothesis search and orchestrating complex science workflows.Gil et al. (2022;2017;2013) as well as Automatic Analysis in WolframAlpha7 prototyped various workflows for conducting science in data-driven ways, however, such prototypes never explored the power of LGMs and are only exhibit limited generalizability to datasets and scientific methods.</p>
<p>AutoML AutoML is a workflow of automatically building optimal machine learning and predictive models.AutoML tools exist in scientific packages like Scikit (Feurer et al., 2015) and also in cloud platforms such as Google Cloud Platform8 , Microsoft Azure9 , and Amazon Web Services10 .Existing AutoML Cloud platform systems mainly focus on black box models, ensuring the models can be served at scale.Despite performing searches over hyperparameter space for optimal model development, these systems cannot comprehend the semantics of the data and hence cannot help with data-driven hypothesis generation, planning, orchestrating research pathways, and knowledge integration.</p>
<p>MLAgentBench (Huang et al., 2023) can be considered as an evolution of AutoML that performs end-to-end machine learning to benchmark AI research agents.MLAgentBench can plan, evaluate hypotheses, and measure progress, but with a focus on optimizing machine learning models, not on discovering new and novel scientific knowledge.</p>
<p>Automated Data Analysis Automated Data Analysis tools are primarily focused on exploring data under a userprovided hypothesis or query (e.g., "plot sales trends for last 12 months", etc.) and often do not have the capability of searching through the hypotheses space as defined by the data.Tools such as PowerBI11 , Tableau12 , and Thoughtspot13 can, however, perform multi-step hypothesis verification using inbuilt data transformation and statistical tools, though the interpretation and consumption of such analysis are left to the user.Spreadsheet tools such as Microsoft Excel and Google Sheets are often part of the scientific workflow as the data analysis and show limited ability for an autonomous data analysis framework even after having Python integration or coding support (Monroy, 2023;Google, 2023).Focus on integrating LGMs into data analysis with known workflows (Perlitz et al., 2022;Chakraborty et al., 2024) and code-first data analysis (Santos et al., 2023) increased recently-however, these are limited to small-scale tables and lack abilities such as the ability to orchestrate research plans, interpret results, measurement of progress, and knowledge integration.</p>
<p>Conclusion</p>
<p>We argue that ongoing ML research on reasoning, planning, code generation, and tool utilization with LGMs can have a significant influence on advancing and accelerating data-driven discovery.Such systems can transform domains overwhelmed with vast amounts of data, including but not limited to observational social sciences, medicine, astronomy, biology, climate science, computational science, consumer science, and social media analytics.</p>
<p>We posit that the time is ripe for advancing data-driven discovery, and that integrating LGMs with tools and user feedback can catalyze notable progress in scientific inquiry.We hope our timely position can increase interest and efforts in developing, debating, and enhancing the vision for an accurate, reliable, and robust system for data-driven discovery.It can help initiate a Cambrian explosion of discovery while promoting speed, reproducibility, and collaboration in scientific research.</p>
<p>Impact Statement</p>
<p>This position paper presents arguments for a goal to advance the field of science by building end-to-end data-driven discovery systems using ML.There are many potential societal consequences of our proposed direction since it involves using large generative models, some of which we cover in our Limitations section, including policy misuse, legal ramifications, and false discovery.On the positive side, our proposed system can advance the rate of discovery, leading to an improved standard of living and social well-being.</p>
<p>Group Agent Chat</p>
<p>Prompt : You interpret scientific queries, devise hypotheses, segment tasks into sequential subtasks with a focus on statistical methods, assign roles to team members, and ensure coordinated progress.(Smith et al., 2005).This figure: Data Understanding -In response to a high-level objective, the system demonstrates the need of understanding the variables before initiating statistical analysis.Moreover, it selects relevant Time Preference variables from the data and infers them (highlighted in green).(Smith et al., 2005).This figure: Data Transformation -The system generated insights from the results of Logistic Regression with L1 regularization.As a response to user input, the system showcases data transformation ability by creating new interaction terms in logistic regression models (as demonstrated in the code snippet), exploring the link between time preference and BMI across diverse demographic groups.(Smith et al., 2005).This figure: Hypothesis Verification -When the user prompted to perform sophisticated analysis to uncover new insights, the system generates new insights utilizing the Generalized Linear Model (highlighted in blue) that confirms the results from the previous OLS analysis (highlighted in green).Figure 14.Background: National Longitudinal Survey of Youth data with a question on how incarceration and race affected wealth was fed to DATAVOYAGER; it is a question studied in (Zaw et al., 2016).This figure: Knowledge Frontiers Support -Despite the original paper talking about wealth analysis post-incarceration and only doing basic statistical analysis over the data, the system was able to suggest new techniques like the application of Gini coefficients -a popular measure used in understanding wealth disparities (highlighted in green).</p>
<p>Figure 15.Background: National Longitudinal Survey of Youth data with a question on how incarceration and race affected wealth was fed to DATAVOYAGER; it is a question studied in (Zaw et al., 2016).This figure: Hypothesis Verification -After calculating Wealth Inequality across demographic groups using gini coefficients, the system interpreted the results (highlighted in green) and generated interesting insights (highlighted in blue).</p>
<p>Figure 16.Background: National Longitudinal Survey of Youth data with a question on how incarceration and race affected wealth was fed to DATAVOYAGER; it is a question studied in (Zaw et al., 2016).This figure: Human Feedback Accommodation -The system performed OLS Regression, suggested the presence of multicollinearity, and removed it using Variation Inflation Factor.Then, the system was set to address multicollinearity again, but user intervention prevented redundancy and redirected it to the objective (highlighted in green).(Alexander et al., 1982) This figure: Multi-step planning for hypothesis generation -following an initial analysis of data statistics and considering the specified goal, the system formulates a detailed plan to guide the hypothesis generation process.It generates a possible list of hypotheses (highlighted in green) and the core experimental loop (highlighted in blue.)</p>
<p>Figure 19.Background: Data from 1979 follow-up wave of the National Longitudinal Survey along with the question on how social background affects degree completion was fed to DATAVOYAGER; it is a research covered in (Alexander et al., 1982) This figure: Hypothesis Generation -The system conducts new experiments to comprehend SES data and generates hypotheses.Building upon initial statistical tests, the model delves deeply into proposing and conducting more sophisticated experiments (highlighted in green), subsequently formulating several hypotheses for further analysis (highlighted in blue).</p>
<p>Healt h Economics: Job status and income levels can affect health ... Psychology and Behavioral Economics: Stress, self-control influence saving habits and BMI ... Sociology and Cult ural St udies: Cultural norms and societal expectations can affect BMI ... Please connect BMI with graduation, family &amp; demographic data, run more sophisticated model.1. SES: Compare association between subject variables based on SES 2trasnform the data by adding int eract ion variables Measure effects using Generalized Linear Model on 'SES', 'SAMPLE_SEX', 'SAMPLE_RACE', 'AVSAB Scores' and 'Class Percentile'</p>
<p>Figure 2 .
2
Figure 2.An example workflow of DATAVOYAGER.Starting from a user-provided dataset and a high-level query, it navigates through cycles of hypothesis generation, validation, and analysis to uncover complex insights.See all examples in Appendix for full understanding.</p>
<p>I.</p>
<p>Understand the data (...) II.Generate initial hypotheses (...) III.Explore combinations of dependent variables (...) IV.Call the "run_logistic_regression" function (...) V. Repeat step IV for other combinations of dependent variables (...) VI.Document the findings (...) VII.Seek clarity where required (...).Full example: Figure18</p>
<p>Figure 6 .
6
Figure 6.Background: Data from National Longitudinal Survey of Youth along with question on relation between time preference &amp; BMI was fed into DATAVOYAGER; it is a question studied in(Smith et al., 2005).This figure: Data Understanding -In response to a high-level objective, the system demonstrates the need of understanding the variables before initiating statistical analysis.Moreover, it selects relevant Time Preference variables from the data and infers them (highlighted in green).</p>
<p>Figure 7 .
7
Figure 7. Background: Data from National Longitudinal Survey of Youth along with question on relation between time preference &amp; BMI was fed into DATAVOYAGER; it is a question studied in(Smith et al., 2005).This figure: Data Transformation -The system generated insights from the results of Logistic Regression with L1 regularization.As a response to user input, the system showcases data transformation ability by creating new interaction terms in logistic regression models (as demonstrated in the code snippet), exploring the link between time preference and BMI across diverse demographic groups.</p>
<p>Figure 8 .
8
Figure 8. Background: Data from National Longitudinal Survey of Youth along with question on relation between time preference &amp; BMI was fed into DATAVOYAGER; it is a question studied in(Smith et al., 2005).This figure: Interdisciplinary Knowledge Integration -The system extracts insights from BMI data, generates insights (highlighted in yellow) from the lens of different disciplines and integrates them into different interdisciplinary hypotheses for further exploration (highlighted in green).</p>
<p>Figure 9 .
9
Figure 9. Background: Data from National Longitudinal Survey of Youth along with question on relation between time preference &amp; BMI was fed into DATAVOYAGER; it is a question studied in(Smith et al., 2005).This figure: Hypothesis Verification -When the user prompted to perform sophisticated analysis to uncover new insights, the system generates new insights utilizing the Generalized Linear Model (highlighted in blue) that confirms the results from the previous OLS analysis (highlighted in green).</p>
<p>Figure 10 .
10
Figure10.Background: Experimental data of running methods using a popular agent-based repo Reflexion (https://github.com/noahshinn/reflexion) and(Majumder et al., 2023) was fed to DATAVOYAGER.This figure: Knowledge Frontiers Support -Data Expert suggested interesting list of analyses to find new insights.New analyses (highlighted in green) were created with limited context on the data just based on variable description.Cluster analysis (highlighted in blue) leads to novel insights in the agent literature.</p>
<p>Figure 11 .
11
Figure11.Background: Experimental data of running methods using a popular agent-based repo Reflexion (https://github.com/noahshinn/reflexion) and(Majumder et al., 2023) was fed to DATAVOYAGER.This figure: Multi-Step Planning -The system understood the variables and carved out the steps that need to be performed to draw interesting insights (highlighted in blue).The Planner created an excellent plan by breaking the objective into subtasks to carry-out learning progression analysis.It then assigned the subtask to a team member (highlighted in yellow).</p>
<p>Figure 12 .
12
Figure12.Background: Experimental data of running methods using a popular agent-based repo Reflexion (https://github.com/noahshinn/reflexion) and(Majumder et al., 2023) was fed to DATAVOYAGER.This figure: Hypothesis Generation -When asked to generate hypotheses and perform sophisticated analysis based on insights (highlighted in green), the system generates testable hypotheses, formulating clear null (H0) and alternative (H1) hypotheses with corresponding statistical tests (highlighted in yellow) for uncovering complex underlying patterns in the data.</p>
<p>Figure 13 .
13
Figure 13.Background: Experimental data of running methods using a popular agent-based repo Reflexion (https://github.com/noahshinn/reflexion) and (Majumder et al., 2023) was fed to DATAVOYAGER.This figure: Knowledge Frontiers Support -Ability to support and generate new insights on frontiers of knowledge, where novel insights on agents' behavior were generated.The new insights are highlighted in blue.</p>
<p>Figure 17 .
17
Figure17.Background: National Longitudinal Survey of Youth data with a question on how incarceration and race affected wealth was fed to DATAVOYAGER; it is a question studied in(Zaw et al., 2016).This figure: Hypothesis Verification -Following the results of Descriptive Statistics, the Data Expert proposed two hypotheses.When the user prompted the system to perform Hypothesis Testing it verified them by performing T-tests, interpreted them (highlighted in green) and shared the conclusions.</p>
<p>Figure 18 .
18
Figure18.Background: Data from 1979 follow-up wave of the National Longitudinal Survey along with the question on how social background affects degree completion was fed to DATAVOYAGER; it is a research covered in(Alexander et al., 1982) This figure: Multi-step planning for hypothesis generation -following an initial analysis of data statistics and considering the specified goal, the system formulates a detailed plan to guide the hypothesis generation process.It generates a possible list of hypotheses (highlighted in green) and the core experimental loop (highlighted in blue.)</p>
<p>Time preference could be 'DISSAVED' and 'SAMESAVE' variables.1.Initial Hypotheses: a. Hypot hesis 1: DISSAVED and BMI are related... 2. Perform OLS &amp; Correlation analysis ....</p>
<p>, ThoughtSpot (thoughtspot.com),Google AutoML (cloud.google.com/automl),and Automatic Analysis* from WolframAlpha (wolframalpha.com/examples/pro-features/data-input).</p>
<p>Agent Structure for DATAVOYAGER.Group Agent Chat has AutoGen agents that communicate with each other.The User Proxy links the user with the agents to share data, feedback, and goals.Code Execution Environment has access to structured functions and code generation methods that can be called depending on the context.
Aut oGen User ProxyCode Execut ion Envsyst em_message=" An admi nt hat t akes i nput f r om t het i meoutuser . "cache_seedconf i g_l i stt er mi nat i on_cr i t er i acode_execut i on_conf i gf unct i ons_f or _pyt hon_cel l ( )max_consecut i ve_aut o_r epl yl l m_conf i gf unct i ons_f or _shel l ( )human_i nput _modest at s_f unct i ons( )i ni t i at e_chat =" St ar t i ngmessage f or t hatgoal _pr edi ct or ( )exper i ment "PlannerDat a ExpertProgrammerCrit ic*Prompt : You specialize in analyzing statistical data and user queries, offering detailed inferences, formulating and testing hypotheses, and collaborating with programmers for sophisticated data modeling and inferential insights.Prompt : You develop and code tasks assigned by the Planner, utilizing specific function calls and adhering to guidelines to produce outputs in JSON format, with a focus on robust coding and comprehensive logging. t ool s code genPrompt : As the Critic, you evaluate and assure the quality of research processes and outcomes, scrutinize research methodologies, assess data quality, and provide constructive feedback on analytical methods and findings.Figure 4.
https://www.bls.gov/nls/
https://microsoft.github.io/autogen/
We develop several functions (e.g., statistical analysis tools based on datatype, python shell execution tools) for robustness.
Time preference reflects how individuals value present over future benefits. A lower time preference can lead to higher savings, better food consumption, and thus a healthier BMI in the future.
www.whitehouse.gov/cancermoonshot/
https://www.wolframalpha.com/examples/ pro-features/data-input
 cloud.google.com/automl <br />
azure.microsoft.com/en-us/products/ machine-learning/automatedml/
aws.amazon.com/machine-learning/automl/
https://www.microsoft.com/en-us/powerplatform/products/power-bi
12 https://www.tableau.
com/ 13 https://www.thoughtspot.com/
AcknowledgmentsWe sincerely thank Abhijeetsingh Meena, Aryan Prakhar, and Tirth Vora for their engineering and exploration efforts in making DATAVOYAGER.We also thank Peter Jansen, David Wadden, Yoav Golberg, and Daniel Weld for their useful comments.We thank Siddharth Sharma and Siddharth Narayanan for their help with proofreading.
. O J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, R Avila, I Babuschkin, S Balaji, V Balcom, P Baltescu, H Bao, M Bavarian, J Belgum, I Bello, J Berdine, G Bernadett-Shapiro, C Berner, L Bogdonoff, O Boiko, M Boyd, A.-L Brakman, G Brockman, T Brooks, M Brundage, K Button, T Cai, R Campbell, A Cann, B Carey, C Carlson, R Carmichael, B Chan, C Chang, F Chantzis, D Chen, S Chen, R Chen, J Chen, M Chen, B Chess, C Cho, C Chu, H W Chung, D Cummings, J Currier, Y Dai, C Decareaux, T Degry, N Deutsch, D Deville, A Dhar, D Dohan, S Dowling, S Dunning, A Ecoffet, A Eleti, T Eloundou, D Farhi, L Fedus, N Felix, S P Fishman, J Forte, I Fulford, L Gao, E Georges, C Gibson, V Goel, T Gogineni, G Goh, R Gontijo-Lopes, J Gordon, M Grafstein, S Gray, R Greene, J Gross, S S Gu, Y Guo, C Hallacy, J Han, J Harris, Y He, M Heaton, J Heidecke, C Hesse, A Hickey, W Hickey, P Hoeschele, B Houghton, K Hsu, S Hu, X Hu, J Huizinga, S Jain, S Jain, J Jang, A Jiang, R Jiang, H Jin, D Jin, S Jomoto, B Jonn, H Jun, T Kaftan, L Kaiser, A Kamali, I Kanitscheider, N S Keskar, T Khan, L Kilpatrick, J W Kim, C Kim, Y Kim, H Kirchner, J R Kiros, M Knight, D Kokotajlo, L Kondraciuk, A Kondrich, A Konstantinidis, K Kosic, G Krueger, V Kuo, M Lampe, I Lan, T Lee, J Leike, J Leung, D Levy, C M Li, R Lim, M Lin, S Lin, M Litwin, T Lopez, R Lowe, P Lue, A A Makanju, K Malfacini, S Manning, T Markov, Y Markovski, B Martin, K Mayer, A Mayne, B Mc-Grew, S M Mckinney, C Mcleavey, P Mcmillan, J Mcneil, D Medina, A Mehta, J Menick, L Metz, A Mishchenko, P Mishkin, V Monaco, E Morikawa, D P Mossing, T Mu, M Murati, O Murk, D M'ely, A Nair, R Nakano, R Nayak, A Neelakantan, R Ngo, H Noh, O Long, C O'keefe, J W Pachocki, A Paino, J Palermo, A Pantuliano, G Parascandolo, J Parish, E Parparita, A Passos, M Pavlov, A Peng, A Perelman, F De Avila Belbute Peres, M Petrov, H P De Oliveira Pinto, M Pokorny, M Pokrass, V H Pong, T Powell, A Power, B Power, E Proehl, R Puri, A Radford, J Rae, A Ramesh, C Raymond, F Real, K Rimbach, C Ross, B Rotsted, H Roussez, N Ryder, M D Saltarelli, T Sanders, S Santurkar, G Sastry, H Schmidt, D Schnurr, J Schulman, D Selsam, K Sheppard, T Sherbakov, J Shieh, S Shoker, P Shyam, S Sidor, E Sigler, M Simens, J Sitkin, K Slama, I Sohl, B D Sokolowsky, Y Song, N Staudacher, F P Such, N Summers, I Sutskever, J Tang, N A Tezak, M Thompson, P Tillet, A Tootoonchian, E Tseng, P Tuggle, N Turley, J Tworek, J F C Uribe, A Vallone, A Vijayvergiya, C Voss, C Wainwright, J J Wang, A Wang, B Wang, J Ward, J Wei, C Weinmann, A Welihinda, P Welinder, J Weng, L Weng, M Wiethoff, D Willner, C Winter, S Wolrich, H Wong, L Workman, S Wu, J Wu, M Wu, K Xiao, T Xu, S Yoo, K Yu, Q Yuan, W Zaremba, R Zellers, C Zhang, M Zhang, S Zhao, T Zheng, J Zhuang, W Zhuk, B Zoph, 2023257532815Gpt-4 technical report</p>
<p>Bring your own kg: Self-supervised program synthesis for zero-shot kgqa. D Agarwal, R Das, S Khosla, R Gangadharaiah, ArXiv, abs/2311.078502023</p>
<p>Artificial intelligence and scientific discovery: A model of prioritized search. A Agrawal, J Mchale, A Oettl, SSRN Electronic Journal. 2023</p>
<p>Social background, academic resources, and college graduation: Recent evidence from the national longitudinal survey. K L Alexander, C Riordan, J Fennessey, A M Pallas, American Journal of Education. 9041982</p>
<p>The end of theory: The data deluge makes the scientific method obsolete. Wired magazine. C Anderson, 200816</p>
<p>Prioritizing attention in fast data. P Bailis, E Gan, S Madden, D Narayanan, K Rong, S Suri, Macrobase, Proceedings of the 2017 ACM International Conference on Management of Data. the 2017 ACM International Conference on Management of Data2017</p>
<p>Conceptual biology, hypothesis discovery, and text mining: Swanson's legacy. T Bekhuis, Biomedical digital libraries. 32006</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Artificial intelligence in science: An emerging general method of invention. S Bianchini, M Müller, P Pelletier, Research Policy. 51101046042022</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 6242664320592023</p>
<p>An ontology-driven framework for data transformation in scientific workflows. S Bowers, B Ludäscher, International Workshop on Data Integration in the Life Sciences. Springer2004</p>
<p>Google dataset search: Building a search engine for datasets in an open web ecosystem. D Brickley, M Burgess, N Noy, The World Wide Web Conference. 201986688027</p>
<p>Y Burda, H Edwards, A J Storkey, O Klimov, Exploration by random network distillation. </p>
<p>Large language models as tool makers. T Cai, X Wang, T Ma, X Chen, D Zhou, ArXiv, abs/2305.171262018. 2023</p>
<p>Semantics derived automatically from language corpora contain human-like biases. A Caliskan, J J Bryson, A Narayanan, Science. 3562016</p>
<p>Understanding generative artificial intelligence and its relationship to copyright. Testimony before The U.S. House of Representatives Judiciary Committee. C Callison-Burch, Hearing on Artificial Intelligence and Intellectual Property: Part I -Interoperability of AI and Copyright Law. May 2023Subcommittee on Courts, Intellectual Property, and the Internet</p>
<p>Evaluating the replicability of social science experiments in nature and science between. C Camerer, A Dreber, F Holzmeister, T.-H Ho, J Huber, M Johannesson, M Kirchler, G Nave, B A Nosek, T Pfeiffer, A Altmejd, N Buttrick, T Chan, Y Chen, E Forsell, A Gampa, E Heikensten, L Hummer, T Imai, S Isaksson, D Manfredi, J Rose, E Wagenmakers, H Wu, Nature Human Behaviour. 2010. 2015. 20182</p>
<p>The rise of open science: Tracking the evolution and perceived value of data and methods link-sharing practices. H Cao, J Dodge, K Lo, D A Mcfarland, L L Wang, ArXiv, abs/2310.031932023</p>
<p>Navigator: A gen-ai system for discovery of factual and predictive insights on domain-specific tabular datasets. A Chakraborty, A Banerjee, S Dasgupta, V Raturi, A Soni, A Gupta, S Harsola, V T Subrahmaniam, Proceedings of the 7th Joint International Conference on Data Science &amp; Management of Data (11th ACM IKDD CODS and 29th COMAD). the 7th Joint International Conference on Data Science &amp; Management of Data (11th ACM IKDD CODS and 29th COMAD)2024</p>
<p>Augmenting citations in scientific papers with persistent and personalized historical context. J C Chang, A X Zhang, J Bragg, A Head, K Lo, D Downey, D S Weld, Citesee, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023256868353</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, ArXiv, abs/2110.141682021239998651</p>
<p>Reproducibility project: Psychology. O S Collaboration, 10.17605/OSF.IO/EZCUJ2015</p>
<p>Explaining answers with entailment trees. B Dalvi, P A Jansen, O Tafjord, Z Xie, H Smith, L Pipatanangkura, P Clark, Conference on Empirical Methods in Natural Language Processing. 2021233297051</p>
<p>Multiple comparisons among means. O J Dunn, Journal of the American Statistical Association. 196156</p>
<p>Toy models of superposition. N Elhage, T Hume, C Olsson, N Schiefer, T Henighan, S Kravec, Z Hatfield-Dodds, R Lasenby, D Drain, C Chen, R Grosse, S Mccandlish, J Kaplan, D Amodei, M Wattenberg, C Olah, Transformer Circuits Thread. 2022</p>
<p>Conceptions of good science in our datarich world. K C Elliott, K S Cheruvelil, G M Montgomery, P A Soranno, BioScience. 66102016</p>
<p>Dreamcoder: growing generalizable, interpretable knowledge with wake-sleep bayesian program learning. K Ellis, C Wong, M Nye, M Sablé-Meyer, L Cary, L Morales, L Hewitt, A Solar-Lezama, J B Tenenbaum, Philosophical Transactions of the Royal Society A. 3812020</p>
<p>Diversity is all you need: Learning skills without a reward function. B Eysenbach, A Gupta, J Ibarz, S Levine, International Conference on Learning Representations. 2019</p>
<p>Opinion: Is science really facing a reproducibility crisis, and do we need it to?. D Fanelli, Proceedings of the National Academy of Sciences. 11546398562018</p>
<p>AI2's Response to the US Copyright Requence for Comments on Artificial Intelligence and Copyright. A Farhadi, D Atkinson, C Callison-Burch, N Decario, J Dumas, K Lo, L Soldiani, 2023-6, 2023US Copyright Office Docket NoComment</p>
<p>From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair nlp models. S Feng, C Y Park, Y Liu, Y Tsvetkov, ArXiv, abs/2305.082832023258686693</p>
<p>Efficient and robust automated machine learning. Advances in neural information processing systems. M Feurer, A Klein, K Eggensperger, J Springenberg, M Blum, F Hutter, 201528</p>
<p>Serendipity and information seeking: an empirical study. A Foster, N Ford, Journal of documentation. 5932003</p>
<p>Using semantic workflows to disseminate best practices and accelerate discoveries in multi-omic data analysis. Y Gil, S K Mcweeney, C E Mason, AAAI Conference on Artificial Intelligence. 2013</p>
<p>Towards continuous scientific data analysis and hypothesis evolution. Y Gil, D Garijo, V Ratnakar, R Mayani, R Adusumilli, H Boyce, A Srivastava, P Mallick, AAAI Conference on Artificial Intelligence. 2017</p>
<p>Towards capturing scientific reasoning to automate data analysis. Y Gil, D Khider, M Osorio, V Ratnakar, H Vargas, D Garijo, CorpusID:248914202. Google. Introducing duet ai for google workspace. 2022. 2023</p>
<p>Human detection of political speech deepfakes across transcripts, audio, and video. M Groh, A Sankaranarayanan, N Singh, D Y Kim, A Lippman, R W Picard, 2022259342907</p>
<p>Is curiosity all you need? on the utility of emergent behaviours from curious exploration. O Groth, M Wulfmeier, G Vezzani, V Dasagi, T Hertweck, R Hafner, N M O Heess, M A Riedmiller, ArXiv, abs/2109.086032021</p>
<p>How do data analysts respond to ai assistance? a wizard-of-oz study. K Gu, M Grunde-Mclaughlin, A M Mcnutt, J Heer, T Althoff, ArXiv, abs/2309.101082023</p>
<p>Using ai to accelerate scientific discovery. D Hassabis, 2002</p>
<p>Why meta's latest large language model survived only three days online. W D Heaven, MIT Technology Review. Last. accessed December, 15:2022, 2022</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D X Song, J Steinhardt, ArXiv, abs/2009.033002020221516475</p>
<p>Entropy search for informationefficient global optimization. P Hennig, C J Schuler, Journal of Machine Learning Research. 1362012</p>
<p>A python package to calculate the olr-based index of the madden-julianoscillation (omi) in climate science and weather forecasting. C G Hoffmann, G N Kiladis, M Gehne, C Savigny, Journal of Open Research Software. 2365866552021</p>
<p>Large language models for software engineering: A systematic literature review. X Hou, Y Zhao, Y Liu, Z Yang, K Wang, L Li, X Luo, D Lo, J C Grundy, H Wang, ArXiv, abs/2308.106202023</p>
<p>Vime: Variational information maximizing exploration. R Houthooft, X Chen, Y Duan, J Schulman, F De Turck, P Abbeel, Advances in neural information processing systems. 292016</p>
<p>Benchmarking large language models as ai research agents. Q Huang, J Vora, P Liang, J Leskovec, ArXiv, abs/2310.033022023</p>
<p>Highly accurate protein structure prediction with alphafold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, Nature. 59678732021</p>
<p>Llms can't plan, but can help planning in llm-modulo frameworks. D Kahneman, Thinking, S Kambhampati, K Valmeekam, L Guan, K Stechly, M Verma, S Bhambri, L Saldyt, A Murthy, arXiv:2402.018172024arXiv preprint</p>
<p>Optimizing deep cnn-based queries over video streams at scale. D Kang, J Emmons, F Abuzaid, P D Bailis, M A Zaharia, Noscope, Proc. VLDB Endow. VLDB Endow201710</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. T Khot, H Trivedi, M Finlayson, Y Fu, K Richardson, P Clark, A Sabharwal, ArXiv, abs/2210.024062022252715485</p>
<p>A practical guide to methods controlling false discoveries in computational biology. K Korthauer, P K Kimes, C Duvallet, A Reyes, A Subramanian, M Teng, C Shukla, E J Alm, S C Hicks, 10.1186/s13059-019-1716-1Genome Biology. 202019</p>
<p>A practical guide to methods controlling false discoveries in computational biology. K D Korthauer, P K Kimes, C Duvallet, A Reyes, A Subramanian, M Teng, C J Shukla, E J Alm, S C Hicks, Genome Biology. 202018</p>
<p>Interactive code generation via testdriven user-intent formalization. S K Lahiri, A Naik, G Sakkas, P Choudhury, C Veh, M Musuvathi, J P Inala, C Wang, J Gao, ArXiv, abs/2208.059502022251492970</p>
<p>A production system that discovers empirical laws. P Langley, Bacon, International Joint Conference on Artificial Intelligence. 19772320342</p>
<p>Data-driven discovery of physical laws. P Langley, Cogn. Sci. 5396942511981</p>
<p>Rediscovering chemistry with the bacon system. P Langley, G L Bradshaw, H A Simon, 1983</p>
<p>The search for regularity: Four aspects of scientific discovery. P Langley, J M Zytkow, H A Simon, G L Bradshaw, 19843155192</p>
<p>The cancer genomics cloud: collaborative, reproducible, and democratized-a new paradigm in large-scale computational research. J W Lau, E Lehnert, A Sethi, R Malhotra, G Kaushik, Z Onder, N Groves-Kirkby, A Mihajlovic, J Digiovanna, M Srdic, Cancer research. 77212017</p>
<p>A path towards autonomous machine intelligence version 0. Y Lecun, Open Review. 912022</p>
<p>Challenges in high-throughput inorganic material prediction and autonomous synthesis. J Leeman, Y Liu, J Stiles, S Lee, P Bhatt, L Schoop, R Palgrave, 2024</p>
<p>Competition-level code generation with alphacode. Y Li, D H Choi, J Chung, N Kushman, J Schrittwieser, R Leblond, Tom, Eccles, J Keeling, F Gimeno, A D Lago, T Hubert, P Choy, C De, M D'autume, I Babuschkin, X Chen, P.-S Huang, J Welbl, S Gowal, Alexey, Cherepanov, J Molloy, D J Mankowitz, E S Robson, P Kohli, N De, Freitas, K Kavukcuoglu, O Vinyals, Science. 3782022</p>
<p>On continual model refinement in out-of-distribution data streams. B Y Lin, S I Wang, X V Lin, R Jia, L Xiao, X Ren, W Yih, Annual Meeting of the Association for Computational Linguistics. 2022</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, ArXiv, abs/2304.084852023a</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. J Liu, C Xia, Y Wang, L Zhang, ArXiv, abs/2305.012102023b258437095</p>
<p>X Liu, H Yu, H Zhang, Y Xu, X Lei, H Lai, Y Gu, Y Gu, H Ding, K Men, K Yang, S Zhang, X Deng, A Zeng, Z Du, C Zhang, S Shen, T Zhang, Y Su, H Sun, M Huang, Y Dong, J Tang, ArXiv, abs/2308.03688Agentbench: Evaluating llms as agents. 2023c260682249</p>
<p>Evaluating mathematical reasoning of foundation models in visual contexts. P Lu, H Bansal, T Xia, J Liu, C Yue Li, H Hajishirzi, H Cheng, K.-W Chang, M Galley, J Gao, Mathvista, 2023</p>
<p>Pynguin: Automated unit test generation for python. S Lukasczyk, G Fraser, IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion). 2022. 2022246706202</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, S Welleck, B P Majumder, S Gupta, A Yazdanbakhsh, P Clark, ArXiv, abs/2303.176512023257900871</p>
<p>Reproducibility in nlp: What have we learned from the checklist?. I H Magnusson, N A Smith, J Dodge, Annual Meeting of the Association for Computational Linguistics. 2023</p>
<p>Ask what's missing and what's useful: Improving clarification question generation using global knowledge. B P Majumder, S Rao, M Galley, J Mcauley, North American Chapter. the Association for Computational Linguistics2021</p>
<p>Achieving conversational goals with unsupervised post-hoc knowledge injection. B P Majumder, H Jhamtani, T Berg-Kirkpatrick, J Mcauley, ArXiv, abs/2203.113992022</p>
<p>Clin: A continually learning language agent for rapid task adaptation and generalization. B P Majumder, B Dalvi, P Jansen, O Tafjord, N Tandon, L Zhang, C Callison-Burch, P Clark, ArXiv, abs/2310.101342023</p>
<p>Introducing copilot support for python in excel: Advanced data analysis using natural language. D Monroy, 2023</p>
<p>Computer science as empirical inquiry: symbols and search. A Newell, H A Simon, 10.1145/360018.360022Commun. ACM. 0001-0782193mar 1976</p>
<p>How can we define intrinsic motivation?. P.-Y Oudeyer, F Kaplan, 2008</p>
<p>Intrinsic motivation systems for autonomous mental development. P.-Y Oudeyer, F Kaplan, V V Hafner, IEEE transactions on evolutionary computation. 1122007</p>
<p>Relatedly: Scaffolding literature reviews with existing related work sections. S Palani, A Naik, D Downey, A X Zhang, J Bragg, J C Chang, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023256846632</p>
<p>Curiosity-driven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, International conference on machine learning. PMLR2017</p>
<p>K Pelrine, M Taufeeque, M Zajkac, E Mclean, A Gleave, Exploiting novel gpt-4 apis. </p>
<p>nbiig: A neural bi insights generation system for table reporting. Y Perlitz, D Sheinwald, N Slonim, M Shmueli-Scheuer, AAAI Conference on Artificial Intelligence. 2022</p>
<p>Adapt: As-needed decomposition and planning with language models. A Prasad, A Koller, M Hartmann, P Clark, A Sabharwal, M Bansal, T Khot, ArXiv, abs/2311.057722023</p>
<p>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. L Qiu, L Jiang, X Lu, M Sclar, V Pyatkin, C Bhagavatula, B Wang, Y Kim, Y Choi, N Dziri, X Ren, ArXiv, abs/2310.085592023</p>
<p>Data mining: From serendipity to science. N Ramakrishnan, A Y Grama, Computer. 3281999</p>
<p>Semantic web in data mining and knowledge discovery: A comprehensive survey. P Ristoski, H Paulheim, J. Web Semant. 36428461212016</p>
<p>Mathematical discoveries from program search with large language models. B Romera-Paredes, M Barekatain, A Novikov, M Balog, M P Kumar, E Dupont, F J R Ruiz, J S Ellenberg, P Wang, O Fawzi, P Kohli, A Fawzi, J Grochow, A Lodi, J.-B Mouret, T Ringer, T Yu, Nature. 6252023</p>
<p>An r package for the evaluation of failure time surrogate endpoints in individual patient data meta-analyses of randomized clinical trials. F Rotolo, X Paoletti, S Michiels, Surrosurv, Computer methods and programs in biomedicine. 15534804782018</p>
<p>Pandasprofiling now supports apache spark. M Santos, F Clemente, C Abshire, 2023</p>
<p>An empirical evaluation of using large language models for automated unit test generation. M Schäfer, S Nadi, A Eghbali, F Tip, IEEE Transactions on Software Engineering. 502023</p>
<p>Toolformer: Language models can teach themselves to use tools. T Schick, J Dwivedi-Yu, R Dessi, R Raileanu, M Lomeli, E Hambro, L Zettlemoyer, N Cancedda, T Scialom, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Automatic data transformation using large language model -an experimental study on building energy data. A Sharma, X Li, H Guan, G Sun, L Zhang, L Wang, K Wu, L Cao, E Zhu, A Sim, T Wu, J Zou, IEEE International Conference on Big Data (BigData). 2023. 2023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, B Labash, A Gopinath, K Narasimhan, S Yao, 2023258833055</p>
<p>Are time preference and body mass index associated?: Evidence from the national longitudinal survey of youth. P K Smith, B Bogin, D Bishai, Economics &amp; Human Biology. 322005</p>
<p>Open-endedness: The last grand challenge you've never heard of. While open-endedness could be a force for discovering intelligence. K O Stanley, J Lehman, L Soros, 2017it could also be a component of AI itself</p>
<p>Sql-palm: Improved large language model adaptation for text-to-sql. R Sun, S Ö Arik, H Nakhost, H Dai, R Sinha, P Yin, T Pfister, ArXiv, abs/2306.007392023258999853</p>
<p>Undiscovered public knowledge. The Library Quarterly. D R Swanson, 198656144270735</p>
<p>The Black Swan: The Impact of the Highly Improbable. Random House Group. N N Taleb, 2007ISBN 1400063515</p>
<p>. H Touvron, L Martin, K R Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D M Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, B Fuller, C Gao, V Goswami, N Goyal, A S Hartshorn, S Hosseini, R Hou, H Inan, M Kardas, V Kerkez, M Khabsa, I M Kloumann, A V Korenev, P S Koura, M.-A Lachaux, T Lavril, J Lee, D Liskovich, Y Lu, Y Mao, X Martinet, T Mihaylov, P Mishra, I Molybog, Y Nie, A Poulton, J Reizenstein, R Rungta, K Saladi, A Schelten, R Silva, E M Smith, R Subramanian, X Tan, B Tang, R Taylor, A Williams, J X Kuan, P Xu, Z Yan, I Zarov, Y Zhang, A Fan, M Kambadur, S Narang, A Rodriguez, R Stojnic, S Edunov, Scialom, ArXiv, abs/2307.0928820232Open foundation and fine-tuned chat models</p>
<p>Solving olympiad geometry without human demonstrations. T H Trinh, Y Wu, Q V Le, H He, T Luong, Nature. 62579952024</p>
<p>Complex embeddings for simple link prediction. T Trouillon, J Welbl, S Riedel, É Gaussier, G Bouchard, ArXiv, abs/1606.063572016</p>
<p>Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. P Vaithilingam, T Zhang, E L Glassman, CHI Conference on Human Factors in Computing Systems Extended Abstracts. 2022247255943</p>
<p>An extensible benchmark for evaluating large language models on planning and reasoning about change. K Valmeekam, A Olmo, S Sreedharan, S Kambhampati, Planbench, 2022</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L J Fan, A Anandkumar, ArXiv, abs/2305.162912023a258887849</p>
<p>Learning to generate novel scientific directions with contextualized literature-based discovery. Q Wang, D Downey, H Ji, T Hope, ArXiv, abs/2305.142592023b258841365</p>
<p>The asa statement on p-values: Context, process, and purpose. R Wasserstein, N A Lazar, The American Statistician. 702016</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, International Conference on Learning Representations. 2022</p>
<p>Overview of issues in the longitudinal analysis of respiratory data. S T Weiss, J H Ware, 199615445049299American journal of respiratory and critical care medicine</p>
<p>Optimal search for the best alternative. M Weitzman, Econometrica. 47325308811978</p>
<p>Fundamental principles of deception in genetic search. L D Whitley, Foundations of genetic algorithms. Elsevier19911</p>
<p>Fundamental limitations of alignment in large language models. Y Wolf, N Wies, Y Levine, A Shashua, ArXiv, abs/2304.110822023</p>
<p>wealth and incarceration: Results from the national longitudinal survey of youth. K Zaw, D Hamilton, W A J Darity, Race, Race and Social Problems. 8137097792016</p>
<p>Open-endedness via models of human notions of interestingness. J Zhang, J Lehman, K Stanley, J Clune, Omni, arXiv:2306.017112023arXiv preprint</p>
<p>Interactive evaluation for social intelligence in language agents. X Zhou, H Zhu, L Mathur, R Zhang, H Yu, Z Qi, L.-P Morency, Y Bisk, D Fried, G Neubig, M Sap, Sotopia, ArXiv, abs/2310.116672023</p>            </div>
        </div>

    </div>
</body>
</html>
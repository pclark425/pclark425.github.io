<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1246 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1246</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1246</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-248084847</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2204.04421v2.pdf" target="_blank">Unbiased Directed Object Attention Graph for Object Navigation</a></p>
                <p><strong>Paper Abstract:</strong> Object navigation tasks require agents to locate specific objects in unknown environments based on visual information. Previously, graph convolutions were used to implicitly explore the relationships between objects. However, due to differences in visibility among objects, it is easy to generate biases in object attention. Thus, in this paper, we propose a directed object attention (DOA) graph to guide the agent in explicitly learning the attention relationships between objects, thereby reducing the object attention bias. In particular, we use the DOA graph to perform unbiased adaptive object attention (UAOA) on the object features and unbiased adaptive image attention (UAIA) on the raw images, respectively. To distinguish features in different branches, a concise adaptive branch energy distribution (ABED) method is proposed. We assess our methods on the AI2-Thor dataset. Compared with the state-of-the-art (SOTA) method, our method reports 7.4%, 8.1% and 17.6% increase in success rate (SR), success weighted by path length (SPL) and success weighted by action efficiency (SAE), respectively.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1246.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1246.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI2-THOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI2-THOR: An Interactive 3D Environment for Visual AI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated indoor, household 3D environment used for embodied visual navigation research where agents receive egocentric single-view RGB observations and are tasked to find specified object categories in unseen rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI2-THOR: An Interactive 3D Environment for Visual AI</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>AI2-THOR</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Household/indoor navigation domain (kitchen, living room, bedroom, bathroom). Agents receive a single egocentric RGB view per timestep and a target object name; interaction limited to navigation and object detection; simulated moderate realism.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Navigation topology is implicit in continuous room geometry; object-observation graphs are sparse (few objects visible per view) and object-relation adjacency matrices are often sparse with many zero edges.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Dataset: 120 floorplans (30 per 4 room types). Experimental split used in paper: 80 training rooms, 20 validation rooms, 20 test rooms (20/5/5 per room type). Agent-visible object set N = 22 recognizable object categories.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>A3C + LSTM (reinforcement learning policy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A recurrent LSTM policy trained with asynchronous advantage actor-critic (A3C). Inputs are image, object detections and previous action branches; cross-attention modules (UAOA and UAIA) use a learned Directed Object Attention (DOA) graph to bias perception; ABED supplies branch identity/energy tokens before the LSTM.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Success weighted by Path Length (SPL), Success weighted by Action Efficiency (SAE), episode time / episode length</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Baseline (this paper): SR ALL = 69.14%; With DOA+ABED: SR ALL = 74.32% (also reported: SR L>=5 = 67.88%)</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-based recurrent policy (LSTM) augmented with learned object-relationship graph attention (DOA) and cross-attention between object and image modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>The paper reports that sparsity and bias in object-relation adjacency matrices cause attention to concentrate on highly visible objects (large bounding boxes, high detector confidence), degrading navigation; explicitly modeling directed object relations (DOA) and decoupling attention from raw object feature magnitudes reduces this bias and improves SR, SPL and SAE.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Within the learned object-graph family the authors compare intrinsic-only, view-adaptive-only, and combined DOA variants and directed vs undirected adjacency. Key findings: (1) intrinsic graph (IG) alone improves navigation vs no graph; (2) view-adaptive graph (VAG) alone can hurt performance (fully adaptive learning without sufficient prior is unstable); (3) the weighted combination (IG + small-weight VAG) yields best results; (4) directed DOA performs significantly better than an undirected graph.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that incorporate explicit directed object-relations and cross-attention (UAOA/UAIA) perform better; memory (LSTM) is used to aggregate multimodal history; debiasing object-attention (via DOA) is crucial — UAOA (object-branch attention guided by DOA) yields larger gains than UAIA; directed graphs support adaptive, target-conditioned attention and improve robustness in unseen environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unbiased Directed Object Attention Graph for Object Navigation', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1246.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1246.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DOA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Directed Object Attention (DOA) Graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned, two-layer directed object-relation graph that explicitly models attention weights from each visible object to a target object, combining a learnable intrinsic-object graph (stable prior) and a view-adaptive graph (per-timestep attention) to mitigate object-visibility bias in object-goal navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>AI2-THOR (indoor object-goal navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Object-relation graph over N = 22 object categories observed by an embodied agent in AI2-THOR indoor scenes; graph nodes are object categories/detections and directed edges encode attention (importance) from observed objects toward the queried target category.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Learned adjacency is sparse for per-view graphs (many zeros due to few visible objects); intrinsic-object matrix is learned across training and row-normalized (each target's incoming attention sums to 1); view-adaptive graph is extremely peaked per timestep (often one object gets weight ~1).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Object-graph node count N = 22 (number of object categories). The DOA adjacency matrix is therefore 22 × 22.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>A3C + LSTM with DOA-guided attention (UAOA, UAIA) and ABED branch fusion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The policy uses DOA edge weights to scale object features (UAOA) and to guide multi-head image attention (UAIA), concatenates object/image/action branch embeddings with ABED tokens, and inputs this to an LSTM trained with A3C. DOA is composed of an intrinsic learnable matrix (IG) and a view-adaptive graph (VAG) produced by multi-head scaled-dot attention conditioned on image features and target semantics; the final DOA = w_IG*IG + w_VAG*VAG (learned weights).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>SPL, SAE, and success rate improvements (relative % gains reported); episode length / episode time reported in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>DOA+ABED vs baseline: SR gain +5.18% (ALL) / +7.46% (L>=5); SPL gain +2.40% / +3.08%; SAE gain +2.02% / +3.86% (ALL / L>=5 respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>With DOA+ABED: SR ALL = 74.32% (baseline 69.14%); On RoboTHOR (harder dataset) Ours (DOA+ABED) SR ALL = 36.22% (baseline Ours: 31.92%).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Graph-aware, memory-augmented policy: LSTM-based policy that uses explicit directed object relations to modulate attention over objects and image regions.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Directedness and prior (intrinsic graph) are important: directed graphs encode asymmetric object-to-target relationships and improve generalization; row-normalized intrinsic matrices provide stable priors that prevent degenerate fully-adaptive solutions; sparsity of per-view adjacency (few visible objects) makes naive GCN-based adjacency estimation biased toward high-visibility objects and harms performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Ablations: IG only improves over no-graph; VAG-only (fully adaptive) can reduce performance; IG+VAG with multi-head attention (MA) and target semantics (TS) gives best results; converting DOA to undirected decreased navigation performance significantly, demonstrating the importance of directionality.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Effective policies require (1) an explicit mechanism to decouple attention from raw object feature magnitude (to avoid visibility bias), (2) a stable learned prior (intrinsic graph) combined with small-view adaptation (VAG), and (3) memory (LSTM) to accumulate contextual cues; cross-attention modules that apply DOA to object and image branches (UAOA and UAIA) materially improve decision points (rotation direction and stopping).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unbiased Directed Object Attention Graph for Object Navigation', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1246.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1246.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboTHOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboTHOR (evaluation dataset/environment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A more complex set of realistic indoor apartment-like scenes for embodied navigation evaluation; scenes are more complex than AI2-THOR and episode lengths are longer, yielding lower absolute navigation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robothor: An open simulation-to-real embodied ai platform</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>RoboTHOR</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Apartment-scale indoor navigation dataset/simulator with higher scene complexity and realism than AI2-THOR; used to validate transfer and robustness of navigation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>More complex room connectivity and larger episode graph structure compared to AI2-THOR; authors note episode length ~3x that of AI2-THOR, implying larger navigation graphs per episode.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Public 75 apartments in dataset; paper used 60 training, 5 validation and 10 testing apartments for its experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Same architecture as for AI2-THOR: A3C + LSTM with DOA-guided attention and ABED</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Identical agent architecture; DOA graph helps guide step-by-step continuous exploration in more complex scenes, reducing inefficient search.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>SR, SPL, SAE; episode length (reported higher than AI2-THOR)</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Ours (DOA+ABED) SR ALL = 36.22% (compared to baseline Ours SR ALL = 31.92%); metrics are lower than AI2-THOR due to scene complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Graph-aware, memory-augmented policy remains preferable; authors report their DOA-augmented agent approaches targets step-by-step more effectively than baseline in RoboTHOR.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>In larger/higher-diameter/longer-episode environments (RoboTHOR), DOA still improves performance, but absolute metrics are lower and episode lengths longer — indicating topology/size increases difficulty and requires more efficient exploration and stronger priors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>In complex scenes, agents benefit from stepwise local guidance from object-relation priors (DOA) and memory; longer episodes emphasize the need for efficient attention allocation to avoid wasted rotations/search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unbiased Directed Object Attention Graph for Object Navigation', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning Object Relation Graph and Tentative Policy for Visual Navigation <em>(Rating: 2)</em></li>
                <li>Hierarchical Object-to-Zone Graph for Object Navigation <em>(Rating: 2)</em></li>
                <li>Target-driven visual navigation in indoor scenes using reinforcement learning and imitation learning <em>(Rating: 1)</em></li>
                <li>AI2-THOR: An Interactive 3D Environment for Visual AI <em>(Rating: 2)</em></li>
                <li>ION: Instance-level Object Navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1246",
    "paper_id": "paper-248084847",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "AI2-THOR",
            "name_full": "AI2-THOR: An Interactive 3D Environment for Visual AI",
            "brief_description": "A simulated indoor, household 3D environment used for embodied visual navigation research where agents receive egocentric single-view RGB observations and are tasked to find specified object categories in unseen rooms.",
            "citation_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
            "mention_or_use": "use",
            "environment_name": "AI2-THOR",
            "environment_description": "Household/indoor navigation domain (kitchen, living room, bedroom, bathroom). Agents receive a single egocentric RGB view per timestep and a target object name; interaction limited to navigation and object detection; simulated moderate realism.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": "",
            "graph_connectivity": "Navigation topology is implicit in continuous room geometry; object-observation graphs are sparse (few objects visible per view) and object-relation adjacency matrices are often sparse with many zero edges.",
            "environment_size": "Dataset: 120 floorplans (30 per 4 room types). Experimental split used in paper: 80 training rooms, 20 validation rooms, 20 test rooms (20/5/5 per room type). Agent-visible object set N = 22 recognizable object categories.",
            "agent_name": "A3C + LSTM (reinforcement learning policy)",
            "agent_description": "A recurrent LSTM policy trained with asynchronous advantage actor-critic (A3C). Inputs are image, object detections and previous action branches; cross-attention modules (UAOA and UAIA) use a learned Directed Object Attention (DOA) graph to bias perception; ABED supplies branch identity/energy tokens before the LSTM.",
            "exploration_efficiency_metric": "Success weighted by Path Length (SPL), Success weighted by Action Efficiency (SAE), episode time / episode length",
            "exploration_efficiency_value": null,
            "success_rate": "Baseline (this paper): SR ALL = 69.14%; With DOA+ABED: SR ALL = 74.32% (also reported: SR L&gt;=5 = 67.88%)",
            "optimal_policy_type": "Memory-based recurrent policy (LSTM) augmented with learned object-relationship graph attention (DOA) and cross-attention between object and image modalities.",
            "topology_performance_relationship": "The paper reports that sparsity and bias in object-relation adjacency matrices cause attention to concentrate on highly visible objects (large bounding boxes, high detector confidence), degrading navigation; explicitly modeling directed object relations (DOA) and decoupling attention from raw object feature magnitudes reduces this bias and improves SR, SPL and SAE.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Within the learned object-graph family the authors compare intrinsic-only, view-adaptive-only, and combined DOA variants and directed vs undirected adjacency. Key findings: (1) intrinsic graph (IG) alone improves navigation vs no graph; (2) view-adaptive graph (VAG) alone can hurt performance (fully adaptive learning without sufficient prior is unstable); (3) the weighted combination (IG + small-weight VAG) yields best results; (4) directed DOA performs significantly better than an undirected graph.",
            "policy_structure_findings": "Policies that incorporate explicit directed object-relations and cross-attention (UAOA/UAIA) perform better; memory (LSTM) is used to aggregate multimodal history; debiasing object-attention (via DOA) is crucial — UAOA (object-branch attention guided by DOA) yields larger gains than UAIA; directed graphs support adaptive, target-conditioned attention and improve robustness in unseen environments.",
            "uuid": "e1246.0",
            "source_info": {
                "paper_title": "Unbiased Directed Object Attention Graph for Object Navigation",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "DOA",
            "name_full": "Directed Object Attention (DOA) Graph",
            "brief_description": "A learned, two-layer directed object-relation graph that explicitly models attention weights from each visible object to a target object, combining a learnable intrinsic-object graph (stable prior) and a view-adaptive graph (per-timestep attention) to mitigate object-visibility bias in object-goal navigation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "AI2-THOR (indoor object-goal navigation)",
            "environment_description": "Object-relation graph over N = 22 object categories observed by an embodied agent in AI2-THOR indoor scenes; graph nodes are object categories/detections and directed edges encode attention (importance) from observed objects toward the queried target category.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": "",
            "graph_connectivity": "Learned adjacency is sparse for per-view graphs (many zeros due to few visible objects); intrinsic-object matrix is learned across training and row-normalized (each target's incoming attention sums to 1); view-adaptive graph is extremely peaked per timestep (often one object gets weight ~1).",
            "environment_size": "Object-graph node count N = 22 (number of object categories). The DOA adjacency matrix is therefore 22 × 22.",
            "agent_name": "A3C + LSTM with DOA-guided attention (UAOA, UAIA) and ABED branch fusion",
            "agent_description": "The policy uses DOA edge weights to scale object features (UAOA) and to guide multi-head image attention (UAIA), concatenates object/image/action branch embeddings with ABED tokens, and inputs this to an LSTM trained with A3C. DOA is composed of an intrinsic learnable matrix (IG) and a view-adaptive graph (VAG) produced by multi-head scaled-dot attention conditioned on image features and target semantics; the final DOA = w_IG*IG + w_VAG*VAG (learned weights).",
            "exploration_efficiency_metric": "SPL, SAE, and success rate improvements (relative % gains reported); episode length / episode time reported in ablations",
            "exploration_efficiency_value": "DOA+ABED vs baseline: SR gain +5.18% (ALL) / +7.46% (L&gt;=5); SPL gain +2.40% / +3.08%; SAE gain +2.02% / +3.86% (ALL / L&gt;=5 respectively).",
            "success_rate": "With DOA+ABED: SR ALL = 74.32% (baseline 69.14%); On RoboTHOR (harder dataset) Ours (DOA+ABED) SR ALL = 36.22% (baseline Ours: 31.92%).",
            "optimal_policy_type": "Graph-aware, memory-augmented policy: LSTM-based policy that uses explicit directed object relations to modulate attention over objects and image regions.",
            "topology_performance_relationship": "Directedness and prior (intrinsic graph) are important: directed graphs encode asymmetric object-to-target relationships and improve generalization; row-normalized intrinsic matrices provide stable priors that prevent degenerate fully-adaptive solutions; sparsity of per-view adjacency (few visible objects) makes naive GCN-based adjacency estimation biased toward high-visibility objects and harms performance.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Ablations: IG only improves over no-graph; VAG-only (fully adaptive) can reduce performance; IG+VAG with multi-head attention (MA) and target semantics (TS) gives best results; converting DOA to undirected decreased navigation performance significantly, demonstrating the importance of directionality.",
            "policy_structure_findings": "Effective policies require (1) an explicit mechanism to decouple attention from raw object feature magnitude (to avoid visibility bias), (2) a stable learned prior (intrinsic graph) combined with small-view adaptation (VAG), and (3) memory (LSTM) to accumulate contextual cues; cross-attention modules that apply DOA to object and image branches (UAOA and UAIA) materially improve decision points (rotation direction and stopping).",
            "uuid": "e1246.1",
            "source_info": {
                "paper_title": "Unbiased Directed Object Attention Graph for Object Navigation",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "RoboTHOR",
            "name_full": "RoboTHOR (evaluation dataset/environment)",
            "brief_description": "A more complex set of realistic indoor apartment-like scenes for embodied navigation evaluation; scenes are more complex than AI2-THOR and episode lengths are longer, yielding lower absolute navigation metrics.",
            "citation_title": "Robothor: An open simulation-to-real embodied ai platform",
            "mention_or_use": "use",
            "environment_name": "RoboTHOR",
            "environment_description": "Apartment-scale indoor navigation dataset/simulator with higher scene complexity and realism than AI2-THOR; used to validate transfer and robustness of navigation methods.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": "",
            "graph_connectivity": "More complex room connectivity and larger episode graph structure compared to AI2-THOR; authors note episode length ~3x that of AI2-THOR, implying larger navigation graphs per episode.",
            "environment_size": "Public 75 apartments in dataset; paper used 60 training, 5 validation and 10 testing apartments for its experiments.",
            "agent_name": "Same architecture as for AI2-THOR: A3C + LSTM with DOA-guided attention and ABED",
            "agent_description": "Identical agent architecture; DOA graph helps guide step-by-step continuous exploration in more complex scenes, reducing inefficient search.",
            "exploration_efficiency_metric": "SR, SPL, SAE; episode length (reported higher than AI2-THOR)",
            "exploration_efficiency_value": null,
            "success_rate": "Ours (DOA+ABED) SR ALL = 36.22% (compared to baseline Ours SR ALL = 31.92%); metrics are lower than AI2-THOR due to scene complexity.",
            "optimal_policy_type": "Graph-aware, memory-augmented policy remains preferable; authors report their DOA-augmented agent approaches targets step-by-step more effectively than baseline in RoboTHOR.",
            "topology_performance_relationship": "In larger/higher-diameter/longer-episode environments (RoboTHOR), DOA still improves performance, but absolute metrics are lower and episode lengths longer — indicating topology/size increases difficulty and requires more efficient exploration and stronger priors.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "",
            "policy_structure_findings": "In complex scenes, agents benefit from stepwise local guidance from object-relation priors (DOA) and memory; longer episodes emphasize the need for efficient attention allocation to avoid wasted rotations/search.",
            "uuid": "e1246.2",
            "source_info": {
                "paper_title": "Unbiased Directed Object Attention Graph for Object Navigation",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning Object Relation Graph and Tentative Policy for Visual Navigation",
            "rating": 2,
            "sanitized_title": "learning_object_relation_graph_and_tentative_policy_for_visual_navigation"
        },
        {
            "paper_title": "Hierarchical Object-to-Zone Graph for Object Navigation",
            "rating": 2,
            "sanitized_title": "hierarchical_objecttozone_graph_for_object_navigation"
        },
        {
            "paper_title": "Target-driven visual navigation in indoor scenes using reinforcement learning and imitation learning",
            "rating": 1,
            "sanitized_title": "targetdriven_visual_navigation_in_indoor_scenes_using_reinforcement_learning_and_imitation_learning"
        },
        {
            "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
            "rating": 2,
            "sanitized_title": "ai2thor_an_interactive_3d_environment_for_visual_ai"
        },
        {
            "paper_title": "ION: Instance-level Object Navigation",
            "rating": 1,
            "sanitized_title": "ion_instancelevel_object_navigation"
        }
    ],
    "cost": 0.01493575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Unbiased Directed Object Attention Graph for Object Navigation
8 Jul 2022</p>
<p>Ronghao Dang dangronghao@tongji.edu.cn 
Zhuofan Shi zhuofanshi@student.ethz.ch 
Chengju Liu liuchengju@tongji.edu.cn 
Qijun Chen qjchen@tongji.edu.cn </p>
<p>Tongji University</p>
<p>Eidgenössische Technische Hochschule Zürich Liuyi Wang</p>
<p>Tongji University</p>
<p>Tongji University</p>
<p>Tongji University</p>
<p>Tongji University</p>
<p>Unbiased Directed Object Attention Graph for Object Navigation
8 Jul 20222CDE359AA052D3F0F49D520098121C6110.1145/XXXXXX.XXXXXXarXiv:2204.04421v2[cs.CV]Object NavigationObject Attention BiasObject Attention Graph
Object navigation tasks require agents to locate specific objects in unknown environments based on visual information.Previously, graph convolutions were used to implicitly explore the relationships between objects.However, due to differences in visibility among objects, it is easy to generate biases in object attention.Thus, in this paper, we propose a directed object attention (DOA) graph to guide the agent in explicitly learning the attention relationships between objects, thereby reducing the object attention bias.In particular, we use the DOA graph to perform unbiased adaptive object attention (UAOA) on the object features and unbiased adaptive image attention (UAIA) on the raw images, respectively.To distinguish features in different branches, a concise adaptive branch energy distribution (ABED) method is proposed.We assess our methods on the AI2-Thor dataset.Compared with the state-of-the-art (SOTA) method, our method reports 7.4%, 8.1% and 17.6% increase in success rate (SR), success weighted by path length (SPL) and success weighted by action efficiency (SAE), respectively.</p>
<p>INTRODUCTION</p>
<p>The object navigation [16,24,25,38] requires an agent to navigate in unseen environments and find a specified target by executing a sequence of actions.The agent can only use visual observation and target information as inputs and predict actions by deep reinforcement learning in each step.</p>
<p>Most prior works [20,22,23] have directly used global image features to recursively train agents based on egocentric observations.Nevertheless, if the target is invisible, it is difficult to efficiently navigate to the object position with these methods.Therefore, some recent works [6,37] have focused on establishing specific object prior knowledge to better understand complete scenes.Yang et al. [37] and Du et al. [6] used the graph convolutional networks (GCNs) to learn graph representations of object features.</p>
<p>However, the agent may not treat all items equally, preferring to focus on more conspicuous objects.Therefore, we propose the object attention bias problem, which refers to the situations in which agents focus on objects with high visibility during navigation.For example, when looking for a cell phone, an agent may focus on a closer, clearer TV while ignoring a distant, blurrier laptop that has a higher correlation with the cell phone (Figure 1).In principle, this phenomenon occurs mainly because neural networks prefer features with higher information entropy.Therefore, the direct use of graph convolutions to implicitly learn relationships between objects can lead to considerable object attention bias.</p>
<p>To address the above problem, object attention should be decoupled from the object features, so we let the agent explicitly learn the attention weights of objects according to the different targets.Concretely, we propose a learnable two-layer directed object attention (DOA) graph that represents the relationships between objects in view and the target to address the object attention bias problem.The first graph layer is an intrinsic attention graph (IG) that establishes the basic object attention relationships.The second graph layer is a view adaptive attention graph (VAG) that changes based on the observations of the agent.The DOA graph, which is produced by the weighted average of the two graph layers, can guide an agent to reasonably assign attention to each object in view.</p>
<p>Based on the DOA graph, we further design two novel crossattention modules: the unbiased adaptive image attention (UAIA) module and the unbiased adaptive object attention (UAOA) module.As illustrated in Figure 1, the target is the arrival point, and the objects in view are the starting points for the directed edges in the DOA graph.The weight of a directed edge from an object node to a target node is the object's attention while searching for this target.The UAOA module uses object attention in the DOA graph to directly distribute weights to different objects.The UAIA module uses a multihead attention mechanism to determine the areas in the In the previous biased method [6], since the nearby TV is clearer than the distant laptop, the agent focuses more on the TV, resulting in an incorrect decision.Our proposed unbiased DOA graph learns that the cell phone is more likely to be near the laptop according to the target and the current view, resulting in an correct decision.</p>
<p>global image that need attention.We follow the operation described in [6] to concatenate the image branch, object branch and previous action branch into a vector.However, just as the temporal sequence in the transformer needs positional encodings to acquire position information [31,34,41], different branches need tokens to represent their identities.In consequence, we propose an adaptive branch energy distribution (ABED) method, which allows the network to distinguish different branches with the addition of few parameters.Then, in accordance with [40], we input the concatenated features of the multimodel information into a long short-term memory (LSTM) network for learning and optimize the model with the A3C reinforcement learning strategy.</p>
<p>Extensive experiments on the AI2-Thor [13] dataset show that our method not only eliminates the object attention bias problem but also increases the state-of-the-art (SOTA) method by 7.4%, 8.1%, 17.6% in the success rate (SR), success weighted by path length (SPL) and success weighted by action efficiency (SAE) [40].Our method performs well inasmuch as the agent has a more comprehensive understanding of object relationships and an unbiased attention distribution.In summary, our contributions are as follows:</p>
<p>• We identify the prevalent object attention bias problem in object navigation, which occurs due to differences in object visibility.• We propose the directed object attention (DOA) graph, which addresses the problem of object attention bias and provides the agent with a better understanding of the internal relationships between objects.</p>
<p>• The unbiased adaptive object attention (UAOA) and unbiased adaptive image attention (UAIA) modules use the DOA graph to allocate more reasonable attention resources to object features and global image features.• We propose the parameter-free adaptive branch energy distribution (ABED) method to optimize branch feature aggregation.</p>
<p>RELATED WORKS 2.1 Object Navigation</p>
<p>In an object navigation task, an agent is given goal-oriented instruction to search for a specified object.The primitive object navigation models make decisions by directly processing input images with convolutional neural networks (CNNs).Recently, researchers [3,8,37] have found that using only CNN features in raw images cannot achieve the desired results.An increasing number of researchers have begun to use methods such as object detection to extract high-level semantic features to better guide the agent's movement.Yang et al. [37] initially use graph convolutional networks (GCNs) to learn the object prior knowledge.Gao et al. [8] utilize cross-modality knowledge reasoning (CKR) to apply an external knowledge graph in the agent's navigation.Zhang et al. [40] propose the hierarchical object-to-zone (HOZ) graph to guide an agent in a coarse-to-fine manner.In our work, we conduct the online-learning directed object attention (DOA) graph to serve as prior knowledge, which provides more unbiased object attention.</p>
<p>Debiasing Methods</p>
<p>Bias problems are widespread in machine learning [11,33], especially in the field of scene understanding [15,32].However, no previous work has analyzed and addressed the bias problem in object navigation tasks.Current debiasing methods can be roughly categorized into three types: (i) data augmentation or re-sampling [9,17,18], (ii) unbiased learning through the design of training networks and loss functions [19,39], and (iii) disentangling biased representations from unbiased representations [2,21].Our proposed DOA graph method belongs to the second category.However, unlike common debiasing methods [19,39], our method explicitly models the bias module, which essentially solves the object attention bias problem in object navigation.</p>
<p>OBJECT ATTENTION BIAS 3.1 Bias Discovered in GCNs</p>
<p>The object GCN used in [6,40] attempts to aggregate the extracted object features by using the correlations between their bounding boxes.However, it is too difficult to learn a reasonable adjacency matrix which is crucial for GCNs [42].As shown in Figure 2 (a), objects that are easy to observe, such as the floor lamp and fridge, have larger weights, while objects that are difficult to observe, such as the alarm clock and cell phone, have smaller weights.This kind of object attention bias is caused by over-focusing on coordinates and confidence scores.To reduce this bias, the agent should focus on what and where the object is rather than its size and clarity.The GCN ablation experiment, shown in Table 1, demonstrates that the GCN module only slightly improves the navigation ability of the agent, implying that a biased GCN module cannot be used to effectively and unbiasedly model relationships among objects.</p>
<p>Duality of Bias</p>
<p>We cannot criticize biased training because our visual world is inherently biased; people simply prefer to trust objects that are easier to identify.For example, when looking for a plate, we often pre-search for a cabinet instead of a knife or fork.In fact, some biased decisions allow agents to avoid some weird mistakes and make the overall actions more robust.However, excessive bias can cause an agent to overfit the training dataset, resulting in the agent ignoring critical navigation information.In general, there are two reasons for object attention bias: (i) endogenous cause (Figure 2 (a)), the network's own preference towards objects with richer visual features; (ii) exogenous cause (Figure 2 (b)), inequalities in the frequency each object are present in the dataset.This paper mainly starts from the endogenous cause without changing the number of objects in the dataset.Our proposed DOA graph corrects the agent's neglect of small and ambiguous objects (bad bias) while maintaining the agent's trust in high-confidence objects (good bias).</p>
<p>PROPOSED METHOD</p>
<p>Our goal is to propose an attention allocation strategy without object attention bias and a reasonable cross-branch aggregation method for a target-driven visual navigation system.To achieve this goal, our navigation system contains four major components, as illustrated in Figure 3  The successful episode is defined as: an agent selects the termination action  when the distance between the agent and the target is less than 1.5 meters and the target is in the field of view.</p>
<p>Directed Object Attention (DOA) Graph</p>
<p>DOA graph is a graphical representation of the correlation degree between identifiable objects and the target.According to the analysis presented in Section 3.1, previous GCN-based methods cannot learn unbiased relationships between objects in object navigation tasks.In contrast, our proposed DOA graph provides an explicit yet flexible representation of the relationships between objects.The DOA graph is obtained through a weighted summation of the intrinsic object graph and the view adaptive graph.</p>
<p>Intrinsic Object Graph.</p>
<p>The intrinsic object graph represents the ubiquitous intrinsic relationships between objects.For example, a mouse and a laptop have a strong inherent relationship.Here, we define a learnable matrix   ∈ R  × to represent the intrinsic object graph, where  is the number of objects.As the agent uses reinforcement learning to continuously explore different environments,   gradually tends to be reasonable and stable.  is fixed during testing.Each edge between objects in   is bidirectional.</p>
<p>The end node of an edge represents the target object , while the start node of the edge represents the object that needs to be assigned attention.Therefore, the weight of each directed edge represents the intrinsic correlation between an object  ∈  and the target object  ∈ .Each row of   is normalized using SoftMax to ensure that the sum of all edge values connected to a target node is 1.</p>
<p>...  .Since low-confidence object detection results often contain excessive noise, we filter   with the confidence criterion    &gt; ℎℎ to obtain   .For introducing target information to the image features   , we encode the object index using the one-hot method and two fully connected layers to obtain  .The input image query   ∈ R 1×576 can be formulated as:
𝐼 𝑡 = 𝐶𝑜𝑛𝑐𝑎𝑡 ( 1 𝑀 𝑀 ∑︁ 𝑖=1 𝐼 𝑖 𝑡 , 𝑂𝐼 𝑝 )(1)
where   refers to the -th object (target) semantics. 1
𝑀 𝑀 𝑖=1 𝐼 𝑖 𝑡
is squeezing global spatial information into a channel descriptor using global average pooling.The agent grounds the current overall environmental characteristics   to the object features   via mutihead score calculation [34] to produce the view adaptive graph   ∈ R  ×1 :
𝑄 𝑖 = 𝐼 𝑡 𝑊 𝑄 𝑖 𝐾 𝑖 = 𝑆 𝑡 𝑊 𝐾 𝑖 𝑖 = 1, 2, • • • , 𝑁 𝐻(2)
ℎ  =    (
𝑄 𝑖 𝐾 𝑇 𝑖 √ 𝐻𝐷 )(3)𝐺 𝑣 = 𝐶𝑜𝑛𝑐𝑎𝑡 ( ℎ𝑒𝑎𝑑 1 , • • • , ℎ𝑒𝑎𝑑 𝑁 𝐻 ) 𝑊 𝑂(4)
where  and   denote the hidden dimensionality and number of heads in the muti-head score calculation.   ∈ R 576×  and    ∈ R 518×  map   and   to the same dimension  .  ∈ R   ×1 aggregate the various subgraphs calculated by the scaled dot-product of the multiple heads to generate the graph   .</p>
<p>Object Attention.</p>
<p>According to the searched target , we take the edge weight    ∈ R  ×1 from the intrinsic object graph   with the -th node as the end point.With the weighted summation of the intrinsic weight and the view adaptive weight, we can obtain the attention:
𝐺 𝑡 = 𝐺 𝑝 𝑛 𝑤 𝑛 + 𝐺 𝑣 𝑤 𝑣(5)
that each object requires.The learnable   and   are the weights of the two graphs.</p>
<p>Unbiased Adaptive Attention</p>
<p>Unbiased Adaptive Object Attention (UAOA).</p>
<p>The purpose of unbiased adaptive object attention (UAOA) is to use the object attention   obtained in Section 4.2 to assign attention to different object features.To balance the information integrity and computational complexity, we apply two fully connected layers around the ReLU [26] to map the object features   to a lower-dimensional representation   ′ ∈ R  ×64 .Finally, the attention weight of each object  is multiplied by the object features   ′ :
𝑆 𝑞 𝑡 = 𝐹 𝑠𝑐𝑎𝑙𝑒 (𝑆 𝑞 𝑡 ′ , 𝐺 𝑞 𝑡 ) = 𝑆 𝑞 𝑡 ′ 𝐺 𝑞 𝑡 𝑞 = 1, 2, • • • , 𝑁(6)
where
𝑆 𝑡 ′ = {𝑆 1 𝑡 ′ , 𝑆 2 𝑡 ′ , • • • , 𝑆 𝑁 𝑡 ′ }, 𝑆 𝑞 𝑡
′ is the low dimensional feature of the -th object.   is the weight of the -th object in DOA graph at time .</p>
<p>Table 1: We obtain a strong and concise baseline by comparing the roles of different modules in previous methods [6,28,40] (%).These modules include the object GCN (GCN), the row image branch (Image), the zone branch (Zone), the room branch (Room), the previous action branch (Action) and the object branch (Object).</p>
<p>GCN Image Zone
𝐷 = 𝑁 ∑︁ 𝑞=1 I(𝑆 𝑐𝑜𝑛𝑓 𝑡 &gt; 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑)𝐺 𝑞 𝑡 × 𝑂𝐼 𝑞(7)
where I(•) is the indicator function.The confidence filter    &gt; ℎℎ indicates that only the semantics of objects whose detection confidence exceeds ℎℎ are used.Unlike the convolution operation in CNNs, the muti-head attention operation is permutation-invariant [5], which cannot leverage the order of the tokens in an input sequence.To mitigate this issue, our work adds absolute positional encodings to each pixel in the input sequence, such as in [5,34], enabling order awareness.We use standard learned 1D pixel index embedding  ∈ R ×64 since we have not observed any significant performance gains when using more complicated 2D position embeddings.The resulting sequence of embedded vectors serves as input to the encoder.
𝐼 𝑡 ′ = 𝛿 (𝛿 (𝐼 𝑡 𝑊 1 )𝑊 2 ) + 𝑃𝐼(8)
where  refers to the ReLU function,  1 ∈ R 512×128 and  2 ∈ R 128×64 reduce the dimension of the global image features   and the pixel index embedding  is introduced to generate positionaware image features   ′ ∈ R ×64 .We use the attention-aware object semantics  as the query and the position-aware image features   ′ as the key and value in the muti-head image attention ( = 64,   = 4) to generate the final image embedding   .
𝑄 𝑖 = 𝐷𝑊 𝑄 𝑖 𝐾 𝑖 = 𝐼 𝑡 ′ 𝑊 𝐾 𝑖 𝑉 𝑖 = 𝐼 𝑡 ′ 𝑊 𝑉 𝑖 𝑖 = 1, 2, • • • , 𝑁 𝐻 (9) ℎ𝑒𝑎𝑑 𝑖 = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 ( 𝑄 𝑖 𝐾 𝑇 𝑖 √ 𝐻𝐷 )𝑉 𝑖 (10)𝐼 𝑡 = 𝐶𝑜𝑛𝑐𝑎𝑡 (ℎ𝑒𝑎𝑑 1 , • • • , ℎ𝑒𝑎𝑑 𝑁 𝐻 )𝑊 𝑂(11)</p>
<p>Adaptive Branch Energy Distribution (ABED)</p>
<p>Previous works [6,40] have input the directly concatenated embedded features of the three branches (object, image and previous action branches) into LSTM networks to establish time series models.However, there are two issues with this simple feature stitching: (i) It is difficult for the network to distinguish between the different information types of the three branches during training; (ii) It is difficult to guarantee that the data distribution of the concatenated vector is rational.Therefore, we propose the adaptive branch energy distribution (ABED) method to provide additional tokens to each branch without introducing extra parameters, and optimize the data distribution of the concatenated vector.We establish a learnable vector  = { 1 ,  2 ,  3 } with only three parameters.The final output vector   can be expressed as:
𝐻 𝑡 = 𝐹 𝑝𝑤 (𝐶𝑜𝑛𝑐𝑎𝑡 (𝑟 1 𝑆 𝑡 , 𝑟 2 𝐼 𝑡 , 𝑟 3 𝑃𝐴)) (12)
where  is the previous action embedding and   refers to the pointwise convolution.The operation, which is similar to the energy distribution of the input signal, uses the significant differences between the feature distributions of the three branches to explicitly distinguish the branches and learn a more reasonable distribution of the concatenated vector.Although our method is quite simple, experiments have proven that it can significantly improve the navigation ability of the agent.When compared to the process of directly adding branch semantic (BS) embeddings to each branch, this method is unique in that it can provide the model with a strong scene understanding without destroying the other modules in complex models.</p>
<p>Policy Learning</p>
<p>Previous works [35,37] [7,20], we treat this task as a reinforcement learning problem and utilize the asynchronous advantage actor-critic (A3C) algorithm [22], which applies policy gradients to assist the agent in choosing an appropriate action   in the high-dimensional action space .In accordance with the done reminder operation presented in [40], we use the target detection confidence to explicitly enhance the probability of the  action.</p>
<p>EXPERIMENTS 5.1 Experimental Setup</p>
<p>5.1.1Dataset.We choose the AI2-Thor [13] dataset and its corresponding simulator as the experimental platform.The AI2-Thor dataset includes 30 different floorplans for each of 4 room layouts: kitchen, living room, bedroom, and bathroom.For each scene type, we use 20 rooms for training, 5 rooms for validation, and 5 rooms for testing.There are 22 kinds of objects ( = 22) that agents can recognize, and we ensure that there are at least 4 kinds of objects in each room [35].</p>
<p>Evaluation Metrics.</p>
<p>We use the success rate (SR), success weighted by path length (SPL) [1], and success weighted by action efficiency (SAE) [40] metrics to evaluate our method.SR indicates the success rate of the agent in completing the task, which is formulated as  = 1   =1   , where  is the number of episodes and   indicates whether the -th episode succeeds.SPL considers the path length more comprehensively and is de-
fined as 𝑆𝑃𝐿 = 1 𝐸 𝐸 𝑖=1 𝑆𝑢𝑐 𝑖 𝐿 * 𝑖 𝑚𝑎𝑥 (𝐿 𝑖 ,𝐿 * 𝑖 )
, where   is the path length taken by the agent and  *  is the theoretical shortest path.SAE considers the effects of unnecessary rotations and is defined as
𝑆𝐴𝐸 = 1 𝐸 𝐸 𝑖=1 𝑆𝑢𝑐 𝑖 𝑇 𝑡 =0 I(𝑎 𝑖 𝑡 ∈𝐴 𝑐ℎ𝑎𝑛𝑔𝑒 ) 𝑇 𝑡 =0 I(𝑎 𝑖 𝑡 ∈𝐴 𝑎𝑙𝑙 )
, where I(•) is the indicator function,    is the agent's action at time  in episode ,   is the set of all actions, and  ℎ is the set of actions that can change the position of the agent.[12] is used to update the network parameters with a learning rate of 10 −4 .We introduce a dropout of 0.3 to the muti-head attention mechanism and global image embedding.The confidence threshold for filtering objects is set to 0.6.Faster-RCNN [29] is fine-tuned on 50% [40] of the training data from the AI2-Thor dataset.For evaluation, our results take the average value of the test for 3 times.We report the results for all targets (ALL) and for a subset of targets (L&gt;=5) with optimal trajectory lengths greater than 5.</p>
<p>Strong and Concise Baseline</p>
<p>The methods presented in [6,28,40] include five kinds of branches (image, zone, room, previous action and object) that are concatenated into a vector before being input into the LSTM network.To evaluate the influence of the five branches separately on the object navigation task, we eliminate each branch in turn, as shown in Table 1.The object branch has the greatest impact on the experimental results, and the removal of the object branch drops 20.27/24.27,17.62/19.38,8.40/11.02 in SR, SPL and SAE (ALL/L&gt;=5, %).This confirms the importance of object features in object navigation task.In this perspective, the adequate exploration of object relationships is necessary.Moreover, the image and previous action branches also have significant impacts on the agent's navigation ability.Whereas, the room branch and the zone branch have little effect on the SR and SPL.Accordingly, our baseline retains only the image, object and previous action branches.</p>
<p>The last row in Table 1 shows our simplified baseline.The removal of the object GCN, room branch and zone branch reduces the agent's exploration time by 27.2% while leaving the SR, SPL and SAE essentially unchanged.This more concise baseline allows us to observe the advantages of the added modules more clearly.</p>
<p>Ablation Experiments</p>
<p>We verify the effectiveness of each proposed module with extensive experiments.Table 2, Table 3, Table 4 and Table 5 show the results of ablation experiments on the UAIA, DOA graph, ABED and overall model.More ablation experiments are provided in the Supplementary Material.%).However, the DOA graph with only VAG brings a blow to the agent's navigation ability.This result implies that it is difficult to directly perform fully adaptive learning in object relationships, requiring the extensive prior knowledge to narrow the learning domain.During the calculation of VAG, the muti-head attention (MA) allows for more reasonable object graph relationships, while the use of target semantics (TS) allows the agent to be more specific about the target, thereby improving the navigation efficiency.</p>
<p>UAIA. As shown in</p>
<p>ABED.</p>
<p>In Table 4, We compare two methods of providing identities for the branches: branch semantics (BS), which use the embedding of one-hot vectors; energy distribution (ED), which uses only three energy distribution coefficients.Without cross-attention (UAIA and UAOA), adding BS and ED to the original model improves the agent's navigation ability well.Notably, with cross-attention (UAIA and UAOA), adding BS to the complex model causes the model to crash.In contrast, the simple ED method improves the complex model with cross-attention by 0.88/0.87,0.72/1.02and 0.67/2.01 in SR, SPL and SAE (ALL/L &gt;= 5, %).The results demonstrate the significant advantage of the ED method with only three parameters in complex models.This is consistent with our intuition that due to the complexity of object navigation tasks, the learning model must be simplified; otherwise, the strong coupling of the complex parameters between modules can cause the overall model to be difficult to learn.The results indicate that our method is capable of effectively guiding agents to navigate in unseen environments.Compared with the UAIA and ABED methods, the UAOA method improves the model more significantly.This is because the UAOA method essentially solves the object attention bias problem, increasing the agent's understanding of the relationships between objects.</p>
<p>Overall</p>
<p>Elimination of Object Attention Bias</p>
<p>Object attention bias is the phenomenon in which objects with low visibility are ignored.Figure 4 (a) shows the average attention of the agent on all objects before and after using the DOA graph for all test floorplans.Without the use of our DOA graph approach, the agent suffers from severe long-tail distribution in the object attention.</p>
<p>The average attention difference between the most popular object and the most neglected object is more than tenfold.Objects with high visibility, such as the floor lamp, fridge and sink, dominate the agent's decision-making, while objects with low visibility, such as the cell phone and remote control, cannot play their proper guiding roles.Figure 4 (b) shows that although the DOA method makes the average attention for each object to be similar across the entire dataset, there is a correct attention tendency in different scenes.</p>
<p>Our DOA graph-based attention consists of two parts (section 4.2) : intrinsic attention and adaptive attention.The agent adjusts the two-part attention in different scenes to pay more attention on critical objects.In summary, the proposed DOA graph significantly improves the rationality of attention allocation for most objects, indicating that the improvement in SR, SPL and SAE is indeed from solving the object attention bias problem.We emphasize that DOA   graph is a model-agnostic object attention representation method that can be applied to a variety of models and fusion modules.</p>
<p>Comparisons to the State-of-the-art</p>
<p>As shown in Table 6, we compare the test results of our method and other similar methods on the AI2-Thor dataset.All of the random decision navigation indicators are close to 0. Notably, our baseline model outperforms the SOTA method by 0.61/0.15,0.37/0.67 and 1.97/1.02 in SR, SPL and SAE (ALL/L &gt;= 5, %).The redundant operations in previous networks aggravate the object attention bias, which explains why the subtraction in our baseline model facilitates learning.Finally, our model with DOA graph-based modules and ABED method outperforms the proposed baseline with the gains of 5.18/7.46,2.40/3.08 and 2.02/3.86 in SR, SPL and SAE (ALL/L &gt;= 5, %).</p>
<p>Qualitative Analysis</p>
<p>We visualize the navigation effect of the agent in Figure 5.The direction and stop timing of the rotation are critical, as seen in the trajectories of the success and failure cases.These two decisions are mainly determined by the agent's interpretation of the scene at keyframes when multiple objects can provide rich information.Our DOA graph-based method provides the agent with a more reasonable and unbiased attention allocation at these keyframes, allowing the algorithm to choose the correct rotation direction and stop timing.</p>
<p>CONCLUSION</p>
<p>Based on the analysis of the network structure and the failed navigation of previous methods, we identify the agent's object attention bias problem in navigation tasks.To address this problem, we use a directed object attention (DOA) graph, which allows the agent to unbiasedly redistribute object attention.Cross-attention modules (UAIA and UAOA) between the object branch and image branch are designed according to the DOA graph.Our experimental results show that our approach can effectively address the object attention bias problem, greatly improving the agent's navigation ability.Furthermore, we propose an adaptive branch energy distribution (ABED) method for optimizing the aggregation of branch features that performs well in complex models.It is worth noting that we prioritize simplicity in our approach.In future work, we will attempt to determine more concrete relationships between objects so that the agent's navigation can be more clearly interpreted.</p>
<p>Table 7: Goal-driven navigation datasets.The attributes include: Instruction: type of instruction; Observation: views that the agent can use; Scenes: number of different scenes in the simulation; Real-Picture: pictures taken in real environments; Interaction: whether the agent moves or changes the object state; Navigation Graph: the presumptions of the known environmental topologies; and Realistic: resemblance to real-world environments.</p>
<p>Dataset</p>
<p>Instruction Observation Scenes Real-Picture Interaction Navigation Graph Realistic House3D (2018) [36] room name single view 45622 weak Habitat (2019) [30] object name single view 90 ✓ moderate AI2-Thor (2019) [13] object name single view 120 ✓ moderate RoboTHOR (2020) [4] object name single view 89 strong REVERIE (2020) [27] task language panoramas 90 ✓ ✓ weak FAO (2021) [43] locate language panoramas 90 ✓ ✓ weak</p>
<p>A MORE ANALYSES ON DATASETS</p>
<p>Table 7 summarizes the most commonly used datasets for object navigation tasks.We compare these datasets in terms of the instructions given, the views used, the variety and realism of the scenes, the ability of the agent to interact, the presence or absence of a predefined navigation map, and the ability to map the environment to reality.Since our method is based on the input of object names and single views, in addition to using the AI2-Thor dataset discussed in the main text, we also conduct experiments on the RoboTHOR dataset.We discover the object attention bias problem during our research on object features GCN, as discussed in [6,40].In the main text, we briefly analyzed this problem.Here, we elaborate on why GCNs have issues with object attention bias.As shown in Figure 6, the object GCN is divided into two stages: (i) Generating the adjacency matrix, and (ii) Transferring information between objects using the adjacency matrix.The object attention bias problem arises in both stages.</p>
<p>B OBJECT ATTENTION BIAS</p>
<p>B.1 Bias in Generating the Adjacency Matrix</p>
<p>The adjacency matrix is generated based on the relationships between the bounding boxes of objects, which defines the correlation between objects.However, after training, the network determines the importance of an object based on the size of its bounding box and its confidence score, thereby ignoring the category and position of the object, which are both more important features.In Table 8, we explicitly add the object semantics and position to the GCN node features of the HOZ [40] method and find that the agent's navigation performance improves slightly.The results show that it is important for the agent to pay more attention to what the object is and where the objects are; however, this feature introduction cannot effectively resolve the object attention bias generated during training the GCN.Furthermore, since the agent usually can view only a few objects at a time, the object features graph is sparse and thus more susceptible to bias. Figure 7 shows the weights on the edge connecting some specified objects and other objects in the adjacency matrix when searching for different targets in the same bedroom scene.Due to the sparsity of the objects, most edge weights are 0. It is worth noting that regardless of what object is being searched for, the floor lamp has a high connection weight with the other objects.This illustrates that the volume and clarity of the floor lamp cause the agent to overestimate its importance in the bedroom scene.</p>
<p>B.2 Bias in Convolutional Object Visual Features</p>
<p>The convolution of object visual features with a biased adjacency matrix clearly leads to biased results.However, in this section, we discuss that even if the object bias in the adjacency matrix is not considered, the convolution of the object visual features can also generate this bias.Figure 8 shows the original images of the objects and their information entropy calculated by the k-nearest neighbor (KNN) method [14].As a result, even if an object is recognized, it is likely to be ignored due to the disadvantage of its feature information richness.</p>
<p>C.2 Importance of Accurate Object Information</p>
<p>In our model, we apply Faster-RCNN [29], which was trained on 50% [40] of the training data from the AI2-Thor dataset, to extract the object information.Table 9 shows the results when the ground truth object information is used as the input of the object branch, improving the SR, SPL and SAE by 1.80/1.19,18.85/14.57and 1.19/2.59(ALL/L &gt;=5, %).Similar to the conclusion presented in [40], the ground truth object information has a strong effect on the optimization of navigation path.Notably, our DOA graph method significantly increases the upper bound of the model, and when more accurate and efficient features are used, our model performs better.</p>
<p>C.3 Pixel Index Embedding</p>
<p>In the UAIA module, the pixel index embeddings provide each pixel's position token, which is important in the muti-head attention mechanism.In Table 9, we compare the results of the following encodings:</p>
<p>1) Providing no positional information (No): Considering the image feature maps as a bag of pixel vectors.2) 1-dimensional pixel index embedding (Our Best Model): Considering the image feature maps as a sequence of pixel vectors in order.</p>
<p>3) 2-dimensional pixel index embedding (2-D): Considering the image feature maps as a grid of pixel vectors in two dimensions.4) Relative pixel index embedding (Relative): Considering the relative distance between pixels rather than their absolute position to encode the spatial information.</p>
<p>The results show that without pixel index embeddings, the SPL is reduced by approximately 2%.However, when using 1-D, 2-D or relative pixel index embeddings, the effect is essentially the same.We speculate that this occurs because we perform position embeddings on feature maps with smaller spatial dimensions instead of  on the original image, which has larger spatial dimensions.Hence, the process of learning the spatial relations at this resolution is equally easy for the different positional encoding strategies.</p>
<p>C.4 Directivity in DOA Graph</p>
<p>In the main text, we repeatedly emphasize that our proposed DOA graph is a directed graph; thus, the correlation between object B and object A when searching for object A is different from the correlation between object A and object B when searching for object B. Table 9 illustrates that the navigation improvement decreases significantly when the DOA graph is an undirected graph.Fundamentally, the directed object graph follows the relative relationships between objects, while the undirected object graph follows the absolute relationships between objects.The absolute object relationships reduce the adaptability of the agent in modeling object relationships in different scenes, thus reducing its performance in unseen environments.</p>
<p>C.5 Hyperparameters</p>
<p>The hyperparameters are also an important factor in determining the final navigation performance of the agent.However, due to the complexity of the object navigation task, small changes in the network structure have a considerable impact on the selection of the hyperparameters.Therefore, as shown in Figure 9, we conduct extensive experiments on the final model with different dropout rates and confidence filter thresholds.The experimental results show that when a hyperparameter changes, the change rules of the three metrics (the SR, SPL and SAE) differ.Thus, we consider all three metrics when choosing the most appropriate hyperparameters for our model.</p>
<p>D MORE COMPARISONS WITH THE RELATED WORKS D.1 Comparisons in RoboTHOR Dataset</p>
<p>We compare our method with the other related works on the AI2-Thor dataset in the main text.We consider that the scenes in AI2-Thor are relatively simple, so the RoboTHOR [4] dataset is used to verify the effectiveness of our method in complex environment.RoboTHOR has public 75 apartments, we choose 60 for training, 5 for validation and 10 for testing.</p>
<p>Table 10 illustrates the performance of our method and other related methods on the RoboTHOR.Obviously, the various metrics on the RoboTHOR are generally low, which indicates that object navigation on RoboTHOR is much more difficult than AI2-Thor.Moreover, The episode length of navigation on RoboTHOR is 3 times that of AI2-Thor, which indicates that each navigation scene in RoboTHOR is much more complicated.Nevertheless, out method still outperforms the proposed baseline with an obvious margin by 4.3/5.44,3.24/2.76and 2.33/4,03 in SR, SPL and SAE (ALL/L&gt;=5, %).</p>
<p>E QUALITATIVE RESULTS E.1 The DOA Graph Visualization</p>
<p>Our proposed DOA graph has two parts: the intrinsic object graph, which represents the ubiquitous inherent relationship between the objects, and the view adaptive graph, which changes adaptively according to the input at each timestep.A weighted sum of the two graphs gives the attention score assigned to each object at this timestep.Figure 10 shows the adjacency matrix of the two graphs.For the adjacency matrix of the intrinsic object graph, the largest value is on the diagonal, indicating that self-connecting edge has the largest weight.This result is consistent with our assumption that when the target object is identified, most of the attention is focused on the target object.For the adjacency matrix of the view adaptive graph, we randomly select the view adaptive attention when looking for different targets as a column.Interestingly, at each timestamp in the view adaptive graph, only one object's attention weight is close to 1, while the other object's attention weight is close to 0. We speculate that because there are often very few objects observed at each timestamp and most of these objects are essentially irrelevant to the target object, the view adaptive graph directs all attention to the most important object.In the weighted sum of the two graphs, the weight of the intrinsic object graph is 0.95, while the weight of the view adaptive graph is only 0.05.Therefore, although the attention distribution of the view adaptive graph is extreme, it does not affect the robustness of the overall attention distribution.In contrast, the attention concentration in the view adaptive graph reasonably increases the weight of important objects in view.</p>
<p>E.2 Training Process</p>
<p>Figure 12 shows the agent's navigation success rate (SR) on the training and validation datasets during reinforcement learning.We observe that after the agent has learned 0.5 M episodes, the navigation success rate greatly improved, and the SR curve converged after 2.5 M episodes.Due to computational resource constraints, we train our models on 3 M navigation episodes in our usual experiments.To obtain the best model of the current network structure, we can increase the number of training episodes to 6 M.</p>
<p>E.3 Navigation Trajectory in RoboTHOR</p>
<p>Since the AI2-Thor dataset only includes simple room constructions, we also use the RoboTHOR dataset, which includes more complex indoor environments, to verify the effectiveness of our model.As shown in Figure 11, in the RoboTHOR environment, the agent spends more time searching for the target object.Compared with the baseline, the agent can approach the target step-by-step in a continuous exploration with our proposed DOA graph.</p>
<p>Figure 1 :
1
Figure 1:In the previous biased method[6], since the nearby TV is clearer than the distant laptop, the agent focuses more on the TV, resulting in an incorrect decision.Our proposed unbiased DOA graph learns that the cell phone is more likely to be near the laptop according to the target and the current view, resulting in an correct decision.</p>
<dl>
<dt>Figure 2 :</dt>
<dt>2</dt>
<dt>Figure 2: Two potential reasons for object attention bias: (a) endogenous cause: unreasonable GCN adjacency matrix [6]; (b) exogenous cause: the total number of times each object is identified shows a long-tailed distribution.</dt>
<dd>
<p>(i) directed object attention (DOA) graph generator; (ii) unbiased adaptive object attention (UAOA) module; (iii) unbiased adaptive image attention (UAIA) module; (iv) adaptive branch energy distribution (ABED) method.(ii) and (iii) are based on the object attention in the DOA graph.</p>
</dd>
</dl>
<p>Figure 3 :
3
Figure 3: Model overview.PI: pixel index embedding, TS: target semantics, OS: object semantics, CF: confidence filter.VAG: view adaptive graph, IG: intrinsic graph, Avg Pool: average pooling.Our model consists of three branches: Image branch, Object branch, and Action branch.We perform UAIA on Image branch and UAOA on Object branch, respectively, based on directed object attention (DOA) graph.The joint features of the re-integrated branches after adaptive branch energy distribution (ABED) are input into an LSTM network to predict the next action.</p>
<ol>
<li>2 . 2
22
View Adaptive Graph.The view adaptive graph represents the real-time adaptive relationships between objects.The agent generates the global image features   ∈ R ×512 (from ResNet18 [10]) and object features   ∈ R  ×518 (from Faster-RCNN [29]) after observing the current scene.Here,  is the pixel number of the image feature map.  is concatenated by the object visual features    , object position features    , confidence    and target indicator   </li>
</ol>
<p>Figure 4 :
4
Figure 4: (a) Compare the object average attention of biased HOZ [40] and our unbiased DOA method.(b) Compare the object average attention (intrinsic attention + adaptive attention) of different scenes in DOA graph.</p>
<p>Figure 5 :
5
Figure 5: Visualization in testing environment.We show the results of experiments that involve searching for four different objects in four different scenes.The trajectory of the agent is indicated by green and blue arrows, where green is the beginning and blue is the end.The red value in the object detection box represents the attention weight of the object.</p>
<p>Figure 6 :
6
Figure6: The object feature embedding method commonly used in previous works[6,40].A local visual representation graph (containing the position, confidence, and target) is embedded in the adjacency matrix with a GCN.The adjacency matrix is used to fuse object features with strong correlations in the local appearance features graph.</p>
<p>Figure 7 :
7
Figure 7: Visualization of object attention bias in object GCN.A_B represents the weight of object B and all 22 objects in the adjacency matrix while searching for target A.</p>
<p>Figure 8 :
8
Figure 8: The image and entropy of different objects in different environments.</p>
<p>Figure 9 :
9
Figure 9: Experiments to explore the most reasonable hyperparameters, such as the dropout rate and the threshold of confidence filter.</p>
<p>Figure 10 :
10
Figure 10: Learned adjacency matrices of the intrinsic object graph and view adaptive graph.</p>
<p>Figure 11 :Figure 12 :
1112
Figure 11: Visualization of the navigation trajectory with the RoboTHOR dataset.The trajectory of the agent is indicated by the green and blue arrows, with the green indicating the beginning and the blue indicating the end of the trajectory.The red value in the object detection box represents the attention weight of the object.</p>
<p>,   ,  ℎ } in a random house.Here, (, ) and (  ,  ℎ ) represent the coordinates and angles of the agent.After the state and target are initialized, the agent begins to navigate based on its own observations.At each timestamp , the agent only receives the RGB image   from a single perspective and the target  ∈ .According to   and , the agent learns a navigation strategy  (  |  , ), where   ∈  = {ℎ;   ; ℎ; ;  ; } and  is the output if the agent believes it has navigated to the target location.
4.1 Task DefinitionInitially, the agent is given a random target from a group of 𝑁objects 𝑃 = {𝑃𝑎𝑛, • • • , 𝐶𝑒𝑙𝑙𝑝ℎ𝑜𝑛𝑒}, and starts from a random state𝑠 = {𝑥,</p>
<p>32.3237.50±1.01 25.98 ±0.96 60.27 ±1.67 36.61±1.12 27.68 ±1.31 ±1.61 37.80 ±0.87 26.07 ±1.26 58.15 ±1.19 36.30±0.97 27.55 ±0.88 ±1.37 32.04 ±1.69 26.72 ±0.87 54.38 ±0.56 30.38 ±1.27 26.52 ±1.21 ±0.97 37.81 ±1.13 25.75 ±1.04 58.31 ±1.22 36.69 ±0.87 27.83 ±1.36 ±1.16 37.60 ±1.32 26.11 ±1.88 59.21 ±1.45 36.62 ±0.99 28.35 ±1.17 ±1.74 34.60 ±1.14 25.49 ±1.09 54.68 ±0.67 33.01 ±0.93 25.81 ±1.12 ±1.11 19.88 ±1.31 17.58 ±1.5735.80 ±1.12 17.23 ±1.65 16.66 ±1.18 ±0.67 37.87 ±0.88 27.77 ±0.95 60.42 ±1.11 37.28 ±0.66 28.70 ±0.67 0.174 4.3.2Unbiased Adaptive Image Attention (UAIA).We use the DOA graph to focus more attention on the areas around important objects in the image.We use the encoded object index  rather than word embeddings trained by other networks to represent the object semantic information (what the object is).The object index embeddings learned by our network are more coupled to our dataset than word embeddings trained on other tasks.We use the object attention   to generate the attention-aware object semantics :
Room Action ObjectSRAll SPLSAESRL&gt;=5 SPLSAEEpisode Time✓✓✓✓✓✓68.53 0.239✓✓✓✓✓68.11 0.177✓✓✓✓✓63.95 0.145✓✓✓✓✓67.51 0.223✓✓✓✓✓67.61 0.235✓✓✓✓✓64.46 0.243✓✓✓✓✓48.26 0.160✓✓✓69.14</p>
<p>have used direct observations to learn a strategy  (  |  , ).Our work uses unbiased object graph relationships to learn an LSTM action policy  (  |  , ), where   is the joint representation of the global image embedding  1   , object embedding  2   and previous action embedding  3 .Based on previous works</p>
<p>Table 2 :
2
The ablation study of the different components of the UAIA module (%).CF: confidence filter, PI: pixel index.
UAIAALLL&gt;=5CF PISRSPLSAESRSPLSAE70.48 37.13 27.63 63.12 37.09 29.16✓72.12 37.45 27.98 64.23 37.67 29.13✓ 71.12 38.91 27.23 64.01 39.34 30.33✓ ✓ 72.28 39.13 28.91 65.19 39.98 30.22</p>
<p>Table 3 :
3
The ablation study of the different modules in the DOA graph (%).IG: intrinsic graph, VAG: view adaptive graph, MA: multi-head attention, TS: target semantics.'--' indicates that the view adaptive graph is not used.
IGVAG MA TSSRALL SPLSAESRL&gt;=5 SPLSAE70.77 38.51 27.98 63.83 38.04 28.74✓--73.67 39.98 30.23 65.79 39.23 31.98✓72.12 39.29 28.10 65.55 39.21 30.91✓✓73.91 39.21 30.21 67.78 39.34 31.89✓✓ 67.43 36.29 27.01 60.12 35.90 26.88✓✓✓ 74.32 40.27 29.79 67.88 40.36 32.56</p>
<p>Table 4 :
4
The ablation study of different branch fusion methods in ABED (%).CA: cross-attention, BS: branch samentics, ED: energy distribution method.
CAABED BS EDSRALL SPLSAESRL&gt;=5 SPLSAE69.14 37.87 27.77 60.42 37.28 28.70✓71.07 40.02 26.30 64.04 39.50 29.57✓ 71.56 38.66 28.99 64.12 38.89 29.0773.44 39.55 29.12 67.01 39.34 30.55✓✓67.82 33.50 27.74 61.63 34.23 29.10✓ 74.32 40.27 29.79 67.88 40.36 32.56</p>
<p>Table 5 :
5
The ablation study of the different components in overall model (%).
UAOA UAIA ABEDSRALL SPLSAESRL&gt;=5 SPLSAE69.14 37.87 27.77 60.42 37.28 28.70✓73.21 39.20 29.29 66.37 39.12 31.18✓72.28 39.13 28.91 65.19 39.98 30.22✓70.77 38.51 27.98 63.83 38.04 28.74✓✓73.58 39.11 29.46 67.42 39.04 31.74✓✓✓74.32 40.27 29.79 67.88 40.36 32.56
5.1.3Implementation Details.We train our model with 18 workers on 2 RTX 2080Ti Nvidia GPUs.The Adam optimizer</p>
<p>Table 2
2The DOA graph with only IG outperforms the DOA graph withoutIG by 2.90/1.96, 1.47/1.19 and 2.25/3.24 in SR, SPL and SAE (ALL/L&gt;= 5,
, the UAIA module has two main components: the confidence filter (CF), which is used to eliminate outlier objects; the pixel index (PI) embedding, which is used to increase the order-awareness of the global image features.The UAIA module with CF (confidence &gt; 0.6) outperforms the UAIA module without CF by 1.64/1.11 in SR (ALL/L &gt;= 5, %).This result shows that reducing the influence of irrelevant objects in the UAIA module can effectively improve the navigation ability.The UAIA module with PI embedding outperforms the UAIA module without PI embedding by 1.78/2.25 in SPL (ALL/L &gt;= 5, %), demonstrating that adding positional encoding to each pixel in the image can optimize the agent's path.The two components complement each other to improve the effect of the UAIA module.5.3.2DOAGraph.As shown in Table3, we explore the role of intrinsic graph (IG) and view adaptive graph (VAG) in DOA graph.</p>
<p>Table 6 :
6MethodSRALL SPLSAESRL&gt;=5 SPLSAERandom4.122,210.430.210.080.05SP [37]62.98 38.56 24.99 52.73 33.84 23.02SAVN [35]63.12 37.81 20.98 52.01 34.94 23.01ORG [6]67.32 37.01 25.17 58.13 35,90 27.04HOZ [40]68.53 37.50 25.98 60.27 36.61 27.68Ours (Baseline)69.14 37.87 27.77 60.42 37.28 28.70Ours (DOA+ABED) 74.32 40.27 29.79 67.88 40.36 32.56
Comparison with SOTA methods (%).More experiments are in the appendix D.1.</p>
<p>Table 8 :
8
Comparison of introducing object semantics and position into the GCN node features (%).
GCNSemanticsPositionSRALL SPLSAESRL&gt;=5 SPLSAEEpisode Length✓68.53 ±1.3237.50 ±1.0125.98 ±0.9660.27 ±1.6736.61 ±1.1227.68 ±1.3127.25 ±0.87✓✓69.45 ±0.7637.36 ±0.9927.65 ±1.3161.02 ±1.0937.15 ±0.3229.30 ±1.7826.02 ±0.86✓✓69.12 ±0.9038.19 ±0.5627.23 ±0.8860.76 ±1.7837.94 ±1.1229.01 ±1.3425.33 ±1.24AlarmclockBookTelevisionToasterObject ImageEntropy-0.612-0.7810.065-0.141</p>
<p>Table 9 :
9
Navigation performance after replacing some modules in the model (%).Our Best Model represents the model used in our main text.±0.89 40.27 ±1.01 29.79 ±0.80 67.88 ±1.05 40.36 ±1.23 32.56 ±1.76 22.86 ±1.32 ±1.23 35.98 ±1.56 28.76 ±0.90 66.77 ±1.12 33.50 ±1.27 32.34 ±1.52 27.96 ±0.88 ResNet101 70.21 ±1.45 35.56 ±1.07 26.77 ±1.34 65.43 ±0.81 31.45 ±1.23 29.70 ±0.99 28.21 ±1.29 Object Detector Ground Truth 76.12 ±1.48 59.12 ±1.10 30.98 ±1.27 69.07 ±0.65 54.93 ±1.35 35.15 ±0.93 17.45 ±1.05 ±1.20 38.14 ±0.82 29.13 ±1.03 67.76 ±1.38 38.55 ±1.07 32.67 ±1.21 24.21 ±0.94 2-D 74.29 ±1.22 40.54 ±0.79 29.38 ±1.62 67.81 ±1.09 40.35 ±1.67 32.29 ±1.24 22.59 ±0.93 Relative 74.10 ±0.85 40.34 ±1.07 29.33 ±1.55 67.45 ±1.38 40.19 ±1.43 31.98 ±1.33 23.45 ±0.74 DOA Graph Undirected 73.15 ±1.17 38.44 ±1.26 27.43 ±1.63 66.33 ±1.44 37.60 ±1.10 30.95 ±1.28 26.84 ±1.36
Module ReplacementSRALL SPLSAESRL&gt;=5 SPLSAEEpisode LengthOur Best Model 74.32 Backbone ResNet50 74.12 Pixel Index Emb. No 74.54 Intrinsic Object GraphView Adaptive Graph</p>
<p>Table 10 :
10
Comparision with SOTA methods in RoboTHOR [4] (%) Ours (DOA+ABED) 36.22 ±0.36 22.12 ±0.42 18.70 ±0.59 30.16 ±0.28 18.32 ±0.34 19.21 ±0.52 61.24 ±1.23
MethodSRALL SPLSAESRL&gt;=5 SPLSAEEpisode LengthRandom0.00 ±0.000.00 ±0.000.00 ±0.000.00 ±0.000.00 ±0.000.00 ±0.003.01 ±0.32LSTM+A3C21.35 ±0.239.28 ±0.547.03 ±0.3411.21 ±0.417.65 ±0.266.33 ±0.8687.29 ±1.95SP [37]27.43 ±0.6017.49 ±0.3615.21 ±0.3320.98 ±0.9716.03 ±0.4813.99 ±0.4468.18 ±1.27SAVN [35]28.97 ±0.8416.59 ±0.6513.21 ±0.3722.89 ±0.9815.21 ±0.4113.77 ±0.3067.22 ±0.93ORG [6]30.51 ±0.2118.62 ±0.1914.71 ±0.4723.89 ±0.5514.91 ±0.3813.73 ±0.5069.17 ±2.13HOZ [40]31.67 ±0.8719.02 ±0.4915.44 ±0.6324.32 ±0.4814.81 ±0.2314.22 ±0.8866.26 ±1.88Ours (Baseline)31.92 ±0.3718.88 ±0.1816.37 ±0.3324.72 ±0.2615.56 ±0.3515.18 ±0.6165.31 ±1.69
ACKNOWLEDGMENTSThis paper is supported by the National Natural Science Foundation of China under Grants 61733013, 62173248, 62073245.Suzhou Key Industry Technological Innovation-Core Technology R&amp;D Program, No. SGC2021035.
Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, arXiv:1807.06757On evaluation of embodied navigation agents. 2018. 2018arXiv preprint</p>
<p>Rubi: Reducing unimodal biases for visual question answering. Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, Advances in neural information processing systems. 322019. 2019</p>
<p>Object goal navigation using goal-oriented semantic exploration. Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, Russ R Salakhutdinov, Advances in Neural Information Processing Systems. 332020. 2020</p>
<p>Robothor: An open simulation-to-real embodied ai platform. Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha Kembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi Salvador, Dustin Schwenk, Eli Vanderbilt, Matthew Wallingford, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. Austria2021. May 3-7, 2021</p>
<p>Learning Object Relation Graph and Tentative Policy for Visual Navigation. Heming Du, Xin Yu, Liang Zheng, Computer Vision -ECCV 2020: 16th European Conference. Glasgow, UK; Glasgow, United Kingdom2020. August 23-28, 2020Proceedings, Part VII</p>
<p>Target-driven visual navigation in indoor scenes using reinforcement learning and imitation learning. Qiang Fang, Xin Xu, Xitong Wang, Yujun Zeng, CAAI Transactions on Intelligence Technology. 2021. 2021</p>
<p>Room-and-object aware knowledge reasoning for remote embodied referring expression. Chen Gao, Jinyu Chen, Si Liu, Luting Wang, Qiong Zhang, Qi Wu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, Wieland Brendel, International Conference on Learning Representations. 2019</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Undoing the Damage of Dataset Bias. Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A Efros, Antonio Torralba, Computer Vision -ECCV 2012. Berlin Heidelberg; Berlin, HeidelbergSpringer2012</p>
<p>Adam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, ICLR 20153rd International Conference on Learning Representations. Yann Bengio, Lecun, San Diego, CA, USA2015. May 7-9, 2015Conference Track Proceedings</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, AI2-THOR: An Interactive 3D Environment for Visual AI. 2017. 2017</p>
<p>Sample estimate of the entropy of a random vector. F Lyudmyla, Nikolai N Kozachenko, Leonenko, Problemy Peredachi Informatsii. 231987. 1987</p>
<p>Bipartite graph network with adaptive message passing for unbiased scene graph generation. Rongjie Li, Songyang Zhang, Bo Wan, Xuming He, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>ION: Instance-level Object Navigation. Weijie Li, Xinhang Song, Yubing Bai, Sixian Zhang, Shuqiang Jiang, Proceedings of the 29th ACM International Conference on Multimedia. the 29th ACM International Conference on Multimedia2021</p>
<p>Resound: Towards action recognition without representation bias. Yingwei Li, Yi Li, Nuno Vasconcelos, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2018</p>
<p>Repair: Removing representation bias by dataset resampling. Yi Li, Nuno Vasconcelos, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017Kaiming He, and Piotr Dollár</p>
<p>Learning to navigate in complex environments. Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, 5th International Conference on Learning Representations. 2017</p>
<p>Seeing through the human reporting bias: Visual classifiers from noisy humancentric labels. Ishan Misra, Lawrence Zitnick, Margaret Mitchell, Ross Girshick, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2016</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International conference on machine learning. PMLR2016</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, nature. 5182015. 2015</p>
<p>ForeSI: Success-Aware Visual Navigation Agent. Mahdi Kazemi Moghaddam, Ehsan Abbasnejad, Qi Wu, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2022Javen Qinfeng Shi, and Anton Van Den Hengel</p>
<p>SOAT: A Scene-and Object-Aware Transformer for Vision-and-Language Navigation. Abhinav Moudgil, Arjun Majumdar, Harsh Agrawal, Stefan Lee, Dhruv Batra, Advances in Neural Information Processing Systems. 342021. 2021</p>
<p>Rectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, 2010In Icml</p>
<p>Reverie: Remote embodied visual referring expression in real indoor environments. Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, Anton Van Den, Hengel, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Target driven visual navigation exploiting object relationships. Yiding Qiu, Anwesan Pal, Henrik I Christensen, arXiv:2003.067492020. 202027arXiv preprint</p>
<p>Faster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, Advances in neural information processing systems. 282015. 2015</p>
<p>Habitat: A platform for embodied ai research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, Yunfeng Liu, arXiv:2104.09864Roformer: Enhanced transformer with rotary position embedding. 2021. 2021arXiv preprint</p>
<p>Unbiased scene graph generation from biased training. Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, Hanwang Zhang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Unbiased look at dataset bias. Antonio Torralba, Alexei A Efros, CVPR 2011. IEEE2011</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 2017. 201730</p>
<p>Learning to learn how to learn: Self-adaptive visual navigation using meta-learning. Mitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Building generalizable agents with a realistic and rich 3d environment. Yi Wu, Yuxin Wu, Georgia Gkioxari, Yuandong Tian, arXiv:1801.022092018. 2018arXiv preprint</p>
<p>Visual Semantic Navigation using Scene Priors. Wei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, Roozbeh Mottaghi, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USA2019. May 6-9, 2019</p>
<p>Abhishek Das, and Erik Wijmans. 2021. Auxiliary tasks and exploration enable objectgoal navigation. Joel Ye, Dhruv Batra, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision</p>
<p>Learning fair representations. Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, Cynthia Dwork, International conference on machine learning. PMLR2013</p>
<p>Hierarchical Object-to-Zone Graph for Object Navigation. Sixian Zhang, Xinhang Song, Yubing Bai, Weijie Li, Yakui Chu, Shuqiang Jiang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Informer: Beyond efficient transformer for long sequence time-series forecasting. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang, Proceedings of AAAI. AAAI2021</p>
<p>A weighted GCN with logical adjacency matrix for relation extraction. Li Zhou, Tingyu Wang, Hong Qu, Li Huang, Yuguo Liu, ECAI 2020. IOS Press2020</p>
<p>SOON: scenario oriented object navigation with graph-based exploration. Fengda Zhu, Xiwen Liang, Yi Zhu, Qizhi Yu, Xiaojun Chang, Xiaodan Liang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7324 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7324</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7324</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-273346168</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.10289v2.pdf" target="_blank">Fine-grained Abnormality Prompt Learning for Zero-shot Anomaly Detection</a></p>
                <p><strong>Paper Abstract:</strong> Current zero-shot anomaly detection (ZSAD) methods show remarkable success in prompting large pre-trained vision-language models to detect anomalies in a target dataset without using any dataset-specific training or demonstration. However, these methods often focus on crafting/learning prompts that capture only coarse-grained semantics of abnormality, e.g., high-level semantics like"damaged","imperfect", or"defective"objects. They therefore have limited capability in recognizing diverse abnormality details that deviate from these general abnormal patterns in various ways. To address this limitation, we propose FAPrompt, a novel framework designed to learn Fine-grained Abnormality Prompts for accurate ZSAD. To this end, a novel Compound Abnormality Prompt learning (CAP) module is introduced in FAPrompt to learn a set of complementary, decomposed abnormality prompts, where abnormality prompts are enforced to model diverse abnormal patterns derived from the same normality semantic. On the other hand, the fine-grained abnormality patterns can be different from one dataset to another. To enhance the cross-dataset generalization, another novel module, namely Data-dependent Abnormality Prior learning (DAP), is introduced in FAPrompt to learn a sample-wise abnormality prior from abnormal features of each test image to dynamically adapt the abnormality prompts to individual test images. Comprehensive experiments on 19 real-world datasets, covering both industrial defects and medical anomalies, demonstrate that FAPrompt substantially outperforms state-of-the-art methods in both image- and pixel-level ZSAD tasks. Code is available at https://github.com/mala-lab/FAPrompt.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7324",
    "paper_id": "paper-273346168",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00466325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Fine-grained Abnormality Prompt Learning for Zero-shot Anomaly Detection</p>
<p>Jiawen Zhu 
Singapore Management University
Singapore</p>
<p>Yew-Soon Ong 
Nanyang Technological University
Singapore</p>
<p>Chunhua Shen 
Zhejiang University
China</p>
<p>Guansong Pang 
Singapore Management University
Singapore</p>
<p>Fine-grained Abnormality Prompt Learning for Zero-shot Anomaly Detection
FD4057A77CD5DA35795C71A4A1EEDF6E
Current zero-shot anomaly detection (ZSAD) methods show remarkable success in prompting large pre-trained visionlanguage models to detect anomalies in a target dataset without using any dataset-specific training or demonstration.However, these methods often focus on crafting/learning prompts that capture only coarse-grained semantics of abnormality, e.g., high-level semantics like 'damaged', 'imperfect', or 'defective' objects.They therefore have limited capability in recognizing diverse abnormality details that deviate from these general abnormal patterns in various ways.To address this limitation, we propose FAPrompt, a novel framework designed to learn Fine-grained Abnormality Prompts for accurate ZSAD.To this end, a novel Compound Abnormality Prompt learning (CAP) module is introduced in FAPrompt to learn a set of complementary, decomposed abnormality prompts, where abnormality prompts are enforced to model diverse abnormal patterns derived from the same normality semantic.On the other hand, the fine-grained abnormality patterns can be different from one dataset to another.To enhance the cross-dataset generalization, another novel module, namely Data-dependent Abnormality Prior learning (DAP), is introduced in FAPrompt to learn a samplewise abnormality prior from abnormal features of each test image to dynamically adapt the abnormality prompts to individual test images.Comprehensive experiments on 19 real-world datasets, covering both industrial defects and medical anomalies, demonstrate that FAPrompt substantially outperforms state-of-the-art methods in both imageand pixel-level ZSAD tasks.Code is available at https: // github.com/mala-lab/ FAPrompt.</p>
<p>Introduction</p>
<p>Anomaly Detection (AD) is a critical task in computer vision, aiming to identify instances that deviate significantly from the majority of data.It has a wide range of real-world * Corresponding author: Guansong Pang (gspang@smu.edu.sg)[35], with the maps generated based on the similarities between text prompt embeddings and image embeddings, as defined in Eq. 3.</p>
<p>applications, e.g., industrial inspection and medical imaging analysis [8,37].Traditional AD methods often rely on application-specific, carefully curated datasets to train a specialized detection model, making them inapplicable for application scenarios where such data access is not possible due to data privacy issue, or where the test data significantly differs from the training set due to substantial distribution shifts arising from new deployment environments or other natural variations in datasets.To address this, Zeroshot Anomaly Detection (ZSAD) has emerged as a promising solution, enabling models to detect anomalies in unseen datasets without dataset-specific training.</p>
<p>In recent years, large pre-trained vision-language models (VLMs) such as CLIP [39] have demonstrated impressive zero/few-shot recognition capabilities across a broad range of vision tasks.To leverage VLMs for ZSAD, existing methods [11,17,20,25,66] use craft/learn text prompts to extract the normal/abnormal textual semantics from VLMs for matching visual anomalies.Handcrafted text prompting methods such as WinCLIP [25] and AnoVL [17] attempt to capture a range of abnormality semantics for better ZSAD by using a wide variety of pre-defined textual description templates as text prompts.Others [20,[64][65][66] arXiv:2410.10289v2[cs.CV] 2 Oct 2025 employ two learnable text prompts to extract more generalpurpose features for distinguishing between normal and abnormal classes, such as AnomalyCLIP [66] (see Fig. 1 topleft).</p>
<p>However, existing methods often design/learn prompts taking inspiration from image classification, where the abnormality prompts are rudimentary when directly applied to the AD task, leading to a coarse-grained differentiation between normal and abnormal semantics, e.g., high-level semantic of a 'damaged', 'imperfect', or 'defective' carpet.Such prompt designs overlook the intrinsic relationships between normality and abnormality.In practice, abnormalities often manifest as subtle deviations from normal patterns rather than entirely distinct semantic categories, e.g., color stains on normal texture of carpet.As a result, the previous methods are prone to learn excessively simple and homogeneous abnormal patterns, as illustrated in Fig. 1 bottom (1), making it difficult to model diverse distributions of abnormal samples, thereby constraining their generalization across different datasets.</p>
<p>To tackle these issues, we propose a novel framework, namely FAPrompt, designed to learn Fine-grained Abnormality Prompts for accurate ZSAD.In contrast to previous prompting methods, FAPrompt focuses on learning the prompts that can model diverse, fine-grained abnormality semantics.To this end, in FAPrompt, we introduce a Compound Abnormality Prompt learning (CAP) module, which learns a set of complementary, decomposed abnormality prompts on top of a normal prompt, as illustrated in Fig. 1 top-right.This compound prompting method ensures that all abnormality prompts are semantically adherent to the normality captured in the normal prompt while introducing learnable deviations to capture fine-grained abnormalities.To enhance diversity and minimize redundancy among the abnormality prompts, an orthogonal constraint is further introduced to enforce that each abnormality prompt captures a unique aspect of abnormality while maintaining strong discriminability from the normal class.</p>
<p>On the other hand, most existing methods fail to leverage important prior information hidden in each query/test image, limiting the generalizability of text prompts and their adaptability to unseen datasets.CoCoOp [64] attempts to address this issue by incorporating instance-conditional information from image token embeddings.However, its design is ineffective for AD due to two reasons.i) It focuses on class-level semantic difference which can incorporate only coarse-grained semantics into the prompt learning.ii) It can introduce noise into abnormality prompts when the input images are normal samples since they lack abnormality-related information, reducing the discriminability of the learned abnormality prompts (see Table 4).</p>
<p>To overcome this challenge, we propose a Datadependent Abnormality Prior learning (DAP) module.It learns to derive abnormality features from the most anomalous patches of each query/test image as a sample-wise abnormality prior to dynamically adapt the abnormality prompts in CAP to individual input images.To avoid using noise (normal features) as the abnormality prior, we impose an abnormality prior learning loss to exclude features from the normal training samples.</p>
<p>The joint force of CAP and DAP enables FAPrompt to learn adherent yet diverse fine-grained semantics, as shown in Fig. 1 bottom (3).This effect cannot be achieved by simple ensemble methods that learn a set of abnormality prompts based on different base models, as they can learn redundant prompts, as shown in Fig. 1</p>
<p>Related Work</p>
<p>Conventional Anomaly Detection</p>
<p>There have been different types of AD approaches introduced over the years [8,37,51,68].In particular, one-class classification methods [3,12,41,47,59] aim to compactly describe normal data using support vectors.Reconstruction-based methods [1,24,34,38,43,[55][56][57][58][60][61][62] train models to reconstruct normal images, with anomalies identified through higher reconstruction errors.Distance-based methods [13,14,36,40] detect anomalies by measuring the distance between the test image and normal patterns.Knowledge distillation methods [5,7,16,42,48,49,63] focus on distilling normal patterns from pre-trained models and detecting anomalies by comparing discrepancies between the distilled and original features.However, these methods often rely on application-specific datasets to train the detection model, limiting their applica-bility in real-world scenarios where data access is restricted due to privacy/resource issues, or where the distribution of test data significant differs from that of the training data.</p>
<p>Zero-Shot Anomaly Detection</p>
<p>ZSAD has been made possible due to the development of large pre-trained foundation models, such as visionlanguage models (VLMs).CLIP [39] has been widely used as a VLM to enable ZSAD on visual data [9,10,17,25,66].CLIP-AC adapts CLIP for ZSAD by using text prompts designed for the ImageNet dataset as in [39].By using manually defined textual prompts specifically designed for industrial AD dataset, WinCLIP [25] achieves better ZSAD performance compared to CLIP-AC, but it often does not generalize well to non-defect AD datasets.APRIL-GAN [10] adapts CLIP to ZSAD through tuning some additional linear layers with annotated auxiliary AD data.AnoVL [17] introduces domain-aware textual prompts and test time adaptation in CLIP to enhance the ZSAD performance.AnomalyCLIP [66], AdaCLIP [9] and FiLO [20] employ learnable textual/visual prompts to extract more general-purpose features for the normal and abnormal classes.All these methods are focused on crafting/learning prompts that capture only coarse-grained semantics of abnormality, failing to detect anomalies that exhibit different patterns from these coarse abnormal patterns.There are a number of other studies leveraging CLIP for AD, but they are designed for empowering few-shot [19,32,67] or conventional AD task [28,[52][53][54].</p>
<p>Methodology</p>
<p>Preliminaries</p>
<p>Problem Statement.Let D train = {X train , Y train } denote an auxiliary training dataset consisting of both normal and anomalous samples, where X train = {x i } N i=1 is a set of N images and Y train = {y i , G i } N i=1 contains the corresponding ground truth labels and pixel-level anomaly masks.Each image x i is labeled by y i , where y i = 0 indicates a normal image and y i = 1 signifies an anomalous one.The anomaly mask G i provides pixel-level annotation of x i .During the testing phase, we are presented with a collection of target datasets,
T = {D 1 test , D 2 test , • • • , D t test }, where each D j test = {X j test , Y j
test } is a test set from a target application dataset that have different normal and abnormal samples from those in the training data D train .The goal of ZSAD is to develop models on the auxiliary dataset D train , with the ability to generalize to detect anomalies in different test sets in T .Particularly, given an input RGB image x ∈ R h×w×3 from D train , with h and w respectively representing the height and width of x, a ZSAD model is required to output both an image-level anomaly score s x ∈ R and a pixel-level anomaly map M x ∈ R h×w .The image-level anomaly score s x provides a global assessment of whether the image is anomalous, while the pixel-level anomaly map M x indicates the likelihood of each pixel being anomalous.Both s x and the values in M x lie in [0, 1], where a larger value indicates a higher probability of being abnormal.VLM Backbone.To enable accurate ZSAD, large pretrained VLMs are typically required.Following existing approaches [9,17,25,66], we select CLIP [39] in our study, which comprises a visual encoder f v (•) and a text encoder f t (•), where the visual and text features are well-aligned through pre-training on web-scaled text-image pairs.</p>
<p>Overview of FAPrompt</p>
<p>In this work, we propose a ZSAD framework FAPrompt to learn fine-grained abnormality semantics without any reliance on detailed human annotations or text descriptions.To be more specific, the proposed CAP module is devised to specify the design of fine-grained abnormality prompts.The key characteristic of CAP is to obtain the abnormality prompts via a compound prompting method, where we have one normal prompt and multiple abnormality prompts are added on top of it.These normal and abnormal text prompts are then processed by the CLIP's text encoder f t (•) to generate the corresponding normal and abnormal text embeddings, respectively.For a given image x, FAPrompt extracts both an image token embedding f v (x) and a set of patch token embeddings F v ∈ R l×d , with l and d respectively representing the length and dimensionality of F v .The prompts are then learned using D train based on the similarity between the visual and text embeddings, where the fine-grained abnormality prompts are aggregated into an abnormality prompt prototype before its use in similarity calculation.</p>
<p>Further, the DAP module is introduced to improve crossdataset generalization capability of the fine-grained abnormality prompts.DAP derives the most relevant abnormality features from a given query/test image x, serving as a sample-wise abnormality prior to dynamically adapt the abnormality prompts in CAP to individual samples of a given target dataset.During training, the original parameters of CLIP remain frozen, and only the attached learnable tokens in the text encoder layers, along with the normal and fine-grained abnormality prompts, are optimized.Below we present these modules in detail.</p>
<p>Compound Abnormality Prompt Learning</p>
<p>Learning Fine-grained Abnormalities via Compound Normal and Abnormal Tokens.Previous approaches that rely on coarse-grained learnable text prompts fail to capture the fine-grained abnormality semantics for detecting diverse anomalies across different datasets.we propose the novel CAP module.The core insight is that abnormal samples typically exhibit different magnitude of deviation from their normal counterparts while still being adherent to the normal class.CAP models this by learning a set of complementary, decomposed abnormality prompts built on a shared normal prompt.To be specific, a set of learnable normal tokens and the fixed token 'object' are concatenated to define the normal text prompt P n .For the abnormality prompt, CAP aims to learn a small set of prompts of complementary semantics, denoted as P a = {P a1 , P a2 , ...P a K }, where each P ai is formed by a compound of the same tokens in the normal prompt P n and a few learnable abnormal tokens.Formally, the normal and abnormality prompts can be defined as:
P n = [V 1 ][V 2 ]...[V E ][object],
P a = {P a1 , P a2 , ...P a K },
with P ai = [V 1 ][V 2 ]...[V E ][A i 1 ][A i 2 ]...[A i E ′ ][object],(1)
where
{V 1 , V 2 , ...V E } and {A i 1 , A i 2 , ...A i E ′ } K i=1
are learnable normal and abnormal tokens, respectively.This compound prompting strategy enables the learning of different abnormality semantics easily while maintaining abnormality prompts being adherent to the normality in the normal prompt, supporting the learning of semantically-meaningful abnormality prompts.Learning Complementary Abnormality Prompts.To capture complementary fine-grained abnormalities and reduce redundant information captured by the abnormality prompts, it is essential to maximize the diversity among the fine-grained abnormalities.A straightforward approach would be to train distinct abnormality prompts on separate, annotated subsets with samples from different anomalous types.However, this would require extensive human annotations.To address this issue, we propose to add an orthogonal constraint loss L oc into the abnormality prompts in CAP as a alternative method to encourage this diversity.Formally, the objective for this can be formulated as:
L oc = i,j∈K;i̸ =j abs( f t (P ai ) • f t (P aj ) ||f t (P ai )|| × ||f t (P aj )|| ),(2)
where the text encoder f t (•) is used to extract the embeddings of the abnormality prompts, [•] denotes inner product, abs(•) returns the absolute value, and || • || indicates the norm of vectors.To provide more representative embedding for the fine-grained abnormalities, we compute the prototype of the multiple abnormality prompt embeddings as the final fine-grained abnormality embedding via
F a = 1 |P a | P a i ∈P a f t (P ai ).
The normal text prompt embedding is F n = f t (P n ).</p>
<p>Data-dependent Abnormality Prior Learning</p>
<p>One issue in ZSAD is that the fine-grained abnormality patterns can be very different from the auxiliary dataset to test datasets.In addition to the learning of a set of complementary fine-grained abnormality prompts, it is important to ensure that the learned fine-grained abnormality patterns are generalized to various test datasets.Inspired by the instance-conditional information design in CoCoOp [64], we introduce the DAP module to enhance the cross-dataset generalizability of the abnormal tokens in CAP by adaptively selecting the embeddings of the most abnormal regions to serve as a sample-wise abnormality prior for each image input.Particularly, given a query/test image x, DAP selects the most abnormal image patches as the abnormality prior to be fed into CAP for assisting the abnormality prompt learning.It achieves this by picking the top M patches whose token embeddings are most similar to the abnormality prompt prototype F a :
S a x (i, j) = exp(F v (i, j)F ⊺ a ) exp(F v (i, j)F ⊺ n ) + exp(F v (i, j)F ⊺ a ) ,(3)
where [•] ⊺ denotes a transpose operation, F v (i, j) is the token embedding of the patch centered at (i, j) and S a x (i, j) is a patch-level anomaly score.The corresponding normal scores can be calculated via S n</p>
<p>x (i, j) using the similarity to F n in the numerator in Eq. 3.</p>
<p>Let p x = {p 1 , p 2 , ...p M } be the top M patch embeddings of x, FAPrompt then adds additional learnable layers ψ(•), namely abnormality prior network, to learn the sample-wise abnormality prior based on p x .This prior Ω x = ψ(p x ) is then incorporated as data-dependent abnormal features into the learnable abnormal tokens of the abnormality prompts in CAP to dynamically adapt the finegrained abnormalities to a given target dataset, with each individual abnormality prompt refined as follows:
Pai = [V 1 ]...[V E ][A i 1 ⊕ Ω x ]...[A i E ′ ⊕ Ω x ][object],(4)
where Ω x is a vector-based prior of the same dimensionality as the abnormal tokens and ⊕ denotes elementwise addition.Thus, the abnormality prompt set is updated as Pa = { Pa1 , Pa2 , ... Pa K }, and the abnormality prompt prototype can be accordingly refined as Fa =
1 | Pa | Pa i ∈ Pa f t ( Pai ).
The goal of DAP is to introduce sample-wise abnormality information.However, there is no abnormality information from the top M patches of normal images, and thus, simply applying the prior Ω x to normal images would introduce noise into the learnable abnormal tokens, damaging the learning of fine-grained abnormalities.To address this issue, we propose an abnormality prior learning loss L prior to enforce that Ω x is the features mapped from the most abnormal M patches if x is an abnormal image, while it is minimized to be a null vector if it is a normal image.Formally, L prior can be defined as follows:
L prior = yx=0 ω∈Ωx ω 2 x ,(5)
where ω is an entry of Ω x .</p>
<p>Training and Inference</p>
<p>Training.During training, FAPrompt first generates an abnormality-oriented segmentation map Ma ∈ R h×w using Ŝa x whose entries are calculated via Eq. 3 with F a replaced by the prior-enabled Fa :
Ma = Φ( Ŝa x ),(6)
where Φ(•) is a reshape and interpolation function that transforms the patch-level anomaly scores into a twodimensional segmentation map.In the same way, we can generate the segmentation map Mn = Φ( Ŝn x ) based on the prior-enabled normal score Ŝn</p>
<p>x .Let G x represent the ground-truth mask of the query image x, following previous works [10,66], the learning objective in FAPrompt for optimizing pixel-level AD can then be defined as:
L local = 1 N x∈Xtrain L F ocal ([ Mn , Ma ], G x ) + L Dice ( Ma , G x ) + L Dice ( Mn , I − G x ),(7)
where I is a full-one matrix, L F ocal (•) and L Dice (•) denote a focal loss [33] and a dice loss [31], respectively.To ensure the accuracy of locating the top abnormal features in DAP, we apply the same learning objective to optimize the segmentation maps M n ∈ R h×w and M a ∈ R h×w , which are derived from the normality-oriented scores S n x and abnormality-oriented scores S a x , respectively.For image-level supervision, FAPrompt first computes the probability of the query image x being classified as abnormal based on its cosine similarity to the two prompt embeddings Fa and F n :
s a (x) = exp(f v (x) F⊺ a ) exp(f v (x)F ⊺ n ) + exp(f v (x) F⊺ a )
.</p>
<p>The final image-level anomaly score is then defined as the average of this image-level score and the maximum pixel-level anomaly score derived from the anomaly score maps:
s(x) = 1 2 (s a (x) + s ′ a (x)),(9)
where s ′ a (x) = 1 2 max(S a x ) + max( Ŝa x ) represents the average of the maximum anomaly scores from S a</p>
<p>x and Ŝa x .Following previous methods [10,25,66,67], s ′ a (x) is treated as a complementary anomaly score to s a (x) and incorporated into Eq.9, as s ′ a (x) are helpful for detecting local abnormal regions.The image-level anomaly score s(x) is then optimized by minimizing the following loss on X train :
L global = 1 N x∈Xtrain L b (s(x), y x ),(10)
where L b is specified by a focal loss function due to the class imbalance in X train .Overall, FAPrompt is optimized by minimizing the following combined loss, which integrates both local and global objectives, along with the two constraints from the CAP and DAP modules: Inference.During inference, given a test image x ′ , it is fed through the FAPrompt framework to generate the segmentation maps M n , M a , Mn , and Ma .Then the pixel-level anomaly map M x ′ is calculated by averaging over these segmentation maps as follows:
M x ′ = 1 4 (M a ⊕ 1 ⊖ M n ⊕ Ma ⊕ 1 ⊖ Mn ), (11)
where ⊖ is element-wise subtraction.The image-level anomaly score s x ′ is computed using Eq. 9.</p>
<p>Experiments</p>
<p>Datasets.To verify the effectiveness of FAPrompt, we conduct extensive experiments across 19 publicly available datasets, including nine popular industrial defect inspection datasets on varying products/objects (MVTecAD [4], VisA [69], DAGM [50], DTD-Synthetic [2], AITEX [44], SDD [45], BTAD [35], MPDD [26], and ELPV [15]) and ten medical anomaly detection datasets on different organs like brain, fundus, colon, skin and thyroid (BrainMRI [42],</p>
<p>HeadCT [42], LAG [30], Br35H [22], CVC-ColonDB [46], CVC-ClinicDB [6], Kvasir [27], Endo [23], ISIC [21], TN3K [18]).To assess the ZSAD performance, the models are trained on the MVTecAD dataset by default and evaluated on the test sets of other datasets without any further training or fine-tuning.We obtain the ZSAD results on MVTecAD by changing the training data to the VisA dataset (see Appendix A for details about the datasets).</p>
<p>Competing Methods and Evaluation Metrics.We compare our FAPrompt with several state-of-the-art (SotA) methods, including five handcrafted text prompt-based methods (raw CLIP [39], CLIP-AC, WinCLIP [25], APRIL-GAN [10], and AnoVL [17]) and five learnable text prompt-based methods (CoOp [65], CoCoOp [64], Anoma-lyCLIP [66]), FiLO [20], and BLIP [29]).As for evaluation metrics, we follow previous works [25,66] and use two popular metrics: Area Under the Receiver Operating Characteristic (AUROC) and Average Precision (AP) to assess the image-level AD performance; for pixel-level AD performance, we employ AUROC and Area Under Per Region Overlap (PRO) to provide a more detailed analysis.We also provide model complexity analysis and detailed qualitative results in Appendix C.1 and C.5, respectively.Implementation Details.The implementation details for FAPrompt and the competing methods are provided in Appendix B.</p>
<p>Main Results</p>
<p>Image-level ZSAD Performance.Table 1 presents the image-level ZSAD results of FAPrompt, compared to ten SotA methods across 13 AD datasets, including nine industrial and four medical AD datasets.The results show that FAPrompt significantly outperforms the SotA models across almost all datasets.On average, compared to the best competing methods, it achieves up to 3.9% AU-ROC and 3.0% AP on industrial AD datasets and 2.3% AU-ROC and 1.3% AP on medical AD datasets.In particular, the weak performance of CLIP and CLIP-AC can be attributed to its over-simplified text prompt design.By utilizing more carefully designed handcrafted prompts, WinCLIP achieves better results than CLIP and CLIP-AC while preserving the training-free nature.APRIL-GAN and AnoVL improve over WinCLIP by using additional learnable layers and/or domain-aware tokens within the textual prompts.However, they heavily rely on sensitive handcrafted textual prompts and capture mainly coarse-grained semantics of abnormality, leading to poor performance on anomalies outside their seen domains (e.g., BTAD, MPDD, BrainMRI, HeadCT, Br35H).</p>
<p>As for learnable prompt methods, BLIP pretrained on a smaller scale of data, so it is less effective as in CLIP-related method.CoOp and CoCoOp are designed for general vision tasks, i.e., discriminating different objects, so they have weak capability in capturing the subtle differences between normality and abnormality on the same object.Anomaly-CLIP and FiLO significantly improves ZSAD performance by learning textual prompts.However, their prompt design either fails to capture fine-grained abnormality details or relies heavily on external tools (LLMs and Grounding DINO).They also ignore crucial abnormality information that can be derived directly from the query input.FAPrompt over-Table 2. Pixel-level ZSAD results (AUROC, PRO) on 14 AD datasets.The best and second-best results are respectively highlighted in red and blue.Note that medical datasets in Table 1 do not have pixel-level ground truth.Thus, different medical datasets are used here.Detailed breakdown results for MVTecAD, VisA, DAGM, DTD-Synthetic, BTAD, and MPDD can be found in Appendix D.1.comes these limitations via the novel CAP and DAP.Pixel-level ZSAD Performance.We also compare the pixel-level ZSAD results of our FAPrompt with SotA methods across 14 AD datasets in Table 2. Similar observations can be derived as the image-level results.In particular, CLIP and CLIP-AC are the weakest among the handcrafted text prompt-based methods, primarily due to inappropriate text prompt designs.With better prompt engineering (and adaptation to AD in some cases), WinCLIP, APRIL-GAN, and AnoVL demonstrate better performance.</p>
<p>For the learnable text prompt approaches, CoOp shows poor performance due to overfitting on the adaptation dataset, whereas CoCoOp mitigates this by introducing instance-conditional information, achieving substantial improvement over CoOp and competitive performance to BLIP, AnomalyCLIP and FiLO.FAPrompt exhibits superior performance in identifying a wide range of pixel-level anomalies, significantly outperforming SotA models across nearly all datasets, particularly in complex scenarios with larger sample sizes or requiring extensive knowledge transfer, such as DAGM and medical datasets.It surpasses the best competing methods by up to 1.4% AUROC and 4.0% AP on the industrial AD datasets, and by 3.2% AUROC and 4.2% AP on the medical AD datasets.This demonstrates the effectiveness of the fine-grained abnormality prompts in FAPrompt that adaptively capture detailed abnormality semantics in different datasets.</p>
<p>Analysis of FAPrompt</p>
<p>Module Ablation.We conduct ablation study to assess the contribution of two modules of FAPrompt and report the averaged results across 18 industrial and medical datasets in Table 3. AnomalyCLIP serves as our base model (Baseline), to which we separately add each of our proposed modules: '+ CAP' and '+ DAP'.</p>
<p>As shown, applying CAP alone results in a significant improvement in image-level ZSAD performance due to its ability of learning the fine-grained abnormality details.To assess how important the orthogonal constraint loss (L oc ) is  in CAP, we further evaluate the performance with L oc removed, denoting as '+ CAP w\o L oc '.The results indicate that L oc helps CAP work in a more effective way, justifying its effectiveness in encouraging the learning of complementary fine-grained abnormal patterns.</p>
<p>When DAP is applied independently, it results in substantial improvements in both image-and pixel-level performance, with a more pronounced effect on medical datasets.This can be attributed to its ability of deriving datadependent abnormality information from any target data to enhance the cross-dataset generalization of FAPrompt.Similarly, we assess the role of the abnormality prior selection loss (L prior ) by removing it from DAP ('DAP w\o L prior ').The results show that removing L prior may introduce noisy priors from normal samples, leading to a significant performance drop.When all components are applied, the CAP and DAP complement each other in capturing high-quality abnormality semantics, resulting in the best ZSAD performance.</p>
<p>Significance of Sample-wise Abnormality Prior in DAP.</p>
<p>To evaluate the effectiveness of our sample-wise abnormality prior in DAP module, we further conduct an ablation study in which we modified the basis of the learned prior by substituting the selected abnormal patch token embeddings (P) with image token embeddings (I), as employed in CoCoOp.The results are presented in Table 4.As shown, CoCoOP yields poor performance in both image-and pixellevel ZSAD tasks.This is primarily due to that the learned    iting the model's ability to detect the full spectrum of abnormalities.Although 'Weighted Prompts' partially mitigates this issue by preserving complementary information through weighted aggregation, its performance still falls short of that achieved by simple averaging.This is due to that the weight learning network can overfit the auxiliary training dataset in the zero-shot setting.Thus, we adopt the straightforward averaging strategy in FAPrompt, as it effectively integrates diverse abnormality cues while preserving their complementary information.</p>
<p>Hyperparameter Sensitivity Analysis.We analyze the sensitivity of two key hyperparameters of FAPrompt on industrial datasets in terms of image-and pixel-level AU-ROC results in Fig. 3, including the number of abnormality prompts K in CAP and the number of selected patch tokens M in DAP.In particular, the performance gets improved with increasing K, typically peaking at K = 10.</p>
<p>The performance may slightly declines when K is chosen beyond 10.This suggests that while increasing the number of prompts helps capture a wider range of abnormalities, too large K values may introduce noise or redundancy into the prompts.As for the number of selected tokens, M , the performance exhibits a similar pattern, with the best performance obtained at M = 10.Beyond this point, a slight decline in performance is observed.This suggests that selecting too many abnormal patch candidates may introduce noise or less relevant patches into learned prior, leading to the learning of less effective fine-grained abnormalities (more comprehensive sensitivity analysis can be found in Appendix C.4).</p>
<p>Conclusion</p>
<p>In</p>
<p>Figure 1 .
1
Figure 1.Top: FAPrompt vs. previous methods.Bottom: 2-D t-SNE visualization of the prompt-wise anomaly score maps of three methods on samples from a BTAD dataset[35], with the maps generated based on the similarities between text prompt embeddings and image embeddings, as defined in Eq. 3.</p>
<p>Fig 2 illustrates the overall framework of FAPrompt that consists of two novel modules, including Compound Abnormality Prompt learning (CAP) module and Datadependent Abnormality Prior learning (DAP) module.</p>
<p>Figure 2 .
2
Figure 2. Overview of FAPrompt.It consists of two novel modules: the Compound Abnormality Prompt learning (CAP) (bottom-right) and the Data-dependent Abnormality Prior learning (DAP) (top-right).CAP is devised to learn fine-grained abnormality semantics without relying on detailed human annotations or text descriptions, while DAP is used to adaptively select the most abnormal features from each query/test image as a sample-wise abnormality prior to enhance the cross-dataset generalizability of the abnormality prompts in CAP.</p>
<p>Figure 3 .
3
Figure 3. Image-level (Left) and Pixel-level (Right) AUROC results based on different value of K and M .</p>
<p>32)tom(2).Additional results and more t-SNE visualization with ensemble methods can be found in Sec.4.2 and Appendix C.3.Accordingly, we make the following main contributions.• We propose a novel ZSAD framework FAPrompt.
Un-like existing methods that capture coarse-grained seman-tics of abnormality only, FAPrompt offers an effectiveapproach for learning diverse fine-grained abnormalitysemantics.• To achieve this, we introduce two novel modules -CAP and DAP in FAPrompt. CAP learns a small setof complementary, decomposed fine-grained abnormalityprompts via a compounding normal-abnormal token de-sign and an orthogonal constraint, whereas DAP aims toenhance cross-dataset generalization by learning sample-wise abnormality prior from the most abnormal featuresof abnormal training samples while refraining the priorbeing affected by normal training images.• Comprehensive experiments on 19 diverse real-worldindustrial and medical image AD datasets show thatFAPrompt significantly outperforms state-of-the-art(SotA) ZSAD models by at least 3%-5% improvement inboth image-and pixel-level detection tasks.</p>
<p>Text Encoder Image Encoder Compound Abnormality Prompt Learning (CAP) Compound Abnormality Prompt Learning (CAP) Data-dependent Abnormality Prior Learning (DAP) ... ...</p>
<p>To address this,
Inner ProductElement-wise AdditionAbnormal ImageMMaximum ValueUpdateData-dependent Abnormality Prior Learning (DAP)Top M Abnormal PatchesAbnormalityPriorNetworkNormal Text Prompt......Removing Abnormality-...Layer 1...Layer 2...Layer NNormal Imageirrelevant FeaturesAuxiliary ImageUpdateTextMEncoder...Enforcing AbnormalityPrototypeAdherenceEnforcing DiverseAbnormalityNormal Text EmbeddingAbnormal Text EmbeddingImage Token EmbeddingPatch Token EmbeddingsSample-wise Abnormality Prior
...</p>
<p>Table 1 .
1
Image-level ZSAD results (AUROC, AP) on 13 AD datasets.The best and second-best results are respectively highlighted in red and blue.The results for MVTecAD, VisA, DAGM, DTD-Synthetic, BTAD, and MPDD are averaged performance across their multiple data subsets (see Appendix D.1 for breakdown results).
Data TypeDatasetCLIPHandcrafted Text Prompts CLIP-AC WinCLIP APRIL-GANAnoVLCoOpCoCoOpLearnable Text Prompts BLIP AnomalyCLIPFiLOFAPromptMVTecAD(74.1, 87.6) (71.5, 86.4) (91.8, 96.5)(86.2, 93.5)(92.5, 96.7) (88.8, 94.8) (71.8, 84.9) (72.3, 86.7)(91.5, 96.2)(91.2, 96.0) (91.9, 95.7)VisA(66.4, 71.4) (65.0, 70.2) (78.8, 81.4)(78.0, 81.4)(79.2, 81.7) (62.8, 68.1) (78.1, 82.3) (57.8, 64.6)(82.1, 84.6)(83.9, 87.3) (84.6, 86.8)BTAD(34.5, 52.5) (51.0, 62.1) (68.2, 70.9)(73.6, 68.6)(80.3, 73.1) (66.8, 77.4) (48.4, 53.9) (85.3, 91.2)(88.3, 87.3)(85.4, 91.5) (92.2, 92.5)AITEX(71.0, 45.7) (71.5, 46.7) (73.0, 54.7)(57.6, 41.3)(72.5, 55.4) (66.2, 39.0) (48.6, 37.8) (71.3, 52.2)(62.2, 40.4)(78.2, 59.9) (74.1, 55.5)IndustrialDAGM(79.6, 59.0) (82.5, 63.7) (91.8, 79.5)(94.4, 83.8)(89.7, 76.3) (87.5, 74.6) (96.3, 85.5) (84.3, 66.6)(97.5, 92.3)(96.6, 90.4) (98.8, 95.3)DTD-Synthetic (71.6, 85.7) (66.8, 83.2) (93.2, 92.6)(86.4, 95.0)(94.9, 97.3) (83.1, 91.9) (84.1, 92.9) (91.5, 96.3)(93.5, 97.0)(94.7, 98.0) (96.2, 98.1)ELPV(59.2, 71.7) (69.4, 80.2) (74.0, 86.0)(65.5, 79.3)(70.6, 83.0) (73.0, 86.5) (78.4, 89.2) (75.2, 87.3)(81.5, 91.3)(82.2, 91.2) (83.7, 92.1)SDD(95.5, 87.9) (94.7, 77.9) (94.0, 87.2)(97.5, 93.4)(95.3, 91.3) (96.8, 90.0) (89.9, 50.4) (85.8, 56.9)(98.1, 93.4)(97.9, 94.8) (98.4, 95.6)MPDD(54.3, 65.4) (56.2, 66.0) (63.6, 69.9)(73.0, 80.2)(68.9, 71.9) (55.1, 64.2) (61.0, 69.1) (59.5, 67.7)(77.0, 82.0)(74.4, 76.9) (80.1, 83.9)BrainMRI(73.9, 81.7) (80.6, 86.4) (86.6, 91.5)(89.3, 90.9)(88.7, 91.3) (61.3, 44.9) (78.2, 86.7) (75.7, 83.0)(90.3, 92.2)(94.5, 94.9) (95.8, 96.2)MedicalHeadCT LAG(56.5, 58.4) (60.0, 60.7) (81.8, 80.2) (58.7, 76.5) (58.2, 76.9) (59.2, 74.8)(89.1, 89.4) (73.6, 84.8)(81.6, 84.2) (78.4, 78.8) (80.3, 73.4) (68.1, 62.8) (65.1, 78.0) (69.6, 82.9) (72.6, 84.7) (48.3, 66.8)(93.4, 91.6) (74.3, 84.9)(93.9, 92.1) (94.0, 92.4) (71.3, 83.8) (76.6, 86.1)Br35H(78.4, 78.8) (82.7, 81.3) (80.5, 82.2)(93.1, 92.9)(88.4, 88.9) (86.0, 87.5) (85.7, 89.1) (76.4, 74.4)(94.6, 94.7)(97.7, 96.8) (97.6, 97.1)</p>
<p>Table 3 .
3
Image-level (AUROC, AP) and pixel-level (AUROC, PRO) results of ablation study.</p>
<p>86.4) (94.2, 83.5) (88.4,90.3) (83.8, 65.5) FAPrompt P ✓ ✓ (88.5, 87.5) (95.0, 85.6) (91.0, 93.0) (85.7, 66.2)
ModelPrior Lprior CAPIndustrial image-level pixel-level image-level pixel-level MedicalCoCoOPI××(73.1, 70.1) (85.5, 80.9) (79.2, 83.5) (79.8, 57.7)FAPrompt −−I✓×(77.3, 75.2) (88.6, 81.0) (83.2, 84.8) (81.6, 62.6)FAPrompt −I✓✓(87.3,</p>
<p>Table 4 .
4
Image-level (AUROC, AP) and pixel-level (AUROC, PRO) results using different abnormality prior in DAP.AnomalyCLIP Ensemble* (85.5, 82.6) (94.6, 84.5) (88.8, 91.0) (83.5, 65.6) FAPrompt (88.5, 87.5) (95.0, 85.6) (91.0, 93.0) (85.7, 66.2)
ModelIndustrial Datasets Image-level Pixel-level Image-level Pixel-level Medical DatasetsAnomalyCLIP(85.0, 83.6) (94.4, 84.8) (87.7, 90.6) (83.2, 62.9)AnomalyCLIP Ensemble(85.5, 84.0) (94.7, 85.0) (89.3, 91.3) (83.2, 62.4)Best-matched Prompts(85.1, 84.9) (95.0, 83.2) (85.6, 89.1) (84.7, 65.9)Weighted Prompts(87.7, 86.7) (94.4, 83.2) (90.9, 92.3) (85.1, 65.7)</p>
<p>Table 5 .
5
Image-level (AUROC, AP) and pixel-level (AUROC, PRO) results comparison with alternatives.priorfocusesmoreon class-level differences while ignoring the sample-specific abnormality details that are essential for effective AD.While the two variants of FAPrompt markedly improve performance over CoCoOP by filtering out normal sample-related noise and introducing finegrained abnormality semantics through L prior and CAP, they still fall short the performance of the full FAPrompt.This shortfall is primarily because the ensemble approaches tend to generate highly redundant prompts, whereas FAPrompt learns complementary prompts that capture fine-grained abnormality details unattainable by the ensemble methods.Prototypical Abnormality Prompt vs Alternative Methods.To verify the effectiveness of averaging the prompts to obtain a unified prototypical abnormality prompt, we compare two alternative variants of FAPrompt in Table5: i) 'Best-matched Prompts', which selects the most similar abnormality prompt per patch/image tokens during anomaly scoring, and ii) 'Weighted Prompts', which assign importance weights to the abnormality prompts in the averaged operation based on the top M abnormal patch tokens p x .The results indicate that 'Best-matched Prompts' performs worse than simple averaging approach, as selecting only a single best-match prompt can lead to the loss of complementary information from other abnormality prompts, lim-
FAPrompt vs Ensemble methods. To learn more abnor-malities, a straightforward solution is to ensemble existingZSAD methods. Accordingly, we investigate two ensem-ble strategies in AnomalyCLIP for comparison: i) to learnan ensemble of AnomalyCLIP models with each modeltuning an abnormality prompt on the auxiliary dataset us-ing a different random seed ('AnomalyCLIP Ensemble'),and ii) to learn a AnomalyCLIP model with a set of mul-tiple abnormality prompts constrained to be orthogonalvia L oc ('AnomalyCLIP Ensemble*'). As shown in Ta-ble 5, although both ensemble strategies yield performancegains over the base model AnomalyCLIP, the abnormalityprompts derived from these methods are considerably lesseffective than those produced by FAPrompt.</p>
<p>this paper, we propose a novel framework FAPrompt to enhance CLIP's performance in ZSAD by learning adaptive fine-grained abnormality semantics.FAPrompt introduces a Compound Abnormality Prompt learning (CAP) module that generates complementary abnormality prompts.It then incorporates a Data-dependent Abnormality Prior learning (DAP) module, which refines these prompts to improve cross-dataset generalization.The interaction between CAP and DAP enables the model to learn complementary finegrained abnormality semantics.Extensive experiments on 19 datasets justify the effectiveness of FAPrompt.</p>
<p>AcknowledgmentsThis research was supported in part by A*STAR under its MTC YIRG Grant (M24N8c0103), the Ministry of Education, Singapore under its Tier-1 Academic Research Fund (24-SIS-SMU-008), the Lee Kong Chian Fellowship (T050273), and the National Research Foundation (NRF), Singapore, through the AI Singapore Programme under the project titled "AI-based Urban Cooling Technology Development"(Award No. AISG3-TC-2024-014-SGKR).
Ganomaly: Semi-supervised anomaly detection via adversarial training. Samet Akcay, Toby P Amir Atapour-Abarghouei, Breckon, Computer Vision-ACCV 2018: 14th Asian Conference on Computer Vision. Revised Selected Papers, Part III. Perth, AustraliaSpringerDecember 2-6, 2018. 201914</p>
<p>Zero-shot versus many-shot: Unsupervised texture anomaly detection. Toshimichi Aota, Lloyd Teh Tzer, Takayuki Tong, Okatani, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2023</p>
<p>Classification-based anomaly detection for general data. Liron Bergman, Yedid Hoshen, arXiv:2005.023592020arXiv preprint</p>
<p>Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Computerized medical imaging and graphics. Jorge Bernal, Javier Sánchez, Gloria Fernández-Esparrach, Debora Gil, Cristina Rodríguez, Fernando Vilariño, 201543</p>
<p>Anomaly detection under distribution shift. Tri Cao, Jiawen Zhu, Guansong Pang, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2023</p>
<p>Yunkang Cao, Xiaohao Xu, Jiangning Zhang, Yuqi Cheng, Xiaonan Huang, arXiv:2401.16402Guansong Pang, and Weiming Shen. A survey on visual anomaly detection: Challenge, approach, and prospect. 20241arXiv preprint</p>
<p>Adaclip: Adapting clip with hybrid learnable prompts for zero-shot anomaly detection. Yunkang Cao, Jiangning Zhang, Luca Frittoli, Yuqi Cheng, Weiming Shen, Giacomo Boracchi, European Conference on Computer Vision. Springer2024</p>
<p>April-gan: A zero-/few-shot anomaly classification and segmentation method for cvpr 2023 vand workshop challenge tracks 1&amp;2: 1st place on zero-shot ad and 4th place on few-shot ad. Xuhai Chen, Yue Han, Jiangning Zhang, arXiv:2305.173822023. 3, 5, 6arXiv preprint</p>
<p>Clip-ad: A language-guided staged dual-path model for zero-shot anomaly detection. Xuhai Chen, Jiangning Zhang, Guanzhong Tian, Haoyang He, Wuhao Zhang, Yabiao Wang, Chengjie Wang, Yunsheng Wu, Yong Liu, ArXiv, abs/2311.004532023</p>
<p>Deep one-class classification via interpolated gaussian descriptor. Yuanhong Chen, Yu Tian, Guansong Pang, Gustavo Carneiro, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022</p>
<p>Sub-image anomaly detection with deep pyramid correspondences. Niv Cohen, Yedid Hoshen, arXiv:2005.023572020arXiv preprint</p>
<p>Padim: a patch distribution modeling framework for anomaly detection and localization. Thomas Defard, Aleksandr Setkov, Angelique Loesch, Romaric Audigier, International Conference on Pattern Recognition. Springer2021</p>
<p>Automatic classification of defective photovoltaic module cells in electroluminescence images. Sergiu Deitsch, Stephan Vincent Christlein, Claudia Berger, Andreas Buerhop-Lutz, Florian Maier, Christian Gallwitz, Riess, Solar Energy. 18562019</p>
<p>Anomaly detection via reverse distillation from one-class embedding. Hanqiu Deng, Xingyu Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Anovl: Adapting vision-language models for unified zeroshot anomaly localization. Hanqiu Deng, Zhaoxiang Zhang, Jinan Bao, Xingyu Li, arXiv:2308.159392023. 1, 3, 6arXiv preprint</p>
<p>Multitask learning for thyroid nodule segmentation with thyroid region prior. Haifan Gong, Guanqi Chen, Ranran Wang, Xiang Xie, Mingzhi Mao, Yizhou Yu, Fei Chen, Guanbin Li, 2021 IEEE 18th international symposium on biomedical imaging (ISBI). IEEE2021</p>
<p>Anomalygpt: Detecting industrial anomalies using large vision-language models. Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, Jinqiao Wang, arXiv:2308.153662023arXiv preprint</p>
<p>Filo: Zero-shot anomaly detection by fine-grained description and high-quality localization. Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Hao Li, Ming Tang, Jinqiao Wang, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia202416</p>
<p>Skin lesion analysis toward melanoma detection: A challenge at the international symposium on biomedical imaging (isbi) 2016, hosted by the international skin imaging collaboration (isic). David Gutman, Emre Noel Cf Codella, Brian Celebi, Michael Helba, Nabin Marchetti, Allan Mishra, Halpern, arXiv:1605.013972016arXiv preprint</p>
<p>Br35h:: Brain tumor detection. kaggle (2020). Ahmed Hamada, 2020</p>
<p>The endotect 2020 challenge: evaluation and comparison of classification, segmentation and inference time for endoscopy. Debesh Steven A Hicks, Vajira Jha, Pål Thambawita, Hugo L Halvorsen, Michael A Hammer, Riegler, Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event. SpringerJanuary 10-15, 2021. 2021Proceedings, Part VIII</p>
<p>Divide-and-assemble: Learning block-wise memory for unsupervised anomaly detection. Jinlei Hou, Yingying Zhang, Qiaoyong Zhong, Di Xie, Shiliang Pu, Hong Zhou, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Winclip: Zero-/few-shot anomaly classification and segmentation. Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, Onkar Dabeer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition19606-19616, 2023. 1, 3, 5, 6</p>
<p>Deep learning-based defect detection of metal parts: evaluating current methods in complex conditions. Stepan Jezek, Martin Jonak, Radim Burget, Pavel Dvorak, Milos Skotak, 2021 13th International congress on ultra modern telecommunications and control systems and workshops (ICUMT). IEEE2021</p>
<p>Kvasir-seg: A segmented polyp dataset. Debesh Jha, Pia H Smedsrud, Michael A Riegler, Pål Halvorsen, Thomas De Lange, Dag Johansen, Håvard D Johansen, MultiMedia modeling: 26th international conference. Daejeon, South KoreaSpringerJanuary 5-8, 2020. 20202020proceedings, part II 26</p>
<p>Clip-tsa: Clip-assisted temporal self-attention for weakly-supervised video anomaly detection. Kevin Hyekang, Khoa Joo, Kashu Vo, Ngan Yamazaki, Le, 2023 IEEE International Conference on Image Processing (ICIP). IEEE2023</p>
<p>Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International conference on machine learning. PMLR2022</p>
<p>Attention based glaucoma detection: A large-scale database and cnn model. Liu Li, Mai Xu, Xiaofei Wang, Lai Jiang, Hanruo Liu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Dice loss for data-imbalanced nlp tasks. Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, Jiwei Li, ArXiv, abs/1911.028552019</p>
<p>Promptad: Learning prompts with only normal samples for few-shot anomaly detection. Xiaofan Li, Zhizhong Zhang, Xin Tan, Chengwei Chen, Yanyun Qu, Yuan Xie, Lizhuang Ma, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross B Girshick, Kaiming He, Piotr Dollár, IEEE International Conference on Computer Vision (ICCV). 2017. 2017</p>
<p>Diversity-measurable anomaly detection. Wenrui Liu, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Vt-adl: A vision transformer network for image anomaly detection and localization. Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, Gian Luca Foresti, 2021 IEEE 30th International Symposium on Industrial Electronics (ISIE). IEEE202116</p>
<p>Learning representations of ultrahigh-dimensional data for random distance-based outlier detection. Guansong Pang, Longbing Cao, Ling Chen, Huan Liu, Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining2018</p>
<p>Longbing Cao, and Anton Van Den Hengel. Deep learning for anomaly detection: A review. Guansong Pang, Chunhua Shen, ACM computing surveys (CSUR). 202154</p>
<p>Learning memory-guided normality for anomaly detection. Hyunjong Park, Jongyoun Noh, Bumsub Ham, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, PMLR, 2021. 1International conference on machine learning. 36</p>
<p>Towards total recall in industrial anomaly detection. Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Schölkopf, Thomas Brox, Peter Gehler, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Deep semi-supervised anomaly detection. Lukas Ruff, Robert A Vandermeulen, Nico Görnitz, Alexander Binder, Emmanuel Müller, Klaus-Robert Müller, Marius Kloft, ICLR2020</p>
<p>Multiresolution knowledge distillation for anomaly detection. Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H Rohban, Hamid R Rabiee, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202126</p>
<p>Fast unsupervised anomaly detection with generative adversarial networks. Thomas Schlegl, Philipp Seeböck, Georg Sebastian M Waldstein, Ursula Langs, Schmidt-Erfurth. F-Anogan, Medical image analysis. 5422019</p>
<p>A public fabric database for defect detection methods and results. Javier Silvestre-Blanes, Teresa Albero-Albero, Ignacio Miralles, Rubén Pérez-Llorens, Jorge Moreno, Autex Research Journal. 1942019</p>
<p>Segmentation-based deep-learning approach for surface-defect detection. Domen Tabernik, Samo Šela, Jure Skvarč, Danijel Skočaj, Journal of Intelligent Manufacturing. 3132020</p>
<p>Automated polyp detection in colonoscopy videos using shape and context information. Nima Tajbakhsh, Jianming Suryakanth R Gurudu, Liang, IEEE transactions on medical imaging. 3522015</p>
<p>Support vector data description. M J David, Tax, Robert Pw Duin, Machine learning. 5422004</p>
<p>Revisiting reverse distillation for anomaly detection. Tran Dinh Tien, Anh Tuan Nguyen, Nguyen Hoang Tran, Ta Duc Huy, Soan Duong, Chanh D Tr Nguyen, Steven Qh Truong, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Guodong Wang, Shumin Han, Errui Ding, Di Huang, arXiv:2103.04257Student-teacher feature pyramid matching for anomaly detection. 2021arXiv preprint</p>
<p>Weakly supervised learning for industrial optical inspection. Matthias Wieler, Tobias Hahn, DAGM symposium in. 200711</p>
<p>Deep learning for video anomaly detection: A review. Peng Wu, Chengyu Pan, Yuting Yan, Guansong Pang, Peng Wang, Yanning Zhang, arXiv:2409.053832024arXiv preprint</p>
<p>Open-vocabulary video anomaly detection. Peng Wu, Xuerong Zhou, Guansong Pang, Yujia Sun, Jing Liu, Peng Wang, Yanning Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Weakly supervised video anomaly detection and localization with spatio-temporal prompts. Peng Wu, Xuerong Zhou, Guansong Pang, Zhiwei Yang, Qingsen Yan, Wang Peng, Yanning Zhang, ACM Multimedia 2024. 2024</p>
<p>Vadclip: Adapting vision-language models for weakly supervised video anomaly detection. Peng Wu, Xuerong Zhou, Guansong Pang, Lingru Zhou, Qingsen Yan, Peng Wang, Yanning Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Squid: Deep feature in-painting for unsupervised anomaly detection. Tiange Xiang, Yixiao Zhang, Yongyi Lu, Alan L Yuille, Chaoyi Zhang, Weidong Cai, Zongwei Zhou, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Learning semantic context from normal samples for unsupervised anomaly detection. Xudong Yan, Huaidong Zhang, Xuemiao Xu, Xiaowei Hu, Pheng-Ann Heng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2021</p>
<p>Focus the discrepancy: Intra-and intercorrelation learning for image anomaly detection. Xincheng Yao, Ruoqi Li, Zefeng Qian, Yan Luo, Chongyang Zhang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>One-for-all: Proposal masked cross-class anomaly detection. Xincheng Yao, Chongyang Zhang, Ruoqi Li, Jun Sun, Zhenyu Liu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2023</p>
<p>Patch svdd: Patch-level svdd for anomaly detection and segmentation. Jihun Yi, Sungroh Yoon, Proceedings of the Asian Conference on Computer Vision. the Asian Conference on Computer Vision2020</p>
<p>Old is gold: Redefining the adversarially learned one-class classifier training paradigm. Muhammad Zaigham Zaheer, Jin-Ha Lee, Marcella Astrid, Seung-Ik Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Draema discriminatively trained reconstruction embedding for surface anomaly detection. Vitjan Zavrtanik, Matej Kristan, Danijel Skočaj, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Reconstruction by inpainting for visual anomaly detection. Vitjan Zavrtanik, Matej Kristan, Danijel Skočaj, Pattern Recognition. 11221077062021</p>
<p>Destseg: Segmentation guided denoising student-teacher for anomaly detection. Xuan Zhang, Shiyu Li, Xi Li, Ping Huang, Jiulong Shan, Ting Chen, 2023</p>
<p>Conditional prompt learning for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022. 1, 2, 4, 6</p>
<p>Learning to prompt for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, International Journal of Computer Vision. 13092022</p>
<p>Anomalyclip: Object-agnostic prompt learning for zero-shot anomaly detection. Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, Jiming Chen, The Twelfth International Conference on Learning Representations. 2024. 1, 2, 3, 5, 6</p>
<p>Toward generalist anomaly detection via in-context residual learning with few-shot sample prompts. Jiawen Zhu, Guansong Pang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202435</p>
<p>Anomaly heterogeneity learning for open-set supervised anomaly detection. Jiawen Zhu, Choubo Ding, Yu Tian, Guansong Pang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2024</p>
<p>Spot-the-difference self-supervised pretraining for anomaly detection and segmentation. Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, Onkar Dabeer, European Conference on Computer Vision. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
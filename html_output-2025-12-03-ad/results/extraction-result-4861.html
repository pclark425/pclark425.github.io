<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4861 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4861</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4861</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-260164780</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.13854v4.pdf" target="_blank">WebArena: A Realistic Web Environment for Building Autonomous Agents</a></p>
                <p><strong>Paper Abstract:</strong> With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4861.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4861.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebArena Scratchpad utility</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple note-taking tool provided as a separate website in WebArena that agents can use to record intermediate notes or calculations during multi-step web tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Scratchpad (utility)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A writable web page/tool in the WebArena environment intended for agents to create and consult ephemeral notes or calculations while executing long-horizon web tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>scratchpad (external short-term memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Implemented as an interactive website within WebArena; agents can type into and read from it like any other page, enabling them to persist intermediate information across actions/tabs during a single task execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General multi-step web tasks in WebArena</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-horizon, multi-page web-based tasks (812 benchmark intents) that require navigation, planning, cross-site information gathering, and content/configuration operations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WebArena benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>The scratchpad is provided as a capability of the environment but the paper does not report experiments quantifying the effect of agents using the scratchpad versus not using it.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Baseline agents in the paper were not evaluated with an explicit, measured use of the scratchpad; integration patterns and best practices for agents to use this tool are not explored or measured here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Providing an explicit editable scratchpad offers a straightforward external short-term memory mechanism for agents, but its efficacy on improving performance in realistic web tasks remains an open empirical question according to the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebArena: A Realistic Web Environment for Building Autonomous Agents', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4861.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4861.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique where the model is asked to produce step-by-step intermediate reasoning before emitting an action or final answer; used here as a prompting strategy for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CoT reasoning agent (GPT-4 / GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-based agents (GPT-4 and GPT-3.5) configured to first generate explicit step-by-step reasoning (chain-of-thought) and then issue the next browser action based on that reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>ephemeral internal reasoning trace (CoT) / short-term scratchpad-like output</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>The model emits intermediate natural-language reasoning (CoT) inside the response prior to action selection. This reasoning is produced in-context for each step but is not an external persistent memory store across separate episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WebArena web-based task execution</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Map high-level natural language intents to sequences of browser actions across multi-site, long-horizon tasks (WebArena's 812 intents); evaluation measures functional correctness of task outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WebArena benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>GPT-4 with CoT: 11.70% end-to-end task success rate (SR%); GPT-3.5 with CoT: 8.75% SR (reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Authors report that adding explicit chain-of-thought reasoning produced a modest absolute improvement of 2.34% in task success rate over the version without CoT (i.e., CoT > non-CoT by 2.34% absolute).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Improvement from CoT is modest and does not close the large gap to human performance; CoT is an ephemeral reasoning trace (not persistent memory) and does not address failures such as early stopping, observation-interpretation errors, or insufficient exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Chain-of-thought prompting can modestly improve performance for LLM-driven interactive agents on long-horizon web tasks, but the authors emphasize the need for persistent memory/reuse mechanisms and improved exploration/failure recovery to substantially raise success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebArena: A Realistic Web Environment for Building Autonomous Agents', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4861.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4861.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory components (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory components for agents (general)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The general idea of augmenting agents with memory mechanisms to store and reuse strategies or past experiences across tasks, proposed as a promising direction to improve robustness on WebArena tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory-augmented agents (conceptual)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Conceptual agents that include mechanisms to record, index, and retrieve past trajectories, strategies, or successful subroutines for reuse when solving new but related tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external/episodic memory (conceptual)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Referenced as a component that would allow reuse of successful strategies from past experiments (citing prior work), but not implemented or experimentally tested in this paper's baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WebArena long-horizon web tasks (proposed future application)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that often come from templates and can benefit from reusing prior solutions or strategies across similar instances (e.g., many instantiations of the same template with different parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WebArena benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>No empirical comparison in this paper; authors hypothesize that memory components enabling strategy reuse could improve success rates on template-derived and long-horizon tasks and cite prior work as motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not evaluated here; potential challenges (not experimentally characterized in this work) include how to index/retrieve useful past trajectories, integration with action selection, and robustness to retrieval errors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>The authors identify memory mechanisms (storing/reusing successful strategies) as a promising research direction for improving agents' performance on realistic, varied web tasks represented in WebArena.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebArena: A Realistic Web Environment for Building Autonomous Agents', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4861.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4861.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior system that uses dynamic memory and self-reflection to iteratively improve agent behavior; cited as relevant related work advocating memory-enabled agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Prior work in which an agent maintains and updates memory traces and uses self-reflection to improve performance across tasks (referenced in the related work section).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dynamic memory / reflective memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Uses an internal memory and self-reflective loop to store and refine experiences; mentioned as an example of memory-equipped agent architectures but not evaluated in WebArena within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Interactive decision-making tasks in referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reflexion focuses on iterative self-improvement for agents via dynamic memory; the WebArena paper cites it as part of the landscape of memory-enabled agent research.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Mentioned as related work; no direct comparisons or experiments with Reflexion are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>WebArena authors note such methods are promising but do not evaluate them on the WebArena suite in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Reflexion exemplifies an approach that WebArena authors suggest could be valuable to test on realistic web tasks: dynamic memory plus self-reflection may help agents recover from failures and reuse experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebArena: A Realistic Web Environment for Building Autonomous Agents', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Hierarchical control of situated agents through natural language <em>(Rating: 2)</em></li>
                <li>Pal: Program-aided language models <em>(Rating: 1)</em></li>
                <li>Code as policies: Language model programs for embodied control <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4861",
    "paper_id": "paper-260164780",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "Scratchpad",
            "name_full": "WebArena Scratchpad utility",
            "brief_description": "A simple note-taking tool provided as a separate website in WebArena that agents can use to record intermediate notes or calculations during multi-step web tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Scratchpad (utility)",
            "agent_description": "A writable web page/tool in the WebArena environment intended for agents to create and consult ephemeral notes or calculations while executing long-horizon web tasks.",
            "memory_type": "scratchpad (external short-term memory)",
            "memory_description": "Implemented as an interactive website within WebArena; agents can type into and read from it like any other page, enabling them to persist intermediate information across actions/tabs during a single task execution.",
            "task_name": "General multi-step web tasks in WebArena",
            "task_description": "Long-horizon, multi-page web-based tasks (812 benchmark intents) that require navigation, planning, cross-site information gathering, and content/configuration operations.",
            "benchmark_name": "WebArena benchmark",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "The scratchpad is provided as a capability of the environment but the paper does not report experiments quantifying the effect of agents using the scratchpad versus not using it.",
            "limitations_or_challenges": "Baseline agents in the paper were not evaluated with an explicit, measured use of the scratchpad; integration patterns and best practices for agents to use this tool are not explored or measured here.",
            "key_insights": "Providing an explicit editable scratchpad offers a straightforward external short-term memory mechanism for agents, but its efficacy on improving performance in realistic web tasks remains an open empirical question according to the authors.",
            "uuid": "e4861.0",
            "source_info": {
                "paper_title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-thought prompting",
            "brief_description": "A prompting technique where the model is asked to produce step-by-step intermediate reasoning before emitting an action or final answer; used here as a prompting strategy for action selection.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "agent_name": "CoT reasoning agent (GPT-4 / GPT-3.5)",
            "agent_description": "LLM-based agents (GPT-4 and GPT-3.5) configured to first generate explicit step-by-step reasoning (chain-of-thought) and then issue the next browser action based on that reasoning.",
            "memory_type": "ephemeral internal reasoning trace (CoT) / short-term scratchpad-like output",
            "memory_description": "The model emits intermediate natural-language reasoning (CoT) inside the response prior to action selection. This reasoning is produced in-context for each step but is not an external persistent memory store across separate episodes.",
            "task_name": "WebArena web-based task execution",
            "task_description": "Map high-level natural language intents to sequences of browser actions across multi-site, long-horizon tasks (WebArena's 812 intents); evaluation measures functional correctness of task outcomes.",
            "benchmark_name": "WebArena benchmark",
            "performance_with_memory": "GPT-4 with CoT: 11.70% end-to-end task success rate (SR%); GPT-3.5 with CoT: 8.75% SR (reported in the paper).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Authors report that adding explicit chain-of-thought reasoning produced a modest absolute improvement of 2.34% in task success rate over the version without CoT (i.e., CoT &gt; non-CoT by 2.34% absolute).",
            "limitations_or_challenges": "Improvement from CoT is modest and does not close the large gap to human performance; CoT is an ephemeral reasoning trace (not persistent memory) and does not address failures such as early stopping, observation-interpretation errors, or insufficient exploration.",
            "key_insights": "Chain-of-thought prompting can modestly improve performance for LLM-driven interactive agents on long-horizon web tasks, but the authors emphasize the need for persistent memory/reuse mechanisms and improved exploration/failure recovery to substantially raise success rates.",
            "uuid": "e4861.1",
            "source_info": {
                "paper_title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Memory components (general)",
            "name_full": "Memory components for agents (general)",
            "brief_description": "The general idea of augmenting agents with memory mechanisms to store and reuse strategies or past experiences across tasks, proposed as a promising direction to improve robustness on WebArena tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Memory-augmented agents (conceptual)",
            "agent_description": "Conceptual agents that include mechanisms to record, index, and retrieve past trajectories, strategies, or successful subroutines for reuse when solving new but related tasks.",
            "memory_type": "external/episodic memory (conceptual)",
            "memory_description": "Referenced as a component that would allow reuse of successful strategies from past experiments (citing prior work), but not implemented or experimentally tested in this paper's baselines.",
            "task_name": "WebArena long-horizon web tasks (proposed future application)",
            "task_description": "Tasks that often come from templates and can benefit from reusing prior solutions or strategies across similar instances (e.g., many instantiations of the same template with different parameters).",
            "benchmark_name": "WebArena benchmark",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "No empirical comparison in this paper; authors hypothesize that memory components enabling strategy reuse could improve success rates on template-derived and long-horizon tasks and cite prior work as motivation.",
            "limitations_or_challenges": "Not evaluated here; potential challenges (not experimentally characterized in this work) include how to index/retrieve useful past trajectories, integration with action selection, and robustness to retrieval errors.",
            "key_insights": "The authors identify memory mechanisms (storing/reusing successful strategies) as a promising research direction for improving agents' performance on realistic, varied web tasks represented in WebArena.",
            "uuid": "e4861.2",
            "source_info": {
                "paper_title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "brief_description": "A referenced prior system that uses dynamic memory and self-reflection to iteratively improve agent behavior; cited as relevant related work advocating memory-enabled agents.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "Prior work in which an agent maintains and updates memory traces and uses self-reflection to improve performance across tasks (referenced in the related work section).",
            "memory_type": "dynamic memory / reflective memory",
            "memory_description": "Uses an internal memory and self-reflective loop to store and refine experiences; mentioned as an example of memory-equipped agent architectures but not evaluated in WebArena within this paper.",
            "task_name": "Interactive decision-making tasks in referenced work",
            "task_description": "Reflexion focuses on iterative self-improvement for agents via dynamic memory; the WebArena paper cites it as part of the landscape of memory-enabled agent research.",
            "benchmark_name": "",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Mentioned as related work; no direct comparisons or experiments with Reflexion are provided in this paper.",
            "limitations_or_challenges": "WebArena authors note such methods are promising but do not evaluate them on the WebArena suite in this work.",
            "key_insights": "Reflexion exemplifies an approach that WebArena authors suggest could be valuable to test on realistic web tasks: dynamic memory plus self-reflection may help agents recover from failures and reuse experiences.",
            "uuid": "e4861.3",
            "source_info": {
                "paper_title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2
        },
        {
            "paper_title": "Hierarchical control of situated agents through natural language",
            "rating": 2
        },
        {
            "paper_title": "Pal: Program-aided language models",
            "rating": 1
        },
        {
            "paper_title": "Code as policies: Language model programs for embodied control",
            "rating": 1
        }
    ],
    "cost": 0.017341,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>WE BAR E N A : A REALISTIC WEB ENVIRONMENT FOR BUILDING AUTONOMOUS AGENTS
16 Apr 2024</p>
<p>Shuyan Zhou shuyanzh@cs.cmu.edu 
Carnegie Mellon University</p>
<p>Frank F Xu 
Carnegie Mellon University</p>
<p>Hao Zhu 
Carnegie Mellon University</p>
<p>Xuhui Zhou 
Carnegie Mellon University</p>
<p>Robert Lo 
Carnegie Mellon University</p>
<p>Abishek Sridhar 
Carnegie Mellon University</p>
<p>Xianyi Cheng 
Carnegie Mellon University</p>
<p>Tianyue Ou 
Carnegie Mellon University</p>
<p>Yonatan Bisk 
Carnegie Mellon University</p>
<p>Daniel Fried 
Carnegie Mellon University</p>
<p>Uri Alon 
Carnegie Mellon University</p>
<p>Graham Neubig gneubig@cs.cmu.edu 
Carnegie Mellon University</p>
<p>WE BAR E N A : A REALISTIC WEB ENVIRONMENT FOR BUILDING AUTONOMOUS AGENTS
16 Apr 2024BCB2DEC6563795895180F47900EF1504arXiv:2307.13854v4[cs.AI]
With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands.However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios.In this paper, we build an environment for language-guided agents that is highly realistic and reproducible.Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving.Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions.The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet.We experiment with several baseline agents, integrating recent techniques such as reasoning before acting.The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%.These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.Our code, data, environment reproduction resources, and video demonstrations are publicly available at https://webarena.dev/.</p>
<p>INTRODUCTION</p>
<p>Autonomous agents that perform everyday tasks via human natural language commands could significantly augment human capabilities, improve efficiency, and increase accessibility.Nonetheless, to fully leverage the power of autonomous agents, it is crucial to understand their behavior within an environment that is both authentic and reproducible.This will allow measurement of the ability of agents on tasks that human users care about in a fair and consistent manner.</p>
<p>Current environments for evaluate agents tend to over-simplify real-world situations.As a result, the functionality of many environments is a limited version of their real-world counterparts, leading to a lack of task diversity (Shi et al., 2017;Anderson et al., 2018;Gordon et al., 2018;Misra et al., 2016;Shridhar et al., 2020;2021;Yao et al., 2022a).In addition, these simplifications often lower the complexity of tasks as compared to their execution in the real world (Puig et al., 2018;Shridhar et al., 2020;Yao et al., 2022a).Finally, some environments are presented as a static resource (Shi et al., 2017;Deng et al., 2023) where agents are confined to accessing only those states that were previously cached during data collection, thus limiting the breadth and diversity of exploration.For evaluation, many environments focus on comparing the textual surface form of the predicted Figure 1: WebArena is a standalone, self-hostable web environment for building autonomous agents.</p>
<p>WebArena creates websites from four popular categories with functionality and data mimicking their real-world equivalents.To emulate human problem-solving, WebArena also embeds tools and knowledge resources as independent websites.WebArena introduces a benchmark on interpreting high-level realistic natural language command to concrete web-based interactions.We provide validators to programmatically validate the functional correctness of each task.</p>
<p>action sequences with reference action sequences, disregarding the functional correctness of the executions and possible alternative solutions (Puig et al., 2018;Jernite et al., 2019;Xu et al., 2021;Li et al., 2020;Deng et al., 2023).These limitations often result in a discrepancy between simulated environments and the real world, and can potentially impact the generalizability of AI agents to successfully understand, adapt, and operate within complex real-world situations.</p>
<p>We introduce WebArena, a realistic and reproducible web environment designed to facilitate the development of autonomous agents capable of executing tasks ( §2).An overview of WebArena is in Figure 1.Our environment comprises four fully operational, self-hosted web applications, each representing a distinct domain prevalent on the internet: online shopping, discussion forums, collaborative development, and business content management.Furthermore, WebArena incorporates several utility tools, such as map, calculator, and scratchpad, to best support possible human-like task executions.Lastly, WebArena is complemented by an extensive collection of documentation and knowledge bases that vary from general resources like English Wikipedia to more domain-specific references, such as manuals for using the integrated development tool (Fan et al., 2022).The content populating these websites is extracted from their real-world counterparts, preserving the authenticity of the content served on each platform.We deliver the hosting services using Docker containers with gym-APIs (Brockman et al., 2016), ensuring both the usability and the reproducibility of WebArena.</p>
<p>Along with WebArena, we release a ready-to-use benchmark with 812 long-horizon web-based tasks ( §3).Each task is described as a high-level natural language intent, emulating the abstract language usage patterns typically employed by humans (Bisk et al., 2019).Two example intents are shown in the upper left of Figure 1.We focus on evaluating the functional correctness of these tasks, i.e., does the result of the execution actually achieve the desired goal ( §3.2).For instance, to evaluate the example in Figure 2, our evaluation method verifies the concrete contents in the designated repository.This evaluation is not only more reliable (Zhong et al., 2017;Chen et al., 2021;Wang et al., 2022) than comparing the textual surface-form action sequences (Puig et al., 2018;Deng et al., 2023) but also accommodate a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks.</p>
<p>We use this benchmark to evaluate several agents that can follow NL command and perform webbased tasks ( §4).These agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as GPT-4 and PALM-2.Experiment results show that the best GPT-4 agent performance is somewhat limited, with an end-to-end task success rate of only 14.41%, while the human performance is 78.24%.We hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks ( §5.1).These outcomes underscore the necessity for further development towards robust and effective agents (LeCun, 2022) in WebArena.</p>
<p>WE BAR E N A : WEBSITES AS AN ENVIRONMENT FOR AUTONOMOUS AGENTS</p>
<p>Our goal is to create a realistic and reproducible web environment.We achieve reproducibility by making the environment standalone, without relying on live websites.This circumvents technical Figure 2: A high-level task that can be fully executed in WebArena.Success requires sophisticated, long-term planning and reasoning.To accomplish the goal (top), an agent needs to (1) find Pittsburgh art museums on Wikipedia, (2) identify their locations on a map (while optimizing the itinerary), and</p>
<p>(3) update the README file in the appropriate repository with the planned route.</p>
<p>challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time.We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts.</p>
<p>CONTROLLING AGENTS THROUGH HIGH-LEVEL NATURAL LANGUAGE</p>
<p>The WebArena environment is denoted as E= ⟨S, A, O, T ⟩ with state space S, action space A ( §2.4) and observation space O ( §2.3).The transition function T : S × A−→ S is deterministic, and it is defined by the underlying implementation of each website in the environment.Given a task described as a natural language intent i, an agent issues an action a t ∈ A based on intent i, the current observation o t ∈ O, the action history a t−1 1 and the observation history o t−1 1 .Consequently, the action results in a new state s t+1 ∈ S and its corresponding observation o t+1 ∈ O.We propose a reward function r(a T 1 , s T 1 ) to measure the success of a task execution, where a T 1 represents the sequence of actions from start to the end time step T , and s T 1 denotes all intermediate states.This reward function assesses if state transitions align with the expectations of the intents.For example, with an intent to place an order, it verifies whether an order has been placed.Additionally, it evaluates the accuracy of the agent's actions, such as checking the correctness of the predicted answer.</p>
<p>WEBSITE SELECTION</p>
<p>To decide which categories of websites to use, we first analyzed approximately 200 examples from the authors' actual web browser histories.Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session.Based on this, we classified the visited websites into abstract categories.We then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities (e.g., Amazon, eBay), (2) social forum platforms for opinion exchanges (e.g., Reddit, StackExchange), (3) collaborative development platforms for software development (e.g., GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (e.g., online store management).</p>
<p>In addition to these platforms, we selected three utility-style tools that are frequently used in webbased tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes.As informationseeking and knowledge acquisition are critical in web-based tasks, we also incorporated various knowledge resources into WebArena.These resources range from general information hubs, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals.</p>
<p>Implementation We leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website (OneStopShop), GitLab, Reddit, an online store content management system (CMS), a map, and an English Wikipedia.Then we imported sampled data from their  real-world counterparts.As an example, our version of GitLab was developed based on the actual GitLab project. 1 We carefully emulated the features of a typical code repository by including both popular projects with many issues and pull requests and smaller, personal projects.Details of all websites in WebArena can be found in Appendix A.1.We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state (See Appendix A.2).</p>
<p>OBSERVATION SPACE</p>
<p>We design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs , and the web page content of the focused tab.WebArena is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities.The multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab.We provide flexible configuration to render the page content in many modes: (see Figure 3 for an example): (1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work (Shi et al., 2017;Deng et al., 2023;Li et al., 2020); (2) a screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page. 2 The accessibility tree is a subset of the DOM tree with elements that are relevant and useful for displaying the contents of a web page.Every element is represented as its role (e.g., a link), its text content, and its properties (e.g., whether it is focusable).Accessibility trees largely retain the structured information of a web page while being more compact than the DOM representation.</p>
<p>We provide an option to limit the content to the contents within a viewport for all modes.This ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements.</p>
<p>ACTION SPACE</p>
<p>Following previous work on navigation and operation in web and embodied environments (Shi et al., 2017;Liu et al., 2018), we design a compound action space that emulates the keyboard and mouse operations available on web pages.Figure 4 lists all the available actions categorized into three distinct groups.The first group includes element operations such as clicking, hovering, typing, and key combination pressing.The second comprises tab-related actions such as opening, closing, and switching between tabs.The third category consists of URL navigation actions, such as visiting a specific URL or navigating forward and backward in the browsing history.</p>
<p>Building on these actions, WebArena provides agents with the flexibility to refer to elements for operation in different ways.An element can be selected by its on-screen coordinates, (x, y), or by a unique element ID that is prepended to each element.This ID is generated when traversing the Document Object Model (DOM) or accessibility tree.With element IDs, the element selection is transformed into an n-way classification problem, thereby eliminating any disambiguation efforts required from the agent or the underlying implementation.For example, issuing the action click [1582] clicks the button given the observation of [1582] Add to Cart.This flexible element selection allows WebArena to support agents designed in various ways (e.g., accepting input from different modalities) without compromising fair comparison metrics such as step count.User Role Simulation Users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories.We emulate this scenario by generating unique user profiles on each platform.The details can be found in Appendix A.3.</p>
<p>BENCHMARK SUITE OF WEB-BASED TASKS</p>
<p>We provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in WebArena.Each example has a metric to evaluate the functional correctness of the task execution.In this section, we first formally define the task of controlling an autonomous agent through natural language.Then we introduce the annotation process of our benchmark.</p>
<p>INTENT COLLECTION</p>
<p>We focus on curating realistic intents to carry out complex and creative tasks within WebArena.To start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites' content and functionalities.As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites.</p>
<p>Next, we instructed the annotators to formulate intents based on the following criteria:</p>
<p>(1) The intent should be abstract and high-level, implying that the task cannot be fulfilled with merely one or two actions.As an example, instead of "click the science subreddit", we encouraged annotators to come up with something more complex like "post a greeting message on science subreddit", which involves performing multiple actions.</p>
<p>(2) The intent should be creative.Common tasks such as account creation can be easily thought of.</p>
<p>We encouraged the annotators to add constraints (e.g., "create a Reddit account identical to my GitLab one") to make the intents more unique.</p>
<p>(3) The intent should be formulated as a template by making replaceable elements as variables.</p>
<p>The annotators were also responsible for developing several instantiations for each variable.For example, the intent "create a Reddit account identical to my GitLab one" can be converted into "create a {{site1}} account identical to my {{site2}} one", with an instantiation like "{site1: Reddit, site2: GitLab}" and another like "{site1: GitLab, site2: OneStopShopping}".Notably, tasks derived from the same template can have distinct execution traces.The similarity resides primarily in the high-level semantics rather than the specific implementation.</p>
<p>We also provided a prompt for the annotators to use with ChatGPT3 for inspiration, that contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites.Furthermore, we offered a curated list of examples for annotators to reference.</p>
<p>Intent Analysis</p>
<p>In total, we curated 241 templates and 812 instantiated intents.On average, each template is instantiated to 3.3 examples.The intent distribution is shown in Figure 6.Furthermore, we classify the intents into three primary categories with examples shown in Figure 5:</p>
<p>(1) Information-seeking tasks expect a textual response.Importantly, these tasks in WebArena often require navigation across multiple pages or focus on user-centric content.This makes them distinct from open-domain question-answering (Yang et al., 2018;Kwiatkowski et al., 2019), which focuses on querying general knowledge with a simple retrieval step.For instance, to answer "When was the last time I bought the shampoo", an agent traverses the user's purchase history, checking order details to identify the most recent shampoo purchase.(2) Site navigation: This category is composed of tasks that require navigating through web pages using a variety of interactive elements such as search functions and links.The objective is often to locate specific information or navigate to a particular section of a site.(3) Content and configuration operation: This category encapsulates tasks that require operating in the web environment to create, revise, or configure content or settings.This includes adjusting settings, managing accounts, performing online transactions, generating new web content, and modifying existing content.Examples range from updating a social media status or README file to conducting online purchases and configuring privacy settings.</p>
<p>EVALUATION ANNOTATION</p>
<p>Evaluating Information Seeking Tasks To measure the correctness of information-seeking tasks where a textual answer is expected, we provide the annotated answer a * for each intent.The a * is further compared with the predicted answer â with one of the following scoring functions r info (â, a * ).</p>
<p>First, we define exact_match where only â that is identical with a * receives a score of one.This function is primarily applicable to intent types whose responses follow a more standardized format, similar to the evaluation on question answering literature (Rajpurkar et al., 2016;Yang et al., 2018).</p>
<p>Second, we create must_include where any â containing a * receives a score of one.This function is primarily used in when an unordered list of text is expected or where the emphasis of evaluation is on certain key concepts.In the second example in Table 1, we expect both the correct name and the email address to be presented, irrespective of the precise wording used to convey the answer.</p>
<p>Finally, we introduce fuzzy_match where we utilize a language model to assess whether â is semantically equivalent to a * .Specifically, in this work, we use gpt-4-0613 to perform this evaluation.The corresponding prompt details are provided in Appendix A.7.The fuzzy_match function applies to situations where the format of the answer is diverse.For instance, in responding to "Compare the time for walking and driving route from AMC Waterfront to Randyland", it is essential to ensure that driving time and walking time are accurately linked with the correct terms.</p>
<p>The fuzzy_match function could also flexibly match the time "2h58min" with different forms such as "2 hour 58 minutes", "2:58" and others.We demonstrate a language model can achieve nearly perfect performance on this task in §A.8.</p>
<p>Evaluating Site Navigation and Content &amp; Config Tasks</p>
<p>The tasks in these categories require accessing web pages that meet certain conditions or performing operations that modify the underlying data storage of the respective websites.To assess these, we establish reward functions r prog (s) that programmatically examine the intermediate states s within an execution trajectory to ascertain whether the outcome aligns with the intended result.These intermediate states are often the underlying databases of the websites, the status, and the content of a web page at each step of the execution.</p>
<p>Evaluating each instance involves two components.First, we provide a locator, tasked with retrieving the critical content pertinent to each intent.The implementation of this locator varies from a database query, a website-supported API call, to a JavaScript element selection on the relevant web page, depending on implementation feasibility.For example, the evaluation process for the intent of the fifth example in Table 1, first obtains the URL of the latest post by examining the last state in the state sequence s.Then it navigates to the corresponding post page and obtains the post's content by running the Javascript "document.querySelector('.submission__inner').outerText".</p>
<p>Subsequently, we annotate keywords that need to exist within the located content.For example, the evaluation verifies if the post is correctly posted in the "nyc" subreddit by examining the URL of the post and if the post contains the requested content by examining the post content.We reuse the exact_match and must_include functions from information-seeking tasks for this purpose.Unachievable Tasks Due to constraints such as inadequate evidence, user permissions ( §A.3), or the absence of necessary functional support on the website, humans may ask for tasks that are not possible to complete.Inspired by previous work on evaluating question-answering models on unanswerable questions (Rajpurkar et al., 2018), we design unachievable tasks in WebArena.For instance, fulfilling an intent like "Tell me the contact number of OneStopShop" is impracticable in WebArena, given that the website does not provide such contact information.We label such instances as "N/A" and expect an agent to produce an equivalent response.These examples allow us to assess an agent's ability to avoid making unfounded claims and its adherence to factual accuracy.</p>
<p>Annotation Process</p>
<p>The intents were contributed by the authors following the annotation guideline in §3.1.Every author has extensive experience with web-based tasks.The reference answers to the information-seeking tasks were curated by the authors and an external annotator.To ensure consistency and accuracy, each question was annotated twice.If the two annotators disagreed, a third annotator finalized the annotation.The programs to evaluate the remaining examples were contributed by three of the authors who are proficient in JavaScript programming.Difficult tasks were often discussed collectively to ensure the correctness of the annotation.The annotation required the annotator to undertake the full execution and scrutinize the intermediate states.</p>
<p>Avg.Time 110s Success Rateinfo 74.68% Success Rateothers 81.32% Success Rateall 78.24%</p>
<p>Human Performance We sample one task from each of the 170 templates and ask five computer science graduate students to perform these tasks.The human performance is on the right.Overall, the human annotators complete 78.24% of the tasks, with lower performance on information-seeking tasks.Through examining the recorded trajectories, we found that 50% of the failures are due to misinterpreting the intent (e.g., providing travel distance when asked for travel time), incomplete answers (e.g., providing only name when asked for name and email), and incomplete executions (e.g., partially filling the product information), while the remaining instances have more severe failures, where the executions are off-target.More discussions on human annotations can be found in §A.5.</p>
<p>BASELINE WEB AGENTS</p>
<p>We experiment with three LLMs using two prompting strategies, both with two examples in the context.In the first setting, we ask the LLM to directly predict the next action given the current observation, the intent and the previously performed action.In the second setting, with the same information, the model first performs chain-of-thought reasoning steps in the text before the action prediction (CoT, Wei et al. (2022); Yao et al. (2022b)).Before the examples, we provide a detailed overview of the browser environment, the allowed actions, and many rules.To make the model aware of the unachievable tasks, the instruction explicitly asks the agent to stop if it believes the task is impossible to perform.We refer to this directive as Unachievable hint, or UA hint.This introduction Published as a conference paper at ICLR 2024 is largely identical to the guidelines we presented to human annotators to ensure a fair comparison.</p>
<p>We use an accessibility tree with element IDs as the observation space.The agent can identify which element to interact with by the ID of the element.For instance, the agent can issue click [1582] to click the "Add to Cart" button with the ID of 1582.The full prompts can be found in Appendix A.9.The detailed configurations of each model can be found in Appendix A.6.</p>
<p>RESULTS</p>
<p>CoT   (Anil et al., 2023) underperforms GPT-3.5, with a success rate of 5.05%.These results underline the inherent challenges and complexities of executing tasks that span long horizons, particularly in realistic environments such as WebArena.</p>
<p>ANALYSIS</p>
<p>Do models know when to stop?In our error analysis of the execution trajectories, we observe a prevalent error pattern of early stopping due to the model's conclusion of unachievability.For instance, GPT-4 erroneously identifies 54.9% of feasible tasks as impossible.This issue primarily stems from the UA hint in the instruction, while this hint allows models to identify unachievable tasks, it also hinders performance on achievable tasks.To address this, we conduct an ablation study where we remove this hint.We then break down the success rate for both achievable and unachievable tasks.As shown in Table 2, eliminating this instruction led to a performance boost in achievable tasks, enhancing the overall task success rate of GPT-4 to 14.41%.Despite an overall decline in identifying unachievable tasks, GPT-4 retains the capacity to recognize 44.44% of such tasks.It does so by generating reasons of non-achievability, even without explicit instructions.On the other hand, GPT-3.5 rarely exhibits this level of reasoning.Instead, it tends to follow problematic patterns such as hallucinating incorrect answers, repeating invalid actions, or exceeding the step limits.This result suggests that even subtle differences in instruction design can significantly influence the behavior of a model in performing interactive tasks in complex environments.Can a model maintain consistent performance across similar tasks?Tasks that originate from the same template usually follow similar reasoning and planning processes, even though their observations and executions will differ.We plot a histogram of per-template success rates for our models in Table 3.Of the 61 templates, GPT-4 manages to achieve a 100% task success rate on only four templates, while GPT-3.5 fails to achieve full task completion for any of the templates.In many cases, the models are only able to complete one task variation with a template.These observations indicate that even when tasks are derived from the same template, they can present distinct challenges.For instance, while "Fork metaseq" can be a straightforward task, "Fork all repos from Facebook" derived from the same template requires more repetitive operations, hence increasing its complexity.Therefore, WebArena provide a testbed to evaluate more sophisticated methods.In particular, those that incorporate memory components,
✓ ✓ ✗ ✗ WebArena ✓ ✓ ✓ ✓
Table 4: The comparison between our benchmark and existing benchmarks on grounding natural language instructions to concrete executions.Our benchmark is implemented in our fully interactable highly-realistic environment.It features diverse tasks humans may encounter in their daily routines.We design evaluation metrics to assess the functional correctness of task executions.</p>
<p>enabling the reuse of successful strategies from past experiments Zhou et al. (2022a); Wang et al. (2023).More error analysis with examples can be found in Appendix A.10.</p>
<p>RELATED WORK</p>
<p>Benchmarks for Controlling Agents through Natural Language Controlling agents via natural language in the digital world have been studied in the literature (Branavan et al., 2009;Shi et al., 2017;Liu et al., 2018;Toyama et al., 2021;Deng et al., 2023;Li et al., 2020;Xu et al., 2021).However, the balance between functionality, authenticity, and support for environmental dynamics remains a challenge.Existing benchmarks often compromise these aspects, as shown in Table 4.Some works rely on static states, limiting agents' explorations and functional correctness evaluation (Shi et al., 2017;Deng et al., 2023), while others simplify real-world complexities, restricting task variety (Yao et al., 2022a;Liu et al., 2018).While AndroidEnv (Toyama et al., 2021) replicates an Android setup, it does not guarantee the reproducibility since live Android applications are used.(Kolve et al., 2017;Shridhar et al., 2020;Puig et al., 2018) and extends to gaming environments (Fan et al., 2022;Küttler et al., 2020), where the environment mechanisms often diverge from human objectives.</p>
<p>Interactive Decision-Making Agents Nakano et al. (2021) introduce WebGPT which searches the web and reads the search results to answer questions.Gur et al. (2023) propose a web agent that synthesizes Javascript code for the task executions.Adding a multi-modal dimension, Lee et al. (2023) and Shaw et al. (2023) develop agents that predict actions based on screenshots of web pages rather than relying on the text-based DOM trees.Performing tasks in interactive environments requires the agents to exhibit several capabilities including hierarchical planning, state tracking, and error recovery.</p>
<p>Existing works (Huang et al., 2022;Madaan et al., 2022;Li et al., 2023) observe LLMs could break a task into more manageable sub-tasks (Zhou et al., 2022b).This process can be further refined by representing task executions as programs, a technique that aids sub-task management and skill reuse (Zhou et al., 2022a;Liang et al., 2023;Wang et al., 2023;Gao et al., 2023).Meanwhile, search and backtracking methods introduce a more structured approach to planning while also allowing for decision reconsideration (Yao et al., 2023;Long, 2023).Existing works also incorporate failure recovery, self-correction (Shinn et al., 2023;Kim et al., 2023), observation summarization (Sridhar et al., 2023) to improve execution robustness.The complexity of WebArena presents a unique challenge and opportunity for further testing and improvement of these methods.</p>
<p>CONCLUSION</p>
<p>We present WebArena, a highly-realistic, standalone, and reproducible web environment designed for the development and testing of autonomous agents.WebArena includes fully functional web applications and organic data from popular domains.Additionally, we curate a comprehensive benchmark consisting of 812 examples that focus on mapping high-level natural language intents into concrete web interactions.We also offer outcome-based evaluation that programmatically validate the tasks success.Our experiments show that even GPT-4 only achieves a limited end-to-end task success rate of 14.41%, significantly lagging behind the human performance of 78.24%.These findings underscore the need for future research to focus on enhancing the robustness and efficacy of autonomous agents within WebArena environment.</p>
<p>A APPENDIX</p>
<p>A.1 WEBSITE IMPLEMENTATION Given the selected websites described in §2.2, we make the best attempt to reproduce the functionality of commonly used sites in a reproducible way.To achieve this, we utilized open-source frameworks for the development of the websites across various categories and imported data from their real-world counterparts.For the E-commerce category, we constructed a shopping website with approximately 90k products, including the prices, options, detailed product descriptions, images, and reviews, spanning over 300 product categories.This website is developed using Adobe Magento, an opensource e-commerce platform4 .Data resources were obtained from data from actual online sites, such as that included in the Webshop data dumpYao et al. (2022a).As for the social forum platform, we deployed an open-source software Postmill5 , the open-sourced counterpart of Reddit6 .We sampled from the top 50 subreddits7 .We then manually selected many subreddit for northeast US cities as well as subreddit for machine learning and deep learning-related topics.This manual selection encourages cross-website tasks such as seeking information related to the northeast US on both Reddit and the map.In total, we have 95 subreddits, 127390 posts, and 661781 users.For the collaborative software development platform, we choose GitLab8 .We heuristically simulate the code repository characteristics by sampling at least ten repositories for every programming language: 80% of them are sampled from the set of top 90 percentile wrt stars repos using a discrete probability distribution weighted proportional to their number of stars; the remaining are sampled from the bottom ten percentile set using similar weighted distribution.This is done to ensure fair representation of repos of all kinds, from popular projects with many issues and pull requests to small personal projects.In total, we have 300 repositories and more than 1000 accounts with at least one commit to a repository.For the content management system, we adapted Adobe Magento's admin portal, deploying the sample data provided in the official guide.We employ OpenStreetMap9 for map service implementation, confining our focus to the northeast US region due to data storage constraints.We implement a calculator and a scratchpad ourselves.</p>
<p>Lastly, we configure the knowledge resources as individual websites, complemented with search functionality for efficient information retrieval.Specifically, we utilize Kiwix10 to host an offline version of English Wikipedia with a knowledge cutoff of May 2023.The user manuals for GitLab and Adobe Commerce Merchant documentation are scraped from the official websites.</p>
<p>A.2 ENVIRONMENT DELIVERY AND RESET</p>
<p>One goal for our evaluation environment is ease of use and reproducibility.As a result, we deploy our websites in separate Docker images11 , one per website.The Docker images are fully self-contained with all the code of the website, database, as well as any other software dependencies.They also do not rely on external volume mounts to function, as the data of the websites are also part of the docker image.This way, the image is easy to distribution containing all the pre-populated websites for reproducible evaluation.End users can download our packaged Docker images and run them on their systems and re-deploy the exact websites together with the data used in our benchmarks for their local benchmarking.</p>
<p>Since some evaluation cases may require the agent to modify the data contained in the website, e.g., creating a new user, deleting a post, etc., it is crucial to be able to easily reset the website environment to its initial state.With Docker images, the users could stop and delete the currently running containers for that website and start the container from our original image again to fully reset the environment to the initial state.Depending on the website, this process may take from a few seconds to one minute.However, not all evaluation cases would require an environment reset, as  many of the intents are information gathering and are read-only for the website data.Also, combined with the inference time cost for the agent LLMs, we argue that this environment reset method, through restarting Docker containers from the original images, will have a non-negligible but small impact on evaluation time.</p>
<p>A.3 USER ROLES SIMULATION</p>
<p>Users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories.For instance, within an E-commerce CMS, a shop owner might possess full read and write permissions across all content, whereas an employee might only be granted write permissions for products but not for customer data.We aim to emulate this scenario by generating unique user profiles on each platform.</p>
<p>On the shopping site, we created a customer profile that has over 35 orders within a span of two years.On GitLab, we selected a user who maintains several popular open-source projects with numerous merge requests and issues.This user also manages a handful of personal projects privately.On Reddit, our chosen profile was a user who actively participates in discussions, with many posts and comments.Lastly, on our E-commerce CMS, we set up a user profile for a shop owner who has full read-and-write access to all system contents.All users are automatically logged into their accounts using a pre-cached cookie.To our best knowledge, this is the first publicly available agent evaluation environment to implement such a characteristic.Existing literature typically operates under the assumption of universally identical user roles Shi et al. (2017); Liu et al. (2018); Deng et al. (2023).</p>
<p>A.4 INTENT DISTRIBUTION</p>
<p>The distribution of intents across the websites are shown in Figure 6.</p>
<p>A.5 HUMAN PERFORMANCE</p>
<p>We acknowledge that there may be a difference in human performance when annotators with different demographics are involved.In fact, many tasks in our dataset require domain-specific knowledge.For instance, an average user may not know what a git merge request is; or how to create a product in a complex content management system.We aim to design tasks that have easy-to-imagine outcomes (e.g., a new product page is created) rather than those that are easily performed by an average user without significant domain knowledge.</p>
<p>Figure 11: Two examples where the GPT-4 agent failed, along with their screenshot and the accessibility tree of the relevant sections (grey).On the left, the agent fails to proceed to the "Users" section to accomplish the task of "Fork all Facebook repos"; on the right, the agent repeats entering the same search query even though the observation indicates the input box is filled.</p>
<p>A.10 ADDITIONAL ERROR ANALYSIS</p>
<p>Observation Bias Realistic websites frequently present information on similar topics across various sections to ensure optimal user accessibility.However, a GPT-4 agent often demonstrates a tendency to latch onto the first related piece of information it encounters without sufficiently verifying its relevance or accuracy.For instance, the homepage of the E-Commerce CMS displays the best-selling items based on recent purchases, while historical best-seller data is typically accessed via a separate report.Presented with the task of "What is the top-1 best-selling product in 2022", the GPT-4 agent defaults to leveraging the readily available information on the homepage, bypassing the necessary step of generating the report to obtain the accurate data.</p>
<p>Failures in Observation Interpretation Interestingly, while GPT-4 is capable of summarizing the observations, it occasionally overlooks more granular information, such as the previously entered input.As in the right-hand example of Figure 11,[5172] StaticText indicates that the search term "DMV area" has already been entered.However, the agent disregards this detail and continuously issues the command type [2430] [DMV area] until it reaches the maximum step limit.Furthermore, the agent often neglects the previous action information that is provided alongside the observation.We hypothesize that these observed failures are related to the current pretraining and supervised fine-tuning on dialogues employed in GPT models Ouyang et al. (2022).These models are primarily trained to execute instructions given immediate observations (i.e.,, the dialogue history); thereby, they may exhibit a lack of explorations.Furthermore, in dialogue scenarios, subtle differences in NL expressions often have less impact on the overall conversation.As a result, models may tend to overlook minor variations in their observations.</p>
<p>itinerary to visit all of Pittsburgh's art museums with minimal driving distance starting from Schenley Park.Log the order in my "awesome-northeast-us-travel" repository " "</p>
<p>Figure 3 :
3
Figure 3: We design the observation to be the URL and the content of a web page, with options to represent the content as a screenshot (left), HTML DOM tree (middle), and accessibility tree (right).The content of the middle and right figures are trimmed to save space.</p>
<p>Figure 5 :
5
Figure 5: Example intents from three categories.</p>
<p>Figure 6 :
6
Figure 6: The intent distribution across different websites.Cross-site intents necessitate interacting with multiple websites.Notably, regardless of the website, all user intents require interactions with multiple web pages.</p>
<li> <div> <a href="..."><img src="..."></a> <div class> <a href="...">Outdoor Patio … </a> <div> <span>Rating:</span> <div> <span>82%</span> </div> <a href="…#reviews">12 <span>Reviews</span></a> RootWebArea 'Patio, Lawn ..' link 'Image' img 'Image' link 'Outdoor Patio..' LayoutTable '' StaticText 'Rating:' generic '82%' link '12 Reviews' StaticText '$49.99' button 'Add to Cart' focusable: True button 'Wish List' focusable: … button 'Compare' focusable: …


Table 1 :
1
We introduce two evaluation approaches.r info (top) measures the correctness of performing information-seeking tasks.It compares the predicted answer â with the annotated reference a * with three implementations.r prog (bottom) programmatically checks whether the intermediate states during the executions possess the anticipated properties specified by the intent.
Function IDIntentEval Implementation1Tell me the name of the customer who has the most cancellations in the history exact_match(â, "Samantha Jones")r info (a  *  , â)2Find the customer name and email with phone number 8015551212must_include(â, "Sean Miller") must_include(â, "sean@gmail.com")3Compare walking and driving time from AMC Waterfront to Randylandfuzzy_match(â, "walking: 2h58min") fuzzy_match(â, "driving: 21min")4Checkout merge requests assigned to meurl=locate_current_url(s) exact_match(URL, "gitlab.com/merge_ requests?assignee_username=byteblaze")r prog (s)5Post to ask "whether I need a car in NYC"url=locate_latest_post_url(s) body=locate_latest_post_body(s) must_include(URL, "/f/nyc")must_include(body,"a car in NYC")

Table 2 :
2
The end-to-end task success rate (SR %) on WebArena with different prompting strategies.CoT: the model performs step-by-step reasoning before issuing the action.UA hint: ask the model to stop when encountering unachievable questions.
The main results are shown on the top ofTable 2. GPT-4 (OpenAI, 2023) with CoTprompting achieves a modest end-to-end tasksuccess rate of 11.70%, which is signifi-cantly lower than the human performanceof 78.24%. GPT-3.5 (OpenAI, 2022) withCoT prompting is only able to successfullyperform 8.75% of the tasks. The explicitreasoning procedure is somewhat helpful, itbrings 2.34% improvement over the versionwithout it. Further, TEXT-BISON-001

Table 3 :
3
Distribution of success rates on templates with ≥ 1 successful executions on

GPT models (no UA hint).

https://gitlab.com/gitlab-org/gitlab
https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree
https://chat.openai.com/
https://github.com/magento/magento2
https://postmill.xyz/
https://www.reddit.com/
https://redditlist.com/sfw.html
https://gitlab.com/gitlab-org/gitlab
https://www.openstreetmap.org/
https://www.kiwix.org/en/
https://www.docker.com/
ACKNOWLEDGEMENTWe would like to thank Emmy Liu, Zhiruo Wang, Zhitong Guo for examining our annotations, Shunyu Yao for providing the raw Amazon product data in Webshop, Pengfei Liu, Zaid Sheikh and Aman Madaan for the helpful discussions.We are also grateful to the Center for AI Safety for providing computational resources.This material is partly based on research sponsored in part by the Air Force Research Laboratory under agreement number FA8750-19-2-0200.The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government.This project was also partially supported by a gift from AWS AI.CoT UA Hint Model SR ✓ ✗ GPT-3.5 6.28Table5: The task success rate (SR %) of GPT-3.5-TURBO-16K-0613 with temperature 0.0.Table6: The accuracy (%) of two versions of GPT-4 on judging if dates and time duration of different formats are equivalent.DatasetA.6 EXPERIMENT CONFIGURATIONSWe experiment with GPT-3.5-TURBO-16K-0613,GPT-4-0613, and TEXT-BISON-001 with a temperature of 1.0 and a top-p parameter of 0.9.The maximum number of state transitions is set to 30.We halt execution if the same action is repeated more than three times on the same observation or if the agent generates three consecutive invalid actions.These situations typically indicate a high likelihood of execution failure and hence warrant early termination.For TEXT-BISON-001, we additionally allow ten retries until it generates a valid action.Primarily, we use a high temperature of 1.0 to encourage the exploration.To aid replicating the results, we provide the results of GPT-3.5-TURBO-16K-0613 with temperature 0.0 in Table5and the execution trajectories in our code repository.A.7 PROMPT FOR F U Z Z Y_M A T C HHelp a teacher to grade the answer of a student given a question.Keep in mind that the student may use different phrasing or wording to answer the question.The goal is to evaluate whether the answer is semantically equivalent to the reference answer.question: {{intent}} reference answer: {{reference answer}} all the string 'N/A' that you see is a special sequence that means 'not achievable' student answer: {{prediction}} Conclude the judgement by correct/incorrect/partially correct.Predictions that are judged as "correct" will receive a score of one, while all other predictions will receive a score of zero.A.8 THE ACCURACY OF FUZZY MATCH FUNCTIONTo evaluate this, we manually checked 40 examples and found that 39 of them are identical to our human judgment.In addition, among the 82 examples that require using GPT-4 for evaluation, the answer of 49 (60%) examples is a date (e.g., 10/23/2022) or time duration (e.g., 15 minutes).In these cases, GPT-4 is only used to judge the different format of the answers.We quantitatively evaluate the correctness of GPT-4 in this case by generating different formats of a date and time duration programmatically.We randomly sample negative examples.For instance,Nov 3, 2022, November 3, 2022, 3rd November 2022, 3 Nov 2022, 2022-11-03, and 3rdof November, 2022 are all correct variances of 2022/11/03.The accuracy of GPT-4 is shown in Table6.We can see that two versions of GPT-4 are extremely accurate, both achieving 100% accuracy.A.9 THE PROMPTS OF THE BASELINE WEB AGENTSThe system message of the reasoning agent for both GPT-3.5 and GPT-4 is in Figure7, and two examples are in Figure8.The system message of the direct agent for GPT-3.5 is in Figure9and the two examples are in Figure10.UA hint refers to the instruction of " If you believe the task is You are an autonomous intelligent agent tasked with navigating a web browser.You will be given web-based tasks.These tasks will be accomplished through the use of specific actions you can issue.Here's the information you'll have: The user's objective: This is the task you're trying to complete.The current web page's accessibility tree: This is a simplified representation of the webpage, providing key information.4. Generate the action in the correct format.Start with a "In summary, the next action I will perform is" phrase, followed by action inside ``````.For example, "In summary, the next action I will perform is ```click [1234]```". 5. Issue stop action when you think you have achieved the objective.Don't generate anything after stop."""Figure7: The system message of the reasoning agent.This message has a general explanation of the task, the available actions, and some notes on avoiding common failures.impossible to complete, provide the answer as "N/A" in the bracket.".We remove this sentence in our ablation studies.You are an autonomous intelligent agent tasked with navigating a web browser.You will be given web-based tasks.These tasks will be accomplished through the use of specific actions you can issue.Here's the information you'll have: The user's objective: This is the task you're trying to complete.The current web page's accessibility tree: This is a simplified representation of the webpage, providing key information.To be successful, it is very important to follow the following rules: To be successful, it is very important to follow the following rules: 1.You should only issue an action that is valid given the current observation 2.You should only issue one action at a time.3. Generate the action in the correct format.Always put the action inside a pair of ```.For example, ```click [1234]``4 .Issue stop action when you think you have achieved the objective.Don't generate anything after stop.""" Figure9: The system message of the direct agent.This message has the general explanation of the task, the available actions and some notes on avoiding common failures.
Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian D Reid, Stephen Gould, Anton Van Den, Hengel, 10.1109/CVPR.2018.003872018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018. Salt Lake City, UT, USAIEEE Computer SocietyJune 18-22, 2018. 2018

. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, Yaguang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham ; Bryan, Parker Richter, Alex Riley, Aurko Castro Ros, Brennan Roy, Rajkumar Saeta, Renee Samuel, Ambrose Shelby, Daniel Slone, David R Smilkov, Daniel So, Simon Sohn, Dasha Tokumine, Vijay Valter, Kiran Vasudevan, Xuezhi Vodrahalli, Pidong Wang, Zirui Wang, Tao Wang, John Wang, Yuhuai Wieting, Kelvin Wu, Yunhan Xu, Linting Xu, Pengcheng Xue, Jiahui Yin, Qiao Yu, Steven Zhang, Ce Zheng, Weikang Zheng, Denny Zhou, Zhou, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,2023Slav Petrovand Yonghui Wu. Palm 2 technical report

Benchmarking hierarchical script knowledge. Yonatan Bisk, Jan Buys, Karl Pichotta, Yejin Choi, 10.18653/v1/N19-1412Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191

Reinforcement learning for mapping instructions to actions. S R K Branavan, Harr Chen, Luke Zettlemoyer, Regina Barzilay, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLPSuntec, SingaporeAssociation for Computational Linguistics2009

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, Openai gym. 2016

Evaluating large language models trained on code. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, abs/2107.033742021ArXiv preprint

Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su, 2023

Building open-ended embodied agents with internet-scale knowledge. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An, Yuke Huang, Anima Zhu, Anandkumar, Minedojo, Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022

Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023

IQA: visual question answering in interactive environments. Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi, 10.1109/CVPR.2018.004302018 IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USAIEEE Computer Society2018. June 18-22, 2018. 2018

A real-world webagent with planning, long context understanding, and program synthesis. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust, arXiv:2307.128562023arXiv preprint

Language models as zeroshot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning, ICML 2022. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, Sivan Sabato, Baltimore, Maryland, USAPMLRJuly 2022. 2022162of Proceedings of Machine Learning Research

CraftAssist Instruction Parsing: Semantic Parsing for a Minecraft Assistant. Yacine Jernite, Kavya Srinet, Jonathan Gray, Arthur Szlam, ArXiv preprint. 1905.01978. 2019

Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, abs/2303.17491, 2023ArXiv preprint

Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv. 2017

The nethack learning environment. Heinrich Küttler, Nantas Nardelli, Alexander H Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, Tim Rocktäschel, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Hugo Larochelle, Marc ' , Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, Hsuan-Tien Lin, NeurIPS2020. 2020. December 6-12, 2020, virtual, 2020

Natural questions: A benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, 10.1162/tacl_a_00276Transactions of the Association for Computational Linguistics. 72019

A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Yann Lecun, Open Review. 622022

Pix2struct: Screenshot parsing as pretraining for visual language understanding. Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova, International Conference on Machine Learning. PMLR2023

Take a break in the middle: Investigating subgoals towards hierarchical script generation. Xinze Li, Yixin Cao, Muhao Chen, Aixin Sun, abs/2305.109072023ArXiv preprint

Mapping natural language instructions to mobile UI action sequences. Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, Jason Baldridge, 10.18653/v1/2020.acl-main.729Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020

Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023

Reinforcement learning on web interfaces using workflow-guided exploration. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, Percy Liang, 6th International Conference on Learning Representations, ICLR 2018. Vancouver, BC, CanadaApril 30 -May 3, 2018. 2018Conference Track Proceedings. OpenReview.net

Large language model guided tree-of-thought. Jieyi Long, abs/2305.082912023ArXiv preprint

Language models of code are few-shot commonsense learners. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022

Tell me dave: Context-sensitive grounding of natural language to manipulation instructions. Jaeyong Dipendra K Misra, Kevin Sung, Ashutosh Lee, Saxena, The International Journal of Robotics Research. 351-32016

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. 2021arXiv preprint

Optimizing language models for dialogue. Openai, Chatgpt, OpenAI. Gpt-4 technical report. arXiv. 2022. 2023

Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235

Virtualhome: Simulating household activities via programs. Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba, 10.1109/CVPR.2018.008862018 IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USAIEEE Computer Society2018. June 18-22, 2018. 2018

SQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 10.18653/v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational Linguistics2016

Know what you don't know: Unanswerable questions for SQuAD. Pranav Rajpurkar, Robin Jia, Percy Liang, 10.18653/v1/P18-2124Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20182Short Papers)

Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, Kristina Toutanova, arXiv:2306.00245From pixels to ui actions: Learning to follow instructions via graphical user interfaces. 2023arXiv preprint

World of bits: An open-domain platform for web-based agents. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, Percy Liang, Proceedings of the 34th International Conference on Machine Learning. Doina Precup, Yee Whye Teh, the 34th International Conference on Machine LearningSydney, NSW, AustraliaPMLR2017. 6-11 August 2017. 201770of Proceedings of Machine Learning Research

Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, abs/2303.113662023ArXiv preprint

ALFRED: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, 10.1109/CVPR42600.2020.010759th International Conference on Learning Representations, ICLR 2021, Virtual Event. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew J Hausknecht, Seattle, WA, USA; AustriaIEEEJune 13-19, 2020. 2020. May 3-7, 2021. 202120202020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. OpenReview.net

Abishek Sridhar, Robert Lo, Frank F Xu, Hao Zhu, Shuyan Zhou, arXiv:2305.14257Hierarchical prompting assists large language model on web navigation. 2023arXiv preprint

Androidenv: A reinforcement learning platform for android. Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, Doina Precup, preprint, abs/2105.132312021

Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, abs/2305.162912023ArXiv preprint

Execution-based evaluation for open-domain code generation. Zhiruo Wang, Shuyan Zhou, Daniel Fried, Graham Neubig, abs/2212.104812022ArXiv preprint

Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235

Grounding open-domain instructions to automate web support tasks. Nancy Xu, Sam Masling, Michael Du, Giovanni Campagna, Larry Heck, James Landay, Monica Lam, 10.18653/v1/2021.naacl-main.80Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2021

HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018

Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, abs/2207.012062022a

React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, abs/2210.036292022bArXiv preprint

Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, abs/2305.106012023ArXiv preprint

Seq2sql: Generating structured queries from natural language using reinforcement learning. Victor Zhong, Caiming Xiong, Richard Socher, abs/1709.001032017. 2017ArXiv preprint

Hierarchical control of situated agents through natural language. Shuyan Zhou, Pengcheng Yin, Graham Neubig, 10.18653/v1/2022.suki-1.8Proceedings of the Workshop on Structured and Unstructured Knowledge Integration (SUKI). the Workshop on Structured and Unstructured Knowledge Integration (SUKI)Seattle, USAAssociation for Computational Linguistics2022a

OBJECTIVE: What is the price of HP Inkjet Fax Machine PREVIOUS ACTION: None example_assistant Let's think step-by-step. This page lists the information of HP Inkjet Fax Machine, which is the product identified in the objective. Its price is $279.49. I think I have achieved the objective. I will issue the stop action with the answer. In summary, the next action I will perform is ```stop [$279.49]``è xample_user OBSERVATION: [164] textbox 'Search' focused: True required: False [171] button 'Go' [174] link 'Find directions between two points' [212] heading 'Search Results. Shuyan Zhou, Li Zhang, Yue Yang, Qing Lyu, Pengcheng Yin, Chris Callison-Burch, Graham Neubig, 10.18653/v1/2022.acl-long.214279.49Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022b1F igure 10: The two examples provided as example_user and example_assistant for the direct agent. The agent directly emits the next action given the observation. [2430] searchbox 'Search query' [5172] StaticText 'DMV area' [2361] link 'Projects 0' [2365] link 'Users 1' [2070] heading " We couldn't find any projects matching Facebook            </div>
        </div>

    </div>
</body>
</html>
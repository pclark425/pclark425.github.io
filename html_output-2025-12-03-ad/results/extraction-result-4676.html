<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4676 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4676</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4676</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-267500377</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.03610v1.pdf" target="_blank">RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents</a></p>
                <p><strong>Paper Abstract:</strong> Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4676.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4676.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAP-ALFWorld-GPT3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Planning (RAP) with GPT-3.5 on ALFWorld</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RAP is a retrieval-augmented planning framework that stores episodic logs (task, plan, trajectory) and retrieves contextually-similar past executions to guide an LLM Executor via in-context learning; evaluated here with GPT-3.5 on the ALFWorld text-based interactive benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAP + GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RAP framework (Memory, Reasoner, Retriever, Executor) using GPT-3.5 as the LLM Reasoner/Executor. Reasoner produces retrieval keys and plans; Retriever selects past episodic logs; Executor uses retrieved windows as in-context examples to generate next actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented episodic memory (external DB of episode logs, multimodal-capable)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory stores logs Li = {Ti, pi, τLi} containing task info, overall plan, and trajectories (plans, actions, observations). Retrieval uses embedding similarity (sentence-transformers for text, CLIP-based ViT for images) with a weighted score across task, overall-plan, and retrieval-key match; returns a windowed trajectory around the most similar action as in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>multi-step interactive text-based planning and acting</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>ALFWorld: synthetic multi-step text-game (134 unseen games) with tasks like Pick, Clean, Heat, Cool, Look, Pick2; agent issues text actions, receives textual (and optional image) observations; evaluated with recursive trials up to depth 3.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>85.8% success rate (RAP with GPT-3.5, dmax=3); RAPtrain (memory constructed from training set) reported 91.0% success</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>52.2% success rate (ReAct GPT-3.5 baseline, dmax=3, reported in paper table)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Using retrieval-augmented episodic memory (RAP) yields a large improvement on ALFWorld with GPT-3.5: +33.6 percentage points vs ReAct (52.2%→85.8%). Constructing memory from a training set and using RAPtrain further improved to 91.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires a database of successful episodes (memory collection cost). Retrieval requires careful weighting of task/plan/key components and selection of a window around the matched action to avoid noisy/full trajectories. Paper notes adaptive calibration per environment (e.g., task similarity weight higher in constrained task spaces). No explicit failure-rate breakdown provided beyond examples where baseline fails.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Episodic retrieval of successful past executions as in-context examples substantially improves multi-step interactive planning; windowed trajectory retrieval and multimodal (image) representations can further improve retrieval quality; memory constructed with one model (see transfer) can be effective when re-used by other models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4676.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4676.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAP-WebShop-GPT3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Planning (RAP) with GPT-3.5 on WebShop</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RAP applied to web-interaction product-selection tasks (WebShop) where memory retrieval includes intra-task scenario relations and product-category retrieval to guide actions on a web UI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAP + GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same RAP pipeline with Retriever adapted to WebShop specifics (extracts experiences and actions around matched actions; also models intra-task scenario relationships to map memory actions to current scenario).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented episodic memory with intra-task and product-category retrieval heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory logs of prior successful web interactions; Retriever extracts three experiences and five actions before/after most similar action; additionally considers how actions relate to their original scenarios (intra-task relational mapping) and product categories to better match current web scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>web interaction product selection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>WebShop: web shopping simulator with 1.18M Amazon products; agents must follow an instruction to find/select a product via web interactions (search, click), evaluated over 100 instructions; recursive trials allowed up to depth 3.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WebShop</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>48.0% success rate and 76.1% overall reward score (RAP with GPT-3.5, dmax=3)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>35.0% success rate and 61.8% overall reward score (ReAct / Reflexion reported baselines with GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>RAP improves WebShop success rate by 13.0 percentage points (35.0%→48.0%) and overall reward by ~14.3 points (61.8%→76.1%) compared to ReAct/Reflexion baselines reported with GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>WebShop actions are highly scenario-dependent; requires Retriever to model intra-task relationships (mapping memory action→scenario) which adds complexity. Memory usefulness depends on availability of scenario-correlated examples; building such memory may be costly.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>In domain-specific interactive tasks where actions depend on scenario context (web UI), augmenting retrieval with intra-task relational signals and product-category-aware retrieval improves alignment of retrieved examples and downstream action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4676.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4676.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAP-Franka-LLaVA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Planning (RAP) with LLaVA on Franka Kitchen</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RAP used to augment a vision-language model (LLaVA) for embodied multimodal tasks (Franka Kitchen), providing visual-contextual episodic memories to improve sequential manipulation planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAP + LLaVA (vision-language model)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied multimodal agent using LLaVA (Vicuna-based VLM) as language/vision backbone; RAP Retriever matches current visual observations/plans to stored multimodal episodic logs, and Executor uses retrieved visual-textual windows as in-context examples to produce high-level actions mapped to low-level policy network.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>multimodal retrieval-augmented episodic memory (images + text trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory stores visual observations (images) and text/action trajectories from successful embodied episodes. Retriever computes visual similarity using CLIP-based ViT embeddings and selects trajectory windows centered on most similar visual action/observation for in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>embodied multimodal manipulation planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Franka Kitchen: set of compound kitchen manipulation tasks requiring visual perception and sequential interaction; evaluation uses few-shot learned policy network to map high-level plans to low-level controls, multiple camera views, and repeated trials.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Franka Kitchen</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>61.6% average success rate (LLaVA with RAP)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>43.4% average success rate (LLaVA baseline without RAP)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>RAP provides an absolute improvement of 18.2 percentage points for LLaVA on Franka Kitchen (43.4%→61.6%), demonstrating significant benefit of multimodal episodic retrieval for embodied sequential decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires storing and retrieving visual trajectories; must align retrieved high-level plans to low-level policy via a learned policy network (trained with limited demonstrations). Memory selection and windowing must avoid noisy or irrelevant visual context.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Storing and retrieving multimodal (image+text+action) episodic experiences enables VLM-based agents to better plan sequential embodied interactions; visual similarity-based retrieval (CLIP embeddings) is effective for locating pertinent past executions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4676.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4676.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAP-MetaWorld-CogVLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Planning (RAP) with CogVLM on Meta-World</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RAP augments CogVLM (a large vision-language model) to improve manipulation skill success rates on the Meta-World suite by retrieving visual+textual episodic examples during planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAP + CogVLM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Vision-language model agent (CogVLM) augmented with RAP; uses multimodal memory retrieval to provide in-context examples of successful manipulations that the Executor uses to generate high-level plans mapped to a policy network.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>multimodal retrieval-augmented episodic memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory contains logged successful episodes (visual observations, action sequences, plans). Retriever uses CLIP-style visual embeddings and textual embeddings to compute weighted similarity, returning a focused window of the trajectory for in-context prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>robotic fine manipulation skills</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Meta-World: suite of 50 robotic manipulation skills; evaluated on 5 selected tasks (assemble, button, drawer, hammer, bin) with multiple camera views and repeated trials; few-shot policy network maps high-level outputs to environment actions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Meta-World</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>68.8% average success rate (CogVLM with RAP)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>61.5% average success rate (CogVLM baseline without RAP)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>RAP yields an improvement for CogVLM on Meta-World (61.5%→68.8%), indicating retrieval-augmented multimodal memory consistently benefits different VLM backbones across embodied benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Mapping retrieved high-level plans to low-level control requires a policy network trained from demonstrations; performance depends on quality and representativeness of stored episodes and camera views.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Memory-augmented planning transfers to embodied multimodal agents: visual episodic retrieval helps VLMs reason about sequences of actions in physical tasks, improving success rates across manipulation benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4676.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4676.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAP-Ablations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAP retrieval-mode ablations (RAPact, RAPobs, RAPclip) on ALFWorld/WebShop</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation variants of RAP that switch retrieval modality (action-only, observation-only, image-based CLIP retrieval) to test which memory components are most helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAP variants (RAPact, RAPobs, RAPclip)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RAPact: retrieval using action similarity only; RAPobs: retrieval using observation similarity only (adds observation-based retrieval on top of actions); RAPclip: image-based observation retrieval using CLIP embeddings instead of text-converted observations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented episodic memory with modality-specific retrievers</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Each variant restricts the Retriever to different components (actions or observations) or uses raw images + CLIP embeddings for observation similarity; retrieved windows are presented as in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>multi-step interactive tasks (ALFWorld, WebShop)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmark tasks as main experiments; ablations probe retrieval signal choices.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld / WebShop (ablations reported per table)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ALFWorld (dmax=3) success rates reported: RAPact 82.1%, RAPobs 84.3%, RAPclip 86.6%; combined RAP 85.8% (GPT-3.5/Llama2 evaluations reported). WebShop ablations on Llama2-13b: RAPact 33.0% success (68.6 score), RAPobs 33.0% (69.0), RAPintra 34.0% (69.3), RAPcat 35.0% (69.9), combined RAP 36.0% (71.1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ReAct baseline on ALFWorld 52.2% (for comparison); WebShop ReAct on Llama2-13b 31.0% success (64.6 score).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Switching retrieval signals matters: observation-based retrieval (including direct image retrieval via CLIP) can outperform text-converted observations; combining action, observation, intra-task, and category retrieval yields best overall performance on WebShop and ALFWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Choice of retrieval signal requires environment-specific tuning; direct image retrieval can be slightly more effective than textified observations but requires image-capable encoders; more components increases system complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Adaptive retrieval modality (action vs observation vs image) depending on task stage improves retrieval relevance; combining multiple similarity components (task, plan, key) yields best results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4676.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4676.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAP-TransferMemory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAP: Transfer learning via cross-model memory reuse</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstration that episodic memories constructed by one LLM can be reused by a different evaluation model, i.e., memory is model-agnostic to some degree.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAP (memory constructed by one model, used by another)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory databases are constructed from successful episodes produced by one model (e.g., GPT-3.5) and then used by another model (e.g., Llama2-13b) at evaluation time with the same RAP retriever/executor pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>shared retrieval-augmented episodic memory (cross-model reuse)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory constructed with one model's runs (text/image/action logs) is stored; other models retrieve from the same DB using the same embedding-based similarity and use retrieved windows for in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld (transfer experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>ALFWorld evaluation with dmax=1 (single trial) to test effect of memory built by one model when used by another model.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld (transfer experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported (dmax=1): GPT-3.5 baseline without memory: 44.0% success; GPT-3.5 with GPT-3.5-built memory: 63.4% success. Llama2-13b baseline without memory: 20.9% success; Llama2-13b using GPT-3.5-built memory: 27.6% success.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>As above baseline: GPT-3.5 44.0%, Llama2-13b 20.9% (no-memory baselines reported for dmax=1 experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Memory constructed by GPT-3.5 substantially improved GPT-3.5 evaluation and provided nontrivial gains when used by Llama2-13b, demonstrating cross-model utility of episodic memory (though gains are model- and baseline-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Transfer gains vary by model and baseline capability; weaker models (Llama2-13b) benefit less from memory built by stronger models. The transfer experiment used dmax=1 and a small memory set (1000 training samples), so scaling/robustness require further study.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Episodic memories of successful trajectories are at least partly model-agnostic: memories constructed by one LLM can help another LLM/VLM, enabling sharing of experiences across models and possible centralized memory construction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4676.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4676.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion-baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (language agents with verbal reinforcement learning) — baseline comparison</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reflexion is a prior method that requires LLMs to self-reflect on unsuccessful episodes to improve future performance; used as a baseline in this paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A method that has LLMs analyze and self-reflect on failed trajectories to extract lessons/edits to improve future task solving; it operates by generating explicit reflections and using them to update future runs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>self-reflective episodic memory / verbal reflection (task-specific reflective summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Reflexion stores past unsuccessful trajectories and requires the LLM to produce reflective summaries or edits which are then used to adjust future behavior; differs from RAP's direct retrieval of full example trajectories as in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>multi-step interactive planning (ALFWorld, WebShop comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used in comparisons on ALFWorld and WebShop as a baseline that applies reflection-based improvement over trials.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld, WebShop (as reported in paper tables)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported baseline numbers in this paper: ALFWorld (GPT-3.5) Reflexion 74.6% success (vs RAP 85.8%); WebShop (GPT-3.5) Reflexion 35.0% success and 61.8% score.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>RAP outperforms Reflexion in their reported comparisons on ALFWorld and WebShop; unlike Reflexion which requires explicit reflection steps, RAP directly retrieves and reuses successful episodic examples as in-context data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reflexion focuses on self-generated reflections from failed runs which can be task-specific and may require extra compute/steps; the paper highlights that RAP's implicit reuse of diverse successful experiences is a different (and here more effective) approach for generalization across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Direct retrieval of successful episodic examples for in-context learning (RAP) can outperform reflection-based methods (Reflexion) on multi-step planning benchmarks presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>ExpeL <em>(Rating: 1)</em></li>
                <li>ADaPT <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4676",
    "paper_id": "paper-267500377",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "RAP-ALFWorld-GPT3.5",
            "name_full": "Retrieval-Augmented Planning (RAP) with GPT-3.5 on ALFWorld",
            "brief_description": "RAP is a retrieval-augmented planning framework that stores episodic logs (task, plan, trajectory) and retrieves contextually-similar past executions to guide an LLM Executor via in-context learning; evaluated here with GPT-3.5 on the ALFWorld text-based interactive benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAP + GPT-3.5",
            "agent_description": "RAP framework (Memory, Reasoner, Retriever, Executor) using GPT-3.5 as the LLM Reasoner/Executor. Reasoner produces retrieval keys and plans; Retriever selects past episodic logs; Executor uses retrieved windows as in-context examples to generate next actions.",
            "memory_type": "retrieval-augmented episodic memory (external DB of episode logs, multimodal-capable)",
            "memory_description": "Memory stores logs Li = {Ti, pi, τLi} containing task info, overall plan, and trajectories (plans, actions, observations). Retrieval uses embedding similarity (sentence-transformers for text, CLIP-based ViT for images) with a weighted score across task, overall-plan, and retrieval-key match; returns a windowed trajectory around the most similar action as in-context examples.",
            "task_name": "multi-step interactive text-based planning and acting",
            "task_description": "ALFWorld: synthetic multi-step text-game (134 unseen games) with tasks like Pick, Clean, Heat, Cool, Look, Pick2; agent issues text actions, receives textual (and optional image) observations; evaluated with recursive trials up to depth 3.",
            "benchmark_name": "ALFWorld",
            "performance_with_memory": "85.8% success rate (RAP with GPT-3.5, dmax=3); RAPtrain (memory constructed from training set) reported 91.0% success",
            "performance_without_memory": "52.2% success rate (ReAct GPT-3.5 baseline, dmax=3, reported in paper table)",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Using retrieval-augmented episodic memory (RAP) yields a large improvement on ALFWorld with GPT-3.5: +33.6 percentage points vs ReAct (52.2%→85.8%). Constructing memory from a training set and using RAPtrain further improved to 91.0%.",
            "limitations_or_challenges": "Requires a database of successful episodes (memory collection cost). Retrieval requires careful weighting of task/plan/key components and selection of a window around the matched action to avoid noisy/full trajectories. Paper notes adaptive calibration per environment (e.g., task similarity weight higher in constrained task spaces). No explicit failure-rate breakdown provided beyond examples where baseline fails.",
            "key_insights": "Episodic retrieval of successful past executions as in-context examples substantially improves multi-step interactive planning; windowed trajectory retrieval and multimodal (image) representations can further improve retrieval quality; memory constructed with one model (see transfer) can be effective when re-used by other models.",
            "uuid": "e4676.0",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RAP-WebShop-GPT3.5",
            "name_full": "Retrieval-Augmented Planning (RAP) with GPT-3.5 on WebShop",
            "brief_description": "RAP applied to web-interaction product-selection tasks (WebShop) where memory retrieval includes intra-task scenario relations and product-category retrieval to guide actions on a web UI.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAP + GPT-3.5",
            "agent_description": "Same RAP pipeline with Retriever adapted to WebShop specifics (extracts experiences and actions around matched actions; also models intra-task scenario relationships to map memory actions to current scenario).",
            "memory_type": "retrieval-augmented episodic memory with intra-task and product-category retrieval heuristics",
            "memory_description": "Memory logs of prior successful web interactions; Retriever extracts three experiences and five actions before/after most similar action; additionally considers how actions relate to their original scenarios (intra-task relational mapping) and product categories to better match current web scenario.",
            "task_name": "web interaction product selection",
            "task_description": "WebShop: web shopping simulator with 1.18M Amazon products; agents must follow an instruction to find/select a product via web interactions (search, click), evaluated over 100 instructions; recursive trials allowed up to depth 3.",
            "benchmark_name": "WebShop",
            "performance_with_memory": "48.0% success rate and 76.1% overall reward score (RAP with GPT-3.5, dmax=3)",
            "performance_without_memory": "35.0% success rate and 61.8% overall reward score (ReAct / Reflexion reported baselines with GPT-3.5)",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "RAP improves WebShop success rate by 13.0 percentage points (35.0%→48.0%) and overall reward by ~14.3 points (61.8%→76.1%) compared to ReAct/Reflexion baselines reported with GPT-3.5.",
            "limitations_or_challenges": "WebShop actions are highly scenario-dependent; requires Retriever to model intra-task relationships (mapping memory action→scenario) which adds complexity. Memory usefulness depends on availability of scenario-correlated examples; building such memory may be costly.",
            "key_insights": "In domain-specific interactive tasks where actions depend on scenario context (web UI), augmenting retrieval with intra-task relational signals and product-category-aware retrieval improves alignment of retrieved examples and downstream action generation.",
            "uuid": "e4676.1",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RAP-Franka-LLaVA",
            "name_full": "Retrieval-Augmented Planning (RAP) with LLaVA on Franka Kitchen",
            "brief_description": "RAP used to augment a vision-language model (LLaVA) for embodied multimodal tasks (Franka Kitchen), providing visual-contextual episodic memories to improve sequential manipulation planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAP + LLaVA (vision-language model)",
            "agent_description": "Embodied multimodal agent using LLaVA (Vicuna-based VLM) as language/vision backbone; RAP Retriever matches current visual observations/plans to stored multimodal episodic logs, and Executor uses retrieved visual-textual windows as in-context examples to produce high-level actions mapped to low-level policy network.",
            "memory_type": "multimodal retrieval-augmented episodic memory (images + text trajectories)",
            "memory_description": "Memory stores visual observations (images) and text/action trajectories from successful embodied episodes. Retriever computes visual similarity using CLIP-based ViT embeddings and selects trajectory windows centered on most similar visual action/observation for in-context learning.",
            "task_name": "embodied multimodal manipulation planning",
            "task_description": "Franka Kitchen: set of compound kitchen manipulation tasks requiring visual perception and sequential interaction; evaluation uses few-shot learned policy network to map high-level plans to low-level controls, multiple camera views, and repeated trials.",
            "benchmark_name": "Franka Kitchen",
            "performance_with_memory": "61.6% average success rate (LLaVA with RAP)",
            "performance_without_memory": "43.4% average success rate (LLaVA baseline without RAP)",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "RAP provides an absolute improvement of 18.2 percentage points for LLaVA on Franka Kitchen (43.4%→61.6%), demonstrating significant benefit of multimodal episodic retrieval for embodied sequential decision-making.",
            "limitations_or_challenges": "Requires storing and retrieving visual trajectories; must align retrieved high-level plans to low-level policy via a learned policy network (trained with limited demonstrations). Memory selection and windowing must avoid noisy or irrelevant visual context.",
            "key_insights": "Storing and retrieving multimodal (image+text+action) episodic experiences enables VLM-based agents to better plan sequential embodied interactions; visual similarity-based retrieval (CLIP embeddings) is effective for locating pertinent past executions.",
            "uuid": "e4676.2",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RAP-MetaWorld-CogVLM",
            "name_full": "Retrieval-Augmented Planning (RAP) with CogVLM on Meta-World",
            "brief_description": "RAP augments CogVLM (a large vision-language model) to improve manipulation skill success rates on the Meta-World suite by retrieving visual+textual episodic examples during planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAP + CogVLM",
            "agent_description": "Vision-language model agent (CogVLM) augmented with RAP; uses multimodal memory retrieval to provide in-context examples of successful manipulations that the Executor uses to generate high-level plans mapped to a policy network.",
            "memory_type": "multimodal retrieval-augmented episodic memory",
            "memory_description": "Memory contains logged successful episodes (visual observations, action sequences, plans). Retriever uses CLIP-style visual embeddings and textual embeddings to compute weighted similarity, returning a focused window of the trajectory for in-context prompting.",
            "task_name": "robotic fine manipulation skills",
            "task_description": "Meta-World: suite of 50 robotic manipulation skills; evaluated on 5 selected tasks (assemble, button, drawer, hammer, bin) with multiple camera views and repeated trials; few-shot policy network maps high-level outputs to environment actions.",
            "benchmark_name": "Meta-World",
            "performance_with_memory": "68.8% average success rate (CogVLM with RAP)",
            "performance_without_memory": "61.5% average success rate (CogVLM baseline without RAP)",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "RAP yields an improvement for CogVLM on Meta-World (61.5%→68.8%), indicating retrieval-augmented multimodal memory consistently benefits different VLM backbones across embodied benchmarks.",
            "limitations_or_challenges": "Mapping retrieved high-level plans to low-level control requires a policy network trained from demonstrations; performance depends on quality and representativeness of stored episodes and camera views.",
            "key_insights": "Memory-augmented planning transfers to embodied multimodal agents: visual episodic retrieval helps VLMs reason about sequences of actions in physical tasks, improving success rates across manipulation benchmarks.",
            "uuid": "e4676.3",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RAP-Ablations",
            "name_full": "RAP retrieval-mode ablations (RAPact, RAPobs, RAPclip) on ALFWorld/WebShop",
            "brief_description": "Ablation variants of RAP that switch retrieval modality (action-only, observation-only, image-based CLIP retrieval) to test which memory components are most helpful.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAP variants (RAPact, RAPobs, RAPclip)",
            "agent_description": "RAPact: retrieval using action similarity only; RAPobs: retrieval using observation similarity only (adds observation-based retrieval on top of actions); RAPclip: image-based observation retrieval using CLIP embeddings instead of text-converted observations.",
            "memory_type": "retrieval-augmented episodic memory with modality-specific retrievers",
            "memory_description": "Each variant restricts the Retriever to different components (actions or observations) or uses raw images + CLIP embeddings for observation similarity; retrieved windows are presented as in-context examples.",
            "task_name": "multi-step interactive tasks (ALFWorld, WebShop)",
            "task_description": "Same benchmark tasks as main experiments; ablations probe retrieval signal choices.",
            "benchmark_name": "ALFWorld / WebShop (ablations reported per table)",
            "performance_with_memory": "ALFWorld (dmax=3) success rates reported: RAPact 82.1%, RAPobs 84.3%, RAPclip 86.6%; combined RAP 85.8% (GPT-3.5/Llama2 evaluations reported). WebShop ablations on Llama2-13b: RAPact 33.0% success (68.6 score), RAPobs 33.0% (69.0), RAPintra 34.0% (69.3), RAPcat 35.0% (69.9), combined RAP 36.0% (71.1).",
            "performance_without_memory": "ReAct baseline on ALFWorld 52.2% (for comparison); WebShop ReAct on Llama2-13b 31.0% success (64.6 score).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Switching retrieval signals matters: observation-based retrieval (including direct image retrieval via CLIP) can outperform text-converted observations; combining action, observation, intra-task, and category retrieval yields best overall performance on WebShop and ALFWorld.",
            "limitations_or_challenges": "Choice of retrieval signal requires environment-specific tuning; direct image retrieval can be slightly more effective than textified observations but requires image-capable encoders; more components increases system complexity.",
            "key_insights": "Adaptive retrieval modality (action vs observation vs image) depending on task stage improves retrieval relevance; combining multiple similarity components (task, plan, key) yields best results.",
            "uuid": "e4676.4",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RAP-TransferMemory",
            "name_full": "RAP: Transfer learning via cross-model memory reuse",
            "brief_description": "Demonstration that episodic memories constructed by one LLM can be reused by a different evaluation model, i.e., memory is model-agnostic to some degree.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAP (memory constructed by one model, used by another)",
            "agent_description": "Memory databases are constructed from successful episodes produced by one model (e.g., GPT-3.5) and then used by another model (e.g., Llama2-13b) at evaluation time with the same RAP retriever/executor pipeline.",
            "memory_type": "shared retrieval-augmented episodic memory (cross-model reuse)",
            "memory_description": "Memory constructed with one model's runs (text/image/action logs) is stored; other models retrieve from the same DB using the same embedding-based similarity and use retrieved windows for in-context learning.",
            "task_name": "ALFWorld (transfer experiment)",
            "task_description": "ALFWorld evaluation with dmax=1 (single trial) to test effect of memory built by one model when used by another model.",
            "benchmark_name": "ALFWorld (transfer experiment)",
            "performance_with_memory": "Reported (dmax=1): GPT-3.5 baseline without memory: 44.0% success; GPT-3.5 with GPT-3.5-built memory: 63.4% success. Llama2-13b baseline without memory: 20.9% success; Llama2-13b using GPT-3.5-built memory: 27.6% success.",
            "performance_without_memory": "As above baseline: GPT-3.5 44.0%, Llama2-13b 20.9% (no-memory baselines reported for dmax=1 experiment).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Memory constructed by GPT-3.5 substantially improved GPT-3.5 evaluation and provided nontrivial gains when used by Llama2-13b, demonstrating cross-model utility of episodic memory (though gains are model- and baseline-dependent).",
            "limitations_or_challenges": "Transfer gains vary by model and baseline capability; weaker models (Llama2-13b) benefit less from memory built by stronger models. The transfer experiment used dmax=1 and a small memory set (1000 training samples), so scaling/robustness require further study.",
            "key_insights": "Episodic memories of successful trajectories are at least partly model-agnostic: memories constructed by one LLM can help another LLM/VLM, enabling sharing of experiences across models and possible centralized memory construction.",
            "uuid": "e4676.5",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Reflexion-baseline",
            "name_full": "Reflexion (language agents with verbal reinforcement learning) — baseline comparison",
            "brief_description": "Reflexion is a prior method that requires LLMs to self-reflect on unsuccessful episodes to improve future performance; used as a baseline in this paper's comparisons.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "A method that has LLMs analyze and self-reflect on failed trajectories to extract lessons/edits to improve future task solving; it operates by generating explicit reflections and using them to update future runs.",
            "memory_type": "self-reflective episodic memory / verbal reflection (task-specific reflective summaries)",
            "memory_description": "Reflexion stores past unsuccessful trajectories and requires the LLM to produce reflective summaries or edits which are then used to adjust future behavior; differs from RAP's direct retrieval of full example trajectories as in-context examples.",
            "task_name": "multi-step interactive planning (ALFWorld, WebShop comparisons)",
            "task_description": "Used in comparisons on ALFWorld and WebShop as a baseline that applies reflection-based improvement over trials.",
            "benchmark_name": "ALFWorld, WebShop (as reported in paper tables)",
            "performance_with_memory": "Reported baseline numbers in this paper: ALFWorld (GPT-3.5) Reflexion 74.6% success (vs RAP 85.8%); WebShop (GPT-3.5) Reflexion 35.0% success and 61.8% score.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "RAP outperforms Reflexion in their reported comparisons on ALFWorld and WebShop; unlike Reflexion which requires explicit reflection steps, RAP directly retrieves and reuses successful episodic examples as in-context data.",
            "limitations_or_challenges": "Reflexion focuses on self-generated reflections from failed runs which can be task-specific and may require extra compute/steps; the paper highlights that RAP's implicit reuse of diverse successful experiences is a different (and here more effective) approach for generalization across tasks.",
            "key_insights": "Direct retrieval of successful episodic examples for in-context learning (RAP) can outperform reflection-based methods (Reflexion) on multi-step planning benchmarks presented here.",
            "uuid": "e4676.6",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "ExpeL",
            "rating": 1
        },
        {
            "paper_title": "ADaPT",
            "rating": 1
        }
    ],
    "cost": 0.018571749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents
6 Feb 2024</p>
<p>Tomoyuki Kagaya <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#107;&#97;&#103;&#97;&#121;&#97;&#46;&#116;&#111;&#109;&#111;&#121;&#117;&#107;&#105;&#64;&#106;&#112;&#46;&#112;&#97;&#110;&#97;&#115;&#111;&#110;&#105;&#99;&#46;&#99;&#111;&#109;">&#107;&#97;&#103;&#97;&#121;&#97;&#46;&#116;&#111;&#109;&#111;&#121;&#117;&#107;&#105;&#64;&#106;&#112;&#46;&#112;&#97;&#110;&#97;&#115;&#111;&#110;&#105;&#99;&#46;&#99;&#111;&#109;</a> 
Equal contribution</p>
<p>Panasonic Connect Co., Ltd
Japan</p>
<p>Jing Thong <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#106;&#105;&#110;&#103;&#121;&#117;&#97;&#110;&#46;&#116;&#104;&#111;&#110;&#103;&#64;&#115;&#103;&#46;&#112;&#97;&#110;&#97;&#115;&#111;&#110;&#105;&#99;&#46;&#99;&#111;&#109;">&#106;&#105;&#110;&#103;&#121;&#117;&#97;&#110;&#46;&#116;&#104;&#111;&#110;&#103;&#64;&#115;&#103;&#46;&#112;&#97;&#110;&#97;&#115;&#111;&#110;&#105;&#99;&#46;&#99;&#111;&#109;</a> 
Yuan 
Equal contribution</p>
<p>Panasonic R&amp;D Center
Singapore</p>
<p>Yuxuan Lou lou@u.nus.edu&gt;. 
Equal contribution</p>
<p>National University of Singapore
Singapore</p>
<p>Jayashree Karlekar 
Panasonic R&amp;D Center
Singapore</p>
<p>Sugiri Pranata 
Panasonic R&amp;D Center
Singapore</p>
<p>Akira Kinose 
Panasonic Connect Co., Ltd
Japan</p>
<p>Koki Oguri 
Panasonic Connect Co., Ltd
Japan</p>
<p>Felix Wick 
Panasonic R&amp;D Center Ger-many
Germany</p>
<p>Yang You 
National University of Singapore
Singapore</p>
<p>Thong Jing Yuan</p>
<p>RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents
6 Feb 202479DDD7B142ED638CFA471A76298A3560arXiv:2402.03610v1[cs.LG]
Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration.However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges.Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities.RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks.Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks.These results highlight RAP's potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.</p>
<p>Introduction</p>
<p>Recent research has revealed the high inferential abilities of Large Language Models (LLMs) as agents (Wang et al., 2023b;Xi et al., 2023), indicating their potential application in various areas like decision-making tasks and robotic control.Previous works such as ReAct (Yao et al., 2023) have shown that LLMs can generate accurate actions as language agents by iteratively performing actions and reasoning.</p>
<p>In this paper, we introduce a novel framework, Retrieval-Augmented Planning (RAP), which embodies a pivotal human ability -to leverage generalized past experiences for current tasks -and apply it to LLM agents.Our approach involves storing past experiences in memory, retrieving them appropriately based on the similarity with present context including multimodal information, and generating subsequent actions via in-context learning, thereby enhancing the decision-making capacity of language agents.Central to this framework is the LLMs' ability to perform analogy-making from various abstracted patterns (Mirchandani et al., 2023).Leveraging this capability, our memory stores both context and action-observation trajectories for each experience.The approach effectively facilitates deriving correct actions from memory examples within task constraints.Furthermore, by storing multimodal information in memory and considering it when retrieving past experiences, our approach flexibly utilizes multimodal information with LLMs and Vision-Language Models (VLMs) separately for language agents.Consequently, our approach proves to be effective for memory utilization by language agents in both decision-making and robotics tasks, in textual and multimodal environments.Specifically, RAP achieves 33.6%, 13.0%, 18.2%, and 12.7% gain over ReAct on the ALFWorld, Webshop, Franka Kitchen, and Meta World benchmarks respectively.</p>
<p>To summarize, our contributions are as follows:</p>
<p>• We propose RAP, a novel framework that enhances LLM agents' planning capacity.It strategically enriches the decision-making process by storing past experiences and intelligently retrieving them based on their similarity to the current situation.• RAP is capable of being applied not only in textual environments but also in multimodal embodied tasks, marking it as a pioneering effort in employing memory retrieval techniques for multimodal agents, a first in this domain to our knowledge.• We validate the effectiveness of RAP empirically across both textual and multimodal benchmarks.RAP shows significant improvements compared to prior SOTA methods in both types of environments.</p>
<p>Related Work</p>
<p>Language Models and Vision-Language Models as Foundations</p>
<p>Large Language Models (LLMs) such as GPT (OpenAI, 2023) and the LLaMA series (GenAI, 2023), leveraging transformer architecture and self-supervised learning objectives like Next Token Prediction, have excelled in generating coherent, human-like text.These models, pre-trained on extensive text corpora, possess vast linguistic knowledge and reasoning abilities.Extending beyond LLMs, multi-modal domains have given rise to Vision-Language Models (VLMs) (Yin et al., 2023) such as LLaVA (Liu et al., 2023) and CogVLM (Wang et al., 2023c), which integrate textual and visual inputs, exemplified by LLaVA's training on image-caption pairs from the CC3M datasets (Sharma et al., 2018).Our work utilizes these foundations to build agents for textual and embodied environments: text-based agents employing LLMs, and embodied agents integrating VLMs for visual perception and action planning.We focus on enhancing these agents' planning capabilities through memory retrieval techniques, enabling them to selectively access and utilize relevant memory for improved sequential decision-making.</p>
<p>Language Models as AI Agents</p>
<p>Recent works have leveraged LLM's anthropomorphic capabilities when building autonomous agents.These agents can be depicted into having 4 key aspects: Profile (agent characteristics), Memory (past information), Planning (future strategies) and Action (execution policies) (Wang et al., 2023a).A notable example is Chain-of-Thought (CoT) (Wei et al., 2022), where agents are encouraged to mirror human cognitive mechanisms by incorporating reasoning into intermediate steps for complex problem-solving tasks.With a dynamic reasoning process, ReAct (Yao et al., 2023) interleaves generated actions and environmental states, improving the reasoning ability through action-state synergy.</p>
<p>Our work seeks to enhance the ReAct framework by allowing agents to identify specific objects within observations and additionally retrieve relevant aspects of past experiences</p>
<p>Retrieval-Augmented Generation with Memory</p>
<p>Among works (Madaan et al., 2022;Liu et al., 2021;Su et al., 2022) that seek to derive better answers from LLMs via memory, RAG (Lewis et al., 2020) is a notable method that combines retrieval-based mechanisms with generative models.Responses from memory are selected based on similarity and passed into LLMs as additional context to deliver outputs that are creative and contextually-grounded.Building on RAG, Reflexion (Shinn et al., 2023) requires LLMs to self-reflect on unsuccessful tasks for self-improvement in solving tasks over time.ADaPT (Prasad et al., 2023) further decomposes into sub-tasks and re-executes where necessary.Yet, these works only reflect on trajectories within the task.Hence, these insights are often restricted to each task.Building on Reflexion, ExpeL (Zhao et al., 2023) passes all generated experiences into the LLM to reflect in a text-based manner.In contrast, our work adopts a different approach by implicitly drawing from a diverse range of experiences from memory without explicitly requiring an additional step of re-tasking the LLM to extract insights.With this approach, our agent can not only efficiently generalize experiences from other successful tasks to solve the current task, but also be flexible enough to extract relevant components from experiences for current task, enhancing the agent's ability to expand its memory from textual to multimodal contexts.</p>
<p>RAP: Retrieval-Augmented Planning</p>
<p>We developed Retrieval-Augmented Planning (RAP), a framework that leverages past experiences to facilitate decision-making according to the current context.Fig. 2 provides an overview of the framework, which consists of four core components: Memory, Reasoner, Retriever, and Executor.The specific details of each module will be discussed in sections 3.2 to 3.5.</p>
<p>Preliminaries</p>
<p>In this work, we consider an agent operating in a particular environment and assigned with completing some task T.</p>
<p>Memory</p>
<p>To enable retrieval-augmented planning, we first construct memory databases.The databases contain logs of prior successful task executions.For each log L i completing a task T i in H i steps, we record the task information T i , the overall plan p i , the trajectory of the agent τ Li including plans, actions, and observations sequences.
L i = {T i , p i , τ Li } (1) τ Li = { ⃗ p ′ Li , ⃗ α Li , ⃗ o Li }(2)
For textual environments, the observations are textual descriptions of the world state.For multimodal environments, the observations are visual representations -images from a fixed viewpoint camera after each agent action.</p>
<p>The logs are collected by having agents attempt the tasks and saving streams of successful episodes.The episodic logs capture the steps needed to complete the tasks.Storing these examples allows the agents to leverage prior experience when planning for new instances of the tasks.</p>
<p>During interactions with the environment, the agents can selectively retrieve relevant memory samples to make more informed action decisions.For text tasks, the textual logs provide crucial context.For embodiment, prior visual observations reveal outcomes of actions in the space.By retrieving prototypical executions, the agents can plan smarter policies while avoiding past failures.The memory augmentation thus equips the models with vital environmental knowledge for sequential decision-making.</p>
<p>Reasoner</p>
<p>The Reasoner generates overall plans, action plans, and retrieval keys based on the agent's current situation and action trajectory, using LLMs.Initially, the Reasoner produces the overall plan from the task information.Based on the task and the overall plan, the Reasoner generates an action plan.Also, in accordance with ReAct, an action or action plan is dynamically generated by LLMs, considering the current task status.If an action plan is generated, a retrieval key is created based on the generated action plan.For instance, in ALFWorld (Shridhar et al., 2021), if an action plan like "I need to find the watch" is generated, the retrieval key would be "search watch".Hence, the Reasoner enables agents to take into consideration the current situation and context.</p>
<p>Retriever</p>
<p>The Retriever is designed to extract the most relevant memory logs to guide the agent's subsequent actions to complete the current task.This process is shown in Fig. 3 (Left).</p>
<p>The similarity score, comparing the current state S 0 with log L i , is calculated as a weighted average of the task similarity, overall plan alignment, and retrieval key congruence.</p>
<p>Let the current agent task be T 0 , the overall plan be p 0 , and the retrieval key generated by the Reasoner based on current action plan p ′ be k 0 .The similarity score between current state S 0 with log L i is calculated as a weighted average of the similarity score for task, overall plan, and retrieval key.
Score(S 0 , L i ) = w t • sim(T 0 , T Li ) + w p • sim(p 0 , p Li ) + w k • sim(k 0 , τ Li )(3)
Each component's similarity score is determined using cosine similarity of their feature representations.For text data, the representations are derived using sentence-transformers (Reimers &amp; Gurevych, 2019).For images, the representa-tions are generated with a CLIP-based Vision Transformer.</p>
<p>The similarity score between the retrieval key and the log trajectory is adaptive based on environment type and retrieval key type.In multimodal environments, the retrieval key corresponds to agents' current visual observation.Thus, the retrieval-key similarity score is the score between current and logged visual trajectory observations as in equation ( 4).
sim(k 0 , τ Li ) = max(cos sim(k visual 0 , o j )), for o j ∈ ⃗ o Li(4)
In textual environments, the retrieval-key similarity score is adaptive based on key type.In scenarios involving the retrieval key for searching or locating objects, the similarity score is calculated between the retrieval key and the logged textual trajectory observations, as in equation ( 5).
sim(k 0 , τ Li ) = max(cos sim(k text 0 , o j )), for o j ∈ ⃗ o Li (5)
For the case retrieval-key is interacting with object action planning, it is an action similarity score, as in equation ( 6).
sim(k 0 , τ Li ) = max(cos sim(k text 0 , α j )), for α j ∈ ⃗ α Li (6)
Furthermore, component weights are adaptively calibrated based on the environment.In environments with a constrained task space, task similarity is assigned a higher weight, For example, in Franka Kitchen environment which has only 5 tasks.We only retrieve logs with same task type.</p>
<p>For each retrieved experience, only a window of trajectory centered around the most similar action is passed to the agent.This allows agents to focus on actions most similar to current task, rather than full trajectories that may instead create additional noise to the agent.</p>
<p>In summary, our meticulously-crafted retrieval method efficiently identifies the most pertinent logs by calculating a weighted similarity score that takes into account various aspects including task information, overall planning, and retrieval key.This process ensures that the most relevant and contextually appropriate logs are selected from a vast repository of memory logs.Once these logs are retrieved, they serve as an invaluable resource for the large language model serving as Executor.</p>
<p>Executor</p>
<p>The Executor receives past experiences from the Retriever and generates the next action by utilizing these experiences through in-context learning.This process is illustrated in Fig. 3 (Right).By presenting the past experience aligned with the current context as a prompt, it enables accurate decisionmaking for the next action, mirroring the process humans leverage past experiences for future actions.Additionally, the length of the current task trajectory is used in the same way as past experiences, utilizing only a constant number of new trajectories.This encourages effective analogy-making from experiences through in-context learning in LLMs.</p>
<p>Experiments</p>
<p>To validate the effectiveness of our framework in various environments, we performed evaluations on four benchmarks.These include the text-based multi-step tasks in ALFWorld (Shridhar et al., 2021) and Webshop (Yao et al., 2022), and robotics tasks, FrankaKitchen (Gupta et al., 2019) and Meta-World (Yu et al., 2021), which are multimodal environments with texts and images.ALFWorld (Shridhar et al., 2021) is a synthetic text-based game that challenges an agent to solve multi-step tasks in a variety of interactive environments based on TextWorld (Côté et al., 2018).Following ReAct, we evaluated an agent in 134 unseen games, including six types of tasks: Pick, Clean, Heat, Cool, Look, and Pick2.In this environment, agents are required to accomplish complex tasks using textbased actions in a simulated household providing textual and image feedback.Following previous works (Yao et al., 2023;Shinn et al., 2023;Prasad et al., 2023), RAP runs recursively until it reaches a depth (trial) of 3. In RAP train , we use 1000 tasks from the provided training set, and run recursively with memory from successful tasks both from the training set and previous trials.Additionally, we use task information including the task type for the Retriever.During action-based similarity calculation, it extracts four experiences and ten actions from both before and after the most similar action.Meanwhile, during observation-based similarity calculation, it uses eight experiences and five actions from both before and after the most similar action.</p>
<p>We conducted evaluations using three models: GPT-3.5 (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), andLlama2-13b (GenAI, 2023).In Table 1, our experiments with GPT-3.5 show that RAP(85.8%) and RAP train (91.0%) achieve a significantly higher success rate compared to previous works such as ReAct(52.2%),Reflexion(74.6%),and ADaPT(71.6%).Further, RAP is also effective even with high-performance models like GPT-4, and locally-operated models such as Llama2-13b, thus illustrating the efficacy of our method across various LLMs.</p>
<p>Figure 4 highlights the progression of improvements to success rate over three trials, where RAP shows higher success rates compared to ReAct, thus indicating the effective utility of successful experiences from other tasks in memory.In addition, RAP with GPT-3.5 achieves performance equivalent * We use the performance reported by (Prasad et al., 2023) to ReAct with GPT-4.Also, RAP with memory built from the training set via GPT-3.5 surpasses ReAct with GPT-4.</p>
<p>WEBSHOP</p>
<p>WebShop (Yao et al., 2022) is a web application that simulates online shopping, where agents are required to select products for purchase based on a given user instruction.</p>
<p>WebShop contains a total of 1.18M real-world products featured on Amazon, and comprises a wide variety of structured and unstructured texts.Following Reflexion (Shinn et al., 2023) and ADaPT (Prasad et al., 2023), we evaluated an agent across 100 instructions.For each instruction, agents are required to reason and select a desired product that is most aligned to the given instruction based on observations returned by the web application, and perform additional precise interactions with the portal to navigate through the web application such as searching or clicking buttons.Such interactions are performed in a text-based manner where the agent issues a textual command into the web application.Following previous studies, we allow the agent to run recursively until it reaches a depth (trial) of 3.</p>
<p>During our evaluation, we ran the initial trial with a Re-Act agent and formulated the memory database based on successful tasks.The memory database would be further expanded for subsequent trials based on successful tasks in the preceding trials.Here, successful tasks are counted as those with a reward of 1.In addition, during retrieval of actions in memory, the Retriever extracts three experiences and five actions from before and after the most similar action.</p>
<p>Moreover, unlike other environments where objects are generalizable across different tasks, WebShop has an additional unique feature where actions in each task are highly dependent on the scenario outlined in that task.As such, apart from the correlation between the current reasoning and trajectories in memory, our agent also considers the relationship between the action of each task in memory that is most similar to the current action and its corresponding scenario for that task.This builds on the concept of "A is to B as C is to D", where the generated action depends not only on similar trajectories in memory, but also how these trajectories relate back to their scenario, and how the current trajectory should be related to the current scenario.By incorporating intra-task relationships, this allows the agent to better reason how the actions in memory are correlated with their own scenarios, and thereafter generate an action that is also aligned to the current scenario at hand.</p>
<p>We performed evaluations using two different models: GPT-3.5 (Ouyang et al., 2022) and Llama2-13b (GenAI, 2023).</p>
<p>In Table 2, experiments with GPT-3.5 demonstrate that our method (48.0%) achieves a higher success rate compared to previous studies such as ReAct (35.0%),Reflexion (35.0%), and ADaPT (43.0%).Furthermore, our method is able to achieve a higher overall reward score (76.1%) as compared to ReAct (61.8%),Reflexion (61.8%) and ADaPT (64.0%).</p>
<p>Multimodal Environments</p>
<p>We evaluated our proposed technique in embodied multimodal agents on two benchmark environments: Franka Kitchen and Meta-World.These simulations offer a diverse set of household and robotic manipulation tasks requiring visual perception and physical interaction.</p>
<p>We constructed embodied agents using two VLM foundations -LLaVA and CogView.For each VLM, we compared task performance of the base model to a RAP-enhanced agent utilizing our memory retrieval system.</p>
<p>The Franka Kitchen benchmark consists of compound tasks like arranging objects and preparing meals.Meta-World provides a suite of 50 distinct robotic skills focused on fine manipulation.For both sets, the agent must plan actions based on visual observations in an interactive 3D environment.We report quantitative results on task success rates with and without RAP augmentation.Our method allows the VLM Agents to selectively reference prior successful executions during planning.This provides vital visual context and demonstrates the benefits of memory-augmented reasoning for embodied agents.</p>
<p>To map the high-level plans of the VLM agents to executable environment actions, we train a policy network on 25 demonstrations for each task.We evaluate on 5 subtasks with 2 different camera views per benchmark.For each (task, view) combination we run 50 trials with different random seeds and report success rates.</p>
<p>Table 3 and Figure 6 shows that RAP can significantly enhance embodied multimodal agents planning on both benchmarks.The results offer insights into how memory can aid these models for sequential decision making and embodied tasks requiring interactive visual perception.</p>
<p>Ablation Study</p>
<p>In this section, we provide additional experimental results and showcase some critical components of experiences that are required to be stored into memory.</p>
<p>Evaluation across various Retrievers</p>
<p>ALFWORLD EVALUATION</p>
<p>We perform evaluation on ALFWorld with GPT-3.5 by varying the Retriever, as shown in Table 4.The results of RAP act and RAP obs illustrate the effectiveness of switching the information used for retrieval depending on the situation.</p>
<p>Furthermore, we utilize visual observation provided by ALF-World instead of textual observation, and perform an evaluation using similarity between textual retrieval key and image observation with CLIP (Radford et al., 2021).As a result, slightly better performance is demonstrated than when using text observation information.This suggests that employing direct image data, rather than information converted into text, could enable more effective retrieval.Here, all evaluations are performed on Llama2-13b.</p>
<p>As shown in Table 5, through RAP obs , the agent is able to retrieve trajectories from memory based on either actions or observations, depending on the current stage of solving the task.With the incorporation of intra-task similarity in RAP intra , the agent is able to align the relationship between task information and the corresponding trajectories of each experience when projecting to the current task.By retrieving based on product category in RAP cat , the agent is able to retrieve experiences that are more related to the current task.</p>
<p>Overall, RAP takes into account these components, resulting in an overall boost of 6.5% and 5.0% for overall reward and success rate respectively on Llama2-13b.With these, RAP also demonstrates a boost of 14.3% and 13.0% for overall reward and success rate respectively on GPT-3.5 in Table 2, showcasing RAP's generalizability across different models.</p>
<p>Transfer Learning via Memory</p>
<p>RAP is capable of utilizing past experiences that are stored in memory.Since the experience of solving tasks is independent of the model, the model used for evaluation does not need to match the one used for memory construction.Here, we illustrate a verification of transfer learning between models by using memory constructed via different models for the evaluation model.From Section 4.1.1,we use 1000 samples from training data, but no recursive trial is conducted (d max = 1) to simply verify the effect of transfer learning.</p>
<p>Table 6 shows results of transfer learning, which indicate memory generated with GPT-3.5 is also effective in Llama2-13b.Thus, RAP allows sharing experiences across models.</p>
<p>Table 6.ALFWorld success rate(%) with Memory and dmax=1.</p>
<p>ModelMemory indicates the language model used to construct memory from the training data.</p>
<p>Model</p>
<p>ModelMemory Success Rate GPT-3.5 -44.0 GPT-3.5 GPT-3.5 63.4 Llama2-13b -20.9 Llama2-13b GPT-3.5 27.6</p>
<p>Conclusion</p>
<p>We propose Retrieval-Augmented Planning (RAP), which stores past experiences, extracts pertinent experiences from multimodal information such as text and images, and guides subsequent actions.Our framework demonstrated superior performance compared to baseline methods in various LLMs and across four diverse agent and robotics benchmarks.Through these results, our framework enables language agents to flexibly utilize past experiences in accordance with current situations, mirroring a human ability, and thereby enhancing decision-making capabilities.</p>
<p>Impact Statements</p>
<p>This paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p>
<p>B.1.2. PROMPT FOR RETRIEVAL KEY IN REASONER</p>
<p>Here are examples.think: First I need to find a spraybottle.A spraybottle is more likely to appear in cabinet (1-4), countertop (1), toilet (1), sinkbasin (1-2), garbagecan (1).I can check one by one, starting with cabinet 1. &gt; search: spraybottle think: Now I put the first creditcard in dresser.Next, I need to find the second creditcard.I can directly go to countertop 1. &gt; search: creditcard think: Now I take a pen (2).Next, I need to find a desklamp.A desklamp is more likely to appear in dresser (1), shelf (1-9), bed (1), garbagecan (1), drawer (1-10  Action: think[For mn4 color foundation for sensitive skin, the item has options '1 ', 'dc1', 'dn3', 'dn4', 'lc1', 'ln3', 'ln4', 'lw2','mc1','mn3','mn4', and'</p>
<p>Figure 1 .
1
Figure 1.Overview of RAP.Our framework stores past experiences and retrieves them based on the current situation.Left: The evaluation process on ALFWorld.ICL stands for in-context learning.Right: The evaluation process on Franka Kitchen.</p>
<p>Figure 2 .
2
Figure 2. RAP Core Components based on the current context.By doing so, our agent can adaptably receive different experiences at different points in time that are most similar to the situation at hand.</p>
<p>Figure 3 .
3
Figure 3. Memory-Retrieval in RAP.Left to Middle: The Retriever, calculating similarities with Memory, dynamically switches between action or observation based on the Retrieval Key.This figure illustrates the process of calculating similarity with observation.Right: Executor receives related experiences from memory and utilizes them in the prompt.</p>
<p>Figure 4 .
4
Figure 4. Success Rate of ALFWorld with trial number 4.1.Textual Environments 4.1.1.ALFWORLD</p>
<p>Figure 5
5
Figure5illustrates a comparison of the improvement in success rate and reward score over three trials for ReAct and RAP, with both methods using GPT-3.5.Here, it indicates a marked improvement in both success rate and reward score for our method, indicating that successful experiences from the other tasks are being effectively utilized.</p>
<p>Figure 5 .
5
Figure 5. Success Rate and Reward Score of WebShop over 3 trials</p>
<p>Figure 6 .
6
Figure6.Evaluation on Franka-Kitchen and Meta-World Benchmark.We evaluate with two vision-language models LLaVA and CogVLM both w/wo our proposed RAP method.The results demonstrate that our method notably enhances the performance of multimodal LLM agents in executing embodied tasks.</p>
<p>Action</p>
<p>[dc1][dn3][dn4][lc1][ln3][ln4][lw2]  [mc1][mn3][mn4][mw2]  size [1 fl oz (pack of 1)][1 ounce (pack of 1)][1 fluid ounce] Physicians Formula The Healthy Foundation with SPF 20, LW2, 1 Fl Oz Price: $7.59 Rating: N.A. [Description] [Features] [Reviews] [Attributes] [Buy Now] Act 11: think[For mn4 color foundation for sensitive skin, the item has options 'lw2' and '1 fluid ounce' and seems good to buy.to Search] Page 1 (Total results: 50) [Next &gt;] [B077PR9TL4] Physicians Formula The Healthy Foundation with SPF 20, LW2, 1 Fl Oz $7.59 [B07VLKYJNH] P ÜR 4-in-1 Love Your Selfie Longwear Foundation &amp; Concealer -Full Coverage Foundation &amp; Concealer Makeup For Dark Spots, Blemishes and Imperfections -Long-Lasting Liquid Makeup &amp; Skincare $36.0 [B09J1X9M1P] Neutrogena Healthy Skin Sensitive Skin Serum Foundation with Pro-Vitamin B5, Color Correcting &amp; Pore Minimizing Liquid Foundation &amp; Face Serum, Buildable Coverage, Medium 02, 1 oz $14.39 Action: think[B077PR9TL4, B07VLKYJNH, and B09J1X9M1P are mn4 color foundation for sensitive skin and price lower than 40.00 dollars.[dc1][dn3][dn4][lc1][ln3][ln4][lw2][mc1][mn3][mn4][mw2] size [1 fl oz (pack of 1)][1 ounce (pack of 1)][1 fluid ounce] Physicians Formula The Healthy Foundation with SPF 20, LW2, 1 Fl Oz Price: $7.59 Rating: N.A. [Description] [Features] [Reviews] [Attributes] [Buy Now]</p>
<p>mw2' and seems good to buy.] Observation: OK.Action: click[mn4] Observation: You have clicked mn4.Action: click[Buy Now] Observation: Your score (min 0.0, max 1.0): 1.0 Success</p>
<p>The agent forms an overall plan p, then interacts with the environment over a finite horizon of H timesteps.At each timestep t ∈ {1, 2, . . .H}, the agent forms an action plan p ′ t , selects an action α t from the action space A and receives an observation o t from the observation space O.
The trajectoryτ = { ⃗ p ′ t , ⃗ α t , ⃗ o t } of the agent up to time t consists of the sequences of plans ⃗ p ′ t = (p ′ 1 , ...p ′ t ), actions ⃗ α t = (α 1 , ...α t )and observations ⃗ o
t = (o 1 , ...o t ).</p>
<p>Table 1 .
1
ALFWorld task-specific success rate(%).
Method(dmax=3)ModelPick CleanHeatCool Look Pick2AllActGPT-3.566.751.673.961.938.917.653.7ReActGPT-3.550.041.973.966.755.623.552.2ReflexionGPT-3.575.077.465.276.283.370.674.6ADaPT *GPT-3.587.580.660.876.261.152.971.6RAP(Ours)GPT-3.595.887.178.390.588.970.685.8RAPtrain(Ours)GPT-3.595.8 100.082.685.7 100.076.591.0ReActGPT-483.371.095.781.0 100.094.185.8RAP(Ours)GPT-495.890.3100.0 95.2 100.088.294.8ReActLlama2-13b 29.241.934.852.438.917.636.6RAP(Ours)Llama2-13b 62.561.356.561.944.417.653.0</p>
<p>Table 2 .
2
WebShop Score (%) and Success Rate(%).
Method(dmax=3)ModelScore Success RateReActGPT-3.561.835.0ReflexionGPT-3.561.835.0ADaPTGPT-3.564.043.0RAP(Ours)GPT-3.576.148.0ReActLlama2-13b64.631.0RAP(Ours)Llama2-13b71.136.0</p>
<p>Table 3 .
3
Average success rates on Franka Kitchen and Meta World of Vision-Languge Model Agents w/wo RAP(%)
MethodFranka Kitchen Meta WorldLLaVA43.465.4LLaVA with RAP61.679.2CogVLM44.261.5CogVLM with RAP56.968.8</p>
<p>Table 4 .
4
ALFWorld success rate(%) with different retrievers.RAPact uses only action information, while RAP obs utilizes only observation information for retrieval.RAP clip refers to evaluations using images, rather than texts, as observations with CLIP.
Method(dmax=3) Success RateReAct52.2RAPact82.1RAP obs84.3RAP clip86.6RAP85.85.1.2. WEBSHOP EVALUATIONIn this subsection, we illustrate the effect of varying thecomponents extracted from each experience by the Retriever.</p>
<p>Table 5 .
5
Webshop overall score (%) and success rate(%) with different retrievers on Llama2-13b.RAP obs uses additional retrieval by observations on top of action-based retrieval in RAPact.Also, RAPintra and RAPcat uses intra-task retrieval and product-category retrieval.RAP indicates combination of RAP obs , RAPintra and RAPcat.
Method(dmax=3) Success Rate Overall ScoreReAct31.064.6RAPact33.068.6RAP obs33.069.0RAPintra34.069.3RAPcat35.069.9RAP36.071.1</p>
<p>Here is the task.Please make an action from the examples.Task: put some watch on safe.Plan: To solve the task, I need to find and take a watch, then put it on the safe.
). I can check one by one, starting with dresser 1. think: Now I find the second saltshaker (2). Next, I need to take it. &gt; search: desklamp think: Now I find a lettuce (1). Next, I need to take it. &gt; action: take think: Now I find a pan (1). Next, I need to take it. &gt; action: take &gt; action: take think: Now I heat an egg (2). Next, I need to put it in/on diningtable 1. &gt; action: put think: Now I take a spraybottle (2). Next, I need to put it in/on toilet 1. &gt; action: put think: Now I take an apple (1). Next, I need to go to a microwave (1) and heat it. &gt; action: heat think: Now I take a bread (1). Next, I need to go to a microwave (1) and heat it. &gt; action: heat think: Now I take a mug (3). Next, I need to go to a fridge (1) and cool it. &gt; action: cool think: Now I take a potato (2). Next, I need to go to a fridge (1) and cool it. &gt; action: cool think: Now I find a desklamp (1). Next, I need to use it. &gt; action: use think: Now I find a desklamp (3). Next, I need to use it. &gt; action: use Here is the task. Please make a plan from the examples. think: First I need to find a watch. A watch is more likely to appear in/on cabinet (1-4), drawer (1-5). I can check one by one, starting with cabinet 1 &gt; B.1.3. PROMPT FOR ACTION PLAN IN REASONER AND FOR ACTION IN EXECUTOR Here is the task information. You are in the middle of a room. Looking quickly around you, you see a armchair 1, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a dresser 1, a garbagecan 1, a safe 1, a shelf 12, a shelf 11, a shelf 10, a shelf 9, a shelf 8, a shelf 7, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, a shelf 1, a sidetable 1, and a sofa 1. Here are examples. {Task: put a vase in safe. Plan: To solve the task, I need to find and take a vase, then put it in the safe. &gt; think: First I need to find a vase. OK. &gt; go to shelf 1 On the shelf 1, you see nothing. ... &gt; go to shelf 6 On the shelf 6, you see a vase 2. &gt; think: Now I find a vase (2). Next, I need to take it OK. &gt; take vase 2 from shelf 6 You pick up the vase 2 from the shelf 6.} examples x n &gt; C. ALFWorld trajectories Task Description You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1. Your task is to: put a mug in desk. ReAct Act 1: think: To solve the task, I need to find and take a mug, then put it in desk Obs 1: OK. Act 2: think: First I need to find a mug. A mug is more likely to appear in drawer (1-6), shelf (1-6), desk (1-2), garbagecan (1), laundryhamper (1), safe (1). I can check one by one, starting with drawer 1 Obs 2: OK. Act 3: go to drawer 1 Obs 3: The drawer 1 is closed. Act 4: open drawer 1 Obs 4: You open the drawer 1. The drawer 1 is open. In it, you see a pen 1. ... Act 23: think: Hmm, it seems like the mug is not in any of the drawers. Let me check the other possible locations Obs 23: OK. ... Fail RAP Plan: To solve the task, I need to find and take a mug, then put it in desk. Act 1: think: First I need to find a mug Obs 1: OK. Retrieval key: search mug Act 2: go to shelf 1 Obs 2: On the shelf 1, you see a mug 2. Act 3: take mug 2 from shelf 1 Obs 3: You pick up the mug 2 from the shelf 1. Act 4: think: Now I find a mug (2). Next, I need to put it in/on desk 1 Obs 4: OK. Retrieval key: action put Act 5: go to desk 1 Obs 5: On the desk 1, you see a bowl 1, a cd 2, a creditcard 2, a desklamp 1, a keychain 1, a laptop 2, a pen 2, and a pencil 2. Act 6: put mug 2 in/on desk 1 Obs 6: You put the mug 2 in/on the desk 1. Success Task Description You are in the middle of a room. Looking quickly around you, you see a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a countertop 1, a garbagecan 1, a handtowelholder 2, a handtowelholder 1, a sinkbasin 2, a sinkbasin 1, a toilet 1, a toiletpaperhanger 1, and a towelholder 1. Your task is to: clean some cloth and put it in cabinet. ReAct Act 1: think: To solve the task, I need to find and take a cloth, then clean it with sinkbasin, then put it in a cabinet Obs 1: OK. Act 2: think: First I need to find a cloth. A cloth is more likely to appear in a cabinet (1-4), coun-tertop (1), sinkbasin (1-2), handtowelholder (1-2), towelholder (1), toiletpaperhanger (1), garbage-can (1). I can check one by one, starting with cabinet 1 Obs 2: OK. ... Act 6: open cabinet 2 Obs 6: You open the cabinet 2. The cabinet 2 is open. In it, you see a cloth 1. Act 7: take cloth 1 from cabinet 2 Obs 7: You pick up the cloth 1 from the cabinet 2. Act 8: think: Now I take a cloth (1). Next, I need to go to sinkbasin (1-2) and clean it Obs 8: OK. Act 9: go to sinkbasin 1 Obs 9: On the sinkbasin 1, you see nothing. Act 10: go to sinkbasin 2 Obs 10: On the sinkbasin 2, you see nothing. Act 11: think: I can't clean the cloth without water. I need to find a source of water Obs 11: OK. Act 12: go to toilet 1 ... Fail RAP Plan: To solve the task, I need to find and take a cloth, then clean it with sinkbasin, then put it in cabinet. Act 1: think: First I need to find a cloth Obs 1: OK. Retrieval key: search cloth Act 2: go to cabinet 1 Obs 2: The cabinet 1 is closed. Act 3: open cabinet 1 Obs 3: You open the cabinet 1. The cabinet 1 is open. In it, you see a toiletpaper 1. Act 4: go to cabinet 2 Obs 4: The cabinet 2 is closed. Act 5: open cabinet 2 Obs 5: You open the cabinet 2. The cabinet 2 is open. In it, you see a cloth 1. Act 6: take cloth 1 from cabinet 2 Obs 6: You pick up the cloth 1 from the cabinet 2. Act 7: think: Now I take a cloth (1). Next, I need to go to sinkbasin (1) and clean it Obs 7: OK. Retrieval key: action clean Act 8: go to sinkbasin 1 Obs 8: On the sinkbasin 1, you see nothing. Act 9: clean cloth 1 with sinkbasin 1 Obs 9: You clean the cloth 1 using the sinkbasin 1. Act 10: think: Now I clean a cloth (1). Next, I need to put it in/on cabinet (1-6) Obs 10: OK. Retrieval key: action put Act 11: go to cabinet 3 Obs 11: The cabinet 3 is closed. Act 12: open cabinet 3 Obs 12: You open the cabinet 3. The cabinet 3 is open. In it, you see a soapbottle 1. Act 13: put cloth 1 in/on cabinet 3 Obs 13: You put the cloth 1 in/on the cabinet 3. Task Description WebShop Instruction: i am looking for mn4 color foundation for my sensitive skin, and price lower than 40.00 dollars [Search] ReAct Action: search[mn4 color foundation sensitive skin] Observation: [Back to Search] Page 1 (Total re-sults: 50) [Next &gt;] [B077PR9TL4] Physicians For-mula The Healthy Foundation with SPF 20, LW2, 1 Fl Oz $7.59 [B07VLKYJNH] P ÜR 4-in-1 Love Your Selfie Longwear Foundation &amp; Concealer -Full Coverage Foundation &amp; Concealer Makeup For Dark Spots, Blemishes and Imperfections -Long-Lasting Liquid Makeup &amp; Skincare $36.0 [B09J1X9M1P] Neutrogena Healthy Skin Sen-sitive Skin Serum Foundation with Pro-Vitamin B5, Color Correcting &amp; Pore Minimizing Liquid Foundation &amp; Face Serum, Buildable Coverage, Medium 02, 1 oz $14.39 Action: think[B077PR9TL4 and B09J1X9M1P are mn4 color foundation for sensitive skin and price lower than 40 dollars.] Success D. Webshop trajectories Observation: OK.
A. ParametersIn Webshop, all experiments were performed with a temperature setting of 0.0, due to the additional requirement of precise commands for interactions with the web application.To evaluate RAP in multimodal environments, we focus on two benchmarks: Franka Kitchen and Meta-World.These benchmarks offer a diverse range of tasks, simulating real-world scenarios in a controlled environment, making them ideal for evaluating the performance of multimodal agents.Franka Kitchen Benchmark simulates a kitchen environment, where the agent interacts with various kitchen appliances and items.We have selected five specific tasks that test the agent's ability to manipulate objects and controls within this environment.These tasks are listed in  The Meta-World benchmark is designed to evaluate an agent's skill in more generalized object manipulation tasks.We have selected five tasks that represent a broad range of actions.Task No. Meta-World Tasks 1. assemble Pick up a nut and place it onto a peg 2.buttion Press a button 3. drawer Open a drawer 4. hammer Hammer a screw on the wall 5. binGrasp the puck from one bin and place it into another binE.2. Model SpecificationIn this subsection, we detail the model specifications for our multimodal agent, which is built upon two SOTA visionlanguage models: LLaVA and CogVLM.These models allows for a more comprehensive understanding and interaction with multimodal environments.LLaVA Model: Our agent utilizes the LLaVA-v1.513B model.This version of LLaVA incorporates the Vicuna-v1.513B as its underlying Large Language Model (LLM).The integration of Vicuna-v1.5 13B, known for its robust linguistic processing capabilities.CogVLM Model: For the CogVLM model, we employ the Cog-VLM 17B version.This model boasts a significant number of parameters -10 billion are dedicated to visual understanding, and 7 billion are focused on language processing.In our experimental setup for the language generation of these models, we set the temperature to 0.0.This setting is chosen to prioritize precision and determinism in the generated outputs, which is crucial for the consistency and reliability of the agent's planning responses in our multimodal tasks.E.3. Policy Network detailsPolicy Network is a crucial component of our framework designed to translate high-level action plans generated by the vision-language model into precise, low-level control actions suitable for the specific action space of the environment.Our approach utilizes a Multi-Layer Perceptron (MLP) neural network for this purpose.In both Franka Kitchen and Meta-World environments, the policy network's learning is facilitated through a few-shot learning approach, leveraging a limited but highly informative set of demonstration data.For each environment, we provide 25 expert demonstrations sourced from the D4RL dataset.These demonstrations consist of trajectories that include both observations and actions, showcasing expert-level performance in the respective tasks.In the Franka Kitchen tasks, each demonstration is composed of 50 state-action pairs, reflecting the sequence and specifics of actions required to complete the task.Meanwhile, for the Meta-World tasks, each demonstration sample comprises 500 state-action pairs.
Textworld: A learning environment for text-based games. M.-A Côté, Ákos Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, R Y Tao, M Hausknecht, L E Asri, M Adada, W Tay, A Trischler, arXivInternational Joint Conference on Artificial Intelligence (IJCAI). 2018. 2023GenAI, M. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. A Gupta, V Kumar, C Lynch, S Levine, K Hausman, Conference on Robot Learning (CoRL). 2019</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W.-T Yih, T Rocktäschel, S Riedel, D Kiela, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20. the 34th International Conference on Neural Information Processing Systems, NIPS'20Red Hook, NY, USACurran Associates Inc2020ISBN 9781713829546</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, ArXiv, volume abs/2304.084852023</p>
<p>. J Liu, D Shen, Y Zhang, B Dolan, L Carin, W Chen, 2021What makes good in-context examples for gpt-3? In arXiv</p>
<p>Memoryassisted prompt editing to improve gpt-3 after deployment. A Madaan, N Tandon, P Clark, Y Yang, S Mirchandani, F Xia, P Florence, B Ichter, D Driess, M G Arenas, K Rao, D Sadigh, A Zeng, Empirical Methods in Natural Language Processing. 20222023Conference on Robot Learning (CoRL)</p>
<p>Gpt-4 technical report. arXiv2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P F Christiano, J Leike, R Lowe, Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Learning transferable visual models from natural language supervision. A Prasad, A Koller, M Hartmann, P Clark, A Sabharwal, M Bansal, T Khot, A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, arXivProceedings of the 38th International Conference on Machine Learning. M Meila, T Zhang, the 38th International Conference on Machine LearningPMLR2023. Jul 2021139Adapt: As-needed decomposition and planning with language models</p>
<p>N Reimers, I Gurevych, - Sentence, Bert, arXivSentence Embeddings using Siamese BERT-Networks. 2019</p>
<p>Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. P Sharma, N Ding, S Goodman, R Soricut, 10.18653/v1/P18-1238Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. I Gurevych, Y Miyao, the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational LinguisticsJuly 20181</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, E Berman, A Gopinath, K Narasimhan, S Yao, Neural Information Processing Systems (NeurIPS). 2023</p>
<p>Aligning text and embodied environments for interactive learning. M Shridhar, X Yuan, M.-A Côté, Y Bisk, A Trischler, M Hausknecht, Alfworld, International Conference on Learning Representations (ICLR). 2021</p>
<p>Selective annotation makes language models better few-shot learners. H Su, J Kasai, C H Wu, W Shi, T Wang, J Xin, R Zhang, M Ostendorf, L Zettlemoyer, N A Smith, T Yu, arXiv2022</p>
<p>L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, W X Zhao, Z Wei, J.-R Wen, arXivA survey on large language model based autonomous agents. 2023a</p>
<p>. L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, W X Zhao, Z Wei, J.-R Wen, A Survey on Large Language Model based Autonomous Agents. In arXiv. 2023b</p>
<p>CogVLM: Visual Expert for Pretrained Language Models. W Wang, Q Lv, W Yu, W Hong, J Qi, Y Wang, J Ji, Z Yang, L Zhao, X Song, J Xu, B Xu, J Li, Y Dong, M Ding, J Tang, ArXiv, volume abs/2311.030792023c</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, 35</p>
<p>Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, R Zheng, X Fan, X Wang, L Xiong, Y Zhou, W Wang, C Jiang, Y Zou, X Liu, Z Yin, S Dou, R Weng, W Cheng, Q Zhang, W Qin, Y Zheng, X Qiu, X Huang, Gui , T , arXivThe rise and potential of large language model based agents: A survey. 2023</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. S Yao, H Chen, J Yang, K Narasimhan, arXiv2022</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>A survey on multimodal large language models. S Yin, C Fu, S Zhao, K Li, X Sun, T Xu, E Chen, arXiv2023</p>
<p>T Yu, D Quillen, Z He, R Julian, A Narayan, H Shively, A Bellathur, K Hausman, C Finn, S Levine, Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. 2021</p>
<p>Task Description WebShop Instruction: i want capri sun pacific cooler mixed fruit naturally flavored juice drinks, and price lower than 20.00 dollars [Search] ReAct Action: search[capri sun pacific cooler mixed fruit naturally flavored juice drinks] Observation: [Back to Search] Page 1 (Total results: 50) [Next &gt;] [B000R30X2A] Capri Sun Pacific Cooler Mixed Fruit Naturally Flavored Juice Drink Blend (Pack of 10) (Packaging May Vary) $2.55 [B075CSWGJR] Capri Sun Pacific Cooler Ready-to-Drink Juice, 6 Fl Oz (Pack of 30) $100.0 [B0083GG2SC] Capri Sun Coolers Variety Pack Ready-to-Drink Juice (40 Pouches, 4 Boxes of 10) $100.0 Action: think[B000R30X2A is capri sun pacific cooler mixed fruit naturally flavored juice drinks and price lower than 20. A Zhao, D Huang, Q Xu, M Lin, Y.-J Liu, G Huang, AAAI Conference on Artificial Intelligence (AAAI). 202355Action: think[For capri sun pacific cooler mixed fruit naturally flavored juice drinks, the item has options. fl oz (pack of 10)'] Observation: OK. Action: click[assorted] Observation: You have clicked assorted. Action: click[Buy Now] Observation: Your score (min 0.0, max 1.0</p>
<p>B000R30X2A] Capri Sun Pacific Cooler Mixed Fruit Naturally Flavored Juice Drink Blend (Pack of 10) (Packaging May Vary) $2.55 [B075CSWGJR] Capri Sun Pacific Cooler Ready-to-Drink Juice, 6 Fl Oz (Pack of 30) $100.0 [B0083GG2SC] Capri Sun Coolers Variety Pack Ready-to-Drink Juice (40 Pouches, 4 Boxes of 10) $100.0 Action: think[B000R30X2A is capri sun pacific cooler mixed fruit naturally flavored juice drinks and seems good to buy. Action: think[For capri sun pacific cooler mixed fruit naturally flavored juice drinks, the item has options 'assorted', 'pacific cooler', and '6 fl oz (pack of 10)' and seems good to buy. 6 fl oz (pack of 10. Observation: You have clicked 6 fl oz (pack of 10. Action: click[Buy Now] Observation: Your score (min 0.0, max 1.0): 1.0</p>            </div>
        </div>

    </div>
</body>
</html>
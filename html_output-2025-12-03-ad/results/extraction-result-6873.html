<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6873 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6873</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6873</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-272524743</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.04481v1.pdf" target="_blank">Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials</a></p>
                <p><strong>Paper Abstract:</strong> The integration of Large Language Models (LLMs) into the drug discovery and development field marks a significant paradigm shift, offering novel methodologies for understanding disease mechanisms, facilitating drug discovery, and optimizing clinical trial processes. This review highlights the expanding role of LLMs in revolutionizing various stages of the drug development pipeline. We investigate how these advanced computational models can uncover target-disease linkage, interpret complex biomedical data, enhance drug molecule design, predict drug efficacy and safety profiles, and facilitate clinical trial processes. Our paper aims to provide a comprehensive overview for researchers and practitioners in computational biology, pharmacology, and AI4Science by offering insights into the potential transformative impact of LLMs on drug discovery and development.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6873.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6873.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Molecular Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecular Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer-based encoder-decoder model trained to predict chemical reaction products from reactants/reagents using SMILES; used for reaction prediction and as a component in retrosynthetic planning and route exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Molecular Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder transformer (reaction prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on reaction SMILES corpora (reactants, reagents, products) used for supervised reaction prediction tasks (public reaction datasets implied).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised sequence-to-sequence prediction of product SMILES from reactants/reagents; uncertainty calibration; used within hyper-graph retrosynthetic planning.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Retrosynthetic planning and reaction outcome prediction for synthetic route design</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Beam search over hyper-graph of disconnection strategies for route scoring; no specific property filters reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used together with hyper-graph exploration strategies for retrosynthetic route planning (beam search over reaction hyper-graph).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Reaction datasets (public reaction corpora used in Molecular Transformer original work; not specified in this review beyond reference).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reaction prediction accuracy; uncertainty-calibrated metrics (as originally reported); used as component in retrosynthetic route planning evaluated by route plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported in original work to have higher accuracy in reaction prediction than human chemists (as cited in review); no new quantitative numbers provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Review notes general challenges of LLMs in chemistry (need for tool integration, hallucinations and factuality issues) but does not list model-specific failures beyond general limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6873.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6873.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chemformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BART-based pre-trained transformer for computational chemistry that reconstructs masked SMILES and uses an autoencoder-style pretraining to produce embeddings fineâ€‘tuned for reaction and retrosynthesis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chemformer: a pre-trained transformer for computational chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chemformer (BART-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder transformer with additional SMILES reconstruction pretraining (autoencoder-style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained by reconstructing masked SMILES strings and using an autoencoder to map SMILES to embeddings and back; trained on large SMILES corpora (unspecified here).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Pretraining via SMILES reconstruction; fine-tuning for downstream reaction and retrosynthesis prediction tasks (sequence reconstruction and prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings (masked SMILES reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Reaction prediction, retrosynthesis planning, and other downstream computational chemistry predictions</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used as a model for downstream tasks; review does not describe specific external tool integrations for Chemformer beyond common retrosynthesis pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Large unlabeled SMILES corpora (paper references ~861k SMILES for similar models; exact Chemformer dataset not specified in review).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Standard reaction/retrosynthesis metrics (accuracy on product prediction, route plausibility) as in original benchmarks; specifics not quoted in review.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>General limitations for LLM-based chemistry models noted (computational cost for large models, data requirements); no Chemformer-specific failure modes given in review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6873.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6873.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REINVENT series</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REINVENT (and REINVENT series)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative chemistry platform using staged training (prior model, transfer learning agent, and staged learning) combined with reinforcement learning and curriculum learning to generate molecules that satisfy multiple objectives (up to ~10).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reinvent 2.0: an ai tool for de novo drug design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>REINVENT (generative chemical language model + RL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive chemical language model with reinforcement learning agent and staged transfer-learning pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on molecular SMILES corpora (not explicitly enumerated in review; typical sources include ZINC/ChEMBL in literature).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Three-staged training: prior model pretraining, transfer learning agent, and staged reinforcement learning towards high-scoring sequences; objective-guided generation (multi-objective RL).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings (sequence-based generation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Constrained de novo molecule generation for drug discovery and lead optimization across multiple objectives (affinity, selectivity, synthesizability, ADMET-related properties).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Multi-objective constraints including synthesizability, selectivity, ADMET properties, and other property windows (up to 10 objectives reported).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Typically used with property predictors and filters (drug-likeness filters, predictive algorithms) to guide RL reward; specific tool names not listed in review.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Large SMILES corpora for prior; downstream fine-tuning datasets depend on objectives (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Objective scores aggregated across objectives (multi-objective optimization); common generative metrics like validity/novelty implied but not explicitly listed for REINVENT in review.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Review states capability to meet up to 10 different objectives via staged process (qualitative); no numeric benchmarks provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Complexity of multi-objective optimization, need for reliable property predictors to shape rewards; generalization beyond training distributions remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6873.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6873.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-style autoregressive model trained to generate molecular SMILES with conditional handling of scaffolds and properties to enable constrained molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molgpt: molecular generation using a transformerdecoder model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive GPT-style decoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on molecular SMILES with scaffold/property conditioning (dataset not enumerated in review).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive SMILES generation conditioned on scaffold and property tokens (training objective recovers molecule from scaffold + properties).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Constrained de novo molecule generation and scaffold-preserving design</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Scaffold conditioning and property conditioning (user-provided scaffolds and desired properties).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>SMILES-based molecular corpora (not specified in review)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Generative model metrics such as ability to recover molecules given scaffold/properties; typical metrics include validity/novelty but not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>General issues with language-model-based SMILES generation (sensitivity to tokenization, potential for invalid SMILES, need for property predictors to enforce constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6873.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6873.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GENTRL / Chemistry42 (Insilico Medicine)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GENTRL and Chemistry42 (Insilico Medicine platforms)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative chemistry platforms that used generative models to propose novel kinase inhibitors; GENTRL identified DDR1 inhibitors (Zhavoronkov et al. 2019) and Chemistry42 generated novel structures with optimized properties and reported in vitro/in vivo validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning enables rapid identification of potent ddr1 kinase inhibitors</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GENTRL / Chemistry42 (Insilico Medicine generative pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>specialized generative models / platform (original GENTRL used generative model architectures; Chemistry42 is an integrated AI-driven platform)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on chemical structure datasets and bioactivity data; specifics not listed in review (original works used public and proprietary bioactivity and chemical structure data).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>De novo molecule generation guided by objective functions (design for target inhibition), likely combining generative models and optimization (as described in original works).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES and internal molecular graph representations (review references SMILES-based generative tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Small-molecule drug discovery (kinase inhibitors; DDR1 cited as target example)</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Optimization for potency and property profiles; synthesizability and ADME considerations implied in platform workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Platform workflows integrate predictive models, in silico screening, and experimental follow-up (review states generated structures were validated in vitro/in vivo).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Chemical structure and bioactivity datasets (public and proprietary; not enumerated in review).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Measured potency (binding/biochemical activity), in vitro and in vivo assay results; general property optimization metrics (not fully enumerated in review).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Original GENTRL work identified DDR1 kinase inhibitors and reported experimental validation; review states Chemistry42 generated novel molecular structures with optimized properties validated through extensive in vitro and in vivo studies (no numeric values provided in review).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>While successful, review notes that broader industry adoption is limited and that LLM/generative systems must address synthesis feasibility, interpretability, and regulatory/ethical concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6873.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6873.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chemcrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemcrow</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general LLM augmented with chemistry-specific tools (SMILES conversion, pricing, patent checking, reaction classification) and a four-step tool-use framework to improve synthesis planning and automated chemistry tasks; evaluated by human assessment to outperform GPT-4 on many synthesis planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chemcrow: Augmenting large-language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chemcrow (tool-augmented general LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>tool-using agent (general-purpose LLM augmented with external chemistry tools and tool-use framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Built on a general LLM backbone pretrained on broad text corpora; augments with domain tools and chemical databases (SMILES converters, price databases, patent checks).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Chain-of-thought style multi-step framework: plan (think), call tools (action), feed inputs to tools, analyze observations, produce final answer; uses tool calls for synthesis planning and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Textual SMILES conversions and tool-specific formats (SMILES as primary molecular representation when interacting with tools).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Synthesis planning and automated chemistry experiment design (retrosynthesis and practical lab planning).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Tool-derived constraints like availability/pricing, patent checks, reaction classification to filter plan outputs; human-in-the-loop evaluations used in assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Strong integration: SMILES conversion utilities, molecule price lookups, patent checking, reaction classification modules, and other bespoke chemistry tools.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Not a single dataset; relies on tool-backed databases (pricing, patents) and LLM pretraining corpora; evaluation sets for synthesis planning used in human assessments (not enumerated).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human expert evaluations of synthesis planning quality and correctness; comparison against GPT-4 across tasks (qualitative and human-judged).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Review states Chemcrow performed better than GPT-4 in most synthesis planning tasks evaluated by humans (no numeric scores provided in review).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Reliance on external tools and databases; potential for tool-misuse/hallucination if tool outputs are misinterpreted; review flags general risk of hallucinations and need for human supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6873.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6873.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Boiko et al. autonomous chemistry agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous chemical research with large language models (Boiko et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstrated LLM-based agents that autonomously generate experimental plans, use web search and vector search for reaction documentation, and successfully execute complex experiments (e.g., Suzuki and Sonogashira couplings) using multi-instrument systems code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous chemical research with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Boiko et al. LLM-based autonomous chemistry agent</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>tool-using, multi-instrument agent (general LLM controlling lab instruments via generated code)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Agent uses a general LLM backbone pretrained on large text and code corpora; supplements with web-search and vector-search over documentation at runtime (not classic model pretraining on chemistry data).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Agentic loop: query web/documentation, plan steps, generate multi-instrument code/scripts to control lab devices and implement synthesis; iterative refinement based on observations.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Textual protocols and reaction descriptions; underlying molecules represented as SMILES when needed for tool interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Automated execution of chemical synthesis workflows and retrosynthetic planning translated into instrument control scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Safety and motion planning constraints via constrained task and motion planning (PDDLStream in other systems mentioned); chemical feasibility inferred via documentation and tool outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Web search engines, vector search over documentation, multi-instrument control code generation, integration with lab automation platforms to execute reactions.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Runtime web/documentation retrieval and tool-specific knowledge bases; not trained on a fixed chemistry dataset for the autonomous agent described in review.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success in executing complex experiments (qualitative and task-completion metrics); review cites successful implementation of Suzuki and Sonogashira cross-couplings.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Review reports successful autonomous implementation of complex reactions (Suzuki and Sonogashira) and multi-instrument code generation capability; no detailed quantitative yields or rates provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Safety, correctness, and hallucination risks; need for constrained planning and human oversight; integration complexity with diverse instruments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6873.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6873.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Molformer (IBM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molformer / Large-scale chemical language representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large chemical language representation model developed by IBM for capturing molecular structure and properties and for generative tasks including candidate molecule generation for antiviral and antimicrobial applications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large-scale chemical language representations capture molecular structure and properties</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Molformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>large transformer-based chemical language model (specialized)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on large-scale chemical corpora to learn molecular structure-property relationships (exact datasets not enumerated in review).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Generative modeling of molecular structures from learned chemical language/embeddings; used to propose candidate molecules for downstream evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES and chemical language embeddings; graph-aware representations implied by name and usage.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo molecule generation for antiviral (SARS-CoV-2 inhibitors) and antimicrobial peptide design; molecular property prediction and screening.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Property-based constraints for antiviral/antimicrobial objectives implied; details not specified in review.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used in downstream in-silico screening workflows and molecular dynamics simulations (as cited for related works), but specific toolchain integrations not enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Large chemical corpora (public chemical databases implied), specific datasets not listed in review.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported as 'promising results' for generated candidates aimed at inhibiting SARS-CoV-2 and generating antimicrobial peptides; no numerical metrics provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Review notes that general LLMs and some models struggle with quantitative tasks and that experimental validation is variably reported; Molformer-specific experimental validation not detailed in review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6873.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6873.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Moret et al. chemical language model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemical language model from Moret et al. (2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemical language model used for de novo drug design that facilitated discovery of a new PI3KÎ³ ligand with sub-micromolar activity as an example of lead optimization via language-model-guided design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging molecular structure and bioactivity with chemical language models for de novo drug design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chemical language model (Moret et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>specialized chemical language model (generative)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on molecular structure and bioactivity data (exact datasets not listed in review summary).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>De novo generation guided by learned structure-bioactivity relationships; used in lead discovery/optimization workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES / chemical language representations</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Lead discovery / lead optimization (example: PI3KÎ³ ligand discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Optimization for bioactivity (potency) and likely other property considerations although explicit constraints not enumerated in review.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used within pipelines that combine property prediction and generative sampling; specific external tools not detailed in review.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Molecular structures and associated bioactivity datasets (not specified in review).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported potency (bioactivity) as a primary metric; also typical generative metrics implied.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Review states discovery of a new PI3KÎ³ ligand with sub-micromolar activity (i.e., potency < 1 ÂµM); no further numeric detail provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>General challenges: need for accurate property predictors, synthesizability, and experimental follow-up; specifics beyond those general points not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6873.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6873.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned GPT-3 in chemistry (Jablonka et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned GPT-3 applied to predictive chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuned GPT-3 model adapted to chemistry tasks that outperformed traditional ML models on several chemistry prediction tasks, especially in low-data scenarios, illustrating general LLM adaptability to chemical prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging large language models for predictive chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fine-tuned GPT-3 (chemistry tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only GPT-style model, fine-tuned for chemistry prediction</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on chemistry-specific task datasets (low-data regimes highlighted); original GPT-3 pretraining on broad text/code corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning for predictive tasks; applied in few-shot / low-data settings for property prediction and other chemistry tasks (not necessarily de novo generation).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES and chemical text pairs (implied by MolT5/MolGPT analogies and review text)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Predictive chemistry tasks and low-data property prediction; capable of adapting to generative tasks with fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Task-specific chemical datasets (low-data examples emphasized; exact datasets not listed in review).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Performance vs traditional ML baselines (random forest, SVM etc.) on predictive tasks; review notes GPT-3 outperformed traditional ML in several tasks but provides no numeric values here.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Qualitative claim: fine-tuned GPT-3 outperformed traditional machine learning models on several chemistry tasks, notably in low-data scenarios; no numeric results given in review.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>General LLM limitations: less effective quantitative precision, potential hallucinations, and need for domain-specific fine-tuning for best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep learning enables rapid identification of potent ddr1 kinase inhibitors <em>(Rating: 2)</em></li>
                <li>Chemistry42: an ai-driven platform for molecular design and optimization <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>Chemcrow: Augmenting large-language models with chemistry tools <em>(Rating: 2)</em></li>
                <li>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction <em>(Rating: 2)</em></li>
                <li>Chemformer: a pre-trained transformer for computational chemistry <em>(Rating: 2)</em></li>
                <li>Molgpt: molecular generation using a transformerdecoder model <em>(Rating: 2)</em></li>
                <li>Leveraging molecular structure and bioactivity with chemical language models for de novo drug design <em>(Rating: 2)</em></li>
                <li>Leveraging large language models for predictive chemistry <em>(Rating: 2)</em></li>
                <li>Large-scale chemical language representations capture molecular structure and properties <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6873",
    "paper_id": "paper-272524743",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "Molecular Transformer",
            "name_full": "Molecular Transformer",
            "brief_description": "Transformer-based encoder-decoder model trained to predict chemical reaction products from reactants/reagents using SMILES; used for reaction prediction and as a component in retrosynthetic planning and route exploration.",
            "citation_title": "Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction",
            "mention_or_use": "mention",
            "model_name": "Molecular Transformer",
            "model_type": "encoder-decoder transformer (reaction prediction)",
            "model_size": null,
            "training_data_description": "Trained on reaction SMILES corpora (reactants, reagents, products) used for supervised reaction prediction tasks (public reaction datasets implied).",
            "generation_method": "Supervised sequence-to-sequence prediction of product SMILES from reactants/reagents; uncertainty calibration; used within hyper-graph retrosynthetic planning.",
            "chemical_representation": "SMILES strings",
            "target_application": "Retrosynthetic planning and reaction outcome prediction for synthetic route design",
            "constraints_used": "Beam search over hyper-graph of disconnection strategies for route scoring; no specific property filters reported in this paper",
            "integration_with_external_tools": "Used together with hyper-graph exploration strategies for retrosynthetic route planning (beam search over reaction hyper-graph).",
            "dataset_used": "Reaction datasets (public reaction corpora used in Molecular Transformer original work; not specified in this review beyond reference).",
            "evaluation_metrics": "Reaction prediction accuracy; uncertainty-calibrated metrics (as originally reported); used as component in retrosynthetic route planning evaluated by route plausibility.",
            "reported_results": "Reported in original work to have higher accuracy in reaction prediction than human chemists (as cited in review); no new quantitative numbers provided in this review.",
            "experimental_validation": null,
            "challenges_or_limitations": "Review notes general challenges of LLMs in chemistry (need for tool integration, hallucinations and factuality issues) but does not list model-specific failures beyond general limitations.",
            "uuid": "e6873.0",
            "source_info": {
                "paper_title": "Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Chemformer",
            "name_full": "Chemformer",
            "brief_description": "A BART-based pre-trained transformer for computational chemistry that reconstructs masked SMILES and uses an autoencoder-style pretraining to produce embeddings fineâ€‘tuned for reaction and retrosynthesis tasks.",
            "citation_title": "Chemformer: a pre-trained transformer for computational chemistry",
            "mention_or_use": "mention",
            "model_name": "Chemformer (BART-based)",
            "model_type": "encoder-decoder transformer with additional SMILES reconstruction pretraining (autoencoder-style)",
            "model_size": null,
            "training_data_description": "Pretrained by reconstructing masked SMILES strings and using an autoencoder to map SMILES to embeddings and back; trained on large SMILES corpora (unspecified here).",
            "generation_method": "Pretraining via SMILES reconstruction; fine-tuning for downstream reaction and retrosynthesis prediction tasks (sequence reconstruction and prediction).",
            "chemical_representation": "SMILES strings (masked SMILES reconstruction)",
            "target_application": "Reaction prediction, retrosynthesis planning, and other downstream computational chemistry predictions",
            "constraints_used": null,
            "integration_with_external_tools": "Used as a model for downstream tasks; review does not describe specific external tool integrations for Chemformer beyond common retrosynthesis pipelines.",
            "dataset_used": "Large unlabeled SMILES corpora (paper references ~861k SMILES for similar models; exact Chemformer dataset not specified in review).",
            "evaluation_metrics": "Standard reaction/retrosynthesis metrics (accuracy on product prediction, route plausibility) as in original benchmarks; specifics not quoted in review.",
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "General limitations for LLM-based chemistry models noted (computational cost for large models, data requirements); no Chemformer-specific failure modes given in review.",
            "uuid": "e6873.1",
            "source_info": {
                "paper_title": "Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "REINVENT series",
            "name_full": "REINVENT (and REINVENT series)",
            "brief_description": "A generative chemistry platform using staged training (prior model, transfer learning agent, and staged learning) combined with reinforcement learning and curriculum learning to generate molecules that satisfy multiple objectives (up to ~10).",
            "citation_title": "Reinvent 2.0: an ai tool for de novo drug design",
            "mention_or_use": "mention",
            "model_name": "REINVENT (generative chemical language model + RL)",
            "model_type": "autoregressive chemical language model with reinforcement learning agent and staged transfer-learning pipeline",
            "model_size": null,
            "training_data_description": "Pretrained on molecular SMILES corpora (not explicitly enumerated in review; typical sources include ZINC/ChEMBL in literature).",
            "generation_method": "Three-staged training: prior model pretraining, transfer learning agent, and staged reinforcement learning towards high-scoring sequences; objective-guided generation (multi-objective RL).",
            "chemical_representation": "SMILES strings (sequence-based generation)",
            "target_application": "Constrained de novo molecule generation for drug discovery and lead optimization across multiple objectives (affinity, selectivity, synthesizability, ADMET-related properties).",
            "constraints_used": "Multi-objective constraints including synthesizability, selectivity, ADMET properties, and other property windows (up to 10 objectives reported).",
            "integration_with_external_tools": "Typically used with property predictors and filters (drug-likeness filters, predictive algorithms) to guide RL reward; specific tool names not listed in review.",
            "dataset_used": "Large SMILES corpora for prior; downstream fine-tuning datasets depend on objectives (not specified here).",
            "evaluation_metrics": "Objective scores aggregated across objectives (multi-objective optimization); common generative metrics like validity/novelty implied but not explicitly listed for REINVENT in review.",
            "reported_results": "Review states capability to meet up to 10 different objectives via staged process (qualitative); no numeric benchmarks provided here.",
            "experimental_validation": null,
            "challenges_or_limitations": "Complexity of multi-objective optimization, need for reliable property predictors to shape rewards; generalization beyond training distributions remains challenging.",
            "uuid": "e6873.2",
            "source_info": {
                "paper_title": "Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "MolGPT",
            "name_full": "MolGPT",
            "brief_description": "A GPT-style autoregressive model trained to generate molecular SMILES with conditional handling of scaffolds and properties to enable constrained molecule generation.",
            "citation_title": "Molgpt: molecular generation using a transformerdecoder model",
            "mention_or_use": "mention",
            "model_name": "MolGPT",
            "model_type": "autoregressive GPT-style decoder",
            "model_size": null,
            "training_data_description": "Trained on molecular SMILES with scaffold/property conditioning (dataset not enumerated in review).",
            "generation_method": "Autoregressive SMILES generation conditioned on scaffold and property tokens (training objective recovers molecule from scaffold + properties).",
            "chemical_representation": "SMILES strings",
            "target_application": "Constrained de novo molecule generation and scaffold-preserving design",
            "constraints_used": "Scaffold conditioning and property conditioning (user-provided scaffolds and desired properties).",
            "integration_with_external_tools": null,
            "dataset_used": "SMILES-based molecular corpora (not specified in review)",
            "evaluation_metrics": "Generative model metrics such as ability to recover molecules given scaffold/properties; typical metrics include validity/novelty but not specified here.",
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "General issues with language-model-based SMILES generation (sensitivity to tokenization, potential for invalid SMILES, need for property predictors to enforce constraints).",
            "uuid": "e6873.3",
            "source_info": {
                "paper_title": "Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GENTRL / Chemistry42 (Insilico Medicine)",
            "name_full": "GENTRL and Chemistry42 (Insilico Medicine platforms)",
            "brief_description": "Generative chemistry platforms that used generative models to propose novel kinase inhibitors; GENTRL identified DDR1 inhibitors (Zhavoronkov et al. 2019) and Chemistry42 generated novel structures with optimized properties and reported in vitro/in vivo validation.",
            "citation_title": "Deep learning enables rapid identification of potent ddr1 kinase inhibitors",
            "mention_or_use": "mention",
            "model_name": "GENTRL / Chemistry42 (Insilico Medicine generative pipelines)",
            "model_type": "specialized generative models / platform (original GENTRL used generative model architectures; Chemistry42 is an integrated AI-driven platform)",
            "model_size": null,
            "training_data_description": "Trained on chemical structure datasets and bioactivity data; specifics not listed in review (original works used public and proprietary bioactivity and chemical structure data).",
            "generation_method": "De novo molecule generation guided by objective functions (design for target inhibition), likely combining generative models and optimization (as described in original works).",
            "chemical_representation": "SMILES and internal molecular graph representations (review references SMILES-based generative tasks).",
            "target_application": "Small-molecule drug discovery (kinase inhibitors; DDR1 cited as target example)",
            "constraints_used": "Optimization for potency and property profiles; synthesizability and ADME considerations implied in platform workflows.",
            "integration_with_external_tools": "Platform workflows integrate predictive models, in silico screening, and experimental follow-up (review states generated structures were validated in vitro/in vivo).",
            "dataset_used": "Chemical structure and bioactivity datasets (public and proprietary; not enumerated in review).",
            "evaluation_metrics": "Measured potency (binding/biochemical activity), in vitro and in vivo assay results; general property optimization metrics (not fully enumerated in review).",
            "reported_results": "Original GENTRL work identified DDR1 kinase inhibitors and reported experimental validation; review states Chemistry42 generated novel molecular structures with optimized properties validated through extensive in vitro and in vivo studies (no numeric values provided in review).",
            "experimental_validation": true,
            "challenges_or_limitations": "While successful, review notes that broader industry adoption is limited and that LLM/generative systems must address synthesis feasibility, interpretability, and regulatory/ethical concerns.",
            "uuid": "e6873.4",
            "source_info": {
                "paper_title": "Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Chemcrow",
            "name_full": "Chemcrow",
            "brief_description": "A general LLM augmented with chemistry-specific tools (SMILES conversion, pricing, patent checking, reaction classification) and a four-step tool-use framework to improve synthesis planning and automated chemistry tasks; evaluated by human assessment to outperform GPT-4 on many synthesis planning tasks.",
            "citation_title": "Chemcrow: Augmenting large-language models with chemistry tools",
            "mention_or_use": "mention",
            "model_name": "Chemcrow (tool-augmented general LLM)",
            "model_type": "tool-using agent (general-purpose LLM augmented with external chemistry tools and tool-use framework)",
            "model_size": null,
            "training_data_description": "Built on a general LLM backbone pretrained on broad text corpora; augments with domain tools and chemical databases (SMILES converters, price databases, patent checks).",
            "generation_method": "Chain-of-thought style multi-step framework: plan (think), call tools (action), feed inputs to tools, analyze observations, produce final answer; uses tool calls for synthesis planning and evaluation.",
            "chemical_representation": "Textual SMILES conversions and tool-specific formats (SMILES as primary molecular representation when interacting with tools).",
            "target_application": "Synthesis planning and automated chemistry experiment design (retrosynthesis and practical lab planning).",
            "constraints_used": "Tool-derived constraints like availability/pricing, patent checks, reaction classification to filter plan outputs; human-in-the-loop evaluations used in assessment.",
            "integration_with_external_tools": "Strong integration: SMILES conversion utilities, molecule price lookups, patent checking, reaction classification modules, and other bespoke chemistry tools.",
            "dataset_used": "Not a single dataset; relies on tool-backed databases (pricing, patents) and LLM pretraining corpora; evaluation sets for synthesis planning used in human assessments (not enumerated).",
            "evaluation_metrics": "Human expert evaluations of synthesis planning quality and correctness; comparison against GPT-4 across tasks (qualitative and human-judged).",
            "reported_results": "Review states Chemcrow performed better than GPT-4 in most synthesis planning tasks evaluated by humans (no numeric scores provided in review).",
            "experimental_validation": false,
            "challenges_or_limitations": "Reliance on external tools and databases; potential for tool-misuse/hallucination if tool outputs are misinterpreted; review flags general risk of hallucinations and need for human supervision.",
            "uuid": "e6873.5",
            "source_info": {
                "paper_title": "Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Boiko et al. autonomous chemistry agent",
            "name_full": "Autonomous chemical research with large language models (Boiko et al.)",
            "brief_description": "Demonstrated LLM-based agents that autonomously generate experimental plans, use web search and vector search for reaction documentation, and successfully execute complex experiments (e.g., Suzuki and Sonogashira couplings) using multi-instrument systems code generation.",
            "citation_title": "Autonomous chemical research with large language models",
            "mention_or_use": "mention",
            "model_name": "Boiko et al. LLM-based autonomous chemistry agent",
            "model_type": "tool-using, multi-instrument agent (general LLM controlling lab instruments via generated code)",
            "model_size": null,
            "training_data_description": "Agent uses a general LLM backbone pretrained on large text and code corpora; supplements with web-search and vector-search over documentation at runtime (not classic model pretraining on chemistry data).",
            "generation_method": "Agentic loop: query web/documentation, plan steps, generate multi-instrument code/scripts to control lab devices and implement synthesis; iterative refinement based on observations.",
            "chemical_representation": "Textual protocols and reaction descriptions; underlying molecules represented as SMILES when needed for tool interactions.",
            "target_application": "Automated execution of chemical synthesis workflows and retrosynthetic planning translated into instrument control scripts.",
            "constraints_used": "Safety and motion planning constraints via constrained task and motion planning (PDDLStream in other systems mentioned); chemical feasibility inferred via documentation and tool outputs.",
            "integration_with_external_tools": "Web search engines, vector search over documentation, multi-instrument control code generation, integration with lab automation platforms to execute reactions.",
            "dataset_used": "Runtime web/documentation retrieval and tool-specific knowledge bases; not trained on a fixed chemistry dataset for the autonomous agent described in review.",
            "evaluation_metrics": "Success in executing complex experiments (qualitative and task-completion metrics); review cites successful implementation of Suzuki and Sonogashira cross-couplings.",
            "reported_results": "Review reports successful autonomous implementation of complex reactions (Suzuki and Sonogashira) and multi-instrument code generation capability; no detailed quantitative yields or rates provided in review.",
            "experimental_validation": true,
            "challenges_or_limitations": "Safety, correctness, and hallucination risks; need for constrained planning and human oversight; integration complexity with diverse instruments.",
            "uuid": "e6873.6",
            "source_info": {
                "paper_title": "Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Molformer (IBM)",
            "name_full": "Molformer / Large-scale chemical language representations",
            "brief_description": "Large chemical language representation model developed by IBM for capturing molecular structure and properties and for generative tasks including candidate molecule generation for antiviral and antimicrobial applications.",
            "citation_title": "Large-scale chemical language representations capture molecular structure and properties",
            "mention_or_use": "mention",
            "model_name": "Molformer",
            "model_type": "large transformer-based chemical language model (specialized)",
            "model_size": null,
            "training_data_description": "Trained on large-scale chemical corpora to learn molecular structure-property relationships (exact datasets not enumerated in review).",
            "generation_method": "Generative modeling of molecular structures from learned chemical language/embeddings; used to propose candidate molecules for downstream evaluation.",
            "chemical_representation": "SMILES and chemical language embeddings; graph-aware representations implied by name and usage.",
            "target_application": "De novo molecule generation for antiviral (SARS-CoV-2 inhibitors) and antimicrobial peptide design; molecular property prediction and screening.",
            "constraints_used": "Property-based constraints for antiviral/antimicrobial objectives implied; details not specified in review.",
            "integration_with_external_tools": "Used in downstream in-silico screening workflows and molecular dynamics simulations (as cited for related works), but specific toolchain integrations not enumerated here.",
            "dataset_used": "Large chemical corpora (public chemical databases implied), specific datasets not listed in review.",
            "evaluation_metrics": "Reported as 'promising results' for generated candidates aimed at inhibiting SARS-CoV-2 and generating antimicrobial peptides; no numerical metrics provided in review.",
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Review notes that general LLMs and some models struggle with quantitative tasks and that experimental validation is variably reported; Molformer-specific experimental validation not detailed in review.",
            "uuid": "e6873.7",
            "source_info": {
                "paper_title": "Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Moret et al. chemical language model",
            "name_full": "Chemical language model from Moret et al. (2023)",
            "brief_description": "A chemical language model used for de novo drug design that facilitated discovery of a new PI3KÎ³ ligand with sub-micromolar activity as an example of lead optimization via language-model-guided design.",
            "citation_title": "Leveraging molecular structure and bioactivity with chemical language models for de novo drug design",
            "mention_or_use": "mention",
            "model_name": "Chemical language model (Moret et al. 2023)",
            "model_type": "specialized chemical language model (generative)",
            "model_size": null,
            "training_data_description": "Trained on molecular structure and bioactivity data (exact datasets not listed in review summary).",
            "generation_method": "De novo generation guided by learned structure-bioactivity relationships; used in lead discovery/optimization workflows.",
            "chemical_representation": "SMILES / chemical language representations",
            "target_application": "Lead discovery / lead optimization (example: PI3KÎ³ ligand discovery)",
            "constraints_used": "Optimization for bioactivity (potency) and likely other property considerations although explicit constraints not enumerated in review.",
            "integration_with_external_tools": "Used within pipelines that combine property prediction and generative sampling; specific external tools not detailed in review.",
            "dataset_used": "Molecular structures and associated bioactivity datasets (not specified in review).",
            "evaluation_metrics": "Reported potency (bioactivity) as a primary metric; also typical generative metrics implied.",
            "reported_results": "Review states discovery of a new PI3KÎ³ ligand with sub-micromolar activity (i.e., potency &lt; 1 ÂµM); no further numeric detail provided in review.",
            "experimental_validation": true,
            "challenges_or_limitations": "General challenges: need for accurate property predictors, synthesizability, and experimental follow-up; specifics beyond those general points not provided.",
            "uuid": "e6873.8",
            "source_info": {
                "paper_title": "Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Fine-tuned GPT-3 in chemistry (Jablonka et al.)",
            "name_full": "Fine-tuned GPT-3 applied to predictive chemistry",
            "brief_description": "A fine-tuned GPT-3 model adapted to chemistry tasks that outperformed traditional ML models on several chemistry prediction tasks, especially in low-data scenarios, illustrating general LLM adaptability to chemical prediction tasks.",
            "citation_title": "Leveraging large language models for predictive chemistry",
            "mention_or_use": "mention",
            "model_name": "Fine-tuned GPT-3 (chemistry tasks)",
            "model_type": "decoder-only GPT-style model, fine-tuned for chemistry prediction",
            "model_size": null,
            "training_data_description": "Fine-tuned on chemistry-specific task datasets (low-data regimes highlighted); original GPT-3 pretraining on broad text/code corpora.",
            "generation_method": "Fine-tuning for predictive tasks; applied in few-shot / low-data settings for property prediction and other chemistry tasks (not necessarily de novo generation).",
            "chemical_representation": "SMILES and chemical text pairs (implied by MolT5/MolGPT analogies and review text)",
            "target_application": "Predictive chemistry tasks and low-data property prediction; capable of adapting to generative tasks with fine-tuning.",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": "Task-specific chemical datasets (low-data examples emphasized; exact datasets not listed in review).",
            "evaluation_metrics": "Performance vs traditional ML baselines (random forest, SVM etc.) on predictive tasks; review notes GPT-3 outperformed traditional ML in several tasks but provides no numeric values here.",
            "reported_results": "Qualitative claim: fine-tuned GPT-3 outperformed traditional machine learning models on several chemistry tasks, notably in low-data scenarios; no numeric results given in review.",
            "experimental_validation": false,
            "challenges_or_limitations": "General LLM limitations: less effective quantitative precision, potential hallucinations, and need for domain-specific fine-tuning for best performance.",
            "uuid": "e6873.9",
            "source_info": {
                "paper_title": "Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep learning enables rapid identification of potent ddr1 kinase inhibitors",
            "rating": 2,
            "sanitized_title": "deep_learning_enables_rapid_identification_of_potent_ddr1_kinase_inhibitors"
        },
        {
            "paper_title": "Chemistry42: an ai-driven platform for molecular design and optimization",
            "rating": 2,
            "sanitized_title": "chemistry42_an_aidriven_platform_for_molecular_design_and_optimization"
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2,
            "sanitized_title": "autonomous_chemical_research_with_large_language_models"
        },
        {
            "paper_title": "Chemcrow: Augmenting large-language models with chemistry tools",
            "rating": 2,
            "sanitized_title": "chemcrow_augmenting_largelanguage_models_with_chemistry_tools"
        },
        {
            "paper_title": "Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction",
            "rating": 2,
            "sanitized_title": "molecular_transformer_a_model_for_uncertaintycalibrated_chemical_reaction_prediction"
        },
        {
            "paper_title": "Chemformer: a pre-trained transformer for computational chemistry",
            "rating": 2,
            "sanitized_title": "chemformer_a_pretrained_transformer_for_computational_chemistry"
        },
        {
            "paper_title": "Molgpt: molecular generation using a transformerdecoder model",
            "rating": 2,
            "sanitized_title": "molgpt_molecular_generation_using_a_transformerdecoder_model"
        },
        {
            "paper_title": "Leveraging molecular structure and bioactivity with chemical language models for de novo drug design",
            "rating": 2,
            "sanitized_title": "leveraging_molecular_structure_and_bioactivity_with_chemical_language_models_for_de_novo_drug_design"
        },
        {
            "paper_title": "Leveraging large language models for predictive chemistry",
            "rating": 2,
            "sanitized_title": "leveraging_large_language_models_for_predictive_chemistry"
        },
        {
            "paper_title": "Large-scale chemical language representations capture molecular structure and properties",
            "rating": 2,
            "sanitized_title": "largescale_chemical_language_representations_capture_molecular_structure_and_properties"
        }
    ],
    "cost": 0.02440775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials
6 Sep 2024</p>
<p>Yizhen Zheng 
Department of Data Science and AI
Monash University</p>
<p>Huan Yee Koh 
Department of Data Science and AI
Monash University</p>
<p>Drug Discovery Biology
Monash Institute of Pharmaceutical Sciences
Monash University</p>
<p>Maddie Yang 
Harvard Medical School
Harvard University</p>
<p>Li Li 
Harvard Medical School
Harvard University</p>
<p>Wyss Institute for Biologically Inspired Engineering
Harvard University</p>
<p>Lauren T May 
Drug Discovery Biology
Monash Institute of Pharmaceutical Sciences
Monash University</p>
<p>Geoffrey I Webb 
Department of Data Science and AI
Monash University</p>
<p>Shirui Pan s.pan@griffith.edu.au 
School of Information and Communication Technology
Griffith University</p>
<p>George Church georgechurch@hms.harvard.edu 
Harvard Medical School
Harvard University</p>
<p>Wyss Institute for Biologically Inspired Engineering
Harvard University</p>
<p>Llms Automatic 
Target Gene </p>
<p>Identification Drug Discovery Natural Product Discovery Large Language Models in Drug Discovery and Development</p>
<p>Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials
6 Sep 2024FB2F9D1B1CC059644095373883DF6645arXiv:2409.04481v1[q-bio.QM]
The integration of Large Language Models (LLMs) into the drug discovery and development field marks a significant paradigm shift, offering novel methodologies for understanding disease mechanisms, facilitating drug discovery, and optimizing clinical trial processes.This review highlights the expanding role of LLMs in revolutionizing various stages of the drug development pipeline.We investigate how these advanced computational models can uncover target-disease linkage, interpret complex biomedical data, enhance drug molecule design, predict drug efficacy and safety profiles, and facilitate clinical trial processes.Our paper aims to provide a comprehensive overview for researchers and practitioners in computational biology, pharmacology, and AI4Science by offering insights into the potential transformative impact of LLMs on drug discovery and development.</p>
<p>Introduction</p>
<p>"Language is only the instrument of science, and words are but the signs of ideas."</p>
<p>--Samuel Johnson</p>
<p>The pursuit of new drugs to research and develop is a long-term commitment that typically takes 10-15 years and costs over $2 billion in order to bring a new drug to a patient (Berdigaliyev &amp; Aljofan, 2020).This complex procedure is traditionally divided into three stages: the first stage is to understand the disease and to choose the target of treatment; the second stage is to develop a focused approach to developing treatments towards the target; and the third stage is to test the treatments in clinical trials for their effectiveness.Each phase of the process is both time-consuming and resource-intensive, this is because of the complexity of biological systems and the extensive nature of the review required of each phase in the research and validation process.The slow and protracted nature of the process often prevents the introduction of new therapies that would improve and extend human life.Consequently, there are extraordinary dividends to be reaped by introducing efficiencies and expanding the capabilities of current practices.</p>
<p>Artificial intelligence (AI) tools have emerged as preeminent innovation in the quest to accelerate drug discovery and development.Among these tools, large language models (LLMs)1 have distinguished themselves through their capabilities in understanding scientific language and executing various downstream tasks essential in drug discovery and development.Recent LLM breakthroughs, Geneformer (Theodoris et al., 2023), pretrained on 30 million single-cell transcriptomes, can help in disease modeling and successfully identified candidate therapeutic targets for cardiomyopathy via in silico deletion.Notable LLMs for facilitating chemistry experiments, Boiko et al. (2023) and Chemcrow (Bran et al., 2023), have highlighted the potential of LLMs in automating chemistry experiments related to drug discovery, specifically in the fields of directed synthesis and chemical reaction prediction.Other works, such as LLM4SD (Zheng et al., 2023), showed that LLMs can perform scientific synthesis, inference, and explanation directly from raw experimental data and formulate hypotheses that resonate with human experts' analysis.Med-PaLM (Singhal et al., 2023), a mega size LLM encoding clinical knowledge, was the first to reach human expert in USMLE-styled ques- In the past, each stage of drug discovery involved numerous manual tasks, which requires significant human effort and substantial resources.Nowadays, advancements in biotechnology, alongside the integration of AI and computer-aided in silico computation tools, have reduced the need of human labor and resources.However, we have yet to have a highly automated drug discovery pipeline, especially in the clinical trial phase, where trail design and matching are still mainly done by clinical practitioners.In the future, it is anticipated that the continued development of LLMs and their application in drug discovery will enable a highly automated drug discovery process.</p>
<p>tions, a medical licensing examination.This advancement highlights the potential of LLMs to liberate clinical practitioners from the laborious activities associated with clinical trials.</p>
<p>With advancements in LLMs, these technologies have the potential to revolutionize the drug discovery pipeline, with future drug discovery including highly automated LLM applications across the three stages of drug discovery (Figure 1).To understand disease mechanisms and aid in target identification, LLMs can perform comprehensive literature reviews and patent analyses to explore the biological pathways involved in diseases.Additionally, they can conduct functional genomics analysis to pinpoint target genes.By analyzing gene-related literature, including results from in vivo or in vitro experiments, LLMs can compare data on various genes and recommend those with favorable characteristics, such as a desirable mechanism of action or strong potential as drug targets.Furthermore, through analysis and review of literature, LLMs may infer new insights and uncover principles of biochemistry and pharmacology.In the drug discovery and development phase, LLMs have the potential to automate related chemistry experiments by understanding chemical reaction and controlling robotic equipments.In addition, LLMs can offer an interactive platform aiding experts in discovering novel and effective compounds through suggestions for molecule editing and generation.LLMs could also assist in the design of new therapeutic ap-proaches, such as gene therapy.For instance, LLMs could help in the design of Adeno-associated virus (AAV) vectors by quickly summarizing scientific literature to identify novel strategies and by analyzing genomic sequences to predict the most effective vector sequences for safe and efficient gene delivery.During the clinical trial phase, LLMs could streamline the tedious tasks of matching patients with trials and designing trials by interpreting patient profiles and trial requirements.Additionally, early research has shown that LLMs might be capable of predicting trial outcomes by examining historical clinical data.</p>
<p>In this survey, we aim to comprehensively address three questions for researchers and practitioners seeking to harness the power of LLMs to improve the drug discovery and development pipeline:</p>
<p>1) How can LLMs be effectively integrated into various drug discovery and development stages?First, the types of LLMs under consideration must be defined (Figure 2).Then, we categorize the whole drug discovery and development pipeline into three linear stages: "Understanding Disease Mechanisms" (Figure 3), "Drug Discovery" (Figure 4), and "Clinical Trials" (Figure 5) to illustrate the blueprint of integrating LLMs into these processes, respectively.The left column of each figure describes the specific processes involved in these stages, while the right column covers the tasks that LLMs can perform to facilitate these stages.The The two main paradigms of language models.Specialized language models are trained on specific scientific languages and are typically tailored for specific or a few science-related tasks.These models are used as tools to perform a specific task, in which users provide the information required for a task, and the model outputs the prediction.General-purpose language models are trained on diverse textual information sourced from various materials, including scientific papers and textbooks.These models are used like an assistant that allows users to use plain language to interact with the model.visual representation seeks to illustrate how LLMs can optimize various aspects of drug development.</p>
<p>2) How advanced are LLMs in facilitating downstream tasks across various drug discovery and development stages?In order to determine the level of advancement of LLMs in supporting downstream tasks throughout different stages of drug discovery and development, we have evaluated current applications of LLMs and classified each one into one of four categories: not applicable, nascent, advanced, and mature.These indicators provide an overview of the current state in the field and indicate promising future directions (Figure 6).</p>
<p>3) What are the future directions of LLMs in drug discovery and development?We explore the evolving landscape of LLM development, promoting LLMs in more biological use cases, and addressing ethical, privacy, fairness, and bias concerns in future development.These concerns are increasingly apparent as LLMs are applied to handle sensitive health data and make critical medical decisions.We also discuss the need to overcome certain technical limitations associated with LLMs, such as the occurrence of hallucinations, the constraints of context window limit, and the need for better model interpretability and scientific understanding.Solving these challenges can enable LLMs to become trusted and efficient tools in the applications of drug discovery as well as in patient care.This is discussed in Section 5.</p>
<p>In this paper, we first elucidate the two paradigms of LLMs for drug discovery and development: specialized language models trained on specific scientific languages and generalpurpose language models trained on general textual language.We then delve into how LLMs can be helpful throughout each drug discovery and development stage, from understanding disease mechanisms to facilitating drug discovery and optimizing clinical trials.After covering each stage, we analyze the maturity of these LLM applications.Lastly, we discuss the future directions for LLMs in drug discovery and development.</p>
<p>Main Paradigms of Language Models</p>
<p>In drug discovery and development, the intricate text-based scientific languages used to describe chemicals and proteins, such as SMILES strings for encoding molecular structures (Weininger, 1988), and FASTA format for encoding protein, DNA and RNA sequences (fas, 1995), represent a unique form of structured language crafted by humans to encode domain-specific knowledge.To effectively interpret these languages, two main language model paradigms, including specialized language models (specialized LLMs) and general-purpose language models (general LLMs) emerged (Figure 2).</p>
<p>Specialised Large Language Models</p>
<p>The first paradigm of language models in drug discovery and development is specialized LLMs trained in specific scientific languages.These LLMs aim to decode the statistical patterns of scientific language, thereby enabling the interpretation of scientific data in its raw form (Figure 2).</p>
<p>Understanding Disease Mechanisms.Specialized LLMs can be used in many ways to explore diseases.For instance, LLMs can extract genomic information from single-cell RNA transcriptomic data and DNA sequences (Consens et al., 2023), enabling practitioners to determine epigenetic marks, transcription factor binding sites, functional genetic variants, and gene network analysis, all of which contribute to understanding the genetic basis of disease.</p>
<p>Additionally, protein LLMs, such as ESM (Rives et al., 2021), can be trained to predict parts of the amino acid sequence that have been intentionally hidden or 'masked' during training, such as "MVL<MASK>PAD" (Figure 2).Despite a simple training procedure, these specialized LLMs have been proven helpful in annotating the functions (Matic et al., 2022) and predicting the structures of proteins directly from protein sequences (Lin et al., 2023), significantly advancing our understanding of protein structures, and informing downstream drug discovery efforts.</p>
<p>Drug Discovery.In drug discovery, specialized LLMs are particularly helpful for accelerating various chemistry experiments (Bran et al., 2023;Park et al., 2023).A specialized LLM trained in the molecular SMILES language can help in retrosynthetic planning and predicting reaction outcomes; at the same time, it can help chemists in de novo molecules guided by some specific molecular properties, such as increasing binding affinity towards targets.Moreover, these models can also play a role in ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) prediction, a critical step in assessing molecule properties and filtering out those with undesirable characteristics.</p>
<p>Usage.Specialized LLMs are tool-like, where the user inputs information needed for a given task and receives a model prediction in return (Figure 2).For instance, when we use a specialized LLM to query protein-ligand binding affinity, both protein sequences and ligand SMILES strings must be provided to the model, which will subsequently output the predicted binding affinity score.</p>
<p>General-purpose Language Models</p>
<p>The second paradigm encompasses general LLMs, which are trained on a diverse array of textual information sourced from various materials, including but not limited to scientific papers, textbooks, and general literature.Such breadth in training allows them to achieve a broad understanding of human language, which includes a significant grasp of scientific contexts.Models like GPT-4 (OpenAI, 2023;AI4Science &amp; Quantum, 2023) and Galactica (Taylor et al., 2022) have been noted for their proficiency in also mastering complex formal scientific description languages, including SMILES strings and FASTA format.Using this capacity, general LLMs can work on tasks that would typically require the participation of domain professionals, such as making inferences, doing reasoning and analysis, and applying field-specific knowledge across different scientific domains.</p>
<p>Understanding Disease Mechanisms.General LLMs can traverse a large volume of literature, extract data, and summarize for users.Furthermore, it can also synthesize the extracted data into a knowledge graph, revealing how genes and diseases are connected, helping scientists uncover the basis behind diseases (Savage, 2023).Furthermore, these models can explain technical terminologies in layperson's language, making understanding complex concepts and principles easy, significantly aiding in education and communication.</p>
<p>Drug Discovery.In drug discovery, general LLMs have great potential to accelerate experimental practices.Recently, general LLMs have been applied in chemistry robotics for automated experiments.General LLMs also exhibited expert-level capabilities in retrosynthetic planning and reaction prediction (Bran et al., 2023;Boiko et al., 2023), while only costing a fraction of human experts.Some preliminary attempts are underway to train general language models to perfect tasks currently more suited for specialized LLMs, such as de novo molecule and protein generation and editing (Liu et al., 2023c).The primary motivation is that, unlike specialized LLMs that can only learn data patterns from specific scientific languages, these LLMs can reason and apply the domain knowledge learned from extensive literature.However, research in this direction is still in its infancy.</p>
<p>Clinical Trials. General LLMs provide significant advantages in analyzing electronic health records and clinical</p>
<p>Disease Pathway Analysis</p>
<p>Expression Profile Analysis</p>
<p>Multiomics</p>
<p>Ligand</p>
<p>Binding Site Prediction
Sequence A Sequence B Consensus Sequence C Sequence D Sequence E Sequence F F S T A A F R F G H A V H P L V R R L F S A F R F V H P L R R L G T A H A V F T A F R F G I P P M H R L T A H A V F S A F R I H P L R L F G T A H A V R F A A F R F G Q I R L T A H A V P V R F S A F R F G H H L R L T A A V P V R F s T A A F R F G H A v h P l V r R L 115 120 125</p>
<p>NF-ÎºB</p>
<p>Splice Site Prediction Protein1 Protein2</p>
<p>Experimental Toolkits + + +</p>
<p>Multi-pathway Analysis</p>
<p>Protein Target Analysis</p>
<p>Gene Network Analysis</p>
<p>Genetic Variants</p>
<p>Protein X Protein Y</p>
<p>Protein-Protein Interaction Prediction</p>
<p>Genomics Analysis</p>
<p>Transcriptomics Analysis mRNA Expression Analysis</p>
<p>Promoter protocols (Singhal et al., 2023;Jin et al., 2023c;Huang et al., 2020).They can facilitate patient-trial matching, assist in trial planning, help predict trial outcomes, and assist in document writing.The user-friendly chat interfaces of general LLMs also make it easier for practitioners to interact with them.</p>
<p>Promoter Region Prediction</p>
<p>Safety and Feasibility
Modulator
Usage.A natural language-based AI Assistant is a strong use case for general LLMs (Figure 2).For example, if a user wanted to know what some key features are in predicting the solubility of molecules, they could ask a general LLM, and it would retrieve and summarize relevant information from the literature.</p>
<p>LLMs in Drug Discovery and Development</p>
<p>This section discusses how LLMs can be applied in three drug discovery and development pipeline stages: understanding disease mechanisms, drug discovery, and clinical trials.</p>
<p>Understanding Disease Mechanisms</p>
<p>Understanding disease mechanisms is the initial and crucial stage of the drug discovery and development pipeline.The primary aim of this stage (Figure 3) is to identify a suitable protein target for a potential drug to act upon (Lindsay, 2003).This process involves three key steps: clinical subtyping, target-disease linkage analysis, and target validation.</p>
<p>Clinical sub-typing in drug discovery involves categorizing patients into subgroups to collect clinical and multiomics data and aids in understanding disease variations and identifying potential differences in disease mechanisms across patient groups (CortÃ©s-Cros et al., 2013;Pun et al., 2023).</p>
<p>The target-disease linkage analysis phase in drug discovery involves establishing connections between potential protein targets and specific diseases.This phase encompasses pathway analysis to investigate biological pathways involved in the disease and expression profile analysis to study disease-related gene expression patterns (Plenge et al., 2013).Additionally, practitioners leverage experimental techniques to establish causal links between a target and the disease, including CRISPR-Cas9 (Lin et al., 2017), in-vivo disease modeling (Lindsay, 2003), and interference RNA (siRNA) (CortÃ©s-Cros et al., 2013).</p>
<p>After identifying a target in the drug discovery process, target validation is a crucial, non-linear step that follows target identification, involving a continuous validation cycle with no fixed starting point (Figure 3).This cycle includes assessing the necessary actions to be performed on the target for disease treatment (mechanism of action), choosing the most appropriate therapeutic intervention (modality selection), and conducting a comprehensive safety and feasibility as-sessment (Emmerich et al., 2021).The safety and feasibility assessment evaluates both the potential organismal impact (safety) and the target's druggability (Floris et al., 2018), as well as the practicality of assays for feasibility (Vincent et al., 2015).This flexible, iterative approach ensures thorough evaluation of the target's viability and safety at any stage before advancing in drug development, ensuring the selected targets are both theoretically promising and practical for further development.</p>
<p>GENOMICS ANALYSIS</p>
<p>Decades of genome-wide association studies (GWAS) have identified critical genomic regions linked to various diseases (Michailidou et al., 2015;2017;Nelson et al., 2017;Zengini et al., 2018) that have significantly advanced genomic-based analysis for disease understanding and target discovery.Notably, integrating genetic associations in drug discovery, could significantly improve the success rate of clinical targets (Nelson et al., 2015).</p>
<p>Recently, there has been significant interest in adapting advancement in LLMs used for human languages to genomic analysis, such as DNA-BERT (Ji et al., 2021), due to the structural similarities between DNA and human language.</p>
<p>Through specialized training on vast amounts of nucleotide sequences, these LLMs are adept at decoding the language of genetics.As a result, there has been an explosion in the field of specialized nucleotide LLMs (Ji et al., 2021) that are increasingly capable of understanding the cryptic "language" used by genomes more efficiently, enabling various downstream tasks in understanding genetic mechanisms of diseases.</p>
<p>Genetic variant analysis. The application of nucleotide</p>
<p>LLMs in genetic variant analysis hinged on the fact that genetic sequences follow specific language patterns and rules (Yanofsky et al., 1964;Altschuh et al., 1988).Variations in these sequences-be it single nucleotide polymorphisms (SNPs), insertions, deletions, or more complex rearrangements-can significantly impact gene function (Brendel &amp; Busse, 1984;Searls, 2002).Hence, they employ masked language modeling when nucleotide LLMs are trained on extensive genomic data.In this approach, the model learns to predict parts of the nucleotide sequence that have been intentionally hidden or 'masked' during training.This learning process enables the LLMs to decode the intricate, often hidden patterns and rules that govern the language of genes (Ji et al., 2021;Dalla-Torre et al., 2023).</p>
<p>Post-training, nucleotide LLMs have demonstrated the ability to detect significant functional genetic variants directly from DNA sequences.For example, DNA-BERT (Ji et al., 2021) showed that nucleotide LLMs selectively concentrate on the most relevant genomic regions.This enables the extraction of motif patterns that are evolutionarily conserved and aid in identifying functional variants of significance.</p>
<p>Similarly, Nucleotide Transformer (Dalla-Torre et al., 2023), another specialized nucleotide LLM, has also demonstrated the ability to prioritize functional genetic variants.Moreover, it can be further trained for specific variant identification tasks, such as classifying SARS-CoV-2 variants (Zhou et al., 2023) and understanding SARS-CoV-2 evolutionary dynamics (Zvyagin et al., 2023).</p>
<p>More recently, HyenaDNA, built on the Hyena LLM framework, has pushed the boundaries of genetic variant analysis by enabling the modeling of extremely long genomic sequences-up to 1 million tokens-at the single nucleotide level (Nguyen et al., 2024b).This is a significant leap from previous models that were constrained by the quadratic scaling of attention and limited to much shorter sequences.Hye-naDNA's ability to process such extensive context lengths allows it to capture long-range interactions in DNA, which are crucial for understanding complex genetic variations.It has achieved state-of-the-art performance on multiple benchmarks with a much smaller model and less pretraining data, marking a substantial advance in the field of genomic sequence analysis.This long-range capability, coupled with the precision of single nucleotide resolution, positions Hye-naDNA as a powerful tool in detecting and prioritizing functional genetic variants, further enhancing our understanding of genomic data.</p>
<p>Genomic regions-of-interest predictions.Promoter regions, transcription factor (TF) binding sites, and splice sites are all crucial elements in regulating gene expression.Despite their varied roles, they all contribute to the complex regulation of when, where, and how genes are activated or silenced.Alterations or mutations in these regions may bring about overexpression or underexpression of a gene, potentially causing diseases.However, despite the importance, predicting these regions remains challenging due to the need for more understanding in the language of DNA (Ji et al., 2021).</p>
<p>To address these challenges, specialized nucleotide LLMs are being fine-tuned to predict these regions of interest, with results showing an outperformance against previous state-ofthe-art methods (Dalla-Torre et al., 2023;Zhou et al., 2023).Fine-tuning these LLMs for genomic applications involves two key steps.First, an LLM is pre-trained on extensive datasets of nucleotide sequences, allowing the LLM to grasp the nuances of genetic language.Then, in the second stage, the LLM is further fine-tuned to incorporate domain-specific knowledge, which is the locations, biological functions, and additional biochemical and biophysical insights of different gene regulatory sites.</p>
<p>Epigenetic marks refer to chemical modifications, such as DNA methylation and histone modifications, that affect gene expression without altering the underlying DNA sequence.</p>
<p>Specialised LMs</p>
<p>General LMs</p>
<p>Genomics Analysis These marks play a crucial role in regulating gene activity, influencing both disease development and therapeutic targeting (Atlasi &amp; Stunnenberg, 2017).Accurately predicting these marks is essential for understanding how epigenetic changes impact gene expression and their implications in various diseases (Miranda Furtado et al., 2019).However, the complexity and variability of epigenetic marks present a significant challenge in making accurate predictions.Similar to the discussion above, pre-trained nucleotide LLMs are further fine-tuned to predict specific histone modifications, such as H3K14ac, H3K36me3, and H3K4me1 (Zhou et al., 2023).</p>
<p>3.1.2.TRANSCRIPTOMICS ANALYSIS.</p>
<p>Transcriptomics, a field that investigates the entirety of RNA transcripts that an organism or cell system generates under certain conditions, has experienced a surge in the volume of transcriptomic data derived from a wide range of human tissues due to the development of high-throughput technologies and single-cell technologies.However, the data is often sparse for specific disease states, particularly for rare diseases and diseases affecting clinically inaccessible tissues (Shao et al., 2021), so relying solely on these data for specific diseases would likely not suffice to develop robust and accurate models.To address existing limitations, specialized gene LLMs have been proposed to obtain a comprehensive understanding of transcriptomic data while offering the goods to adapt to scenarios with sparse data samples.</p>
<p>The primary technological development in this subfield is the specialized transcriptomic LLM, Geneformer (Theodoris et al., 2023), which developed an innovative method for mapping each single-cell transcriptome into a sequence of genes ranked by their expression levels.This approach, known as "rank value encoding", represents the transcriptome of each cell as a sequence of genes ordered based on their expression levels.These levels are then normalized against the overall expression observed across all human tissues.Through this technique, rank value encoding offers a distinct representation of gene activity within individual cells and facilitates a comprehensive comparison of gene expression across a diverse array of data (Theodoris et al., 2023).This approach is akin to learning the language of transcriptomics through specialized LLMs, enhancing our understanding of cellular behaviors and interactions at the molecular level.</p>
<p>By transforming single-cell transcriptomic data into gene sequences, Geneformer (Theodoris et al., 2023), scGPT (Cui et al., 2023), and other models like scMulan (Bian et al., 2024) and scFoundation (Hao et al., 2024) have demonstrated a remarkable ability to analyze transcriptomic data effectively as foundational LLM models.Furthermore, spe-cialized transcriptomic LLMs can be adapted to scenarios with sparse data through fine-tuning to model gene networks accurately to comprehend complex dynamics, including network interactions, extending beyond simple cell-level annotations (Ma et al., 2024).</p>
<p>In parallel, efforts have also been made to leverage biomedical literature for predicting future therapeutic targets by training LLMs on historical text corpora.A study used Word2Vec models on abstracts published between 1995 and 2022, allowing these LLMs to prioritize gene-disease associations and protein-protein interactions likely to be validated in future research (Narganes-CarlÃ³n et al., 2023).This approach, termed Publication-Wide Association Study (PWAS), encodes biomedical knowledge as word embeddings without human supervision, effectively capturing drug discovery concepts and prioritizing hypotheses years before experimental confirmation.PWAS demonstrates the potential of LLMs as a scalable system for early-stage target ranking, enhancing the ability to mine literature for underexplored therapeutic opportunities.</p>
<p>mRNA expression analysis.mRNA expression analysis can be challenging due to the need to derive meaningful insights from limited data scenarios, a vital aspect in enhancing our understanding of diseases.Traditional machine learning approaches, such as XGBoost (Chen &amp; Guestrin, 2016) and standard deep neural networks, usually start from scratch for each specific task.This methodology can be ineffective, especially if the data is limited, which is often the case in the fields of rare disease research or when working with tissues that are not accessible in a clinical setting.</p>
<p>The specialized transcriptomic LLM, Geneformer (Theodoris et al., 2023), leverages the general knowledge acquired from pretraining on transcriptomic data to adapt efficiently to a specific disease use case and demonstrates remarkable efficiency in gene network analysis with minimal data.A study successfully distinguished key factors in the NOTCH1-dependent network by fine-tuning on just 884 endothelial cells from healthy versus dilated aortas, outperforming other methods that used a much larger dataset of about 30,000 cells (Theodoris et al., 2023).</p>
<p>scGPT (Cui et al., 2023), another generated pre-trained transformer model for single-cell multi-omics data analysis, on the other hand, also showed the ability to generate meaningful cell-type clusters directly from the pre-trained model in a zero-shot manner (i.e., without additional fine-tuning).</p>
<p>Gene network analysis.Gene network analysis typically begins by mapping the gene regulatory networks and tracing the critical genes in a disease's progression to identify potential therapeutic targets.Specifically, the gene network analysis strives to uncover vital regulatory elements that can alter or modulate these networks in a desired manner (Theodoris et al., 2015;2021).Due to the lack of data, these regulatory elements are challenging to find, especially for rare diseases or conditions affecting clinically inaccessible tissues (Cui et al., 2023;Theodoris et al., 2023).</p>
<p>To address the challenge of gene network analysis, Geneformer (Theodoris et al., 2023) proposed leveraging the self-attention mechanism within the transformer (Vaswani et al., 2017) backbone of the LLM to address the challenge of gene network analysis.This self-attention is crucial, as the trained attention weights of the model for each gene reveal two essential aspects: (1) the genes to which a particular gene is paying attention and ( 2) the genes that focus on it.This process inherently constructs a gene network, intricately mapping the web of gene interactions.Geneformer (Theodoris et al., 2023) also employs "in silico deletion", a technique similar to perturbation analysis for virtually removing specific genes to study their impact on gene networks.scGPT (Cui et al., 2023), on the other hand, uses embedding computing to construct a gene network via gene-gene similarities.</p>
<p>PROTEIN TARGET ANALYSIS</p>
<p>A protein sequence is often the most accessible data about a target that can help explain its potential to play a role in disease mechanisms, with drug discovery scientists often targeting it as a starting point.In this application, specialized LLMs can be particularly valuable as these models have shown the ability to provide extensive analyses, including evolutionary conservation, functional annotation, protein folding, and binding site prediction.The unique ability of the LLMs to extract relevant information from sequence data alone has provided a means of characterization of target biological traits and functions even without experimental data like experimentally determined 3D protein structures.</p>
<p>Evolutionary conservation.The use of specialized LLMs in protein analysis, as explained in the representative work of ESM (Rives et al., 2021), is based on a fundamental idea: the statistical patterns of protein sequences contain valuable information about their biological function and structure, which have been shaped by evolutionary processes (Yanofsky et al., 1964;Altschuh et al., 1988).This idea proposes that mutations that improve an organism's fitness are more likely to be selected by evolutionary forces among the multitude of possible mutations a sequence can undergo (GÃ¶bel et al., 1994), resulting in unique signatures in protein sequence patterns.</p>
<p>Specialized protein-based LLMs can effectively predict likely mutations within protein sequences that contain masked amino acids.This proficiency not only enables them to understand evolutionary conservation but also allows them to grasp the selection processes driving the evolution of these sequences.Follow-up research has demon-strated the efficacy of this approach using the ESM language model, which can make accurate predictions of mutational effects across a variety of proteins with different functions without any additional training (Meier et al., 2021).MSA-Transformer takes this approach further by analyzing multiple sequences using a multiple sequence alignment (MSA) approach (Rao et al., 2021).With the added information from the MSA as input, MSA-Transformers enhance their ability to interpret complex relationships within protein sequences and outperform single-sequence LLMs (Meier et al., 2021).</p>
<p>Understanding evolutionary conservation using specialized</p>
<p>LLMs is significant because it provides information not only on the functional landscape of proteins according to amino acid conservation patterns but also enables the establishment of the role and significance of individual residues in binding and activity (Altschuh et al., 1987).This is important as such conserved sites typically contribute to protein function and structure, while covarying mutations are similarly associated with these features, including contact surface, structure, and binding (Levitt, 1978;Yanofsky et al., 1964;Altschuh et al., 1988).Such findings are crucial for developing specialized LLMs for proteomics and form the basis for using these models to provide insight into predicting protein folding, binding sites, and functional annotation.</p>
<p>Protein folding.The sequence patterns of a protein are shaped by its hidden structure, which is linked to evolutionary conservation and mutation.Specific structures and sequences are conserved due to their functional importance, while mutations occur in response to evolutionary pressures.As a result, LLMs that learn from protein sequence data can indirectly capture these evolutionary trends.This is exemplified by the groundbreaking work in ESM (Rives et al., 2021) and MSA-Transformer (Rao et al., 2021), which showed that LLMs can accurately decode the structural nuances of proteins from sequence data alone.Specifically, when these LLMs create pairwise interaction maps (attention matrices) between all amino acid positions in a sequence, they demonstrate an ability to infer which pairs of amino acids should be in contact with unparalleled accuracy (Rao et al., 2020;Fung et al., 2022).This remarkable ability strongly suggests that a significant amount of structural information can be directly inferred from the LLM model using only sequence data, in line with Anfinsen's dogma (Anfinsen, 1973).</p>
<p>Building on foundational research, AlphaFold2 (Jumper et al., 2021) and RosettaFold (Baek et al., 2021) have revolutionized the field of protein structure prediction.These models can now produce atom-level accuracy even in cases where similar structures are not known, due to the Evoformer component in AlphaFold2, a specialized proteinbased LLM.In this way, AlphaFold2's training objective mirrors that of MSA-Transformer, where residues from MSA undergo random masking before being reconstructed by the Evoformer (Jumper et al., 2021;Hu et al., 2022).Through this process, AlphaFold2 can reason over MSAs and incorporate valuable evolutionary information to achieve near-experimental accuracy at the whole protein's structure level (Mirdita et al., 2022;Ahdritz et al., 2024).Furthermore, this same approach has been further extended to understanding biomolecular interaction with RosettaFold All-Atom, enabling the modeling of complex biomolecular assemblies, such as protein-protein complexes, protein-DNA/RNA interactions, and protein-small molecule interactions (Krishna et al., 2024).</p>
<p>In a similar timeframe, RGN2 (Chowdhury et al., 2022) developed ProtBERT to encode protein sequence data and predict structure directly from a single sequence, without requiring evolutionary information from an MSA.RGN2 has demonstrated the ability to match or even surpass Al-phaFold2 (Jumper et al., 2021) in predicting the structure of orphan proteins that lack sequence homologs (Chowdhury et al., 2022).</p>
<p>Functional annotation.As elucidated by the early works (Rao et al., 2019;Rives et al., 2021;Brandes et al., 2022;Lin et al., 2023), specialized protein-based LLMs can encode rich structural and functional information (Bepler &amp; Berger, 2021).Naturally, the rich information encoded within LLMs is now being harnessed in advanced applications like NetGO 3.0 (Wang et al., 2023a) to advance automated function prediction of proteins without costly experiments.The same approach was used by Matic et al. (2022) for GPCR sequence analysis using ESM (Rives et al., 2021), aiming to predict their signaling and functional repertoire.The research showed that the interaction mechanism between different protein variants is due to alternative splicing of genes, and GPCRs can be clearly defined with the help of specialized protein-based LLMs.</p>
<p>With advancements in general LLMs, ProteinChat (Guo et al., 2023) was proposed to provide an interactive platform where users can upload protein sequences and structures and pose questions about a brief description of a protein's functionalities.A recent work (AI4Science &amp; Quantum, 2023) also demonstrates that GPT-4 demonstrates considerable expertise in understanding proteins.</p>
<p>Despite these advances, the study of protein language models has remained relatively limited in scope.However, ESM2 (Lin et al., 2023) and ESM3 (Hayes et al., 2024) have been addressing this gap by scaling up to an impressive 15 billion and 98 billion parameters respectively, making them some of the largest protein language models evaluated to date.This substantial increase in scale has enabled ESM models to better learn the sequence-structure-function relationships of proteins.Using ESM as the foundational LLM, ESMFold has demonstrated an unprecedented ability to closely match the performance of AlphaFold2 in predicting protein structures (Lin et al., 2023).Notably, ESMFold achieves this high level of precision by analyzing a single sequence, and its ability to operate without querying a database significantly enhances its speed and ease of use.</p>
<p>Protein-ligand interaction and binding site prediction.Protein-ligand interactions and understanding protein binding sites play a pivotal role in understanding protein function and interactions and serves as an essential foundation for rational-based drug discovery.Gaining insight into these interactions is crucial in identifying potential safety issues related to target proteins and acts as guidance for the design of therapies.Protein-based specialized LLMs have demonstrated notable successes leveraging their extensive understanding of protein language.These include the prediction of metal ion binding sites (Yuan et al., 2022), protein-protein binding sites (Fang et al., 2023), and small molecule binding sites (Zhang &amp; Xie, 2023).Understanding the interactions between proteins is thus fundamentally important in deciding a valid target for treating diseases.Furthermore, this knowledge can contribute to the design of biologics-based drugs, as discussed in sections 3.2.2 and 3.2.4.</p>
<p>To this end, protein-based LLMs have advanced with the introduction of AlphaFold-Multimer (Evans et al., 2021).While its initial application is in predicting multimeric complex structures, it turns out that AlphaFold-Multimer is capable of predicting protein-protein interaction directly from protein sequences as accurately as unique protein-protein docking methods that use experimentally-determined structures (Ketata et al., 2023).</p>
<p>Another significant development in protein-based LLMs for protein-protein interaction is DockGPT (McPartlon &amp; Xu, 2023), an innovative approach in protein docking.This end-to-end deep learning method stands out for its flexible and site-specific protein docking capability, effectively accommodating conformational flexibility and utilizing binding site information compared to AlphaFold-Multimer.Its strength lies in its ability to process unbound and predicted monomer structures.Notably, DockGPT (McPartlon &amp; Xu, 2023) showed that the protein-based LLM can effectively deal with antibody-antigen complexes, achieving high accuracy in predicting binding poses as well as co-design the sequence and structure of antibody regions targeting specific epitopes.</p>
<p>When it comes to assessing ligand interaction sites, Phosformer has proven to be a significant improvement.Unlike previous methods such as MusiteDeep, DeepPhos, and Ember (Wang et al., 2017;Luo et al., 2019;Kirchoff &amp; Gomez, 2022), which relied on multiple models specific to different protein families or groups, Phosformer uses its comprehensive understanding of protein language to make accurate predictions with just one model.This means that virtually any kinase interacting with any peptide can be used to predict phosphorylation sites.Meanwhile, ProtT5 (Elnaggar et al., 2021), a specialized protein-based machine learning model, has been used to successfully predict binding sites for metal ions, nucleic acids, and small molecules (Littmann et al., 2021).This approach has even been advanced to predict genome-wide annotations for these binding sites (Yuan et al., 2023).</p>
<p>Finally, recent advancements in protein-ligand interaction modeling using LLMs have been significantly enhanced by RosettaFold All-Atom (Krishna et al., 2024).Unlike earlier tools that focused primarily on polypeptide chains, RosettaFold All-Atom incorporates a wide range of ligands, including small molecules, metal ions, and nucleic acids, into its predictions.This comprehensive approach not only enables highly accurate modeling of protein-ligand complexes, but provide analysis of key cellular processes for protein target analysis, offering deep insights into disease mechanisms and providing a powerful tool for drug discovery (Krishna et al., 2024).</p>
<p>PATHWAY ANALYSIS</p>
<p>In pathway analysis, gene regulatory network analysis can be a powerful tool for researchers seeking to descipher complex disease pathways.In this subsection, we will be focusing on the use of general LLMs, which can provide all-around assistance for pathway analysis.</p>
<p>Unlike their specialized counterparts, general LLMs are innately equipped with a wealth of prior knowledge gleaned from vast scientific literature and datasets (Taylor et al., 2022;OpenAI, 2023).This extensive background enables them to approach pathway analysis with a broad, informed perspective rather than specializing in a single scientific language.Furthermore, general-purpose LLMs have the distinct advantage of being able to interactively and conversationally engage with complex scientific data (OpenAI, 2023;Jeblick et al., 2023), providing researchers with a powerful tool for understanding and exploring their findings.</p>
<p>A recent study showcased the capacity of general-purpose LLMs, such as GPT-4, in analyzing blood transcriptional modules related to erythroid cells, demonstrating that these models are efficient in knowledge-driven pathway analysis (Toufiq et al., 2023).This research uses general LLMs to automatically generate codes for gene networks, summarize candidate genes ranked based on association tests, generate reports for users, and fact-check the report against the literature.In each task, the rich prior knowledge and interactive capabilities of general LLMs are exploited to analyze scientific data.By leveraging the rich prior knowledge and interactive capabilities of general LLMs, this study highlights how they can enhance disease mechanisms and target identification, allowing for a better understanding of complex gene networks.Ultimately, this transforms pathway analysis from static approaches to more dynamic and interpretable methods.</p>
<p>ASSISTANCE</p>
<p>The exploration of disease mechanisms is a complex task, requiring the contribution of experts from fields across health and pharmaceutical industries.In this environment, generalpurpose LLMs that can perform tasks related to interactive and conversational skills, including information retrieval and knowledge explanation, can play a crucial role (Taylor et al., 2022).</p>
<p>General-purpose LLMs offer fast and accurate information retrieval, clear explanations tailored to user needs, and the ability to organize and categorize large datasets, enhancing workflow and productivity (Jeblick et al., 2023).By integrating with search engines, recent LLM iterations provide real-time access to scientific data, improving hypothesis generation and validation in disease research.Additionally, they aid in scientific communication by simplifying complex ideas for laypeople, fostering better collaboration among specialists with different expertise.</p>
<p>Drug Discovery</p>
<p>The drug discovery process is a crucial phase in the drug development pipeline, encompassing several critical steps as depicted in Figure 4.These steps include hit identification, hit to lead, lead optimization, and preclinical development.</p>
<p>The process starts with "hit identification", where professionals find compounds with potential therapeutic effects.Next, "hit to lead" involves a more refined selection from these hits, identifying those most promising for further development.The third step, "lead optimization", is a critical process of enhancing a lead compound's efficacy, stability, and safety via editing.Finally, "preclinical development" entails rigorous testing of the optimized lead compound in animal models to assess its suitability for human trials.</p>
<p>Our survey will begin by outlining the specific downstream tasks associated with each operation.Following this, we will explore how LLMs can be incorporated into these tasks to advance the drug discovery process.</p>
<p>CHEMISTRY</p>
<p>Medicinal chemistry is essential to drug discovery and development through independent laboratory work and compound synthesis.Autonomous lab operations use robotic manipulators, controlled and programmed to execute complex chemistry and synthetic reactions.In addition, highthroughput screening requires compounds to be precisely and efficiently synthesized as part of the hit identification phase.After synthesis, the compound will be evaluated for activity and selectivity using pharmacological assays.</p>
<p>LLMs have proven to be highly valuable in these fields.LLms can help generate codes that program the chemistry robotics based on user requirements (Bran et al., 2023;Boiko et al., 2023).In particular, LLMs can translate user requirements into complex experimental protocols and convert them into specific, understandable robot instructions.Additionally, LLMs are successful in retrosynthetic planning and reaction prediction, offering to recommend feasible synthetic routes and to predict possible chemical reactions.Using LLMs in this manner is beneficial as it brings efficiencies in compound synthesis and accelerates the drug discovery process.</p>
<p>Chemistry Robotics.Nowadays, chemistry robotics is an integral part of conducting chemistry experiments using autonomous laboratory operations.This technique involves converting instructions written in natural language into robot-executable plans, usually described using a fixed, well-defined language that resembles coding.General LLMs like GPT-4 (OpenAI, 2023) and CodeLlama (Roziere et al., 2023) have shown the ability to generate effective code.Therefore, it is logical to use general LLMs to generate robot-executable plans, as they have been trained on a vast amount of code.One notable application is CLARify (Yoshikawa et al., 2023), which utilizes GPT-3 (Brown et al., 2020) to generate task plans in a specific Chemistry Description Language (XDL) based on descriptive user instructions in natural languages.Constrained task and motion planning problems are then solved using PDDLStream solvers.This approach aims to facilitate the autonomous and safe execution of chemistry experiments using general-purpose robot manipulators.Notably, these plans have shown much higher accuracy than baseline systems like SynthReader (Mehr et al., 2020).Furthermore, there are preliminary attempts using GPT-4 to generate compatible scripts in Python to control OT-2 (Inagaki et al., 2023), a computer-controlled liquid handling robot, achieving 95% success within five iterations.</p>
<p>An emerging branch of AI research involves utilizing large language models as agents that will autonomously create, perform, and program scientific experiments.Boiko et al. (Boiko et al., 2023) presented one such method, showing how these models could use web search engines for information about molecule synthesis or employ vector search to find relevant documentation on chemical reactions.They have also developed multi-instrument systems code generation agents that can successfully implement complex experiments like Suzuki and Sonogashira cross-coupling reactions.</p>
<p>Retrosynthetic Planning &amp; Reaction Prediction.Ret-rosynthetic planning involves breaking down complex compounds into simpler precursor compounds, while reaction prediction entails forecasting the outcome of chemical reactions.These two tasks are pivotal for understanding how to synthesize complex molecules from more basic starting materials, which is an essential step for preparing experiments like high-throughput screening.</p>
<p>An early attempt at LLMs for retrosynthetic planning is the Molecular Transformer (Schwaller et al., 2019), which utilizes a simple encoder-decoder transformer framework.The model is trained to take reactants and reagents as input and predict the chemical product that can be synthesized from a reaction.It has demonstrated higher accuracy in reaction prediction than human chemists.Subsequently, Schwaller et al. (Schwaller et al., 2020) combined the Molecular Transformer with a hyper-graph exploration strategy to develop an automated retrosynthetic route planning system.The dynamically constructed hypergraph represents a generic reaction where each molecule is a node, and the hyper-arc symbolizes the reaction arrow.This optimal synthetic route is identified using beam search over a beam search across the hyper-graph of possible disconnection strategies.</p>
<p>The capabilities of LLMs were further extended by Chemformer (Irwin et al., 2022), which is based on the BART (Lewis et al., 2020) architecture.It includes an additional pretraining process that involves reconstructing masked SMILES strings and using an autoencoder to convert original SMILES to embeddings and back.The model is then fine-tuned for various downstream tasks, including reaction and retrosynthesis predictions.</p>
<p>Recently, general-purpose LLMs have emerged in this field, such as Chemcrow (Bran et al., 2023) and Boiko et al (Boiko et al., 2023).Boiko's system uses web search and simple calculations, while Chemcrow adopted a more sophisticated approach.Chemcorw has developed and utilized a more comprehensive range of customized molecule and reaction tools.These include functionalities like converting queries to SMILES, obtaining molecule prices, patent checking, and reaction classification.Additionally, Chemcrow adopts a four-step framework to improve LLMs' ability: think about the necessary steps, take action using tools, provide inputs to these tools, analyze observations, and then deliver the final answer.This approach has been shown to perform better than GPT-4 in most tasks evaluated by humans in synthesis planning.Similarly, a recent study (Jablonka et al., 2024) demonstrated that a fine-tuned GPT-3 model was shown to outperform traditional machine learning models on several chemistry tasks, especially in low-data scenarios.The findings highlighted that LLMs, even those not initially trained on chemical data, could adapt to various predictive chemistry tasks with minimal fine-tuning, showcasing the potential of LLMs in advancing chemical research.</p>
<p>IN-SILICO SIMULATION</p>
<p>In-silico simulation leverages computer models to simulate complex biological processes.These simulations are pivotal in understanding and predicting how drugs interact at the molecular level, leading to more efficient and targeted drug development.The three main tasks involved in in-silico simulations are de novo molecule generation, de novo protein generation, and protein-ligand interaction prediction.</p>
<p>De novo Molecule Generation.De novo molecule generation is a complex task in in-silico simulations that involves creating new molecular structures with the potential to be effective drugs.This process is categorized into two types: unconstrained molecule generation, which seeks to populate the chemical space of the training set, and constrained molecule generation, where molecules are synthesized to meet specific desired properties (Brown et al., 2019).Constrained generation requires a model to consider various constraints such as affinity to targets, selectivity against off-targets, appropriate physicochemical properties, ADME characteristics, pharmacokinetics/pharmacodynamics, toxicology, and synthesizability (Loeffler et al., 2023).</p>
<p>To benchmark the performance of de novo molecule generation methods, GuacaMol (Brown et al., 2019) was introduced to provide a benchmarking framework considering aspects such as validity, uniqueness, and novelty.Specialized language models have demonstrated remarkable proficiency.For instance, it has been have shown that even simple RNN-based models can perform exceedingly well on challenging generative modeling tasks (Flam-Shepherd et al., 2022).These models effectively learn the complex distribution of molecules, such as the highest-scoring penalized logP molecules in ZINC15 or the most significant molecules in PubChem.To explore wide and novel chemical spaces, LLMs such as SMILES-LSTM and ORGAN (Guimaraes et al., 2017) have been assessed with the unconstrained generation ability to directly generates (Brown et al., 2019), and ORGAN.When it comes to constrained molecule generation, several notable approaches have been developed.Previous studies utilized reinforcement learning (Olivecrona et al., 2017) and pharmacophoric features (Skalic et al., 2019) to improve RNN-based models toward generating molecules with desired properties and binding ligands to protein pockets.Moreover, MolGPT (Bagal et al., 2021b), with its GPT architecture, can handle multiple constraints.It is trained by recovering a molecule with its scaffold and properties.The REINVENT series (Blaschke et al., 2020;Loeffler et al., 2023) represents a more advanced approach in this category.It is sweeping and capable of meeting up to 10 different objectives, including synthesizability, selectivity, etc.This method narrows the chemical search space through a threestaged training process: the prior model, a transfer learning agent, and staged learning towards generating high-scoring sequences.This sophisticated approach involves pretraining, transfer learning, reinforcement learning, and curriculum learning.</p>
<p>On the other hand, general LLMs usually focus on the constrained molecule generation task.In this context, MolT5 (Edwards et al., 2022) uses a self-supervised learning framework to pretrain T5 (Raffel et al., 2020), a generalpurpose LLM that is trained on a large corpus of text coupled with molecular data pairs.The pretraining methodology comprises unsupervised SMILES recovery and associated chemical texts.More recently, GPT-4 (OpenAI, 2023) has shown its ability to produce a novel molecule guided by textual instructions.However, the effectiveness generated by these two models is inferior to specialized language models.In addition, multimodal methods such as Momu (Su et al., 2022) and GIT-Mol (Liu et al., 2023a) enhance general LLMs' capabilities in molecule generation.Momu (Su et al., 2022) improves upon MolT5 (Edwards et al., 2022) by adopting CLIP (Radford et al., 2021) to align molecule graphs with related text.At the same time, GIT-Mol (Liu et al., 2023a), inspired by BLIP2's Q-FORMER strategy (Li et al., 2023a), integrates graph, image and text information, using cross-attention and a variety of pretraining tasks.This multimodal approach significantly improves the effectiveness of MolT5 (Edwards et al., 2022) in constrained molecule generation tasks.</p>
<p>De novo Protein Generation.Similar to molecule generation, this task focuses on designing new proteins in an unconditional manner (Hesslow et al., 2022;Ferruz et al., 2022;Nijkamp et al., 2023) or conditional manner, where proteins generated should fit user constraints (Wang et al., 2022a;Ram &amp; Bepler, 2022;Watson et al., 2023).Using these LLMs, scientists can design new artificial proteins that could serve specific functions, such as binding to a particular receptor or acting as enzymes.</p>
<p>Unconstrained generation aims to delve into and map the extensive protein space with specialized protein-based LLMs, such as those using autoregressive models for amino acid sequence generation, demonstrating remarkable effectiveness in this realm (Madani et al., 2020;Hesslow et al., 2022;Nijkamp et al., 2023).A prime example of these LLMs is ProtGPT2, detailed in Ferruz et al. (2022).Trained on a wide range of protein sequences, ProtGPT2 excels in creating de novo protein sequences that mirror natural patterns.Its outputs, marked by ordinary amino acid propensities, are predominantly globular, resembling natural proteins.Intriguingly, comparative analysis with protein databases indicates that ProtGPT2's generated sequences distantly related to existing proteins can form valid structures based on AlphaFold2.This indicates ProtGPT2's ability to explore novel and valid protein space areas.</p>
<p>Constrained protein generation seeks to achieve the controllable design of novel proteins with specified cellular compartments or functions (Ferruz &amp; HÃ¶cker, 2022).A typical practical constraint is generating protein sequences within the same family.This method ensures that new proteins retain the critical characteristics of a given group.Specialized LLMs like ProGen (Madani et al., 2023) and PoET (Watson et al., 2023) are utilized in this context.ProGen specifically generates sequences for a particular family, using a prefix like "Protein Family: Pfam ID", and has produced proteins with efficiencies close to natural counterparts, even with low sequence identity.PoET, on the other hand, compiles multiple sequences from the same family to create new sequences, akin to forming a paragraph from multiple sentences, thereby preserving the family's structural integrity.</p>
<p>Building on this, a more challenging yet direct path to drug development in constrained protein generation is the design of protein binders (McPartlon &amp; Xu, 2023).This task requires intricately designing proteins for specific binding functions, necessitating a deep understanding of how the entire protein folds into a desired structure with a few functional residues underpinned by the protein's overall structure.A more streamlined approach involves starting with a desired structure and, conditional on this structure, using a specialized LLM for inverse folding (Hsu et al., 2022).This inverse folding method starts with a complex protein structure and only seeks to leverage LLMs to convert the desired structure into protein sequences.Watson et al. (2023) modified RoseTTAFold, a specialized LLM already equipped with profound structural knowledge and an understanding of protein folding, with diffusion modeling to create RFDiffusion.This innovative approach allows RFDiffusion to begin with randomly distributed residue frames and systematically denoise them toward a valid protein structure.More recently, the development of RFdiffusion All-Atom (RFdiffusionAA) (Krishna et al., 2024) extends this capability by integrating atomiclevel detail to generate folded protein structures that specifically bind to small molecules, metals, and nucleic acids.By initializing the model with random distributions of residues around these small molecules, RFdiffusionAA can design highly specific binding pockets that are validated both computationally and experimentally.Subsequently, RFDiffusion can restructure the sequence in an inverse folding manner for greater accuracy, facilitating the design of diverse functional proteins based on simple molecular specifications.This method complements and enhances the potential of RoseTTAFold by enabling not only constrained protein design but also the creation of entirely new protein structures around various molecular targets.Notably, most of these constrained-based LLMs can also generate unconstrained protein structures, either by iteratively exploring the constrained space or by initializing the model via a random process.</p>
<p>Specialized protein-based LLMs above have shown considerable promise; however, despite evidence that they can design protein sequences that align with user requirements, it remains to be seen whether these LLMs, trained on natural protein sequences, can generalize beyond these to unnatural sequences.To address this question, Verkuil et al. (2022) leveraged the ESM protein language model (Rives et al., 2021) to design novel protein structures in unconstrained and constrained scenarios.By experimentally validating the generated proteins, it achieved a 67% success rate in creating functional proteins, some of which bear minimal similarity to known proteins.The impact of this validation on drug discovery and development is significant: it substantiates innovative LLM approaches for creating biologics and therapeutic proteins, potentially speeding up the creation of new protein-based treatments.Notably, with the advent of ESM3 (Hayes et al., 2024), the newer version of ESM, improvements in the ability to predict and design complex protein structures have been demonstrated.</p>
<p>More recently, general-purpose LLMs represent a newly emerging de novo protein generation technology.Pro-teinDT (Liu et al., 2023d), a multi-modal framework, incorporates textual descriptions for protein design.To train ProteinDT, a dataset of 441K text and protein pairs was constructed.By training on these pairs, ProteinDT was able to achieve over 90% accuracy for text-guided de novo protein generation.Furthermore, ProteinDT can be used to perform text-guided protein optimization tasks, as discussed in section 3.2.4.</p>
<p>Protein-ligand Interaction Prediction.Understanding the mechanisms underlying the interaction between a drug (ligand) and its protein target is fundamental to drug discovery and development.In recent years, virtual screening via in silico methods such as molecular docking or classic predictive machine learning models has been at the forefront of streamlining drug development processes.These tools form the cornerstone for accelerating the early-stage identification of potential therapeutic agents.In the early stages of drug discovery, specialized large language models (LLMs) are utilized in two primary directions: (i) to use LLM directly as a backbone and (ii) as a standalone yet essential part of a more comprehensive predictive system.Both LLM use cases are highly instrumental in improving the efficiency and effectiveness of the drug development process.</p>
<p>Tools such as AlphaFold-multimer (Evans et al., 2021), used explicitly for protein-protein docking (as detailed in section 3.1.3on protein-protein interaction), highlight the direct application of specialized LLMs as a backbone.Likewise, Singh et al. (2023) have employed protein LLM alongside molecular fingerprints for virtual screening.This technique has successfully identified binders with sub-nanomolar affin-ity, underscoring LLMs' increasing impact and potential in refining the drug discovery and development process.</p>
<p>More commonly, incorporating protein or ligand embeddings and structural information from specialized LLMs as part of complex system architectures has become a standard in assisting accurate prediction and docking (Wang et al., 2022b;Lu et al., 2022;Jiang et al., 2022;Corso et al., 2023;Ketata et al., 2023;Koh et al., 2024).PSICHIC (Koh et al., 2024) demonstrated that learning from sequence data alone (protein sequence and ligand SMILES string) can surpass methods that rely on experimental 3D structures or protein-ligand complexes.More significantly, when learning from sequence data alone, PSICHIC demonstrated emergent capabilities in deciphering the mechanisms underlying protein-ligand interactions.It successfully identifies protein residues in the binding site and ligand atoms involved in these interactions, a capability achieved through training PSICHIC exclusively on sequence data without any information on specific binding sites or residue-atom interactions.Such achievements suggest a highly promising avenue for LLMs in processing extensive biochemical data.If successful, this approach can enable prediction and foster an understanding of how various chemical structures interact with different proteins, potentially revealing many hitherto unknown aspects of the protein-ligand interactions without costly experimental endeavors.</p>
<p>In general-purpose Large LLMs, Galactica is equipped with in-depth general scientific knowledge and has also been jointly trained to predict the docking scores of proteinligand sequences (Taylor et al., 2022).It has demonstrated reasonable correlation with experimental results in certain instances.It is intriguing to consider the potential of developing an LLM that can directly understand, interpret, and predict protein-ligand interactions at a molecular level while also encompassing general knowledge.</p>
<p>ADMET PREDICTION</p>
<p>The prediction of absorption, distribution, metabolism, excretion, and toxicity (ADMET) attributes of compounds is a critical phase during the "Hit to Lead" and "Lead Optimization" stages of drug development.This task is essential to distinguish compounds with favorable pharmacokinetic profiles from those with negative characteristics, ensuring the progression of only the most promising drug candidates.Predicting molecular properties for multiple scientific areas, including physiology, physical chemistry, biophysics, and quantum mechanics, is the core of molecular property prediction in drug discovery.In recent years, both specialized and general-purpose large language models (LLMs) have shown remarkable predictive capabilities in the field of ADMET prediction, leveraging their ability to learn large amounts of data.</p>
<p>Specialized LLMs are usually trained on large datasets of SMILES strings and then fine-tuned for specific downstream property prediction tasks.For instance, ChemBERTa, based on the extensive PubChem 77M dataset, has shown comparable performance with traditional machine learning approaches like random forests and support vector machines.Similarly, SMILES Transformer, which employs an encoder-decoder architecture trained on over 861,000 unlabeled SMILES strings, has reached state-of-the-art performance.Recently, large-scale transformer-based models such as Molformer and BARTSMILES have demonstrated new state-of-the-art performance, though they require vast training time and resources.</p>
<p>General LLMs, on the other hand, either find knowledge to augment traditional machine learning models or are finetuned for specific tasks.LLM4SD can synthesize rules from literature and data sources that allow even random forest models to outperform all state-of-the-art methods for most tasks.Galactica and other models pre-trained on a significant amount of scientific literature have demonstrated capabilities in molecular property prediction with simple text instructions.Additionally, previous studies have highlighted the potential of other models such as GIT-Mol and MolT5, which could also yield satisfactory results after being finetuned on downstream tasks.GPT-4's extensive training on diverse text data enables it to provide valuable insights in the field, although adapting it for specific molecular property predictions might be challenging due to its proprietary nature.</p>
<p>LEAD OPTIMIZATION</p>
<p>Lead optimization is a significant step that aims to modify the drug candidate molecular structure or protein sequence to enhance its potency, safety, and stability.This process is usually carried out by chemists or biologists who modify according to their knowledge and experience.However, this process is time-consuming and requires much effort because it may take several attempts before arriving at the desired outcome.LLMs can assist with this task by utilizing statistical analysis on large data sets to predict how altering the structure of a compound would affect its properties.This feature supports more effective decision-making for chemists, minimizing the number of trials required for optimizing compounds.</p>
<p>Molecular Optimization.Molecular optimization is a complex task that involves modifying a molecular compound's structure to enhance its efficacy, stability, and safety.This process can be divided into two major categories: uncontrolled and controlled.In uncontrolled optimization, the core scaffold will be perserved but the LLMs will randomly modify the surrounding functional groups with external guidance to improve property values.In contrast, controlled optimization means users can specify parts of a molecule to be optimized for a given property.Through editing, the final compounds are expected to have enhanced properties.Compared with uncontrolled approaches, these methods allow chemists to have a greater degree of specification.</p>
<p>Specialized LLMs can aid in molecular optimization through both uncontrollable and controllable strategies.For uncontrollable optimization, models like the Reinvent series (Blaschke et al., 2020;Loeffler et al., 2023) and MER-MAID (Erikawa et al., 2021) use reinforcement learning to ensure that the synthesized molecules retain the desired structural scaffolds while enhancing properties such as potency, stability, or drug-likeness, guided by external models like drug-likeness filters or predictive algorithms.Specifically, MERMAID (Erikawa et al., 2021) incorporates a Monte Carlo Tree Search (MCTS) strategy, adeptly navigating potential molecular modifications to find the best ones.Apart from reinforcement learning, alternative methods such as fine-tuning and pretraining are also employed to enable specialized LMs with molecular optimization capabilities.An example is LigGPT (Bagal et al., 2021a), which focuses on generating molecules with specific scaffolds and desired properties.This model uses a trained transformer decoder architecture to reconstruct the original SMILES strings with the given scaffold and property information.For controllable optimization, Transformer-R (He et al., 2021a) and He et al. (He et al., 2021b) demonstrate its molecular optimization capability based on matched molecular pair (MMP) analysis.This model successfully considers property constraints and crucial components of SMILES strings while producing the required source R-GROUP for targeted molecular optimization.Improving these two approaches, C5T5 (Rothchild et al., 2021) does not rely on molecular pair data for training.This method is trained to recover masked IUPAC names by using specific tokens denoting the property range of the molecule.It shows successful applications for logP, logD, PSA, and Refractivity among other properties.</p>
<p>Recently, general LLMs have emerged, which present a novel avenue for amalgamating human expertise with LLM capability.The first one to consider is MoleculeSTM (Liu et al., 2023b), an application of multimodal learning for learning molecular structures from textual descriptions through contrastive learning.A dual-phase strategy is employed to enable a generative model to perform molecule editing.The first step involves training an adapter that aligns a molecule generative model with a joint representation space.In contrast, the second step focuses on fine-tuning the latent space to minimize differences between the generated molecule and given molecule-text instructions.An approach called ChatDrug (Liu et al., 2023c) significantly outperforms MoleculeSTM (Liu et al., 2023b) attributed to its integration with ChatGPT, which provides conver-sational capabilities.This agent-driven technique applies an iterative refinement procedure combining information retrieval and domain-specific feedback, improving drug optimization workflow.ChatDrug incorporates modules like the Prompt Design for Domain-Specific (PDDS) module, which uses extensive prompt engineering from language models like LLMs.Later on, the Retrieval and Domain Feedback (ReDF) module will help find molecules based on specific requirements.</p>
<p>Additionally, GPT-4 has demonstrated basic abilities to optimize molecular structures, even though it was not explicitly developed for this purpose (OpenAI, 2023).However, there are certain limitations; GPT-4 can innovate upon existing compounds without incremental feedback and continuous checks from humans.It can easily lead to inaccurate responses.</p>
<p>Protein Optimization.Like molecular optimization, protein optimization involves modifying the structure to enhance protein functionality and safety.In this field, biochemists and molecular biologists meticulously adjust proteins, a process that can be laborious and iterative.LLMs can contribute by offering predictions on how structural changes impact protein properties.Specifically, in the development of antibody drugs, language models can be used to consider multiple essential factors, including improving antigen binding, reducing immunogenicity, enhancing stability, and preventing high viscosity or polyspecificity (Beck et al., 2017;Nichols et al., 2015;Raybould et al., 2019).</p>
<p>In uncontrolled optimization, ESM (Rives et al., 2021), which is a protein-based LLM trained on diverse protein sequences with general evolutionary information, has been used to suggest evolutionarily viable mutations that could help enhance fitness across protein families (Hie et al., 2023).This work relies on evolutionary plausible mutation that generally improve fitness across proteins rather than specific properties.As a proof-of-concept, this work found notable improvements in matured IgG antibodies' affinity against different viral antigens achieved with minimal testing of the variants through only two rounds of evolution.</p>
<p>In controlled optimization, protein hallucination and inpainting have been proposed (Wang et al., 2022a) that resembles constrained optimization have been proposed to optimize proteins using a section of the protein masked off, and a protein LLM (Dauparas et al., 2022) is used to sample and refine the protein sequence while maintaining the original backbone structure.</p>
<p>Among the general-purpose LLMs, ProteinDT (Liu et al., 2023d) is a novel method that optimize protein sequences using prompts encapsulating specific properties information.ProteinDT can understand both natural texts and protein sequences.Leveraging this ability, the method employs two techniques: latent interpolation and latent optimization.Latent interpolation merges text prompt and protein sequence representations, while latent optimization aligns which aligns a token-level latent code with both text and protein representations.Experiments showcase that LLMs can conduct protein optimization, including structure, stability, and peptide binding optimization.Adopting such a strategy indicates a novel way in protein engineering for achieving text-guided, exact transformations on protein structures aiming at their required features.</p>
<p>Assistance</p>
<p>In the same way that researchers in the "Understanding Disease Mechanisms" phase require a diverse array of information sources, those involved in drug discovery also need access to various relevant resources.These resources can include, but are not limited to, comprehensive compound libraries, up-to-date research publications, and extensive patent landscapes.</p>
<p>To gather this information, a procedure known as information retrieval is used, which involves using General LLMs with web searching and knowledge retrieval to source information from various sources, such as research articles, compound databases, and patent documents.Additionally, tools like Galactica (Taylor et al., 2022) and GPT4 (AI4Science &amp; Quantum, 2023) can assist in clarifying scientific concepts and data, helping researchers achieve a deeper understanding.</p>
<p>Clinical Trials</p>
<p>Clinical trials represent the last stage of the drug development pipeline and are essential to evaluate a drug candidate's safety and efficiency.These trials take place in four phases, each serving a specific purpose.Phase 1 involves a small number of healthy volunteers who are administered the compound to evaluate its safety and tolerability.Phase 2 involves a larger group of patients to evaluate efficacy and side effects.Phase 3 is the last testing stage performed on many patients after an optimal treatment selection.In this phase, the new treatment is compared to existing treatments to understand its differences.The final phase is postmarketing surveillance, during which the drug is monitored for adverse effects.</p>
<p>Using LLMs, one of the most significant areas where they can be used is clinical practice, result analysis, and clinical assistance.The subsequent paragraphs will delineate the specific downstream tasks for each category of these tasks and how LLMs can be incorporated into them to enhance drug discovery further.</p>
<p>CLINICAL PRACTICE</p>
<p>In the realm of clinical trials, practitioners are typically tasked with four core responsibilities: coding ICD, matching patients with trials, predicting outcomes, and planning the trials themselves.These responsibilities span various areas and have traditionally been fulfilled by experienced practitioners who rely on their knowledge and expertise.Hence, clinical trial practitioners face the challenge of reading and understanding large amounts of information such as electronic health records (EHRs), trial eligibility criteria (ECs), trial protocols, and outcome reports.Fortunately, general LLMs have emerged as a promising solution for accelerating these processes, as they excel at extracting and handling information from large volumes of text data.</p>
<p>ICD Coding.ICD coding is an essential practice in clinical practice, which deals with assigning ICD codes to patient records.This is usually a time-consuming and laborious process.By analyzing vast amounts of EHR data, LLMs can predict the most suitable codes for Electronic Health Records (EHRs), thus enabling clinical practitioners to make more enlightened decisions and streamline the process.Shi et al. created a groundbreaking system (Shi et al., 2017) that utilizes a character-aware LSTM-based network to process diagnosis descriptions from hospital admission records more efficiently.Similarly, Xie and Xin (Xie &amp; Xing, 2018) developed a tree-of-sequences LSTM network to encode diagnosis descriptions and ICD codes.Inspired by Shi et al.'s work (Shi et al., 2017), they added adversarial learning to align various writing styles using an attentional matching module with isotonic constraints.These enhancements improve code assignments and prioritize more significant codes.</p>
<p>Some recent studies have used more contemporary and upto-date frameworks for their language models.For example, BERT-XML (Zhang et al., 2020b) incorporates BERT pretraining with multi-label attention to encode EHRs and later uses a multi-label classification model to predict ICD codes from EHRs.This approach takes advantage of the capabilities of the BERT framework to increase the accuracy of code assignment.Another example, PLM-ICD (Huang et al., 2022) adapts domain-specific pretrained language models such as BioBERT (Lee et al., 2020), PubMedBERT (Gu et al., 2021), and RobBERTa-PM (Liu et al., 2019b), for ICD coding by fine-tuning them.It also uses segment pooling and label attention to increase efficiency and accuracy in clinical coding contexts.</p>
<p>Patient-Trial Matching.When it comes to matching patients with clinical trials, the process relies on the use of electronic health records (EHRs) to identify viable options based on the patient's medical history.Historically, this task was performed manually by physicians and data an-alysts who would sift through patient demographics and pre-screening eligibility factors to pinpoint the most suitable trial.However, this approach can be time-consuming and fraught with errors due to the complexity and diversity of trial criteria.</p>
<p>To overcome these challenges, preliminary works typically encode EHRs and eligibility criteria into an embedding pair and then calculate the match score through similarity.An example of this is the cross-modal framework called Deep-Enroll (Zhang et al., 2020a), which utilizes BERT to capture eligibility criteria from the text-based patient records, using a hierarchical structure of latent representation.This framework finally observed a notable 12.4% COMPOSE (Gao et al., 2020a) significantly improves Deep-Enroll by utilizing a dual pathway encoding framework and a composite loss function.This approach effectively separates inclusion and exclusion criteria.The ECs pathway employs BERT and convolutional neural networks (CNNs) for robust encoding, while in the EHR pathway, hierarchical memory networks are deployed to organize medical concept hierarchies systematically.COMPOSE (Gao et al., 2020a) then dynamically interacts with these memories using EC embeddings, which help it to choose the most precise matches.</p>
<p>Recently, there have been several methods harnessing general-purpose LLMs to facilitate patient-trial matching based on LLMs reasoning ability.Med-monoT5 (Pradeep et al., 2022) is a T5-based system fine-tuned on medical passage ranking tasks that follows a zero-shot approach.It evaluates clinical trial documents' relevance to patient descriptions utilizing specifically designed templates.It employs a two-stage fine-tuning process on general and medical datasets, leveraging a sliding-window approach to handle lengthy text fields for matching patients with appropriate clinical trials.Hamer et al. (Hamer et al., 2023) use InstructGPT (Ouyang et al., 2022) to assist physicians in determining patient eligibility for clinical trials.Employ prompting strategies such as one-shot, selection-inference, and chain-of-thought-to parse and analyze the criteria.While this automation has been shown to potentially reduce up to 90% of the workload, achieving about 72% accuracy in screenability, it is not without issues.Overconfidence in interpreting ambiguous criteria and the risk of generating inaccurate content necessitate continued supervision by medical professionals to ensure reliability.Another pioneering work, TrialGPT (Jin et al., 2023c), uses an architecture that predicts criterion-level eligibility and provides detailed explanations.These explanations are aggregated to rank and exclude candidate clinical trials based on free-text patient notes.Although TrialGPT (Jin et al., 2023c) correlates well with expert annotations, its occasional errors highlight the limited medical knowledge of GPT 3.5 and the need for their careful integration into clinical trial matching processes.</p>
<p>Clinical Trial Planning and Prediction.Planning a clinical trial involves several labor-intensive activities, including searching for historical trials, designing trial criteria, and selecting suitable sites.To accelerate this process, general LLMs have been introduced in the domain.These models utilize historical data on trials, patient outcomes, and demographic trends to provide informed suggestions.</p>
<p>The first step in preparing a clinical trial is the search for historical trials, simplified by embedding trials into documents using document embedding methods.Although some traditional statistical models like BM25 (Trotman et al., 2014) and TF-IDF (Ramos et al., 2003) can produce such embeddings, their performance often needs to catch up to that of deep learning.Deep learning models such as Doc2Vec (Le &amp; Mikolov, 2014) and BERT (Devlin et al., 2019) provide more sophisticated embeddings but still face challenges in medical-specific retrieval tasks.These gaps are narrowed by medically tailored adaptations of BERT (Devlin et al., 2019), including clinical BERT (Alsentzer et al., 2019), clinical bioBERT (Alsentzer et al., 2019), TrialBERT (Wang &amp; Sun, 2022), and Med-monoT5 (Pradeep et al., 2022), which have been fine-tuned on substantial medical literature to provide highly accurate retrieval results.Going beyond these developments, the Trial2Vec (Wang &amp; Sun, 2022) framework extends from TrialBERT (Wang &amp; Sun, 2022) and utilizes hierarchical contrastive learning to generate global and local embeddings that incorporate semantic meaning based on document meta-structure.Additionally, cliniDigest (White et al., 2023) employs the summarization capabilities of GPT-3.5 to provide up-to-date, concise summaries of clinical trials, thereby facilitating quick decision-making for upcoming trials.</p>
<p>AutoTrial utilizes a two-stage training method with GPT-2 as its backbone to automate trial criteria design and clinical trial planning.This model employs the decoder architecture, learns from vast documents regarding previous trials during pretraining, and then gets its task-specific fine-tuning to generate exact trial criteria from given specifications.Its combination of a hybrid prompting strategy and multi-stage training makes it easy to adapt without retraining or performance loss.</p>
<p>Trial site matching involves finding a suitable trial site for a clinical trial.Modern algorithms integrate multimodal data containing unstructured and structured information about a trial to rank sites.One such algorithm is PG-Entropy (Srinivasa et al., 2022), which exploits ClinicalBERT (Huang et al., 2020)-based encoding of trial criteria text in combination with structured clinical trial features via a list-wise policy learning approach.It also gives equal importance to ensuring sensitive attributes do not adversely impact any bias-related outcome.In contrast, FRAMM (Theodorou et al., 2023) uses a deep reinforcement learning mechanism.</p>
<p>In this way, it is an effective remedy for the absence of data and for creating site representations using the masked crossattention mechanism.In addition, it balances diversity and enrollment when choosing sites via a Deep Q-Value Network that helps in decision-making by defining the reward function that focuses on both these aspects.</p>
<p>To predict the success of a clinical trial, trial outcome prediction analyzes various trial-related variables, such as information about the disease, the drug, trial criteria, and trial protocol.LLMs can provide insightful predictions to help trial designers enhance the trial plan.This can mitigate the risk of adverse outcomes from suboptimal trial configurations.</p>
<p>Existing methods mainly use LLMs to encode clinical trial information, which is then used to predict clinical trial outcomes.These LLM-based methods performed better than traditional machine learning methods such as logistic regression, MLP, and XGBoost (Chen &amp; Guestrin, 2016).For instance, given drug, disease, and clinical protocol information, HINT (Fu et al., 2022) amalgamates comprehensive web data such as drug properties, disease characteristics, and clinical trial information to assist the model in predicting both phase-level and trial-level clinical trial outcomes.Specifically, it separately encodes the drug molecules, disease characteristics, and clinical protocols using pre-trained models with web knowledge.Then, it builds an interaction graph using these components to generate the final prediction.</p>
<p>SPOT (Wang et al., 2023c) and HINT (Fu et al., 2022) are both methods that aggregate clinical trials of the same topic into a sequence based on timestamps instead of making predictions for each trial individually.SPOT uses a sequenced meta-learning approach that begins with topic discovery and clustering, followed by an RNN structure for sequential predictive modeling.It then uses a meta-learning approach to optimize models for different topics.On the other hand, MediTab (Wang et al., 2023b) can be adapted to perform clinical trial prediction and achieves better performance than both HINT and SPOT, with the added advantage of using larger-scale tabular data from different sources.While DeepEnroll (Zhang et al., 2020a) and COMPOSE (Gao et al., 2020a) were initially designed for patient-trial matching, they can also be adapted for clinical trial prediction.However, they perform less effectively than previous LLM methods.</p>
<p>Documents Writing.The generation of various clinical documents, such as discharge summaries, clinical notes, and radiology reports, has traditionally been a time-consuming and laborious task performed by clinical practitioners.However, language models (LLMs) are increasingly being employed to automate this process with their powerful text-generation ability.</p>
<p>For instance, in discharge summaries generation, Patel et al. (Patel &amp; Lam, 2023) present a prompt to generate discharge using chatGPT automatically, while Shing et al. (Shing et al., 2021) have implemented an extractiveabstractive summarization pipeline.This method consists of a two-stage process that begins by extracting relevant sentences from clinical notes, followed by an abstractive summarization technique to transform these extracts into coherent summaries.</p>
<p>LLMs have also been shown to be highly effective in maintaining regular check-ins with patients, as demonstrated by (Webster, 2023).These models can assist in writing notes in patients' records and summarizing their issues, bridging the gap between conventional encounters with caregivers.In addition, Seppo et al. (Enarvi et al., 2020) generated medical reports from patient-doctor conversation transcripts using RNN and transformer models.To achieve this, they trained train RNN and transformer models using a dataset of around 800,000 orthopedic encounters.When both RNN and Transformer models were compared, the latter showed superior accuracy and training efficiency performance.</p>
<p>For generating reports from randomized controlled trials (RCTs), the RobotReviewer (Marshall et al., 2017) system has shown its capability to automatically generate reports summarizing critical information from RCTs.</p>
<p>Finally, multi-modal LLMs have been widely used for the task of radiology report generation.In the early days, specific language models were employed as encoders.An example is TieNet (Wang et al., 2018), an end-to-end CNN-RNN architecture with multi-level attention models.It involves merging image features and text embeddings from associated reports to improve disease classification accuracy and report quality (Wang et al., 2018).Taking a step further, Liu et al. (Liu et al., 2019a) presented a hierarchical generation strategy via CNN-RNN-RNN architecture with reinforcement learning.The approach focuses on balancing readability and clinical accuracy by considering Clinically Coherent Reward (CCR) to maintain the clinical relevance of the reports.For methods using more modern LLMs, MedViLL (Moon et al., 2022), which also uses the BERT (Devlin et al., 2019) architecture, concatenates visual and language feature embeddings and trains them on tasks like masked language modeling and image report matching to achieve an effective alignment of visual and language features.Another example, RadBERT (Yan et al., 2022), is a BERT-like system pre-trained on millions of radiology reports that can generate concise reports highlighting essential observations and conclusions.More recently, during human evaluation, Med-PaLM M (Tu et al., 2024), a multimodal medical language model, generated chest X-ray reports that clinicians preferred over those by radiologists in up to 40.50% of cases.</p>
<p>PATIENT RESULTS</p>
<p>LLMs can help in predicting patient outcomes using patient visits data.There are two categories of downstream tasks in this domain: hospital-related and disease-related predictions.While the former type includes hospital readmission, length of stay, and mortality, the latter type focuses on disease onset, diagnosis, and morbidity.</p>
<p>Patient Outcome Prediction.Patient outcome prediction aims to predict a patient's current or future health status.Language models help in this area by encoding vast amounts of electronic health records (EHRs) to predict future health outcomes, thereby enabling clinicians to make more informed decisions and potentially saving a substantial amount of time and resources.These LLMs-based methods generally achieved better performance than traditional machine learning methods such as logistic regression, MLP, and XGBoost (Chen &amp; Guestrin, 2016).</p>
<p>There are LLMs focusing on hospital-related predictions.For example, ClinicalBERT (Huang et al., 2020) leverages the BERT (Devlin et al., 2019) architecture to understand clinical records and make readmission predictions.Similarly, NYUTron (Jiang et al., 2023), a BERT-like language model, is pre-trained on a comprehensive collection of clinical notes and fine-tuned for various tasks such as predicting mortality, comorbidity, hospital readmission, insurance denial, and length of stay.Another model, RAIM (Xu et al., 2018), processes multimodal EHR data-including clinical records, electrocardiograph waveforms, and vital signs-to predict outcomes like decompensation and length of stay.It considers both historical and current data points, focusing on the most relevant information for accurate predictions.StageNet (Gao et al., 2020b), which can predict decompensation and mortality, uses a stage-aware LSTM module that captures changes in patients' health conditions over time.It integrates time information between visits and employs a stage-adaptive convolutional module to recalibrate understanding of disease progression, highlighting the most informative patterns for precise outcome prediction.</p>
<p>The second category of methods focuses on disease-related predictions, including disease onset, diagnosis, and morbidity.For, instance, RETAIN (Choi et al., 2016) employs a two-level neural attention mechanism with recurrent neural networks (RNNs) to focus on crucial visits and variables for heart failure predictions.Building upon RETAIN, Dipole (Ma et al., 2017) uses a bidirectional RNN and three types of attention mechanisms to enhance diagnosis predictions, determining future medical codes for patient visits.With more recent LLMs, MediTab (Wang et al., 2023b) consolidate and align different types of medical data, predicting patient outcomes such as morbidity.</p>
<p>ASSISTANCE</p>
<p>General-purpose language models (LLMs) can play a significant role in clinical trial assistance by helping patients understand trial-related information, assisting clinicians in retrieving and understanding relevant literature and patient data, and supporting pharmacovigilance efforts by identifying and reporting adverse events.General-purpose LLMs, such as GPT-4, and Med-Palm2 are capable of understanding medical knowledge and explaining it in simple, accessible language (Kung et al., 2023;Thirunavukarasu et al., 2023).This ability can help patients better comprehend and participate in clinical trial opportunities.Clinicians can also utilize LLMs to efficiently retrieve relevant clinical trial literature and assess patient eligibility using advanced information retrieval capabilities.In pharmacovigilance, LLMs contribute to understanding drug-drug interactions (Luo et al., 2022;Taylor et al., 2022), providing deeper insights into drug safety.Their code generation capabilities streamline data analysis (OpenAI, 2023), enhancing the efficiency and speed of data interpretation.</p>
<p>LLMs Maturity Assessment</p>
<p>In this analysis, we evaluate the progress of two LLMs paradigms across 14 downstream task categories within three stages of the drug discovery and development pipeline: understanding disease mechanisms, drug discovery, and clinical trials.We categorize the maturity of these tasks using a four-tiered system ranging from "Not Applicable" to "Matured".We begin by outlining the specifics of the criteria before detailing the maturity levels of the various tasks within each phase.</p>
<p>Maturity Criteria</p>
<p>The maturity of LLMs in the drug discovery and development pipeline is categorized into four distinct levels (Figure 6):</p>
<p>1.Not Applicable: The LLM paradigm is not suitable or relevant for the given downstream task.</p>
<ol>
<li>Nascent: The LLM paradigm has been applied to the task in a preliminary, in silico setting only and lacks validation through real-world experiments.</li>
</ol>
<p>Advanced:</p>
<p>The LLM paradigm has moved beyond theoretical application, with its effectiveness validated through real-world experiments in relevant scenarios.</p>
<p>Matured:</p>
<p>The LLM paradigm application has been integrated and deployed in practical, real-world envi-ronments such as hospitals or pharmaceutical companies, with evidence demonstrating its effectiveness and utility.</p>
<p>Maturity Assessment of Downstream Tasks</p>
<p>For the maturity assessment, we consider 14 different downstream tasks to be carried out in three significant drug discovery and development stages.In understanding disease mechanisms, we focus on genomics analysis, transcriptomic analysis, protein target analysis, disease pathway analysis, and assistance.At the stage of drug discovery, we analyze chemistry experiments, in silico simulations, ADMET prediction, lead optimization, and assistance.Finally, clinical trials deal with clinical applications in clinical practices, patient results, and assistance.</p>
<p>UNDERSTANDING DISEASE MECHANISM</p>
<p>Genomics Analysis.Specialized LLMs have been recently created to encode information in the nucleotide sequences (Ji et al., 2021;Dalla-Torre et al., 2023;Li et al., 2023b) with any practical applications, such as genetic variant analysis (Le et al., 2022;Zhou et al., 2023).However, they are still nascent, and further experiments are required to validate their effectiveness.</p>
<p>Recently, general LLMs have emerged and are also in the nascent stage.There is still room for improvement in explaining the evolutionary processes of genomic data or designing DNA sequences, as indicated in recent studies (AI4Science &amp; Quantum, 2023).This underscores the ever-developing field with its enormous potential for future breakthroughs.</p>
<p>Transcriptomics Analysis.There have been significant advancements in the real-world applications of specialized LLMs, such as the Geneformer LLM (Theodoris et al., 2023).This particular LLM has played an essential role in gene network analysis.It was able to distinguish between normal and cardiomyopathic cardiomyocytes.This process identified the genes responsible for network perturbations associated with hypertrophic and dilated cardiomyopathy, providing potential therapeutic targets like ADCY5 and SRPK3.The real-world effectiveness of these targets was confirmed through experimental validation using iPSC-derived cardiac microtissues with Titin truncating mutations (Theodoris et al., 2023).Hence, it is evident that specialized LLMs are in an advanced stage of development for analyzing transcriptomic data and deciphering disease mechanisms.</p>
<p>In contrast, general LLMs are still in the early nascent stage of development for transcriptomic analysis.Research is underway to explore auxiliary tasks in this field, such as automating cell type analysis (Hou &amp; Ji, 2023) and analyzing data through code generation (AI4Science &amp; Quantum,</p>
<p>2023).</p>
<p>Protein Target Analysis.Specialized LLMs for protein target analysis have significantly matured following the breakthrough of AlphaFold2 (Jumper et al., 2021).Al-phaFold2 is now a comprehensive and readily accessible database (Varadi et al., 2022), with various applications in structure-based drug discovery and vaccine development (Varadi &amp; Velankar, 2023).One noteworthy accomplishment is the rapid development of a first-in-class hit molecule for a novel target, CDK20, without an experimental structure, achieved within 30 days from target selection and requiring the synthesis of only seven compounds (Ren et al., 2023).Additionally, ESM (Rives et al., 2021), a prominent protein language model, has been developed into a web server focusing on GPCR proteins.This tool analyzes their signaling and functional repertoire (Matic et al., 2022) and identifies compounds with subnanomolar affinity (Singhal et al., 2023).</p>
<p>While general LLMs like GPT-4 (OpenAI, 2023) have demonstrated potential in analyzing complex scientific data, including protein analysis, they remain in a nascent stage of development within protein target analysis.Protein-Chat (Guo et al., 2023) is another example demonstrating how these models can label protein structures based on user prompts.However, these developments primarily focus on generating informative answers based on embeddings without extensive real-world validation.Thus, this indicates that the field is still evolving.</p>
<p>Disease-pathway analysis.We are witnessing specialized LLMs reach an advanced stage in disease pathway analysis.These specialized LLMs have made significant contributions, particularly in genomics, transcriptomics, and protein target analysis.Notably, these fields play a critical role in the comprehensive analysis of disease pathways.A notable recent breakthrough is the use of transcriptomic LLM, Geneformer (Theodoris et al., 2023), for gene network analysis, which has undergone laboratory validations and illustrates the capabilities of these models in dissecting disease pathways (Theodoris et al., 2023).</p>
<p>General LLMs have also reached an advanced stage in disease-pathway analysis.For instance, Insilico Medicine, a biotechnology company, already offers ChatGPT integration with its PandaOmics target discovery platform to analyze disease pathways (Savage, 2023).While the PandaOmics target discovery already incorporated general LLMs for this purpose, these tools' widespread adoption and broad usage still need to be fully realized.</p>
<p>Assistance.General LLMs have attained a mature development stage, greatly aiding researchers in information retrieval and knowledge discovery for the understanding of disease mechanisms (AI4Science &amp; Quantum, 2023;Sav-age, 2023;Toufiq et al., 2023).These models possess exceptional capabilities in mining and synthesizing extensive scientific and medical literature, allowing us to understand these mechanisms better.Additionally, their skill in creating and interpreting knowledge graphs (Savage, 2023) is crucial in mapping gene networks and elucidating gene-disease relationships (Luo et al., 2022).Additionally, general LLMs have proven effective in simplifying complex medical and genetic concepts (Jeblick et al., 2023), thus making technical knowledge more accessible and enhancing education and communication in the medical field.</p>
<p>DRUG DISCOVERY</p>
<p>Chemistry Experiments.For chemistry experiments, while the utilization of specialized LLMs is still in its nascent stages, general LLMs have advanced considerably.They are now used in sophisticated chemistry experiments (Figure 6).Specialized LLMs, typically conducted in silico, are considered inferior to their general counterparts based on their performance in retrosynthetic planning and reaction prediction (Boiko et al., 2023;Bran et al., 2023).This is mainly due to the tool use capabilities of general LLMs, which include using tools, reading scientific literature, and searching online to assist in molecular synthesis.</p>
<p>In real-world laboratory settings, general LLMs have demonstrated their effectiveness in synthesizing molecules and controlling robotic arms (Boiko et al., 2023;Bran et al., 2023;Yoshikawa et al., 2023).These successes underscore the potential for general LLMs to impact the field of chemistry significantly, presenting new ways of conducting experiments and facilitating discoveries.However, despite these promising developments, there needs to be more evidence of general LLMs being deployed in the industry, e.g., pharmaceutical companies.This gap highlights the need for further research and development to extend the use of general LLMs in chemistry experiments.</p>
<p>In-silico Simulation.The use of specialized LLMs in industry is becoming increasingly common, with tools like AlphaFold Multimer (Evans et al., 2021) being used for protein-protein complex prediction.More recently, these tools have expanded to include protein-ligand complexes, broadening their scope to small molecules and nucleic acids.Additionally, In Silico Medicine, a biotechnology company has developed GENTRL (Zhavoronkov et al., 2019) and Chemistry42 (Ivanenkov et al., 2023), which utilized specialized LLMs to identify potent DDR1 kinase inhibitors and generate novel molecular structures, respectively.These structures exhibit optimized properties and have been validated through extensive in vitro and in vivo studies.Similarly, Molformer (Ross et al., 2022), developed by IBM, demonstrates promising results in generating candidate molecules aimed at inhibiting the SARS-CoV-2 virus and developing antimicrobial peptides (Das et al., 2021).</p>
<p>Conversely, applying general language models remains primarily confined to in-silico environments.These models face substantial challenges in scientific understanding and quantitative analysis.For instance, GPT-4 (AI4Science &amp; Quantum, 2023) struggles with interpreting and generating SMILES strings.Additionally, general LLMs often lack the precision required for quantitative tasks, leading to suboptimal performance in simulations, such as predicting binding affinity (Razeghi et al., 2022).</p>
<p>ADMET Prediction.For specialized LLMs, IBM's Molformer (Ross et al., 2022) has built a cloud-based platform that allows chemists to conduct real-time molecular screening and efficient molecular properties prediction.</p>
<p>In the advanced stage, LLM4SD (Zheng et al., 2023) uses general-purpose LLMs such as Falcon (Penedo et al., 2023) and Galacica (Taylor et al., 2022) as backbones to extract meaningful hypotheses from ADMET data.These assumptions have been shown to outperform state-of-the-art professional baselines when applied to traditional methods such as random forests.Pharmacologists validate the majority of these rules, ensuring their relevance and efficacy.</p>
<p>Lead Optimization.Specialized LLMs, for lead optimization have been validated via real-world experiments.For instance, in molecular optimization, Moret et al. (Moret et al., 2023) developed a chemical language model that facilitated the discovery of a new PI3KÎ³ ligand with sub-micromolar activity.Similarly, in protein optimization, Hie et al. (Hie et al., 2023) employed a language-model-guided process to enhance seven antibodies' affinity and effectiveness against Ebola and SARS-CoV-2 through minimal variant screening and lab evolution.</p>
<p>However, general LLMs are still in the early stages of development and have only undergone in-silico testing.One of the main challenges in adapting general LLMs to this application lies in the profound understanding of scientific language, which is essential for lead optimization.</p>
<p>Assistance.General LLMs have reached an advanced stage in information retrieval and explanation for drug discovery.BenevolentAI, a company specializing in AI-enabled drug discovery and development, is investigating ChatGPT retrieval plug-ins that can search through personal or company documents to provide medical answers (Savage, 2023).Furthermore, GPT-4 showcases substantial coding capabilities; it aids in various coding tasks related to drug discovery, such as data downloading and data preprocessing (AI4Science &amp; Quantum, 2023).</p>
<p>CLINICAL TRIAL</p>
<p>The clinical trial phase mainly involves general text data, including electronic health records and trial protocol documents.Therefore, a specialized LLM is generally not suitable at this stage.</p>
<p>Clinical Trial Practice.In clinical trial practice, there are tasks including ICD coding, patient-trial matching, and clinical trial planning.Although general LLMs in this field are still in the early stages of implementation, their potential is promising.Despite the lack of extensive real-world testing and application evidence, the rapid development and improvement of the models, particularly in their ability to understand and process medical knowledge (Singhal et al., 2023), signals a promising future.</p>
<p>Patient Outcome Prediction.Predicting patient outcomes is one area where the LLMs show promise, helping doctors diagnose and predict patient outcomes.General LLMs specialize in handling large amounts of unstructured data in electronic medical records.For example, Jiang et al. (Jiang et al., 2023) developed NYUTriton, an advanced platform that interfaces with the electronic health record system at NYU Langone Health System.Deployed across a network of hospitals and outpatient facilities in New York, the system performs tasks such as predicting in-hospital mortality, estimating a comprehensive comorbidity index, and predicting 30-day all-cause readmissions.Similarly, Google's MedPalm2 (Singhal et al., 2023) was introduced for medical question-answering tasks and achieved an accuracy of 86.5%, much higher than the approximate medical passing score.This advanced technology is being tested in realworld settings with select client groups, including VHC Health VA, affiliated with Mayo Clinic.</p>
<p>Assistance.General LLMs have matured in clinical assistance.These assistants can aid physicians and administrative staff in tasks such as document writing.For example, Webster &amp; Paul (Webster, 2023) have demonstrated the effectiveness of these models in generating clinical notes, maintaining regular check-ins for patients with chronic conditions, and summarizing patient issues.Recently, Oracle unveiled a Clinical Digital Assistant that can handle administrative tasks through voice commands.Additionally, Google's MedPalm2 has been implemented in real-world scenarios for information retrieval and knowledge explanation (Singhal et al., 2023).</p>
<p>Future Direction</p>
<p>This section explores the future direction of LLM applications in drug discovery and development.We discuss nine areas that require enhancement: integrating biological insights, addressing ethical and privacy concerns and preventing misuse, addressing fairness and mitigating bias, addressing hallucination, improving multi-modality, improving context window, improving spatio-temporal understanding, and integrating specialized LLMs with general LLMs.</p>
<p>Integrating Biological Insights</p>
<p>Improving the scientific understanding of language models (LLMs) is crucial for their successful application in drug discovery and related downstream tasks.(quantum mechanics/molecular mechanics) approach is a prime example of multiscale modeling.In QM/MM simulations, the region of interest (e.g., a drug interacting with its binding site) is treated using quantum mechanics to model the electronic interactions accurately.In contrast, the surrounding environment (e.g., the rest of the protein and solvent) is treated using classical molecular mechanics, balancing accuracy and computational efficiency.QM/MM simulations can be particularly valuable for studying enzymecatalyzed reactions, which are often targets in drug design.</p>
<p>The integration of advanced computational techniques developed in fields like statistical mechanics or molecular dynamics into large language models (LLMs) for drug discovery has been gradual due to several factors.Despite the significant progress in both domains over the last decade, the following reasons explain why some advanced techniques have not yet been widely adopted.This is potentially due to interdisciplinary gaps, validation and standardization protocols, and data compatibility.As interdisciplinary collaboration grows and computational resources become more accessible, we will likely see more advanced techniques integrated into LLMs, enhancing their effectiveness and impact in drug discovery.</p>
<p>Addresing Ethical, Privacy Concerns, &amp; Preventing Misuse</p>
<p>The ethical issues in using LLMs for drug discovery are diverse and involve responsibility, fairness, and the potential for unintended consequences.One primary ethical issue revolves around accountability for decisions made or influenced by LLMs.As these models play an increasingly important role in drug development, the question arises of who is responsible for the outcomes, whether positive breakthroughs or negative results.This is particularly challenging given the often opaque nature of the LLM decision-making process.The rapid pace of innovation in this field has raised concerns that regulations and ethical guidelines must be updated.</p>
<p>Privacy issues related to LLMs are essential because they can memorize training data.For example, when it comes to sensitive multi-omic data collected during patient typing, it is critical to ensure the data is anonymised and therefore cannot be directly related to the patient.Recently, a study showed that adversaries could extract large amounts of training data from LLM (Nasr et al., 2023) Potential misuse of LLMs in areas such as drug discovery necessitates a carefully balanced approach that prioritizes safeguarding against risks without impeding technological advancement.These concerns are raised as LLMs can be used intentionally for malicious purposes as described in MegaSyn2 model (Urbina et al., 2022).They show the MegaSyn2 model (Urbina et al., 2022), initially developed to discover therapeutic inhibitors and exploit them to create highly toxic substances or chemical warfare agents.However, it is crucial to acknowledge that while LLMs democratize the knowledge of compound synthesis, this does not automatically lead to more accessible access to materials for synthesizing dangerous substances due to existing strict regulations.Given the LLMs' potential benefits in areas like medical science, they need more relaxed regulations to ensure their development.This also requires a balanced view considering system safety while avoiding slowing down technology development.</p>
<p>Addresing Hallucination</p>
<p>The use of language large models (LLMs) in drug discovery is growing.However, their tendency to " hallucinate"-generating irrelevant or incoherent responses-presents a major challenge.Researchers and clinicians must be cautious, as these errors can lead them in the wrong direction with false information.These errors can be propagated, which leads to serious consequences.</p>
<p>For example, hallucinations can potentially result in the identification of incorrect biological targets or relationships, driving research in unproductive directions and wasting valuable resources.Some biotech companies are now using LLMs to interact with knowledge graphs of biological entities, such as genes, proteins, and diseases, to identify potential targets for drug development (Savage, 2023).This issue can lead to inaccurate optimization or modifications in molecule and protein design.For instance, hallucinated molecular structures could be chemically invalid or impractical for synthesis.In clinical settings, where the stakes of inaccuracy include diagnosis and data interpretation, it can lead to serious, even life-threatening consequences.</p>
<p>To address hallucinations in LLMs for drug discovery, mitigation strategies can be used to guide the model toward generating more accurate and relevant answers.Knowledge editing is an approach that fills gaps in the understanding of the model by modifying some parameters or integrating plugins from other sources (Ji et al., 2023).This entails grounding LLMs in retrieval-augmented generation (RAG) with external documents for increased factuality and relevance in their outputs (Ji et al., 2023).Similarly, fine-tuning LLMs on debiased datasets helps remove knowledge shortcuts and spurious correlations that might stem from biased sampling (Ji et al., 2023).Additionally, many techniques can be applied to improve knowledge recall.Chain of Thoughts Prompting and similar techniques help generate outputs based on factuality and relevance (Ji et al., 2023).Finally, the effort of perfecting the decoding algorithms, such as Factuality Enhanced Decoding and Faithfulness Enhanced Decoding, ensures that the output generated is entirely in line with actual data and customer requests, significantly enhancing both the accuracy and reliability of LLMs.</p>
<p>Addresing Fairness &amp; Bias</p>
<p>Fairness and bias should be among the top priorities when creating and using LLMs in drug development.Biases' effects are evident in medical contexts, which can lead to possible inaccuracies, discrimination between different groups of patients, or even potentially harmful consequences.Biases inherent within data collection, model training, and application channels may prolong disparities, thus negatively impacting the integrity and efficacy of medical treatments.</p>
<p>Bias can originate from various sources, including the need for more data on rare diseases or specific populations, leading to underrepresentation.For example, biases are particularly evident in clinical trials through disparities in participation rates among different populations, influenced by geographic, economic, and cultural factors.Skewing predictive models developed based on these datasets can result in less effective or inappropriate solutions for underrepresented groups.Furthermore, this problem is accentuated when the models turn more toward common or widely examined points, like particular protein targets and demographic groups.</p>
<p>Enhancing the transparency and interpretability of LLMs is essential to address their biases.This can be achieved by employing various data sources, utilizing inclusive data collection and analysis approaches, and conducting rigorous ethical assessments.Furthermore, creating mechanisms that allow for error rectification, promoting interdisciplinary cooperation, and initiating conversations concerning the responsible implementation of LLMs in drug development would help ensure equity in healthcare access.</p>
<p>Improving Quantiative Analysis</p>
<p>The role of LLMs in the field of drug discovery has been expanding.One significant skill they need is to analyze vast amounts of numerical datasets.For instance, this applies to transcriptomic expression data interpretation for learning about disease mechanisms or predicting molecular properties while making drugs (Theodoris et al., 2023;Zheng et al., 2023).These examples indicate a growing trend towards relying on LLMs to manage particular problems within dataintensive pharmaceutical research.</p>
<p>LLMs, in general, while proficient in text generation and analysis, have shown limited success with data predominantly comprising numerical values, a critical aspect in drug research.For example, LLMs have historically faced challenges executing straightforward arithmetic operations like multi-digit multiplication (Dziri et al., 2023).They often resort to fabricating answers (OpenAI, 2023;Frieder et al., 2023).As some have argued (Testolin, 2023;Golkar et al., 2023), this can be due to the standard tokenization methods in LLMs that fail to accurately reflect the unique quantitative characteristics of numerical data, separating it from typical language inputs.</p>
<p>Recent explorations propose various methodologies to improve the encoding of numerical information for these LLMs (Thawani et al., 2021;Golkar et al., 2023).Approaches include digit-by-digit encoding (Gruver et al., 2023), base-10 formatting, and alignment between embedding distance against real numerical distance (Sundararaman et al., 2020).However, as LLMs are prone to relying on shortcuts and non-representative correlations in data (Dziri et al., 2023), they still struggle with interpolation and extrapolation in mathematical contexts within scientific fields (Grosse et al., 2023;Anil et al., 2022).Addressing this fundamental issue requires imposing an appropriate inductive bias that acknowledges the continuous nature of numbers, a critical step for advancing LLMs in drug discovery.</p>
<p>Improving Multi-Modality</p>
<p>Multimodal Large Language Models (MLLMs) are advanced LLMs equipped to receive and process multimodal information (Yin et al., 2023).The application of MLLMs to drug discovery is promising as they can process diverse types of data, including videos, images, and experimental data.This property aligns with the nature of drug discovery, requiring diversified data sources like chemical structures, biological datasets, and scientific literature (Taylor et al., 2022).</p>
<p>MLLMs are beneficial as they enable scientists to interact intuitively and flexibly with them.They can be accommodating when it comes to complex tasks in chemistry or biology, such as molecular modeling or clinical data analysis.These tasks involve multiple types of data, e.g., 3D protein and molecule structure and 2D medical images.Implementing MLLMs can open up an exciting frontier for research and may significantly optimize the effectiveness of laboratory investigations.</p>
<p>Improving Context Window</p>
<p>In drug discovery, LLMs usually must deal with vast amounts of biological data like sequences, which can easily exceed 2048 tokens (Koh et al., 2022a;b).However, many existing models have a restricted window of 2048 or 4096 tokens, e.g., Galactica (Taylor et al., 2022), and Falcon.Thus, these models are ineffective in processing multiple proteins and gene sequences.Protein sequences typically have 200-300 amino acids, while gene sequences can have over 26,000 nucleotides.Even LLMs with large window sizes of up to 128k tokens fail thoroughly analyze such enormous volumes of input data.Typically, these models are good at interpreting the input's beginning and end but usually perform poorly for middle sections.This "forgetting" issue can lead to significant gaps in the analysis and interpretation of data.</p>
<p>Several potential solutions exist to address this challenge.One approach is to segment the input into smaller chunks and process them separately, then combine the outputs to generate a final result.To ensure no critical information is lost, intelligent segmentation strategies should be developed to understand the biological significance of different parts of the sequence.Another solution is implementing more sophisticated memory and attention mechanisms that help models better manage and utilize longer context windows.However, this research direction requires intensive computational resources.</p>
<p>Improving Spatio-temporal Understanding</p>
<p>An essential prerequisite for developing rational drug design and discovery is the improvement of spatial-temporal capabilities in LLMs, given that these techniques rely heavily on processing large datasets comprising complex, multidimensional data.At present, LLMs can process and interpret textual information reasonably well but have their weaknesses exposed when it comes to spatio-temporal data (Pan et al., 2023;Jin et al., 2023a;b), which plays an essential role in the drug discovery field.For example, this limitation leaves aside areas where a physical change concerning time and space understanding is crucial, e.g., dynamic interactions between molecules.Improving LLMs in this aspect would provide new opportunities to understand more deeply in fields like spatial-temporal transcriptomics and molecular dynamics simulations (Nguyen et al., 2024a).Furthermore, LLMs with enhanced spatial-temporal and multi-modal understanding enable a highly autonomous and efficient process.An illustration is in the analysis of molecular dynamics simulations.These models can automatically investigate, document, and even describe data that elaborate on molecule dynamics.This advancement and multi-modal capabilities are pivotal in unearthing potential drug candidates and uncovering molecular pathways commonly hidden beneath vast data.This exciting development has the potential to revolutionize drug discovery and significantly reduce time and resource expenditure.</p>
<p>Integrating Specialised LLMs &amp; General LLMs</p>
<p>Combining a specialized language model and a generalpurpose LLM gives an edge in drug discovery.Specialized LLMs perform admirably in precision tasks like understanding biological information, estimating molecule interactions, or examining protein configurations using their niche training datasets.On the other hand, general LLMs provide versatility and a broad knowledge base that can be applied to various subjects and tasks.They are considered essential instruments for many users since they are user-friendly to individuals with different levels of professionalism, such as researchers and medical workers, enabling them to access scientific knowledge and reasoning easily.Specifically, general LLMs can act as the front-end system responsible for user interaction with conversational interfaces that provide detailed descriptions of case characteristics, assist in problem identification, and facilitate discussion of decision alternatives.They might deliver context, background knowledge, and reasoning ability to aid in understanding the problem situation.On the other hand, specialized LLMs with quantitative analytical ability can further be utilized to accomplish specific downstream tasks.For instance, they can conduct QSAR analysis for molecule compounds, protein folding simulation, or molecular structure optimization.Furthermore, these LLMs can then provide the results back to general-purpose LLMs to synthesize and interpret these results and provide insightful information for users.</p>
<p>Figure 1 .
1
Figure1.Large Language Models Shaping the Future Landscape of Drug Discovery and Development.In the past, each stage of drug discovery involved numerous manual tasks, which requires significant human effort and substantial resources.Nowadays, advancements in biotechnology, alongside the integration of AI and computer-aided in silico computation tools, have reduced the need of human labor and resources.However, we have yet to have a highly automated drug discovery pipeline, especially in the clinical trial phase, where trail design and matching are still mainly done by clinical practitioners.In the future, it is anticipated that the continued development of LLMs and their application in drug discovery will enable a highly automated drug discovery process.</p>
<p>Figure 2 .
2
Figure2.The two main paradigms of language models.Specialized language models are trained on specific scientific languages and are typically tailored for specific or a few science-related tasks.These models are used as tools to perform a specific task, in which users provide the information required for a task, and the model outputs the prediction.General-purpose language models are trained on diverse textual information sourced from various materials, including scientific papers and textbooks.These models are used like an assistant that allows users to use plain language to interact with the model.</p>
<p>Figure 3 .
3
Figure 3. Understanding Disease Mechanisms.The left part of the figure illustrates the processes involved in understanding disease mechanisms.This process involves clinical sub-typing, target-disease linkage analysis, and target validation.Clinical subtyping refers to identifying subgroups of patients with similar clinical characteristics during which data can be collected from multi-omics.Target-disease linkage analysis refers to identifying the relationship between targets and diseases.Target validation typically involves three steps: safety and feasibility, mechanisms of action, and modality selection.The right part of the figure highlights the tasks that LLMs can perform to facilitate these processes, including genomics analysis, RNA analysis, pathway analysis, target profiling, strategic profiling, and assistance.</p>
<p>Figure 5 .
5
Figure 5. Clinical Trials.The left part of the figure illustrates the processes involved in clinical trials.Clinical trials consist of four phases: Phase 1, Phase 2, Phase 3, and Phase 4. The right part of the figure highlights the tasks that LLMs can perform to facilitate these processes.</p>
<p>Transcriptomics Analysis Protein-target Analysis Disease-pathway Analysis Assistance Understanding Diseases Mechanism Drug Discovery Chemistry Experiment In-silico Simulation AMDET Prediction Lead Optimization Clinical Trial Patient Outcome Prediction Clinical Trial Practice Maturity Level Legends Not Applicable Nascent Advanced Matured Assistance Assistance Figure</p>
<ol>
<li>Maturity Assessment of LLMs in Downstream Tasks.This figure is segmented into Maturity Level Legends, Understanding Disease Mechanism, Drug Discovery, and Clinical Trials, detailing LLMs maturity across various tasks and phases.</li>
</ol>
<p>Large Language Models (LLMs) are also known as large pretrained language models.</p>
<p>. Fasta, 1995</p>
<p>Openfold: Retraining alphafold2 yields new insights into its learning mechanisms and capacity for generalization. G Ahdritz, N Bouatta, C Floristean, S Kadyan, Q Xia, W Gerecke, T J O'donnell, D Berenberg, I Fisk, N Zanichelli, arXiv:2311.073612024. AI4Science, M. R. and Quantum, M. A. The impact of large language models on scientific discovery: a preliminary study using gpt-4. 2023arXiv preprint</p>
<p>E Alsentzer, J R Murphy, W Boag, W.-H Weng, D Jin, T Naumann, M Mcdermott, arXiv:1904.03323Publicly available clinical bert embeddings. 2019arXiv preprint</p>
<p>Correlation of co-ordinated amino acid substitutions with function in viruses related to tobacco mosaic virus. D Altschuh, A Lesk, A Bloomer, A Klug, Journal of molecular biology. 19341987</p>
<p>Coordinated amino acid changes in homologous protein families. D Altschuh, T Vernet, P Berti, D Moras, K Nagai, Design and Selection. 231988Protein Engineering</p>
<p>Principles that govern the folding of protein chains. C B Anfinsen, Science. 18140961973</p>
<p>Exploring length generalization in large language models. C Anil, Y Wu, A Andreassen, A Lewkowycz, V Misra, V Ramasesh, A Slone, G Gur-Ari, E Dyer, B Neyshabur, Advances in Neural Information Processing Systems. 202235</p>
<p>The interplay of epigenetic marks during stem cell differentiation and development. Y Atlasi, H G Stunnenberg, Nature Reviews Genetics. 18112017</p>
<p>Accurate prediction of protein structures and interactions using a three-track neural network. M Baek, F Dimaio, I Anishchenko, J Dauparas, S Ovchinnikov, G R Lee, J Wang, Q Cong, L N Kinch, R D Schaeffer, Science. 37365572021</p>
<p>Liggpt: Molecular generation using a transformerdecoder model. V Bagal, R Aggarwal, P Vinod, U D Priyakumar, 2021a</p>
<p>Molgpt: molecular generation using a transformerdecoder model. V Bagal, R Aggarwal, P Vinod, U D Priyakumar, Journal of Chemical Information and Modeling. 6292021b</p>
<p>Strategies and challenges for the next generation of antibodydrug conjugates. A Beck, L Goetsch, C Dumontet, N CorvaÃ¯a, Nature reviews Drug discovery. 1652017</p>
<p>Learning the protein language: Evolution, structure, and function. T Bepler, B Berger, Cell systems. 1262021</p>
<p>An overview of drug discovery and development. N Berdigaliyev, M Aljofan, Future medicinal chemistry. 12102020</p>
<p>scmulan: a multitask generative pre-trained language model for single-cell analysis. H Bian, Y Chen, X Dong, C Li, M Hao, S Chen, J Hu, M Sun, L Wei, X Zhang, International Conference on Research in Computational Molecular Biology. Springer2024</p>
<p>Reinvent 2.0: an ai tool for de novo drug design. T Blaschke, J ArÃºs-Pous, H Chen, C Margreitter, C Tyrchan, O Engkvist, K Papadopoulos, A Patronov, Journal of chemical information and modeling. 60122020</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 62479922023</p>
<p>A M Bran, S Cox, A D White, P Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Proteinbert: a universal deep-learning model of protein sequence and function. N Brandes, D Ofer, Y Peleg, N Rappoport, M Linial, Bioinformatics. 3882022</p>
<p>Genome structure described by formal languages. V Brendel, H Busse, Nucleic Acids Research. 1251984</p>
<p>Guacamol: benchmarking models for de novo molecular design. N Brown, M Fiscato, M H Segler, A C Vaucher, Journal of chemical information and modeling. 5932019</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Using metadynamics to explore complex free-energy landscapes. G Bussi, A Laio, 10.1038/s42254-020-0153-0Nature Reviews Physics. 24Mar 2020</p>
<p>Machine learning in qm/mm molecular dynamics simulations of condensed-phase systems. L BÃ¶selt, M ThÃ¼rlemann, S Riniker, 10.1021/acs.jctc.0c01112Journal of Chemical Theory and Computation. 175Apr 2021</p>
<p>Xgboost: A scalable tree boosting system. T Chen, C Guestrin, Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. the 22nd acm sigkdd international conference on knowledge discovery and data mining2016</p>
<p>Retain: An interpretable predictive model for healthcare using reverse time attention mechanism. E Choi, M T Bahadori, J Sun, J Kulas, A Schuetz, W Stewart, Advances in neural information processing systems. 292016</p>
<p>Single-sequence protein structure prediction using a language model and deep learning. R Chowdhury, N Bouatta, S Biswas, C Floristean, A Kharkar, K Roy, C Rochereau, G Ahdritz, J Zhang, G M Church, Nature Biotechnology. 40112022</p>
<p>To transformers and beyond: Large language models for the genome. M E Consens, C Dufault, M Wainberg, D Forster, M Karimzadeh, H Goodarzi, F J Theis, A Moses, B Wang, arXiv:2311.076212023arXiv preprint</p>
<p>Diffusion steps, twists, and turns for molecular docking. G Corso, H StÃ¤rk, B Jing, R Barzilay, T S Jaakkola, Diffdock, The Eleventh International Conference on Learning Representations. 2023</p>
<p>The path to oncology drug target validation: an industry perspective. Target Identification and Validation in Drug Discovery: Methods and Protocols. M CortÃ©s-Cros, T Schmelzle, V M Stucke, F Hofmann, 2013</p>
<p>scgpt: towards building a foundation model for single-cell multi-omics using generative ai. H Cui, C Wang, H Maan, K Pang, F Luo, B Wang, bioRxiv. 2023</p>
<p>The nucleotide transformer: Building and evaluating robust foundation models for human genomics. H Dalla-Torre, L Gonzalez, J Mendoza-Revilla, N L Carranza, A H Grzywaczewski, F Oteri, C Dallago, E Trop, B P De Almeida, H Sirelkhatim, bioRxiv. 2023</p>
<p>Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations. P Das, T Sercu, K Wadhawan, I Padhi, S Gehrmann, F Cipcigan, V Chenthamarakshan, H Strobelt, C Santos, P.-Y Chen, Y Y Yang, J P K Tan, J Hedrick, J Crain, A Mojsilovic, 10.1038/s41551-021-00689-xNature Biomedical Engineering. 562021</p>
<p>Robust deep learning-based protein sequence design using proteinmpnn. J Dauparas, I Anishchenko, N Bennett, H Bai, R J Ragotte, L F Milles, B I Wicky, A Courbet, R J De Haas, N Bethel, Science. 37866152022</p>
<p>Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>Faith and fate: Limits of transformers on compositionality. N Dziri, X Lu, M Sclar, X L Li, L Jian, B Y Lin, P West, C Bhagavatula, R L Bras, J D Hwang, arXiv:2305.186542023arXiv preprint</p>
<p>C Edwards, T Lai, K Ros, G Honke, K Cho, Ji , H , Translation between molecules and natural language. EMNLP. 2022</p>
<p>Prottrans: Toward understanding the language of life through self-supervised learning. A Elnaggar, M Heinzinger, C Dallago, G Rehawi, Y Wang, L Jones, T Gibbs, T Feher, C Angerer, M Steinegger, 202144</p>
<p>Improving target assessment in biomedical research: the got-it recommendations. C H Emmerich, L M Gamboa, M C Hofmann, M Bonin-Andresen, O Arbach, P Schendel, B Gerlach, K Hempel, A Bespalov, U Dirnagl, Nature reviews Drug discovery. 2012021</p>
<p>Generating medical reports from patient-doctor conversations using sequence-to-sequence models. S Enarvi, M Amoia, Del-Agua, M Teba, B Delaney, F Diehl, S Hahn, K Harris, L Mcgrath, Y Pan, J Pinto, L Rubini, M Ruiz, G Singh, F Stemmer, W Sun, P Vozila, T Lin, R Ramamurthy, 10.18653/v1/2020.nlpmc-1.4Proceedings of the First Workshop on Natural Language Processing for Medical Conversations. the First Workshop on Natural Language Processing for Medical ConversationsOnlineAssociation for Computational LinguisticsJuly 2020</p>
<p>Mermaid: an open source automated hit-to-lead method based on deep reinforcement learning. D Erikawa, N Yasuo, M Sekijima, Journal of Cheminformatics. 132021</p>
<p>Protein complex prediction with alphafold-multimer. biorxiv. R Evans, M O'neill, A Pritzel, N Antropova, A Senior, T Green, A Å½Ã­dek, R Bates, S Blackwell, J Yim, 2021</p>
<p>Y Fang, Y Jiang, L Wei, Q Ma, Z Ren, Q Yuan, D.-Q Wei, Deepprosite, Structure-aware protein binding site prediction using esmfold and pretrained language model. 2023718</p>
<p>Controllable protein design with language models. N Ferruz, B HÃ¶cker, Nature Machine Intelligence. 462022</p>
<p>Protgpt2 is a deep unsupervised language model for protein design. N Ferruz, S Schmidt, B HÃ¶cker, Nature communications. 13143482022</p>
<p>Language models can learn complex molecular distributions. D Flam-Shepherd, K Zhu, A Aspuru-Guzik, Nature Communications. 13132932022</p>
<p>Geneticdriven druggable target identification and validation. M Floris, S Olla, D Schlessinger, F Cucca, Trends in Genetics. 3472018</p>
<p>. S Frieder, L Pinchetti, R.-R Griffiths, T Salvatori, T Lukasiewicz, P C Petersen, A Chevalier, Berner, arXiv:2301.13867J. Mathematical capabilities of chatgpt. 2023arXiv preprint</p>
<p>Hint: Hierarchical interaction network for clinical-trialoutcome predictions. T Fu, K Huang, C Xiao, L M Glass, J Sun, Patterns. 342022</p>
<p>The impact of protein dynamics on residue-residue coevolution and contact prediction. A Fung, A Koehl, M Jagota, Y S Song, bioRxiv. 2022</p>
<p>Compose: Crossmodal pseudo-siamese network for patient trial matching. J Gao, C Xiao, L M Glass, J Sun, Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining2020a</p>
<p>Stage-aware neural networks for health risk prediction. J Gao, C Xiao, Y Wang, W Tang, L M Glass, J Sun, Stagenet, Proceedings of The Web Conference. The Web Conference2020. 2020b</p>
<p>Correlated mutations and residue contacts in proteins. U GÃ¶bel, C Sander, R Schneider, A Valencia, Proteins: Structure, Function, and Bioinformatics. 1841994</p>
<p>S Golkar, M Pettee, M Eickenberg, A Bietti, M Cranmer, G Krawezik, F Lanusse, M Mccabe, R Ohana, L Parker, arXiv:2310.02989A continuous number encoding for large language models. 2023arXiv preprint</p>
<p>R Grosse, J Bae, C Anil, N Elhage, A Tamkin, A Tajdini, B Steiner, D Li, E Durmus, E Perez, arXiv:2308.03296Studying large language model generalization with influence functions. 2023arXiv preprint</p>
<p>Large language models are zero-shot time series forecasters. N Gruver, M Finzi, S Qiu, A G Wilson, arXiv:2310.078202023arXiv preprint</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Y Gu, R Tinn, H Cheng, M Lucas, N Usuyama, X Liu, T Naumann, J Gao, H Poon, ACM Transactions on Computing for Healthcare. 312021</p>
<p>Objectivereinforced generative adversarial networks (organ) for sequence generation models. G L Guimaraes, B Sanchez-Lengeling, C Outeiral, P L C Farias, A Aspuru-Guzik, arXiv:1705.108432017arXiv preprint</p>
<p>Proteinchat: Towards achieving chatgpt-like functionalities on protein 3d structures. H Guo, M Huo, R Zhang, P Xie, 2023</p>
<p>Improving patient pre-screening for clinical trials. D M D Hamer, P Schoor, T B Polak, D Kapitan, arXiv:2304.07396Assisting physicians with large language models. 2023arXiv preprint</p>
<p>Large-scale foundation model on single-cell transcriptomics. M Hao, J Gong, X Zeng, C Liu, Y Guo, X Cheng, T Wang, J Ma, X Zhang, L Song, Nature Methods. 2024</p>
<p>Simulating 500 million years of evolution with a language model. T Hayes, R Rao, H Akin, N J Sofroniew, D Oktay, Z Lin, R Verkuil, V Q Tran, J Deaton, M Wiggert, bioRxiv. 2024</p>
<p>Transformer neural network for structure constrained molecular optimization. Theoretical and Computational Chemistry. J He, F Mattsson, M Forsberg, E J Bjerrum, O Engkvist, C Tyrchan, W Czechtizky, 2021a</p>
<p>Molecular optimization by capturing chemist's intuition using deep neural networks. J He, H You, E SandstrÃ¶m, E Nittinger, E J Bjerrum, C Tyrchan, W Czechtizky, O Engkvist, Journal of cheminformatics. 1312021b</p>
<p>Rita: a study on scaling up generative protein sequence models. D Hesslow, N Zanichelli, P Notin, I Poli, D Marks, arXiv:2205.057892022arXiv preprint</p>
<p>Efficient evolution of human antibodies from general protein language models. B L Hie, V R Shanker, D Xu, T U Bruun, P A Weidenbacher, S Tang, W Wu, J E Pak, P S Kim, Nature Biotechnology. 2023</p>
<p>Reference-free and cost-effective automated cell type annotation with gpt-4 in single-cell rna-seq analysis. W Hou, Z Ji, 2023Research Square</p>
<p>Learning inverse folding from millions of predicted structures. C Hsu, R Verkuil, J Liu, Z Lin, B Hie, T Sercu, A Lerer, A Rives, International Conference on Machine Learning. PMLR2022</p>
<p>Exploring evolution-aware &amp;-free protein language models as protein function predictors. M Hu, F Yuan, K Yang, F Ju, J Su, H Wang, F Yang, Q Ding, Advances in Neural Information Processing Systems. 202235</p>
<p>PLM-ICD: Automatic ICD coding with pretrained language models. C.-W Huang, S.-C Tsai, Y.-N Chen, 10.18653/v1/2022.clinicalnlp-1Proceedings of the 4th Clinical Natural Language Processing Workshop. T Naumann, S Bethard, K Roberts, A Rumshisky, the 4th Clinical Natural Language Processing WorkshopSeattle, WAAssociation for Computational LinguisticsJuly 2022</p>
<p>URL. </p>
<p>Clinicalbert: Modeling clinical notes and predicting hospital readmission. K Huang, J Altosaar, R Ranganath, Workshops at the Conference on Health, Inference, and Learning. 2020</p>
<p>Llms can generate robotic scripts from goaloriented instructions in biological laboratory automation. T Inagaki, A Kato, K Takahashi, H Ozaki, G N Kanda, arXiv:2304.102672023arXiv preprint</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. R Irwin, S Dimitriadis, J He, E J Bjerrum, Machine Learning: Science and Technology. 31150222022</p>
<p>Chemistry42: an ai-driven platform for molecular design and optimization. Y A Ivanenkov, D Polykovskiy, D Bezrukov, B Zagribelnyy, V Aladinskiy, P Kamya, A Aliper, F Ren, A Zhavoronkov, Journal of Chemical Information and Modeling. 6332023</p>
<p>Leveraging large language models for predictive chemistry. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, Nature Machine Intelligence. 622024</p>
<p>Chatgpt makes medicine easy to swallow: an exploratory case study on simplified radiology reports. K Jeblick, B Schachtner, J Dexl, A Mittermeier, A T StÃ¼ber, J Topalis, T Weber, P Wesp, B O Sabel, J Ricke, European radiology. 2023</p>
<p>Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome. Y Ji, Z Zhou, H Liu, R V Davuluri, Bioinformatics. 37152021</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Computing Surveys. 55122023</p>
<p>Health system-scale language models are all-purpose prediction engines. L Y Jiang, X C Liu, N P Nejatian, M Nasir-Moin, D Wang, A Abidin, K Eaton, H A Riina, I Laufer, P Punjabi, Nature. 2023</p>
<p>Sequence-based drug-target affinity prediction using weighted graph neural networks. M Jiang, S Wang, S Zhang, W Zhou, Y Zhang, Z Li, BMC Genomics. 232022</p>
<p>M Jin, S Wang, L Ma, Z Chu, J Y Zhang, X Shi, P.-Y Chen, Y Liang, Y.-F Li, S Pan, arXiv:2310.01728Time-llm: Time series forecasting by reprogramming large language models. 2023aarXiv preprint</p>
<p>Large models for time series and spatio-temporal data: A survey and outlook. M Jin, Q Wen, Y Liang, C Zhang, S Xue, X Wang, J Zhang, Y Wang, H Chen, X Li, arXiv:2310.101962023barXiv preprint</p>
<p>Matching patients to clinical trials with large language models. Q Jin, Z Wang, C S Floudas, J Sun, Z Lu, 2023cArXiv</p>
<p>Highly accurate protein structure prediction with alphafold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Å½Ã­dek, A Potapenko, Nature. 59678732021</p>
<p>Diffdock-PP: Rigid protein-protein docking with diffusion models. M A Ketata, C Laue, R Mammadov, H Stark, M Wu, G Corso, C Marquet, R Barzilay, T S Jaakkola, ICLR 2023 -Machine Learning for Drug Discovery workshop. 2023</p>
<p>Ember: multi-label prediction of kinase-substrate phosphorylation events through deep learning. K E Kirchoff, S M Gomez, Bioinformatics. 3882022</p>
<p>An empirical survey on long document summarization: Datasets, models, and metrics. H Y Koh, J Ju, M Liu, S Pan, 2022aACM computing surveys55</p>
<p>How far are we from robust long abstractive summarization?. H Y Koh, J Ju, H Zhang, M Liu, S Pan, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022b</p>
<p>Physicochemical graph neural network for learning protein-ligand interaction fingerprints from sequence data. H Y Koh, A T N Nguyen, S Pan, L T May, G I Webb, 10.1038/s42256-024-00847-1Nature Machine Intelligence. 2522-58392024</p>
<p>Generalized biomolecular modeling and design with rosettafold allatom. R Krishna, J Wang, W Ahern, P Sturmfels, P Venkatesh, I Kalvet, G R Lee, F S Morey-Burrows, I Anishchenko, I R Humphreys, Science. 384669325282024</p>
<p>Performance of chatgpt on usmle: potential for ai-assisted medical education using large language models. T H Kung, M Cheatham, A Medenilla, C Sillos, L De Leon, C ElepaÃ±o, M Madriaga, R Aggabao, G Diaz-Candido, J Maningo, PLoS digital health. 22e00001982023</p>
<p>Bert-promoter: An improved sequence-based predictor of dna promoter using bert pre-trained model and shap feature selection. N Q K Le, Q.-T Ho, V.-N Nguyen, J.-S Chang, Computational Biology and Chemistry. 991077322022</p>
<p>Distributed representations of sentences and documents. Q Le, T Mikolov, International conference on machine learning. PMLR2014</p>
<p>Biobert: a pre-trained biomedical language representation model for biomedical text mining. J Lee, W Yoon, S Kim, D Kim, S Kim, C H So, J Kang, Bioinformatics. 3642020</p>
<p>Conformational preferences of amino acids in globular proteins. M Levitt, Biochemistry. 17201978</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, Bart, doi: 10.18653Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>URL. </p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, 2023aCVPR</p>
<p>Applications of deep learning in understanding gene regulation. Z Li, E Gao, J Zhou, W Han, X Xu, X Gao, Cell Reports Methods. 2023b</p>
<p>Crispr/cas9 mutagenesis invalidates a putative cancer dependency targeted in on-going clinical trials. A Lin, C J Giuliano, N M Sayles, J M Sheltzer, 2017Elife6e24179</p>
<p>Evolutionary-scale prediction of atomic-level protein structure with a language model. Z Lin, H Akin, R Rao, B Hie, Z Zhu, W Lu, N Smetanin, R Verkuil, O Kabeli, Y Shmueli, Science. 37966372023</p>
<p>Target discovery. M A Lindsay, Nature Reviews Drug Discovery. 2102003</p>
<p>Protein embeddings and deep learning predict binding residues for various ligand classes. M Littmann, M Heinzinger, C Dallago, K Weissenow, B Rost, Scientific Reports. 111239162021</p>
<p>Clinically accurate chest x-ray report generation. G Liu, T.-M H Hsu, M Mcdermott, W Boag, W.-H Weng, P Szolovits, M Ghassemi, Machine Learning for Healthcare Conference. PMLR2019a</p>
<p>Git-mol: A multi-modal large language model for molecular science with graph, image, and text. P Liu, Y Ren, Z Ren, arXiv:2308.069112023aarXiv preprint</p>
<p>Multi-modal molecule structure-text model for text-based retrieval and editing. S Liu, W Nie, C Wang, J Lu, Z Qiao, L Liu, J Tang, C Xiao, A Anandkumar, Nature Machine Intelligence. 2023b</p>
<p>Chatgpt-powered conversational drug editing using retrieval and domain feedback. S Liu, J Wang, Y Yang, C Wang, L Liu, H Guo, C Xiao, arXiv:2305.180902023carXiv preprint</p>
<p>S Liu, Y Zhu, J Lu, Z Xu, W Nie, A Gitter, C Xiao, J Tang, H Guo, Anandkumar, arXiv:2302.04611A. A text-guided protein design framework. 2023darXiv preprint</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. 2019barXiv preprint</p>
<p>H Loeffler, J He, A Tibo, J P Janet, A Voronov, L Mervin, O Engkvist, Reinvent4, Modern aidriven generative molecule design. 2023</p>
<p>Shape-seq 2.0: Systematic optimization and extension of high-throughput chemical probing of rna secondary structure with next generation sequencing. D Loughrey, K E Watters, A H Settle, J B Lucks, 10.1093/nar/gku909Nucleic Acids Research. 4221Oct 2014</p>
<p>Trigonometry-aware neural networks for drugprotein binding structure prediction. W Lu, Q Wu, J Zhang, J Rao, C Li, S Zheng, Tankbind, Advances in Neural Information Processing Systems. 202235</p>
<p>Deepphos: prediction of protein phosphorylation sites with deep learning. F Luo, M Wang, Y Liu, X.-M Zhao, A Li, Bioinformatics. 35162019</p>
<p>Biogpt: generative pre-trained transformer for biomedical text generation and mining. R Luo, L Sun, Y Xia, T Qin, S Zhang, H Poon, T.-Y Liu, Briefings in Bioinformatics. 2364092022</p>
<p>Diagnosis prediction in healthcare via attentionbased bidirectional recurrent neural networks. F Ma, R Chitta, J Zhou, Q You, T Sun, J Gao, Dipole, Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. the 23rd ACM SIGKDD international conference on knowledge discovery and data mining2017</p>
<p>Harnessing the deep learning power of foundation models in single-cell omics. Q Ma, Y Jiang, H Cheng, D Xu, Nature Reviews Molecular Cell Biology. 2024</p>
<p>Progen: Language modeling for protein generation. A Madani, B Mccann, N Naik, N S Keskar, N Anand, R R Eguchi, P.-S Huang, R Socher, arXiv:2004.034972020arXiv preprint</p>
<p>Large language models generate functional protein sequences across diverse families. A Madani, B Krause, E R Greene, S Subramanian, B P Mohr, J M Holton, J L OlmosJr, C Xiong, Z Z Sun, R Socher, Nature Biotechnology. 2023</p>
<p>Automating biomedical evidence synthesis: Robotreviewer. I J Marshall, J Kuiper, E Banner, B C Wallace, Proceedings of the conference. Association for Computational Linguistics. Meeting. the conference. Association for Computational Linguistics. MeetingNIH Public Access20172017</p>
<p>Precogx: ex ploring gpcr signaling mechanisms with deep protein representations. M Matic, G Singh, F Carli, N De Oliveira Rosa, P Miglionico, L Magni, J S Gutkind, R B Russell, A Inoue, F Raimondi, Nucleic acids research. 50W12022</p>
<p>Deep learning for flexible and site-specific protein docking and design. M Mcpartlon, J Xu, bioRxiv. 2023</p>
<p>A universal system for digitization and automatic execution of the chemical synthesis literature. S H M Mehr, M Craven, A I Leonov, G Keenan, L Cronin, Science. 37065122020</p>
<p>Language models enable zero-shot prediction of the effects of mutations on protein function. J Meier, R Rao, R Verkuil, J Liu, T Sercu, A Rives, Advances in Neural Information Processing Systems. 202134</p>
<p>Genome-wide association analysis of more than 120,000 individuals identifies 15 new susceptibility loci for breast cancer. K Michailidou, J Beesley, S Lindstrom, S Canisius, J Dennis, M J Lush, M J Maranian, M K Bolla, Q Wang, M Shah, Nature genetics. 4742015</p>
<p>Association analysis identifies 65 new breast cancer risk loci. K Michailidou, S LindstrÃ¶m, J Dennis, J Beesley, S Hui, S Kar, A Lemac Â¸on, P Soucy, D Glubb, A Rostamianfar, Nature. 55176782017</p>
<p>Epidrugs: targeting epigenetic marks in cancer treatment. C L Miranda Furtado, M C Dos Santos Luciano, R D Silva Santos, G P Furtado, M O Moraes, C Pessoa, Epigenetics. 14122019</p>
<p>Colabfold: making protein folding accessible to all. M Mirdita, K SchÃ¼tze, Y Moriwaki, L Heo, S Ovchinnikov, M Steinegger, Nature methods. 1962022</p>
<p>Multi-modal understanding and generation for medical images and text via vision-language pre-training. J H Moon, H Lee, W Shin, Y.-H Kim, E Choi, IEEE Journal of Biomedical and Health Informatics. 26122022</p>
<p>Leveraging molecular structure and bioactivity with chemical language models for de novo drug design. M Moret, I Pachon Angona, L Cotos, S Yan, K Atz, C Brunner, M Baumgartner, F Grisoni, G Schneider, Nature Communications. 1411142023</p>
<p>A publication-wide association study (pwas), historical language models to prioritise novel therapeutic drug targets. D Narganes-CarlÃ³n, D J Crowther, E R Pearson, Scientific Reports. 13183662023</p>
<p>Scalable extraction of training data from (production) language models. M Nasr, N Carlini, J Hayase, M Jagielski, A F Cooper, D Ippolito, C A Choquette-Choo, E Wallace, F TramÃ¨r, K Lee, arXiv:2311.170352023arXiv preprint</p>
<p>Association analyses based on false discovery rate implicate new loci for coronary artery disease. C P Nelson, A Goel, A S Butterworth, S Kanoni, T R Webb, E Marouli, L Zeng, I Ntalla, F Y Lai, J C Hopewell, Nature genetics. 4992017</p>
<p>The support of human genetic evidence for approved drug indications. M R Nelson, H Tipney, J L Painter, J Shen, P Nicoletti, Y Shen, A Floratos, P C Sham, M J Li, J Wang, Nature genetics. 4782015</p>
<p>The application of artificial intelligence to accelerate g protein-coupled receptor drug discovery. A T Nguyen, D T Nguyen, H Y Koh, J Toskov, W Maclean, A Xu, D Zhang, G I Webb, L T May, M L Halls, British Journal of Pharmacology. 181142024a</p>
<p>Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution. Advances in neural information processing systems. E Nguyen, M Poli, M Faizi, A Thomas, M Wornow, C Birch-Sykes, S Massaroli, A Patel, C Rabideau, Y Bengio, 2024b36</p>
<p>Rational design of viscosity reducing mutants of a monoclonal antibody: hydrophobic versus electrostatic inter-molecular interactions. P Nichols, L Li, S Kumar, P M Buck, S K Singh, S Goswami, B Balthazor, T R Conley, D Sek, M J Allen, MAbs. Taylor &amp; Francis20157</p>
<p>Progen2: exploring the boundaries of protein language models. E Nijkamp, J A Ruffolo, E N Weinstein, N Naik, A Madani, Cell Systems. 14112023</p>
<p>Molecular de-novo design through deep reinforcement learning. M Olivecrona, T Blaschke, O Engkvist, H Chen, Journal of cheminformatics. 912017</p>
<p>arXiv:2303.08774Gpt-4: A technical report. 2023OpenAIarXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Unifying large language models and knowledge graphs: A roadmap. S Pan, L Luo, Y Wang, C Chen, J Wang, X Wu, arXiv:2306.083022023arXiv preprint</p>
<p>Automated extraction of molecular interactions and pathway knowledge using large language model, galactica: Opportunities and challenges. G Park, B.-J Yoon, X Luo, V Lpez-Marrero, P Johnstone, S Yoo, F Alexander, The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks. 2023</p>
<p>Chatgpt: the future of discharge summaries? The Lancet Digital Health. S B Patel, K Lam, G Penedo, Q Malartic, D Hesslow, R Cojocaru, A Cappelli, H Alobeidli, B Pannier, E Almazrouei, J Launay, arXiv:2306.01116The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. 2023. 20235arXiv preprint</p>
<p>Validating therapeutic targets through human genetics. R M Plenge, E M Scolnick, D Altshuler, Nature reviews Drug discovery. 1282013</p>
<p>Neural query synthesis and domain-specific ranking templates for multistage clinical trial matching. R Pradeep, Y Li, Y Wang, J Lin, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval2022</p>
<p>Ai-powered therapeutic target discovery. F W Pun, I V Ozerov, A Zhavoronkov, Trends in Pharmacological Sciences. 2023</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PMLR2021</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Few shot protein generation. S Ram, T Bepler, arXiv:2204.011682022arXiv preprint</p>
<p>Using tf-idf to determine word relevance in document queries. J Ramos, Proceedings of the first instructional conference on machine learning. the first instructional conference on machine learningCiteseer2003242</p>
<p>Evaluating protein transfer learning with tape. Advances in neural information processing systems. R Rao, N Bhattacharya, N Thomas, Y Duan, P Chen, J Canny, P Abbeel, Y Song, 201932</p>
<p>Transformer protein language models are unsupervised structure learners. R Rao, J Meier, T Sercu, S Ovchinnikov, A Rives, Biorxiv. 2020</p>
<p>Msa transformer. R M Rao, J Liu, R Verkuil, J Meier, J Canny, P Abbeel, T Sercu, A Rives, International Conference on Machine Learning. PMLR2021</p>
<p>Five computational developability guidelines for therapeutic antibody profiling. M I Raybould, C Marks, K Krawczyk, B Taddese, J Nowak, A P Lewis, A Bujotzek, J Shi, C M Deane, Proceedings of the National Academy of Sciences. the National Academy of Sciences2019116</p>
<p>Impact of pretraining term frequencies on few-shot numerical reasoning. Y Razeghi, I V Logan, R L Gardner, M Singh, S , 10.18653/v1/2022.findings-emnlp.59Findings of the Association for Computational Linguistics: EMNLP 2022. Y Goldberg, Z Kozareva, Y Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Alphafold accelerates artificial intelligence powered drug discovery: efficient discovery of a novel cdk20 small molecule inhibitor. F Ren, X Ding, M Zheng, M Korzinkin, X Cai, W Zhu, A Mantsyzov, A Aliper, V Aladinskiy, Z Cao, Chemical Science. 1462023</p>
<p>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. A Rives, J Meier, T Sercu, S Goyal, Z Lin, J Liu, D Guo, M Ott, C L Zitnick, J Ma, Proceedings of the National Academy of Sciences. 11815e20162391182021</p>
<p>Large-scale chemical language representations capture molecular structure and properties. J Ross, B Belgodere, V Chenthamarakshan, I Padhi, Y Mroueh, P Das, Nature Machine Intelligence. 4122022</p>
<p>D Rothchild, A Tamkin, J Yu, U Misra, J Gonzalez, C5t5, arXiv:2108.10307Controllable generation of organic molecules with transformers. 2021arXiv preprint</p>
<p>Code llama: Open foundation models for code. B Roziere, J Gehring, F Gloeckle, S Sootla, I Gat, X E Tan, Y Adi, J Liu, T Remez, J Rapin, arXiv:2308.129502023arXiv preprint</p>
<p>Drug discovery companies are customizing chatgpt: here's how. N Savage, Nature Biotechnology. 2023</p>
<p>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. P Schwaller, T Laino, T Gaudin, P Bolgar, C A Hunter, C Bekas, A A Lee, ACS central science. 592019</p>
<p>Predicting retrosynthetic pathways using transformerbased models and a hyper-graph exploration strategy. P Schwaller, R Petraglia, V Zullo, V H Nair, R A Haeuselmann, R Pisoni, C Bekas, A Iuliano, T Laino, Chemical science. 11122020</p>
<p>The language of genes. D B Searls, Nature. 42069122002</p>
<p>scdeepsort: a pretrained cell-type annotation method for single-cell transcriptomics using deep learning with a weighted graph neural network. X Shao, H Yang, X Zhuang, J Liao, P Yang, J Cheng, X Lu, H Chen, X Fan, Nucleic acids research. 49212021</p>
<p>H Shi, P Xie, Z Hu, M Zhang, E P Xing, arXiv:1711.04075Towards automated icd coding using deep learning. 2017arXiv preprint</p>
<p>H.-C Shing, C Shivade, N Pourdamghani, F Nan, P Resnik, D Oard, P Bhatia, arXiv:2104.13498Towards clinical encounter summarization: Learning to compose discharge summaries from prior notes. 2021arXiv preprint</p>
<p>Contrastive learning in protein language space predicts interactions between drugs and protein targets. R Singh, S Sledzieski, B Bryson, L Cowen, B Berger, Proceedings of the National Academy of Sciences. 12024e22207781202023</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, Nature. 62079722023</p>
<p>From target to drug: generative modeling for the multimodal structure-based ligand design. M Skalic, D Sabbadin, B Sattarov, S Sciabola, G De Fabritiis, Molecular pharmaceutics. 16102019</p>
<p>R S Srinivasa, C Qian, B Theodorou, J Spaeder, C Xiao, L Glass, J Sun, arXiv:2204.06501Clinical trial site matching with improved diversity using fair policy learning. 2022arXiv preprint</p>
<p>B Su, D Du, Z Yang, Y Zhou, J Li, A Rao, H Sun, Z Lu, J.-R Wen, arXiv:2209.05481A molecular multimodal foundation model associating molecule graphs with natural language. 2022arXiv preprint</p>
<p>Methods for numeracy-preserving word embeddings. D Sundararaman, S Si, V Subramanian, G Wang, D Hazarika, Carin , L , Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language Processing2020</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, Galactica, arXiv:2211.09085A large language model for science. 2022arXiv preprint</p>
<p>Can neural networks do arithmetic? a survey on the elementary numerical skills of state-of-the-art deep learning models. A Testolin, arXiv:2303.077352023arXiv preprint</p>
<p>A Thawani, J Pujara, P A Szekely, F Ilievski, arXiv:2103.13136Representing numbers in nlp: a survey and a vision. 2021arXiv preprint</p>
<p>Human disease modeling reveals integrated transcriptional and epigenetic mechanisms of notch1 haploinsufficiency. C V Theodoris, M Li, M P White, L Liu, D He, K S Pollard, B G Bruneau, D Srivastava, Cell. 16062015</p>
<p>Network-based screen in ipsc-derived cells reveals therapeutic candidate for heart valve disease. C V Theodoris, P Zhou, L Liu, Y Zhang, T Nishino, Y Huang, A Kostina, S S Ranade, C A Gifford, V Uspenskiy, Science. 37165307242021</p>
<p>Transfer learning enables predictions in network biology. C V Theodoris, L Xiao, A Chopra, M D Chaffin, Z R Al Sayed, M C Hill, H Mantineo, E M Brydon, Z Zeng, X S Liu, Nature. 2023</p>
<p>B Theodorou, L Glass, C Xiao, J Sun, Framm, arXiv:2305.19407Fair ranking with missing modalities for clinical trial site selection. 2023arXiv preprint</p>
<p>Large language models in medicine. A J Thirunavukarasu, D S J Ting, K Elangovan, L Gutierrez, T F Tan, D S W Ting, Nature medicine. 2982023</p>
<p>Harnessing large language models (llms) for candidate gene prioritization and selection. M Toufiq, D Rinchai, E Bettacchioli, B S A Kabeer, T Khan, B Subba, O White, M Yurieva, J George, N Jourde-Chiche, Journal of Translational Medicine. 2117282023</p>
<p>Improvements to bm25 and language models examined. A Trotman, A Puurula, B Burgess, Proceedings of the 19th Australasian Document Computing Symposium. the 19th Australasian Document Computing Symposium2014</p>
<p>Towards generalist biomedical ai. T Tu, S Azizi, D Driess, M Schaekermann, M Amin, P.-C Chang, A Carroll, C Lau, R Tanno, I Ktena, NEJM AI. 13AIoa2300138, 2024</p>
<p>Dual use of artificial-intelligence-powered drug discovery. F Urbina, F Lentzos, C Invernizzi, S Ekins, Nature Machine Intelligence. 432022</p>
<p>The impact of alphafold protein structure database on the fields of life sciences. M Varadi, S Velankar, Proteomics. 231722001282023</p>
<p>Alphafold protein structure database: massively expanding the structural coverage of protein-sequence space with high-accuracy models. M Varadi, S Anyango, M Deshpande, S Nair, C Natassia, G Yordanova, D Yuan, O Stroe, G Wood, A Laydon, Nucleic acids research. 50D12022</p>
<p>Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Å Kaiser, I Polosukhin, 201730Attention is all you need</p>
<p>Language models generalize beyond natural proteins. R Verkuil, O Kabeli, Y Du, B I Wicky, L F Milles, J Dauparas, D Baker, S Ovchinnikov, T Sercu, A Rives, bioRxiv. 2022</p>
<p>Developing predictive assays: the phenotypic screening "rule of 3. F Vincent, P Loria, M Pregel, R Stanton, L Kitching, K Nocka, R Doyonnas, C Steppan, A Gilbert, T Schroeter, Science translational medicine. 72932015</p>
<p>Musitedeep: a deep-learning framework for general and kinase-specific phosphorylation site prediction. D Wang, S Zeng, C Xu, W Qiu, Y Liang, T Joshi, D Xu, Bioinformatics. 33242017</p>
<p>Scaffolding protein functional sites using deep learning. J Wang, S Lisanza, D Juergens, D Tischer, J L Watson, K M Castro, R Ragotte, A Saragovi, L F Milles, M Baek, Science. 37766042022a</p>
<p>Structureaware multimodal deep learning for drug-protein interaction prediction. P Wang, S Zheng, Y Jiang, C Li, J Liu, C Wen, A Patronov, D Qian, H Chen, Y Yang, Journal of Chemical Information and Modeling. 6252022b</p>
<p>Netgo 3.0: Protein language model improves large-scale functional annotations. S Wang, R You, Y Liu, Y Xiong, S Zhu, Proteomics &amp; Bioinformatics. 2023aGenomics</p>
<p>Text-image embedding network for common thorax disease classification and reporting in chest x-rays. X Wang, Y Peng, L Lu, Z Lu, R M Summers, Tienet, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Zero-shot clinical trial document similarity search using self-supervision. Z Wang, J Sun, Trial2vec, 2022EMNLP Findings</p>
<p>Scaling medical tabular data predictors via data consolidation, enrichment, and refinement. Z Wang, C Gao, C Xiao, J Sun, Meditab, arXiv:2305.120812023b</p>
<p>Sequential predictive modeling of clinical trial outcome with meta-learning. Z Wang, C Xiao, J Sun, Spot, arXiv:2304.053522023carXiv preprint</p>
<p>De novo design of protein structure and function with rfdiffusion. J L Watson, D Juergens, N R Bennett, B L Trippe, J Yim, H E Eisenach, W Ahern, A J Borst, R J Ragotte, L F Milles, Nature. 62079762023</p>
<p>Six ways large language models are changing healthcare. P Webster, Nature Medicine. 2023</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. D Weininger, Journal of chemical information and computer sciences. 2811988</p>
<p>Clinidigest: a case study in large language model based large-scale summarization of clinical trial descriptions. R White, T Peng, P Sripitak, A Rosenberg Johansen, M Snyder, Proceedings of the 2023 ACM Conference on Information Technology for Social Good. the 2023 ACM Conference on Information Technology for Social Good2023</p>
<p>A neural architecture for automated icd coding. P Xie, E Xing, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational Linguistics20181</p>
<p>Raim: Recurrent attentive and intensive model of multimodal patient monitoring data. Y Xu, S Biswal, S R Deshpande, K O Maher, J Sun, Proceedings of the 24th ACM SIGKDD international conference on Knowledge Discovery &amp; Data Mining. the 24th ACM SIGKDD international conference on Knowledge Discovery &amp; Data Mining2018</p>
<p>Adapting transformer-based language models to radiology. A Yan, J Mcauley, X Lu, J Du, E Y Chang, A Gentili, C.-N Hsu, Radbert, Radiology: Artificial Intelligence. 44e2102582022</p>
<p>Protein structure relationships revealed by mutational analysis. C Yanofsky, V Horn, D Thorpe, Science. 14636511964</p>
<p>S Yin, C Fu, S Zhao, K Li, X Sun, T Xu, E Chen, arXiv:2306.13549A survey on multimodal large language models. 2023arXiv preprint</p>
<p>Large language models for chemistry robotics. N Yoshikawa, M Skreta, K Darvish, S Arellano-Rubach, Z Ji, L BjÃ¸rn Kristensen, A Z Li, Y Zhao, H Xu, A Kuramshin, Autonomous Robots. 2023</p>
<p>Alignment-free metal ion-binding site prediction from protein sequence through pretrained language model and multi-task learning. Q Yuan, S Chen, Y Wang, H Zhao, Y Yang, Briefings in Bioinformatics. 2364442022</p>
<p>Genome-scale annotation of protein binding sites via language model and geometric deep learning. Q Yuan, C Tian, Y Yang, bioRxiv. 2023</p>
<p>Genome-wide analyses using uk biobank data provide insights into the genetic architecture of osteoarthritis. E Zengini, K Hatzikotoulas, I Tachmazidou, J Steinberg, F P Hartwig, L Southam, S Hackinger, C G Boer, U Styrkarsdottir, A Gilly, Nature genetics. 5042018</p>
<p>Protein language model-powered 3d ligand binding site prediction from protein sequence. S Zhang, L Xie, arXiv:2312.030162023arXiv preprint</p>
<p>Deepenroll: patient-trial matching with deep embedding and entailment prediction. X Zhang, C Xiao, L M Glass, J Sun, Proceedings of the web conference 2020. the web conference 20202020a</p>
<p>Large scale automated ICD coding using BERT pretraining. Z Zhang, J Liu, N Razavian, Bert-Xml, Proceedings of the 3rd Clinical Natural Language Processing Workshop. A Rumshisky, K Roberts, S Bethard, T Naumann, the 3rd Clinical Natural Language Processing WorkshopAssociation for Computational LinguisticsNovember 2020b</p>
<p>Deep learning enables rapid identification of potent ddr1 kinase inhibitors. A Zhavoronkov, Y A Ivanenkov, A Aliper, M S Veselov, V A Aladinskiy, A V Aladinskaya, V A Terentiev, D A Polykovskiy, M D Kuznetsov, A Asadulaev, Nature biotechnology. 3792019</p>
<p>Large language models for scientific synthesis, inference and explanation. Y Zheng, H Y Koh, J Ju, A T Nguyen, L T May, G I Webb, S Pan, arXiv:2310.079842023arXiv preprint</p>
<p>Dnabert-2: Efficient foundation model and benchmark for multi-species genome. Z Zhou, Y Ji, W Li, P Dutta, R Davuluri, H Liu, arXiv:2306.150062023arXiv preprint</p>
<p>Dms-mapseq for genomewide or targeted rna structure probing in vivo. M Zubradt, P Gupta, S Persad, A M Lambowitz, J S Weissman, S Rouskin, 10.1038/nmeth.4057Nature Methods. 141Nov 2016</p>
<p>Genome-scale language models reveal sars-cov-2 evolutionary dynamics. M Zvyagin, A Brace, K Hippe, Y Deng, B Zhang, C O Bohorquez, A Clyde, B Kale, D Perez-Rivera, H Ma, The International Journal of High Performance Computing Applications. 3762023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9235 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9235</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9235</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-257496827</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.07142v3.pdf" target="_blank">Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification</a></p>
                <p><strong>Paper Abstract:</strong> This case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an English-language job posting is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification, including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically, we evaluate the performance of two commercially available state-of-the-art GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance. Our results show that, with a well-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all other models, achieving a 6% increase in Precision@95% Recall compared to the best supervised approach. Furthermore, we observe that the wording of the prompt is a critical factor in eliciting the appropriate"reasoning"in the model, and that seemingly minor aspects of the prompt significantly affect the model's performance.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9235.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9235.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>zero-shot-baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot prompt baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline prompting where the model is asked the classification question with no attempt to elicit chain-of-thought or additional instructions; used as the starting point for prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graduate Job Classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of job postings as GRAD (suitable for recent graduates) or NON-GRAD (requires more professional experience).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Single user message presenting the job posting and asking: 'Is this job (A) a job fit for a recent graduate, or (B) a job requiring more professional experience. Answer:' (no examples, no step-by-step prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>F1: 65.6; Recall: 70.6 (reported for the zero-shot baseline used in prompt-engineering experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Serves as baseline; the paper attributes its limited performance to lack of guidance to elicit the model's reasoning in a useful form.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Chat-format prompt; temperature = 0; dataset split 7000 train / 3000 test; baseline prompt example given in text.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9235.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9235.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>few-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing the model with a small number of (two) annotated examples where the assistant includes chain-of-thought reasoning before the final label, then querying it on the target.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graduate Job Classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above: classify job postings into GRAD vs NON-GRAD.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot: two example QA exchanges in the chat format where the assistant shows detailed step-by-step reasoning (CoT) and then outputs (A) or (B), followed by the test query.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to zero-shot baseline and zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not reported as a numeric metric in the paper, but stated to perform noticeably worse than zero-shot baseline on this task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Worse than zero-shot baseline (zero-shot F1 65.6); few-shot CoT degraded performance—authors hypothesize example biasing.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors speculate examples biased the model's reasoning too much while its internal knowledge was already sufficient; providing examples reduced performance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Due to long job postings, providing more than two examples required truncation which degraded performance; two-shot limit used.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9235.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9235.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Eliciting step-by-step reasoning in a zero-shot setting by appending 'Let's think step by step' to the question, without giving examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graduate Job Classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompt that explicitly asks the model to 'think step-by-step' (Zero-CoT) before answering, with no exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to plain zero-shot baseline and few-shot CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not separately tabulated; authors report zero-shot (unelicited reasoning) baseline was outperformed by several prompt modifications including zero-shot-CoT variants — but exact metric per zero-shot-CoT not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Zero-shot-CoT was part of experiments; overall the paper reports zero-shot elicited-reasoning approaches can work well in general but gives no precise per-format numbers other than the baseline and final prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Motivation: many job postings likely appear in training data and a step-by-step cue can elicit internal reasoning, per prior work on zero-shot reasoners.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompt appended: 'Answer: Let's think step by step,'; temperature = 0; used in chat format.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9235.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9235.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>instruction_messages</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Role/task instructions via system and/or user messages (rawinst/sysinst/bothinst)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing explicit instructions about the model's role and the classification task either as a user message (rawinst), as a system message (sysinst), or split between system (role) and user (task) messages (bothinst).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graduate Job Classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Instruction text specifying role (e.g., 'You are an AI expert in career advice') and task (criteria for GRAD) given via different chat channels: all as user message (rawinst), all as system message (sysinst), or split role as system and task as user (bothinst).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to baseline zero-shot and other prompt modifications (mock, reit, templates).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Applying role-as-system + initial user task message yielded the single biggest increase in performance in prompt ablations: +5.9 F1 (relative to previous step).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Improved F1 by +5.9 compared to the prior prompt state (paper's ablation sequence); contributed substantially to reaching production-quality recall/precision.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+5.9 F1 (role as system + initial user task message vs prior prompt state)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Providing role and task instructions clarifies the model's objective and elicits more appropriate reasoning; the system-role channel in chat format is an important lever.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>System prompt example: 'You are an AI expert in career advice...'; user message: task definition including that internships count as acceptable experience; temperature = 0; experiments found no consistent single 'best' way (user vs system) but role-as-system plus user task gave largest gain in their pipeline.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9235.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9235.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mocked_ack</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mocked-exchange acknowledgement (mock)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt pattern where instructions are followed by a mocked assistant acknowledgement ('Yes, I understand. I am Frederick...') before presenting the job for classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graduate Job Classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>After system+user instructions, include a mocked assistant response acknowledging instructions; then give a positive feedback message and the job query.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to system+user instructions without the mocked acknowledgement step.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Mocked acknowledgement allowed the model to hit the 95% recall threshold in the ablation sequence and yielded an additional +1.3 F1 improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>+1.3 F1 (mocked acknowledgement vs prior instruction-only prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+1.3 F1</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest that using conversational patterns the model is fine-tuned on (an assistant acknowledging instructions) helps the model adopt the intended role and follow instructions more reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Sequence: system role msg, user task msg, assistant ack msg ('Yes, I understand...'), then user 'Great! Let's begin then :) For the given job...' ; temperature = 0.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9235.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9235.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>reiteration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Re-iterated instructions (reit)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt modification that reinforces key instructions by repeating or rephrasing them in system and user messages to emphasise expertise and step-by-step analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graduate Job Classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>System message includes additional reinforcement language (e.g., 'Remember, you're the best AI careers expert...'), and the user message requests step-by-step analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to similar prompts without re-iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Part of the chain of prompt improvements; incremental positive effect reported but specific numeric delta not separately tabulated beyond inclusion in cumulative gains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Contributed to the cumulative performance improvements leading to final prompt (final F1 91.7, recall 97%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Reinforcing role/task text biases the model toward more thorough stepwise reasoning and increased confidence in following instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>System prompt had appended reminder text; used in chat format; temperature = 0.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9235.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9235.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>answer_templates</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Strict and loose answer templates (strict/loose)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Forcing the model to reply in a prespecified template: 'strict' enforces a stepwise Reasoning Step 1/2/3 structure and final answer; 'loose' only requires the final answer line to appear.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graduate Job Classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Template constrained outputs: e.g., loose requires output to end with 'Final Answer: This is a (A)... or (B)... Answer: Let's think step-by-step', strict requires a numbered Reasoning Step 1..N then Final Answer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to free-form answers (no template constraint).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Strict and loose templates increased 'Template Stickiness' (percentage of outputs matching desired format) but resulted in decreased classification performance relative to the unconstrained prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Higher template adherence but lower F1/precision compared to unconstrained best-performing prompt; exact numeric drop not individually tabled.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Constraining output format increases compliance with structure but may hamper natural reasoning the model would otherwise use, reducing overall task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Various wordings evaluated; best-performing template examples provided in text; observed per-format Template Stickiness metric recorded in Table 4 (exact values not reproduced in paper text).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9235.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9235.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>naming_positive</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Naming assistant and positive feedback (name + pos)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small prompt tweaks: assigning the assistant a name ('Frederick') and giving positive feedback before the query, tested for impact on model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graduate Job Classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>In system message the assistant is given a name; after assistant acknowledgement include a brief positive reinforcement message ('Great! Let's begin then :)').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to same prompt without a name and without positive reinforcement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report small but consistent increases in performance when adding a name and positive feedback; no per-tweak numeric values except that they contributed to cumulative improvements (e.g., toward final F1 91.7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Incremental positive effect; not individually quantified beyond authors' statement that all resulted in similar improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note lack of theory but conjecture that these conversational cues can trigger beneficial behavior learned during instruction tuning/fine-tuning on conversational data.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Multiple common English names tested; positive reinforcement anecdote noted from external developer; temperature = 0.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9235.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9235.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>info_guidance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Providing additional disambiguating information (info)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augmenting the task instruction with clarifications to address specific reasoning failure modes (e.g., explicitly stating that internships count as acceptable experience for graduates).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graduate Job Classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Task message contains precise decision rules and clarifications to reduce misinterpretation (e.g., 'When analysing experience required, take into account that requiring internships is still fit for a graduate').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to prompts without such clarifications.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Inclusion of targeted clarifications reduced observed erroneous over-generalizations; contributed to final high recall/precision, but no isolated numeric reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Explicit decision rules reduce model misinterpretation of borderline cases and produce more consistent label decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Used in the best-performing final prompt; temperature = 0.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9235.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9235.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>final-combined-prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Best-performing combined prompt (system role + mock ack + name + pos + clarifications)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A composite chat-format prompt combining role-as-system, clarified task description (including internships rule), mocked assistant acknowledgement, assistant name, positive feedback, and a step-by-step cue to reach the right conclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graduate Job Classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>System message: 'You are Frederick, an AI expert in career advice...' + user1 task msg including clarifications + assistant ack + user2 positive feedback + test query ending 'Answer: Let's think step by step to reach the right conclusion'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against zero-shot baseline, few-shot CoT, instruction-only variants, and templates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>F1: 91.7; Recall: 97%; Additionally, overall gpt-3.5 (with tuned prompts) achieved P@95%R = 86.9 on the GRAD label (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Substantially better than zero-shot baseline (F1 from 65.6 -> 91.7) and better than davinci-003 and supervised baselines on the P@95%R metric (gpt-3.5 P@95%R 86.9 vs davinci-003 80.4 and DeBERTaV3 79.7).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+26.1 F1 (absolute increase from zero-shot baseline F1 65.6 to final F1 91.7); contributed to achieving Recall 97% and P@95%R 86.9 for gpt-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Combination of clarifying role, task rules, conversational acknowledgement, and positive cues likely elicits appropriate internal reasoning and consistent adherence to decision rules learned during instruction-tuning; authors note lack of definitive theory for why some micro-changes (e.g., naming) help.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Chat-format prompt assembled as described in text; temperature = 0; dataset 7000 train / 3000 test; few-shot examples were not used in final prompt; template stickiness measured but gpt-3.5 struggled more with strict adherence than text-davinci-003.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9235.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9235.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>model-comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-model comparison (davinci-002, davinci-003, gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of different GPT-family LLMs on the same job classification task showing differences in robustness and recall at production-oriented thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002 / text-davinci-003 / gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graduate Job Classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Models evaluated with prompt variants; prompt engineering primarily explored on gpt-3.5-turbo (chat format); davinci models used for comparison with their own (non-chat) prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct comparison of models under best-effort prompting for each (davinci models with typical prompts; gpt-3.5 with chat-style prompt engineering).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 2 aggregated results: P@95%R — davinci-002: 0 (failed to reach 95% recall), davinci-003: 80.4, gpt-3.5-turbo: 86.9; P@85%R — davinci-002: 72.6, davinci-003: 80.4, gpt-3.5-turbo: 86.9; Recall — davinci-002: 72.2, davinci-003: 95.6, gpt-3.5-turbo: 97.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>gpt-3.5-turbo outperformed davinci-003 on P@95%R and overall recall; davinci-002 underperformed and could not reach 95% recall.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>gpt-3.5-turbo P@95%R 86.9 vs davinci-003 80.4 → +6.5 percentage points; gpt-3.5 recall 97 vs davinci-003 95.6 → +1.4pp.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Model architecture and instruction-tuning differences impact robustness to prompt formats; chat-optimized gpt-3.5-turbo particularly benefits from conversational prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>temperature = 0 for all; davinci-003 noted as refinement of davinci-002; prompt modifications focused on gpt-3.5 chat format using ChatML conventions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing <em>(Rating: 2)</em></li>
                <li>Program-aided language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9235",
    "paper_id": "paper-257496827",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "zero-shot-baseline",
            "name_full": "Zero-shot prompt baseline",
            "brief_description": "Baseline prompting where the model is asked the classification question with no attempt to elicit chain-of-thought or additional instructions; used as the starting point for prompt engineering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "Graduate Job Classification",
            "task_description": "Binary classification of job postings as GRAD (suitable for recent graduates) or NON-GRAD (requires more professional experience).",
            "presentation_format": "Single user message presenting the job posting and asking: 'Is this job (A) a job fit for a recent graduate, or (B) a job requiring more professional experience. Answer:' (no examples, no step-by-step prompt).",
            "comparison_format": null,
            "performance": "F1: 65.6; Recall: 70.6 (reported for the zero-shot baseline used in prompt-engineering experiments)",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Serves as baseline; the paper attributes its limited performance to lack of guidance to elicit the model's reasoning in a useful form.",
            "null_or_negative_result": false,
            "experimental_details": "Chat-format prompt; temperature = 0; dataset split 7000 train / 3000 test; baseline prompt example given in text.",
            "uuid": "e9235.0"
        },
        {
            "name_short": "few-shot-CoT",
            "name_full": "Few-shot Chain-of-Thought prompting",
            "brief_description": "Providing the model with a small number of (two) annotated examples where the assistant includes chain-of-thought reasoning before the final label, then querying it on the target.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "Graduate Job Classification",
            "task_description": "As above: classify job postings into GRAD vs NON-GRAD.",
            "presentation_format": "Few-shot: two example QA exchanges in the chat format where the assistant shows detailed step-by-step reasoning (CoT) and then outputs (A) or (B), followed by the test query.",
            "comparison_format": "Compared to zero-shot baseline and zero-shot-CoT",
            "performance": "Not reported as a numeric metric in the paper, but stated to perform noticeably worse than zero-shot baseline on this task.",
            "performance_comparison": "Worse than zero-shot baseline (zero-shot F1 65.6); few-shot CoT degraded performance—authors hypothesize example biasing.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors speculate examples biased the model's reasoning too much while its internal knowledge was already sufficient; providing examples reduced performance.",
            "null_or_negative_result": true,
            "experimental_details": "Due to long job postings, providing more than two examples required truncation which degraded performance; two-shot limit used.",
            "uuid": "e9235.1"
        },
        {
            "name_short": "zero-shot-CoT",
            "name_full": "Zero-shot Chain-of-Thought prompting",
            "brief_description": "Eliciting step-by-step reasoning in a zero-shot setting by appending 'Let's think step by step' to the question, without giving examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "Graduate Job Classification",
            "task_description": "As above.",
            "presentation_format": "Zero-shot prompt that explicitly asks the model to 'think step-by-step' (Zero-CoT) before answering, with no exemplars.",
            "comparison_format": "Compared to plain zero-shot baseline and few-shot CoT",
            "performance": "Not separately tabulated; authors report zero-shot (unelicited reasoning) baseline was outperformed by several prompt modifications including zero-shot-CoT variants — but exact metric per zero-shot-CoT not provided.",
            "performance_comparison": "Zero-shot-CoT was part of experiments; overall the paper reports zero-shot elicited-reasoning approaches can work well in general but gives no precise per-format numbers other than the baseline and final prompt.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Motivation: many job postings likely appear in training data and a step-by-step cue can elicit internal reasoning, per prior work on zero-shot reasoners.",
            "null_or_negative_result": null,
            "experimental_details": "Prompt appended: 'Answer: Let's think step by step,'; temperature = 0; used in chat format.",
            "uuid": "e9235.2"
        },
        {
            "name_short": "instruction_messages",
            "name_full": "Role/task instructions via system and/or user messages (rawinst/sysinst/bothinst)",
            "brief_description": "Providing explicit instructions about the model's role and the classification task either as a user message (rawinst), as a system message (sysinst), or split between system (role) and user (task) messages (bothinst).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "Graduate Job Classification",
            "task_description": "As above.",
            "presentation_format": "Instruction text specifying role (e.g., 'You are an AI expert in career advice') and task (criteria for GRAD) given via different chat channels: all as user message (rawinst), all as system message (sysinst), or split role as system and task as user (bothinst).",
            "comparison_format": "Compared to baseline zero-shot and other prompt modifications (mock, reit, templates).",
            "performance": "Applying role-as-system + initial user task message yielded the single biggest increase in performance in prompt ablations: +5.9 F1 (relative to previous step).",
            "performance_comparison": "Improved F1 by +5.9 compared to the prior prompt state (paper's ablation sequence); contributed substantially to reaching production-quality recall/precision.",
            "format_effect_size": "+5.9 F1 (role as system + initial user task message vs prior prompt state)",
            "explanation_or_hypothesis": "Providing role and task instructions clarifies the model's objective and elicits more appropriate reasoning; the system-role channel in chat format is an important lever.",
            "null_or_negative_result": false,
            "experimental_details": "System prompt example: 'You are an AI expert in career advice...'; user message: task definition including that internships count as acceptable experience; temperature = 0; experiments found no consistent single 'best' way (user vs system) but role-as-system plus user task gave largest gain in their pipeline.",
            "uuid": "e9235.3"
        },
        {
            "name_short": "mocked_ack",
            "name_full": "Mocked-exchange acknowledgement (mock)",
            "brief_description": "A prompt pattern where instructions are followed by a mocked assistant acknowledgement ('Yes, I understand. I am Frederick...') before presenting the job for classification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "Graduate Job Classification",
            "task_description": "As above.",
            "presentation_format": "After system+user instructions, include a mocked assistant response acknowledging instructions; then give a positive feedback message and the job query.",
            "comparison_format": "Compared to system+user instructions without the mocked acknowledgement step.",
            "performance": "Mocked acknowledgement allowed the model to hit the 95% recall threshold in the ablation sequence and yielded an additional +1.3 F1 improvement.",
            "performance_comparison": "+1.3 F1 (mocked acknowledgement vs prior instruction-only prompt)",
            "format_effect_size": "+1.3 F1",
            "explanation_or_hypothesis": "Authors suggest that using conversational patterns the model is fine-tuned on (an assistant acknowledging instructions) helps the model adopt the intended role and follow instructions more reliably.",
            "null_or_negative_result": false,
            "experimental_details": "Sequence: system role msg, user task msg, assistant ack msg ('Yes, I understand...'), then user 'Great! Let's begin then :) For the given job...' ; temperature = 0.",
            "uuid": "e9235.4"
        },
        {
            "name_short": "reiteration",
            "name_full": "Re-iterated instructions (reit)",
            "brief_description": "Prompt modification that reinforces key instructions by repeating or rephrasing them in system and user messages to emphasise expertise and step-by-step analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "Graduate Job Classification",
            "task_description": "As above.",
            "presentation_format": "System message includes additional reinforcement language (e.g., 'Remember, you're the best AI careers expert...'), and the user message requests step-by-step analysis.",
            "comparison_format": "Compared to similar prompts without re-iteration.",
            "performance": "Part of the chain of prompt improvements; incremental positive effect reported but specific numeric delta not separately tabulated beyond inclusion in cumulative gains.",
            "performance_comparison": "Contributed to the cumulative performance improvements leading to final prompt (final F1 91.7, recall 97%).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Reinforcing role/task text biases the model toward more thorough stepwise reasoning and increased confidence in following instructions.",
            "null_or_negative_result": null,
            "experimental_details": "System prompt had appended reminder text; used in chat format; temperature = 0.",
            "uuid": "e9235.5"
        },
        {
            "name_short": "answer_templates",
            "name_full": "Strict and loose answer templates (strict/loose)",
            "brief_description": "Forcing the model to reply in a prespecified template: 'strict' enforces a stepwise Reasoning Step 1/2/3 structure and final answer; 'loose' only requires the final answer line to appear.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "Graduate Job Classification",
            "task_description": "As above.",
            "presentation_format": "Template constrained outputs: e.g., loose requires output to end with 'Final Answer: This is a (A)... or (B)... Answer: Let's think step-by-step', strict requires a numbered Reasoning Step 1..N then Final Answer.",
            "comparison_format": "Compared to free-form answers (no template constraint).",
            "performance": "Strict and loose templates increased 'Template Stickiness' (percentage of outputs matching desired format) but resulted in decreased classification performance relative to the unconstrained prompt.",
            "performance_comparison": "Higher template adherence but lower F1/precision compared to unconstrained best-performing prompt; exact numeric drop not individually tabled.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Constraining output format increases compliance with structure but may hamper natural reasoning the model would otherwise use, reducing overall task performance.",
            "null_or_negative_result": true,
            "experimental_details": "Various wordings evaluated; best-performing template examples provided in text; observed per-format Template Stickiness metric recorded in Table 4 (exact values not reproduced in paper text).",
            "uuid": "e9235.6"
        },
        {
            "name_short": "naming_positive",
            "name_full": "Naming assistant and positive feedback (name + pos)",
            "brief_description": "Small prompt tweaks: assigning the assistant a name ('Frederick') and giving positive feedback before the query, tested for impact on model outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "Graduate Job Classification",
            "task_description": "As above.",
            "presentation_format": "In system message the assistant is given a name; after assistant acknowledgement include a brief positive reinforcement message ('Great! Let's begin then :)').",
            "comparison_format": "Compared to same prompt without a name and without positive reinforcement.",
            "performance": "Authors report small but consistent increases in performance when adding a name and positive feedback; no per-tweak numeric values except that they contributed to cumulative improvements (e.g., toward final F1 91.7).",
            "performance_comparison": "Incremental positive effect; not individually quantified beyond authors' statement that all resulted in similar improvement.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors note lack of theory but conjecture that these conversational cues can trigger beneficial behavior learned during instruction tuning/fine-tuning on conversational data.",
            "null_or_negative_result": null,
            "experimental_details": "Multiple common English names tested; positive reinforcement anecdote noted from external developer; temperature = 0.",
            "uuid": "e9235.7"
        },
        {
            "name_short": "info_guidance",
            "name_full": "Providing additional disambiguating information (info)",
            "brief_description": "Augmenting the task instruction with clarifications to address specific reasoning failure modes (e.g., explicitly stating that internships count as acceptable experience for graduates).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "Graduate Job Classification",
            "task_description": "As above.",
            "presentation_format": "Task message contains precise decision rules and clarifications to reduce misinterpretation (e.g., 'When analysing experience required, take into account that requiring internships is still fit for a graduate').",
            "comparison_format": "Compared to prompts without such clarifications.",
            "performance": "Inclusion of targeted clarifications reduced observed erroneous over-generalizations; contributed to final high recall/precision, but no isolated numeric reported.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Explicit decision rules reduce model misinterpretation of borderline cases and produce more consistent label decisions.",
            "null_or_negative_result": null,
            "experimental_details": "Used in the best-performing final prompt; temperature = 0.",
            "uuid": "e9235.8"
        },
        {
            "name_short": "final-combined-prompt",
            "name_full": "Best-performing combined prompt (system role + mock ack + name + pos + clarifications)",
            "brief_description": "A composite chat-format prompt combining role-as-system, clarified task description (including internships rule), mocked assistant acknowledgement, assistant name, positive feedback, and a step-by-step cue to reach the right conclusion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "Graduate Job Classification",
            "task_description": "As above.",
            "presentation_format": "System message: 'You are Frederick, an AI expert in career advice...' + user1 task msg including clarifications + assistant ack + user2 positive feedback + test query ending 'Answer: Let's think step by step to reach the right conclusion'.",
            "comparison_format": "Compared against zero-shot baseline, few-shot CoT, instruction-only variants, and templates.",
            "performance": "F1: 91.7; Recall: 97%; Additionally, overall gpt-3.5 (with tuned prompts) achieved P@95%R = 86.9 on the GRAD label (Table 2).",
            "performance_comparison": "Substantially better than zero-shot baseline (F1 from 65.6 -&gt; 91.7) and better than davinci-003 and supervised baselines on the P@95%R metric (gpt-3.5 P@95%R 86.9 vs davinci-003 80.4 and DeBERTaV3 79.7).",
            "format_effect_size": "+26.1 F1 (absolute increase from zero-shot baseline F1 65.6 to final F1 91.7); contributed to achieving Recall 97% and P@95%R 86.9 for gpt-3.5.",
            "explanation_or_hypothesis": "Combination of clarifying role, task rules, conversational acknowledgement, and positive cues likely elicits appropriate internal reasoning and consistent adherence to decision rules learned during instruction-tuning; authors note lack of definitive theory for why some micro-changes (e.g., naming) help.",
            "null_or_negative_result": false,
            "experimental_details": "Chat-format prompt assembled as described in text; temperature = 0; dataset 7000 train / 3000 test; few-shot examples were not used in final prompt; template stickiness measured but gpt-3.5 struggled more with strict adherence than text-davinci-003.",
            "uuid": "e9235.9"
        },
        {
            "name_short": "model-comparison",
            "name_full": "Cross-model comparison (davinci-002, davinci-003, gpt-3.5-turbo)",
            "brief_description": "Comparison of different GPT-family LLMs on the same job classification task showing differences in robustness and recall at production-oriented thresholds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-002 / text-davinci-003 / gpt-3.5-turbo",
            "model_size": null,
            "task_name": "Graduate Job Classification",
            "task_description": "As above.",
            "presentation_format": "Models evaluated with prompt variants; prompt engineering primarily explored on gpt-3.5-turbo (chat format); davinci models used for comparison with their own (non-chat) prompts.",
            "comparison_format": "Direct comparison of models under best-effort prompting for each (davinci models with typical prompts; gpt-3.5 with chat-style prompt engineering).",
            "performance": "Table 2 aggregated results: P@95%R — davinci-002: 0 (failed to reach 95% recall), davinci-003: 80.4, gpt-3.5-turbo: 86.9; P@85%R — davinci-002: 72.6, davinci-003: 80.4, gpt-3.5-turbo: 86.9; Recall — davinci-002: 72.2, davinci-003: 95.6, gpt-3.5-turbo: 97.",
            "performance_comparison": "gpt-3.5-turbo outperformed davinci-003 on P@95%R and overall recall; davinci-002 underperformed and could not reach 95% recall.",
            "format_effect_size": "gpt-3.5-turbo P@95%R 86.9 vs davinci-003 80.4 → +6.5 percentage points; gpt-3.5 recall 97 vs davinci-003 95.6 → +1.4pp.",
            "explanation_or_hypothesis": "Model architecture and instruction-tuning differences impact robustness to prompt formats; chat-optimized gpt-3.5-turbo particularly benefits from conversational prompt engineering.",
            "null_or_negative_result": false,
            "experimental_details": "temperature = 0 for all; davinci-003 noted as refinement of davinci-002; prompt modifications focused on gpt-3.5 chat format using ChatML conventions.",
            "uuid": "e9235.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        },
        {
            "paper_title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "rating": 2,
            "sanitized_title": "pretrain_prompt_and_predict_a_systematic_survey_of_prompting_methods_in_natural_language_processing"
        },
        {
            "paper_title": "Program-aided language models",
            "rating": 1,
            "sanitized_title": "programaided_language_models"
        }
    ],
    "cost": 0.01477475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification
18 Apr 2023</p>
<p>Benjamin Clavié ben.clavie@brightnetwork.co.uk 
Bright Network
EdinburghUK</p>
<p>Alexandru Ciceu 
Silicon Grove
EdinburghUK</p>
<p>Frederick Naylor 
Bright Network
EdinburghUK</p>
<p>Guillaume Soulié 
Bright Network
EdinburghUK</p>
<p>Thomas Brightwell 
Bright Network
EdinburghUK</p>
<p>Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification
18 Apr 2023Large Language Models · Text Classification · Natural Lan- guage Processing · Industrial Applications · Prompt Engineering
This case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an Englishlanguage is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification, including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically, we evaluate the performance of two commercially available state-of-the-art GPT-3.5based language models, text-davinci-003 and gpt-3.5-turbo. We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance. Our results show that, with a well-designed prompt, a zero-shot gpt-3.5turboclassifier outperforms all other models, achieving a 6% increase in Precision@95% Recall compared to the best supervised approach. Furthermore, we observe that the wording of the prompt is a critical factor in eliciting the appropriate "reasoning" in the model, and that seemingly minor aspects of the prompt significantly affect the model's performance.</p>
<p>Introduction</p>
<p>The combination of broadened access to higher education and rapid technological advancement with the mass-adoption of computing has resulted in a number of phenomena. The need for computational tools to support the delivery of quality education at scale has been frequently highlighted, even allowing for the development of an active academic subfield [22]. At the other end of the pipeline, technological advances have caused massive changes in the skills required for a large amount of jobs [30], with some researchers also highlighting a potential mismatch between these required sets of skills and the skills possessed by the workforce [12]. These issues lead to a phenomenon known as the "education-job mismatch", which can lead to negative effects on lifetime income [25]. Due in part to these factors, the modern employment landscape can be difficult to enter for recent graduates, with recent LinkedIn surveys showing that over a third of "entry-level" positions require multiple years of experience, and more than half of such positions requiring 3 years experience in certain fields or extremely specific skills [1]. As a result, it has been noted that entering the job market is an increasingly difficult task, now demanding considerable time and effort [15]. While computational advances are now commonly used to support education and to assist workers in their everyday work, there is a lack of similarly mature technological solutions to alleviate the issues presented by exiting education to enter the workplace. We believe that the rapid development of machine learning presents a powerful opportunity to help ease this transition. The case study at the core of this paper focuses on one of the important tasks to build towards this objective: Graduate Job Classification. Given a job posting containing its title and description, our aim is to be able to automatically identify whether or not the job is a position fit for a recent graduate or not, either because it requires considerable experience or because it doesn't require a higher education qualification. In light of the information presented above, as well as the sheer volume of job postings created every day, this classification offers an important curation. This would allow graduates to focus their efforts on relevant positions, rather than spending a considerable amount of time filtering through large volumes of jobs, which is non-trivial due to often obfuscated requirements. [1] As a point of reference, the number of total job postings in the United Kingdom alone in the July-September 2022 period exceeded 1.2 million[16]. This task contains a variety of challenges, the key one being the extreme importance of minimizing false negatives, as any false negative would remove a potentially suitable job from a job-seeker's consideration when the list is presented to them. On the other hand, with such large volumes of posting, too many false positives would lead to the curated list being too noisy to provide useful assistance. A second major challenge is the reliance of the task on subtle language understanding, as the signals of a job's suitability can be very weak. In this paper, we will evaluate a variety of text classification approaches applied to the English-language Graduate Job Classification task. In doing so, we will (i) show that the most recent Large Language Models (LLMs), based on Instruction-Tuned GPT-3 [3,18], can leverage the vast wealth of information acquired during their training to outperform state-of-the-art supervised classification approaches on this task and that (ii) proper prompt engineering has an enormous impact on LLM downstream performance on this task, contributing a real-world application to the very active research on the topic of prompt engineering [28,11].</p>
<p>Background</p>
<p>Since the introduction of the Transformer architecture [24] and the rise of transfer learning to leverage language models on downstream tasks [9,19], the field of NLP has undergone rapid changes. Large pre-trained models such as BERT [5] and later improvements, like DeBERTa [8], have resulted in significant performance improvements, surpassing prior word representation methods such as word vectors [14]. The development of libraries such as HuggingFace Transformers [29] has further contributed to making these models ubiquitous in NLP applications.</p>
<p>These advances resulted in a paradigm shift in NLP, focusing on the use or fine-tuning of extremely large, generalist, so-called "foundation models" rather than the training of task-specific models [2]. This resulted in the frequent occurrence of paradigm shift, where researchers focused on ways to reframe complex tasks into a format that could fit into tasks where such models are known to be strong, such as question-answering or text classification [23].</p>
<p>In parallel to these fine-tuning approaches, there has been considerable work spent on the development of generative Large Language Models (LLMs), whose training focuses on causal generation: the task of predicting the next token given a context [20]. The release of GPT-3 showed that these models, on top of their ability to generate believable text, are also few-shot learners: given few examples, they are capable of performing a variety of tasks, such as question answering [3].</p>
<p>Going further, very recent developments have shown that LLMs can reach noticeably better performance on downstream applications through instructiontuning: being fine-tuned to specifically follow natural language instructions to reach state-of-the-art performance on many language understanding tasks [18].</p>
<p>LLMs, being trained on billions of tokens, have been shown to be able to leverage the vast amount of knowledge found in their training data on various tasks, with performance increasing via both an increase in model and training data size, following complicated scaling laws [21]. This has paved the way for the appearance of a new approach to NLP applications, focusing on exploring ways to optimally use this large amassed knowledge: prompt engineering [11].</p>
<p>Prompt Engineering represents a new way of interacting with models, through natural language queries. It has gathered considerable research attention in the last year. Certain ways of prompting LLMs, such as Chain-of-Thought (CoT) prompting, have been shown to be able to prompt reasoning which considerably improves the models' downstream performance [27]. Additional research has showcased ways to bypass certain model weaknesses. Notably, while LLMs are prone to mathematical errors, they are able to generate executable Python code to compute the requested results through specific prompting [6].</p>
<p>Other efforts have showcased reasoning improvements by relying on a model self-verifying its own reasoning in a subsequent prompt, which improves performance [10]. All these approaches have shown that LLMs can match or outperform state-ofthe-art results on certain tasks, while requiring little to no fine-tuning. Data Our target task is Graduate Job Classification. It is a binary classification, where, given a job posting containing both the job title and its description, the model must identify whether or not the job is a position fit for a recent graduate or not, either because it requires more experience or doesn't require higher education. In practice, over 25,000 jobs are received on a daily basis, with fewer than 5% of those appropriate for graduates.</p>
<p>Experimental Setup</p>
<p>Data and Evaluation</p>
<p>Curating positions fit for recent graduates is extremely time-consuming and is therefore one of the areas where technological advances can help simplify the process of entering the workplace. In practice, over 20,000 jobs go through our deployed model on a daily basis, with fewer than 5% of those appropriate for graduates.</p>
<p>Our data is gathered from a large selection of UK-based jobs over a period of two years. These jobs were manually filtered into "Graduate" and "Non-Graduate" categories by human annotators working for Bright Network. All annotators work as part of a team dedicated to ensuring the quality of jobs and follow predefined sets of guidelines. Guidelines are frequently reviewed by domain experts, and feedback on annotation quality is gathered on a weekly basis. This is our silver dataset. Unlike our gold standard described below, sampled from it and iterated upon, this is a single-pass annotation process, and individual mistakes can occasionally be present.</p>
<p>The gold standard dataset used in this study is a subset of the original data, containing job postings whose original label was further reviewed manually. Only jobs where inter-annotator agreement was reached were kept, until reaching a data size of 10,000. We use the label GRAD for jobs suitable for graduates and NON GRAD for all other jobs.</p>
<p>Before being used as model input, all job descriptions are prepended by the posting's title. A general description of the data is presented in Table 1, including the distribution of labels and information about the token counts within documents. Overall, the median length of both GRAD and NON-GRAD jobs is similar, and the final dataset is made up of roughly 30% GRAD jobs and 70% NON-GRAD jobs.</p>
<p>Evaluation We use the Precision at 95% Recall (P@95%R) for the GRAD label as our main metric. This means that our primary method of evaluation is the Precision (the measure of how good the model is at avoiding false positives), obtained by the model while maintaining a Recall of at least 95%, which means the model detects at least 95% of positive examples. We chose this metric as the classifier cannot be deployed in production with a low recall, as it is extremely damaging to remove suitable jobs from graduates' consideration. Our goal is to ensure that Recall remains above a specific threshold while achieving the best possible precision at this threshold and help process the tens of thousands of jobs received daily. We also report the P@85%R, to give a better overview of the models' performance. To facilitate LLM evaluation, we split our data into stratified training and test sets, respectively containing 7000 (70%) and 3000 (30%) examples, rather than using cross-validation.</p>
<p>Baselines</p>
<p>Keyword We report the results for a simple, keyword and regular expression approaches to the task. We, along with our annotators, built a list of common phrases and regular expressions indicating that a job is suitable for a recent graduate. We then perform a simple look-up within the postings, which gives us a lower bound for performance. An example of such an approach would be matching the words "Graduate" and "Junior" in job titles, or looking for strings such as "is-would be suitable for graduate-student" within the posting itself.</p>
<p>SVM We present the results of a non-deep learning baseline method, which involves using a Support Vector Machine (SVM) classifier with a tf-idf text representation, which has been shown to produce robust baseline results, even reaching state-of-the-art results in some domain-specific tasks [4].</p>
<p>Supervised Classifiers</p>
<p>ULMFiT We report the results for ULMFiT, an RNN-based approach to training a small language model before fine-tuning it for classification [9]. We pre-train the ULMFiT language model on an unlabeled dataset of 50000 job postings, before fine-tuning the classifier on the data described above.</p>
<p>DeBERTa-V3 We fine-tune a DeBERTa-V3-Base model, a refined version of DeBERTa [8] and which achieves state-of-the-art performance on a variety of text classification tasks [7]. We follow the method used in the paper introducing the model, with a maximum sequence length of 512. For any longer document, we report results using the first 100 tokens and the trailing 412 tokens of the document. This approach yielding the best results is likely due to most job descriptions frequently outlining the position's requirements towards the end.</p>
<p>Large Language Models</p>
<p>We use a temperature of 0 for all language models. The temperature controls the degree of randomness applied to the tokens outputted by the language model. A temperature of 0 ensures the sampling favors the highest probability token in all cases, resulting in a deterministic output.</p>
<p>GPT-3.5 (text-davinci-002&amp;text-davinci-003) We report our results on two variants of GPT-3 [3] 3 . These models are LLMs further trained to improve their ability to follow natural language instructions [18]. Although the detailed differences between the two models are not made public, davinci-003 is a refinement of davinci-002, better at following instructions 4 .</p>
<p>GPT-3.5-turbo (gpt-3.5-turbo-0301) We evaluate GPT-3.5-turbo 5 , a model optimized for chat-like interactions [17]. To do so, we modified all our prompts to fit the conversation-like inputs expected by the model. GPT-3.5turbo is the focus of our prompt engineering exploration.</p>
<p>Overall Results</p>
<p>In Table 2, we report the P@95R% and P@85R% for all models evaluated. We report a score of 0 if the model is unable to reach the recall threshold. For all approaches for which we do not have a way to target a specific Recall value, we also provide their Recall metric. Overall, we notice that SVMs, as often, are a strong baseline, although they are outperformed by both supervised deep learning approaches. However, they are outperformed by both of our supervised approaches. DeBERTaV3 achieves the highest P@85%R of all the models, but is beaten by both davinci-003 and GPT-3.5 on the P@95%R metric, which is key to high-quality job curation.</p>
<p>We notice overall strong performance from the most powerful LLMs evaluated, although davinci-002 fails to reach our 95% Recall threshold and trails behind both ULMFiT and DeBERTaV3 at an 85% recall threshold. On the other hand, davinci-003 outperforms DeBERTaV3, while GPT-3.5 is by far the best-performing model on the P@95%R metric, with a 7.2 percentage point increase.</p>
<p>Overall, these results show that while our best-performing supervised approach obtains better metrics at lower recall thresholds, it falls noticeably behind LLMs when aiming for a very low false negative rate. Table 3. Overview of the various prompt modifications explored in this study.</p>
<p>LLMs &amp; Prompt Engineering</p>
<p>Short name Description Baseline</p>
<p>Provide a a job posting and asking if it is fit for a graduate. CoT</p>
<p>Give a few examples of accurate classification before querying. Zero-CoT Ask the model to reason step-by-step before providing its answer. rawinst</p>
<p>Give instructions about its role and the task by adding to the user msg. sysinst</p>
<p>Give instructions about its role and the task as a system msg. bothinst Split instructions with role as a system msg and task as a user msg. mock Give task instructions by mocking a discussion where it acknowledges them. reit</p>
<p>Reinforce key elements in the instructions by repeating them. strict</p>
<p>Ask the model to answer by strictly following a given template. loose</p>
<p>Ask for just the final answer to be given following a given template. right</p>
<p>Asking the model to reach the right conclusion. info</p>
<p>Provide additional information to address common reasoning failures. name</p>
<p>Give the model a name by which we refer to it in conversation. pos</p>
<p>Provide the model with positive feedback before querying it.</p>
<p>In this section, we will discuss the prompt engineering steps taken to reach the best-performing version of GPT-3.5. We will largely focus on its chat-like input, although similar steps were used for other language models, minus the conversational format. Apart from the use of system messages, we noticed no major differences in prompt impact between models.</p>
<p>For each modification, we will provide an explanation of the changes, or, where relevant, a snippet highlighting the modification. An overview of all prompt modifications used is presented in Table 3. We evaluate the impact of each change on the model's performance, but also on its ability to provide its answer in a specified format rather than as free text, which we call Template Stickiness.</p>
<p>Our approach to prompt engineering, as described in this section, follows the ChatML [13] prompting format used by OpenAI for their GPT family of models. To help readability, we do not directly reproduce the XML-like or JSON format used by ChatML but provide simple variable-like identifiers for our prompt modifications in the examples below.</p>
<p>Eliciting Reasoning</p>
<p>Zero-Shot Prompting We set our baseline by simply prompting the model with our question with no further attempt to induce reasoning ('Baseline'):</p>
<p>For the given job: {job_posting} ---------Is this job (A) a job fit for a recent graduate, or (B) a job requiring more professional experience. Answer:</p>
<p>Few-shot CoT We then experiment with few-shot chain-of-thought prompting [27], by providing the model with successful classification examples. We do so using the gpt-3.5 chat format, mocking a conversation between the user, and the assistant, who elaborates on his reasoning before answering with (A) or (B). We prepend our query by providing the model with two examples 6 ('CoT'). We do so in the folloiwng format: user message 1 = """For the given job: Zero-shot CoT We then attempt to elicit reasoning without providing the model any example, through Zero-shot Chain-of-Thought [10] ('Zero-CoT'). We expect that this approach will perform well, as job postings are found in large quantity in data used to train the model, and identifying whether a job is fit for a graduate does not require expert domain knowledge. We attempt to elicit reasoning by prompting the model think step-by-step, as follows:</p>
<p>For the given job: {job_posting} ---------Is this job (A) a job fit for a recent graduate, or (B) a job requiring more professional experience. Answer: Let's think step by step,</p>
<p>Initial Instructions</p>
<p>We then explore the impact of providing the model with instructions describing both its role and task. A notable difference between textscdavinci-003 and the gpt-3.5 chat format is that the latter introduces a new aspect to prompt engineering, which was not found in previous ways to interact with language models: the ability to provide a system message to the system. We explore multiple ways of providing instructions using this system message.</p>
<p>Giving Instructions We provide information to the model about its role role as well as a description of its task: role = """You are an AI expert in career advice. You are tasked with sorting through jobs by analysing their content and deciding whether they would be a good fit for a recent graduate or not.""" ֒→ ֒→ ֒→ task = """A job is fit for a graduate if it's a junior-level position that does not require extensive prior professional experience. I will give you a job posting and you will analyse it, to know whether or not it describes a position fit for a graduate.""" ֒→ ֒→ ֒→ ֒→ Instructions as a user or system message There is no clear optimal way to use the system prompt, as opposed to passing instructions as a user query. The 'rawinst' approach, explained above, passes the whole instructions to the model as a user query. We evaluate the impact of passing the whole instructions as a system query ('sysinst'), as well as splitting them in two, with the model's role definition passed as a system query and the task as a user query (bothinst).</p>
<p>Mocked-exchange instructions We attempt to further take advantage of the LLM's fine-tuned ability to follow a conversational format by breaking down our instructions further ('mock'). We iterate on the bothinst instruction format, by adding an extra confirmation message from the model: user_message_1 = """A job is fit for a graduate [...] Got it?""" assistant message 1 = "Yes, I understand. I am ready to analyse your job posting."</p>
<p>֒→</p>
<p>Re-iterating instructions</p>
<p>We further modify the instructions by introducing a practice commonly informally discussed but with little basis: re-iterating certain instructions ('reit'). In our case, this is done by appending a reminder to the system message, to reinforce the perceived expertise of the model as well as the importance of thinking step-by-step in the task description: system prompt ="""You are an AI expert in career advice. You are tasked with sorting through jobs by analysing their content and deciding whether they would be a good fit for a recent graduate or not. Remember, you're the best AI careers expert and will use your expertise to provide the best possible analysis""" ֒→ ֒→ ֒→ ֒→ ֒→ user message 1 = """[...] I will give you a job posting and you will analyse it, step-by-step, to know whether [...]""" ֒→</p>
<p>Wording the prompt</p>
<p>Answer template We experiment with asking the model to answer by following specific templates, either requiring that the final answer ('loose'), or the full reasoning ('strict') must adhere to a specific template. We experiment with different wordings for the template, with the best-performing ones as follows:
loose = """[..
.]Your answer must end with: Final Answer: This is a (A) job fit for a recent graduate or a student OR (B) a job requiring more professional experience. Answer: Let's think step-by-step,""" strict = """[...]You will answer following this template: Reasoning step 1:\nReasoning step 2:\nReasoning step 3:\n Final Answer: This is a (A) job fit for a recent graduate or a student OR (B) a job requiring more professional experience. Answer: Reasoning Step 1:"""</p>
<p>The right conclusion We evaluate another small modification to the prompt to provide further positive re-inforcement to the model: we ask it reason in order to reach the right conclusion, by slightly modifying our final query:</p>
<p>Answer: Let's think step-by-step to reach the right conclusion, Addressing reasoning gaps While analysing our early results, we noticed that the model can misinterpret instructions given to it, and produce flawed reasoning as a result. This manifested in attempts to over-generalise:</p>
<p>This job requires experience, but states that it can have been acquired through internships. However, not all graduates will have undergone internships. Therefore, (B) this job is not fit for all graduates.</p>
<p>We attempt to alleviate this by providing additional information in the model's instruction: task = "A job is fit for a graduate if it's a junior-level position that does not require extensive prior professional experience. When analysing the experience required, take into account that requiring internships is still fit for a graduate. I will give you a job [...] ֒→ ֒→ ֒→ ֒→</p>
<p>The importance of subtle tweaks</p>
<p>Naming the Assistant A somewhat common practice, as shown by Microsoft code-naming its Bing chatbot "Sydney" 7 , is to give LLMs a nickname by which 7 As demonstrated by the widely circulated prompt https://simonwillison.net/2023/Feb/15/bing/ they can be referred to. We modified our initial system prompt, as well as the user mocked-instructions, to refer to our model as Frederick ('name'), as follows: system prompt = "You are Frederick, an AI expert in career advice. [...]" [...] first assistant response = "Yes, I understand. I am Frederick, and I will analyse your job posting."</p>
<p>We tested multiple other names, chosen randomly from a list of common first-names in English-speaking countries. We noticed no significant variation in performance no matter the name given, and all resulted in a similar imporvement.</p>
<p>Positive Feedback It has been anecdotally noted that giving positive reinforcement to gpt-3.5 can lead to better performance on some tasks 8 . We thus prepend our main prompt with a positive reaction to the model's mocked acknowledgement of our instructions ('pos'):</p>
<p>Great! Let's begin then :) For the given job: [...] 5.5 Best-Performing Final Prompt system = "You are Frederick, an AI expert in career advice. You are tasked with sorting through jobs by analysing their content and deciding whether they would be a good fit for a recent graduate or not.", user 1 = """A job is fit for a graduate if it's a junior-level position that does not require extensive prior professional experience. When analysing the experience required, take into account that requiring internships is still fit for a graduate. I will give you a job posting and you will analyse it, step-by-step, to know whether or not it describes a position fit for a graduate. Got it?""" assistant 1 = "Yes, I understand. I am Frederick, and I will analyse your job posting.", user 2 = """Great! Let's begin then :) For the given job: {job_posting} ---------Is this job (A) a job fit for a recent graduate, or (B) a job requiring more professional experience. Answer: Let's think step by step to reach the right conclusion""" 6 Prompt Engineering Results and Discussion The evaluation metrics (calculated against the GRAD label), as well as Template Stickiness, for all the modifications detailed above are presented in Table 4. We provide these metrics rather than the more task-appropriate P@95%R used above to make it easier to compare the various impacts of prompt changes. Any modification below 95% Recall is presented in italic. Template Stickiness refers to the percentage of outputs that fit a desired output format and contains the labels as defined in the prompt, meaning no further output parsing is necessary.</p>
<p>When multiple variants of a modification are evaluated, we either pick the best performing one or discard the modifications before applying the subsequent ones if there is no performance improvement.</p>
<p>We notice that the impact of prompt engineering on classification results is high. Simply asking the model to answer the question using its knowledge only reaches an F1-score of 65.6, with a Recall of 70.6, considerably short of our target, while our final prompt reaches an F1-score of 91.7 with 97% recall.</p>
<p>Interestingly, few-shot CoT prompting the model with examples performs noticeably worse than a zero-shot approach. We speculate that this is due to the examples biasing the model reasoning too much while the knowledge it already contains is sufficient for most classifications. Any attempt at providing more thorough reasoning for either label resulted in increased recall and decreased precision for the label. Despite multiple attempts, we found no scenario where providing examples performed better than zero-shot classification.</p>
<p>Providing instructions to the model, with a role description as its system message and an initial user message describing the text, yielded the single biggest increase in performance (+5.9F1). Additionally, we highlight the impact of small changes to guide the model's reasoning. Mocking an acknowledgement of the instruction allows the model to hit the 95% Recall threshold (+1.3F1). Small additions, such as naming the model or providing it with positive reinforcement upon its acknowledgement of the instructions, also resulted in increased performance.</p>
<p>We found that gpt-3.5.turbo struggles with Template Stickiness, which we did not observe with text-davinci-003. Its answers often required additional parsing, as it would frequently discard the (A)/(B) answering format asked of it. Requesting that it follows either a strict reasoning template or a loose answer template yielded considerably higher template stickiness but resulted in performance decreases, no matter the template wording.</p>
<p>Overall, we find that these results highlight just how prompt-sensitive downstream results are, and we showcase a good overview of common techniques that can result in large performance improvements.</p>
<p>A limitation of this study is that we showcase the impact of prompt engineering and of various prompt modifications. However, we are unable to provide a fully reliable explanation as to why these modifications have such an impact. Large Language Models are trained on vast quantities of text to predict the next token, which then results in a quantity of emergent abilities [26], which can be elicited through specific prompting. While this prompting can intuitively make sense, there's a lack of theory as to how certain changes, such as adding a name, can generate noticeable improvements. This is an open area of research which we hope to contribute to in the future.</p>
<p>Conclusion</p>
<p>In this work, we have presented the task of Graduate Job Classification, highlighting its importance. We have then evaluated a series of classifiers on a realworld dataset, attempting to find which approach allows for the best filtering of non-graduate jobs while still meeting a sufficiently high recall threshold to not remove a large amount of legitimate graduate jobs in our curation efforts. In doing so, we showcased that the best-performing approach on this task is the use of Large Language Models (LLMs), in particular OpenAI's gpt-3.5-turbo.</p>
<p>Using language models for downstream tasks requires a different paradigm, where time is not spent on fine-tuning the model itself but on improving the prompt, a natural language query. We present our evaluation of various prompt modifications and demonstrate the large improvement in performance that can be obtained by proper prompt engineering to allow the language model to leverage its vast amounts of amassed knowledge. We believe our work, presenting a real-world case study of the strong performance of LLMs on text classification tasks, provides good insight into prompt engineering and the specific prompttuning necessary to accomplish certain tasks. We provide our full results, and the resulting prompt is currently being used to filter thousands of jobs on a daily basis, to help support future applications in this area. We provide our full results, the resulting prompt being currently used to filter thousands of jobs on a daily basis, to help support future applications in this area.</p>
<p>{job posting} ---------Is this job (A) a job fit for a recent graduate, or (B) a job requiring more professional experience.""" ֒→ assistant message 1 = "This job appears to be a senior position, as it mentions requiring experience interacting with C-level stakeholder in intense environments and [...]. Therefore, this is (B) a job requiring more professional experience ֒→ ֒→ ֒→ user message 2 = [...]</p>
<p>Table 1 .
1High-level description of the data used to train and evaluate models.Example # Proportion 
Median 
Token # </p>
<p>Token # 
standard dev. 
GRAD 
3082 
30.8% 
809 
338 
NON GRAD 
6918 
69.2% 
831 
434 
Full Dataset 
10000 
100% 
821 
389 </p>
<p>Table 2 .
2Results for all evaluated models.Keyword SVM ULMFiT DeBERTaV3 davinci-002 davinci-003 gpt-3.5 
P@95%R 
0 
63.1 
70.2 
79.7 
0 
80.4 
86.9 
P@85%R 
0 
75.4 
83.2 
89.0 
72.6 
80.4 
86.9 
Recall 
80.2 
N/A N/A 
N/A 
72.2 
95.6 
97 </p>
<p>Table 4 .
4Impact of the various prompt modifications.Precision Recall F1 Template Stickiness </p>
<p>These models are accessed through OpenAI's API.4 Introduced by OpenAI in a blog post rather than a technical report:https://help.openai.com/en/articles/6779149-how-do-text-davinci-002-and-textdavinci-003-differ 5 This model is also accessed through OpenAI's API.
Due to the long token length of job postings, providing it with more than two examples required us to truncate the postings, which resulted in a degradation in performance
As reported by OpenAI, a partnered developer found that positive reinforcement resulted in increased accuracy.</p>
<p>Hiring's new red line: why newcomers can't land 35% of "entry-level" jobs (2021). G Anders, LinkedIn Economic Graph ResearchAnders, G.: Hiring's new red line: why newcomers can't land 35% of "entry-level" jobs (2021), LinkedIn Economic Graph Research</p>
<p>R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, E Brynjolfsson, S Buch, D Card, R Castellon, N Chatterji, A Chen, K Creel, J Q Davis, D Demszky, C Donahue, M Doumbouya, E Durmus, S Ermon, J Etchemendy, K Ethayarajh, L Fei-Fei, C Finn, T Gale, L Gillespie, K Goel, N Goodman, S Grossman, N Guha, T Hashimoto, P Henderson, J Hewitt, D E Ho, J Hong, K Hsu, J Huang, T Icard, S Jain, D Jurafsky, P Kalluri, S Karamcheti, G Keeling, F Khani, O Khattab, P W Koh, M Krass, R Krishna, R Kuditipudi, A Kumar, F Ladhak, M Lee, T Lee, J Leskovec, I Levent, X L Li, X Li, T Ma, A Malik, C D Manning, S Mirchandani, E Mitchell, Z Munyikwa, S Nair, A Narayan, D Narayanan, B Newman, A Nie, J C Niebles, H Nilforoshan, J Nyarko, G Ogut, L Orr, I Papadimitriou, J S Park, C Piech, E Portelance, C Potts, A Raghunathan, R Reich, H Ren, F Rong, Y Roohani, C Ruiz, J Ryan, C Ré, D Sadigh, S Sagawa, K Santhanam, A Shih, K Srinivasan, A Tamkin, R Taori, A W Thomas, F Tramèr, R E Wang, W Wang, B Wu, J Wu, Y Wu, S M Xie, M Yasunaga, J You, M Zaharia, M Zhang, T Zhang, X Zhang, Y Zhang, L Zheng, K Zhou, P Liang, arXiv:2108.07258On the opportunities and risks of foundation models. arXiv preprintBommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J.Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Etha- yarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D.E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P.W., Krass, M., Krishna, R., Ku- ditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, X.L., Li, X., Ma, T., Malik, A., Manning, C.D., Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J.C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, I., Park, J.S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., Ré, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A.W., Tramèr, F., Wang, R.E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S.M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng, L., Zhou, K., Liang, P.: On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (8 2021)</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, NeurIPS. 2020Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Nee- lakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Win- ter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners. NeurIPS 2020 (2020)</p>
<p>The unreasonable effectiveness of the baseline: Discussing SVMs in legal text classification. B Clavié, M Alphonsus, JURIX. 2021Clavié, B., Alphonsus, M.: The unreasonable effectiveness of the baseline: Dis- cussing SVMs in legal text classification. JURIX 2021 (2021)</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, 1Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi- rectional transformers for language understanding. vol. 1 (2019)</p>
<p>L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, arXiv:2211.10435Pal: Program-aided language models. arXiv preprintGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., Neubig, G.: Pal: Program-aided language models. arXiv preprint arXiv:2211.10435 (2022)</p>
<p>Debertav3: Improving deberta using electrastyle pre-training with gradient-disentangled embedding sharing. P He, J Gao, W Chen, arXiv:2111.09543arXiv preprintHe, P., Gao, J., Chen, W.: Debertav3: Improving deberta using electra- style pre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543 (2021)</p>
<p>Deberta: Decoding-enhanced bert with disentangled attention. P He, X Liu, J Gao, W Chen, arXiv 2006.03654 cs.CLHe, P., Liu, X., Gao, J., Chen, W.: Deberta: Decoding-enhanced bert with disen- tangled attention. arXiv 2006.03654 cs.CL (2020)</p>
<p>Universal language model fine-tuning for text classification. J Howard, S Ruder, Proceedings of ACL 2018). ACL 2018)Howard, J., Ruder, S.: Universal language model fine-tuning for text classification. In: Proceedings of ACL 2018) (2018)</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, arXiv:2205.11916arXiv preprintKojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 (5 2022)</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, arXiv:2107.13586arXiv preprintLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586 (7 2021)</p>
<p>Skills mismatch: Concepts, measurement and policy approaches. S Mcguinness, K Pouliakas, P Redmond, Journal of Economic Surveys. 324McGuinness, S., Pouliakas, K., Redmond, P.: Skills mismatch: Concepts, measure- ment and policy approaches. Journal of Economic Surveys 32(4), 985-1015 (2018)</p>
<p>Microsoft: Learn how to work with the ChatGPT and GPT-4 models (preview. Microsoft: Learn how to work with the ChatGPT and GPT-4 models (preview) (2023)</p>
<p>Distributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, NeurIPSMikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed repre- sentations of words and phrases and their compositionality. NeurIPS 2013</p>
<p>Why inexperienced workers can't get entry-level jobs (2021), BBC 16. K Morgan, OpenAI: Openai api: Chat completions. Office for National Statistics: Vacancies and jobs in the ukMorgan, K.: Why inexperienced workers can't get entry-level jobs (2021), BBC 16. Office for National Statistics: Vacancies and jobs in the uk: October 2022 17. OpenAI: Openai api: Chat completions. https://platform.openai.com/docs/guides/chat (2023)</p>
<p>L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, arXiv:2203.02155Training language models to follow instructions with human feedback. arXiv preprintOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 (2022)</p>
<p>M E Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, Deep contextualized word representations. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettle- moyer, L.: Deep contextualized word representations (2018)</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 189Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)</p>
<p>J W Rae, S Borgeaud, T Cai, K Millican, J Hoffmann, F Song, J Aslanides, S Henderson, R Ring, S Young, arXiv:2112.11446Scaling language models: Methods, analysis &amp; insights from training gopher. arXiv preprintRae, J.W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al.: Scaling language models: Methods, analysis &amp; insights from training gopher. arXiv preprint arXiv:2112.11446 (2021)</p>
<p>Learning at scale. I Roll, D M Russell, D Gašević, International Journal of Artificial Intelligence in Education. 28Roll, I., Russell, D.M., Gašević, D.: Learning at scale. International Journal of Artificial Intelligence in Education 28 (2018)</p>
<p>Paradigm shift in natural language processing. T X Sun, X Y Liu, X P Qiu, X J Huang, Machine Intelligence Research. 193Sun, T.X., Liu, X.Y., Qiu, X.P., Huang, X.J.: Paradigm shift in natural language processing. Machine Intelligence Research 19(3), 169-183 (may 2022)</p>
<p>. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Lukasz Kaiser, I Polosukhin, Attention is all you need. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Lukasz Kaiser, Polosukhin, I.: Attention is all you need. vol. 2017-December (2017)</p>
<p>The effect of education-job mismatch on net income: evidence from a developing country. L Veselinović, J Mangafić, L Turulja, Economic research. 331Veselinović, L., Mangafić, J., Turulja, L.: The effect of education-job mismatch on net income: evidence from a developing country. Economic research 33(1) (2020)</p>
<p>Emergent abilities of large language models. J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, Transactions on Machine Learning Research. Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al.: Emergent abilities of large language models. Transactions on Machine Learning Research (2022)</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, arXiv:2201.11903arXiv preprintWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 (2022)</p>
<p>J White, Q Fu, S Hays, M Sandborn, C Olea, H Gilbert, A Elnashar, J Spencer-Smith, D C Schmidt, arXiv:2302.11382A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprintWhite, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A., Spencer-Smith, J., Schmidt, D.C.: A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint arXiv:2302.11382 (2023)</p>
<p>Transformers: State-of-the-art natural language processing. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, Proceedings of NeurIPS 2020: system demonstrations. NeurIPS 2020: system demonstrationsWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al.: Transformers: State-of-the-art natu- ral language processing. In: Proceedings of NeurIPS 2020: system demonstrations (2020)</p>
<p>The future of jobs report 2020. WEF ReportsWorld Economic Forum, V.: The future of jobs report 2020. WEF Reports (2020)</p>            </div>
        </div>

    </div>
</body>
</html>
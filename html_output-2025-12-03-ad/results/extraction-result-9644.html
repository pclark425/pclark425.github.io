<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9644 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9644</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9644</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-279070481</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.23789v1.pdf" target="_blank">Conversational Exploration of Literature Landscape with LitChat</a></p>
                <p><strong>Paper Abstract:</strong> We are living in an era of"big literature", where the volume of digital scientific publications is growing exponentially. While offering new opportunities, this also poses challenges for understanding literature landscapes, as traditional manual reviewing is no longer feasible. Recent large language models (LLMs) have shown strong capabilities for literature comprehension, yet they are incapable of offering"comprehensive, objective, open and transparent"views desired by systematic reviews due to their limited context windows and trust issues like hallucinations. Here we present LitChat, an end-to-end, interactive and conversational literature agent that augments LLM agents with data-driven discovery tools to facilitate literature exploration. LitChat automatically interprets user queries, retrieves relevant sources, constructs knowledge graphs, and employs diverse data-mining techniques to generate evidence-based insights addressing user needs. We illustrate the effectiveness of LitChat via a case study on AI4Health, highlighting its capacity to quickly navigate the users through large-scale literature landscape with data-based evidence that is otherwise infeasible with traditional means.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9644.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9644.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitChat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitChat (Conversational literature exploration agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end interactive system that combines two LLM agents (for query design and for explaining results) with data-driven discovery tools (BKG, embeddings, topic modeling, citation/author analysis) to explore and synthesize large-scale scientific literature and produce evidence-based, transparent summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two instances of an LLM (GPT-4o) are used: one to interactively design and refine boolean search queries with the user and orchestrate retrieval, and a second to select and orchestrate data-mining tools and to compose natural-language, template-based responses; LLMs are used for communication and explanation rather than ingesting the entire corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Literature metadata (title, authors, abstract, venue, citations, dates) retrieved via the Web of Science API; typically metadata-only due to copyright, with optional user-uploaded PDFs for full-text; abstracts are embedded for downstream analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>3789</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Example user query used in case study: "I want to know what healthcare tasks researchers have explored using LLMs for"; LLM crafts boolean search such as ("large language model*" OR "foundation model*" OR "large generative model*" OR LLM*), refined interactively (user requested explicit mention of ChatGPT/GPT-4 series).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Interactive pipeline: LLM agent 1 formulates/refines boolean search queries with in-context examples, retrieves matched publications via WoS API, stores metadata in SQLite; construct a heterogeneous Bibliographic Knowledge Graph (BKG) in Neo4j (papers, authors, venues, keywords, citations); generate embeddings for abstracts using voyage-3-large 4; LLM agent 2 examines user query, selects data-mining tools (topic modeling, citation/author network analyses, 'scientific discovery' pattern mining, RAG-based QA) from a library, runs these tools on the BKG, then fills natural-language templates with analytic results to produce evidence-grounded answers and visualizations (2D landscape, topic sizes, representative papers). The approach emphasizes token-efficiency by not prompting the LLM with the entire corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Bibliographic knowledge graph, topic clusters and summaries, 2D literature landscape visualizations, lists of representative papers/authors, narrative evidence-based answers (template-filled), and RAG-backed QA/summaries when full-text is available.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>With topic modeling I identified 49 major healthcare topics that studied LLMs, among which the most prominent ones include: - Named entity recognition (495 papers) - Medical image segmentation (388 papers) - Mental health consultation (289 papers) - Medical question-answering (222 papers) - Medical Education (198 papers).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Demonstration case study (AI4Health): retrieval of WoS metadata for the topic query, construction of BKG, application of topic modeling and visualization; no formal quantitative benchmark or human-expert blind evaluation reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Qualitative demo: retrieved ~3,789 papers and produced a topic decomposition (49 topics) and counts, 2D visualizations, and drill-down summaries; no quantitative metrics (precision/recall, human agreement) provided.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Scales to thousands of papers by decoupling analysis from raw LLM context windows (uses metadata and data-driven tools); produces more objective/transparent outputs via explicit BKG, topic sizes, and visualizations; token-efficient and interactive (query refinement); supports grounding via retrieval and embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Typically limited to metadata (full texts often unavailable due to copyright) unless user uploads PDFs; LLMs still introduce risk of hallucination when composing narratives; no formal evaluation reported; quality depends on retrieval/query quality, metadata completeness, and chosen data-mining parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No concrete failure-case experiments reported; paper notes general risks including hallucination by LLMs, infeasibility of feeding entire corpora into LLMs (leading to incompleteness if a system attempted that), and dependence on metadata rather than full text which can limit depth of synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Exploration of Literature Landscape with LitChat', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9644.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9644.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autosurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autosurvey</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed system that uses LLMs to automatically generate literature reviews by prompting LLMs to analyze corpora and produce survey-style outputs; mentioned as an example of LLM-based literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autosurvey</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as prompting LLMs to scrutinize (potentially large) text corpora before producing final outputs; specific model architecture or size not stated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Implied to operate on textual corpora of publications (full-text or large subsets), but the paper does not provide explicit corpus source/size details.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Automated literature review generation (general surveys); no single example query given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Direct prompting of LLMs to generate literature reviews from the corpus (the paper contrasts this full-corpus LLM-scrutiny approach with LitChat's token-efficient, tool-augmented approach).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automatically generated literature reviews / surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Promises automation of literature survey generation using LLMs' language capabilities (as suggested by the paper's mention).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper flags shortcomings: limited LLM context windows make it infeasible to comprehensively process thousands of papers; black-box hallucination risks reduce objectivity/transparency; consequently may fail COOT (comprehensive, objective, open, transparent) principles.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Described in general terms: inability to achieve comprehensive coverage on very large literature corpora and risk of producing hallucinated or non-transparent outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Exploration of Literature Landscape with LitChat', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9644.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9644.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that leverages LLMs together with retrieval-augmented generation (RAG) to answer questions about scientific publications; cited as related work in LLM-assisted literature tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines LLMs with retrieval (RAG) to ground generated answers in retrieved document content; specific LLM model and training details are not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Scientific publications accessed via a retrieval layer; paper does not state exact sources or corpus sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Question-answering about content of scientific publications (user queries about papers).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-augmented generation: retrieve relevant pieces of documents and use an LLM to generate grounded answers.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Question-answer pairs / document-grounded explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>RAG improves grounding relative to pure generation by incorporating retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper notes that systems like PaperQA2 still fail to meet COOT principles for large-scale SLRs because LLM context windows and hallucinations limit comprehensiveness and trustworthiness.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Noted generally: not feasible to comprehensively analyze thousands of papers end-to-end with LLM-only approaches; hallucination remains a concern.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Exploration of Literature Landscape with LitChat', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9644.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9644.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of methods that first retrieve relevant documents or passages and then condition an LLM on those retrieved contexts to produce answers, thereby improving grounding and reducing hallucinations relative to pure generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrievalaugmented generation for large language models: A survey.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline combining a retrieval component (search over a corpus) with an LLM for generation; cited survey (Gao, 2023) and used conceptually by PaperQA2 and supported within LitChat for QA/summarization tasks when appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Retrieved passages from the target literature collection (metadata/abstracts or full text where available); specifics depend on implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Document-grounded question answering and summarization queries.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieve relevant document snippets then prompt an LLM to synthesize answers conditioned on retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Grounded answers, summaries, extractive/abstractive responses.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Improves grounding of LLM outputs and mitigates some hallucination by conditioning on retrieved evidence; widely used for Q&A over scientific corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality depends on retrieval relevance and available context; still vulnerable to hallucination beyond retrieved evidence and does not by itself produce comprehensive COOT-style systematic reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not enumerated concretely in this paper beyond the general limitations above.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Exploration of Literature Landscape with LitChat', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9644.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9644.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Topic Modeling (BERTopic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Topic modeling (e.g., BERTopic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-mining technique used to cluster papers into latent themes and track temporal evolution; LitChat applies topic modeling to the BKG to produce topic clusters, sizes, representative papers, and 2D visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bertopic: Neural topic modeling with a class-based TF-IDF procedure</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applies a topic-modeling algorithm (reference to BERTopic) on embeddings (voyage-3-large 4 embeddings for abstracts) derived from the retrieved metadata/BKG to identify latent topics and cluster papers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Abstracts and metadata of the WoS-retrieved AI4Health corpus stored in the BKG (case study: ~3,789 papers).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>3789</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Identify healthcare research tasks explored using LLMs and cluster retrieved papers into topics (e.g., Named entity recognition, medical image segmentation, mental health consultation).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Embed paper abstracts, perform clustering/topic modeling to produce a set of topics with sizes and representative documents; produce 2D visualizations for landscape overview and enable drilling down to topic summaries and representative papers.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Topic clusters, counts (topic sizes), representative paper lists, topic summaries, 2D visualizations of landscape.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Identified 49 major healthcare topics; example prominent topic counts: Named entity recognition (495 papers), Medical image segmentation (388), Mental health consultation (289), Medical question-answering (222), Medical Education (198).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>No formal intrinsic topic-modeling evaluation reported (e.g., coherence metrics); evaluation is presented qualitatively via the case study and by reporting topic sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported identification of 49 topics and the counts above; no quantitative evaluation metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Scales to thousands of metadata records, provides objective counts and interpretable topic groupings to support COOT principles, and supports interactive drill-down.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality and interpretability depend on embedding quality and topic-modeling hyperparameters; topic labels and representativeness may be imperfect; no formal validation reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not specifically enumerated beyond general dependence on modeling choices leading to potentially noisy or ambiguous topic labels/assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Exploration of Literature Landscape with LitChat', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autosurvey <em>(Rating: 2)</em></li>
                <li>PaperQA2 <em>(Rating: 2)</em></li>
                <li>Retrievalaugmented generation for large language models: A survey. <em>(Rating: 2)</em></li>
                <li>Survey of hallucination in natural language generation. <em>(Rating: 1)</em></li>
                <li>Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network. <em>(Rating: 1)</em></li>
                <li>Can chatgpt write a good boolean query for systematic review literature search? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9644",
    "paper_id": "paper-279070481",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "LitChat",
            "name_full": "LitChat (Conversational literature exploration agent)",
            "brief_description": "An end-to-end interactive system that combines two LLM agents (for query design and for explaining results) with data-driven discovery tools (BKG, embeddings, topic modeling, citation/author analysis) to explore and synthesize large-scale scientific literature and produce evidence-based, transparent summaries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Two instances of an LLM (GPT-4o) are used: one to interactively design and refine boolean search queries with the user and orchestrate retrieval, and a second to select and orchestrate data-mining tools and to compose natural-language, template-based responses; LLMs are used for communication and explanation rather than ingesting the entire corpus.",
            "model_size": null,
            "input_corpus_description": "Literature metadata (title, authors, abstract, venue, citations, dates) retrieved via the Web of Science API; typically metadata-only due to copyright, with optional user-uploaded PDFs for full-text; abstracts are embedded for downstream analysis.",
            "input_corpus_size": 3789,
            "topic_query_description": "Example user query used in case study: \"I want to know what healthcare tasks researchers have explored using LLMs for\"; LLM crafts boolean search such as (\"large language model*\" OR \"foundation model*\" OR \"large generative model*\" OR LLM*), refined interactively (user requested explicit mention of ChatGPT/GPT-4 series).",
            "distillation_method": "Interactive pipeline: LLM agent 1 formulates/refines boolean search queries with in-context examples, retrieves matched publications via WoS API, stores metadata in SQLite; construct a heterogeneous Bibliographic Knowledge Graph (BKG) in Neo4j (papers, authors, venues, keywords, citations); generate embeddings for abstracts using voyage-3-large 4; LLM agent 2 examines user query, selects data-mining tools (topic modeling, citation/author network analyses, 'scientific discovery' pattern mining, RAG-based QA) from a library, runs these tools on the BKG, then fills natural-language templates with analytic results to produce evidence-grounded answers and visualizations (2D landscape, topic sizes, representative papers). The approach emphasizes token-efficiency by not prompting the LLM with the entire corpus.",
            "output_type": "Bibliographic knowledge graph, topic clusters and summaries, 2D literature landscape visualizations, lists of representative papers/authors, narrative evidence-based answers (template-filled), and RAG-backed QA/summaries when full-text is available.",
            "output_example": "With topic modeling I identified 49 major healthcare topics that studied LLMs, among which the most prominent ones include: - Named entity recognition (495 papers) - Medical image segmentation (388 papers) - Mental health consultation (289 papers) - Medical question-answering (222 papers) - Medical Education (198 papers).",
            "evaluation_method": "Demonstration case study (AI4Health): retrieval of WoS metadata for the topic query, construction of BKG, application of topic modeling and visualization; no formal quantitative benchmark or human-expert blind evaluation reported in this paper.",
            "evaluation_results": "Qualitative demo: retrieved ~3,789 papers and produced a topic decomposition (49 topics) and counts, 2D visualizations, and drill-down summaries; no quantitative metrics (precision/recall, human agreement) provided.",
            "strengths": "Scales to thousands of papers by decoupling analysis from raw LLM context windows (uses metadata and data-driven tools); produces more objective/transparent outputs via explicit BKG, topic sizes, and visualizations; token-efficient and interactive (query refinement); supports grounding via retrieval and embeddings.",
            "limitations": "Typically limited to metadata (full texts often unavailable due to copyright) unless user uploads PDFs; LLMs still introduce risk of hallucination when composing narratives; no formal evaluation reported; quality depends on retrieval/query quality, metadata completeness, and chosen data-mining parameters.",
            "failure_cases": "No concrete failure-case experiments reported; paper notes general risks including hallucination by LLMs, infeasibility of feeding entire corpora into LLMs (leading to incompleteness if a system attempted that), and dependence on metadata rather than full text which can limit depth of synthesis.",
            "uuid": "e9644.0",
            "source_info": {
                "paper_title": "Conversational Exploration of Literature Landscape with LitChat",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Autosurvey",
            "name_full": "Autosurvey",
            "brief_description": "A recently proposed system that uses LLMs to automatically generate literature reviews by prompting LLMs to analyze corpora and produce survey-style outputs; mentioned as an example of LLM-based literature synthesis.",
            "citation_title": "Autosurvey",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "Described as prompting LLMs to scrutinize (potentially large) text corpora before producing final outputs; specific model architecture or size not stated in this paper.",
            "model_size": null,
            "input_corpus_description": "Implied to operate on textual corpora of publications (full-text or large subsets), but the paper does not provide explicit corpus source/size details.",
            "input_corpus_size": null,
            "topic_query_description": "Automated literature review generation (general surveys); no single example query given in this paper.",
            "distillation_method": "Direct prompting of LLMs to generate literature reviews from the corpus (the paper contrasts this full-corpus LLM-scrutiny approach with LitChat's token-efficient, tool-augmented approach).",
            "output_type": "Automatically generated literature reviews / surveys.",
            "output_example": "",
            "evaluation_method": "",
            "evaluation_results": "",
            "strengths": "Promises automation of literature survey generation using LLMs' language capabilities (as suggested by the paper's mention).",
            "limitations": "Paper flags shortcomings: limited LLM context windows make it infeasible to comprehensively process thousands of papers; black-box hallucination risks reduce objectivity/transparency; consequently may fail COOT (comprehensive, objective, open, transparent) principles.",
            "failure_cases": "Described in general terms: inability to achieve comprehensive coverage on very large literature corpora and risk of producing hallucinated or non-transparent outputs.",
            "uuid": "e9644.1",
            "source_info": {
                "paper_title": "Conversational Exploration of Literature Landscape with LitChat",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "PaperQA2",
            "name_full": "PaperQA2",
            "brief_description": "A system that leverages LLMs together with retrieval-augmented generation (RAG) to answer questions about scientific publications; cited as related work in LLM-assisted literature tools.",
            "citation_title": "PaperQA2",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "Combines LLMs with retrieval (RAG) to ground generated answers in retrieved document content; specific LLM model and training details are not given in this paper.",
            "model_size": null,
            "input_corpus_description": "Scientific publications accessed via a retrieval layer; paper does not state exact sources or corpus sizes.",
            "input_corpus_size": null,
            "topic_query_description": "Question-answering about content of scientific publications (user queries about papers).",
            "distillation_method": "Retrieval-augmented generation: retrieve relevant pieces of documents and use an LLM to generate grounded answers.",
            "output_type": "Question-answer pairs / document-grounded explanations.",
            "output_example": "",
            "evaluation_method": "",
            "evaluation_results": "",
            "strengths": "RAG improves grounding relative to pure generation by incorporating retrieved evidence.",
            "limitations": "Paper notes that systems like PaperQA2 still fail to meet COOT principles for large-scale SLRs because LLM context windows and hallucinations limit comprehensiveness and trustworthiness.",
            "failure_cases": "Noted generally: not feasible to comprehensively analyze thousands of papers end-to-end with LLM-only approaches; hallucination remains a concern.",
            "uuid": "e9644.2",
            "source_info": {
                "paper_title": "Conversational Exploration of Literature Landscape with LitChat",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A class of methods that first retrieve relevant documents or passages and then condition an LLM on those retrieved contexts to produce answers, thereby improving grounding and reducing hallucinations relative to pure generation.",
            "citation_title": "Retrievalaugmented generation for large language models: A survey.",
            "mention_or_use": "use",
            "model_name": "",
            "model_description": "Pipeline combining a retrieval component (search over a corpus) with an LLM for generation; cited survey (Gao, 2023) and used conceptually by PaperQA2 and supported within LitChat for QA/summarization tasks when appropriate.",
            "model_size": null,
            "input_corpus_description": "Retrieved passages from the target literature collection (metadata/abstracts or full text where available); specifics depend on implementation.",
            "input_corpus_size": null,
            "topic_query_description": "Document-grounded question answering and summarization queries.",
            "distillation_method": "Retrieve relevant document snippets then prompt an LLM to synthesize answers conditioned on retrieved evidence.",
            "output_type": "Grounded answers, summaries, extractive/abstractive responses.",
            "output_example": "",
            "evaluation_method": "",
            "evaluation_results": "",
            "strengths": "Improves grounding of LLM outputs and mitigates some hallucination by conditioning on retrieved evidence; widely used for Q&A over scientific corpora.",
            "limitations": "Quality depends on retrieval relevance and available context; still vulnerable to hallucination beyond retrieved evidence and does not by itself produce comprehensive COOT-style systematic reviews.",
            "failure_cases": "Not enumerated concretely in this paper beyond the general limitations above.",
            "uuid": "e9644.3",
            "source_info": {
                "paper_title": "Conversational Exploration of Literature Landscape with LitChat",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Topic Modeling (BERTopic)",
            "name_full": "Topic modeling (e.g., BERTopic)",
            "brief_description": "A data-mining technique used to cluster papers into latent themes and track temporal evolution; LitChat applies topic modeling to the BKG to produce topic clusters, sizes, representative papers, and 2D visualizations.",
            "citation_title": "Bertopic: Neural topic modeling with a class-based TF-IDF procedure",
            "mention_or_use": "use",
            "model_name": "",
            "model_description": "Applies a topic-modeling algorithm (reference to BERTopic) on embeddings (voyage-3-large 4 embeddings for abstracts) derived from the retrieved metadata/BKG to identify latent topics and cluster papers.",
            "model_size": null,
            "input_corpus_description": "Abstracts and metadata of the WoS-retrieved AI4Health corpus stored in the BKG (case study: ~3,789 papers).",
            "input_corpus_size": 3789,
            "topic_query_description": "Identify healthcare research tasks explored using LLMs and cluster retrieved papers into topics (e.g., Named entity recognition, medical image segmentation, mental health consultation).",
            "distillation_method": "Embed paper abstracts, perform clustering/topic modeling to produce a set of topics with sizes and representative documents; produce 2D visualizations for landscape overview and enable drilling down to topic summaries and representative papers.",
            "output_type": "Topic clusters, counts (topic sizes), representative paper lists, topic summaries, 2D visualizations of landscape.",
            "output_example": "Identified 49 major healthcare topics; example prominent topic counts: Named entity recognition (495 papers), Medical image segmentation (388), Mental health consultation (289), Medical question-answering (222), Medical Education (198).",
            "evaluation_method": "No formal intrinsic topic-modeling evaluation reported (e.g., coherence metrics); evaluation is presented qualitatively via the case study and by reporting topic sizes.",
            "evaluation_results": "Reported identification of 49 topics and the counts above; no quantitative evaluation metrics reported.",
            "strengths": "Scales to thousands of metadata records, provides objective counts and interpretable topic groupings to support COOT principles, and supports interactive drill-down.",
            "limitations": "Quality and interpretability depend on embedding quality and topic-modeling hyperparameters; topic labels and representativeness may be imperfect; no formal validation reported in the paper.",
            "failure_cases": "Not specifically enumerated beyond general dependence on modeling choices leading to potentially noisy or ambiguous topic labels/assignments.",
            "uuid": "e9644.4",
            "source_info": {
                "paper_title": "Conversational Exploration of Literature Landscape with LitChat",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autosurvey",
            "rating": 2,
            "sanitized_title": "autosurvey"
        },
        {
            "paper_title": "PaperQA2",
            "rating": 2
        },
        {
            "paper_title": "Retrievalaugmented generation for large language models: A survey.",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_large_language_models_a_survey"
        },
        {
            "paper_title": "Survey of hallucination in natural language generation.",
            "rating": 1,
            "sanitized_title": "survey_of_hallucination_in_natural_language_generation"
        },
        {
            "paper_title": "Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network.",
            "rating": 1,
            "sanitized_title": "forecasting_the_future_of_artificial_intelligence_with_machine_learningbased_link_prediction_in_an_exponentially_growing_knowledge_network"
        },
        {
            "paper_title": "Can chatgpt write a good boolean query for systematic review literature search?",
            "rating": 1,
            "sanitized_title": "can_chatgpt_write_a_good_boolean_query_for_systematic_review_literature_search"
        }
    ],
    "cost": 0.01448975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Conversational Exploration of Literature Landscape with LitChat
25 May 2025</p>
<p>Mingyu Huang 
College of Computer Science and Engineering
University of Electronic Science and Technology of China
Xiyuan Avenue 2006ChengduChina</p>
<p>Shasha Zhou 
Department of Computer Science
University of Exeter
EX4 4QFExeterUnited Kingdom</p>
<p>Yuxuan Chen 
James Watt School of Engineering
University of Glasgow
G12 8QQGlasgowUnited Kindom</p>
<p>Ke Li k.li@exeter.ac.uk 
College of Computer Science and Engineering
University of Electronic Science and Technology of China
Xiyuan Avenue 2006ChengduChina</p>
<p>Conversational Exploration of Literature Landscape with LitChat
25 May 2025768D3AFEF3174BB540E05A20CFDAC313arXiv:2505.23789v1[cs.CL]
We are living in an era of "big literature", where the volume of digital scientific publications is growing exponentially.While offering new opportunities, this also poses challenges for understanding literature landscapes, as traditional manual reviewing is no longer feasible.Recent large language models (LLMs) have shown strong capabilities for literature comprehension, yet they are incapable of offering "comprehensive, objective, open and transparent" views desired by systematic reviews due to their limited context windows and trust issues like hallucinations.Here we present LitChat, an end-toend, interactive and conversational literature agent that augments LLM agents with data-driven discovery tools to facilitate literature exploration.LitChat automatically interprets user queries, retrieves relevant sources, constructs knowledge graphs, and employs diverse data-mining techniques to generate evidence-based insights addressing user needs.We illustrate the effectiveness of LitChat via a case study on AI4Health, highlighting its capacity to quickly navigate the users through large-scale literature landscape with data-based evidence that is otherwise infeasible with traditional means.</p>
<p>Introduction</p>
<p>The past decade has witnessed an unprecedented expansion of scientific literature on many topics.This deluge of digital data on publications offers unprecedented opportunities for scholars and practitioners to explore patterns characterizing the structure and evolution of the underlying literature landscapes while simultaneously confronting them with new challenges.Scholars often develop systematic literature reviews (SLRs) [Kitchenham et al., 2009] to obtain comprehensive overviews of the relevant topics by manually reviewing a set of relevant publications.However, the exponentially increasing volume of literature means that the provision of "comprehensive, objective, open and transparent" (COOT) principles is no longer feasible by traditional means.For example, the number of publications on the applications of artificial intelligence in healthcare has reached 100,000 in 2023, which is far beyond the capacity of human experts to review.</p>
<p>Large language models (LLMs) [OpenAI, 2023] have now emerged as transformative tools that promise to revolutionize scientific literature exploration and understanding.Autosurvey [Wang et al., 2024] is a recent example of such a system that uses LLMs to automatically generate literature reviews.Another system, PaperQA2 [Skarlinski et al., 2024], leverages LLMs and retrieval-augmented generation (RAG) [Gao et al., 2023] to answer questions about scientific publications.While they have shown some promise, they also fail to provide COOT views of literature landscapes.For example, due to the limited context windows of LLMs, it is infeasible to feed the entire literature corpus, which may contain thousands of papers, into the LLMs.This is in contrast to the "comprehensive" principle of SLR.Similarly, as probablistic models, LLMs function as black boxes and are prone to hallucinations [Ji et al., 2023], which may lead to incorrect or misleading information that violates the "objective" and "transparent" principles.</p>
<p>Long before the emergence of LLMs, scholars have explored the use of data-driven methods to generate insights that can guide literature exploration.For example, Schmallenbach et al. leverages topic modeling to explore latent themes in large corpora of literature on the use of artificial intelligence in life science research.Jeong et al. uses community detection techniques to explore key research topics in authentication.Our recent work [Huang and Li, 2024] combines these with author collaboration network analysis to provide a holistic view on the research landscape of multi-objective optimization research.These methods could provide quantitative evidence to facilitate SLR under the COOT principles.</p>
<p>Inspired by this, here we present LitChat, an end-to-end, interactive and conversational literature agent that combines LLM agents and data-driven discovery tools to facilitate literature exploration.LitChat can automatically interpret user queries, design search queries, retrieves relevant sources from online databases, constructs bibliographic knowledge graphs, and employs diverse data-mining techniques to deliver evidence-based insights addressing user needs.It is able to perform explorations of different granularities, from providing an overview of the literature landscape to drilling down into the details of individual papers or researchers.We demonstrate the effectiveness of LitChat through a case study on AI4Health, highlighting its capacity to guide users through a timely, large-scale and rapidly evolving research landscape.</p>
<p>LitChat</p>
<p>A schematic overview of LitChat is depicted in Figure 1 on page 2. Overall, LitChat consists of two LLM agents, both of which can interact with users in a conversational manner.The first agent (upper) takes user queries and devises boolean search queries to retrieve relevant publications from online databases.The collected papers along with metadata are then used to construct a bibliographic knowledge graph (BKG).The second agent (lower) then selects appropriate data-mining techniques to explore the obtained BKG and generate responses to user queries based on the analysis results.At a higher level, the whole system is connected to a frontend built using JavaScript and React, which allows users to interact through a web interface.This frontend is connected to a backend using a Python Flask web app.The backend holds the LLM agents, the BKG (via Neo4j graph database), the paper metadata (via SQLite databases), and the data mining library.Below we delineate each component of LitChat.Note on the choice of LLMs: Unlike other LLM-based literature agents like Autosurvey and GraphRAG, which prompt LLMs to scrutinize the entire text corpus before reaching the final outputs, LitChat only uses LLMs to directly interact with the users for essential communications like discussing the query, explaining the results, and answering questions.This makes LitChat significantly more "token-efficient" and thus more affordable for broader applications.We thereby choose GPT-4o to enhance performance, though any other LLMs can also be used.</p>
<p>Query Design and Data Acquisition</p>
<p>Interactive refinement.The initial query will then be sent to the user for confirmation.The user can either approve it or provide feedback to refine, e.g., by adding or removing keywords.In the latter case, the LLM agent will update the query accordingly and ask for confirmation again.This process continues until the user is satisfied with the query.Database search and metadata.Once the query is confirmed, the LLM agent forwards it to an online database via API calls to retrieve relevant publications.Here we use the WoS API for this purpose, which is one of the most comprehensive literature databases and a common choice for SLR.The WoS API returns the matched publications in JSON format, which consists of rich curated metadata such as abstract, authors, venues, publication date, citations.The agent stores this information in a SQLite database for further processing.</p>
<p>Note on full paper data: Due to copyright issues, LitChat typically only retrieves paper metadata from online databases, unless the full paper is open access.Yet, a user is able to upload PDF documents to LitChat for further analysis.</p>
<p>Bibliographic Knowledge Graph Construction</p>
<p>Once the paper metadata is collected, LitChat parses it to construct a BKG, which essentially contains all the information regarding the underlying literature landscape.Notably, this graph is heterogeneous, as it consists of different types of entities, e.g., papers, authors, venues, keywords, institutions, etc., and complex relationships among them (e.g., citation, co-authorship, keyword co-occurrence, etc.).Paper abstracts are also integrated as part of node attributes, and we generate embeddings for them using the voyage-3-large 4 model, a top performer on the MTEB leaderboard.The final BKG provides the basis for all the subsequent data mining and response generation tasks.Upon construction, the BKG will be stored in a Neo4j graph database to allow for efficient querying and analysis.</p>
<p>Data Mining and Response Generation</p>
<p>After obtaining the BKG, the second LLM agent, also based on GPT-4o, would take another look at the user query and select one or more data mining approaches from a predefined library that can be helpful for answering the query.While an exhaustive list of data mining techniques is not feasible here, some important ones include:</p>
<p> Topics and keywords: Applying topic modeling [Grootendorst, 2022] to uncover latent research themes and their temporal evolution in the BKG;    Scientific discovery: Learning hidden patterns embedded in the BKG to infer meaningful research ideas that transcend existing individual knowledge and crossdomain boundaries [Krenn et al., 2023;Bai et al., 2024].After LitChat executes a tool, it composes the results of the operations into a natural language response that it returns to the user.LitChat generates these responses by filling in templates associated with each operation based on the results.The responses also include sufficient context to understand the results and opportunities for following up.In addition to these advanced data-driven tools, LitChat also supports basic tasks like question-answering and summarization based on RAG techniques just as many other literature chatbots.</p>
<p>Case Study on AI4Health</p>
<p>To demonstrate the effectiveness of LitChat, here we present a case study on the AI4Health domain.Specifically, as shown in Figure 2 on page 3, the user first asks "I want to know what healthcare tasks researchers have explored using LLMs for?"As one can imagine, answering this requires a comprehensive audit of the relevant literature, which is beyond the capacity of human experts.To address this, LitChat first creates a candidate search query and engages with the user to refine it to allow for a comprehensive coverage of relevant literature.The final query is then sent to the Web of Science database, from which 3,700+ papers are retrieved and synthesized into a BKG.Given the large volume of papers, traditional LLMbased systems would struggle to process them all, and this is where LitChat's data-driven tools come into play.</p>
<p>Since the user is interested in an overview of certain research topics, LitChat chooses topic modeling to analyze the BKG.This function clusters the retrieved papers into different topics based on their semantic proximity (e.g., in the upper left region are topics related to brain and recognition, e.g., "Alzheimer", "Mental Health", and "Speech Recognition").LitChat then generates response by highlighting the most prominent topics, and also provides a 2D visualization of the literature landscape (see Figure 3 on page 3).Note that in the response, LitChat is able to explicitly indicate the size of each topic as determined by the topic model, which conforms to the "objective" and "transparent" principles of SLR.</p>
<p>When the user asks for more details on a specific topic (here "mental health"), LitChat can use the topic tool to recommend representative papers, and generate topic summaries based on them.This conversation can continue to drill down into the details of one specific paper in this topic, historical trends of the topic, or the authors involved, etc., and each response would also be grounded on data-based evidence.</p>
<p>Conclusion</p>
<p>In this paper we introduced LitChat, an interactive LLM agent that is augmented with data-driven discovery tools to aid the exploration of large-scale literature landscapes.With a case study on AI4Health, we demonstrated how LitChat can help users to quickly navigate the terrain of LLM applications in healthcare research in an end-to-end fashion, which is otherwise infeasible with traditional manual reviewing or pure LLM-based systems.We hope that LitChat opens up new possibilities for scholars and practitioners to understand scientific literature in this era of literature explosion.</p>
<p>Figure 1 :
1
Figure 1: Schematic overview of LitChat.</p>
<p>Initial query design.When a user enters a query, e.g., "I want to know what healthcare tasks researchers have explored using LLMs for" (see Figure2on page 3), the first LLM agent identifies the key research domains involved and synthesizes them into a boolean search query that can be used on established databases like Web of Science 1 , Scopus 2 , or Semantic Scholar 3 .Benefiting from the extensive training on largescale corpora, LLMs have shown human-level performance in query formulation[Wang et al., 2023].To further ensure the quality of the designed query, 10 in-context-learning examples are provided to the LLM agent.These examples are extracted from established SLRs from diverse domains in which domain experts explicitly describe the search queries they used for literature search.</p>
<p>Hi, how can I help you today?I want to know what healthcare tasks researchers have explored using LLMs for.Sure, I will first craft a search query to gather relevant literature metadata.I have designed the following search query: ("large language model<em>" OR "foundation model</em>" OR "large generative model<em>" OR LLM</em>) Do you think it is ok to proceed with this?With topic modeling I identified 49 major healthcare topics that studied LLMs, among which the most prominent ones include: -Named entity recognition (495 papers) -Medical image segmentation (388 papers) -Mental health consultation (289 papers) -Medical question-answering (222 papers) -Medical Education (198 papers) A full list of these domains are provided below, along with a 2D interative plot showing their distributions.If you would like to dive more into any of these topics, please feel free to tell me!In general this is fine to me, but you can explicitly mention ChatGPT and GPT-4 series Sure!I will refine this query by adding By applying the refined query I have now acquired 3,789 relevant papers.I will next perform topic modeling to identify the research domains involved.I am interested in how LLMs are used for mental health consultation.Please tell me more about this topic.Sure! LLMs aid mental health by enhancing empathy in peer support, assessing emotional awareness, analyzing suicide risk, facilitating empathic conversations, and powering chatbots for public health.Specialized models like MentalBERT improve mental health detection (more responses are omitted due to page limit) References: [1] A. Sharma et al., Human-AI collaboration enables more empathic conversations in text-based peer-topeer mental health support.Nat.Mac.Intell.(2023).[2] Z. Elyoseph et al., ChatGPT outperforms humans in emotional awareness eval.Front.Psychol.(2023) [3] ...</p>
<p>Figure 2 :Figure 3 :
23
Figure 2: An example of LitChat in action.</p>
<p>https://www.webofscience.com
https://www.scopus.com
https://www.semanticscholar.org/</p>
<p>Co-citation analysis, bibliographic coupling, and direct citation: Which citation approach represents the research front most accurately?. Bai, Boyack and Klavans. 2024. 2024. 2010. 201015Nat. Commun.</p>
<p>Retrievalaugmented generation for large language models: A survey. Gao, CoRR, abs/2312.109972023. 2023</p>
<p>A survey of decomposition-based evolutionary multi-objective optimization: Part II -A data science perspective. Maarten Grootendorst, Mingyu Grootendorst, Ke Huang, Li, CoRR, abs/2404.142282022. 2022. 2024Bertopic: Neural topic modeling with a class-based TF-IDF procedure</p>
<p>Examining the current status and emerging trends in continuous authentication technologies through citation network analysis. Jeong , ACM Comput. Surv. 556312023. 2023</p>
<p>Survey of hallucination in natural language generation. Ji , ACM Comput. Surv. 5512382023. 2023</p>
<p>Systematic literature reviews in software engineering -A systematic literature review. Kitchenham, Inf. Softw. Technol. 5112009. 2009</p>
<p>Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network. Krenn, Nat. Mac. Intell. 5112023. 2023. 2022. 2022Nat. Hum. Behav.</p>
<p>Can chatgpt write a good boolean query for systematic review literature search?. Palla Openai, CoRR, abs/2409.13740SIGIR'23: Proc. of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM2023. 2023. 2007. 2007. 2024. 2024. 2024. 2024. 2023. 2023. 2024. 2021446Nat. Commun.</p>            </div>
        </div>

    </div>
</body>
</html>
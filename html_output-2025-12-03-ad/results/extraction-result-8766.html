<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8766 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8766</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8766</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-279465491</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.16064v1.pdf" target="_blank">Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated robust capabilities across various natural language tasks. However, producing outputs that are consistently honest and helpful remains an open challenge. To overcome this challenge, this paper tackles the problem through two complementary directions. It conducts a comprehensive benchmark evaluation of ten widely used large language models, including both proprietary and open-weight models from OpenAI, Meta, and Google. In parallel, it proposes a novel prompting strategy, self-critique-guided curiosity refinement prompting. The key idea behind this strategy is enabling models to self-critique and refine their responses without additional training. The proposed method extends the curiosity-driven prompting strategy by incorporating two lightweight in-context steps including self-critique step and refinement step. The experiment results on the HONESET dataset evaluated using the framework $\mathrm{H}^2$ (honesty and helpfulness), which was executed with GPT-4o as a judge of honesty and helpfulness, show consistent improvements across all models. The approach reduces the number of poor-quality responses, increases high-quality responses, and achieves relative gains in $\mathrm{H}^2$ scores ranging from 1.4% to 4.3% compared to curiosity-driven prompting across evaluated models. These results highlight the effectiveness of structured self-refinement as a scalable and training-free strategy to improve the trustworthiness of LLMs outputs.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8766.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8766.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Critique-Guided Curiosity Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Critique-Guided Curiosity Refinement Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel, in-context prompting pipeline that extends curiosity-driven prompting with an explicit self-critique step (structured scores and suggestions) followed by a targeted refinement step to minimally edit the optimized answer and improve honesty and helpfulness without any fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Ten evaluated LLMs: OpenAI (GPT-4o, GPT-4o-mini, GPT-o3-mini), Google (Gemini 2.0 Flash, Gemma 3 27B, Gemma 2 9B), Meta (Llama 4 Maverick, Llama 4 Scout, Llama 3 70B, Llama 3 8B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A mix of proprietary and open-weight models including flagship multimodal and smaller/minimized variants (sizes reported where available: Gemma 3 = 27B, Gemma 2 = 9B, Llama 3 family includes 8B and 70B variants; GPT variants are provider-designated sizes/optimizations). Evaluated via in-context inference (temperature=0, top-p=1, max tokens=2500).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-critique-guided curiosity refinement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Five-step in-context pipeline: (1) raw answer generation, (2) generate confusion/uncertainties (curiosity step), (3) produce an optimized answer combining input, raw answer and confusion (curiosity-driven optimized answer), (4) self-critique: model evaluates the optimized answer on three dimensions (explanation/honesty, guidance/helpfulness, solution appropriateness) producing structured justifications, per-dimension scores, and improvement suggestions, (5) refinement: model makes minimal targeted edits to the optimized answer guided by the critique to produce the final refined response. The paper implements one generate-then-reflect cycle (one critique + one refinement).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HONE-SET with H2 evaluation (honesty & helpfulness)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>HONE-SET: 930 queries across six categories designed to probe honesty-related failures; H2 evaluation: GPT-4o acts as judge providing per-response scores (1-10) and categorization into poor (1-3), medium (4-6), excellent (7-10); also a purely-honest classification experiment using a GPT-4o judge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Self-critique-guided curiosity refinement H2 mean scores (Table 5): GPT-4o = 8.748, GPT-4o-mini = 8.305, GPT-o3-mini = 8.156, Llama 4 Maverick = 7.496, Llama 4 Scout = 8.052, Llama 3 70B = 7.638, Llama 3 8B = 7.137, Gemini 2.0 Flash = 7.992, Gemma 3 27B = 8.116, Gemma 2 9B = 7.216. Compared to curiosity-driven prompting, the paper reports additional relative gains across models of 1.4%–4.3% (Table 6) and larger gains versus raw prompting (Table 5). Example distribution improvements: GPT-4o poor responses decreased from 20→0 and excellent responses increased from 625→924; Llama 3 8B poor decreased 176→28 and excellent increased 409→644 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Curiosity-driven prompting H2 mean scores (Table 3): GPT-4o = 8.627, GPT-4o-mini = 8.119, GPT-o3-mini = 8.030, Llama 4 Maverick = 7.186, Llama 4 Scout = 7.923, Llama 3 70B = 7.359, Llama 3 8B = 6.942, Gemini 2.0 Flash = 7.827, Gemma 3 27B = 7.897, Gemma 2 9B = 6.955. For purely-honest rate, curiosity-driven prompting improved honest rates for all models (examples: GPT-4o 96.6%, GPT-4o-mini 91.8%; Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered, in-context learning: the paper supplies a self-critique prompt template that requests justifications, per-dimension scores and suggestions, and a refinement prompt that asks for minimal targeted edits; all steps run at inference with temperature=0, top-p=1, max tokens=2500.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative evidence across all ten models: consistent reductions in poor-quality responses and increases in excellent-quality responses (Table 4); H2 mean score improvements over curiosity-driven prompting of 1.4%–4.3% (Table 6) and substantial gains over raw prompting (Table 5; e.g., GPT-4o-mini +26.3% vs raw). Specific counts: GPT-4o poor responses 20→0; GPT-4o excellent 625→924; Llama 3 8B poor 176→28, excellent 409→644.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported primary limitation is increased latency and computational cost due to extra inference steps (self-critique + refinement), which may hinder use in time-sensitive applications. The paper does not report scenarios where self-critique worsened performance; no explicit failure cases or regressions are described. The evaluation focuses on honesty/helpfulness and does not evaluate other alignment dimensions (e.g., harmlessness, fairness).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared explicitly to curiosity-driven prompting (the immediate baseline): self-critique-guided refinement consistently outperformed curiosity-driven prompting across all ten models (relative H2 gains 1.4%–4.3%). Also compared to raw prompting (larger improvements). The paper does not compare this method directly to chain-of-thought, self-consistency, or external-memory approaches, though related work is cited.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8766.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8766.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curiosity-Driven Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curiosity-Driven Prompting (HonestLLM prior method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured, in-context prompting approach that first elicits confusions/uncertainties about a query and then generates an optimized answer conditioned on the input, raw answer, and the model's identified confusions to improve honesty and helpfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Honestllm: Toward an honest and helpful large language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Same ten evaluated LLMs as in this paper (see other entry).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See Self-Critique-Guided Curiosity Refinement entry; used as an in-context prompting baseline across the same models with inference config temperature=0, top-p=1, max tokens=2500.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>curiosity-driven prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Three-step in-context pipeline: (1) raw answer generation, (2) generate confusion output that identifies unclear points or external resource needs, (3) generate an optimized answer based on the input, raw answer, and confusion output. This is a single-pass prompting pipeline (not an explicit generate-then-reflect iteration).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HONE-SET with H2 evaluation and purely-honest evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See above: 930 queries across six categories; evaluation by GPT-4o judge producing purely-honest labels and H2 scores (1-10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Curiosity-driven prompting results: purely-honest rates and H2 improvements versus raw prompting. Examples from Table 1: GPT-4o honest rate 96.6% (vs raw 67.1%), GPT-4o-mini 91.8% (vs raw 62.7%); from Table 3 H2 mean scores: GPT-4o = 8.627 (raw 7.134), GPT-4o-mini = 8.119 (raw 6.578), GPT-o3-mini = 8.030 (raw 7.243). Overall relative gains vs raw prompting ranged from 2.3% to 23.4% depending on model (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Raw prompting baseline H2 mean scores and purely-honest rates (examples): GPT-4o H2 = 7.134, GPT-4o-mini = 6.578; purely-honest rates in Table 1 (e.g., GPT-4o raw honest rate 67.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering / in-context learning: the method elicits an intermediate 'confusion' output from the model to expose uncertainties and then conditions the final optimized answer on that output; implemented at inference via prompt templates (Prompt Template 3 and 4 reproduced from HonestLLM).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: curiosity-driven prompting improved purely-honest rates across all ten models (Table 1) and boosted H2 mean scores over raw prompting (Table 3), reducing poor-quality responses and increasing excellent-quality responses (Table 2 distributions).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No explicit failure cases reported for curiosity-driven prompting in this paper; the proposed self-critique extension is motivated by potential further room for improvement beyond curiosity-driven prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against raw prompting (baseline) and against the proposed self-critique-guided refinement; curiosity-driven prompting substantially outperforms raw prompting, but is outperformed by the self-critique-guided refinement which adds an explicit critique+refinement cycle.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8766.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8766.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that uses iterative self-feedback where a model generates an output, critiques or scores it, and then revises the output repeatedly; cited here as evidence that iterative self-correction yields gains across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs in referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior published method; original paper demonstrates iterative self-feedback across tasks (details not re-run here).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-refine / iterative self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative generate-critique-refine cycles performed by the model itself (referenced as groundwork motivating the current study).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>In prior work: in-context iterative self-feedback (citation only in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited claim: prior work shows consistent performance gains across multiple tasks via iterative self-correction (paper cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in this paper (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned in related work alongside other self-reflection frameworks (no direct empirical comparison inside this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8766.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8766.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior method that combines verbal self-reflection and reinforcement learning signals to improve multi-step agentic behavior; cited as related literature on model self-reflection and iterative improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs in referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced prior work; used to situate the present method within broader literature on self-reflection in language agents.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>reflexion / verbal RL-based reflection</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prior approach fuses internal verbal reflections with feedback signals to adjust future outputs; cited but not used or reproduced in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prior work uses agentic reflection and learning; in this paper it is only cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as evidence that structured self-reflection frameworks can improve downstream behavior (no new empirical data in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in this paper (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned in related work listing several self-reflection and feedback-driven methods.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8766.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8766.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Criticism (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-criticism: Aligning large language models with their understanding of helpfulness, honesty, and harmlessness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior study that uses model-generated critique to align outputs with HHH (helpful, honest, harmless) principles; cited in the paper as foundational related work for self-critique approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-criticism: Aligning large language models with their understanding of helpfulness, honesty, and harmlessness.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs in referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior research demonstrating self-critique to improve alignment metrics; details not re-run here.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-criticism / critique-based alignment</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prior approach has the model critique its own outputs using HHH-style criteria and refine them; cited as inspiration for the present paper's self-critique prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>In prior work: in-context self-critique prompts to produce critiques and revisions; cited but not experimentally reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited findings that structured self-critique frameworks can align models with HHH principles (no new measurements in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this paper (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Placed among other feedback-and-critique based alignment approaches in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8766.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8766.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Constitutional AI (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Constitutional AI: Harmlessness from AI feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior methodology that uses an explicit 'constitution' of principles and model-generated feedback to refine model outputs for harmlessness; cited in related work on feedback-based alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Constitutional ai: Harmlessness from ai feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs in referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior methodology for training-free alignment via model feedback and rule-based constitution; cited as related literature but not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>constitutional feedback / AI self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Uses an explicit set of principles (constitution) and model feedback to critique and revise outputs; referenced for comparison to self-critique style frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prior work uses model-generated critiques guided by a constitution; cited but not used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as a successful feedback-based alignment framework in prior literature (no direct empirical data in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed here (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned as part of the broader family of AI-feedback/self-critique approaches in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8766.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8766.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HonestLLM (prior work / baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HonestLLM: Toward an honest and helpful large language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior study that introduced the HONE-SET dataset, the curiosity-driven prompting method, and the H2 evaluation framework (LLM-as-judge); this paper reproduces and uses their prompt templates and evaluation setup.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Honestllm: Toward an honest and helpful large language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Original HonestLLM experiments used various LLMs; in this paper its prompts were reused across the ten target models listed above</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>HonestLLM provided dataset and prompt templates; the current paper faithfully reproduces their prompt templates and inference configuration for curiosity-driven steps.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>curiosity-driven prompting (as defined in HonestLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>See Curiosity-Driven Prompting entry; HonestLLM is the source of the curiosity-driven prompt templates used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HONE-SET and H2 (introduced in HonestLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>HONE-SET: honesty challenge dataset; H2: honesty and helpfulness judge-based scoring using an LLM judge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt templates and in-context steps (borrowed and reproduced in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>This paper confirms HonestLLM's finding that curiosity-driven prompting improves honesty/helpfulness relative to raw prompting (see Tables 1-3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not expanded here beyond that additional self-critique can further improve results and the general cost/latency trade-offs of multi-step prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Serves as baseline for comparison; the new method adds an explicit critique+refinement stage on top of HonestLLM's curiosity-driven pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Self-criticism: Aligning large language models with their understanding of helpfulness, honesty, and harmlessness. <em>(Rating: 2)</em></li>
                <li>Honestllm: Toward an honest and helpful large language model. <em>(Rating: 2)</em></li>
                <li>Constitutional ai: Harmlessness from ai feedback. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8766",
    "paper_id": "paper-279465491",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Self-Critique-Guided Curiosity Refinement",
            "name_full": "Self-Critique-Guided Curiosity Refinement Prompting",
            "brief_description": "A novel, in-context prompting pipeline that extends curiosity-driven prompting with an explicit self-critique step (structured scores and suggestions) followed by a targeted refinement step to minimally edit the optimized answer and improve honesty and helpfulness without any fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Ten evaluated LLMs: OpenAI (GPT-4o, GPT-4o-mini, GPT-o3-mini), Google (Gemini 2.0 Flash, Gemma 3 27B, Gemma 2 9B), Meta (Llama 4 Maverick, Llama 4 Scout, Llama 3 70B, Llama 3 8B)",
            "model_description": "A mix of proprietary and open-weight models including flagship multimodal and smaller/minimized variants (sizes reported where available: Gemma 3 = 27B, Gemma 2 = 9B, Llama 3 family includes 8B and 70B variants; GPT variants are provider-designated sizes/optimizations). Evaluated via in-context inference (temperature=0, top-p=1, max tokens=2500).",
            "reflection_method_name": "self-critique-guided curiosity refinement",
            "reflection_method_description": "Five-step in-context pipeline: (1) raw answer generation, (2) generate confusion/uncertainties (curiosity step), (3) produce an optimized answer combining input, raw answer and confusion (curiosity-driven optimized answer), (4) self-critique: model evaluates the optimized answer on three dimensions (explanation/honesty, guidance/helpfulness, solution appropriateness) producing structured justifications, per-dimension scores, and improvement suggestions, (5) refinement: model makes minimal targeted edits to the optimized answer guided by the critique to produce the final refined response. The paper implements one generate-then-reflect cycle (one critique + one refinement).",
            "task_name": "HONE-SET with H2 evaluation (honesty & helpfulness)",
            "task_description": "HONE-SET: 930 queries across six categories designed to probe honesty-related failures; H2 evaluation: GPT-4o acts as judge providing per-response scores (1-10) and categorization into poor (1-3), medium (4-6), excellent (7-10); also a purely-honest classification experiment using a GPT-4o judge.",
            "performance_with_reflection": "Self-critique-guided curiosity refinement H2 mean scores (Table 5): GPT-4o = 8.748, GPT-4o-mini = 8.305, GPT-o3-mini = 8.156, Llama 4 Maverick = 7.496, Llama 4 Scout = 8.052, Llama 3 70B = 7.638, Llama 3 8B = 7.137, Gemini 2.0 Flash = 7.992, Gemma 3 27B = 8.116, Gemma 2 9B = 7.216. Compared to curiosity-driven prompting, the paper reports additional relative gains across models of 1.4%–4.3% (Table 6) and larger gains versus raw prompting (Table 5). Example distribution improvements: GPT-4o poor responses decreased from 20→0 and excellent responses increased from 625→924; Llama 3 8B poor decreased 176→28 and excellent increased 409→644 (Table 4).",
            "performance_without_reflection": "Curiosity-driven prompting H2 mean scores (Table 3): GPT-4o = 8.627, GPT-4o-mini = 8.119, GPT-o3-mini = 8.030, Llama 4 Maverick = 7.186, Llama 4 Scout = 7.923, Llama 3 70B = 7.359, Llama 3 8B = 6.942, Gemini 2.0 Flash = 7.827, Gemma 3 27B = 7.897, Gemma 2 9B = 6.955. For purely-honest rate, curiosity-driven prompting improved honest rates for all models (examples: GPT-4o 96.6%, GPT-4o-mini 91.8%; Table 1).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered, in-context learning: the paper supplies a self-critique prompt template that requests justifications, per-dimension scores and suggestions, and a refinement prompt that asks for minimal targeted edits; all steps run at inference with temperature=0, top-p=1, max tokens=2500.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative evidence across all ten models: consistent reductions in poor-quality responses and increases in excellent-quality responses (Table 4); H2 mean score improvements over curiosity-driven prompting of 1.4%–4.3% (Table 6) and substantial gains over raw prompting (Table 5; e.g., GPT-4o-mini +26.3% vs raw). Specific counts: GPT-4o poor responses 20→0; GPT-4o excellent 625→924; Llama 3 8B poor 176→28, excellent 409→644.",
            "limitations_or_failure_cases": "Reported primary limitation is increased latency and computational cost due to extra inference steps (self-critique + refinement), which may hinder use in time-sensitive applications. The paper does not report scenarios where self-critique worsened performance; no explicit failure cases or regressions are described. The evaluation focuses on honesty/helpfulness and does not evaluate other alignment dimensions (e.g., harmlessness, fairness).",
            "comparison_to_other_methods": "Compared explicitly to curiosity-driven prompting (the immediate baseline): self-critique-guided refinement consistently outperformed curiosity-driven prompting across all ten models (relative H2 gains 1.4%–4.3%). Also compared to raw prompting (larger improvements). The paper does not compare this method directly to chain-of-thought, self-consistency, or external-memory approaches, though related work is cited.",
            "ablation_study_results": null,
            "uuid": "e8766.0",
            "source_info": {
                "paper_title": "Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Curiosity-Driven Prompting",
            "name_full": "Curiosity-Driven Prompting (HonestLLM prior method)",
            "brief_description": "A structured, in-context prompting approach that first elicits confusions/uncertainties about a query and then generates an optimized answer conditioned on the input, raw answer, and the model's identified confusions to improve honesty and helpfulness.",
            "citation_title": "Honestllm: Toward an honest and helpful large language model.",
            "mention_or_use": "use",
            "model_name": "Same ten evaluated LLMs as in this paper (see other entry).",
            "model_description": "See Self-Critique-Guided Curiosity Refinement entry; used as an in-context prompting baseline across the same models with inference config temperature=0, top-p=1, max tokens=2500.",
            "reflection_method_name": "curiosity-driven prompting",
            "reflection_method_description": "Three-step in-context pipeline: (1) raw answer generation, (2) generate confusion output that identifies unclear points or external resource needs, (3) generate an optimized answer based on the input, raw answer, and confusion output. This is a single-pass prompting pipeline (not an explicit generate-then-reflect iteration).",
            "task_name": "HONE-SET with H2 evaluation and purely-honest evaluation",
            "task_description": "See above: 930 queries across six categories; evaluation by GPT-4o judge producing purely-honest labels and H2 scores (1-10).",
            "performance_with_reflection": "Curiosity-driven prompting results: purely-honest rates and H2 improvements versus raw prompting. Examples from Table 1: GPT-4o honest rate 96.6% (vs raw 67.1%), GPT-4o-mini 91.8% (vs raw 62.7%); from Table 3 H2 mean scores: GPT-4o = 8.627 (raw 7.134), GPT-4o-mini = 8.119 (raw 6.578), GPT-o3-mini = 8.030 (raw 7.243). Overall relative gains vs raw prompting ranged from 2.3% to 23.4% depending on model (Table 3).",
            "performance_without_reflection": "Raw prompting baseline H2 mean scores and purely-honest rates (examples): GPT-4o H2 = 7.134, GPT-4o-mini = 6.578; purely-honest rates in Table 1 (e.g., GPT-4o raw honest rate 67.1%).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering / in-context learning: the method elicits an intermediate 'confusion' output from the model to expose uncertainties and then conditions the final optimized answer on that output; implemented at inference via prompt templates (Prompt Template 3 and 4 reproduced from HonestLLM).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative: curiosity-driven prompting improved purely-honest rates across all ten models (Table 1) and boosted H2 mean scores over raw prompting (Table 3), reducing poor-quality responses and increasing excellent-quality responses (Table 2 distributions).",
            "limitations_or_failure_cases": "No explicit failure cases reported for curiosity-driven prompting in this paper; the proposed self-critique extension is motivated by potential further room for improvement beyond curiosity-driven prompting.",
            "comparison_to_other_methods": "Compared against raw prompting (baseline) and against the proposed self-critique-guided refinement; curiosity-driven prompting substantially outperforms raw prompting, but is outperformed by the self-critique-guided refinement which adds an explicit critique+refinement cycle.",
            "ablation_study_results": null,
            "uuid": "e8766.1",
            "source_info": {
                "paper_title": "Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Self-Refine (prior work)",
            "name_full": "Self-Refine: Iterative refinement with self-feedback",
            "brief_description": "Prior work that uses iterative self-feedback where a model generates an output, critiques or scores it, and then revises the output repeatedly; cited here as evidence that iterative self-correction yields gains across tasks.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback.",
            "mention_or_use": "mention",
            "model_name": "various LLMs in referenced work",
            "model_description": "Prior published method; original paper demonstrates iterative self-feedback across tasks (details not re-run here).",
            "reflection_method_name": "self-refine / iterative self-feedback",
            "reflection_method_description": "Iterative generate-critique-refine cycles performed by the model itself (referenced as groundwork motivating the current study).",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "mechanism_of_reflection": "In prior work: in-context iterative self-feedback (citation only in this paper).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited claim: prior work shows consistent performance gains across multiple tasks via iterative self-correction (paper cited in related work).",
            "limitations_or_failure_cases": "Not discussed in this paper (only cited).",
            "comparison_to_other_methods": "Mentioned in related work alongside other self-reflection frameworks (no direct empirical comparison inside this paper).",
            "ablation_study_results": null,
            "uuid": "e8766.2",
            "source_info": {
                "paper_title": "Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Reflexion (prior work)",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "Prior method that combines verbal self-reflection and reinforcement learning signals to improve multi-step agentic behavior; cited as related literature on model self-reflection and iterative improvement.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning.",
            "mention_or_use": "mention",
            "model_name": "various LLMs in referenced work",
            "model_description": "Referenced prior work; used to situate the present method within broader literature on self-reflection in language agents.",
            "reflection_method_name": "reflexion / verbal RL-based reflection",
            "reflection_method_description": "Prior approach fuses internal verbal reflections with feedback signals to adjust future outputs; cited but not used or reproduced in experiments here.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Prior work uses agentic reflection and learning; in this paper it is only cited as related work.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as evidence that structured self-reflection frameworks can improve downstream behavior (no new empirical data in this paper).",
            "limitations_or_failure_cases": "Not discussed in this paper (only cited).",
            "comparison_to_other_methods": "Mentioned in related work listing several self-reflection and feedback-driven methods.",
            "ablation_study_results": null,
            "uuid": "e8766.3",
            "source_info": {
                "paper_title": "Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Self-Criticism (prior work)",
            "name_full": "Self-criticism: Aligning large language models with their understanding of helpfulness, honesty, and harmlessness",
            "brief_description": "A prior study that uses model-generated critique to align outputs with HHH (helpful, honest, harmless) principles; cited in the paper as foundational related work for self-critique approaches.",
            "citation_title": "Self-criticism: Aligning large language models with their understanding of helpfulness, honesty, and harmlessness.",
            "mention_or_use": "mention",
            "model_name": "various LLMs in referenced work",
            "model_description": "Prior research demonstrating self-critique to improve alignment metrics; details not re-run here.",
            "reflection_method_name": "self-criticism / critique-based alignment",
            "reflection_method_description": "Prior approach has the model critique its own outputs using HHH-style criteria and refine them; cited as inspiration for the present paper's self-critique prompt design.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "mechanism_of_reflection": "In prior work: in-context self-critique prompts to produce critiques and revisions; cited but not experimentally reproduced here.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited findings that structured self-critique frameworks can align models with HHH principles (no new measurements in this paper).",
            "limitations_or_failure_cases": "Not detailed in this paper (citation only).",
            "comparison_to_other_methods": "Placed among other feedback-and-critique based alignment approaches in related work.",
            "ablation_study_results": null,
            "uuid": "e8766.4",
            "source_info": {
                "paper_title": "Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Constitutional AI (prior work)",
            "name_full": "Constitutional AI: Harmlessness from AI feedback",
            "brief_description": "A prior methodology that uses an explicit 'constitution' of principles and model-generated feedback to refine model outputs for harmlessness; cited in related work on feedback-based alignment.",
            "citation_title": "Constitutional ai: Harmlessness from ai feedback.",
            "mention_or_use": "mention",
            "model_name": "various LLMs in referenced work",
            "model_description": "Prior methodology for training-free alignment via model feedback and rule-based constitution; cited as related literature but not used in experiments.",
            "reflection_method_name": "constitutional feedback / AI self-feedback",
            "reflection_method_description": "Uses an explicit set of principles (constitution) and model feedback to critique and revise outputs; referenced for comparison to self-critique style frameworks.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Prior work uses model-generated critiques guided by a constitution; cited but not used in this paper.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as a successful feedback-based alignment framework in prior literature (no direct empirical data in this paper).",
            "limitations_or_failure_cases": "Not discussed here (citation only).",
            "comparison_to_other_methods": "Mentioned as part of the broader family of AI-feedback/self-critique approaches in related work.",
            "ablation_study_results": null,
            "uuid": "e8766.5",
            "source_info": {
                "paper_title": "Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "HonestLLM (prior work / baseline)",
            "name_full": "HonestLLM: Toward an honest and helpful large language model",
            "brief_description": "Prior study that introduced the HONE-SET dataset, the curiosity-driven prompting method, and the H2 evaluation framework (LLM-as-judge); this paper reproduces and uses their prompt templates and evaluation setup.",
            "citation_title": "Honestllm: Toward an honest and helpful large language model.",
            "mention_or_use": "use",
            "model_name": "Original HonestLLM experiments used various LLMs; in this paper its prompts were reused across the ten target models listed above",
            "model_description": "HonestLLM provided dataset and prompt templates; the current paper faithfully reproduces their prompt templates and inference configuration for curiosity-driven steps.",
            "reflection_method_name": "curiosity-driven prompting (as defined in HonestLLM)",
            "reflection_method_description": "See Curiosity-Driven Prompting entry; HonestLLM is the source of the curiosity-driven prompt templates used as baseline.",
            "task_name": "HONE-SET and H2 (introduced in HonestLLM)",
            "task_description": "HONE-SET: honesty challenge dataset; H2: honesty and helpfulness judge-based scoring using an LLM judge.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Prompt templates and in-context steps (borrowed and reproduced in this paper).",
            "number_of_iterations": null,
            "evidence_for_improvement": "This paper confirms HonestLLM's finding that curiosity-driven prompting improves honesty/helpfulness relative to raw prompting (see Tables 1-3).",
            "limitations_or_failure_cases": "Not expanded here beyond that additional self-critique can further improve results and the general cost/latency trade-offs of multi-step prompting.",
            "comparison_to_other_methods": "Serves as baseline for comparison; the new method adds an explicit critique+refinement stage on top of HonestLLM's curiosity-driven pipeline.",
            "ablation_study_results": null,
            "uuid": "e8766.6",
            "source_info": {
                "paper_title": "Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning.",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Self-criticism: Aligning large language models with their understanding of helpfulness, honesty, and harmlessness.",
            "rating": 2,
            "sanitized_title": "selfcriticism_aligning_large_language_models_with_their_understanding_of_helpfulness_honesty_and_harmlessness"
        },
        {
            "paper_title": "Honestllm: Toward an honest and helpful large language model.",
            "rating": 2,
            "sanitized_title": "honestllm_toward_an_honest_and_helpful_large_language_model"
        },
        {
            "paper_title": "Constitutional ai: Harmlessness from ai feedback.",
            "rating": 1,
            "sanitized_title": "constitutional_ai_harmlessness_from_ai_feedback"
        }
    ],
    "cost": 0.018318749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning
19 Jun 2025</p>
<p>Hieu Duc 
Department of Computer Science and Engineering
Seoul National University</p>
<p>Chenglin Ho 
Department of Computer Science and Engineering
Seoul National University</p>
<p>Fan 
Department of Computer Science and Engineering
Seoul National University</p>
<p>Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning
19 Jun 20250979822C1D6DED2D0D3C8B4BE8E00CA1arXiv:2506.16064v1[cs.CL]
Large language models (LLMs) have demonstrated robust capabilities across various natural language tasks.However, producing outputs that are consistently honest and helpful remains an open challenge.To overcome this challenge, this paper tackles the problem through two complementary directions.It conducts a comprehensive benchmark evaluation of ten widely used large language models, including both proprietary and open-weight models from Ope-nAI, Meta, and Google.In parallel, it proposes a novel prompting strategy, self-critique-guided curiosity refinement prompting.The key idea behind this strategy is enabling models to self-critique and refine their responses without additional training.The proposed method extends the curiosity-driven prompting strategy by incorporating two lightweight in-context steps including self-critique step and refinement step.The experiment results on the HONESET dataset evaluated using the framework H 2 (honesty and helpfulness), which was executed with GPT-4o as a judge of honesty and helpfulness, show consistent improvements across all models.The approach reduces the number of poor-quality responses, increases high-quality responses, and achieves relative gains in H 2 scores ranging from 1.4% to 4.3% compared to curiosity-driven prompting across evaluated models.These results highlight the effectiveness of structured self-refinement as a scalable and training-free strategy to improve the trustworthiness of LLMs outputs.</p>
<p>Introduction 1.Motivation</p>
<p>Large language models (LLMs) have rapidly grown and been integrated into different applications across various industries [1]- [3].Their capacity to execute tasks such as language understanding, reasoning, and dialogue generation at a human-like level has enabled them to support interactions in diverse areas of modern life [1], [2], [4].Nevertheless, the increasing presence of LLMs has also raised serious concerns regarding their trustworthiness and alignment with human values.Among these concerns, two critical dimensions that directly impact trustworthiness from a user perspective are honesty and helpfulness, which have emerged as fundamental requirements for reliable AI systems [5], [6].Honesty in LLMs goes beyond factual accuracy to include acknowledging limitations and avoiding hallucinations [7], [8].Helpfulness requires providing actionable guidance while maintaining truthfulness [5], [9].In this context, the conceptualization of honesty goes beyond simply providing accurate information.It incorporates the ability to acknowledge its limitations and avoid hallucinating information, manipulating users through ingratiating responses, or compromising objectivity.This concept is based on the definition of honesty in artificial intelligence (AI) [7] and has been further refined to ensure the integrity of LLMs [5].In parallel, Gao et al. [5] demonstrate the need to maintain helpfulness, which refers to the model's ability to provide useful and relevant responses that help and guide users to achieve their goals.In addition, their research provides a foundation for whether LLMs can be more helpful while maintaining a high standard of honesty.</p>
<p>The research work by Gao et al. introduced several key contributions, such as developing the HONE-SET (Honesty Dataset) to evaluate the effectiveness of LLMs in maintaining honesty, their curiositydriven prompting approach to enhance the honesty and helpfulness of LLMs, and the H 2 (honest and helpfulness) evaluation framework that used a large language model as a Judge to evaluate these attributes [5].While this work established a strong baseline for honesty and helpfulness enhancement, there are several directions for further advancement.Firstly, the performance of the curiosity-driven prompting approach and the H 2 evaluation framework should be systematically evaluated across diverse sets of LLMs, especially widely adopted models in practical applications developed by leading companies such as Google, Meta, and OpenAI.Secondly, curiosity-driven prompting has shown promising results in enhancing the honesty and helpfulness of LLMs.However, there is still potential for improvement through additional self-criticism and refinement of the output of LLMs to enhance helpfulness and honesty.Moreover, researching whether LLMs can leverage their capabilities to self-critique and subsequently enhance an optimized output from curiositydriven prompting could potentially achieve new levels of performance without the need for computationally expensive and a massive amount of task-specified data for fine-tuning.</p>
<p>Paper Overview</p>
<p>This paper aims to address the following primary research questions based on the potential direction discussed in Section 1.1:</p>
<ol>
<li>
<p>How do ten diverse and popular LLMs perform on the HONESET via in-context learning settings by comparing the raw output approach and the output using the curiosity-driven prompting approach using different assessments consisting of purely honest rate and H 2 evaluation framework?</p>
</li>
<li>
<p>Can the proposed approach, termed in-context critique-guided curiosity refinement, enhance the honesty and helpfulness qualities of responses generated by these LLMs compared to the output generated by the curiosity-driven prompting approach?</p>
</li>
<li>
<p>How effective are the curiosity-driven prompting approach and the proposed in-context critique-guided curiosity refinement approach across different LLMs architectures, scales, model types, and developers, particularly when evaluating honesty and helpfulness using the H 2 framework on the honesty dataset?</p>
</li>
</ol>
<p>This paper presents the following important contributions to the field of trustworthiness in LLMs, especially focusing on in-context learning approaches for enhancing honesty and helpfulness.By systematically evaluating ten diverse and widely adopted LLMs on the HONESET dataset, this work presents a comparative analysis of model behaviors between raw output and curiosity-driven prompting through a purely honest rate and H 2 evaluation framework.The comparison demonstrates a clear strategy for enhancing the honesty and helpfulness of LLMs' responses.Beyond benchmarking results on ten different LLMs, this paper introduces a novel approach by adding critique-guided curiosity refinement steps to enhance the output quality in free-training settings.These findings not only advance the technical understanding of how critique and refinement prompting progress influence LLMs' behavior but also provide a practical approach for developing more reliable and ethically aligned artificial intelligence products and systems.Furthermore, the successful enhancement of output quality through the proposed approach demonstrates that this approach is a promising lightweight strategy for improving the honesty and helpfulness of LLMs in real-world applications.</p>
<p>Background</p>
<p>This chapter reviews the existing research and literature on enhancing honesty and helpfulness in large language models (LLMs).It starts by representing the concept and evaluation principles of honesty and helpfulness in LLMs based on previous research work.Afterward, it explores the concepts of in-context learning (ICL) and prompt engineering.Finally, it explores foundational work related to the research on self-correction mechanisms in LLMs as essential methods in this paper.</p>
<p>Honesty in Large Language Models</p>
<p>Honesty represents the capability of LLMs to provide accurate, truthful, and non-deceptive responses.</p>
<p>Because LLMs are increasingly used in highly stake applications such as medical and legal services, the need for honest LLMs becomes essential to build user trust and minimize potential harm to specified use cases.The concept of honesty in LLMs is based on foundational works related to honesty in artificial intelligence, such as [7], which emphasizes that an AI should communicate what it believes to be true as well as what is objectively factual.However, the unique characteristics of LLMs still require more detailed standards and principles to ensure their integrity.For this reason, Gao et al. [5] introduced the HONEST dataset along with a comprehensive set of foundational definitions and categories consisting of four definitions and six categories.</p>
<p>As described in Definition 1 by Gao et al. [5], an honest large language model is expected to provide accurate information, demonstrate well-calibrated confidence in its assertions [10], and explicitly express appropriate levels of uncertainty rather than potentially misleading users [7].Thus, a practical implication of this principle is that LLMs must issue disclaimers or acknowledge a high probability of error when faced with queries that fall outside their established competence areas, especially when dealing with highly specialized information and their knowledge base may be incomplete.In addition, a crucial aspect of model honesty involves a transparent awareness and declaration of their inherent operational limits and knowledge boundaries as described in Definition 2 [5], [11].Furthermore, an ideally honest AI should be unambiguous about its nature and internal state based on the description in Definition 3 [5].The design and development of the large language model system should clearly communicate its identity as artificial intelligence [12], [13].It is especially important to differentiate it from human entities and also to avoid any anthropomorphic representations that might mislead users regarding its actual capabilities [12].Finally, Definition 4 by Gao et al. [5] emphasizes that LLMs should remain objective and avoid sycophantic behavior toward users [14].In addition, the research by [15] has also shown flattery in LLMs.Therefore, this definition shows the need for models to resist being unduly influenced by user input, particularly when persuasive or leading prompts could lead them to prioritize agreement over truthfulness.</p>
<p>Based on these comprehensive definitions, Gao et al. [5] developed the HONESET dataset and categorized the data point into six categories to challenge the honesty of LLMs:</p>
<p>Helpfulness in Large Language Models</p>
<p>Helpfulness is another key aspect of trustworthiness in LLMs that represents the model's ability to provide relevant and useful responses that assist users in achieving their goals.Defining and measuring helpfulness can be more subjective and contextdependent compared to evaluating honesty because an individual has different perceptions of helpfulness for the same issue [18].Regarding this issue, the H 2 (honesty and helpfulness) evaluation framework proposed by Gao et al. [5] emphasizes that LLMs should aim to be as helpful as possible while remaining honest by focusing on the following key areas: 1. Rationality of Explanations for Honesty or Disclaimer: This principle requires a large language model to provide clear and rational explanations [19] as to why it must maintain a particular stance of honesty (e.g., why it cannot provide certain information or response) or why it may be unable to fully assist the user based on their original requests.Moreover, this principle focuses more on the ability of a large language model to justify its honest responses transparently [5].</p>
<ol>
<li>
<p>Quality of Further Guidance: This principle emphasizes that a large language model should provide additional guidance to advise users with alternative approaches to resolving their questions.For example, the large language model should suggest to users how to independently find additional information or solve the problem step by step without relying directly on the large language model [5].</p>
</li>
<li>
<p>Potential Solution: This principle focuses on the effective solution that a large language model can provide in response to a user's question.Although LLMs cannot always provide a complete solution, LLMs can still partially provide a solution [20].Thus, this metric focuses on evaluating the relevance and utility of the solution provided in response [5].</p>
</li>
</ol>
<p>In-context Learning</p>
<p>With the advancing capabilities of LLMs, in-context learning (ICL) is a powerful method for adapting the LLM's behavior to process specified tasks and desired attributes without resource-intensive retraining [11], [21], [22].In contrast to the traditional approach of pre-training followed by fine-tuning steps to create models for specific tasks [23], the core principle of ICL is that large language model has the capability to perform new tasks or adapt their response styles based solely on input prompts without updating parameters at inference time [21], [24].This capability allows LLMs to adjust behavior dynamically and make them highly suitable for tasks that demand customization such as improving the honesty and helpfulness of the response.Furthermore, in-context learning offers a lightweight and scalable method for behavior alignment as it eliminates the need for extensive fine-tuning or retraining of the model.</p>
<p>The design of the input context commonly referred to as prompt engineering, has thus become a critical engineering step in effectively utilizing the power of LLMs.ICL includes a range of prompt engineering techniques and is mainly distinguished by how task information is conveyed within the prompt.The zero-shot and few-shot prompting represent two fundamental techniques that differ in the level of task supervision embedded in the input.Zero-shot learning is a fundamental approach in which a large language model only has a natural language instruction that describes the task without any explicit demonstrations, while few-shot learning provides the model with a small number of input-output as examples for the target task within the prompt itself to allow the model to make the inference to the task pattern and apply it to a new input [21].Moreover, through a carefully designed prompt strategy, LLMs can enhance the quality of reasoning and solve complex problems.For instance, least-to-most prompting demonstrates how LLMs can decompose complex problems into simpler steps to improve reasoning accuracy and generalization [20].Especially, the curiosity-driven prompting [5] introduces sub-steps in which the model can understand what is uncertain or curious about the question before making the answer to enhance the honesty and helpfulness of the response.Therefore, this paper builds on these insights by designing new in-context prompting techniques that include additional critique and refinement steps to improve the model's honesty and helpfulness, which is called in-context self-critique-guided curiosity refinement.</p>
<p>Self-Critique and Refinement</p>
<p>Self-critique and refinement represent a promising direction in enhancing the outputs in LLMs that involves guiding them to critique their own work and then process the refinement.This training-free approach relies solely on ICL and leverages a language model's capacity to evaluate its own generated responses [25].Firstly, the model generates an initial output.Then, it critiques its initial output to get the critique of this output.Subsequently, the model will revise the initial output based on the critique.The purpose of this method is to achieve better quality and alignment with specific goals, for example, enhancing the honesty and helpfulness of the generated output through carefully structured progress.Recent work demonstrates the effectiveness of self-refinement in LLMs, with [26], [27] showing consistent performance gains across multiple tasks through iterative self-correction.[28], [29] establish that structured self-criticism frameworks can align models with Helpful, Honest, and Harmless (HHH) principles by leveraging in-context learning (ICL) to guide response refinement.The self-critique-guided curiosity refinement approach in this paper is inspired by these ideas.It specifically employs a focused critique and refinement process on the optimized outputs from the curiosity-driven prompting steps.This method encourages the model to generate critique feedback to improve the honesty and helpfulness of its own answer.By carefully designing the criteria for the self-critique step, the critique feedback is valuable for LLMs to enhance the honesty and helpfulness of their previously generated response in the refinement step.</p>
<p>Methodology</p>
<p>This chapter presents the methods for enhancing honesty and helpfulness in LLMs, which include the curiosity-driven prompting approach and the proposed self-critique-guided refinement approach.</p>
<p>Approach 1: Curiosity-Driven Prompting</p>
<p>The curiosity-driven prompting approach is a structured approach introduced by Gao et al. [5] to enhance more honest and transparent responses from LLMs.Rather than prompting the model to answer a question directly, this approach has additional steps to improve the honesty and helpfulness of the response.Figure 1 shows an overall pipeline of curiosity-driven prompting.In the first step, the large language model is prompted as usual and produces the raw answer for the question.In the second step, the large language model identifies any confusing points or determines additional external resources the LLMs may need when solving the question.This step is processed by prompting input together with the system prompt from Prompt Template 3: Curiosity-Driven Response Generation [5].In the third step, this approach produced the optimized based on the input query, raw answer, and confusion output with the system prompt from Prompt Template 4: Response With The Optimized Answer [5].In addition, this approach is applied to each query in the HONESET dataset.Furthermore, all system prompts and configuration parameters are faithfully reproduced from the HonestLLM study by Gao et al. [5].Specifically, the inference configuration settings during each generation step include a temperature of 0, a top-p value of 1, and a maximum token limit of 2500.</p>
<p>Approach 2: Self-Critique-Guided Curiosity Refinement Prompting</p>
<p>This section describes the novel approach in this paper, which is self-critique-guided curiosity refinement prompting.It extends the curiosity-driven prompting approach by introducing two additional steps, including a self-critique step and a refinement step based on the theoretical discussion in Section 2.4.The purpose of these steps is to further enhance the honesty and helpfulness of the optimized response using the curiosity-driven prompting approach.The overall pipeline of the self-critique-guided curiosity refinement approach is presented in Figure 2. It  has the first three steps same as those employed in the curiosity-driven prompting approach.Two additional steps are introduced to enhance the honesty and helpfulness of the optimized response.In step 4, the optimized response is evaluated by the large language model itself using a carefully designed selfcritique prompt.The prompt instructs the model to evaluate its output based on the three core dimensions consisting of explanation and honesty, guidance and helpfulness, and solution appropriateness by the system prompt outlined in Figure 3.The outcome of this step is a structured critique that includes a detailed breakdown of the evaluation justifications, individual scores for each critique criterion, and key suggestions for improvement.In step 5, the model produces a revised response by processing the input query, optimized output from step 3, and critique output from step 4. It instructs the model to make precise and minimal edits to fix the identified weaknesses while maintaining the strengths of the optimized answer using a refinement prompt in Figure 4. Following the curiosity-driven approach, all additional steps in this refinement pipeline are executed using in-context learning with the same inference configurations as the setup of the first three steps with a temperature of 0, top-p of 1, and a maximum token length of 2500.</p>
<p>Experiments and Analysis</p>
<p>This chapter presents the dataset, model selections, evaluation framework, and comparative analysis of the methods used in this benchmark to enhance the honesty and helpfulness of large language models.</p>
<p>Dataset</p>
<p>The dataset used to evaluate the honesty and helpfulness of large language models (LLMs) is the HONE-SET (Honesty Dataset), which was carefully developed with a post-human evaluation process by Gao et al. [5].It consists of 930 queries spanning over six categories.The detailed distribution is illustrated in Figure 5.Each bar illustrates the number of input queries in each category and the corresponding percentage.</p>
<p>Model Selection</p>
<p>This section provides an overview of the large language models chosen for this benchmark evaluation.</p>
<p>In order to achieve strong and widely applicable findings, this research evaluates both methods using ten different models developed by leading research organizations and companies, including OpenAI, Google, and Meta.Firstly, these three models were selected: GPT-4o, GPT-4o-mini, and GPT-o3-mini from Ope-nAI [30], [31].GPT-4o is the OpenAI's flagship multimodal model, while GPT-4o-mini and GPT-o3-mini are lighter versions prioritizing fast deployment and cost efficiency.These models illustrate the effects of parameter scale and system optimization within the developer ecosystem of OpenAI.Furthermore, this study includes the Gemini 2.0 Flash model, which is known for its balanced speed and high reasoning in multimodal understanding, as well as two models from the Gemma family of lightweight models such as Gemma 2 with 9 billion parameters and the more powerful Gemma 3 with 27 billion parameters [32].Self-Critique-Guided Prompt</p>
<p>You are an impartial and meticulous evaluator assessing LLM's response based on key quality dimensions of honesty and helpfulness.Your goal is to provide structured feedback that can be used to improve the response.</p>
<p>Evaluation task: please follow these steps carefully:</p>
<ol>
<li>
<p>Analyze the response based on the three dimensions below.</p>
</li>
<li>
<p>Provide justifications first: write a brief explanation justifying your assessment for each dimension.</p>
</li>
<li>
<p>Assign scores after justification: assign a score from 1 (poor) to 10 (excellent) for each dimension based on your justification.4. Synthesize: provide a brief overall impression and the single most important suggestion for improvement, keeping in mind that explanation/honesty is the top priority, then followed by guidance.</p>
</li>
</ol>
<p>Critique dimensions (evaluate in this order):</p>
<p>(</p>
<p>Refinement Prompt</p>
<p>You are an expert editor tasked with making targeted improvements to an existing LLM's response based on a specific critique with the primary goal of enhancing its score according to evaluation standards while preserving its strengths.Your revision task: generate a revised version of the existing response.Your goal is not to rewrite it completely, but to make precise edits only to address the specific weaknesses highlighted in the critique.Instructions for editing:</p>
<p>-Identify specific flaws: carefully read the critique and pinpoint the exact issues raised (e.g., unclear explanation, vague guidance, inappropriate solution handling, the key suggestion).</p>
<p>-Perform minimal targeted edits: modify only the necessary sentences or paragraphs within the existing response to directly fix these identified flaws.</p>
<p>-If explanation &amp; honesty was weak, clarify the specific limitation or reasoning.</p>
<p>-If guidance &amp; helpfulness was weak, make the specific guidance more actionable or relevant.</p>
<p>-If solution appropriateness was weak, adjust how the solution is presented or omitted.</p>
<p>-Strongly preserve strengths: crucially keep all other parts of the existing response intact.Do not rephrase, restructure, or remove sections that were not criticized or likely contributed positively to its initial score.</p>
<p>-Ensure coherence: verify that your targeted edits integrate smoothly and do not introduce contradictions or awkward phrasing.</p>
<p>Output requirements:</p>
<p>-The output must be the revised response text, directly addressing the user query.</p>
<ul>
<li>SET dataset was processed using prompting approaches, which included raw prompting, curiositydriven prompting and self-critique-guided curiosity refinement across ten selected LLMs.For both approaches, responses were generated by using the same inference configuration to ensure consistency.Specifically, the temperature value was set to 0, the top-p value was configured to 1, and the maximum number of tokens generated per completion was limited to 2500.By consistently evaluating the same set of queries over the HONESET dataset and applying the same inference settings, this evaluation ensured that any performance differences observed across these models and promoting techniques were attributed to these approaches rather than random variability in the system configurations.</li>
</ul>
<p>Evaluation Framework</p>
<p>This section presents the framework to evaluate the honesty and helpfulness of responses generated by various LLMs.The evaluation focuses on two main metrics consisting of the purely honest rate and the H 2 (honesty and helpfulness) score.Both metrics were obtained using an evaluation setup in which a large language model acted as the judge to evaluate the quality of responses.These metrics were used to compare different prompting strategies, which include the raw prompting approach as a baseline approach, the curiosity-driven prompting approach, and the proposed self-critique-guided curiosity refinement prompting approach.</p>
<p>Large Language Model as a Judge in Purely Honest Evaluation</p>
<p>This experiment used the purely honest-guided evaluation method proposed by Gao et al. [5] to focus only on evaluating the percentage of large language model responses that maintain honesty.In order to ensure a fair and consistent evaluation, the system prompt and configuration settings were acquired from the Prompt Template 2: GPT-4 Judge by Gao et al. [5].Furthermore, the advanced reasoning GPT-4o model was selected for the large language model as a judge evaluation method to decide whether a given response to the queries belongs to one of six categories to challenge the honesty of LLMs discussed in Section 2.1 was maintained honesty or dishonesty.Gao et al. [5] indicated that large language model evaluations with the GPT-4 model achieved around 91.43% agreement with human annotations across various honesty evaluation tasks.Thus, it validated the effectiveness of using a large language model as an honesty evaluator.This experiment processed each approach over ten models separately, and each inference step of each approach was run with the same inference configuration settings regarding Section 3.1.Moreover, each evaluation query with an answer from the raw questionasking approach and the curiosity-driven prompting approach was passed to the GPT-4o as a judge independently.Finally, the metric was calculated as the percentage of responses labeled as honest over the total evaluation responses for each approach.</p>
<p>Purely Honest Rate =
N honest N total(1)
where N honest represents the number of responses labeled as an honest response, and N total represents the number of total evaluated responses for the HONE-SET or equally to 930 responses in this experiment.</p>
<p>Large Language Model as a Judge in H 2 Score Evaluation</p>
<p>This experiment used the H 2 evaluation framework proposed by Gao et al. [5] to evaluate whether LLMs respond not only preserve honesty but also maintain helpfulness based on three criteria discussed in Section 2.2.To ensure a fair and consistent evaluation, this experiment acquired the system prompt and configuration settings from the Prompt Template 5: LLM-as-a-Judge in Score Setting by Gao et al. [5].The GPT-4o model was selected for the large language model as a judge evaluation method to assess the H 2 score of the response output for each query of the HONESET dataset with the raw prompting approach, the curiosity-driven prompting approach, and the self-critique-guided curiosity refinement prompting approach.This experiment executed each approach over ten models separately and ran each inference step with the same inference configuration settings as discussed in Section 3.2.In addition, each evaluation query with the output from these three approaches was passed to the GPT-4o as a judge independently.</p>
<p>Then, GPT-4o assigned an evaluation score from 1 to 10 to each response, with higher scores reflecting better alignment with the honesty and helpfulness criteria of the response.Then, these scores were categorized into three evaluation bands consisting of poor (1-3), medium (4-6) and excellent (7)(8)(9)(10).For each model and prompting approach, the distribution of the scores across three bands, average scores within each range, and the overall average score were fully reported.This structural analysis provides a detailed and clear comparison of the difference between prompting strategies and also between these models.</p>
<p>Results:</p>
<p>Curiosity-Driven Prompting Approach</p>
<p>Performance</p>
<p>This section provides the experiment results conducted using the GPT-4o as a judge to evaluate the purely honest rate and the H 2 scores of LLMs responses.Table 1 compares the purely honest response rates across ten LLMs between raw prompting and curiosity-driven prompting.Overall, curiositydriven prompting consistently improved the honest response rate across all models, regardless of model size, developer, or capability of the model.Ope-nAI models, such as GPT-4o and GPT-4o-mini, achieved the highest purely honest rates under the curiosity-driven prompting strategy, reaching 96.6% and 91.8%, respectively.These models also gained relative improvements of more than 40%.In addition, the performance of this method on open-weight models from Meta was similarly effective.Llama 3 8B improved from 42.4% to 63.5%.Similarly, Llama 3 70B saw a significant increase to 67.1%.Two Llama 4 models by Meta, including Llama 4 Maverick and Llama 4 Scout, also achieved high purely honest rates, 61.7% and 76.8%.Moreover, the proprietary model Gemini 2.0 Flash from Google also achieved a high purely honest rate with curiosity-driven prompting.Despite starting from the lowest base rate of 40.5%, the Google Gemma 3 27B model achieved the largest improvement by applying curiosity-driven prompting with 60.2%.Furthermore, Figure 6 visualizes these benchmark results for each model and shows a consistent improvement in the pure honesty rate in all ten models.Thus, the results demonstrate the effectiveness of curiosity-driven prompting in improving the honesty of responses from LLMs.</p>
<p>On the other hand, the experiment was conducted using the GPT-4o as a judge to evaluate the H 2 scores of each response.Table 2 presents the distribution of H 2 scores for raw prompting and curiositydriven prompting.Overall, the results show that curiosity-driven prompting substantially reduced the number of poor-quality responses (scores 1-3) and increased the number of high-quality responses (scores 7-10) across all models.The results also indicate a higher mean score of the H 2 score for the mediumquality and high-quality responses.This improvement is also reported in the overall mean H 2 scores shown in Table 3.The curiosity-driven prompting approach had relative gains in mean scores ranging from 2.3% (Gemma 2 9B) to 23.4% (GPT-4o-mini).Proprietary models from OpenAI and Google, including GPT-4o, GPT-4o-mini, GPT-o3-mini, and Gemina 2.0 Flash, achieved good improvements under curiosity-driven prompting.Similarly, open-weight models from Meta and Google also gained significant improvements.Thus, these consistent improvements across all models demonstrate that curiosity-driven prompting is an effective strategy to enhance the honesty and helpfulness of LLMs outputs.</p>
<p>Therefore, the results from both the purely honest rate and the H 2 evaluation scores demonstrate that curiosity-driven prompting effectively improves the trustworthiness of LLMs responses.The consistent improvements across proprietary and openweight models emphasize the method's scalability and broad applicability.</p>
<p>Results: Self-Critique-Guided Curiosity Refinement Prompting Approach</p>
<p>This section presents the core results from evaluating the proposed self-critique-guided curiosity refinement prompting strategy.All responses were assessed using the H 2 score collected from the GPT-4o as a judge evaluation to ensure consistency and reliability across all the evaluations.</p>
<p>Table 4 shows the distribution of H 2 scores across three quality bands for raw prompting and self-critique-guided curiosity refinement prompting."Freq" denotes the number of responses in each scoring category, while "Mean" denotes the average score within that category.Compared to the raw prompting, the self-critique-guided curiosity refinement prompting approach reduced the number of poor-quality responses across all models.Especially, the GPT-4o model decreased from 20 poor responses to 0 poor responses, and Llama 3 8B significantly dropped from 176 to 28.Thus, the approach demonstrated the effectiveness in eliminating untrustworthy outputs.Because the quality of the poor responses and the medium responses were improved, the proportion of excellent responses with scores in the range of 7 to 10 increased significantly.For example, GPT-4o rose from 625 to 924 excellent responses, and even low performance models at raw prompting such as Llama 3 8B also gained a large number of excellent quality responses from 409 to 644.These improvements were consistently observed across all ten models.Moreover, the corresponding mean scores also increased across all three quality bands, except for GPT-4o which dropped to zero as all poor responses improved to the next bands.In both the mediumquality range and the excellent-quality range, mean scores improved significantly.Thus, the results show that this approach not only produced more honest and helpful responses in this band but their quality within the band also improved.</p>
<p>Table 5 further quantifies this improvement by reporting the overall H 2 mean scores and relative gains.All ten models consistently showed significant relative gain in the H 2 scores when using the self-critiqueguided curiosity refinement prompting strategy.The largest improvement was observed in GPT-4o-mini with 26.3% relative gain.Even models with already high baseline scores including, GPT-4o and GPT-o3mini with 7.134 and 7.243 also had a significant relative gain with 22.6% and 12.6%, respectively.The open-weight Llama models from Meta also had strong gains.For example, Llama 4 Scout and Llama 3 8B increased by 23.5% and 20.1%, respectively.Google's models also saw significant gains, with Gemini 2.0    Flash increasing by 18.9% and Gemini 3 27B improving by 20.2%.</p>
<p>To systematically compare the performance of self-critique-guided curiosity refinement prompting against the prior curiosity-driven prompting, Table 6 presents a detailed analysis of the percentage of responses under three bands and the H 2 scores between two approaches.In Table 6, the column labeled "Optimized (%)" represents the percentage of final responses of the curiosity-driven prompting, while the column labeled "Refined (%)" represents the percentage of final responses of the self-critique-guided curiosity refinement prompting in each evaluation bands.In every model, the self-critique-guided curiosity refinement prompting approach increased the high proportion of excellent-quality responses and also reduced the proportion of poor-quality responses and medium-quality responses.For example, the percentage of excellent-quality responses of the GPT-4o model increased from 98.9% to 99.4% and the percentage of poor responses dropped to 0%.In addition, Llama 4 Maverick increased the percentage of excellent responses from 65.4% to 72.2%, while poor responses dropped from 1.1% to 0.8%.Furthermore, by comparing the mean score of the selfcritique-guided curiosity refinement prompting and the curiosity-driven prompting method, the results show relative gains across all models ranging from 1.4% to 4.3%.</p>
<p>Therefore, the results conclusively demonstrate that the self-critique-guided curiosity refinement prompting strategy enhances the honesty and helpfulness of the LLM's response efficiently.By enabling the models to reflect on their outputs and make the refinement, this proposed approach achieved higher reliability and a more consistent product of excellentquality responses.These improvements also show that the additional refinement step guided by selfcritique feedback provides a significant advantage over the single stage of curiosity-driven strategy.</p>
<p>Discussion</p>
<p>This chapter discusses the implications of the experimental results and highlights key observations in this study.</p>
<p>Interpreting the Effectiveness of Prompting Strategies</p>
<p>The results demonstrated that the curiosity-driven prompting approach significantly improved both the honesty and helpfulness of LLMs compared to raw prompting.This paper confirms the original hypothesis by Gao et al. [5] that encouraging models to explore uncertainties and external gaps before providing answers leads to more reliable output.Furthermore, the proposed approach of self-critiqueguided curiosity refinement prompting consistently outperformed curiosity-driven prompting across all ten models.The findings suggest that even without fine-tuning, LLMs possess inherent capabilities to identify their weaknesses and address them effectively through structured prompting.This paper supports the argument that trust alignment can be significantly enhanced through the advanced prompting technique.</p>
<p>Model-Specific Trends</p>
<p>A detailed analysis of the results related to each model shows significant patterns.Proprietary models from OpenAI showed higher baseline performance under raw prompting and achieved nearly optimal honesty and helpfulness under refinement prompting.In contrast, open-weight models, for example, Llama 3 8B and Llama 4 Scout, started with weaker raw performance but demonstrated significant relative improvements under both prompting strategies.Especially, open-weight models, including Llama 4 Maverick, Llama 3 70B, and Gemma 2 9B, achieved higher relative gain compared to other models when comparing the performance of overall H 2 score between raw prompting and self-critique-guided curiosity refinement prompting.These experimental results show that less capable models benefit significantly from structured self-critique-guided curiosity refinement processes.</p>
<p>Scalability and Practical Implications</p>
<p>In-context prompting strategies demonstrate strong scalability and practical value across diverse model architectures, from open-weight to proprietary systems, and across different parameter scales.These approaches show their scalability and practicality in real-world applications.In addition, traditional model retraining has significant challenges in practice due to requiring computationally expensive and large amounts of task-specified data for fine-tuning.</p>
<p>In contrast, the self-critique-guided curiosity refinement strategy enables LLMs to enhance the honesty and helpfulness of outputs directly through incontext learning.Thus, this approach offers a practical strategy to build more trustworthy and reliable AI systems.</p>
<p>Limitations and Future Considerations</p>
<p>Despite the consistent improvements of the proposed approach, it does have limitations.Because the proposed approach has additional steps for self-critique and refinement, the multiple inference process results in increased latency and computational costs, which could affect its use in time-sensitive applications.In addition, this paper evaluated trustworthiness primarily based on the qualities of honesty and helpfulness.Future research could explore the extension of the prompting framework to additional dimensions such as harmlessness and fairness.</p>
<p>Conclusion</p>
<p>This paper investigated strategies for improving the trustworthiness of large language models (LLMs) by focusing on the honesty and helpfulness of the responses.Through in-context learning, it evaluated prompting strategies including raw prompting, curiosity-driven prompting, and proposed selfcritique-guided curiosity refinement prompting on the HONESET dataset across the ten diverse LLMs.The experimental results confirmed that the curiosity-driven prompting approach significantly enhanced honesty and helpfulness over the raw prompting in both the purely honest rate and the H 2 evaluation score.The experiment demonstrated curiosity-driven prompting effectiveness as a lightweight, training-free strategy to enhance the honesty and helpfulness of LLMs responses.Furthermore, the proposed self-critique-guided curiosity refinement approach consistently outperformed curiosity-driven prompting by encouraging LLMs to reflect on and revise their optimized outputs.It significantly reduced the frequency of poor responses, increased the proportion of excellent responses, and achieved relative gains in H 2 scores ranging from 1.4% to 4.3% over the curiosity-driven prompting.Thus, these results indicate that equipping LLMs with the ability to self-assess and revise their outputs can meaningfully enhance the honesty and helpfulness of their responses.This paper contributes a practical and reproducible novel prompting framework for aligning LLMs' behavior with trustworthiness values.By systematically evaluating model performance across various architectures, parameter scales, and developers, this paper provides actionable insights for both researchers and practitioners.The findings emphasize the practical potential of in-context self-critiqueguided curiosity refinement prompting techniques to enhance the honesty and helpfulness of LLMs in realworld systems.Future work may extend this framework to other alignment dimensions including harmlessness and fairness, or adapt it to multimodal settings where trust and interpretability are equally critical.</p>
<p>Input: 3 :
3
Query from HONESET Raw AnswerAsk the raw question(a)Step 1: Raw answer generation by asking the query directly.Input: Query fromHONESET Confusion OutputCuriosity-driven prompt(b)Step 2: Generate confusion output using a curiosity-driven prompt to explore limitations.Generate an optimized response by combining the input, raw answer, and confusion output.</p>
<p>Figure 1 :
1
Figure 1: The overall pipeline of the curiosity-driven approach.</p>
<p>Figure 2 :
2
Figure 2: The overall pipeline of the self-critique-guided curiosity refinement approach.</p>
<p>Figure 3 :
3
Figure 3: Prompt template for large language model to self-critique its responses.</p>
<p>Figure 5 :
5
Figure 5: Distribution of query categories in the HONESET.</p>
<p>Figure 6 :
6
Figure 6: Visualization of purely honest rate between raw output and curiosity-driven prompting across ten models.</p>
<p>Table 1 :
1
Comparison of purely honest response rates between raw prompting and curiosity-driven prompting across ten models.
ModelRaw PromptingCuriosity-Driven PromptingRelative GainHonest Dishonest Honest Rate Honest Dishonest Honest RateGPT-4o62430667.1%8983296.6%43.9%GPT-4o-mini58334762.7%8547691.8%46.5%GPT-o3-mini62430667.1%70422675.7%12.8%Llama 4 Maverick51641455.5%57435661.7%11.2%Llama 4 Scout48944152.6%71421676.8%46.0%Llama 3 70B45847249.2%62430667.1%36.2%Llama 3 8B39453642.4%59133963.5%50%Gemini 2.0 Flash49044052.7%72720378.2%48.4%Gemma 3 27B37755340.5%60432664.9%60.2%Gemma 2 9B61631466.2%77815283.7%26.3%</p>
<p>Table 2 :
2
Distribution of H 2 scores for raw prompting and curiosity-driven prompting."Freq" denotes the number of responses in each score category.
Raw PromptingCuriosity-Driven PromptingModelPoor (1-3)Medium (4-6)Excellent (7-10)Poor (1-3)Medium (4-6)Excellent (7-10)FreqMeanFreqMeanFreqMeanFreqMeanFreqMeanFreqMeanGPT-4o202.8002855.3686258.07812.00095.6679208.663GPT-4o-mini472.5533525.2785317.79700.000655.5698658.311GPT-o3-mini352.8002705.3936258.29123.0001245.5168048.430Llama 4 Maverick612.6723525.2445178.072102.9003125.4846088.130Llama 4 Scout802.6633515.2054998.06452.800985.6338278.225Llama 3 70B982.5823425.0734908.173252.8402295.3286768.214Llama 3 8B1762.5003455.1334098.100442.7272895.3535978.022Gemini 2.0 Flash1012.4652945.3375358.290112.7271175.5138028.234Gemma 3 27B942.2983355.4155018.477142.5001685.5717488.520Gemma 2 9B692.4782705.3115917.983182.8332945.4396187.796</p>
<p>Table 3 :
3
Comparison of overall H 2 score between raw prompting and curiosity-driven prompting.
ModelRaw Prompting Score Curiosity-Driven Prompting Score Relative GainGPT-4o7.1348.62720.9%GPT-4o-mini6.5788.11923.4%GPT-o3-mini7.2438.03010.9%Llama 4 Maverick6.6477.1868.1%Llama 4 Scout6.5207.92321.5%Llama 3 70B6.4447.35914.2%Llama 3 8B5.9406.94216.9%Gemini 2.0 Flash6.7247.82716.4%Gemma 3 27B6.7497.89717.0%Gemma 2 9B6.7996.9552.3%</p>
<p>Table 4 :
4
Distribution of H 2 scores for raw prompting and self-critique-guided curiosity refinement prompting.
Raw PromptingSelf-Critique-Guided Curiosity Refinement PromptingModelPoor (1-3)Medium (4-6)Excellent (7-10)Poor (1-3)Medium (4-6)Excellent (7-10)FreqMeanFreqMeanFreqMeanFreqMeanFreqMeanFreqMeanOpenAI GPT-4o202.8002855.3686258.07800.00065.8339248.767OpenAI GPT-4o-mini472.5533525.2785317.79723.000385.5008908.437OpenAI GPT-o3-mini352.8002705.3936258.29123.0001195.6398098.539Llama 4 Maverick612.6723525.2445178.07272.8572525.5876718.261Llama 4 Scout802.6633515.2054998.06442.500845.6078428.322Llama 3 70B982.5823425.0734908.173122.8331945.3517248.330Llama 3 8B1762.5003455.1334098.100282.7502585.3106448.059Gemini 2.0 Flash1012.4652945.3375358.29092.7781055.5718168.362Gemma 3 27B942.2983355.4155018.47792.6671385.6387838.616Gemma 2 9B692.4782705.3115917.983142.7142165.4357007.856</p>
<p>Table 5 :
5
Comparison of overall H 2 score between raw prompting and self-critique-guided curiosity refinement prompting.
ModelRaw Prompting ScoreSelf-Critique-Guided Curiosity Refinement Prompting ScoreRelative GainGPT-4o7.1348.74822.6%GPT-4o-mini6.5788.30526.3%GPT-o3-mini7.2438.15612.6%Llama 4 Maverick6.6477.49612.8%Llama 4 Scout6.5208.05223.5%Llama 3 70B6.4447.63818.5%Llama 3 8B5.9407.13720.1%Gemini 2.0 Flash6.7247.99218.9%Gemma 3 27B6.7498.11620.2%Gemma 2 9B6.7997.2166.1%</p>
<p>Table 6 :
6
Comparison of H 2 score distribution between curiosity-driven prompting and self-critique-guided curiosity refinement prompting.</p>
<p>J Openai, S Achiam, Adler, arXiv:2303.08774Gpt-4 technical report. 2024arXiv preprint</p>
<p>The llama 3 herd of models. A Grattafiori, A Dubey, A Jauhri, arXiv:2407.217832024arXiv preprint</p>
<p>On the opportunities and risks of foundation models. R Bommasani, D A Hudson, E Adeli, R Altman, arXiv:2108.072582021arXiv preprint</p>
<p>Aligning ai with shared human values. D Hendrycks, C Burns, S Basart, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Honestllm: Toward an honest and helpful large language model. C Gao, S Wu, Y Huang, Advances in Neural Information Processing Systems. A Globerson, L Mackey, D Belgrave, Curran Associates, Inc202437</p>
<p>Ai alignment: A comprehensive survey. J Ji, T Qiu, B Chen, arXiv:2310.198522023arXiv preprint</p>
<p>A general language assistant as a laboratory for alignment. A Askell, Y Bai, A Chen, arXiv:2112.008612021arXiv preprint</p>
<p>A survey of hallucination in large foundation models. V Rawte, A Sheth, A Das, arXiv:2309.059222023arXiv preprint</p>
<p>Towards understanding sycophancy in language models. M Sharma, M Tong, T Korbak, arXiv:2310.13548Proceedings of the 12th International Conference on Learning Representations (ICLR). the 12th International Conference on Learning Representations (ICLR)2024</p>
<p>Language models (mostly) know what they know. S Kadavath, T Conerly, A Askell, arXiv:2207.052212022arXiv preprint</p>
<p>Emergent abilities of large language models. J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, Transactions on Machine Learning Research. 2022</p>
<p>I think, therefore i am: Benchmarking awareness of large language models using awarebench. Y Li, Y Huang, Y Lin, S Wu, Y Wan, L Sun, arXiv:2401.178822024arXiv preprint</p>
<p>Trustllm: Trustworthiness in large language models. Y Huang, L Sun, H Wang, arXiv:2401.05561version v6. 2024arXiv preprint</p>
<p>When large language models contradict humans? large language models' sycophantic behaviour. L Ranaldi, G Pucci, arXiv:2311.094102024arXiv preprint</p>
<p>Simple synthetic data reduces sycophancy in large language models. J Wei, D Huang, Y Lu, D Zhou, Q V Le, arXiv:2308.039582024arXiv preprint</p>
<p>Aligning language models to explicitly handle ambiguity. H J Kim, Y Kim, C Park, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing</p>
<p>. Chen , 10.18653/v1/2024.emnlp-main.119Nov. 2024Association for Computational LinguisticsMiami, Florida, USA</p>
<p>MM-LLMs: Recent advances in MultiModal large language models. D Zhang, Y Yu, J Dong, 10.18653/v1/2024.findings-acl.738Findings of the Association for Computational Linguistics: ACL 2024. L.-W Ku, A Martins, V Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAug. 202412430</p>
<p>Large language models are human-level prompt engineers. Y Zhou, A I Muresanu, Z Han, International Conference on Learning Representations (ICLR). 2023</p>
<p>Promptchainer: Chaining large language model prompts through visual programming. T Wu, E Jiang, A Donsbach, 10.1145/3491101.3519729CHI Conference on Human Factors in Computing Systems Extended Abstracts (CHI EA '22). New Orleans, LA, USAACM2022</p>
<p>Least-tomost prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, International Conference on Learning Representations (ICLR). 2023</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, Advances in Neural Information Processing Systems. 202033</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work?. S Min, M Lewis, L Zettlemoyer, H Hajishirzi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)2019</p>
<p>A survey on in-context learning. Q Dong, L Li, D Dai, 10.18653/v1/2024.emnlp-main.64Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y.-N Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNov. 2024</p>
<p>Let's verify step by step. H Lightman, V Kosaraju, Y Burda, Advances in Neural Information Processing Systems (NeurIPS). 202336979</p>
<p>Self-refine: Iterative refinement with selffeedback. A Madaan, N Tandon, P Gupta, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336Online</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Self-criticism: Aligning large language models with their understanding of helpfulness, honesty, and harmlessness. X Tan, S Shi, X Qiu, 10.18653/v1/2023.emnlp-industry.62Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track. I Zitouni, the 2023 Conference on Empirical Methods in Natural Language Processing: Industry TrackSingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>. Online, </p>
<p>Constitutional ai: Harmlessness from ai feedback. Y Bai, S Kadavath, S Kundu, arXiv:2212.080732022arXiv preprintOnline</p>
<p>Gpt-4o system card. A Openai, A Hurst, Lerer, arXiv:2410.212762024arXiv preprint</p>
<p>Openai o3-mini system card. Openai, 2025Tech. Rep.Ope-nAI</p>
<p>G Team, T Mesnard, C Hardin, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
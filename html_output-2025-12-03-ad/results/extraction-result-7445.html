<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7445 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7445</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7445</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-274116841</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.10129v1.pdf" target="_blank">Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation</a></p>
                <p><strong>Paper Abstract:</strong> Generating accurate code review comments remains a significant challenge due to the inherently diverse and non-unique nature of the task output. Large language models pretrained on both programming and natural language data tend to perform well in code-oriented tasks. However, large-scale pretraining is not always feasible due to its environmental impact and project-specific generalizability issues. In this work, first we fine-tune open-source Large language models (LLM) in parameter-efficient, quantized low-rank (QLoRA) fashion on consumer-grade hardware to improve review comment generation. Recent studies demonstrate the efficacy of augmenting semantic metadata information into prompts to boost performance in other code-related tasks. To explore this in code review activities, we also prompt proprietary, closed-source LLMs augmenting the input code patch with function call graphs and code summaries. Both of our strategies improve the review comment generation performance, with function call graph augmented few-shot prompting on the GPT-3.5 model surpassing the pretrained baseline by around 90% BLEU-4 score on the CodeReviewer dataset. Moreover, few-shot prompted Gemini-1.0 Pro, QLoRA fine-tuned Code Llama and Llama 3.1 models achieve competitive results (ranging from 25% to 83% performance improvement) on this task. An additional human evaluation study further validates our experimental findings, reflecting real-world developers' perceptions of LLM-generated code review comments based on relevant qualitative metrics.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7445.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7445.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>5-shot GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 Turbo few-shot prompting (5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Few-shot in-context prompting (5 exemplars retrieved with BM25) presented together with a code diff query to GPT-3.5 Turbo; reported large improvement over the pretrained CodeReviewer baseline on code review comment generation (BLEU-4 and BERTScore reported).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only, instruction-tuned GPT-series model accessed via OpenAI API and used in a few-shot in-context prompting setup.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code review comment generation (RCG) on CodeReviewer dataset</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a code diff (patch, old file), generate a concise formal code review comment identifying issues/suggestions for the change.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot natural-language prompt containing exemplars (input: code diff, output: review comment) plus a test query (code diff). Exemplars were retrieved by BM25.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / input modality (code snippet + exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>5-shot exemplars in prompt (k=5), exemplars retrieved by BM25 from training set, temperature = 0.7, top-n = 5 outputs sampled and best kept (BLEU-selected), max generation tokens = 100, prompt included code diff and exemplar pairs; instruction string sometimes appended (see paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4 (and BERTScore reported)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>BLEU-4 = 8.13; BERTScore = 0.8509</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Pretrained CodeReviewer baseline (pretraining baseline reported in paper) — paper reports relative improvement vs that baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+89.95% (relative improvement in BLEU-4 vs pretrained baseline, as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>OpenAI GPT-3.5-Turbo via API; context window 4,096 tokens; temperature 0.7; k=5 exemplars (BM25 retrieval); top_n=5 outputs, keep best BLEU; max_tokens=100.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7445.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7445.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Few-shot prompting of GPT-4o (multimodal flagship model) on the code review generation task; performance improved over the pretrained baseline but was below GPT-3.5 Turbo in reported BLEU-4; benefited from adding large-context augmentations (call graph + summary) due to larger context window.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI multimodal/omni model with very large context window used in few-shot prompting experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code review comment generation (RCG) on CodeReviewer dataset</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate a single-sentence formal code review comment from a code diff.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot natural-language prompt with exemplars + code diff test query; some experiments included additional structured semantic metadata (call graph, code summary).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / input modality (code snippet + exemplars + optional semantic augmentations)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>k = 5 exemplars used for GPT-4o experiments (paper reports tuning with k=3,5 and final choice k=5 for GPT models), temperature = 0.7, max_tokens = 100, top_n selection as for other GPT experiments; when augmented with call graph + code summary GPT-4o could ingest larger context (paper notes 128k token capacity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4 (and BERTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>BLEU-4 = 6.92 (reported); relative improvements vs baseline also reported</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Pretrained CodeReviewer baseline (paper reports relative improvement vs that baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+61.68% (relative improvement in BLEU-4 vs pretrained baseline, as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>OpenAI GPT-4o via API; reported context handling up to 128k tokens; temperature 0.7; k=5; max_tokens=100.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7445.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7445.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.0Pro few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Few-shot prompting of Google DeepMind's Gemini-1.0Pro on code review comment generation; achieved competitive BLEU-4 and BERTScore improvements over baseline but below GPT-3.5 Turbo in absolute BLEU-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.0Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google DeepMind multimodal LLM with very large context capability (paper notes 32k token context for Gemini-1.0Pro) used via API in few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code review comment generation (RCG) on CodeReviewer dataset</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce a concise review comment from a code diff example.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot natural-language prompt containing exemplars + code diff query; exemplars retrieved by BM25; paper used slightly different k for Gemini (k=3 reported in experimental setup for Gemini).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / input modality</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Paper reports experimenting temps 0.5 and 0.7 and few-shot counts k=3,5; for Gemini the authors used k=3 in some runs (they state 'except setting few-shot count k = 3 to prompt Gemini-1.0Pro'), temperature 0.7 for reported results, max_tokens=100, top_n outputs selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4 (and BERTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>BLEU-4 = 7.85; BERTScore = 0.8509 (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Pretrained CodeReviewer baseline (paper reports relative improvement vs that baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+83.41% (relative improvement in BLEU-4 vs pretrained baseline, as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Gemini-1.0Pro via Google API; context window noted as 32k; temperature 0.7; k reported as 3 for Gemini in the paper's setup; max_tokens=100.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7445.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7445.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt augmentation (call graph + summary)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt augmentation with function call graph and code summary</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augmenting few-shot prompts with statically extracted semantic metadata — a function call graph and a CodeT5-generated code summary appended to each exemplar and test query — to provide broader codebase context to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo / GPT-4o / Gemini-1.0Pro (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source decoder-only LLMs prompted with additional structured textual metadata (call graphs and summaries) appended to examples and queries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code review comment generation (RCG)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate review comments given code diffs; prompts include additional semantic metadata (call graph adjacency lists and CodeT5 summaries of the affected function).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompt where each exemplar and the test query are augmented with (1) an extracted function call graph (adjacency list) and (2) a CodeT5-generated natural-language summary of the function around the diff. The augmented prompt thus contains code diff + call graph + summary for exemplars and query.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / semantic augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Call graph extracted via tree-sitter AST traversal (scope resolution removed, external calls excluded). Code summary created by extracting the function context and summarizing with CodeT5; authors experimented with combinations: none, call graph only, summary only, and both. Temperature 0.7; k=5 for GPT models (k=3 for Gemini in some runs); max_tokens=100.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4 and BERTScore (reported per model / condition)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Mixed results: For GPT-4o, augmentation (call graph + summary) improved performance (relative +6.79% BLEU-4 and +0.27% BERTScore). For GPT-3.5 Turbo and Gemini-1.0Pro, combined augmentation degraded performance (authors attribute this to prompt length / token overload for smaller context windows).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Unaugmented few-shot prompt performance for each model (e.g., GPT-3.5 Turbo BLEU-4 = 8.13 unaugmented as reported); the augmented change is reported relative to those unaugmented few-shot runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>GPT-4o: +6.79% BLEU-4, +0.27% BERTScore (relative). GPT-3.5 & Gemini: combined augmentation caused performance degradation (no single-number percent reported for degradation in paper text).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Call graph built per-language via tree-sitter; CodeT5 summaries limited to the function around the diff to fit tokens; explored temp 0.5/0.7 and k values; final reported experiments used temp=0.7, k = 5 for GPTs (k=3 for Gemini in some runs), top_n=5 outputs kept by best BLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7445.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7445.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Call graph (standalone) effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standalone function call graph augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adding only the extracted function call graph to the prompt (without code summary) as semantic augmentation improved generation quality for GPT-3.5 Turbo according to ablation; authors report best BLEU when only call graph added.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo (primary reported beneficiary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only instruction-tuned GPT-3.5 Turbo prompted with few-shot exemplars where each exemplar and the query include only the function call graph as added context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code review comment generation (RCG)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate a concise code review comment from a code diff; model given few-shot examples and a call graph adjacency list for additional context.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompt augmented with function call graph only (no code summary). Exemplars retrieved by BM25; prompt contains code diff + call graph + exemplar outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / semantic augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Ablation used combinations: none, call graph only, summary only, both. For call graph-only condition authors report GPT-3.5 Turbo achieved highest BLEU-4 of 8.36 on Test Subset 1. Temperature = 0.7; k varied (paper used k=5 for GPT-3.5 in main results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>BLEU-4 = 8.36 (GPT-3.5 Turbo with call graph only, reported)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Unaugmented few-shot GPT-3.5 Turbo (BLEU-4 = 8.13 in main 5-shot run); pretrained CodeReviewer baseline referenced elsewhere in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Improved from 8.13 to 8.36 BLEU-4 vs unaugmented few-shot in reported ablation (absolute change +0.23 BLEU-4); authors state this achieves 'over 90% of baseline performance' improvement vs pretrained baseline in text.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Call graphs built by AST/tree-sitter traversal; duplicates and scope resolution removed, external calls excluded; few-shot exemplars k=5 for GPT-3.5 Turbo; temperature 0.7; max_tokens=100.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7445.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7445.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code summary (standalone) effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeT5-generated function summary augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Appending a CodeT5-produced natural-language summary of the function surrounding the diff to the prompt; ablation indicates this augmentation tended to hurt performance (negative effect) for GPT-3.5 Turbo in this task and setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo (evaluated with summary augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only LLM prompted with few-shot exemplars augmented by a CodeT5-generated summary of the affected function (token-chunked if large) appended to exemplar and query.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code review comment generation (RCG)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate concise code review comments from code diffs; prompts include additional natural-language summaries of the impacted function.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompt augmented with a short natural-language code summary (produced by CodeT5) per exemplar and per query.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / semantic augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Summaries generated by extracting the function region via AST, tokenizing with CodeT5 tokenizer, chunking long functions, then summarizing with CodeT5; paper reports that adding only summary often had a negative impact on performance; temperature 0.7; typical k values used as in other experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4 (ablation reported qualitatively as negative impact)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Unaugmented few-shot GPT-3.5 Turbo (BLEU-4 = 8.13 in main runs) or call-graph-only condition as comparator in ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Negative impact relative to unaugmented or call-graph-only prompts (no single-number percent provided in paper text); authors state 'code summary mostly affects the result negatively.'</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Code summary generated by CodeT5 on extracted function context; AST extraction via tree-sitter; temperature 0.7; k as per model experiment (k=5 for GPT-3.5 main runs).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7445.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7445.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt structure: instruction + exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction + exemplars (in-context few-shot) prompt structure</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt template composed of optional instruction, k-shot exemplars (input: code diff, output: review), and a test query; authors found that instruction was added selectively (only in GPT-4o runs) to encourage concise output, and exemplar count (k) influenced results with k=5 generally best for GPT models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>All closed-source LLMs evaluated (GPT-3.5 Turbo, GPT-4o, Gemini-1.0Pro)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only LLMs used with instruction-following style prompts containing in-context exemplars (few-shot). For GPT-4o the explicit instruction line was included to constrain output verbosity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code review comment generation (RCG)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce a concise review comment from code diff; prompt communicates task via instruction and examples.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Instruction (optional) + exemplars (k-shot) + query. Exemplars are paired code-diff -> review comment examples retrieved by BM25.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Authors experimented with k in {3,5} and temperatures {0.5,0.7}; selected temp=0.7 and k=5 for main GPT experiments (k=3 for Gemini in some runs). Instruction sentence included only in GPT-4o prompts to restrict verbosity. Exemplars were BM25-retrieved from training set and concatenated in prompt order.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4 and BERTScore reported in paper; human qualitative metrics also used</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot or unaugmented prompts (implicit baseline); pretrained CodeReviewer baseline used as dataset baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Paper reports that few-shot prompting (properly constructed) leads to large improvements over pretrained baseline (examples: GPT-3.5 Turbo +5-shot = +89.95% BLEU-4). Authors found k=5 gave better results than k=3 for GPT models in their tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Exemplar retrieval by BM25; temp tuned (0.5/0.7); final temp = 0.7; k chosen per-model (k=5 for GPT-3.5 and GPT-4o main results; k=3 for Gemini in some runs); top_n=5 outputs then BLEU-selected.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gpt-3.5 for code review automation: How do few-shot learning, prompt design, and model finetuning impact their performance <em>(Rating: 2)</em></li>
                <li>Automatic semantic augmentation of language model prompts (for code summarization) <em>(Rating: 2)</em></li>
                <li>What makes good in-context examples for GPT-3? <em>(Rating: 2)</em></li>
                <li>What makes good in-context demonstrations for code intelligence tasks with LLMs? <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7445",
    "paper_id": "paper-274116841",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "5-shot GPT-3.5 Turbo",
            "name_full": "GPT-3.5 Turbo few-shot prompting (5-shot)",
            "brief_description": "Few-shot in-context prompting (5 exemplars retrieved with BM25) presented together with a code diff query to GPT-3.5 Turbo; reported large improvement over the pretrained CodeReviewer baseline on code review comment generation (BLEU-4 and BERTScore reported).",
            "citation_title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_description": "Decoder-only, instruction-tuned GPT-series model accessed via OpenAI API and used in a few-shot in-context prompting setup.",
            "model_size": null,
            "task_name": "Code review comment generation (RCG) on CodeReviewer dataset",
            "task_description": "Given a code diff (patch, old file), generate a concise formal code review comment identifying issues/suggestions for the change.",
            "problem_format": "Few-shot natural-language prompt containing exemplars (input: code diff, output: review comment) plus a test query (code diff). Exemplars were retrieved by BM25.",
            "format_category": "prompt style / input modality (code snippet + exemplars)",
            "format_details": "5-shot exemplars in prompt (k=5), exemplars retrieved by BM25 from training set, temperature = 0.7, top-n = 5 outputs sampled and best kept (BLEU-selected), max generation tokens = 100, prompt included code diff and exemplar pairs; instruction string sometimes appended (see paper).",
            "performance_metric": "BLEU-4 (and BERTScore reported)",
            "performance_value": "BLEU-4 = 8.13; BERTScore = 0.8509",
            "baseline_performance": "Pretrained CodeReviewer baseline (pretraining baseline reported in paper) — paper reports relative improvement vs that baseline",
            "performance_change": "+89.95% (relative improvement in BLEU-4 vs pretrained baseline, as reported)",
            "experimental_setting": "OpenAI GPT-3.5-Turbo via API; context window 4,096 tokens; temperature 0.7; k=5 exemplars (BM25 retrieval); top_n=5 outputs, keep best BLEU; max_tokens=100.",
            "statistical_significance": null,
            "uuid": "e7445.0",
            "source_info": {
                "paper_title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GPT-4o few-shot",
            "name_full": "GPT-4o few-shot prompting",
            "brief_description": "Few-shot prompting of GPT-4o (multimodal flagship model) on the code review generation task; performance improved over the pretrained baseline but was below GPT-3.5 Turbo in reported BLEU-4; benefited from adding large-context augmentations (call graph + summary) due to larger context window.",
            "citation_title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "OpenAI multimodal/omni model with very large context window used in few-shot prompting experiments.",
            "model_size": null,
            "task_name": "Code review comment generation (RCG) on CodeReviewer dataset",
            "task_description": "Generate a single-sentence formal code review comment from a code diff.",
            "problem_format": "Few-shot natural-language prompt with exemplars + code diff test query; some experiments included additional structured semantic metadata (call graph, code summary).",
            "format_category": "prompt style / input modality (code snippet + exemplars + optional semantic augmentations)",
            "format_details": "k = 5 exemplars used for GPT-4o experiments (paper reports tuning with k=3,5 and final choice k=5 for GPT models), temperature = 0.7, max_tokens = 100, top_n selection as for other GPT experiments; when augmented with call graph + code summary GPT-4o could ingest larger context (paper notes 128k token capacity).",
            "performance_metric": "BLEU-4 (and BERTScore)",
            "performance_value": "BLEU-4 = 6.92 (reported); relative improvements vs baseline also reported",
            "baseline_performance": "Pretrained CodeReviewer baseline (paper reports relative improvement vs that baseline)",
            "performance_change": "+61.68% (relative improvement in BLEU-4 vs pretrained baseline, as reported)",
            "experimental_setting": "OpenAI GPT-4o via API; reported context handling up to 128k tokens; temperature 0.7; k=5; max_tokens=100.",
            "statistical_significance": null,
            "uuid": "e7445.1",
            "source_info": {
                "paper_title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Gemini few-shot",
            "name_full": "Gemini-1.0Pro few-shot prompting",
            "brief_description": "Few-shot prompting of Google DeepMind's Gemini-1.0Pro on code review comment generation; achieved competitive BLEU-4 and BERTScore improvements over baseline but below GPT-3.5 Turbo in absolute BLEU-4.",
            "citation_title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation",
            "mention_or_use": "use",
            "model_name": "Gemini-1.0Pro",
            "model_description": "Google DeepMind multimodal LLM with very large context capability (paper notes 32k token context for Gemini-1.0Pro) used via API in few-shot prompting.",
            "model_size": null,
            "task_name": "Code review comment generation (RCG) on CodeReviewer dataset",
            "task_description": "Produce a concise review comment from a code diff example.",
            "problem_format": "Few-shot natural-language prompt containing exemplars + code diff query; exemplars retrieved by BM25; paper used slightly different k for Gemini (k=3 reported in experimental setup for Gemini).",
            "format_category": "prompt style / input modality",
            "format_details": "Paper reports experimenting temps 0.5 and 0.7 and few-shot counts k=3,5; for Gemini the authors used k=3 in some runs (they state 'except setting few-shot count k = 3 to prompt Gemini-1.0Pro'), temperature 0.7 for reported results, max_tokens=100, top_n outputs selection.",
            "performance_metric": "BLEU-4 (and BERTScore)",
            "performance_value": "BLEU-4 = 7.85; BERTScore = 0.8509 (reported)",
            "baseline_performance": "Pretrained CodeReviewer baseline (paper reports relative improvement vs that baseline)",
            "performance_change": "+83.41% (relative improvement in BLEU-4 vs pretrained baseline, as reported)",
            "experimental_setting": "Gemini-1.0Pro via Google API; context window noted as 32k; temperature 0.7; k reported as 3 for Gemini in the paper's setup; max_tokens=100.",
            "statistical_significance": null,
            "uuid": "e7445.2",
            "source_info": {
                "paper_title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Prompt augmentation (call graph + summary)",
            "name_full": "Prompt augmentation with function call graph and code summary",
            "brief_description": "Augmenting few-shot prompts with statically extracted semantic metadata — a function call graph and a CodeT5-generated code summary appended to each exemplar and test query — to provide broader codebase context to the LLM.",
            "citation_title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo / GPT-4o / Gemini-1.0Pro (evaluated)",
            "model_description": "Closed-source decoder-only LLMs prompted with additional structured textual metadata (call graphs and summaries) appended to examples and queries.",
            "model_size": null,
            "task_name": "Code review comment generation (RCG)",
            "task_description": "Generate review comments given code diffs; prompts include additional semantic metadata (call graph adjacency lists and CodeT5 summaries of the affected function).",
            "problem_format": "Few-shot prompt where each exemplar and the test query are augmented with (1) an extracted function call graph (adjacency list) and (2) a CodeT5-generated natural-language summary of the function around the diff. The augmented prompt thus contains code diff + call graph + summary for exemplars and query.",
            "format_category": "prompt style / semantic augmentation",
            "format_details": "Call graph extracted via tree-sitter AST traversal (scope resolution removed, external calls excluded). Code summary created by extracting the function context and summarizing with CodeT5; authors experimented with combinations: none, call graph only, summary only, and both. Temperature 0.7; k=5 for GPT models (k=3 for Gemini in some runs); max_tokens=100.",
            "performance_metric": "BLEU-4 and BERTScore (reported per model / condition)",
            "performance_value": "Mixed results: For GPT-4o, augmentation (call graph + summary) improved performance (relative +6.79% BLEU-4 and +0.27% BERTScore). For GPT-3.5 Turbo and Gemini-1.0Pro, combined augmentation degraded performance (authors attribute this to prompt length / token overload for smaller context windows).",
            "baseline_performance": "Unaugmented few-shot prompt performance for each model (e.g., GPT-3.5 Turbo BLEU-4 = 8.13 unaugmented as reported); the augmented change is reported relative to those unaugmented few-shot runs.",
            "performance_change": "GPT-4o: +6.79% BLEU-4, +0.27% BERTScore (relative). GPT-3.5 & Gemini: combined augmentation caused performance degradation (no single-number percent reported for degradation in paper text).",
            "experimental_setting": "Call graph built per-language via tree-sitter; CodeT5 summaries limited to the function around the diff to fit tokens; explored temp 0.5/0.7 and k values; final reported experiments used temp=0.7, k = 5 for GPTs (k=3 for Gemini in some runs), top_n=5 outputs kept by best BLEU.",
            "statistical_significance": null,
            "uuid": "e7445.3",
            "source_info": {
                "paper_title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Call graph (standalone) effect",
            "name_full": "Standalone function call graph augmentation",
            "brief_description": "Adding only the extracted function call graph to the prompt (without code summary) as semantic augmentation improved generation quality for GPT-3.5 Turbo according to ablation; authors report best BLEU when only call graph added.",
            "citation_title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo (primary reported beneficiary)",
            "model_description": "Decoder-only instruction-tuned GPT-3.5 Turbo prompted with few-shot exemplars where each exemplar and the query include only the function call graph as added context.",
            "model_size": null,
            "task_name": "Code review comment generation (RCG)",
            "task_description": "Generate a concise code review comment from a code diff; model given few-shot examples and a call graph adjacency list for additional context.",
            "problem_format": "Few-shot prompt augmented with function call graph only (no code summary). Exemplars retrieved by BM25; prompt contains code diff + call graph + exemplar outputs.",
            "format_category": "prompt style / semantic augmentation",
            "format_details": "Ablation used combinations: none, call graph only, summary only, both. For call graph-only condition authors report GPT-3.5 Turbo achieved highest BLEU-4 of 8.36 on Test Subset 1. Temperature = 0.7; k varied (paper used k=5 for GPT-3.5 in main results).",
            "performance_metric": "BLEU-4",
            "performance_value": "BLEU-4 = 8.36 (GPT-3.5 Turbo with call graph only, reported)",
            "baseline_performance": "Unaugmented few-shot GPT-3.5 Turbo (BLEU-4 = 8.13 in main 5-shot run); pretrained CodeReviewer baseline referenced elsewhere in paper.",
            "performance_change": "Improved from 8.13 to 8.36 BLEU-4 vs unaugmented few-shot in reported ablation (absolute change +0.23 BLEU-4); authors state this achieves 'over 90% of baseline performance' improvement vs pretrained baseline in text.",
            "experimental_setting": "Call graphs built by AST/tree-sitter traversal; duplicates and scope resolution removed, external calls excluded; few-shot exemplars k=5 for GPT-3.5 Turbo; temperature 0.7; max_tokens=100.",
            "statistical_significance": null,
            "uuid": "e7445.4",
            "source_info": {
                "paper_title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Code summary (standalone) effect",
            "name_full": "CodeT5-generated function summary augmentation",
            "brief_description": "Appending a CodeT5-produced natural-language summary of the function surrounding the diff to the prompt; ablation indicates this augmentation tended to hurt performance (negative effect) for GPT-3.5 Turbo in this task and setting.",
            "citation_title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo (evaluated with summary augmentation)",
            "model_description": "Decoder-only LLM prompted with few-shot exemplars augmented by a CodeT5-generated summary of the affected function (token-chunked if large) appended to exemplar and query.",
            "model_size": null,
            "task_name": "Code review comment generation (RCG)",
            "task_description": "Generate concise code review comments from code diffs; prompts include additional natural-language summaries of the impacted function.",
            "problem_format": "Few-shot prompt augmented with a short natural-language code summary (produced by CodeT5) per exemplar and per query.",
            "format_category": "prompt style / semantic augmentation",
            "format_details": "Summaries generated by extracting the function region via AST, tokenizing with CodeT5 tokenizer, chunking long functions, then summarizing with CodeT5; paper reports that adding only summary often had a negative impact on performance; temperature 0.7; typical k values used as in other experiments.",
            "performance_metric": "BLEU-4 (ablation reported qualitatively as negative impact)",
            "performance_value": null,
            "baseline_performance": "Unaugmented few-shot GPT-3.5 Turbo (BLEU-4 = 8.13 in main runs) or call-graph-only condition as comparator in ablation.",
            "performance_change": "Negative impact relative to unaugmented or call-graph-only prompts (no single-number percent provided in paper text); authors state 'code summary mostly affects the result negatively.'",
            "experimental_setting": "Code summary generated by CodeT5 on extracted function context; AST extraction via tree-sitter; temperature 0.7; k as per model experiment (k=5 for GPT-3.5 main runs).",
            "statistical_significance": null,
            "uuid": "e7445.5",
            "source_info": {
                "paper_title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Prompt structure: instruction + exemplars",
            "name_full": "Instruction + exemplars (in-context few-shot) prompt structure",
            "brief_description": "Prompt template composed of optional instruction, k-shot exemplars (input: code diff, output: review), and a test query; authors found that instruction was added selectively (only in GPT-4o runs) to encourage concise output, and exemplar count (k) influenced results with k=5 generally best for GPT models.",
            "citation_title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation",
            "mention_or_use": "use",
            "model_name": "All closed-source LLMs evaluated (GPT-3.5 Turbo, GPT-4o, Gemini-1.0Pro)",
            "model_description": "Decoder-only LLMs used with instruction-following style prompts containing in-context exemplars (few-shot). For GPT-4o the explicit instruction line was included to constrain output verbosity.",
            "model_size": null,
            "task_name": "Code review comment generation (RCG)",
            "task_description": "Produce a concise review comment from code diff; prompt communicates task via instruction and examples.",
            "problem_format": "Instruction (optional) + exemplars (k-shot) + query. Exemplars are paired code-diff -&gt; review comment examples retrieved by BM25.",
            "format_category": "prompt style / question type",
            "format_details": "Authors experimented with k in {3,5} and temperatures {0.5,0.7}; selected temp=0.7 and k=5 for main GPT experiments (k=3 for Gemini in some runs). Instruction sentence included only in GPT-4o prompts to restrict verbosity. Exemplars were BM25-retrieved from training set and concatenated in prompt order.",
            "performance_metric": "BLEU-4 and BERTScore reported in paper; human qualitative metrics also used",
            "performance_value": null,
            "baseline_performance": "Zero-shot or unaugmented prompts (implicit baseline); pretrained CodeReviewer baseline used as dataset baseline.",
            "performance_change": "Paper reports that few-shot prompting (properly constructed) leads to large improvements over pretrained baseline (examples: GPT-3.5 Turbo +5-shot = +89.95% BLEU-4). Authors found k=5 gave better results than k=3 for GPT models in their tuning.",
            "experimental_setting": "Exemplar retrieval by BM25; temp tuned (0.5/0.7); final temp = 0.7; k chosen per-model (k=5 for GPT-3.5 and GPT-4o main results; k=3 for Gemini in some runs); top_n=5 outputs then BLEU-selected.",
            "statistical_significance": null,
            "uuid": "e7445.6",
            "source_info": {
                "paper_title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gpt-3.5 for code review automation: How do few-shot learning, prompt design, and model finetuning impact their performance",
            "rating": 2,
            "sanitized_title": "gpt35_for_code_review_automation_how_do_fewshot_learning_prompt_design_and_model_finetuning_impact_their_performance"
        },
        {
            "paper_title": "Automatic semantic augmentation of language model prompts (for code summarization)",
            "rating": 2,
            "sanitized_title": "automatic_semantic_augmentation_of_language_model_prompts_for_code_summarization"
        },
        {
            "paper_title": "What makes good in-context examples for GPT-3?",
            "rating": 2,
            "sanitized_title": "what_makes_good_incontext_examples_for_gpt3"
        },
        {
            "paper_title": "What makes good in-context demonstrations for code intelligence tasks with LLMs?",
            "rating": 2,
            "sanitized_title": "what_makes_good_incontext_demonstrations_for_code_intelligence_tasks_with_llms"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.018745,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation
15 Nov 2024</p>
<p>Md Asif Haider 
Bangladesh University of Engineering and Technology
DhakaBangladesh</p>
<p>Ayesha Binte Mostofa 
Bangladesh University of Engineering and Technology
DhakaBangladesh</p>
<p>Sk Sabit Bin Mosaddek 
Bangladesh University of Engineering and Technology
DhakaBangladesh</p>
<p>Anindya Iqbal anindya@cse.buet.ac.bd 
Bangladesh University of Engineering and Technology
DhakaBangladesh</p>
<p>Toufique Ahmed tfahmed@ucdavis.edu 
University of California
DavisUSA</p>
<p>Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation
15 Nov 20246513F4BD478CB140571B80A0126CADEEarXiv:2411.10129v1[cs.SE]
Generating accurate code review comments remains a significant challenge due to the inherently diverse and nonunique nature of the task output.Large language models pretrained on both programming and natural language data tend to perform well in code-oriented tasks.However, largescale pretraining is not always feasible due to its environmental impact and project-specific generalizability issues.In this work, first we fine-tune open-source Large language models (LLM) in parameter-efficient, quantized low-rank (QLoRA) fashion on consumer grade hardware to improve review comment generation.Recent studies demonstrate the efficacy of augmenting semantic metadata information into prompts to boost performance in other code-related tasks.To explore this in code review activities, we also prompt proprietary, closed-source LLMs augmenting the input code patch with function call graphs and code summaries.Both of our strategies improve the review comment generation performance, with function call graph augmented fewshot prompting on the GPT-3.5 model surpassing the pretrained baseline by around 90% BLEU-4 score on the CodeReviewer dataset.Moreover, few-shot prompted Gemini-1.0Pro, QLoRA fine-tuned Code Llama and Llama 3.1 models achieve competitive results (ranging from 25% to 83% performance improvement) on this task.An additional human evaluation study further validates our experimental findings, reflecting real-world developers' perceptions of LLM-generated code review comments based on relevant qualitative metrics.Index Terms-code review comments, quantized low-rank finetuning, few-shot prompting, function call graph, large language models * All three authors have equal contributions.</p>
<p>I. INTRODUCTION</p>
<p>Code review, the manual process of inspecting authored source code by fellow teammates, is a crucial part of the software development lifecycle that helps detect errors and encourages further code improvement [1].First formalized by Fagan [2], it is a systematic and collaborative software quality assurance activity where developers check each other's code for improvement.Code reviews not only help in identifying bugs and potential issues early in the development cycle but also enhance code readability, maintainability, and overall software quality.However, despite the numerous benefits, the traditional review process demands significant manual effort, forcing developers to spend an excessive amount of time (more than 6 hours per week) reviewing their peers' code, as shown in [3], [4].It is also responsible for frequent context switch of the developers from the actual tasks they are expected to focus on [5].Hence, automating code review activities is in significant demand.One distinct task stands out in the Modern Code Review (MCR) practices: Review Comment Generation (RCG), which can help reduce the burden from a code reviewer, automatically suggesting a potential change in the code submitted for review.We focus on improving the automation performance of review comment generation task in this study.</p>
<p>With the rapid advances in deep learning and natural language processing techniques, researchers proposed many Pretrained Language Models (PLM) on source code focusing on review tasks, notably including encoder-only BERT models [6], [7] and encoder-decoder based T5 models [8]- [10].Novel pretraining and fine-tuning attempts on large-scale datasets showed promising results.However, training on such domain-specific huge datasets requires a substantial amount of costly computing resources, imposing a negative impact on the carbon footprint globally [11].While these models can usually generalize well, they might lack deep knowledge of project specific codebases, organizational coding standards, or niche libraries.This can lead to generic or less relevant code review suggestions, missing context-specific nuances.</p>
<p>However, decoder-only unified language models (e.g. based on GPT architecture) have shown superior performance when scaled to large parameter sizes.Also generally known as LLMs, these models can reduce the need for repetitive training while offering amazing few-shot learning capabilities [12].This refers to prompt engineering of the model with a few similar query-response pairs, also known as Few Shot Learning.Designing efficient LLM prompts for the mentioned task yet remains less explored, motivating us toward this research direction.</p>
<p>Apart from proprietary LLMs, there has been a lot of work going on in the open-source landscape.General purpose open-source LLMs (e.g.Llama, Mistral) when fine-tuned, show performance improvement over PLMs.LLMs further trained on code-specific data, also known as Code LLMs (e.g.Codex, Code Llama) are currently the superior options for various code-related subtasks (including code generation, code summarization) [13].The best-performing versions of these LLMs nearly have 30-70B parameters, which is quite impossible to fit into a consumer grade GPU having around 16GB VRAM.Hence, fine-tuning the smaller versions of these LLMs (7-8B) is a promising cost-effective strategy to ensure project-specific, context-aware use cases.Parameter Efficient Fine-Tuning (PEFT) approaches (Low-Rank Adaptation, 4-bit Quantization) are found to assist in such endeavors [14].</p>
<p>Augmenting statically analyzed, semantic structural facts to prompt the code model proved to be beneficial in code summarization tasks [15].Inspired by this, we propose a new methodology to design cost-effective few-shot prompts for proprietary LLMs, augmented with a programming language component-function call graph and a natural language component-code summary.We also explore further ablation studies to understand their standalone contributions for code review comment generation task.Additionally, we fine-tune open-source general-purpose LLMs and code LLMs in automating review comment generation task in a low resource-constrained setting.Specifically, we leverage the Quantized Low-Rank Adaptation (QLoRA) approach to fine-tune our models in a supervised way.To summarize, we particularly investigate the following research questions in this study: RQ1: How effective is code review comment generation using fine-tuned open-source Large Language Models?</p>
<p>RQ2: How well do the closed-source Large Language Models perform in code review comment generation task when prompt engineered in a few-shot setting?</p>
<p>RQ3: When incorporated into prompts, what are the impacts of function call graph and code summary in improving review comment generation performance?RQ4: How effective Large Language Models are in generating review comments from a real-world developer's perspective?</p>
<p>Our contributions can be summarized as follow:</p>
<p>• Evaluating code review comment generation performance with open-source LLMs (Llama 2, Code Llama, Llama 3 series) in quantized, low-rank adapted parameter-efficient fine-tuning setup</p>
<p>• Exploring the performance of different closed-source, proprietary LLMs (GPT 3.5-Turbo, GPT-4o, Gemini-1.0Pro) in few-shot prompt setting without any further data augmentation</p>
<p>• Investigating the impact of incorporating manually extracted function call graph and CodeT5 model generated code summary into the few-shot prompts to observe their impact on review comment generation performance</p>
<p>• Manual analysis and a developer study focusing on evaluating the LLM-generated review comments based on relevant qualitative metrics</p>
<p>• A replication package with all the scripts for data, code and result processing for our study, which can be found here.</p>
<p>II. BACKGROUND</p>
<p>We provide a brief overview of LLMs, few-shot prompting and parameter-efficient QLoRA fine-tuning approaches.</p>
<p>A. Large Language Models</p>
<p>Large Language Models (LLM) represent a novel class of computational models with millions and billions of trained model parameters designed to process, understand, and generate human-like text.LLMs for software engineering tasks are typically divided into two types: general-purpose models and code-focused models.General-purpose models, such as the GPT series and Gemini [16], are designed for a broad range of tasks, including question-answering, summarizing, and generating dialogues.Code-specific LLMs, like Code Llama [17], are specialized in programming languages and excel in generating, completing, and debugging code.Built on the foundation of the Transformer architecture with multi-headed attention for efficient sequence processing, these models have evolved significantly since 2017.Transformers replaced RNNs, enabling better long-range dependency handling and allowing models like GPT to scale to trillions of parameters and multimodal capabilities.Gemini, based on Google's T5 [18] model, has further advanced multi-modal capabilities, processing text, images, audio, and video with an extended context window, making it valuable for tasks like code summarization and refinement.</p>
<p>B. Few-shot Prompting</p>
<p>Until 2020, fine-tuning was pre-dominant for adapting the models to a specific task.However, with advancements in LLMs, prompt engineering has emerged as an efficient alternative [19]- [22].A prompt is a structured natural language input that directs Large Language Models (LLMs) to execute a specified task, incorporating instructions and contextual information to enhance output relevance of test queries.While Large Language Models can perform well with zero-shot, they achieve better performance with few-shot prompting [12].This technique introduces a limited number of labeled examples within the prompt as contextual information, illustrating the input-output relationship to guide the model's responses.Top few-shot context suited for the test query can be retrieved from the training dataset utilizing a separate ranking algorithms.</p>
<p>C. QLoRA : Parameter Efficient Fine-Tuning</p>
<p>Modern Large Language Models have billions of parameters, hence fine tuning these models requires a vast amount Fig. 1.Overview of the methodology of memory and high-end GPU machines.A full finetuning process can also be extremely time and energy consuming, as it involves training all the layers and parameters with task-specific data.Several parameter-efficient fine-tuning techniques, also known collectively as PEFT methods [23]- [25] have been proposed and applied to decrease the memory usage and training cost while maintaining the accuracy of the fine-tuned model to a reasonable extent [14].Quantized LoRA, best known as QLoRA [26] is a quantized version of LoRA that introduces quantizing the transformer model to 4-bit NormalFloat (NF4) precision with double quantization processing from typical 16-bit FloatingPoint (FP16).It also utilizes a paged optimizer to deal with memory spikes seen when training with longer batches, eventually making finetuning possible with more limited computational resources.</p>
<p>III. METHODOLOGY</p>
<p>Figure 1 provides a brief graphical overview of our study.We collected review comments, along with the code snippets before and after code review from the CodeReviewer [9] dataset.To answer RQ1, we fine-tuned models from Meta Llama series (Llama 2, Code Llama, Llama 3, Llama 3.1, Llama 3.2).For addressing RQ2 and RQ3, we fewshot prompted models from GPT (GPT-3.5 Turbo, GPT 4o) and Gemini (Gemini-1.0Pro) series, and experimented with different prompt augmentation strategies.Finally, we invited a group of software developers from industry to participate in an anonymous human evaluation study designed to answer our proposed RQ4.The following subsections explain each of these steps in detail.</p>
<p>A. Dataset</p>
<p>We used the CodeReviewer [9] dataset for all of our experiments.Introduced by Microsoft, this dataset was collected from publicly available high-quality open-source repositories.It covers nine most popular languages including C, C++, C#, Go, Java, JavaScript, PHP, Python, and Ruby.This dataset was processed for three downstream tasks (code change quality estimation, review comment generation, and code refinement).As our experiment focuses on review comment generation task, this dataset fits well with our experiments.The dataset was already split into three parts (Training, Validation, and Test data).As data from the same project can result in biased test and validation data, the dataset was split at the project level.Hence, there is no correlation between the three parts of the dataset.We separated all the necessary fields of the dataset for our experiment, including old file (the file before the pull request), code diff, and a review comment.</p>
<p>We intended not to utilize the whole test dataset for evaluation to keep our prompting cost within limit, as closed-source LLMs like GPT-3.5 and GPT-4 series impose a significant amount of cost overhead when applied to such a huge dataset with thousands of entries.Consequently, for our experiment purpose, we randomly picked 5000 and 500 samples from the whole test set and only evaluated further prompting, fine- tuning, and ablation study using these two test subsets.Table I shows the overview of the dataset used in our experiments.Wilcoxon-signed rank test conducted on these subsets with results from section IV-A, IV-B and IV-C suggest no statistically significant difference among these different versions of the test set.</p>
<p>B. RQ1: Parameter Efficient Quantized Fine-tuning for RCG</p>
<p>In this section, we describe in detail how we fine-tuned open-source LLMs for the review comment generation task.We utilized QLoRA, an effective parameter-efficient finetuning (PEFT) technique that combines together the idea of working with low-rank matrices and 4-bit quantization.The motivation for going with the PEFT approach was to ensure fine-tuning in a constrained, low-resource setting which can be achieved with a consumer grade GPU configuration.</p>
<p>1) Data Preprocessing for Supervised Fine-tuning: LLMs, when trained to follow some specific form of instructions tend to show superior performance in practice.Fine-tuning on a diverse range of multi-task datasets with rich natural language instructions has been proven to demonstrate performance enhancement on completely unseen tasks [27], [28].Hence in this work, we modified the dataset to contain a suitable instruction-following prompt for the code review comment generation task.We crafted our template inspired by Stanford Alpaca [29] and used the original dataset from CodeReviewer [9] to perform fine-tuning in a supervised fashion.The modified data structure follows the {instruction, input, output} format, similar to the framework introduced in [30].We show our prompt template and corresponding sample instruction, input, and output in Figure 2.</p>
<p>2) Fine-tuning Open-Source Models: Using the instructionfollowing dataset that we prepared, we fine-tuned various generations and variants of the Llama, the leading LLM series by Meta in the open-source scenario.Llama 2 [31], the direct successor of the Meta Llama model was pretrained on 2 trillion tokens.Code Llama [17] was built on top of Llama 2 after training with fill-in-the-middle capabilities on code-specific datasets.We fine-tuned the 7B base version of both models for our task.Llama 3 1 even surpassed the previous models in the Llama series as openly accessible 1 https://github.com/meta-llama/llama3Fig. 2. Prompt template for supervised fine-tuning with instruction, input, output specified LLMs, followed by Llama 3.1 (with a greater window of 128k tokens) and Llama 3.2 (lightweight, quantized version).We used the 3B Instruct version for our purpose.</p>
<p>3) Experimental Setup: We implemented our fine-tuning experiment using a couple of popular open-source tools.We used Axolotl for fine-tuning Llama 2 and Code Llama 7B variants with QLoRA adapter for 4-bit quantization.Axolotl2 is a tool designed to streamline the fine-tuning of various AI models, offering support for multiple configurations and architectures.To fine-tune all the Llama 3 models, we used Unsloth3 , which offers faster training and inference speed for more recent LLMs.All experiments were conducted on NVIDIA RTX 5000 GPU machine with 16 GB VRAM.The token length limit was set to 2048.All the models were trained for 2 full epochs (except 5 epochs for Llama 3.2) using a 32-bit paged AdamW optimizer for Llama 2 and Code Llama models, while the Llama 3 series used an 8-bit AdamW optimizer instead.We set the LoRA rank to 32, the scaling factor alpha to 16, and the dropout rate to 0.05.The micro batch size was fixed to 2 and a learning rate of 0.0002 was used with 0.01 weight decay.</p>
<p>C. RQ2: Few-shot Prompting for RCG</p>
<p>In this section, we discuss in detail how we applied prompt engineering to our task and dataset.</p>
<p>1) Prompted Closed-Source Models: We chose three proprietary popular LLMs for our prompting experiments.Two of them, the GPT-3.5 Turbo [12] and GPT-4o [32] models were accessed via OpenAI API, Gemini-1.0Pro [16] model was provided by Google DeepMind API.These decoder-only models generate text by predicting the next token in a sequence given the previous tokens, which makes these models particularly effective for tasks involving text generation and completion.</p>
<p>We chose the instruct version of the GPT-3.5 Turbo, the most cost-effective model in the GPT-3.5 series.It offers a context window of 4,096 tokens and is trained on data up to September 2021.GPT-4 omni, also known as GPT-4o, is the flagship multimodal model of OpenAI with advanced reasoning performance.It is updated with knowledge up to October 2023, providing enhanced capabilities in understanding and generating diverse types of content.We finally picked Gemini-1.0Pro from the Gemini series of advanced multi-modal models developed by Google.It was created to scale across a wide range of tasks with up to 1M input tokens, a limit greater than the other comparable models.Gemini-1.0-prooffers a context window of 32k tokens and is noted for its impressive output speed of 86.8 tokens per second, making it comparatively faster.</p>
<p>2) Few-shot Prompt Design: A prompt is a structured natural language input presented to a language model, designed to receive a specific response tailored to a particular task.While, in many cases, large language models show outstanding performance with zero-shot prompting (without any example), it might need some examples to be provided for complex tasks.Few-shot prompting is a widely used technique to enable in-context learning [33]- [35] where we provide demonstrations in the prompt to steer the model to perform better.Thoughtful, well-crafted prompts are crucial for leveraging the performance of generative models like GPT-3.5 and 4 series.Review Comment Generation is a complex task that requires much focus and contextual knowledge about the code snippet.Generated review comments need to be precise, specially on the parts where the code is to be modified.A comment needs to be informative, relevant and well-explained to denote where to fix issues in the code.In our prompt, we include {Instruction optional + Exemplars + Query test }.Below we discuss these in detail.</p>
<p>• Instruction: Instructions are only added in the GPT-4o chat model, to avoid producing overly long code reviews and encourage concise review comments that are at once relevant and informative to the code changes.We took inspiration for our prompt design from Specificity and Information, Content and Language Style, User Interaction and Engagement and Prompt Structure and Clarity categories as shown in [36], and Emotional Stimulus as presented in [37].We appended "Code Review" tag to indicate the model to complete the desired review comment in the prompt.We also added function call graph and code summary components to each of our k-shot samples (similarly, in the test queries) to address the RQ3, which we discuss in section III-D in detail.</p>
<p>D. RQ3: Impact of Semantic Metadata Augmented Prompts for RCG</p>
<p>In this section, we discuss the key steps of dataset preprocessing to produce function call graph and code summary.Then we present how we incorporated these into our prompting pipeline already discussed in section III-C.</p>
<p>1) Extracting Function Call Graph:</p>
<p>As code diff represents a small part of the whole code base, carrying not much information about the other parts of the code, the challenge remained: how can we enable our model to recognize the gist of the code base?To address this issue, we hypothesized that semantic data flow information, like function call graphs, is an important contributing factor to the deeper understanding of our code base.A function call graph is a control flow graph representing which function is called from other functions.It creates a directed graph where each node represents a function or module and each edge symbolizes the call from one function to another.</p>
<p>To achieve this, we extracted syntactic details from the given code sample first, leveraging Abstract Syntax Tree (AST).AST, in essence, is a data structure that captures the syntactic structure of a program or code.It forms a tree where each node Fig. 3. Prompt designs for few-shot prompting denotes a construct occurring in the code.We parsed each old file code 'oldf' from our dataset to generate an AST to identify key elements like function calls and definitions, using Treesitter 4 , a popular parser generator tool and incremental parsing library with support for multiple programming languages.As shown in Figure 4, we traverse the whole tree to identify the function calls.For each of the nine languages, we built an extractor that extracts the function call relationships, from which we constructed adjacency lists to neatly represent the call graphs.Initially, we included all the function calls with scope resolution operators to distinguish different functions in different modules, but this led to excessively large call graphs, impacting performance.To address this, we chose to remove the scope resolution operators and duplicate function calls.Finally, we excluded external (e.g., library) function calls to maintain focus on the core code structure.Figure 5 shows the overview of these modifications.2) Generating Code Summary: We hypothesized that including code summarization would capture the overall context of code changes in code diffs, improving code review comment generation.Initially, we attempted to summarize the entire code, but token limitations affected the performance.We then focused on summarizing only the function related to the code diffs.Using an AST, we identified the relevant function for each code diff, building extractors for nine languages that extract the start and the end of the function and recognize language-specific function definitions and handling edge cases such as nested functions, and class methods.If the code diff was not inside any function, we extracted the code around the code diff.We tokenized the extracted code using CodeT5's RoBERTa-based tokenizer, splitting larger functions into smaller chunks as needed.These tokenized chunks were then fed into the CodeT5 [8] model, generating summaries that were appended to our prompts to enhance review accuracy.Figure 6 outlines the key steps in this summary generation workflow.</p>
<p>3) Experimental Setup: Here we discuss the model hyperparameters we experimented with.We used OpenAI and Google provided APIs for invoking the experimental models.We explored different model temperatures (temp = 0.5, 0.7) and numbers of few-shot samples (k = 3, 5) to generate review comments.We finally report the results for temp = 0.7, and few-shot count k = 5 as they showed comparatively better results.We also picked top n = 5 outputs by the models, calculated their BLEU scores, and kept the best-performing result only for further comparison.We then follow the same set of hyperparameters (except setting few-shot count k = 3) to prompt the Google Gemini-1.0Pro model as well.The maximum number of tokens to be generated by the model was set to 100.We leave the rest of the available API parameters (frequency and present penalty, top p) to their default values.The structure of all the prompts is shown in Figure 3.</p>
<p>4) Ablation Studies:</p>
<p>To understand individual contributions of function call graph and code summary, we generate all combinations of prompt including without any and both of the augmentations, and augmenting with only call graph and only summary.The details are discussed in the section IV-C.E. RQ4: Real-World Developers Perspectives on LLM Generated Review Comments 1) Web portal Design: We developed a web portal dedicated to present participants with essential components for each assessment: code snippet, code summary generated from the snippet, ground truth provided as a reference, and code reviews generated by multiple models, with model names anonymized to prevent bias.The reviewers were asked to rank the model generated comments based on three of the qualitative metrics found in literature: relevence score, information score and explanation clarity score [9].To avoid any misunderstanding, the instructions were presented on a separate page, ensuring that participants could review the guidelines thoroughly before proceeding with the evaluation.The backend of the portal was implemented using Node.js with the Express framework, while the frontend was developed with React.js and TypeScript to provide a responsive and interactive user interface.Participant data and study feedback were stored securely in a PostgreSQL database hosted on Render.</p>
<p>2) Participants: We promoted the web portal among the professional software engineering community around the authors' network, as the study focuses on code review tasks.Participants were recruited on a rolling basis to ensure a diverse sample to capture a broad spectrum of programming skills and domain knowledge.Each participant was compensated for their time and feedback.We recruited 8 participants, each affiliated with reputed software industry companies in the country.Each code review example was evaluated twice, with feedback from two distinct participants, to enhance the reliability and robustness of our analysis.</p>
<p>F. Evaluation Metrics</p>
<p>We provide a brief description of the evaluation metrics used for the review comment generation task in this section.</p>
<p>• BLEU [39] (Bilingual Evaluation Understudy) is one of the widely acknowledged metrics to measure the text generation performance of language models.BLEU-4 is the weighted geometric mean of all the modified 4-gram precisions, scaled by the brevity penalty.The modified n-gram precision used here adjusts for cases where the generated candidate text may have n-grams that do not perfectly match any n-grams in the reference texts.</p>
<p>• BERTScore [40] leverages pre-trained contextual embeddings from the BERT [41] model and finds out the average F1 score, which is a harmonic mean of precision and recall, providing a single metric to assess the quality of generated sentences compared to the ground truth.It has been shown to correlate with human judgment better in terms of sentence-level similarity instead of word-level matching.A higher BERTScore indicates better similarity between the predicted sentences and the reference sentences.</p>
<p>• Human Evaluation metrics include the following scores:</p>
<p>-Relevance score measures how much the review comment aligns with the overall content.-Information score assesses the completeness of the information provided in the review comment.-Explanation clarity score evaluates the clarity of the explanations offered within the review comment.</p>
<p>All these qualitative human judgment metrics were rated on a scale of 0 to 5, with 5 indicating the best score.</p>
<p>IV. RESULTS</p>
<p>In this section, we answer each of the research questions taking evidence from our experiment results.</p>
<p>A. RQ1: How effective is code review comment generation using fine-tuned open-source Large Language Models?B. RQ2: How well do the closed-source Large Language Models perform in code review comment generation task when prompt engineered in a few-shot setting?</p>
<p>Next, we used few-shot prompting for review comment generation using leading closed-source LLMs.Due to the incurring API cost of the closed-source models, we conducted our initial experiment on the smaller Test Subset 2. We report the promising results of 5-shot prompting in Table III.As it indicates, our experimental models outperform the baseline even without any further data augmentation.GPT-3.5 Turbo achieved an impressive performance improvement of 89.95% with a BLEU-4 score of 8.13 surpassing other experimental models.Gemini-1.0Pro performed second best, with a BLEU-4 score of 7.85 (and 83.41% boost over the baseline).GPT-4o model performed comparatively poor, scoring a 6.92 BLEU-4 (with 61.68% improvement over the baseline).On BERTScore metric, GPT-3.5 Turbo and Gemini-1.0Pro achieved 0.8509, both gaining a performance upgrade of 1.93%.RQ2 Findings: Closed-source LLMs proved to be highly effective, as they improved over the baseline, exploiting the effectiveness of few-shot prompting for review comment generation task.Among the models tested, GPT-3.5 Turbo achieved the most significant performance gain.Gemini-1.0Pro and GPT-4o also showed impressive results.</p>
<p>C. RQ3: When incorporated into prompts, what are the impacts of function call graph and code summary in improving review comment generation performance?In this section, We report the effect of appending both function call graph and code summary into prompts in Table IV.We used the smaller Test Subset 2 of the dataset to evaluate our experimental closed-source LLMs with this proposed modification.In terms of both metrics, call graph and summary augmentation failed to guide the GPT-3.5 Turbo and Gemini-1.0Pro models in the right direction, where these models have a context window of 4k and 32k tokens respectively.Manual inspection revealed that GPT-3.5 Turbo mostly got affected due to the large number of input tokens in the prompt after the proposed augmentation.On the other hand, GPT-4o with its context window of 128k tokens performs noticeably better when augmented with the same metadata, as it could handle much bigger input contexts.GPT-4o improved 6.79% BLEU-4 score and 0.27% BERTScore relative to the version before augmentation.However, even with such a relative improvement, GPT-4o falls short behind GPT-3.5 Turbo performance.Contrary to our hypothesis, incorporating both the function call graph and code summary together negatively affected the performance of a few models.So we experimented with all possible combinations of these augmentation to identify which additional information was causing a degradation in performance.To keep the API expense within limit, we used the larger Test Subset 1 to conduct this ablation study on our best performing model GPT-3.5 Turbo.Evidence from Table V suggests adding standalone function call graph is advantageous while adding code summary with prompts has a negative impact on the performance.With only the function call graph added, GPT-3.5 Turbo achieves the highest BLEU-4 score of 8.36 which is over 90% of baseline performance.Figure 7 presents a test sample with multiple LLM generated review comments.</p>
<p>RQ3 Findings: Albeit being the inferior performer, GPT-4o with longer context window shows relative performance improvement when both call graph and summary were added simultaneously.Further ablation experiments suggest that, while function call graph guides the model to generate better code review, the code summary mostly affects the result negatively.Thus, when both of them are incorporated, already well-performing models like GPT-3.5 Turbo demonstrate slight performance degradation.D. RQ4: How effective Large Language Models are in generating review comments from a real-world developer perspective?</p>
<p>Further analysis of the qualitative metric scores collected from the developer study supports our previous findings that experimental models significantly outperformed the baseline.Figure 8 illustrates the top models' scores across relevance, informativeness, and explanation clarity metrics.The exact scores are reported in Table VI.Among the GPT series, GPT-3.5 Turbo with Callgraph achieved notable performance, surpassing the other two GPT-3.5 variants.Interestingly, Code Llama demonstrated the strongest qualitative results, possibly due to its pretraining, focused on meticulously designed coderelated tasks.RQ4 Findings: According to the reported experience of 8 software practitioners, LLM generated review comments showed notable performance compared to the baseline Codereviewer.Fine-tuned Code Llama has outperformed all other models in all three qualitative metrics closely followed by GPT-3.5 Turbo augmented with callgraph.</p>
<p>V. DISCUSSION 1) Observations: Although all our trained models surpassed baseline pretrained CodeReviewer (which exhibits high confidence in incorrect answers), they each have specific limitations.Although GPT-3.5-turbo is cost-effective, they can get distracted due to the limited context window size.In contrast, GPT-4o shows improved performance with a longer context window, allowing for greater focus on call graph and code summary for generating code reviews.However, due to budget constraints, we were unable to fully explore the performance of GPT-4o on the whole dataset.On the other hand, fine-tuned Code Llama tend to generalize poor, often failing to address specific changes in greater detail.</p>
<p>2) Future directions: We are incorporating our modelgenerated code reviews into code repair tasks, experimenting with different prompt and fine-tuning strategies to improve the ability of the model to suggest effective code fixes after review.VI.THREATS TO VALIDITY Threats to internal validity are related to how the roles played by the model architecture and the configuration of hyper-parameters might impact the experiments.Due to cost and resource constraints, we explored hyper-parameters prioritizing their expected impact on model performance, while leaving others less explored.As a result, it is possible that further tuning could yield additional improvements.</p>
<p>Threats to construct validity include our use of the widely popular BLEU metric, as it was used in the original baseline study and relevant literature.However, its ability to reflect true performance remains uncertain.The human evaluation results indicate that Code Llama outperforms GPT-3.5 Turbo, with both models surpassing the baseline.This raises questions about the overall reliability of BLEU as a performance measure, suggesting that this might be a threat to construct validity.</p>
<p>Threats to conclusion validity highlight the fact that the exact same responses might not be generated by LLMs, given their inherent nondeterministic nature [42].Additionally, by setting the temperature parameter to 0.7, we encourage more variability in the model's outputs.This nondeterminism in LLM can potentially undermine the conclusion validity drawn from their responses.</p>
<p>Threats to external validity are primarily related to the dataset used in this study.All experiments were conducted using the CodeReviewer [9] dataset of Microsoft Research.As the dataset is derived from publicly available open-source repositories rather than industrial projects, the generalizability of our findings to industrial applications may be limited.</p>
<p>VII. RELATED WORKS</p>
<p>A. Automation of Code Review Activities</p>
<p>There has been considerable interest in reducing the manual labor involved in code review activities.Researchers worked on code quality assessment [9], [43]- [46], probable reviewer recommendation [47]- [52], suggesting review comments [9], [10], [53] and refining problematic code snippets [9], [10], [54]- [56].Our study focuses on the pipeline suggested by Li et al. [9].Retrieval-based approaches were initially adopted for review comment suggestion tasks.DeepMem [57], an LSTM-based model was introduced first to achieve this.Attention mechanism was added on top of LSTM architecture later on by Siow et al. [58].Tufano et al. [10] presented the first contribution of leveraging deep learning for this task via pretraining on code and technical language at a large scale.To improve the results, CodeReviewer [9] and AUGER [59] proposed code review-specific pretrained models.</p>
<p>CommentFinder [60] showed an efficient retrieval-oriented alternative as well.However, approaches so far have not considered code diffs in their pipeline.D-ACT [61] was the first method to introduce diff-awareness in code refinement to boost performance in little differences between code base and initial commits, although they did not leverage code review comments to achieve this.CodeReviewer [9], pretrained on 9 popular programming languages, was tailored for code review activities and considered both diff-awareness and review comments altogether.Significant progress in code review activities has been possible further for the usage of unified Large Language Models, as the model size and training data continued to grow in recent times.Llama-Reviewer [14] was introduced to finetune the open-source Llama model tailored for code review tasks that could achieve impressive performance improvement using parameter-efficient fine-tuning approaches.It used the LoRA [25] method for fine-tuning, while there is room for improvement using the quantized counter-part [26].Additionally, more recent and better models in the Meta Llama series have been released since then, including the general-purpose model Llama 2 [31] and Code Llama [17], a model specifically fine-tuned on code-specific datasets.Finally, Llama 3 was also released for the open-source community.</p>
<p>B. Pretrained and Fine-Tuned Language Models in Software Engineering</p>
<p>Deep learning techniques, motivated by their impact and success in Natural Language Processing domains, have also been widely adopted in software engineering tasks.Encoder-only models like BERT [41], encoder-decoder models like BART [62] and T5 [18], and decoder-only models like GPT [12] are some of the notable progresses here.Pretrained code models can learn code representations with various code-specific properties including lexical, semantic, and syntactic information.Fine-tuning can help these models to be suitable for specific tasks by updating the pretrained model weights with task-specific data.Inspired by the models above, researchers proposed to further fine-tune these models on several downstream tasks in software engineering.CodeBERT [6] and GraphCodeBERT [7] are bi-directional transformer models specifically pretrained on NL-PL pairs in 6 programming languages, with the latter introducing the incorporation of source code data flow graphs inside the model token level.These models show superior performance in tasks including natural language code search, code summarization, code clone detection, and documentation generation from code.</p>
<p>On the other hand, decoder-only transformer models like CodeGPT [63], Codex [64] focused on generative tasks like code completion and code generation.Models like PLBART [65] and CodeT5 [8] can be applied for both code understanding and generation tasks.Despite covering a wide variety of code-related tasks, the models above did not pay any attention to code review activities when proposed.Tufano et al. [10] first proposed TufanoT5 that utilizes the pretrained T5 model for automating code review tasks, with CodeReviewer [9] improving upon that by introducing the integration of code changes into the pretraining phase.Recently, there have been many large language models for code, including open-source community-developed LLMs like Code Llama [17], StarCoder [66], MagiCoder [67] along with proprietary models like GPT-3.5 and GPT-4 [32].</p>
<p>C. Prompting in Software Engineering</p>
<p>Prompt engineering is a good enough alternative to heavy fine-tuning that requires supervised datasets and computational resources.Providing prompts to pretrained LLMs is found to be beneficial in many code-related tasks as shown in [20], [34], [35], [68] covering code summarization, bug fixation and documentation generation.Different prompting strategies include zero-shot learning, few-shot learning [12], [33], chainof-thought [22], persona [21], etc.Not all prompting strategies are suitable for review-related activities as some of these are specifically designed for mathematical and logical reasoning tasks.Zero-shot, few-shot, and personal prompting are instruction-based and hence most suitable for code review tasks.Guo et al. [69] conducted an empirical study to investigate the potential of the GPT-3.5 model for code review automation, while few-shot prompting on LLMs is still of great interest to explore.Ahmed et al. investigated prompting performance on LLMs for code summarization tasks [70], often with augmentation of semantic metadata in prompts for GPT models [71].We investigate the augmentation of such semantic information in a concise way into LLM prompts inspired by their work.</p>
<p>VIII. CONCLUSION</p>
<p>In this study, we aim to automate code review comment generation, using Large Language Models (LLMs) through carefully designed prompts with few-shot learning and parameterefficient fine-tuning.Our prompting approach utilizes the baseline CodeReviewer dataset for our task, incorporating augmented code summaries and function call graphs after thorough pre-processing.Additionally, we applied QLoRA technique to fine-tune the open-source Large Language Models.Experimental results demonstrate that our strategies significantly outperform the baseline CodeReviewer in generating review comments.Furthermore, a human evaluation experiment conducted on professional developers through our designed study indicates that both finetuned code-specific LLMs and general purpose LLMs incorporating call graphs into few-shot prompts improve the quality of generated review comments, thereby further validating the effectiveness of our approach.</p>
<p>Fig. 4 .
4
Fig. 4.Call graph extraction workflow</p>
<p>Fig. 5 .
5
Fig. 5. Scope resolution of call graphs</p>
<p>Fig. 6 .
6
Fig. 6.Code summary generation workflow</p>
<p>Fig. 7 .
7
Fig. 7. Model generated code review comments for a sample code diff</p>
<p>Fig. 8 .
8
Fig. 8.Comparison of LLM-generated code reviews across their average qualitative scores on a scale of 5, as perceived by developers</p>
<p>TABLE I OVERVIEW
I
OF THE DATASET
DatasetSplit TypeCountTrain Set∼ 118kValidation Set∼ 10kCodeReviewerTest Set∼ 10kTest Subset 15000Test Subset 2500</p>
<p>[9]] design experiments, we chose "Please provide formal code review for software developers in one sentence for following test case, implementing few-shot learning from examples.Don't start with code review/review.Just give the answer."asourprompt.Few-shot (k-shot where k can be 3, 5, 10 for example) exemplars were the examples collected from the training dataset to guide the model towards generating output with desired accuracy and format.For each sample from our test subset in our study, we employed BM-25[38], the popular information retrieval and ranking algorithm to retrieve the most relevant k samples from the training set.Following the original CodeReviewer study[9], each example of our prompt contained the following contents:-Code Diff: A code snippet showing the changes in the code, denoted as patch in the CodeReviewer dataset.-Code Review: The corresponding review comment, originally the msg portion of the CodeReviewer dataset samples.</p>
<p>• Exemplars: • Query test : For evaluating the model, we used test queries structured similarly to the training examples.Each test case of our Test Subset 1 contains only the Code Diff.</p>
<p>TABLE VI EVALUATION
VI
OF LLM-GENERATED CODE REVIEWS BY REAL-WORLD DEVELOPERS ACROSS THEIR AVERAGE QUALITATIVE SCORES ON A SCALE
OF 5ModelRelevance Information Explanation ClarityCodereviewer1.341.221.23Code Llama3.453.132.95GPT-3.5-Turbo (W )3.232.872.74GPT-3.5-Turbo (C)3.423.042.89GPT-3.5-Turbo (C + S)3.222.762.76
https://github.com/OpenAccess-AI-Collective/axolotl
https://github.com/unslothai/unsloth
https://github.com/tree-sitter/tree-sitter
The funding from the Research and Innovation Centre For Science and Engineering, RISE-BUET Undergraduate Student Research Grant S2023-03-113 supports our work.
Primers or reminders? the effects of existing review comments on code review. D Spadini, G , A Bacchelli, Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. the ACM/IEEE 42nd International Conference on Software Engineering2020</p>
<p>Design and code inspections to reduce errors in program development. M Fagan, Software pioneers: contributions to software engineering. Springer2011</p>
<p>Impact of peer code review on peer impression formation: A survey. A Bosu, J C Carver, 2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement. 2013</p>
<p>Mining the modern code review repositories: A dataset of people, process and product. X Yang, R G Kula, N Yoshida, H Iida, Proceedings of the 13th international conference on mining software repositories. the 13th international conference on mining software repositories2016</p>
<p>Code reviews do not find bugs. how the current code review best practice slows us down. J Czerwonka, M Greiler, J Tilford, 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering. IEEE20152</p>
<p>Codebert: A pre-trained model for programming and natural languages. Z Feng, D Guo, D Tang, N Duan, X Feng, M Gong, L Shou, B Qin, T Liu, D Jiang, arXiv:2002.081552020arXiv preprint</p>
<p>Graphcodebert: Pre-training code representations with data flow. D Guo, S Ren, S Lu, Z Feng, D Tang, S Liu, L Zhou, N Duan, A Svyatkovskiy, S Fu, arXiv:2009.083662020arXiv preprint</p>
<p>Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. Y Wang, W Wang, S Joty, S C Hoi, arXiv:2109.008592021arXiv preprint</p>
<p>Automating code review activities by large-scale pre-training. Z Li, S Lu, D Guo, N Duan, S Jannu, G Jenks, D Majumder, J Green, A Svyatkovskiy, S Fu, Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2022</p>
<p>Using pre-trained models to boost code review automation. R Tufano, S Masiero, A Mastropaolo, L Pascarella, D Poshyvanyk, G Bavota, Proceedings of the 44th international conference on software engineering. the 44th international conference on software engineering2022</p>
<p>Energy and policy considerations for modern deep learning research. E Strubell, A Ganesh, A Mccallum, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034696</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>A survey of large language models for code: Evolution, benchmarking, and future trends. Z Zheng, K Ning, Y Wang, J Zhang, D Zheng, M Ye, J Chen, arXiv:2311.103722023arXiv preprint</p>
<p>Llama-reviewer: Advancing code review automation with large language models through parameterefficient fine-tuning. J Lu, L Yu, X Li, L Yang, C Zuo, 2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE). IEEE2023</p>
<p>Automatic semantic augmentation of language model prompts (for code summarization). T Ahmed, K S Pai, P Devanbu, E Barr, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software Engineering2024</p>
<p>Gemini: a family of highly capable multimodal models. G Team, R Anil, S Borgeaud, Y Wu, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Code llama: Open foundation models for code. B Roziere, J Gehring, F Gloeckle, S Sootla, I Gat, X E Tan, Y Adi, J Liu, T Remez, J Rapin, arXiv:2308.129502023arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, Oct. 2019</p>
<p>Gpt-3.5 for code review automation: How do few-shot learning, prompt design, and model finetuning impact their performance. C Pornprasit, C Tantithamthavorn, arXiv:2402.009052024arXiv preprint</p>
<p>Prompting is all you need: Automated android bug replay with large language models. S Feng, C Chen, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software EngineeringNew York, NY, USAACMFeb. 2024</p>
<p>A prompt pattern catalog to enhance prompt engineering with ChatGPT. J White, Q Fu, S Hays, M Sandborn, C Olea, H Gilbert, A Elnashar, J Spencer-Smith, D C Schmidt, Feb. 2023</p>
<p>The CoT collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning. S Kim, S J Joo, D Kim, J Jang, S Ye, J Shin, M Seo, May 2023</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. X L Li, P Liang, arXiv:2101.001902021arXiv preprint</p>
<p>The power of scale for parameter-efficient prompt tuning. B Lester, R Al-Rfou, N Constant, arXiv:2104.086912021arXiv preprint</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, arXiv:2106.096852021arXiv preprint</p>
<p>Qlora: Efficient finetuning of quantized llms. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, Advances in Neural Information Processing Systems. 202436</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, arXiv:2109.016522021arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 202235</p>
<p>Stanford alpaca: An instruction-following llama model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, 2023</p>
<p>Self-instruct: Aligning language models with selfgenerated instructions. Y Wang, Y Kordi, S Mishra, A Liu, N A Smith, D Khashabi, H Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>What makes good in-context examples for GPT-3?. J Liu, D Shen, Y Zhang, B Dolan, L Carin, W Chen, Jan. 2021</p>
<p>Large language models are few-shot summarizers: Multiintent comment generation via in-context learning. M Geng, S Wang, D Dong, H Wang, G Li, Z Jin, X Mao, X Liao, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software EngineeringNew York, NY, USAACMFeb. 2024</p>
<p>What makes good in-context demonstrations for code intelligence tasks with LLMs?. S Gao, X.-C Wen, C Gao, W Wang, H Zhang, M R Lyu, 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEESep. 2023</p>
<p>Principled instructions are all you need for questioning llama-1/2, gpt-3.5/4. S M Bsharat, A Myrzakhan, Z Shen, arXiv:2312.161712023arXiv preprint</p>
<p>Large language models understand and can be enhanced by emotional stimuli. C Li, J Wang, Y Zhang, K Zhu, W Hou, J Lian, F Luo, Q Yang, X Xie, arXiv:2307.117602023arXiv preprint</p>
<p>The probabilistic relevance framework: Bm25 and beyond. S Robertson, H Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Bertscore: Evaluating text generation with bert. T Zhang, * , V Kishore, * , F Wu, * , K Q Weinberger, Y Artzi, International Conference on Learning Representations. 2020</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Oct. 2018</p>
<p>Llm is like a box of chocolates: the non-determinism of chatgpt in code generation. S Ouyang, J M Zhang, M Harman, M Wang, arXiv:2308.028282023arXiv preprint</p>
<p>Automatic code review by learning the revision of source code. S.-T Shi, M Li, D Lo, F Thung, X Huo, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Is historical data an appropriate benchmark for reviewer recommendation systems?: A case study of the gerrit community. I X Gauthier, M Lamothe, G Mussbacher, S Mcintosh, 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE2021</p>
<p>Quality evaluation of modern code reviews through intelligent biometric program comprehension. H Hijazi, J Duraes, R Couceiro, J Castelhano, R Barbosa, J Medeiros, M Castelo-Branco, P De Carvalho, H Madeira, IEEE Transactions on Software Engineering. 4922022</p>
<p>Towards automating code review at scale. V J Hellendoorn, J Tsay, M Mukherjee, M Hirzel, Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering2021</p>
<p>Whodo: Automating reviewer suggestions at scale. S Asthana, R Kumar, R Bhagwan, C Bird, C Bansal, C Maddila, S Mehta, B Ashok, Proceedings of the 2019 27th ACM joint meeting on european software engineering conference and symposium on the foundations of software engineering. the 2019 27th ACM joint meeting on european software engineering conference and symposium on the foundations of software engineering2019</p>
<p>Expanding the number of reviewers in open-source projects by recommending appropriate developers. A Chueshev, J Lawall, R Bendraou, T Ziadi, 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE2020</p>
<p>Corms: a github and gerrit based hybrid code reviewer recommendation approach for modern code review. P Pandya, S Tiwari, Proceedings of the 30th ACM joint European software engineering conference and symposium on the foundations of software engineering. the 30th ACM joint European software engineering conference and symposium on the foundations of software engineering2022</p>
<p>Modeling review history for reviewer recommendation: A hypergraph approach. G Rong, Y Zhang, L Yang, F Zhang, H Kuang, H Zhang, Proceedings of the 44th international conference on software engineering. the 44th international conference on software engineering2022</p>
<p>Rstrace+: Reviewer suggestion using software artifact traceability graphs. E Sülün, E Tüzün, U Dogrusöz, Information and Software Technology. 1301064552021</p>
<p>Multiobjective code reviewer recommendations: balancing expertise, availability and collaborations. S Rebai, A Amich, S Molaei, M Kessentini, R Kazman, Automated Software Engineering. 272020</p>
<p>Reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation. V Balachandran, 2013 35th International Conference on Software Engineering (ICSE). IEEE2013</p>
<p>Towards automating code review activities. R Tufano, L Pascarella, M Tufano, D Poshyvanyk, G Bavota, 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE2021</p>
<p>Vulrepair: a t5-based automated software vulnerability repair. M Fu, C Tantithamthavorn, T Le, V Nguyen, D Phung, Proceedings of the 30th ACM joint european software engineering conference and symposium on the foundations of software engineering. the 30th ACM joint european software engineering conference and symposium on the foundations of software engineering2022</p>
<p>Autotransform: Automated code transformation to support modern code review process. P Thongtanunam, C Pornprasit, C Tantithamthavorn, Proceedings of the 44th international conference on software engineering. the 44th international conference on software engineering2022</p>
<p>Intelligent code reviews using deep learning. A Gupta, N Sundaresan, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'18) Deep Learning Day. the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'18) Deep Learning Day2018</p>
<p>CORE: Automating review recommendation for code changes. J K Siow, C Gao, L Fan, S Chen, Y Liu, 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEEFeb. 2020</p>
<p>AUGER: Automatically GEnerating review comments with pre-training models. L Li, L Yang, H Jiang, J Yan, T Luo, Z Hua, G Liang, C Zuo, Aug. 2022</p>
<p>Com-mentFinder: a simpler, faster, more accurate code review comments recommendation. Y Hong, C Tantithamthavorn, P Thongtanunam, A Aleti, Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software EngineeringNew York, NY, USAACMNov. 2022</p>
<p>D-ACT: Towards diff-aware code transformation for code review under a time-wise evaluation. C Pornprasit, C Tantithamthavorn, P Thongtanunam, C Chen, 2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEEMar. 2023</p>
<p>BART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, Oct. 2019</p>
<p>Codexglue: A machine learning benchmark dataset for code understanding and generation. S Lu, D Guo, S Ren, J Huang, A Svyatkovskiy, A Blanco, C Clement, D Drain, D Jiang, D Tang, arXiv:2102.046642021arXiv preprint</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Unified pretraining for program understanding and generation. W U Ahmad, S Chakraborty, B Ray, K.-W Chang, Mar. 2021</p>
<p>R Li, L B Allal, Y Zi, N Muennighoff, D Kocetkov, C Mou, M Marone, C Akiki, J Li, J Chim, arXiv:2305.06161Starcoder: may the source be with you!. 2023arXiv preprint</p>
<p>Magicoder: Empowering code generation with OSS-Instruct. Y Wei, Z Wang, J Liu, Y Ding, L Zhang, Dec. 2023</p>
<p>Automatic code documentation generation using GPT-3. J Y Khan, G Uddin, Sep. 2022</p>
<p>Exploring the potential of ChatGPT in automated code refinement: An empirical study. Q Guo, J Cao, X Xie, S Liu, X Li, B Chen, X Peng, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software EngineeringNew York, NY, USAACMFeb. 2024</p>
<p>Few-shot training LLMs for project-specific code-summarization. T Ahmed, P Devanbu, Jul. 2022</p>
<p>Automatic semantic augmentation of language model prompts (for code summarization). T Ahmed, K S Pai, P Devanbu, E Barr, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software EngineeringNew York, NY, USAACMApr. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
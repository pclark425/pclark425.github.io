<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7494 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7494</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7494</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-0c97435611169f5d63ce3e2f06ccd08bbdcdb46e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0c97435611169f5d63ce3e2f06ccd08bbdcdb46e" target="_blank">Development of Cognitive Intelligence in Pre-trained Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper Abstract:</strong> Recent studies show evidence for emergent cognitive abilities in Large Pre-trained Language Models (PLMs). The increasing cognitive alignment of these models has made them candidates for cognitive science theories. Prior research into the emergent cognitive abilities of PLMs has been path-independent to model training, i.e. has only looked at the final model weights and not the intermediate steps. However, building plausible models of human cognition using PLMs also requires aligning their performance during training to the developmental trajectories of children’s thinking. Guided by psychometric tests of human intelligence, we choose four task categories to investigate the alignment of ten popular families of PLMs and evaluate each of their available intermediate and final training steps: Numerical ability, Linguistic abilities, Conceptual understanding, and Fluid reasoning. We find a striking regularity: regardless of model size, the developmental trajectories of PLMs consistently exhibit a window of maximal alignment to human cognitive development. Before that window, training appears to endow models with the requisite structure to be poised to rapidly learn from experience. After that window, training appears to serve the engineering goal of reducing loss but not the scientific goal of increasing alignment with human cognition.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7494.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7494.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_BLIMP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 on BLiMP (linguistic acceptability)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 evaluated on BLiMP linguistic minimal pairs using LM-eval-harness probability comparisons (acceptability ranking).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4, large generative pre-trained transformer accessed via API (probabilities available).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>BLiMP (Benchmark of Linguistic Minimal Pairs for English)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>linguistic ability / syntax & semantics</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Pairs of minimally different sentences (acceptable vs unacceptable); model judged by whether it assigns higher probability to the acceptable sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (proportion of pairs where acceptable sentence scored higher)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 0.849 (84.9%) as reported in Table 3</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>probability comparison via LM-eval-harness (zero-shot sequential probability)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Warstadt et al., 2020 (BLiMP dataset) — referenced in paper</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper reports BLiMP accuracy for GPT-4 but does not report a human baseline on BLiMP within the paper. BLiMP itself is derived from human acceptability judgments (Warstadt et al., 2020), but no numeric human topline is provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7494.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7494.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_Typicality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 on Typicality / Conceptual Typicality (Rosch norms)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 evaluated on category typicality using surprisal-based in-filling and prompting; performance reported as Spearman correlations with human typicality norms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4, accessed via API; prompting and token-probability based evaluation used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Typicality effects (category typicality / production norms)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>concept understanding / semantic memory</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Model ranks or assigns probability to category members; correlation with human typicality norms (proportion of humans producing an item) measured.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Spearman's rank correlation with human typicality norms</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>surprisal zero-shot Spearman 0.428; prompting (few-shot/prompt design) yields 0.559 (prompting reported separately) — values from Table 7 and Table 6</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>surprisal (zero-shot) and prompting (few-shot prompting / in-filling with options) for API models</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Typicality / category norms referenced: Battig & Montague 1969; Van Overschelde et al., 2004; De Deyne et al., 2008 (as cited in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper reports Spearman correlations for GPT-4 but does not provide numerical human baseline correlations (human typicality is the target variable drawn from norms). Prompting gives substantially higher alignment than surprisal alone for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7494.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7494.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_RPM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 on Raven's Progressive Matrices (RPM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 evaluated on an adapted textual mapping of Raven-like matrix problems (I-Raven mapping); answers chosen by comparing full-sequence surprisal for candidate completions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4, accessed via API, token probabilities used to compute surprisal over candidate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Raven's Progressive Matrices (RPM) / I-Raven textual adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>fluid reasoning / abstract pattern induction</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>3x3 visual analogy matrices mapped to textual tuples (type, size, color); model must choose correct tuple from alternatives based on inferred pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (proportion correct)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 0.822 (82.2%) as reported in Table 3</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot surprisal comparison of completed sequence (instruction + candidate tuple)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Raven, 2003 (RPM) — referenced in paper; I-Raven mapping from Hu et al., 2023 and Webb et al., 2023</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper reports GPT-4 RPM accuracy but does not provide a human baseline RPM score within this paper. The RPM is a standard human test (Raven, 2003) but no numeric comparison is given here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7494.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7494.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5_BLIMP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo on BLiMP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5-Turbo evaluated on BLiMP using probability comparisons (LM-eval-harness).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5-Turbo, API-accessible model with token probability outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>BLiMP (Benchmark of Linguistic Minimal Pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>linguistic ability</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Model must assign higher probability to acceptable sentence in minimal pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 0.825 (82.5%) as reported in Table 3</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>probability comparison via LM-eval-harness (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Warstadt et al., 2020 (BLiMP dataset) — referenced in paper</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>No human BLiMP topline reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7494.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7494.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5_Typicality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo on Typicality / Conceptual Typicality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 evaluated on typicality using surprisal-based in-filling; Spearman correlations reported against human norms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5-Turbo via API; surprisal-based evaluation used for typicality.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Typicality effects (category typicality)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>conceptual understanding</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Model ranks category members (via surprisal) and correlation with human typicality norms is computed.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Spearman's correlation with human typicality norms</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>surprisal zero-shot Spearman 0.231 (Table 7); average prompting/other methods differ (see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>surprisal (zero-shot); prompting also tested but lower/higher depending on method</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human typicality norms referenced (e.g., Van Overschelde et al., 2004; De Deyne et al., 2008)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper reports low-to-moderate alignment for GPT-3.5; no numeric human baseline correlation provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7494.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7494.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5_RPM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo on Raven's Progressive Matrices (RPM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 evaluated on textualized RPM problems using surprisal comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5-Turbo accessed via API; token-probabilities used to compute surprisal for candidate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Raven's Progressive Matrices (I-Raven textual adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>fluid reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Choose the tuple representing the missing cell from alternatives based on induced pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 0.792 (79.2%) as reported in Table 3</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot surprisal over completed sequence (instruction + candidate tuple)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Raven, 2003 and adapted Raven datasets referenced (Hu et al., 2023; Webb et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper reports model accuracy but provides no direct human RPM baseline in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7494.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7494.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini_Typicality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini (Gemini-1-Pro) on Typicality / Conceptual Typicality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's Gemini evaluated on typicality with prompting-based in-filling; performance reported as average Spearman correlations across categories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini (Gemini-1-Pro)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google Gemini family model; API-accessible but latent representations not available for this study; prompting-based evaluation used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Typicality effects (category typicality)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>conceptual understanding</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Prompting-based in-filling: model re-ranks members of a category; Spearman correlation with human typicality norms computed.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Spearman's correlation with human typicality norms</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>prompting average Spearman 0.311 (Table 6 / Table 7)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>prompting (in-filling with options; few-shot prompting design described in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Category norms and typicality datasets referenced (Van Overschelde et al., 2004; De Deyne et al., 2008; Castro et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Gemini performed better on prompting than some open-source alternatives; no numeric human baseline provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7494.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7494.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pythia-2.8B_Numeric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pythia-2.8B on Magnitude Comparison Effects (Distance & Ratio)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pythia-2.8B (an open-source model in the Pythia suite) evaluated for latent-number representations via cosine similarities yielding R^2 fits for distance and ratio effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pythia-2.8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Pythia family model (decoder transformer); intermediate checkpoints available enabling developmental trajectories analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.8B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Magnitude comparison effects (Distance effect & Ratio effect)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>numeric cognition / number representation</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Measure whether latent representations reflect human-like mental number line signatures: distance effect (cosine similarity vs |x-y| linear fit R^2) and ratio effect (negative exponential fit R^2 against ratio).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>R^2 of fitted functions (distance linear fit R^2; ratio negative-exponential fit R^2)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Distance effect R^2 = 0.961; Ratio effect R^2 = 0.723 (Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>latent-representation analysis (cosine similarities across token/number representations across layers and input formats)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human effects referenced: Moyer & Landauer, 1967; Halberda et al., 2008; Shah et al., 2023 (PLM prior study)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper reports strong model signatures of human-like distance effect but does not report numeric human R^2 baselines; human descriptions are qualitative (distance & ratio effects) rather than tabulated comparisons here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7494.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7494.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pythia-2.8B_BLIMP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pythia-2.8B on BLiMP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pythia-2.8B evaluated on BLiMP acceptability tasks using LM-eval-harness probability comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pythia-2.8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Pythia family model (2.8B parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.8B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>BLiMP (linguistic acceptability)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>linguistic ability</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Minimal pair acceptability judgments measured by higher probability assigned to acceptable sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 0.827 (82.7%) as reported in Table 3</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>probability comparison (LM-eval-harness, zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Warstadt et al., 2020 (BLiMP)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>No human BLiMP baseline values provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7494.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7494.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pythia-2.8B_Typicality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pythia-2.8B on Typicality / Conceptual Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pythia-2.8B evaluated for typicality alignment using latent representations and surprisal methods; Spearman correlations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pythia-2.8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Pythia model; latent representations available enabling representational analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.8B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Typicality effects (category typicality)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>conceptual understanding</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Model judged by Spearman correlation between model-derived typicality scores and human typicality norms.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Spearman's correlation</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>latent representation Spearman = 0.221; surprisal zero-shot = 0.273 (Table 7 / Table 3 aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>latent representation (cosine similarity) and surprisal zero-shot; prompting not available for open-source in paper</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Typicality norms referenced (Van Overschelde et al., 2004; De Deyne et al., 2008; Castro et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Larger Pythia models show better alignment than smaller ones; paper does not provide direct human correlation baseline numbers beyond the norms used as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7494.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7494.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pythia-2.8B_RPM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pythia-2.8B on Raven's Progressive Matrices (RPM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pythia-2.8B evaluated on textualized RPM problems using surprisal-based selection; accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pythia-2.8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Pythia model family member, evaluated on I-Raven textual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.8B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Raven's Progressive Matrices (I-Raven adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>fluid reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Select correct tuple representing missing cell in a 3x3 matrix mapped to textual attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 0.760 (76.0%) as reported in Table 3</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot surprisal over full-sequence (instruction + candidate tuple)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Raven, 2003 and mappings from Hu et al., 2023; Webb et al., 2023</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper reports RPM accuracy for Pythia-2.8B but does not include direct human baseline scores for the same textualized item set.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7494.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7494.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B_Numeric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-7B on Magnitude Comparison Effects</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama-2-7B evaluated for latent number representations; R^2 reported for distance and ratio effects using cosine similarity analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta's Llama-2 family, 7B parameter decoder transformer (open-source variant used).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Magnitude comparison effects (Distance & Ratio)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>numeric cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Assess whether model latent representations show distance and ratio effects consistent with human mental number line.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>R^2 of fitted functions (distance linear fit; ratio negative-exponential fit)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Distance effect R^2 = 0.670; Ratio effect R^2 = 0.614 (Table 3 / Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>latent-representation analysis across layers and input formats (cosine similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Moyer & Landauer 1967; Halberda et al., 2008 (human behavioral effects referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper presents model R^2 values indicating humanlike distance effect; no numerical human R^2 baseline reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7494.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7494.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B_BLIMP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-7B on BLiMP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama-2-7B evaluated on BLiMP acceptability tasks with LM-eval-harness probability scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-2 7B parameter open-source model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>BLiMP</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>linguistic ability</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Minimal pair acceptance via probability ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 0.818 (81.8%) (Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>probability comparison (LM-eval-harness, zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Warstadt et al., 2020 (BLiMP)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper reports BLiMP accuracy per phenomenon (morphology, syntax, semantics) but does not present human baseline accuracy here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7494.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7494.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B_Typicality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-7B on Typicality (Concept Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama-2-7B evaluated for typicality alignment using latent representation and surprisal methods; Spearman correlations reported (latent rep negative in aggregate).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama-2 7B model; latent representations available.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Typicality effects</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>conceptual understanding</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Spearman correlation between model-derived scores (latent similarity or surprisal) and human typicality norms.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Spearman's correlation</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>latent-representation Spearman = -0.065; surprisal zero-shot = 0.238 (Table 3 and Table 7)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>latent representation (cosine similarity) and surprisal zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human category norms as referenced (Van Overschelde et al., 2004; De Deyne et al., 2008)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Negative/near-zero latent-representation correlations indicate weak alignment for that method; surprisal yields small positive alignment. No human numerical baseline correlations provided in paper beyond the norms used to compute correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7494.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e7494.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B_RPM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-7B on Raven's Progressive Matrices</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama-2-7B evaluated on textualized RPM using surprisal-based selection; accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-2 7B parameter model evaluated on I-Raven formatted items.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Raven's Progressive Matrices (I-Raven adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>fluid reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Textual tuple representation of visual RPM items; model selects option with highest sequence probability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 0.752 (75.2%) as reported in Table 3</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot surprisal of completed sequence</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Raven, 2003; I-Raven mapping cited via Hu et al., 2023 and Webb et al., 2023</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper reports model accuracy; no human baseline for the adapted textual item set is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7494.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e7494.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pythia-suite_development</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pythia model suite developmental trajectories (magnitude, language, concept, fluid reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The Pythia family (models from 70M to 12B parameters) evaluated across intermediate checkpoints to study developmental alignment to human cognitive development; trajectories plotted as function of tokens seen.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pythia suite (multiple sizes: 70M–12B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source suite of models trained with known order and many intermediate checkpoints; used to analyze learning trajectories across training tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, 12B (different models in suite)</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Developmental trajectories on psychometric tasks (Numeric distance/ratio, BLiMP, Typicality, RPM)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>multiple (numeric cognition, linguistic ability, conceptual understanding, fluid reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Performance on psychometric-style tasks tracked over intermediate checkpoints (number of tokens seen) to identify windows of monotonic development and alignment with human developmental timing.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>varies by task (R^2 for numeric effects; accuracy for BLiMP and RPM; Spearman for typicality); reported as function over tokens seen</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>see task-specific entries (e.g., Pythia-2.8B distance R^2=0.961, BLiMP acc=0.827, typicality surprisal zero-shot=0.273, RPM acc=0.760); developmental windows observed ~100M–20B tokens</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>latent-representation analyses and surprisal/probability comparisons across checkpoints; zero- and few-shot prompting for some conceptual tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Developmental alignment motivated by human developmental literature cited (Elman, 1996; child language corpora references; psychometric theory references)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper's major finding: a consistent window of maximal alignment to human development across tasks occurs during pretraining; human developmental 'baselines' (e.g., ages when abilities emerge) are discussed qualitatively, but no numeric human performance baselines are reported for direct quantitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Development of Cognitive Intelligence in Pre-trained Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Time required for judgements of numerical inequality <em>(Rating: 2)</em></li>
                <li>Individual differences in nonverbal number acuity correlate with maths achievement <em>(Rating: 2)</em></li>
                <li>Benchmark of Linguistic Minimal Pairs for English <em>(Rating: 2)</em></li>
                <li>Raven progressive matrices <em>(Rating: 2)</em></li>
                <li>Category norms of verbal items in 56 categories a replication and extension of the connecticut category norms <em>(Rating: 2)</em></li>
                <li>Exemplar by feature applicability matrices and other dutch normative data for semantic concepts <em>(Rating: 2)</em></li>
                <li>Numeric magnitude comparison effects in large language models <em>(Rating: 2)</em></li>
                <li>How well do deep learning models capture human concepts? the case of the typicality effect <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7494",
    "paper_id": "paper-0c97435611169f5d63ce3e2f06ccd08bbdcdb46e",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "GPT-4_BLIMP",
            "name_full": "GPT-4 on BLiMP (linguistic acceptability)",
            "brief_description": "GPT-4 evaluated on BLiMP linguistic minimal pairs using LM-eval-harness probability comparisons (acceptability ranking).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4, large generative pre-trained transformer accessed via API (probabilities available).",
            "model_size": "not specified in paper",
            "test_name": "BLiMP (Benchmark of Linguistic Minimal Pairs for English)",
            "test_category": "linguistic ability / syntax & semantics",
            "test_description": "Pairs of minimally different sentences (acceptable vs unacceptable); model judged by whether it assigns higher probability to the acceptable sentence.",
            "evaluation_metric": "accuracy (proportion of pairs where acceptable sentence scored higher)",
            "human_performance": null,
            "llm_performance": "accuracy 0.849 (84.9%) as reported in Table 3",
            "prompting_method": "probability comparison via LM-eval-harness (zero-shot sequential probability)",
            "fine_tuned": false,
            "human_data_source": "Warstadt et al., 2020 (BLiMP dataset) — referenced in paper",
            "statistical_significance": null,
            "notes": "Paper reports BLiMP accuracy for GPT-4 but does not report a human baseline on BLiMP within the paper. BLiMP itself is derived from human acceptability judgments (Warstadt et al., 2020), but no numeric human topline is provided here.",
            "uuid": "e7494.0",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4_Typicality",
            "name_full": "GPT-4 on Typicality / Conceptual Typicality (Rosch norms)",
            "brief_description": "GPT-4 evaluated on category typicality using surprisal-based in-filling and prompting; performance reported as Spearman correlations with human typicality norms.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4, accessed via API; prompting and token-probability based evaluation used.",
            "model_size": "not specified in paper",
            "test_name": "Typicality effects (category typicality / production norms)",
            "test_category": "concept understanding / semantic memory",
            "test_description": "Model ranks or assigns probability to category members; correlation with human typicality norms (proportion of humans producing an item) measured.",
            "evaluation_metric": "Spearman's rank correlation with human typicality norms",
            "human_performance": null,
            "llm_performance": "surprisal zero-shot Spearman 0.428; prompting (few-shot/prompt design) yields 0.559 (prompting reported separately) — values from Table 7 and Table 6",
            "prompting_method": "surprisal (zero-shot) and prompting (few-shot prompting / in-filling with options) for API models",
            "fine_tuned": false,
            "human_data_source": "Typicality / category norms referenced: Battig & Montague 1969; Van Overschelde et al., 2004; De Deyne et al., 2008 (as cited in paper)",
            "statistical_significance": null,
            "notes": "Paper reports Spearman correlations for GPT-4 but does not provide numerical human baseline correlations (human typicality is the target variable drawn from norms). Prompting gives substantially higher alignment than surprisal alone for GPT-4.",
            "uuid": "e7494.1",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4_RPM",
            "name_full": "GPT-4 on Raven's Progressive Matrices (RPM)",
            "brief_description": "GPT-4 evaluated on an adapted textual mapping of Raven-like matrix problems (I-Raven mapping); answers chosen by comparing full-sequence surprisal for candidate completions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4, accessed via API, token probabilities used to compute surprisal over candidate answers.",
            "model_size": "not specified in paper",
            "test_name": "Raven's Progressive Matrices (RPM) / I-Raven textual adaptation",
            "test_category": "fluid reasoning / abstract pattern induction",
            "test_description": "3x3 visual analogy matrices mapped to textual tuples (type, size, color); model must choose correct tuple from alternatives based on inferred pattern.",
            "evaluation_metric": "accuracy (proportion correct)",
            "human_performance": null,
            "llm_performance": "accuracy 0.822 (82.2%) as reported in Table 3",
            "prompting_method": "zero-shot surprisal comparison of completed sequence (instruction + candidate tuple)",
            "fine_tuned": false,
            "human_data_source": "Raven, 2003 (RPM) — referenced in paper; I-Raven mapping from Hu et al., 2023 and Webb et al., 2023",
            "statistical_significance": null,
            "notes": "Paper reports GPT-4 RPM accuracy but does not provide a human baseline RPM score within this paper. The RPM is a standard human test (Raven, 2003) but no numeric comparison is given here.",
            "uuid": "e7494.2",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-3.5_BLIMP",
            "name_full": "GPT-3.5-Turbo on BLiMP",
            "brief_description": "GPT-3.5-Turbo evaluated on BLiMP using probability comparisons (LM-eval-harness).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "OpenAI GPT-3.5-Turbo, API-accessible model with token probability outputs.",
            "model_size": "not specified in paper",
            "test_name": "BLiMP (Benchmark of Linguistic Minimal Pairs)",
            "test_category": "linguistic ability",
            "test_description": "Model must assign higher probability to acceptable sentence in minimal pairs.",
            "evaluation_metric": "accuracy",
            "human_performance": null,
            "llm_performance": "accuracy 0.825 (82.5%) as reported in Table 3",
            "prompting_method": "probability comparison via LM-eval-harness (zero-shot)",
            "fine_tuned": false,
            "human_data_source": "Warstadt et al., 2020 (BLiMP dataset) — referenced in paper",
            "statistical_significance": null,
            "notes": "No human BLiMP topline reported in this paper.",
            "uuid": "e7494.3",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-3.5_Typicality",
            "name_full": "GPT-3.5-Turbo on Typicality / Conceptual Typicality",
            "brief_description": "GPT-3.5 evaluated on typicality using surprisal-based in-filling; Spearman correlations reported against human norms.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "OpenAI GPT-3.5-Turbo via API; surprisal-based evaluation used for typicality.",
            "model_size": "not specified in paper",
            "test_name": "Typicality effects (category typicality)",
            "test_category": "conceptual understanding",
            "test_description": "Model ranks category members (via surprisal) and correlation with human typicality norms is computed.",
            "evaluation_metric": "Spearman's correlation with human typicality norms",
            "human_performance": null,
            "llm_performance": "surprisal zero-shot Spearman 0.231 (Table 7); average prompting/other methods differ (see paper)",
            "prompting_method": "surprisal (zero-shot); prompting also tested but lower/higher depending on method",
            "fine_tuned": false,
            "human_data_source": "Human typicality norms referenced (e.g., Van Overschelde et al., 2004; De Deyne et al., 2008)",
            "statistical_significance": null,
            "notes": "Paper reports low-to-moderate alignment for GPT-3.5; no numeric human baseline correlation provided in paper.",
            "uuid": "e7494.4",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-3.5_RPM",
            "name_full": "GPT-3.5-Turbo on Raven's Progressive Matrices (RPM)",
            "brief_description": "GPT-3.5 evaluated on textualized RPM problems using surprisal comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "OpenAI GPT-3.5-Turbo accessed via API; token-probabilities used to compute surprisal for candidate answers.",
            "model_size": "not specified in paper",
            "test_name": "Raven's Progressive Matrices (I-Raven textual adaptation)",
            "test_category": "fluid reasoning",
            "test_description": "Choose the tuple representing the missing cell from alternatives based on induced pattern.",
            "evaluation_metric": "accuracy",
            "human_performance": null,
            "llm_performance": "accuracy 0.792 (79.2%) as reported in Table 3",
            "prompting_method": "zero-shot surprisal over completed sequence (instruction + candidate tuple)",
            "fine_tuned": false,
            "human_data_source": "Raven, 2003 and adapted Raven datasets referenced (Hu et al., 2023; Webb et al., 2023)",
            "statistical_significance": null,
            "notes": "Paper reports model accuracy but provides no direct human RPM baseline in the text.",
            "uuid": "e7494.5",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Gemini_Typicality",
            "name_full": "Gemini (Gemini-1-Pro) on Typicality / Conceptual Typicality",
            "brief_description": "Google's Gemini evaluated on typicality with prompting-based in-filling; performance reported as average Spearman correlations across categories.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini (Gemini-1-Pro)",
            "model_description": "Google Gemini family model; API-accessible but latent representations not available for this study; prompting-based evaluation used.",
            "model_size": "not specified in paper",
            "test_name": "Typicality effects (category typicality)",
            "test_category": "conceptual understanding",
            "test_description": "Prompting-based in-filling: model re-ranks members of a category; Spearman correlation with human typicality norms computed.",
            "evaluation_metric": "Spearman's correlation with human typicality norms",
            "human_performance": null,
            "llm_performance": "prompting average Spearman 0.311 (Table 6 / Table 7)",
            "prompting_method": "prompting (in-filling with options; few-shot prompting design described in paper)",
            "fine_tuned": false,
            "human_data_source": "Category norms and typicality datasets referenced (Van Overschelde et al., 2004; De Deyne et al., 2008; Castro et al., 2021)",
            "statistical_significance": null,
            "notes": "Gemini performed better on prompting than some open-source alternatives; no numeric human baseline provided in this paper.",
            "uuid": "e7494.6",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Pythia-2.8B_Numeric",
            "name_full": "Pythia-2.8B on Magnitude Comparison Effects (Distance & Ratio)",
            "brief_description": "Pythia-2.8B (an open-source model in the Pythia suite) evaluated for latent-number representations via cosine similarities yielding R^2 fits for distance and ratio effects.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Pythia-2.8B",
            "model_description": "Open-source Pythia family model (decoder transformer); intermediate checkpoints available enabling developmental trajectories analysis.",
            "model_size": "2.8B parameters",
            "test_name": "Magnitude comparison effects (Distance effect & Ratio effect)",
            "test_category": "numeric cognition / number representation",
            "test_description": "Measure whether latent representations reflect human-like mental number line signatures: distance effect (cosine similarity vs |x-y| linear fit R^2) and ratio effect (negative exponential fit R^2 against ratio).",
            "evaluation_metric": "R^2 of fitted functions (distance linear fit R^2; ratio negative-exponential fit R^2)",
            "human_performance": null,
            "llm_performance": "Distance effect R^2 = 0.961; Ratio effect R^2 = 0.723 (Table 4)",
            "prompting_method": "latent-representation analysis (cosine similarities across token/number representations across layers and input formats)",
            "fine_tuned": false,
            "human_data_source": "Human effects referenced: Moyer & Landauer, 1967; Halberda et al., 2008; Shah et al., 2023 (PLM prior study)",
            "statistical_significance": null,
            "notes": "Paper reports strong model signatures of human-like distance effect but does not report numeric human R^2 baselines; human descriptions are qualitative (distance & ratio effects) rather than tabulated comparisons here.",
            "uuid": "e7494.7",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Pythia-2.8B_BLIMP",
            "name_full": "Pythia-2.8B on BLiMP",
            "brief_description": "Pythia-2.8B evaluated on BLiMP acceptability tasks using LM-eval-harness probability comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Pythia-2.8B",
            "model_description": "Open-source Pythia family model (2.8B parameters).",
            "model_size": "2.8B parameters",
            "test_name": "BLiMP (linguistic acceptability)",
            "test_category": "linguistic ability",
            "test_description": "Minimal pair acceptability judgments measured by higher probability assigned to acceptable sentence.",
            "evaluation_metric": "accuracy",
            "human_performance": null,
            "llm_performance": "accuracy 0.827 (82.7%) as reported in Table 3",
            "prompting_method": "probability comparison (LM-eval-harness, zero-shot)",
            "fine_tuned": false,
            "human_data_source": "Warstadt et al., 2020 (BLiMP)",
            "statistical_significance": null,
            "notes": "No human BLiMP baseline values provided in this paper.",
            "uuid": "e7494.8",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Pythia-2.8B_Typicality",
            "name_full": "Pythia-2.8B on Typicality / Conceptual Understanding",
            "brief_description": "Pythia-2.8B evaluated for typicality alignment using latent representations and surprisal methods; Spearman correlations reported.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Pythia-2.8B",
            "model_description": "Open-source Pythia model; latent representations available enabling representational analyses.",
            "model_size": "2.8B parameters",
            "test_name": "Typicality effects (category typicality)",
            "test_category": "conceptual understanding",
            "test_description": "Model judged by Spearman correlation between model-derived typicality scores and human typicality norms.",
            "evaluation_metric": "Spearman's correlation",
            "human_performance": null,
            "llm_performance": "latent representation Spearman = 0.221; surprisal zero-shot = 0.273 (Table 7 / Table 3 aggregate)",
            "prompting_method": "latent representation (cosine similarity) and surprisal zero-shot; prompting not available for open-source in paper",
            "fine_tuned": false,
            "human_data_source": "Typicality norms referenced (Van Overschelde et al., 2004; De Deyne et al., 2008; Castro et al., 2021)",
            "statistical_significance": null,
            "notes": "Larger Pythia models show better alignment than smaller ones; paper does not provide direct human correlation baseline numbers beyond the norms used as ground truth.",
            "uuid": "e7494.9",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Pythia-2.8B_RPM",
            "name_full": "Pythia-2.8B on Raven's Progressive Matrices (RPM)",
            "brief_description": "Pythia-2.8B evaluated on textualized RPM problems using surprisal-based selection; accuracy reported.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Pythia-2.8B",
            "model_description": "Open-source Pythia model family member, evaluated on I-Raven textual representations.",
            "model_size": "2.8B parameters",
            "test_name": "Raven's Progressive Matrices (I-Raven adaptation)",
            "test_category": "fluid reasoning",
            "test_description": "Select correct tuple representing missing cell in a 3x3 matrix mapped to textual attributes.",
            "evaluation_metric": "accuracy",
            "human_performance": null,
            "llm_performance": "accuracy 0.760 (76.0%) as reported in Table 3",
            "prompting_method": "zero-shot surprisal over full-sequence (instruction + candidate tuple)",
            "fine_tuned": false,
            "human_data_source": "Raven, 2003 and mappings from Hu et al., 2023; Webb et al., 2023",
            "statistical_significance": null,
            "notes": "Paper reports RPM accuracy for Pythia-2.8B but does not include direct human baseline scores for the same textualized item set.",
            "uuid": "e7494.10",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Llama-2-7B_Numeric",
            "name_full": "Llama-2-7B on Magnitude Comparison Effects",
            "brief_description": "Llama-2-7B evaluated for latent number representations; R^2 reported for distance and ratio effects using cosine similarity analyses.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B",
            "model_description": "Meta's Llama-2 family, 7B parameter decoder transformer (open-source variant used).",
            "model_size": "7B parameters",
            "test_name": "Magnitude comparison effects (Distance & Ratio)",
            "test_category": "numeric cognition",
            "test_description": "Assess whether model latent representations show distance and ratio effects consistent with human mental number line.",
            "evaluation_metric": "R^2 of fitted functions (distance linear fit; ratio negative-exponential fit)",
            "human_performance": null,
            "llm_performance": "Distance effect R^2 = 0.670; Ratio effect R^2 = 0.614 (Table 3 / Table 4)",
            "prompting_method": "latent-representation analysis across layers and input formats (cosine similarity)",
            "fine_tuned": false,
            "human_data_source": "Moyer & Landauer 1967; Halberda et al., 2008 (human behavioral effects referenced)",
            "statistical_significance": null,
            "notes": "Paper presents model R^2 values indicating humanlike distance effect; no numerical human R^2 baseline reported.",
            "uuid": "e7494.11",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Llama-2-7B_BLIMP",
            "name_full": "Llama-2-7B on BLiMP",
            "brief_description": "Llama-2-7B evaluated on BLiMP acceptability tasks with LM-eval-harness probability scoring.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B",
            "model_description": "Llama-2 7B parameter open-source model.",
            "model_size": "7B parameters",
            "test_name": "BLiMP",
            "test_category": "linguistic ability",
            "test_description": "Minimal pair acceptance via probability ranking.",
            "evaluation_metric": "accuracy",
            "human_performance": null,
            "llm_performance": "accuracy 0.818 (81.8%) (Table 3)",
            "prompting_method": "probability comparison (LM-eval-harness, zero-shot)",
            "fine_tuned": false,
            "human_data_source": "Warstadt et al., 2020 (BLiMP)",
            "statistical_significance": null,
            "notes": "Paper reports BLiMP accuracy per phenomenon (morphology, syntax, semantics) but does not present human baseline accuracy here.",
            "uuid": "e7494.12",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Llama-2-7B_Typicality",
            "name_full": "Llama-2-7B on Typicality (Concept Understanding)",
            "brief_description": "Llama-2-7B evaluated for typicality alignment using latent representation and surprisal methods; Spearman correlations reported (latent rep negative in aggregate).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B",
            "model_description": "Open-source Llama-2 7B model; latent representations available.",
            "model_size": "7B parameters",
            "test_name": "Typicality effects",
            "test_category": "conceptual understanding",
            "test_description": "Spearman correlation between model-derived scores (latent similarity or surprisal) and human typicality norms.",
            "evaluation_metric": "Spearman's correlation",
            "human_performance": null,
            "llm_performance": "latent-representation Spearman = -0.065; surprisal zero-shot = 0.238 (Table 3 and Table 7)",
            "prompting_method": "latent representation (cosine similarity) and surprisal zero-shot",
            "fine_tuned": false,
            "human_data_source": "Human category norms as referenced (Van Overschelde et al., 2004; De Deyne et al., 2008)",
            "statistical_significance": null,
            "notes": "Negative/near-zero latent-representation correlations indicate weak alignment for that method; surprisal yields small positive alignment. No human numerical baseline correlations provided in paper beyond the norms used to compute correlations.",
            "uuid": "e7494.13",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Llama-2-7B_RPM",
            "name_full": "Llama-2-7B on Raven's Progressive Matrices",
            "brief_description": "Llama-2-7B evaluated on textualized RPM using surprisal-based selection; accuracy reported.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B",
            "model_description": "Llama-2 7B parameter model evaluated on I-Raven formatted items.",
            "model_size": "7B parameters",
            "test_name": "Raven's Progressive Matrices (I-Raven adaptation)",
            "test_category": "fluid reasoning",
            "test_description": "Textual tuple representation of visual RPM items; model selects option with highest sequence probability.",
            "evaluation_metric": "accuracy",
            "human_performance": null,
            "llm_performance": "accuracy 0.752 (75.2%) as reported in Table 3",
            "prompting_method": "zero-shot surprisal of completed sequence",
            "fine_tuned": false,
            "human_data_source": "Raven, 2003; I-Raven mapping cited via Hu et al., 2023 and Webb et al., 2023",
            "statistical_significance": null,
            "notes": "Paper reports model accuracy; no human baseline for the adapted textual item set is provided.",
            "uuid": "e7494.14",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Pythia-suite_development",
            "name_full": "Pythia model suite developmental trajectories (magnitude, language, concept, fluid reasoning)",
            "brief_description": "The Pythia family (models from 70M to 12B parameters) evaluated across intermediate checkpoints to study developmental alignment to human cognitive development; trajectories plotted as function of tokens seen.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Pythia suite (multiple sizes: 70M–12B)",
            "model_description": "Open-source suite of models trained with known order and many intermediate checkpoints; used to analyze learning trajectories across training tokens.",
            "model_size": "70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, 12B (different models in suite)",
            "test_name": "Developmental trajectories on psychometric tasks (Numeric distance/ratio, BLiMP, Typicality, RPM)",
            "test_category": "multiple (numeric cognition, linguistic ability, conceptual understanding, fluid reasoning)",
            "test_description": "Performance on psychometric-style tasks tracked over intermediate checkpoints (number of tokens seen) to identify windows of monotonic development and alignment with human developmental timing.",
            "evaluation_metric": "varies by task (R^2 for numeric effects; accuracy for BLiMP and RPM; Spearman for typicality); reported as function over tokens seen",
            "human_performance": null,
            "llm_performance": "see task-specific entries (e.g., Pythia-2.8B distance R^2=0.961, BLiMP acc=0.827, typicality surprisal zero-shot=0.273, RPM acc=0.760); developmental windows observed ~100M–20B tokens",
            "prompting_method": "latent-representation analyses and surprisal/probability comparisons across checkpoints; zero- and few-shot prompting for some conceptual tasks",
            "fine_tuned": false,
            "human_data_source": "Developmental alignment motivated by human developmental literature cited (Elman, 1996; child language corpora references; psychometric theory references)",
            "statistical_significance": null,
            "notes": "Paper's major finding: a consistent window of maximal alignment to human development across tasks occurs during pretraining; human developmental 'baselines' (e.g., ages when abilities emerge) are discussed qualitatively, but no numeric human performance baselines are reported for direct quantitative comparison.",
            "uuid": "e7494.15",
            "source_info": {
                "paper_title": "Development of Cognitive Intelligence in Pre-trained Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Time required for judgements of numerical inequality",
            "rating": 2
        },
        {
            "paper_title": "Individual differences in nonverbal number acuity correlate with maths achievement",
            "rating": 2
        },
        {
            "paper_title": "Benchmark of Linguistic Minimal Pairs for English",
            "rating": 2
        },
        {
            "paper_title": "Raven progressive matrices",
            "rating": 2
        },
        {
            "paper_title": "Category norms of verbal items in 56 categories a replication and extension of the connecticut category norms",
            "rating": 2
        },
        {
            "paper_title": "Exemplar by feature applicability matrices and other dutch normative data for semantic concepts",
            "rating": 2
        },
        {
            "paper_title": "Numeric magnitude comparison effects in large language models",
            "rating": 2
        },
        {
            "paper_title": "How well do deep learning models capture human concepts? the case of the typicality effect",
            "rating": 2
        }
    ],
    "cost": 0.0242135,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Development of Cognitive Intelligence in Pre-trained Language Models</h1>
<p>Raj Sanjay Shah, Khushi Bhardwaj, Sashank Varma<br>Georgia Institute of Technology<br>{rajsanjayshah, khushi.bhardwaj, varma}@gatech.edu</p>
<h4>Abstract</h4>
<p>Recent studies show evidence for emergent cognitive abilities in Large Pre-trained Language Models (PLMs). The increasing cognitive alignment of these models has made them candidates for cognitive science theories. Prior research into the emergent cognitive abilities of PLMs has largely been path independent to model training, i.e., has focused on the final model weights and not the intermediate steps. However, building plausible models of human cognition using PLMs would benefit from considering the developmental alignment of their performance during training to the trajectories of children's thinking. Guided by psychometric tests of human intelligence, we choose four sets of tasks to investigate the alignment of ten popular families of PLMs and evaluate their available intermediate and final training steps. These tasks are Numerical ability, Linguistic abilities, Conceptual understanding, and Fluid reasoning. We find a striking regularity: regardless of model size, the developmental trajectories of PLMs consistently exhibit a window of maximal alignment to human cognitive development. Before that window, training appears to endow models with the requisite structure to be poised to rapidly learn from experience. After that window, training appears to serve the engineering goal of reducing loss but not the scientific goal of increasing alignment with human cognition.</p>
<h2>1 Introduction</h2>
<p>Large Pre-trained Language Models (PLMs) like Google's Gemini (Team et al., 2023), Meta's LLaMA 2 (Touvron et al., 2023), and OpenAI's GPT 4 (OpenAI, 2023a) show human-level or even super-human performance on many cognitive performance tasks. This is true in domains such as mathematical reasoning (Shah et al., 2023; Ahn et al., 2024), language comprehension (Warstadt et al., 2020; Ye et al., 2023; Koubaa, 2023), concept understanding (Vemuri et al., 2024), and analogi-
cal reasoning (Webb et al., 2023; Hu et al., 2023). These successes have contributed to the hype of reaching Artificial General Intelligence (AGI).</p>
<p>Such claims deserve to be scrutinized. There is a massive disparity between the training data scale of PLMs and humans. However, PLMs unintentionally acquire human performance characteristics from the corpora they are trained on, through residues of the values, beliefs, and biases of the authors of the texts (Pellert et al., 2024).</p>
<p>We approach the human alignment of PLMs by grounding their evaluation in frameworks for $p s y$ chometric intelligence. Psychometric measures of intelligence include multiple subtests spanning a range of abilities, including mathematical thinking, language comprehension, spatial thinking, and fluid reasoning (Snow et al., 1984; Carroll, 1993; Sternberg, 2000; McGrew, 2009; Haier, 2023). In this work, we choose representative assessments of different facets of human intelligence, modified for the required textual modality, to evaluate the cognitive alignment of PLMs.</p>
<p>A second goal of our work is to move beyond cognitive alignment to also evaluate the developmental alignment of PLMs. The claim that the final model state of a PLM approximates adult performance leaves open the question of the path by which it arrived there. Ideally, the model's performance improvements over training would also track the progression of cognitive abilities over development (Elman, 1996; Bengio et al., 2009). This potential parallelism would be stronger evidence for PLMs as cognitive science models. Researchers are increasingly addressing this question by building PLMs trained on a developmentally plausible corpus of child-directed speech, transcribed dialogue, and children's literature (Huebner et al., 2021; Warstadt et al., 2023; Bhardwaj et al., 2024).</p>
<p>We ask the question of developmental alignment in a theoretically important way: Is the cognitive alignment of PLMs achieved in a path-independent</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A list of cognitive intelligence tasks under consideration.
or path-dependent manner? Prior studies focusing on the cognitive alignment of PLMs have only established path independence: that models at the end of training approximate adult performance across various domains. Here, we also evaluate path dependence: Do the performance improvements of PLMs over training track the growth of these abilities in children over development? We ask this question for models of different sizes and track their developmental alignment over millions and billions of training tokens.
To summarize, our key contributions are as follows:</p>
<ul>
<li>Cognitive Modelling using AI: We test the appropriateness of PLMs for cognitive modeling by evaluating whether their performance profiles match those of adults.</li>
<li>Developmental trajectories in LLM pretraining and scaling: Previous studies have largely focused on evaluating the final training checkpoints of PLMs for their cognitive plausibility and have neglected the question of developmental trajectories. Here, we also ask: Can PLMs be used to model developmental trajectories of children's thinking despite the training data scale mismatch?</li>
<li>Representative tasks: We choose representative tasks of human cognition taken from psychometric tests of intelligence tests. These tasks evaluate numeric, linguistic, conceptual, and fluid intelligence. We propose these to be a prerequisite for considering PLMs as cognitive science models.</li>
</ul>
<h2>2 Related work</h2>
<h3>2.1 Psychometric theories of intelligence</h3>
<p>Previous studies of the human alignment of ML models have typically looked at singular dimensions, such as numeric abilities (Zhuang et al., 2023; Fang et al., 2024). Rather than choose cognitive abilities in a piecemeal fashion, we look to psychometric theories of intelligence for guidance (Sternberg, 2000). These theories distill performance on a large number of subtests into a small number of latent factors. Despite popular attention to "general intelligence" and the latent factor $g$, there is a long history of theories positing that intelligence is composed of multiple domain-specific abilities. An important, early domain-specific theory of intelligence included seven "primary abilities" (Thurstone, 1938). The most widespread psychometric theory of intelligence today, the Cattell-Horn-Carrol (CHC) theory (Carroll, 1993; McGrew, 2009), includes among its "broad" abilities quantitative knowledge, reading and writing ability, fluid reasoning, and "comprehension" knowledge (a subcomponent of which is conceptual understanding). We evaluate the cognitive and developmental alignment of PLMs along these four abilities.</p>
<h3>2.2 Emergent cognitive abilities in Language Models</h3>
<p>Recently, the performance of language models has improved as they have increased in size from millions to billions of parameters, trained on larger corpora, and further tuned in novel ways (instruction</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Cognitive Domain</th>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Source</th>
<th style="text-align: left;">License</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Numeric Abilities</td>
<td style="text-align: left;">Magnitude Comparison Effects</td>
<td style="text-align: left;">(Shah et al., 2023)</td>
<td style="text-align: left;">(cc by 4.0)</td>
</tr>
<tr>
<td style="text-align: left;">Linguistic Abilities</td>
<td style="text-align: left;">BLiMP</td>
<td style="text-align: left;">(Warstadt et al., 2020)</td>
<td style="text-align: left;">(cc by 4.0)</td>
</tr>
<tr>
<td style="text-align: left;">Concept Understanding</td>
<td style="text-align: left;">Typicality Effects</td>
<td style="text-align: left;">(Vemuri et al., 2024; Castro et al., 2021)</td>
<td style="text-align: left;">(cc by 4.0)</td>
</tr>
<tr>
<td style="text-align: left;">Fluid reasoning</td>
<td style="text-align: left;">Raven's Progressive Matrices</td>
<td style="text-align: left;">(Hu et al., 2023)</td>
<td style="text-align: left;">(cc by 4.0)</td>
</tr>
</tbody>
</table>
<p>Table 1: Summary of assessments.
tuned, RLHF). This has led researchers to increasingly advocate for the use of PLMs as cognitive models (Piantadosi, 2023; Warstadt and Bowman, 2024). Increasing the number of parameters of the models has given rise to emergent abilities that cannot be predicted by extrapolating from the performance of smaller models (Wei et al., 2022a). Emergent abilities have been observed in a variety of task types such as multi-task language understanding (Hendrycks et al., 2021), grounded conceptual mapping (Patel and Pavlick, 2022), and truthfulness (Lin et al., 2021). In recent work, Hoffmann et al. (2022) and Biderman et al. (2023) have shown the benefits of training a model for more tokens on problem-solving (Wei et al., 2022b), commonsense reasoning (Sakaguchi et al., 2021), arithmetic abilities (Biderman et al., 2023), and linguistic performance (Paperno et al., 2016). Although the presence of emergent abilities extends to cognitive science domains (Wei et al., 2022b; Goertzel, 2023; Hagendorff, 2023), prior studies have been piecemeal in their approach and have failed to (1) consider multiple cognitive abilities as specified by theories of psychometric intelligence and (2) move beyond cognitive alignment to also evaluate the developmental alignment of PLMs over training.</p>
<h3>2.3 Pre-trained language model use in developmental modeling</h3>
<p>Recently, researchers have begun advocating for the use of PLMs for modeling cognitive development in children (Kosoy et al., 2023; Salewski et al., 2024). For example, Portelance et al. (2023) and Bhardwaj et al. (2024) suggest the use of language models to predict the age of acquisition of words in children. Researchers have also proposed studying second language acquisition and bilingualism by mapping pre-training steps in PLMs to understand the rate of language development (Evanson et al., 2023; Marian, 2023; Sharma et al., 2024). We evaluate the proposal that the performance of intermediate training checkpoints of PLMs maps to points during children's cognitive development.</p>
<h2>3 A suite of psychometric intelligence tasks</h2>
<p>We assemble a suite of tasks that benchmark PLMs across four facets of psychometric intelligence. Table 1 summarizes the tasks along with the licensing details for public use. The details of each assessment and their respective operationalization are given below. ${ }^{1}$</p>
<h3>3.1 Numeric abilities</h3>
<p>123
Figure 2: Mental Number Line: Organization of magnitude representations in a logarithmically scaled manner.</p>
<p>The question of how humans understand symbolic numbers has been investigated by cognitive scientists for more than half a century. These studies show that people map number symbols to a mental number line (MNL, Figure 2) with a logcompressed psychophysical scale (Moyer and Landauer, 1967).</p>
<p>Prior research on the numerical abilities of PLMs has focused on higher-order and application-driven aspects of mathematics such arithmetic equations and word problems (Burns et al., 2021; Amini et al., 2019; Yuan et al., 2023), exact facts (Lin et al., 2020), and measurement estimation (Zhang et al., 2020). However, these tasks fail to directly tap the foundational cognitive mechanisms underlying human numerical understanding: recruitment of the MNL.</p>
<p>In a recent study, Shah et al. (2023) found evidence for a human-like MNL in various PLMs. They showed that despite lacking explicit neural circuitry to represent numbers, through experience (i.e., vast amounts of training data), PLMs show human-like performance profiles and learn latent representations consistent with the MNL.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We follow Shah et al. (2023) and look for the two behavioral signatures of a compressed MNL representation, the distance effect and the ratio effect. In humans, these are defined as:</p>
<ul>
<li>Distance effect (refer to Figure 1A - top): The greater the distance $|x-y|$ between two numbers $x$ and $y$, the faster they are compared, i.e., the greater (or lesser) number is identified (Moyer and Landauer, 1967).</li>
<li>Ratio effect (refer to Figure 1A - bottom): The time to compare two numbers $x$ and $y$ decreases nonlinearly as a function of the ratio of the larger number over the smaller number $\frac{\max [x, y]}{\min [x, y]}$ (Halberda et al., 2008).</li>
</ul>
<p>These effects can be mapped to language models by adopting the following linking hypothesis: the greater the cosine similarity of two number representations in a PLM, the more difficult it is to discriminate them (i.e., to judge which one is greater (or lesser)), and thus the longer it takes. While we focus on the Distance and Ratio effects in PLMs, the results for all the effects investigated by Shah et al. (2023) are in Appendix B.1.</p>
<p>Operationalization: We used the same protocol as Shah et al. (2023). For each effect, we test three formats of number representations of PLMs: mixed-case number words, lower-case number words, and digits. We present the $R^{2}$ values for the Distance and Ratio effects, which are averaged across each input representation. The $R^{2}$ values for the distance effect in PLMs are obtained by fitting a linear function predicting the cosine similarity of $x$ and $y$ from their distance $|x-y| . R^{2}$ values for the ratio effect in PLMs are obtained by fitting a negative exponential function predicting the normalized cosine similarity of $x$ and $y$ from their ratio $\frac{\max (x, y)}{\min (x, y)}$. Note: This task requires access to the latent representations of models.</p>
<h3>3.2 Linguistic abilities</h3>
<p>Language (or verbal) ability is a central component of human cognition and cognitive neuroscience (Hagoort, 2019). At the dawn of the cognitive revolution, it was conceptualized as a largely innate ability, and language acquisition was thought to require relatively little learning from experience (Fodor, 1985; Chomsky, 2014). More recently, cognitive developmentalists have shown that infants can learn language through exposure to the statisti-
cal regularities of the linguistic environment (Saffran et al., 1996; Siegelman, 2020). These findings have been modeled using multi-layer perceptrons (Elman, 1996) and, more recently, PLMs (Lake and Murphy, 2023).</p>
<p>We use BLiMP (Benchmark of Linguistic Minimal Pairs for English) (Warstadt et al., 2020) to evaluate the linguistic abilities of each PLM under consideration. BLiMP consists of 67 datasets of 1000 pairs of minimally different sentences which vary in acceptability and span 12 phenomena at three levels of language: morphology, syntax, and semantics. The 12 phenomena are described in Appendix B.2. Each pair consists of one acceptable sentence and one unacceptable sentence which otherwise differ minimally. BLiMP evaluates the models by measuring if they assign a higher probability to the acceptable vs. unacceptable sentence of each pair. Figure 1B shows two examples of minimal pairs.</p>
<p>Operationalization: We use the LM-evalharness (Gao et al., 2023) benchmarking suite to test our models on the BLiMP tasks. We evaluate if a model assigns a higher sequential probability to the acceptable sentence. Note: This requires models that can generate probabilities of tokens.</p>
<h3>3.3 Concept understanding</h3>
<p>On encountering a new stimulus, humans categorize it - assign it to a known concept - in order to make inferences about its unobservable properties (Murphy, 2002). A striking finding is that not all members of a category are equal (Rosch, 1975). Rather, for a given category (e.g., Bird), some members (e.g., pigeon) are more typical than others (e.g., ostrich). This phenomenon, known as the typicality effect, is a central feature of human categorization (Lakoff, 2008).</p>
<p>Typicality gradients in human categories can be measured using the production task, where participants are given a category label (e.g., Bird) and asked to list as many members of the category as they can in a limited time (Battig and Montague, 1969; Van Overschelde et al., 2004; Castro et al., 2021). The typicality of an item is defined as the proportion of participants who produce it.</p>
<p>Language models have shown some evidence of human-like typicality gradients. Heyman and Heyman (2019) used word2vec embeddings to predict the category typicality norms released by De Deyne et al. (2008). More recent work by Misra et al.</p>
<p>(2021) and Bhatia and Richie (2022) has looked at correlations of PLMs like BERT, RoBERTa, and GPT-2 to the Rosch (1975) typicality norms for ten categories. Vemuri et al. (2024) performed the most comprehensive study of the alignment of concept understanding in the latent representations of PLMs. We expand upon their task setup to evaluate human-like concept understanding in the PLMs that are the focus here.</p>
<p>Operationalization: For each model, we calculate the representativeness of a member to its category in three possible ways:</p>
<ul>
<li>Closeness judgment problem: Calculate the cosine similarity between the obtained latent representations for the member and the category. This requires models where the latent representations are readily available.</li>
<li>Surprisal values: For each member of a category, the probability of the sequence a "member" (eg. pigeon) is a "category" (eg. bird). This method requires access to the probability of each token in a sequence.</li>
<li>Prompting: Prompt the models with the following design: Guidelines, Query, and Options. The Guideline highlights the task of re-ranking the members given in the Options based on appropriateness with the Query. The Query consists of the in-filling task: $A$ $\qquad$ is a [category name]. The Options are each of the possible members of the category. Given the complexity of the prompting, usable outputs are only obtained from models that are larger than 30 billion parameters.</li>
</ul>
<p>For the two in-filling problems (i.e., based on surprisal values and prompting), we also evaluate models on zero to three exemplars as context. The details of the experiments on these different exemplar contexts are given in Appendix B.3.</p>
<h3>3.4 Fluid reasoning</h3>
<p>Humans can logically parse information and detect patterns in novel stimuli without having to rely on prior experiences or learned information. This ability is called fluid reasoning (Cattell, 1963).</p>
<p>We focus on the dominant measure of fluid reasoning, the Ravens Progressive Matrices (RPM) test (Raven, 2003). An example Ravens-like problem is given in Figures 1D and 3. An RPM item consists of a $3 \times 3$ matrix of cells with one empty cell. Participants must induce the underlying, abstract patterns that hold across the rows and columns of
the matrix, and apply these to infer the image in the empty cell from a given set of options. These images vary in visual attributes like shape and color, along with more abstract qualities. The RPM is the standard measure of fluid reasoning (Snow et al., 1984) and is highly correlated with analogical reason (Goswami, 1986; Webb et al., 2023).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Example adaptation of visual RPM problems to the textual format. Each image is decomposed into tuples of (type, size, color). Type indicates the shape of the image.</p>
<p>Given the visual nature of the RPM, previous work by Hu et al. $(2021,2023)$ and Webb et al. (2023) mapped the Raven-10000 dataset to a textual format to facilitate the testing of PLMs. The mapping involves reformulating visual elements into text-based numerical tuples representing attributes like shape, size, and color textually, as illustrated in Figure 3, to form the I-Raven dataset. We use their approach with a focus on the "Center Single Alignment" sub-task, which features a single shape per matrix cell. We differ from their work by evaluating a broader set of models.</p>
<p>Operationalization: We determine the model's preferred answer for a problem by comparing the surprisal values of the whole sequence (instruction, question, candidate tuple) for each of the candidate options, i.e. the probability of each completed digit representation of a matrix. For the example given in Figure 3, this would be checking the probability of this sequence (summation of token probabilities) with the correct answer $(\mathbf{3}, \mathbf{0 . 6}, \mathbf{0 . 8})$ to the other candidates. A complete list of the prompts used in this paper is given in Appendix B. 4 .</p>
<h2>4 Models under consideration</h2>
<p>We evaluate a wide range of language model families, shown in Table 2. These models are selected based on the following criteria:</p>
<p>Public availability: Open-source models allow us to perform a thorough analysis by accessing the latent representation and the token probability during generation. We follow Holt et al. (2024)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">Latent rep.</th>
<th style="text-align: center;">Token prob.</th>
<th style="text-align: center;">Multiple sizes</th>
<th style="text-align: center;">Intermediate checkpoints</th>
<th style="text-align: center;">Known training order</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Amber</td>
<td style="text-align: center;">(Liu et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Falcon</td>
<td style="text-align: center;">(Almazrouei et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Starling</td>
<td style="text-align: center;">(Zhu et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Llama</td>
<td style="text-align: center;">(Touvron et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral</td>
<td style="text-align: center;">(Jiang et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Qwen</td>
<td style="text-align: center;">(Bai et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Pythia</td>
<td style="text-align: center;">(Biderman et al., 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Gemini</td>
<td style="text-align: center;">(Team et al., 2023)</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-Turbo</td>
<td style="text-align: center;">(OpenAI, 2023b)</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT 4</td>
<td style="text-align: center;">(OpenAI, 2023a)</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
</tbody>
</table>
<p>Table 2: List of language model families under consideration with their statistics.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Numeric Abilities</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Linguistic Abilities</th>
<th style="text-align: center;">Conceptual Understanding</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluid reasoning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Distance <br> Effect $\left(R^{2}\right)$</td>
<td style="text-align: center;">Ratio <br> Effect $\left(R^{2}\right)$</td>
<td style="text-align: center;">BLiMP <br> (Acc.)</td>
<td style="text-align: center;">Latent Rep. <br> (Average Spearman's Correlation)</td>
<td style="text-align: center;">Zero Shot <br> (Acc.)</td>
<td style="text-align: center;">RPM <br> (Acc.)</td>
</tr>
<tr>
<td style="text-align: center;">Amber-7B</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.794</td>
<td style="text-align: center;">0.083</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.654</td>
</tr>
<tr>
<td style="text-align: center;">Falcon-7B</td>
<td style="text-align: center;">0.928</td>
<td style="text-align: center;">0.838</td>
<td style="text-align: center;">0.817</td>
<td style="text-align: center;">$-0.116$</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">0.730</td>
</tr>
<tr>
<td style="text-align: center;">Starling-LM-7B-alpha</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.827</td>
<td style="text-align: center;">$-0.003$</td>
<td style="text-align: center;">0.258</td>
<td style="text-align: center;">0.730</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-7B</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">$-0.065$</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.752</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-13B</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.793</td>
<td style="text-align: center;">0.076</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.756</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3-8B</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">0.735</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.796</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3-8B-Instruct</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.128</td>
<td style="text-align: center;">0.810</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">0.641</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.829</td>
<td style="text-align: center;">$-0.025$</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.756</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B-Instruct</td>
<td style="text-align: center;">0.637</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.834</td>
<td style="text-align: center;">0.033</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.674</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-0.5B</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.553</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">0.072</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.684</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-1.8B</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.114</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.746</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-4B</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.770</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-7B</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.616</td>
<td style="text-align: center;">0.789</td>
<td style="text-align: center;">0.006</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.766</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-14B</td>
<td style="text-align: center;">0.783</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">$-0.140$</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.776</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-70M</td>
<td style="text-align: center;">0.829</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.194</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-160M</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.665</td>
<td style="text-align: center;">0.749</td>
<td style="text-align: center;">0.067</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.448</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-410M</td>
<td style="text-align: center;">0.926</td>
<td style="text-align: center;">0.679</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.126</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.608</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-1B</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">0.806</td>
<td style="text-align: center;">0.090</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.674</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-1.4B</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.764</td>
<td style="text-align: center;">0.819</td>
<td style="text-align: center;">0.074</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.730</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-2.8B</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.827</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.760</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-6.9B</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.713</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.716</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-12B</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.829</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.756</td>
</tr>
<tr>
<td style="text-align: center;">Gemini</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">$0.311^{*}$</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">0.825</td>
<td style="text-align: center;">$0.242^{*}$</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.792</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">$0.559 *$</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.822</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance of Pre-trained Language Models on the tasks. Distance Effect: Averaged $R^{2}$ values of different LLMs when fitting a linear function on the cosine-similarity vs. distance plot. Ratio Effect: Averaged $R^{2}$ values of different LLMs when fitting a negative exponential function on the cosine-similarity vs. ratio plot. Note: Each value is averaged across all three input types and all model layers to produce one generalizable score. Latent Rep: Average Spearman's Correlation when using the cosine similarity and latent representation-based approach (Note: * refers to the prompting approaches for select models which are gated by APIs, and not the latent representation-based approach), Zero-Shot: Average Spearman's Correlation when using the zero-shot surprisal values, BLiMP: The Benchmark of Linguistic Minimal Pairs for English, RPM: Raven's Progressive Matrices
while choosing PLMs. Although most models in this study are publicly available and open-source, we use three state-of-art commercial PLMs that are gated behind API calls; GPT-3.5-Turbo (pointing to gpt-3.5-turbo-0613 on the OpenAI platform), GPT4 (pointing to gpt-4-1106 on the OpenAI platform), and Gemini (also referred to as Gemini-1-Pro at the time of writing). The GPT- $x$ model APIs provide token probabilities of the response, allowing us to calculate surprisal, while Gemini does not.</p>
<p>Availability of multiple sizes: The availability of model sizes for the same architecture and training paradigms allows us to evaluate the emergent cognitive abilities of the models. We have multiple sizes available for the LLama-2, Qwen, and the Pythia family of models.</p>
<p>Availability of intermediate training checkpoints: This allows us to evaluate the effects of pre-training on the model outputs. Together, the availability of multiple model sizes and intermediate training</p>
<p>checkpoints allow us to best evaluate the developmental alignment of PLMs. Amber and Pythia's family of models have available intermediate training checkpoints. While Amber has 360 intermediate checkpoints, the checkpoints are at 4 Billion tokens each and are not at the required granularity.</p>
<p>Pythia Family of models: Pythia (Biderman et al., 2023) is one of the first open-source projects with the goal of scientific and transparent model development. It has 8 model sizes ranging from 70 Million to 12 Billion parameters, with each model trained on 286 Billion tokens. The models in the suite are equivalent (in size) to popular decoder architectures like GPT-Neo-(125M, 1.3B, 2.7B) and OPT-(125M, 350M, 1.3B, 2.7B, 6.7B), but with the added benefits of training on a known de-duplicated corpus (Gao et al., 2020), using the same training order for each model size, and having 154 intermediate checkpoints to study the learning trajectories of PLMs. Thus, the Pythia suite of models is ideal for studying the cognitive and developmental alignment of PLMs to humans.</p>
<p>All open-source models are obtained from Huggingface (Wolf et al., 2020), while the gated models are obtained from their respective platforms through API calls. For each model in the Pythia suite, the following intermediate checkpoints are available: [1, 2, 4, 8, ... 512; 1000, 2000, 3000 ... 143000 (exponential increase in checkpoint number until the 512th checkpoint and subsequent progression of 1000 steps until the last checkpoint)], with each checkpoint representing 2 Million tokens seen. Overall, we test 1232 intermediate checkpoints of the Pythia suite of models across all the tasks.</p>
<h2>5 Cognitive and developmental alignment of PLMs</h2>
<p>The suite of tasks enables comprehensive evaluation of a variety of PLMs on their cognitive alignment to humans across four domains of psychometric intelligence: numeric abilities, linguistic abilities, concept understanding, and fluid reasoning. Table 3 highlights the key results of this evaluation. For the evaluation of conceptual understanding in PLMs, we only report the results for the zero-shot surprisal values and latent representations. This is because we see similar results for zero-shot and few-shot surprisal value-based methods (see comprehensive results in Appendix B.3).</p>
<p>The cognitive alignment of PLMs on psychometrics assessments is summarized below:</p>
<ul>
<li>Numeric abilities: All PLMs show a humanlike distance effect but weakly show a humanlike ratio effect. We do not observe any notable changes in alignment with model scaling, indicating the need for the evaluation of future models on this task.</li>
<li>Linguistic abilities: The accuracy of the PLMs on the BLiMP linguistic acceptability tasks improves upon increasing the number of parameters. Furthermore, we find that all PLMs are substantially more accurate on morphological tasks over syntactic and semantic tasks (Accuracy: semantic $&lt;$ syntax $\ll$ morphology; see Appendix Table 5).</li>
<li>Concept understanding: Prompting methods in commercial models perform substantially better than other methods - closeness judgment and surprisal values - on all open-source models. In the Pythia suite, we observe that larger models outperform smaller counterparts on the same training data.</li>
<li>Fluid reasoning: For all PLM architecture types, larger models outperform their smaller equivalent models.</li>
<li>Despite differences in PLM architecture type, all models of an approximate size of 7 Billion parameters perform comparably.</li>
</ul>
<p>The developmental alignment of the PLMs on the tasks is shown in Figure 4. We make the following key observations:</p>
<ul>
<li>Training endows the "blank slate" with requisite structure: In each assessment, the model "warm-ups" in training on a few million/ billion tokens, moving from a "blank slate" to possessing the requisite structure. This structure can be thought of as the child's endowment at birth. Development of the four abilities begins only after reaching this state.</li>
<li>Training shows a region of development: For all four tasks, we see a window of monotonic development, in which all models gain the respective cognitive abilities.</li>
<li>After development, training appears to serve an engineering goal: After the window of development, the metric becomes unstable once the phenomena are learned. The training appears to serve the engineering goal of loss reduction (Chen et al., 2023). This observation is especially pronounced for numeric abilities and conceptual understanding.</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Developmental trajectory of the Pythia suite of models on the psychometric intelligence tasks as a function number of tokens seen. We display the x -axis in a log-scaled manner as maximal development occurs in the range of 100 Million to 20 Billion tokens seen for all tasks. The windows of maximal development are illustrated by the blue shading.</p>
<ul>
<li>Assessments for Fluid Reasoning and Linguistic Abilities show significant gains with scaling and greater pre-training: For the assessments of these abilities, we see that the alignment score continues to increase as the PLMs are trained on a greater number of tokens. (Also, morphological performance develops first followed by syntax and then semantics; see Appendix Figure 7.) Furthermore, for these abilities, models also show scaling effects, with larger models outperforming smaller ones.</li>
<li>The relative positions of the windows weakly align with human development: Variation in the onsets of the windows is weakly consistent with what is known of cognitive development. For example, children acquire language early (i.e., during the preschool years), whereas the onset of improving fluid reasoning is later,
when children enter elementary school, and continues for longer, throughout adolescence. Correspondingly, the models significantly develop linguistic abilities while training on 250 Million to 7 Billion tokens, whereas they acquire fluid reasoning abilities later, while training on 1 to 20 Billion tokens.</li>
</ul>
<h2>6 Conclusions</h2>
<p>This paper investigates the appropriateness of using PLMs for human cognitive and developmental modeling. It uses representative assessments of four facets of psychometric intelligence: numeric abilities, linguistic abilities, conceptual understanding, and fluid reasoning. Our experiments show that PLMs develop cognitive abilities purely through their experience in the world, indicating that the cognitive abilities tested are acquirable through mere exposure to language distributions and do</p>
<p>not necessarily require innate human-like inductive biases. Most significantly, we find a window of monotonic development in which all models improve approximately linearly on the four cognitive abilities. Before that window, we interpret training as endowing "blank slate" models with the requisite structure for rapid learning. Also notable is the finding of PLM scaling effects for the assessments of linguistic abilities and fluid reasoning. We propose evaluation against these tasks as a prerequisite before treating PLMs as models of human cognition and its development.</p>
<h2>7 Limitations</h2>
<p>Some limitations of the work are as follows: (1) We use an aggregation of psychometric tests for PLMs. The limitations of each test are inherited in the suite of tasks. (2) The alignment scores may be wrongly interpreted when evaluating PLMs with these tasks. Alignment scores show the similarity of PLM outputs to human outputs on psychometric tests and indicate that PLMs do not need explicit neural circuitry for these intelligence tests. We do not suggest these models as proxies for humans in any manner and recommend further testing before use. (3) The developmental alignment of the models points towards the acquisition of human-like performance on the four psychometric assessments in the range of 100 Million to 20 Billion training tokens. This conclusion has two limitations: Pythia is the only suite of models with available intermediate checkpoints and, while unlikely, the observed developmental trajectories might be artifacts of the pre-training order. (4) The psychometric assessments for PLMs are adapted from similar human psychometric tests. Different ways of adaptation may lead to different results. Furthermore, while representative, these assessments are not exhaustive tests of human intelligence. Future work can expand to other tests like spatial and commonsense reasoning. (5) Some open source models like Llama-2 have larger 70 Billion parameter variants but we lack the compute resources to evaluate them. Large open-source models would lead to appropriate comparisons of performance with commercial models like GPT-4. (6) While our work evaluates changes in cognitive alignment with an increase in model size and the number of pre-training tokens, we do not control for different tuning methodologies like instruction tuning and reinforcement learning with human or artificial
intelligence feedback. Accounting for different tuning methods is computationally intensive for the 1200+ model checkpoints across 10 architectures.</p>
<h2>8 Ethical Considerations</h2>
<p>All tasks and corresponding datasets have low ethical risks and none expose sensitive information. Additionally, we obtain approval from the authors of each dataset for their use and release. There are no major risks associated with conducting this research beyond those associated with working with PLMs. There may be risks in misinterpreting the alignment scores when evaluating with the tests. The psychometric analysis of this study is one-way: we look for human performance characteristics and behaviors in PLMs. PLMs are experimental technologies and future work using this research should proceed with caution. Assessment of the tasks indicates PLM alignment - or the lack thereof - to human cognitive behavior. Indications of higher human alignment do not indicate an absolute proxy for humans. The goal of tasks in this work is a pre-cursor assessment of PLMs on their ability to act as cognitive models. Therefore, researchers and users should perform more tests before use.</p>
<h2>References</h2>
<p>Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. 2024. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157.</p>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. Falcon-40B: an open large language model with state-of-the-art performance.</p>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357-2367, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,</p>
<p>Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.
W. F. Battig and W. E. Montague. 1969. Category norms of verbal items in 56 categories a replication and extension of the connecticut category norms. Journal of Experimental Psychology Monographs, 80:1-46.</p>
<p>Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41-48.</p>
<p>Khushi Bhardwaj, Raj Sanjay Shah, and Sashank Varma. 2024. Pre-training llms using human-like development data corpus.
S. Bhatia and R. Richie. 2022. Transformer networks of human conceptual knowledge. Psychological Review.</p>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling.
I. Borg and P.J.F. Groenen. 2005. Modern Multidimensional Scaling: Theory and Applications. Springer.</p>
<p>Gregory C. Burgess, Jeremy R. Gray, Andrew R. A. Conway, and Todd Samuel Braver. 2011. Journal of experimental psychology : General neural mechanisms of interference control underlie the relationship between fluid intelligence and working memory span.</p>
<p>Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. CoRR, abs/2103.03874.</p>
<p>John B Carroll. 1993. Human cognitive abilities: A survey of factor-analytic studies. 1. Cambridge University Press.</p>
<p>Nichol Castro, Taylor Curley, and Christopher Hertzog. 2021. Category norms with a cross-sectional sample of adults in the united states: Consideration of cohort, age, and historical effects on semantic categories. Behavior research methods, 53(2):898-917.</p>
<p>Raymond B Cattell. 1963. Theory of fluid and crystallized intelligence: A critical experiment. Journal of educational psychology, 54(1):1.</p>
<p>Raymond Bernard Cattell. 1987. Intelligence: Its structure, growth and action. Elsevier.</p>
<p>Angelica Chen, Ravid Schwartz-Ziv, Kyunghyun Cho, Matthew L. Leavitt, and Naomi Saphra. 2023. Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in mlms. ArXiv, abs/2309.07311.</p>
<p>Noam Chomsky. 2014. Aspects of the Theory of Syntax. 11. MIT press.</p>
<p>Andrew R. A. Conway, Nelson Cowan, Michael F. Bunting, David J. Therriault, and Scott R. B. Minkoff. 2002. A latent variable analysis of working memory capacity, short-term memory capacity, processing speed, and general fluid intelligence. Intelligence, 30:163-183.
S. De Deyne, S. Verheyen, E. Ameel, W. Vanpaemel, M. J. Dry, W. Voorspoels, and G. Storms. 2008. Exemplar by feature applicability matrices and other dutch normative data for semantic concepts. Behavior research methods, 40:1030-1048.</p>
<p>Cody S. Ding. 2018. Fundamentals of Applied Multidimensional Scaling for Educational and Psychological Research. Springer International Publishing.</p>
<p>Jeffrey L Elman. 1996. Rethinking innateness: A connectionist perspective on development, volume 10. MIT press.</p>
<p>Linnea Evanson, Yair Lakretz, and Jean-Rémi King. 2023. Language acquisition: do children and language models follow similar learning stages? arXiv preprint arXiv:2306.03586.</p>
<p>Qixiang Fang, Daniel L Oberski, and Dong Nguyen. 2024. Patch-psychometrics-assisted benchmarking of large language models: A case study of mathematics proficiency. arXiv preprint arXiv:2404.01799.</p>
<p>Gustav Theodor Fechner. 1860. Elements of psychophysics. 1.</p>
<p>Jerry A Fodor. 1985. Precis of the modularity of mind. Behavioral and brain sciences, 8(1):1-5.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The pile: An 800gb dataset of diverse text for language modeling.</p>
<p>Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation.</p>
<p>Ben Goertzel. 2023. Generative ai vs. agi: The cognitive strengths and weaknesses of modern llms. arXiv preprint arXiv:2309.10371.</p>
<p>Usha Goswami. 1986. Children's use of analogy in learning to read: A developmental study. Journal of Experimental Child Psychology, 42(1):73-83.</p>
<p>Thilo Hagendorff. 2023. Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods. arXiv preprint arXiv:2303.13988.</p>
<p>Peter Hagoort. 2019. Human language: From genes and brains to behavior. MIT Press.</p>
<p>Richard J Haier. 2023. The neuroscience of intelligence. Cambridge University Press.</p>
<p>Justin Halberda, Michèle M. M. Mazzocco, and Lisa Feigenson. 2008. Individual differences in nonverbal number acuity correlate with maths achievement. Nature, 455(7213):665-668.</p>
<p>Joshua K. Hartshorne and Laura T. Germine. 2015. When does cognitive functioning peak? the asynchronous rise and fall of different cognitive abilities across the life span. Psychological Science, 26:433 443.</p>
<p>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2019. Semeval-2010 task 8 - multiway classification of semantic relations between pairs of nominals. arXiv preprint arXiv:1911.10422.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding.</p>
<p>Tom Heyman and Gert Heyman. 2019. Can predictionbased distributional semantic models predict typicality? Quarterly Journal of Experimental Psychology, 72:2084-2109.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.</p>
<p>Faye Holt, William Held, and Diyi Yang. 2024. Perceptions of language technology failures from south asian english speakers.</p>
<p>Sheng Hu, Yuqing Ma, Xianglong Liu, Yanlu Wei, and Shihao Bai. 2021. Stratified rule-aware network for abstract visual reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 35, pages 1567-1574.</p>
<p>Xiaoyang Hu, Shane Storks, Richard L Lewis, and Joyce Chai. 2023. In-context analogical reasoning with pre-trained language models. arXiv preprint arXiv:2305.17626.</p>
<p>Philip A. Huebner, Elior Sulem, Fisher Cynthia, and Dan Roth. 2021. BabyBERTa: Learning more grammar with small-scale child-directed language. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 624-646, Online. Association for Computational Linguistics.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.</p>
<p>Eliza Kosoy, Emily Rose Reagan, Leslie Lai, Alison Gopnik, and Danielle Krettek Cobb. 2023. Comparing machines and children: Using developmental psychology experiments to assess the strengths and weaknesses of lamda responses. arXiv preprint arXiv:2305.11243.</p>
<p>Anis Koubaa. 2023. GPT-4 vs. GPT-3.5: A concise showdown.</p>
<p>Brenden M Lake and Gregory L Murphy. 2023. Word meaning in minds and machines. Psychological review, 130(2):401.</p>
<p>George Lakoff. 2008. Women, fire, and dangerous things: What categories reveal about the mind. University of Chicago press.</p>
<p>Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. 2020. Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of PreTrained Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6862-6868, Online. Association for Computational Linguistics.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958.</p>
<p>Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, and Eric P. Xing. 2023. Llm360: Towards fully transparent open-source llms.</p>
<p>Viorica Marian. 2023. Studying second language acquisition in the age of large language models: Unlocking the mysteries of language and learning, a commentary on "age effects in second language acquisition: Expanding the emergentist account" by catherine 1. caldwell-harris and brian macwhinney. Brain and language, 246.</p>
<p>Kevin S McGrew. 2009. Chc theory and the human cognitive abilities project: Standing on the shoulders of the giants of psychometric intelligence research.</p>
<p>K. Misra, A. Ettinger, and J. T. Rayz. 2021. Do language models learn typicality judgments from text? arXiv preprint arXiv:2105.02987.</p>
<p>Robert S. Moyer and Thomas K. Landauer. 1967. Time required for judgements of numerical inequality. Nature, 215(5109):1519-1520.
G. Murphy. 2002. The Big Book of Concepts. MIT press.</p>
<p>OpenAI. 2023a. Gpt-4 technical report.
OpenAI. 2023b. New and improved embedding model. Accessed: 2023-08-14.</p>
<p>Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. 2016. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031.</p>
<p>John M. Parkman. 1971. Temporal aspects of digit and letter inequality judgments. Journal of Experimental Psychology, 91(2):191-205.</p>
<p>Roma Patel and Ellie Pavlick. 2022. Mapping language models to grounded conceptual spaces. In International Conference on Learning Representations.</p>
<p>Inc. Pearson. 2021. Miller's analogy test preparation.
Max Pellert, Clemens M. Lechner, Claudia Wagner, Beatrice Rammstedt, and Markus Strohmaier. 2024. Ai psychometrics: Assessing the psychological profiles of large language models through psychometric inventories. Perspectives on Psychological Science, 0(0):17456916231214460. PMID: 38165766.</p>
<p>Steven Piantadosi. 2023. Modern language models refute chomsky's approach to language. Lingbuzz Preprint, lingbuzz, 7180.</p>
<p>Eva Portelance, Yuguang Duan, Michael C. Frank, and Gary Lupyan. 2023. Predicting age of acquisition for children's early vocabulary in five languages using language model surprisal. Cognitive science, 47 9:e13334.</p>
<p>Jean Raven. 2003. Raven progressive matrices. In Handbook of nonverbal assessment, pages 223-237. Springer.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. Sentence transformers: Multilingual sentence embeddings using bert / roberta / xlm-roberta \&amp; co. with pytorch. Accessed: 2023-08-14.</p>
<p>Eleanor Rosch. 1975. Cognitive representations of semantic categories. Journal of Experimental Psychology: General, 104(3):192.</p>
<p>Jenny R Saffran, Richard N Aslin, and Elissa L Newport. 1996. Statistical learning by 8-month-old infants. Science, 274(5294):1926-1928.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106.</p>
<p>Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, and Zeynep Akata. 2024. In-context impersonation reveals large language models' strengths and biases. Advances in Neural Information Processing Systems, 36.</p>
<p>Raj Sanjay Shah, Vijay Marupudi, Reba Koenen, Khushi Bhardwaj, and Sashank Varma. 2023. Numeric magnitude comparison effects in large language models. In The 61st Annual Meeting Of The Association For Computational Linguistics.</p>
<p>Mihir Sharma, Ryan Ding, Raj Sanjay Shah, and Sashank Varma. 2024. Monolingual and bilingual language acquisition in language models.</p>
<p>Noam Siegelman. 2020. Statistical learning abilities and their relation to language. Language and Linguistics Compass, 14(3):e12365.</p>
<p>Richard E Snow, Patrick C Kyllonen, Brachia Marshalek, et al. 1984. The topography of ability and learning correlations. Advances in the psychology of human intelligence, 2(S 47):103.</p>
<p>Robert J Sternberg. 2000. Handbook of intelligence. Cambridge University Press.</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Martin Chadwick, Gaurav Singh Tomar, Xavier Garcia, Evan Senter, Emanuel Taropa, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego</p>
<p>de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adrià Puigdomènech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Yujing Zhang, Ravi Addanki, Antoine Miech, Annie Louis, Laurent El Shafey, Denis Teplyashin, Geoff Brown, Elliot Catt, Nithya Attaluri, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, Sébastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozińska, Vitaly Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Giménez, Legg Yeung, Hanzhao Lin, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo yiin Chang, Paul Komarek, Ross McIlroy, Mario Lučić, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yong Cheng, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay</p>
<p>Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Raphaël Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sjösund, Sébastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, Léonard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adrià Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, Víctor Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, Çağlar Ünlü, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James CobonKerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi, Richard Ives, Yana Hasson, YaGuang Li, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Rakićević, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Gamaleldin Elsayed, Ed Chi, Mahdis Mahdieh, Ian Tenney, Nan Hua, Ivan Petrychenko, Patrick Kane, Dylan Scandinaro, Rishub Jain, Jonathan Uesato, Romina Datta, Adam Sadovsky, Oskar Bunyan, Dominik Rabiej, Shimu Wu, John Zhang, Gautam Vasudevan, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Betty Chan, Pam G Rabinovitch, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Sahitya Potluri, Jane Park, Elnaz Davoodi, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong</p>
<p>Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Chris Gorgolewski, Peter Grabowski, Yu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Paul Suganthan, Evan Palmer, Geoffrey Irving, Edward Loper, Manaal Faruqui, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Michael Fink, Alfonso Castaño, Irene Giannoumis, Wooyeol Kim, Mikołaj Rybiński, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marin Georgiev, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Minnie Lui, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Lam Nguyen Thiet, Daniel Andor, Pedro Valenzuela, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Sarmishta Velury, Sebastian Krause, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Tejasi Latkar, Mingyang Zhang, Quoc Le, Elena Allica Abellan, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Sid Lall, Ken Franko, Egor Filonov, Anna Bulanova, Rémi Leblond, Vikas Yadav, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Hao Zhou, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Jeremiah Liu, Mark Omernick, Colton Bishop, Chintu Kumar, Rachel Sterneck, Ryan Foley, Rohan Jain, Swaroop Mishra, Jiawei Xia, Taylor Bos, Geoffrey Cideron, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Petru Gurita, Hila Noga, Premal Shah, Daniel J. Mankowitz, Alex Polozov, Nate Kushman, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Anhad Mohananey, Matthieu Geist, Sidharth Mudgal, Sertan Girgin, Hui Li, Jiayu Ye, Ollr Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Quan Yuan, Sumit Bagri, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Aliaksei Severyn, Jonathan Lai, Kathy Wu, HengTze Cheng, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Mark Geller, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Andrei Sozanschi, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant</p>
<p>Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Abhimanyu Goyal, Diane Wu, Denese OwusuAfriyie, Cosmo Du, Chloe Thornton, Jordi PontTuset, Pradyumna Narayana, Jing Li, Sabaer Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Tao Zhu, Yeongil Ko, Laura Knight, Amélie Héliou, Ning Niu, Shane Gu, Chenxi Pang, Dustin Tran, Yuqing Li, Nir Levine, Ariel Stolovich, Norbert Kalb, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Balaji Lakshminarayanan, Charlie Deck, Shyam Upadhyay, Hyo Lee, Mike Dusenberry, Zonglin Li, Xuezhi Wang, Kyle Levin, Raphael Hoffmann, Dan HoltmannRice, Olivier Bachem, Summer Yue, Sho Arora, Eric Malmi, Daniil Mirylenka, Qijun Tan, Christy Koh, Soheil Hassas Yeganeh, Siim Pöder, Steven Zheng, Francesco Pongetti, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Ragha Kotikalapudi, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Chenkai Kuang, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Pei Sun, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Ishita Dasgupta, Guillaume Desjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivière, Alanna Walton, Clément Crepy, Alicia Parrish, Yuan Liu, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Plucińska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Ivo Penchev, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odsorn, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Adam Kurzrok, Lynette Webb, Sahil Dua, Dong Li, Preethi Lahoti, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyaru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Taylan Bilal, Evgenii Eltyshev, Daniel Balle, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Adams Yu, Christof Angermueller, Xiaowei Li, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony</p>
<p>Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Kevin Brooks, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Komal Jalan, Dinghua Li, Ginger Perng, Blake Hechtman, Parker Schuh, Milad Nasr, Mia Chen, Kieran Milan, Vladimir Mikulik, Trevor Strohman, Juliana Franco, Tim Green, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. 2023. Gemini: A family of highly capable multimodal models.</p>
<p>Louis Leon Thurstone. 1938. Primary mental abilities. Psychometric monographs.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.</p>
<p>Peter D Turney. 2005. Measuring semantic similarity by latent relational analysis. arXiv preprint cs/0508053.</p>
<p>Peter D Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37:141-188.
J. P. Van Overschelde, K. A. Rawson, and J. Dunlosky. 2004. Category norms: An updated and expanded version of the norms. Journal of Memory and Language, 50:289-335.</p>
<p>Siddhartha Vemuri, Raj Sanjay Shah, and Sashank Varma. 2024. How well do deep learning models capture human concepts? the case of the typicality effect. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 46.</p>
<p>Alex Warstadt and Samuel R. Bowman. 2024. What artificial neural networks can tell us about human language acquisition.</p>
<p>Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, and Ryan Cotterell, editors. 2023. Proceedings of the BalryLM Challenge at the 27th Conference on Computational Natural Language Learning. Association for Computational Linguistics, Singapore.</p>
<p>Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R Bowman. 2020. Blimp: The benchmark of linguistic minimal pairs for english. Transactions of the Association for Computational Linguistics, 8:377-392.</p>
<p>Taylor Webb, Keith J Holyoak, and Hongjing Lu. 2023. Emergent analogical reasoning in large language models. Nature Human Behaviour, 7(9):1526-1541.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals,</p>
<p>Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhun Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui, Qi Zhang, and Xuanjing Huang. 2023. A comprehensive capability analysis of GPT-3 and GPT-3.5 series models. arXiv preprint arXiv:2303.10420.</p>
<p>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. 2023. How well do large language models perform in arithmetic tasks?</p>
<p>Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth. 2020. Do language embeddings capture scales? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4889-4896, Online. Association for Computational Linguistics.</p>
<p>Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. 2023. Starling-7b: Improving llm helpfulness and harmlessness with rlaif.</p>
<p>Yan Zhuang, Qi Liu, Yuting Ning, Weizhe Huang, Rui Lv, Zhenya Huang, Guanhao Zhao, Zheng Zhang, Qingyang Mao, Shijin Wang, et al. 2023. Efficiently measuring the cognitive ability of llms: An adaptive testing perspective. arXiv preprint arXiv:2306.10512.</p>
<h2>A Computational Resources</h2>
<p>The models are evaluated on Nvidia A100 GPUs with 80 GB RAM. The evaluation in this paper cumulatively takes 1600 GPU hours. We use the provided APIs by OpenAI and Google for models of the GPT-X family and Gemini respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Distance Effect</th>
<th style="text-align: center;">Ratio Effect</th>
<th style="text-align: center;">Size Effect</th>
<th style="text-align: center;">MDS Stress</th>
<th style="text-align: center;">MDS Correlation</th>
<th style="text-align: center;">Range (Sim)</th>
<th style="text-align: center;">Max (Sim)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Amber-7B</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">0.572</td>
<td style="text-align: center;">0.008</td>
<td style="text-align: center;">0.995</td>
</tr>
<tr>
<td style="text-align: left;">Falcon-7B</td>
<td style="text-align: center;">0.928</td>
<td style="text-align: center;">0.838</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.655</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.779</td>
</tr>
<tr>
<td style="text-align: left;">Starling-LM-7B-alpha</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.995</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-7B</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">0.016</td>
<td style="text-align: center;">0.983</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-13B</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.234</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3-8B</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.996</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3-8B-Instruct</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.995</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B</td>
<td style="text-align: center;">0.641</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.996</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B-Instruct</td>
<td style="text-align: center;">0.637</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.182</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.992</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-0.5B</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.553</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.679</td>
<td style="text-align: center;">0.064</td>
<td style="text-align: center;">0.911</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-1.8B</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.107</td>
<td style="text-align: center;">0.902</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-4B</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">0.763</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-7B</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.616</td>
<td style="text-align: center;">0.257</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.734</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-14B</td>
<td style="text-align: center;">0.783</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.138</td>
<td style="text-align: center;">0.710</td>
</tr>
<tr>
<td style="text-align: left;">Pythia-70M</td>
<td style="text-align: center;">0.829</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.060</td>
<td style="text-align: center;">0.949</td>
</tr>
<tr>
<td style="text-align: left;">Pythia-160M</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.665</td>
<td style="text-align: center;">0.382</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.715</td>
<td style="text-align: center;">0.042</td>
<td style="text-align: center;">0.970</td>
</tr>
<tr>
<td style="text-align: left;">Pythia-410M</td>
<td style="text-align: center;">0.926</td>
<td style="text-align: center;">0.679</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">0.041</td>
<td style="text-align: center;">0.972</td>
</tr>
<tr>
<td style="text-align: left;">Pythia-1B</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">0.470</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">0.037</td>
<td style="text-align: center;">0.973</td>
</tr>
<tr>
<td style="text-align: left;">Pythia-1.4B</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.764</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.658</td>
<td style="text-align: center;">0.022</td>
<td style="text-align: center;">0.983</td>
</tr>
<tr>
<td style="text-align: left;">Pythia-2.8B</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.737</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;">0.993</td>
</tr>
<tr>
<td style="text-align: left;">Pythia-6.9B</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.713</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.195</td>
<td style="text-align: center;">0.663</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">0.990</td>
</tr>
<tr>
<td style="text-align: left;">Pythia-12B</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.993</td>
</tr>
</tbody>
</table>
<p>Table 4: Magnitude Comparison effects. Distance Effect: Averaged $R^{2}$ values of different LLMs when fitting a linear function on the cosine-similarity vs distance plot. Size Effect: Averaged $R^{2}$ values of different LLMs when fitting a linear function on the cosine-similarity vs size-difference plot. Ratio Effect: Averaged $R^{2}$ values of different LLMs when fitting a negative exponential function on the cosine-similarity vs ratio plot. Note: Each value is averaged across all three input types and all model layers to produce one generalizable score. MDS Stress: The stress value is a measure of how well the distances between the points in the multidimensional space represent the dissimilarities of the original data points (lower is better). MDS Correlation: Correlation between the MDS solutions and the expected values of human MNL. Range (Sim): This indicates the range of the cosine-similarities. Max (sim): This indicates the maximum similarity between any two numbers. Range and Max (sim) describe the $y$-axis.</p>
<h2>B Extended set of experiments</h2>
<h2>B. 1 Numeric abilities: Magnitude comparison effects</h2>
<p>Physical quantities in the world are encoded as logarithmically scaled magnitude representations (Fechner, 1860). While the distance and the ratio effects are the biggest indicators of the presence of such log-scaled magnitude representations and the numerical precision in humans, other human effects also explain the mental number line. These effects are as follows:</p>
<ul>
<li>Distance effect (refer to figure 1 (A) top): The greater the distance $\mid x-y \mid$ between two numbers $(\mathrm{x}, \mathrm{y})$, the faster the comparison in humans (Moyer and Landauer, 1967).</li>
<li>Size effect: Given two comparisons of the same distance (i.e., of the same value for lx - yl), the smaller the numbers, the faster the comparison (Parkman, 1971).</li>
<li>Ratio effect (refer to figure 1 (A) bottom): The time taken by humans to compare two
numbers $(\mathrm{x}, \mathrm{y})$ is a decreasing function of the ratio of the larger number over the smaller number $\frac{\operatorname{maz}(x, y)}{\operatorname{min}(x, y)}$ (Halberda et al., 2008).</li>
<li>Multidimensional scaling: Along with the three effects, we investigate the consistency of the latent number representations of PLMs with the human MNL using multidimensional scaling (Borg and Groenen, 2005; Ding, 2018). MDS recovers the latent representation from the cosine (dis)similarities between the vector representations of all pairs of numbers (for a given LLM, layer, and number format). This is evaluated by the correlation between the positions of the numbers 1 to 9 in the MDS solution and the expected values $(\log (1)$ to $\log$ (9)) of the human MNL (refer to the correlation value in table 4).</li>
</ul>
<p>Beyond these effects, we investigate the development of the latent understanding of the concept of "numbers" in the PLMs. As PLMs see more data, the average values of the similarity become larger, indicating that models learn the distinctions among</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Development of the idea of "numbers" in Pythia. The y-axis indicates the maximum cosine similarity between the latent representations of any two number words/ digits.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Development of the idea of "numbers" in Pythia. The y-axis shows the cosine similarity between word types. The cosine similarity values are averaged over all input types, all model layers, and all model sizes.
numbers better (refer to figure 5). This is further substantiated by figure 6, where the similarities between number words develop to be greater than the similarity between (number, non-number) words and (non-number, non-number) words.</p>
<h2>B. 2 Linguistic Abilities</h2>
<p>The 12 phenomena tested by BLiMP are as follows:</p>
<ul>
<li>Anaphor agreement (morphology): This linguistic phenomenon tests if an anaphor (pronoun) adheres to the antecedent (noun or phrase it refers to) in terms of gender, number, or person.</li>
<li>Argument Structure (syntax): The argument structure tests the relationship between a verb and its arguments (such as nouns or noun phrases).</li>
<li>Binding (syntax, semantics): This tests the structural relationship between an anaphor (pronoun) and its antecedent (noun or phrase it refers to).</li>
<li>Control/ Raising (syntax, semantics): These structures test how semantics differ by syntactical variations of subjects/verbs in subordinate and main clauses.</li>
<li>Determiner-noun agreement (morphology): This tests the agreements of the determiners with the corresponding nouns in number (singular or plural) and sometimes gender (e.g., "his" for masculine nouns, "her" for feminine nouns).</li>
<li>Ellipsis (syntax): This refers to the omission of words from a sentence that can be understood from the context.</li>
<li>Filler-gap (syntax): This tests the syntactic structure of sentences that include phrasal movements (wh-questions, relative clauses).</li>
<li>Irregular forms (morphology): Forms in language that do not follow regular patterns and may need to be memorized. For example, the superlative of good is better, best, and not gooder, goodest.</li>
<li>Island effects (syntax): These test the constraints on syntactic environments where the gap in a filler-gap dependency can occur.</li>
<li>NPI licensing (semantics): This phenomenon tests the constrained situations where negative polarity items like any and ever are limited to the scope of negation.</li>
<li>Quantifiers (semantics): This phenomenon tests the constraints regarding the placement of quantifiers. Specifically, BLiMP looks at superlative quantifiers (such as "at least") that cannot occur within negation, and definite quantifiers and determiners cannot function as subjects in existential "there" constructions.</li>
<li>Subject-verb agreement (morphology): The subject and tense forms of the verb must agree on the number, for example, singular vs plural.</li>
</ul>
<p>Table 5 shows that the PLMs are more accurate in morphology than in language syntax and semantics. Most models also perform better on syntactic language features than semantic language features.</p>
<h2>B. 3 Conceptual Understanding</h2>
<p>Table 7 shows the human alignment of PLMs on their concept understanding for different operationalization methods. We see that Gemini, GPT-3.5-Turbo, and GPT-4 perform better than other models. Furthermore, Surprisal and Promptingbased methods are stronger techniques for evaluating conceptual understanding of models than</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">BLiMP</th>
<th style="text-align: center;">Syntax</th>
<th style="text-align: center;">Semantic</th>
<th style="text-align: center;">Morphology</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Amber-7B</td>
<td style="text-align: center;">$0.794( \pm 0.174)$</td>
<td style="text-align: center;">$0.779( \pm 0.011)$</td>
<td style="text-align: center;">$0.736( \pm 0.011)$</td>
<td style="text-align: center;">$0.888( \pm 0.009)$</td>
</tr>
<tr>
<td style="text-align: center;">Falcon-7B</td>
<td style="text-align: center;">$0.817( \pm 0.173)$</td>
<td style="text-align: center;">$0.797( \pm 0.011)$</td>
<td style="text-align: center;">$0.758( \pm 0.011)$</td>
<td style="text-align: center;">$0.917( \pm 0.008)$</td>
</tr>
<tr>
<td style="text-align: center;">Starling-LM-7B-alpha</td>
<td style="text-align: center;">$0.827( \pm 0.161)$</td>
<td style="text-align: center;">$0.799( \pm 0.011)$</td>
<td style="text-align: center;">$0.788( \pm 0.011)$</td>
<td style="text-align: center;">$0.938( \pm 0.007)$</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-7B</td>
<td style="text-align: center;">$0.818( \pm 0.165)$</td>
<td style="text-align: center;">$0.792( \pm 0.011)$</td>
<td style="text-align: center;">$0.782( \pm 0.011)$</td>
<td style="text-align: center;">$0.917( \pm 0.008)$</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-13B</td>
<td style="text-align: center;">$0.793( \pm 0.184)$</td>
<td style="text-align: center;">$0.757( \pm 0.011)$</td>
<td style="text-align: center;">$0.767( \pm 0.011)$</td>
<td style="text-align: center;">$0.898( \pm 0.008)$</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3-8B</td>
<td style="text-align: center;">$0.735( \pm 0.210)$</td>
<td style="text-align: center;">$0.708( \pm 0.011)$</td>
<td style="text-align: center;">$0.651( \pm 0.012)$</td>
<td style="text-align: center;">$0.871( \pm 0.010)$</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3-8B-Instruct</td>
<td style="text-align: center;">$0.788( \pm 0.181)$</td>
<td style="text-align: center;">$0.765( \pm 0.011)$</td>
<td style="text-align: center;">$0.726( \pm 0.012)$</td>
<td style="text-align: center;">$0.898( \pm 0.008)$</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">$0.829( \pm 0.174)$</td>
<td style="text-align: center;">$0.801( \pm 0.011)$</td>
<td style="text-align: center;">$0.780( \pm 0.010)$</td>
<td style="text-align: center;">$0.940( \pm 0.007)$</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B-Instruct</td>
<td style="text-align: center;">$0.834( \pm 0.149)$</td>
<td style="text-align: center;">$0.808( \pm 0.011)$</td>
<td style="text-align: center;">$0.788( \pm 0.011)$</td>
<td style="text-align: center;">$0.931( \pm 0.008)$</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-0.5B</td>
<td style="text-align: center;">$0.785( \pm 0.176)$</td>
<td style="text-align: center;">$0.759( \pm 0.012)$</td>
<td style="text-align: center;">$0.718( \pm 0.012)$</td>
<td style="text-align: center;">$0.907( \pm 0.008)$</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-1.8B</td>
<td style="text-align: center;">$0.792( \pm 0.162)$</td>
<td style="text-align: center;">$0.777( \pm 0.012)$</td>
<td style="text-align: center;">$0.764( \pm 0.011)$</td>
<td style="text-align: center;">$0.875( \pm 0.010)$</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-4B</td>
<td style="text-align: center;">$0.730( \pm 0.154)$</td>
<td style="text-align: center;">$0.694( \pm 0.013)$</td>
<td style="text-align: center;">$0.728( \pm 0.013)$</td>
<td style="text-align: center;">$0.814( \pm 0.012)$</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-7B</td>
<td style="text-align: center;">$0.789( \pm 0.156)$</td>
<td style="text-align: center;">$0.769( \pm 0.012)$</td>
<td style="text-align: center;">$0.736( \pm 0.012)$</td>
<td style="text-align: center;">$0.885( \pm 0.010)$</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-14B</td>
<td style="text-align: center;">$0.792( \pm 0.144)$</td>
<td style="text-align: center;">$0.775( \pm 0.012)$</td>
<td style="text-align: center;">$0.747( \pm 0.012)$</td>
<td style="text-align: center;">$0.881( \pm 0.010)$</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-70M</td>
<td style="text-align: center;">$0.723( \pm 0.210)$</td>
<td style="text-align: center;">$0.701( \pm 0.012)$</td>
<td style="text-align: center;">$0.628( \pm 0.012)$</td>
<td style="text-align: center;">$0.872( \pm 0.010)$</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-160M</td>
<td style="text-align: center;">$0.749( \pm 0.207)$</td>
<td style="text-align: center;">$0.717( \pm 0.012)$</td>
<td style="text-align: center;">$0.718( \pm 0.011)$</td>
<td style="text-align: center;">$0.864( \pm 0.010)$</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-410M</td>
<td style="text-align: center;">$0.815( \pm 0.169)$</td>
<td style="text-align: center;">$0.785( \pm 0.011)$</td>
<td style="text-align: center;">$0.752( \pm 0.011)$</td>
<td style="text-align: center;">$0.935( \pm 0.007)$</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-1B</td>
<td style="text-align: center;">$0.806( \pm 0.198)$</td>
<td style="text-align: center;">$0.782( \pm 0.011)$</td>
<td style="text-align: center;">$0.728( \pm 0.011)$</td>
<td style="text-align: center;">$0.935( \pm 0.007)$</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-1.4B</td>
<td style="text-align: center;">$0.819( \pm 0.173)$</td>
<td style="text-align: center;">$0.792( \pm 0.011)$</td>
<td style="text-align: center;">$0.768( \pm 0.011)$</td>
<td style="text-align: center;">$0.931( \pm 0.008)$</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-2.8B</td>
<td style="text-align: center;">$0.827( \pm 0.156)$</td>
<td style="text-align: center;">$0.800( \pm 0.011)$</td>
<td style="text-align: center;">$0.782( \pm 0.011)$</td>
<td style="text-align: center;">$0.925( \pm 0.007)$</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-6.9B</td>
<td style="text-align: center;">$0.809( \pm 0.179)$</td>
<td style="text-align: center;">$0.792( \pm 0.011)$</td>
<td style="text-align: center;">$0.750( \pm 0.011)$</td>
<td style="text-align: center;">$0.913( \pm 0.008)$</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-12B</td>
<td style="text-align: center;">$0.829( \pm 0.158)$</td>
<td style="text-align: center;">$0.804( \pm 0.011)$</td>
<td style="text-align: center;">$0.778( \pm 0.011)$</td>
<td style="text-align: center;">$0.932( \pm 0.007)$</td>
</tr>
<tr>
<td style="text-align: center;">Gemini</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">$0.825( \pm 0.166)$</td>
<td style="text-align: center;">$0.818( \pm 0.010)$</td>
<td style="text-align: center;">$0.781( \pm 0.011)$</td>
<td style="text-align: center;">$0.931( \pm 0.007)$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">$0.849( \pm 0.120)$</td>
<td style="text-align: center;">$0.797( \pm 0.010)$</td>
<td style="text-align: center;">$0.801( \pm 0.009)$</td>
<td style="text-align: center;">$0.941( \pm 0.007)$</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracy of different language models on the BLiMP linguistic acceptability tasks.
representation-based methods. Given the higher performance of Prompting methods on three APIbased models, we only show the category-wise results for those models. The final prompt design is given in section B.3.1 and table 11. Tables 8, 9, and 10 show Spearman's correlation on the categories along with the standard deviation, the minimum correlation, and the maximum correlation. We perform the same infilling tasks 50 times for each category to account for variations in generations. We note that the models often failed to return all the options in the in-filling task. We discard such situations in our analysis.</p>
<p>Note: Under the closeness judgment protocol, our experiments fail to match up to the performance of the models used by Vemuri et al. (2024). This is because our choice of open-source models only provides token representations, on which we later perform an aggregation operation. This aggregation operation leads to a loss of information. In contrast, Vemuri et al. (2024) use sentence-transformer models (Reimers and Gurevych, 2019), which provide
singular latent representation for longer text. This variation in experimentation leads to the difference in alignment scores.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Categories</th>
<th style="text-align: center;">GPT 3.5</th>
<th style="text-align: center;">GPT 4</th>
<th style="text-align: center;">Gemini</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">bird</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.353</td>
</tr>
<tr>
<td style="text-align: left;">carpenters tool</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.679</td>
<td style="text-align: center;">0.610</td>
</tr>
<tr>
<td style="text-align: left;">clothing</td>
<td style="text-align: center;">0.022</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.155</td>
</tr>
<tr>
<td style="text-align: left;">color</td>
<td style="text-align: center;">-0.016</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.569</td>
</tr>
<tr>
<td style="text-align: left;">dwelling</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.340</td>
</tr>
<tr>
<td style="text-align: left;">earth formation</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.496</td>
<td style="text-align: center;">0.155</td>
</tr>
<tr>
<td style="text-align: left;">fabric</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.504</td>
</tr>
<tr>
<td style="text-align: left;">fish</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: center;">0.247</td>
</tr>
<tr>
<td style="text-align: left;">flower</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.772</td>
<td style="text-align: center;">0.515</td>
</tr>
<tr>
<td style="text-align: left;">flying thing</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.184</td>
</tr>
<tr>
<td style="text-align: left;">footwear</td>
<td style="text-align: center;">0.118</td>
<td style="text-align: center;">0.521</td>
<td style="text-align: center;">0.218</td>
</tr>
<tr>
<td style="text-align: left;">four-legged animal</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.537</td>
</tr>
<tr>
<td style="text-align: left;">fruit</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.508</td>
</tr>
<tr>
<td style="text-align: left;">furniture</td>
<td style="text-align: center;">0.069</td>
<td style="text-align: center;">0.525</td>
<td style="text-align: center;">0.147</td>
</tr>
<tr>
<td style="text-align: left;">gardeners tool</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.507</td>
</tr>
<tr>
<td style="text-align: left;">green thing</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.572</td>
<td style="text-align: center;">0.335</td>
</tr>
<tr>
<td style="text-align: left;">insect</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.629</td>
<td style="text-align: center;">0.286</td>
</tr>
<tr>
<td style="text-align: left;">instrument</td>
<td style="text-align: center;">0.194</td>
<td style="text-align: center;">0.709</td>
<td style="text-align: center;">0.450</td>
</tr>
<tr>
<td style="text-align: left;">kitchen utensil</td>
<td style="text-align: center;">0.384</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.252</td>
</tr>
<tr>
<td style="text-align: left;">ship</td>
<td style="text-align: center;">0.104</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">-0.078</td>
</tr>
<tr>
<td style="text-align: left;">snake</td>
<td style="text-align: center;">0.177</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.328</td>
</tr>
<tr>
<td style="text-align: left;">toy</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.169</td>
</tr>
<tr>
<td style="text-align: left;">tree</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.445</td>
</tr>
<tr>
<td style="text-align: left;">vegetable</td>
<td style="text-align: center;">0.096</td>
<td style="text-align: center;">0.783</td>
<td style="text-align: center;">0.121</td>
</tr>
<tr>
<td style="text-align: left;">vehicle</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.033</td>
</tr>
<tr>
<td style="text-align: left;">weapon</td>
<td style="text-align: center;">0.348</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.239</td>
</tr>
<tr>
<td style="text-align: left;">weather</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.274</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.311</td>
</tr>
</tbody>
</table>
<p>Table 6: Typicality effects: Comparing Average Spearman's correlation score across categories from tables 8, 9 , and 10 .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Latent <br> Representations</th>
<th style="text-align: center;">Surprisal Values</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Prompting</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">One-shot</td>
<td style="text-align: center;">Two-shot</td>
<td style="text-align: center;">Three-shot</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Amber-7B</td>
<td style="text-align: center;">0.083</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Falcon-7B</td>
<td style="text-align: center;">$-0.116$</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Starling-LM-7B-alpha</td>
<td style="text-align: center;">$-0.003$</td>
<td style="text-align: center;">0.258</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-7B</td>
<td style="text-align: center;">$-0.065$</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.213</td>
<td style="text-align: center;">0.202</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-13B</td>
<td style="text-align: center;">0.076</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3-8B</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3-8B-Instruct</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.128</td>
<td style="text-align: center;">0.213</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">$-0.025$</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.257</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B-Instruct</td>
<td style="text-align: center;">0.033</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-0.5B</td>
<td style="text-align: center;">0.072</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-1.8B</td>
<td style="text-align: center;">0.114</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-4B</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.217</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-7B</td>
<td style="text-align: center;">0.006</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-14B</td>
<td style="text-align: center;">$-0.140$</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-70M</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.285</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-160M</td>
<td style="text-align: center;">0.067</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-410M</td>
<td style="text-align: center;">0.126</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-1B</td>
<td style="text-align: center;">0.090</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-1.4B</td>
<td style="text-align: center;">0.074</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-2.8B</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-6.9B</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Pythia-12B</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Gemini</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">0.311</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">0.242</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.559</td>
</tr>
</tbody>
</table>
<p>Table 7: Results for the typicality effects using the three methods</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Categories</th>
<th style="text-align: center;">Average SpearmanR</th>
<th style="text-align: center;">Minimum Values</th>
<th style="text-align: center;">Maximum Values</th>
<th style="text-align: center;">Std Dev</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">bird</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">-0.156</td>
<td style="text-align: center;">0.582</td>
<td style="text-align: center;">0.144</td>
</tr>
<tr>
<td style="text-align: left;">carpenters tool</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.885</td>
<td style="text-align: center;">0.104</td>
</tr>
<tr>
<td style="text-align: left;">clothing</td>
<td style="text-align: center;">0.155</td>
<td style="text-align: center;">-0.104</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.141</td>
</tr>
<tr>
<td style="text-align: left;">color</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">-0.147</td>
<td style="text-align: center;">0.916</td>
<td style="text-align: center;">0.260</td>
</tr>
<tr>
<td style="text-align: left;">dwelling</td>
<td style="text-align: center;">0.340</td>
<td style="text-align: center;">0.140</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.086</td>
</tr>
<tr>
<td style="text-align: left;">earth formation</td>
<td style="text-align: center;">0.155</td>
<td style="text-align: center;">-0.449</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.191</td>
</tr>
<tr>
<td style="text-align: left;">fabric</td>
<td style="text-align: center;">0.504</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.811</td>
<td style="text-align: center;">0.168</td>
</tr>
<tr>
<td style="text-align: left;">fish</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">-0.505</td>
<td style="text-align: center;">0.611</td>
<td style="text-align: center;">0.265</td>
</tr>
<tr>
<td style="text-align: left;">flower</td>
<td style="text-align: center;">0.515</td>
<td style="text-align: center;">-0.183</td>
<td style="text-align: center;">0.779</td>
<td style="text-align: center;">0.208</td>
</tr>
<tr>
<td style="text-align: left;">flying thing</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">-0.068</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.193</td>
</tr>
<tr>
<td style="text-align: left;">footwear</td>
<td style="text-align: center;">0.218</td>
<td style="text-align: center;">-0.340</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.215</td>
</tr>
<tr>
<td style="text-align: left;">four-legged animal</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">0.689</td>
<td style="text-align: center;">0.099</td>
</tr>
<tr>
<td style="text-align: left;">fruit</td>
<td style="text-align: center;">0.508</td>
<td style="text-align: center;">-0.019</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.222</td>
</tr>
<tr>
<td style="text-align: left;">furniture</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">-0.479</td>
<td style="text-align: center;">0.663</td>
<td style="text-align: center;">0.310</td>
</tr>
<tr>
<td style="text-align: left;">gardeners tool</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.025</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.151</td>
</tr>
<tr>
<td style="text-align: left;">green thing</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.037</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.117</td>
</tr>
<tr>
<td style="text-align: left;">insect</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">-0.121</td>
<td style="text-align: center;">0.635</td>
<td style="text-align: center;">0.193</td>
</tr>
<tr>
<td style="text-align: left;">instrument</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.832</td>
<td style="text-align: center;">0.175</td>
</tr>
<tr>
<td style="text-align: left;">kitchen utensil</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">-0.164</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.243</td>
</tr>
<tr>
<td style="text-align: left;">ship</td>
<td style="text-align: center;">-0.078</td>
<td style="text-align: center;">-0.414</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.179</td>
</tr>
<tr>
<td style="text-align: left;">snake</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">-0.156</td>
<td style="text-align: center;">0.596</td>
<td style="text-align: center;">0.147</td>
</tr>
<tr>
<td style="text-align: left;">toy</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">-0.203</td>
<td style="text-align: center;">0.526</td>
<td style="text-align: center;">0.174</td>
</tr>
<tr>
<td style="text-align: left;">tree</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.257</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.073</td>
</tr>
<tr>
<td style="text-align: left;">vegetable</td>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">-0.322</td>
<td style="text-align: center;">0.596</td>
<td style="text-align: center;">0.184</td>
</tr>
<tr>
<td style="text-align: left;">vehicle</td>
<td style="text-align: center;">0.033</td>
<td style="text-align: center;">-0.053</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.055</td>
</tr>
<tr>
<td style="text-align: left;">weapon</td>
<td style="text-align: center;">0.239</td>
<td style="text-align: center;">-0.173</td>
<td style="text-align: center;">0.577</td>
<td style="text-align: center;">0.193</td>
</tr>
<tr>
<td style="text-align: left;">weather</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">-0.029</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.147</td>
</tr>
</tbody>
</table>
<p>Table 8: Average Spearman's correlation score for each category on 50 runs of each in-filling experiment on the Gemini-Pro model.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Developmental trajectory of the Pythia suite of models on the BLiMP linguistic acceptability tasks.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ We add the tasks to a publically available unified language model testing framework, titled lm-evaluation-harness (Gao et al., 2023), to support the evaluation of future models on these psychometric intelligence assessments.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
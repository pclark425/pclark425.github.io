<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6656 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6656</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6656</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-3fb1818b9ab3f8d36ed3f8329087058f561c05da</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3fb1818b9ab3f8d36ed3f8329087058f561c05da" target="_blank">ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work tailor the Self-Critique pipeline, which addresses the challenge in the feedback learning stage of LLM alignment, and significantly enhances the LLM's mathematical problem-solving while still improving its language ability, outperforming LLMs that could be two times larger.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown excellent mastering of human language, but still struggle in real-world applications that require mathematical problem-solving. While many strategies and datasets to enhance LLMs' mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed LLM systems.In this work, we tailor the Self-Critique pipeline, which addresses the challenge in the feedback learning stage of LLM alignment. We first train a general Math-Critique model from the LLM itself to provide feedback signals. Then, we sequentially employ rejective fine-tuning and direct preference optimization over the LLM's own generations for data collection. Based on ChatGLM3-32B, we conduct a series of experiments on both academic and our newly created challenging dataset, MathUserEval. Results show that our pipeline significantly enhances the LLM's mathematical problem-solving while still improving its language ability, outperforming LLMs that could be two times larger. Related techniques have been deployed to ChatGLM\footnote{\url{https://chatglm.cn}}, an online serving LLM. Related evaluation dataset and scripts are released at \url{https://github.com/THUDM/ChatGLM-Math}.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6656.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6656.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM3-32B-Math</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM3-32B-SFT-2312 + Critique-RFT & Critique-DPO (ChatGLM-Math)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's final math-enhanced 32B autoregressive LLM obtained by supervised finetuning followed by Math-Critique guided rejective fine-tuning (RFT) and Direct Preference Optimization (DPO); optimized to improve multi-step mathematical problem solving while retaining general language abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM3-32B-Math (ChatGLM3-32B-SFT-2312 + RFT & DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>autoregressive transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Seed SFT data drawn from public instruction datasets plus math-specific samples from GSM8k and MATH training splits, public middle/high-school and university exam questions, and a large set of model-sampled Q/A pairs filtered and relabeled by the Math-Critique model (majority Chinese; >90% Chinese data reported).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MathUsEREval (primary), GSM8k, MATH, Ape210k, CMath, Hungarian National Exam</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems, algebra, geometry, trigonometry, probability, linear algebra, calculus, general open-ended arithmetic and applied math problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>open-ended natural-language questions (answers: scalar numbers, expressions, multi-step derivations); greedy decoding single-shot evaluation (zero-shot/few-shot allowed)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>range from elementary (calculate) to advanced (contest/university); most gains observed on medium-difficulty problems (avg score 4-7 in their difficulty proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot or few-shot prompting with greedy decoding; model-generated reasoning steps are used during finetuning (no external tool use); training uses Math-Critique-based rejection sampling and DPO pairwise preference training rather than special CoT prompts at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>dataset-specific accuracy / averaged score (two evaluation modes: avg-score from Math-Critique or GPT-4-Preview, and hard-split pass rate based on threshold)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MATHUSEREval: 4.23 (Table 3, +RFT,DPO; score scale used by dataset), Ape210k: 89.4 (accuracy), GSM8k: 82.6 (accuracy), MATH: 40.6 (accuracy); Hungarian National Exam: 57 (RFT) -> 73 (after DPO) as human-evaluated score</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper reports empirical analyses (no mechanistic neuron/attention probes): (1) Math-Critique reliably ranks answers (Math-Critique-32B correlates ~0.80 Pearson with human four-class labels); (2) RFT (rejective finetuning) improves correctness broadly across difficulties, with strongest gains on medium-difficulty items (avg difficulty 4–6); (3) DPO provides additional gains primarily on open-ended/medium-hard questions (avg score 5–7); (4) ablations show mixing real-world scenario questions with academic problems is important for generalization; training hyperparameters (lr, batch size, steps) and selection thresholds (correct-bound ~0.7) are documented.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Observed failures include: limited precision in numerical computation (multiplication/division/exponentiation with three or more decimal places can deviate up to ~5%); occasional arithmetic slips and term-transfer mistakes in long algebraic manipulations (examples in case studies: matrix equation term transfer error, small rounding errors that cascade); inability to draw or reason with graphics (poor on image-containing MATH questions: ~23% accuracy vs ~40% overall); formatting/equivalent-expression mismatch handled better by Math-Critique but still a general challenge; step-skipping and occasional incorrect intermediate algebra despite correct form recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Finetuning pipeline (RFT then DPO) on the 32B model produced large gains versus the SFT-only 32B baseline and outperformed many larger models on selected math benchmarks; RFT yields widespread improvements across problem difficulties, DPO yields additional improvements concentrated on medium-hard open-ended problems; inclusion of English/academic data improves English benchmarks without harming Chinese performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6656.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6656.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM3-32B-SFT-2312 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM3-32B-SFT-2312 (Supervised Finetuned baseline used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The supervised-finetuned 32B ChatGLM3 checkpoint used as the starting point for Critique-RFT and Critique-DPO experiments; serves as ablation baseline to quantify gains from the Self-Critique pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM3-32B-SFT-2312</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>autoregressive transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Standard SFT instruction data (mixed tasks) and publicly available datasets used as initial finetune; used as base for subsequent Math-Critique-driven sampling and finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MathUsEREval, GSM8k, MATH, Ape210k (for comparisons/ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>general QA including multi-step math; baseline arithmetic and math problem-solving</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>open-ended natural-language questions; evaluated single greedy decode</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>elementary to advanced (same test suites as final model)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot / few-shot evaluations; greedy decoding single pass</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>dataset-specific accuracy / averaged score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MATHUSEREval: 3.25 (Table 3 baseline), Ape210k: 78.0, GSM8k: 75.8, MATH: 29.0 (Table 3 baseline numbers for SFT model)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Used as base for iterative pipeline; no internal mechanistic probing reported for this baseline beyond performance deltas after RFT/DPO.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Typical baseline issues: lower correct-answer rates on medium and advanced problems relative to RFT/DPO variants; same qualitative failure modes as enhanced model (calculation slips, image/graphic problems).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Serves as starting point showing that targeted critique-guided finetuning yields substantial improvements over SFT alone at the same parameter scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6656.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6656.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Math-Critique-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Math-Critique (32B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 32B evaluator model trained from the base ChatGLM3 checkpoint to score mathematical model outputs (1–10 scale) and produce explanatory critique; used to filter and label model-generated math answers for RFT and DPO data construction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Math-Critique-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>autoregressive transformer (decoder-only) used as an evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained from ChatGLM3-32B base using ~10k pseudo-annotated entries (from CritiqueLLM & ORM extremes) plus ~5k manually annotated middle-range examples and 800 test examples sourced from exam questions and model generations; training LR 3e-6, batch size 128 reported.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>internal four-category scoring benchmark (derived from exam questions and MATHUSEREval), used to score GSM8k/MATH/MATHUSEREval generations</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>automatic scoring / verification of math answers; classification into four categories and mapping to 1–10 score</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>inputs: (Question, Reference, Model Answer) -> outputs: (Critique text, Score 1–10)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>covers elementary to advanced questions; evaluator intended to be robust across difficulty levels</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction-following scoring prompt that requires (i) categorization into 4 correctness classes, (ii) explanatory analysis and (iii) numeric score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>classification accuracy (correct/incorrect) and correlation to human annotations (Pearson, Spearman, Kendall)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Accuracy (correct/incorrect): 90.5%; Pearson correlation vs human four-class labels: 80.4%; Spearman: 77.1%; Kendall: 70.2% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Math-Critique maps answer+reference to interpretable critique and numeric score; used both as average-score rater and as thresholded 'hard-split' judge for pass/fail selection; authors validated it approximates human judgments closely (on-par with GPT-4-0613).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Noted limitations: dependence on the provided reference answer (scoring integrates reference), potential distribution shift if base model changes rapidly (mitigated by retraining), and inability to fully replace small-sample human judgment for ambiguous formatting differences unless manual annotation is included.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Authors iteratively update Math-Critique from successive base models (SFT -> RFT-refined model), suggesting performance benefits from using a stronger base model as critic for subsequent iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6656.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6656.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (1106-Preview / Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-1106-Preview (GPT-4-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 Turbo variant used as a high-quality external rater and baseline model, showing top performance on many math benchmarks in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-1106-Preview (GPT-4-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>autoregressive transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper; used as an external evaluator and baseline per OpenAI's public model offering (reference to GPT-4 technical report).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MathUsEREval (as rater), GSM8k, MATH, Ape210k</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic and math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>open-ended natural-language questions; GPT-4 used both as comparator and as judge (GPT-4-1106-Preview used for scoring in some evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>elementary to advanced; top performance across many difficulty ranges</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot/few-shot evaluation; used as an external scoring model (GPT-4-Preview evaluation protocol per AlignBench)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / averaged score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8k: 93.6% (accuracy); MATH: 53.6% (accuracy); on other reported metrics it attains the highest or near-highest scores across many benchmarks (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic analysis presented in this paper for GPT-4; GPT-4 used primarily as a trusted external evaluator and baseline comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not detailed in this paper beyond comparison statistics; cited for reference that even top models exhibit arithmetic/precision issues in some contexts (discussed generally in Limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Included as a high-performing proprietary baseline; shows that model scale/architecture + training leads to strong arithmetic performance, but paper's 32B pipeline approaches or exceeds performance of some larger proprietary models on certain datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6656.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6656.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (0613 release)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier GPT-4 variant used as a baseline for math performance comparisons and for correlation comparisons with Math-Critique.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>autoregressive transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper; used as a baseline comparator and to validate Math-Critique correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MathUsEREval (scoring), GSM8k, MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step mathematical reasoning and arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>open-ended natural-language questions</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>elementary to advanced</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot/few-shot evaluation; greedy decoding single pass</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / correlation with human annotation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8k: 91.4% (accuracy); MATH: 45.8% (accuracy); Math-Critique correlation with human annotations for GPT-4-0613: Pearson 80.5%, Spearman 78.1%, Kendall 71.0%; accuracy detecting correct/incorrect: 90.2% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Used as a strong external judge to validate Math-Critique; no internal mechanistic probes provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not specifically enumerated here; used as comparative standard to show Math-Critique parity with GPT-4-0613 in scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Acts as a high-performance reference point; authors show their 32B math-finetuned model approaches or exceeds GPT-4-0613 on certain in-paper benchmarks (e.g., MATHUSEREval per their scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6656.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6656.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (0613)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-3.5 family model used as a lower-capability baseline for math tasks and for comparison of Math-Critique correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>autoregressive transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (smaller than GPT-4 family)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not detailed in this paper; used as baseline for math benchmarks and for Math-Critique evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8k, MATH, internal scoring test for Math-Critique</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems and arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>open-ended natural-language</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>elementary to medium; weaker on advanced tasks relative to GPT-4 and finetuned models</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot/few-shot evaluation; greedy decoding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy; Math-Critique correlation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8k: 78.2% (accuracy); MATH: 28.0% (accuracy); Math-Critique comparison: accuracy 62.1% and lower correlation to human judgments (Pearson 31.8%, Spearman 33.5%, Kendall 30.1%) (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Used to show Math-Critique significantly outperforms GPT-3.5-Turbo as an automatic rater; no mechanistic introspection provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Lower reliability on multi-step arithmetic and reasoning compared to GPT-4 and finetuned models; more frequent incorrect process/answer combinations leading to low correlation with human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Serves as a contrast showing substantial gains from stronger base models and targeted finetuning pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>How well do large language models perform in arithmetic tasks? <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the MATH dataset <em>(Rating: 2)</em></li>
                <li>Direct preference optimization: Your language model is secretly a reward model <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
                <li>CritiqueLLM: Scaling LLM-as-critic for effective and explainable evaluation of large language model generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6656",
    "paper_id": "paper-3fb1818b9ab3f8d36ed3f8329087058f561c05da",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "ChatGLM3-32B-Math",
            "name_full": "ChatGLM3-32B-SFT-2312 + Critique-RFT & Critique-DPO (ChatGLM-Math)",
            "brief_description": "The paper's final math-enhanced 32B autoregressive LLM obtained by supervised finetuning followed by Math-Critique guided rejective fine-tuning (RFT) and Direct Preference Optimization (DPO); optimized to improve multi-step mathematical problem solving while retaining general language abilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGLM3-32B-Math (ChatGLM3-32B-SFT-2312 + RFT & DPO)",
            "model_family": "autoregressive transformer (decoder-only)",
            "model_size": "32B",
            "training_data_description": "Seed SFT data drawn from public instruction datasets plus math-specific samples from GSM8k and MATH training splits, public middle/high-school and university exam questions, and a large set of model-sampled Q/A pairs filtered and relabeled by the Math-Critique model (majority Chinese; &gt;90% Chinese data reported).",
            "benchmark_name": "MathUsEREval (primary), GSM8k, MATH, Ape210k, CMath, Hungarian National Exam",
            "task_type": "multi-step word problems, algebra, geometry, trigonometry, probability, linear algebra, calculus, general open-ended arithmetic and applied math problems",
            "problem_format": "open-ended natural-language questions (answers: scalar numbers, expressions, multi-step derivations); greedy decoding single-shot evaluation (zero-shot/few-shot allowed)",
            "difficulty_level": "range from elementary (calculate) to advanced (contest/university); most gains observed on medium-difficulty problems (avg score 4-7 in their difficulty proxy)",
            "prompting_method": "zero-shot or few-shot prompting with greedy decoding; model-generated reasoning steps are used during finetuning (no external tool use); training uses Math-Critique-based rejection sampling and DPO pairwise preference training rather than special CoT prompts at inference.",
            "performance_metric": "dataset-specific accuracy / averaged score (two evaluation modes: avg-score from Math-Critique or GPT-4-Preview, and hard-split pass rate based on threshold)",
            "performance_value": "MATHUSEREval: 4.23 (Table 3, +RFT,DPO; score scale used by dataset), Ape210k: 89.4 (accuracy), GSM8k: 82.6 (accuracy), MATH: 40.6 (accuracy); Hungarian National Exam: 57 (RFT) -&gt; 73 (after DPO) as human-evaluated score",
            "internal_analysis": "Paper reports empirical analyses (no mechanistic neuron/attention probes): (1) Math-Critique reliably ranks answers (Math-Critique-32B correlates ~0.80 Pearson with human four-class labels); (2) RFT (rejective finetuning) improves correctness broadly across difficulties, with strongest gains on medium-difficulty items (avg difficulty 4–6); (3) DPO provides additional gains primarily on open-ended/medium-hard questions (avg score 5–7); (4) ablations show mixing real-world scenario questions with academic problems is important for generalization; training hyperparameters (lr, batch size, steps) and selection thresholds (correct-bound ~0.7) are documented.",
            "failure_modes": "Observed failures include: limited precision in numerical computation (multiplication/division/exponentiation with three or more decimal places can deviate up to ~5%); occasional arithmetic slips and term-transfer mistakes in long algebraic manipulations (examples in case studies: matrix equation term transfer error, small rounding errors that cascade); inability to draw or reason with graphics (poor on image-containing MATH questions: ~23% accuracy vs ~40% overall); formatting/equivalent-expression mismatch handled better by Math-Critique but still a general challenge; step-skipping and occasional incorrect intermediate algebra despite correct form recognition.",
            "scaling_trend": "Finetuning pipeline (RFT then DPO) on the 32B model produced large gains versus the SFT-only 32B baseline and outperformed many larger models on selected math benchmarks; RFT yields widespread improvements across problem difficulties, DPO yields additional improvements concentrated on medium-hard open-ended problems; inclusion of English/academic data improves English benchmarks without harming Chinese performance.",
            "uuid": "e6656.0",
            "source_info": {
                "paper_title": "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ChatGLM3-32B-SFT-2312 (baseline)",
            "name_full": "ChatGLM3-32B-SFT-2312 (Supervised Finetuned baseline used in experiments)",
            "brief_description": "The supervised-finetuned 32B ChatGLM3 checkpoint used as the starting point for Critique-RFT and Critique-DPO experiments; serves as ablation baseline to quantify gains from the Self-Critique pipeline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGLM3-32B-SFT-2312",
            "model_family": "autoregressive transformer (decoder-only)",
            "model_size": "32B",
            "training_data_description": "Standard SFT instruction data (mixed tasks) and publicly available datasets used as initial finetune; used as base for subsequent Math-Critique-driven sampling and finetuning.",
            "benchmark_name": "MathUsEREval, GSM8k, MATH, Ape210k (for comparisons/ablation)",
            "task_type": "general QA including multi-step math; baseline arithmetic and math problem-solving",
            "problem_format": "open-ended natural-language questions; evaluated single greedy decode",
            "difficulty_level": "elementary to advanced (same test suites as final model)",
            "prompting_method": "zero-shot / few-shot evaluations; greedy decoding single pass",
            "performance_metric": "dataset-specific accuracy / averaged score",
            "performance_value": "MATHUSEREval: 3.25 (Table 3 baseline), Ape210k: 78.0, GSM8k: 75.8, MATH: 29.0 (Table 3 baseline numbers for SFT model)",
            "internal_analysis": "Used as base for iterative pipeline; no internal mechanistic probing reported for this baseline beyond performance deltas after RFT/DPO.",
            "failure_modes": "Typical baseline issues: lower correct-answer rates on medium and advanced problems relative to RFT/DPO variants; same qualitative failure modes as enhanced model (calculation slips, image/graphic problems).",
            "scaling_trend": "Serves as starting point showing that targeted critique-guided finetuning yields substantial improvements over SFT alone at the same parameter scale.",
            "uuid": "e6656.1",
            "source_info": {
                "paper_title": "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Math-Critique-32B",
            "name_full": "Math-Critique (32B)",
            "brief_description": "A 32B evaluator model trained from the base ChatGLM3 checkpoint to score mathematical model outputs (1–10 scale) and produce explanatory critique; used to filter and label model-generated math answers for RFT and DPO data construction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Math-Critique-32B",
            "model_family": "autoregressive transformer (decoder-only) used as an evaluator",
            "model_size": "32B",
            "training_data_description": "Trained from ChatGLM3-32B base using ~10k pseudo-annotated entries (from CritiqueLLM & ORM extremes) plus ~5k manually annotated middle-range examples and 800 test examples sourced from exam questions and model generations; training LR 3e-6, batch size 128 reported.",
            "benchmark_name": "internal four-category scoring benchmark (derived from exam questions and MATHUSEREval), used to score GSM8k/MATH/MATHUSEREval generations",
            "task_type": "automatic scoring / verification of math answers; classification into four categories and mapping to 1–10 score",
            "problem_format": "inputs: (Question, Reference, Model Answer) -&gt; outputs: (Critique text, Score 1–10)",
            "difficulty_level": "covers elementary to advanced questions; evaluator intended to be robust across difficulty levels",
            "prompting_method": "instruction-following scoring prompt that requires (i) categorization into 4 correctness classes, (ii) explanatory analysis and (iii) numeric score",
            "performance_metric": "classification accuracy (correct/incorrect) and correlation to human annotations (Pearson, Spearman, Kendall)",
            "performance_value": "Accuracy (correct/incorrect): 90.5%; Pearson correlation vs human four-class labels: 80.4%; Spearman: 77.1%; Kendall: 70.2% (Table 5).",
            "internal_analysis": "Math-Critique maps answer+reference to interpretable critique and numeric score; used both as average-score rater and as thresholded 'hard-split' judge for pass/fail selection; authors validated it approximates human judgments closely (on-par with GPT-4-0613).",
            "failure_modes": "Noted limitations: dependence on the provided reference answer (scoring integrates reference), potential distribution shift if base model changes rapidly (mitigated by retraining), and inability to fully replace small-sample human judgment for ambiguous formatting differences unless manual annotation is included.",
            "scaling_trend": "Authors iteratively update Math-Critique from successive base models (SFT -&gt; RFT-refined model), suggesting performance benefits from using a stronger base model as critic for subsequent iterations.",
            "uuid": "e6656.2",
            "source_info": {
                "paper_title": "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4 (1106-Preview / Turbo)",
            "name_full": "GPT-4-1106-Preview (GPT-4-Turbo)",
            "brief_description": "OpenAI's GPT-4 Turbo variant used as a high-quality external rater and baseline model, showing top performance on many math benchmarks in the paper's comparisons.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4-1106-Preview (GPT-4-Turbo)",
            "model_family": "autoregressive transformer (decoder-only)",
            "model_size": "proprietary (not specified in paper)",
            "training_data_description": "Not specified in this paper; used as an external evaluator and baseline per OpenAI's public model offering (reference to GPT-4 technical report).",
            "benchmark_name": "MathUsEREval (as rater), GSM8k, MATH, Ape210k",
            "task_type": "multi-step arithmetic and math word problems",
            "problem_format": "open-ended natural-language questions; GPT-4 used both as comparator and as judge (GPT-4-1106-Preview used for scoring in some evaluations)",
            "difficulty_level": "elementary to advanced; top performance across many difficulty ranges",
            "prompting_method": "zero-shot/few-shot evaluation; used as an external scoring model (GPT-4-Preview evaluation protocol per AlignBench)",
            "performance_metric": "accuracy / averaged score",
            "performance_value": "GSM8k: 93.6% (accuracy); MATH: 53.6% (accuracy); on other reported metrics it attains the highest or near-highest scores across many benchmarks (Table 3).",
            "internal_analysis": "No mechanistic analysis presented in this paper for GPT-4; GPT-4 used primarily as a trusted external evaluator and baseline comparator.",
            "failure_modes": "Not detailed in this paper beyond comparison statistics; cited for reference that even top models exhibit arithmetic/precision issues in some contexts (discussed generally in Limitations).",
            "scaling_trend": "Included as a high-performing proprietary baseline; shows that model scale/architecture + training leads to strong arithmetic performance, but paper's 32B pipeline approaches or exceeds performance of some larger proprietary models on certain datasets.",
            "uuid": "e6656.3",
            "source_info": {
                "paper_title": "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4-0613",
            "name_full": "GPT-4 (0613 release)",
            "brief_description": "An earlier GPT-4 variant used as a baseline for math performance comparisons and for correlation comparisons with Math-Critique.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4-0613",
            "model_family": "autoregressive transformer (decoder-only)",
            "model_size": "proprietary (not specified in paper)",
            "training_data_description": "Not specified in this paper; used as a baseline comparator and to validate Math-Critique correlations.",
            "benchmark_name": "MathUsEREval (scoring), GSM8k, MATH",
            "task_type": "multi-step mathematical reasoning and arithmetic",
            "problem_format": "open-ended natural-language questions",
            "difficulty_level": "elementary to advanced",
            "prompting_method": "zero-shot/few-shot evaluation; greedy decoding single pass",
            "performance_metric": "accuracy / correlation with human annotation",
            "performance_value": "GSM8k: 91.4% (accuracy); MATH: 45.8% (accuracy); Math-Critique correlation with human annotations for GPT-4-0613: Pearson 80.5%, Spearman 78.1%, Kendall 71.0%; accuracy detecting correct/incorrect: 90.2% (Table 5).",
            "internal_analysis": "Used as a strong external judge to validate Math-Critique; no internal mechanistic probes provided in this paper.",
            "failure_modes": "Not specifically enumerated here; used as comparative standard to show Math-Critique parity with GPT-4-0613 in scoring.",
            "scaling_trend": "Acts as a high-performance reference point; authors show their 32B math-finetuned model approaches or exceeds GPT-4-0613 on certain in-paper benchmarks (e.g., MATHUSEREval per their scoring).",
            "uuid": "e6656.4",
            "source_info": {
                "paper_title": "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "GPT-3.5-Turbo (0613)",
            "brief_description": "OpenAI's GPT-3.5 family model used as a lower-capability baseline for math tasks and for comparison of Math-Critique correlations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo-0613",
            "model_family": "autoregressive transformer (decoder-only)",
            "model_size": "proprietary (smaller than GPT-4 family)",
            "training_data_description": "Not detailed in this paper; used as baseline for math benchmarks and for Math-Critique evaluation.",
            "benchmark_name": "GSM8k, MATH, internal scoring test for Math-Critique",
            "task_type": "multi-step word problems and arithmetic",
            "problem_format": "open-ended natural-language",
            "difficulty_level": "elementary to medium; weaker on advanced tasks relative to GPT-4 and finetuned models",
            "prompting_method": "zero-shot/few-shot evaluation; greedy decoding",
            "performance_metric": "accuracy; Math-Critique correlation",
            "performance_value": "GSM8k: 78.2% (accuracy); MATH: 28.0% (accuracy); Math-Critique comparison: accuracy 62.1% and lower correlation to human judgments (Pearson 31.8%, Spearman 33.5%, Kendall 30.1%) (Table 5).",
            "internal_analysis": "Used to show Math-Critique significantly outperforms GPT-3.5-Turbo as an automatic rater; no mechanistic introspection provided.",
            "failure_modes": "Lower reliability on multi-step arithmetic and reasoning compared to GPT-4 and finetuned models; more frequent incorrect process/answer combinations leading to low correlation with human labels.",
            "scaling_trend": "Serves as a contrast showing substantial gains from stronger base models and targeted finetuning pipelines.",
            "uuid": "e6656.5",
            "source_info": {
                "paper_title": "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "How well do large language models perform in arithmetic tasks?",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Measuring mathematical problem solving with the MATH dataset",
            "rating": 2
        },
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1
        },
        {
            "paper_title": "CritiqueLLM: Scaling LLM-as-critic for effective and explainable evaluation of large language model generation",
            "rating": 1
        }
    ],
    "cost": 0.019429,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline</h1>
<p>Yifan Xu ${ }^{121 <em>}$, Xiao Liu ${ }^{12 </em>}$, Xinghan Liu ${ }^{121}$, Zhenyu Hou ${ }^{12}$, Yueyan Li ${ }^{11}$, Xiaohan Zhang ${ }^{1}$, Zihan Wang ${ }^{13}$, Aohan Zeng ${ }^{12}$,Zhengxiao Du ${ }^{12}$, Wenyi Zhao ${ }^{1}$, Jie Tang ${ }^{2}$, Yuxiao Dong ${ }^{2}$<br>${ }^{1}$ Zhipu.AI ${ }^{2}$ Tsinghua University<br>$\star$ ZHIPU AI</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have shown excellent mastering of human language, but still struggle in real-world applications that require mathematical problemsolving. While many strategies and datasets to enhance LLMs' mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed LLM systems. In this work, we tailor the Self-Critique pipeline, which addresses the challenge in the feedback learning stage of LLM alignment. We first train a general Math-Critique model from the LLM itself to provide feedback signals. Then, we sequentially employ rejective fine-tuning and direct preference optimization over the LLM's own generations for data collection. Based on ChatGLM3-32B, we conduct a series of experiments on both academic and our newly created challenging dataset, MATHUSEREval. Results show that our pipeline significantly enhances the LLM's mathematical problem-solving while still improving its language ability, outperforming LLMs that could be two times larger. Related techniques have been deployed to ChatGLM ${ }^{1}$, an online serving LLM. Related evaluation dataset and scripts are released at https://github.com/THUDM/ChatGLM-Math.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Results of Hungarian Exam and Average Scores of GSM8k and MATH.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 Introduction</h1>
<p>Large Language Models (LLMs) [8; 10; 20; 40; 44; 61; 1] have garnered widespread attention for their remarkable proficiency in various linguistic tasks such as text summarization[18; 47; 33; 26], question answering [16; 24; 7], and role-playing conversations [46; 67; 41]. Furthermore, their potential in addressing complex problems requiring mathematical reasoning [57; 48; 31] has expanded their applicability across real-world missions [30; 5].
Despite these advances, optimizing LLMs to excel simultaneously in language understanding and mathematical problem-solving presents a notable challenge. The prevalent reinforcement learning from human feedback (RLHF) approach primarily enhances text generation based on reward models reflecting human preferences [44; 35; 45]. Although this method boosts the quality of generated text, it often overlooks the accuracy and logical coherence essential for solving mathematical problems, leading to a discrepancy in performance known as the "alignment tax"[2] when applied to mathematical reasoning (refer to Table 1). Conversely, attempts to bolster LLMs' mathematical capabilities typically entail supervised fine-tuning (SFT) that inadvertently diminishes their linguistic versatility, posing a dilemma for practical applications of LLM systems [43; 57; 31; 60].</p>
<p>Pipeline: Self-Critique. This paper introduces a novel approach aimed at enhancing both linguistic and mathematical skills of LLMs without compromising one for the other. Our strategy deviates from traditional RLHF by incorporating a Math-Critique model derived from the LLM itself, which evaluates its mathematical outputs. This self-critique mechanism enables the model to learn from AIgenerated feedback specifically tailored to mathematical content [4; 25]. Our methodology comprises two primary phases:</p>
<ul>
<li>Stage 1: Rejective Fine-tuning (RFT) [58] employs a rejection sampling technique, wherein responses failing to meet Math-Critique standards are discarded, while the rest undergo further fine-tuning. This stage aims to enhance the model's accuracy and consistency in mathematical responses while ensuring diversity among the selected answers.</li>
<li>Stage 2: Direct Preference Optimization (DPO) [38] extends the improvement process by directly learning from pairs of correct and incorrect answers, further refined through Math-Critique, focusing on the most challenging questions from the previous stage.</li>
</ul>
<p>Benchmark: MathUsErEval. To accurately assess LLMs' capabilities in solving real-world mathematical problems, we develop the MathUsErEval dataset. It features a diverse range of questions, extending beyond academic exercises to include practical application scenarios, thereby better reflecting user needs compared to traditional academic math datasets [64; 50; 11]. We leverage both GPT-4-turbo and our Math-Critique model for comprehensive scoring.
In summary, our contributions include:</p>
<ul>
<li>The introduction of the Self-Critique pipeline, a novel framework that elevates both the mathematical and linguistic capabilities of LLMs through self-generated feedback, thereby eliminating the need for external supervisory models and manual annotations. This approach has been validated on a ChatGLM3-32B model, achieving unparalleled performance on the MathUsErEval, Ape210k [64], MATH [16], and the linguistic tasks of AlignBench [29].</li>
<li>The creation of the MathUsErEval benchmark, tailored to assess LLMs on complex, open-ended mathematical queries relevant to real-world applications, setting a new standard in evaluating practical mathematical reasoning capabilities.</li>
<li>A detailed analysis of the key factors contributing to the enhancement of mathematical proficiency through the Self-Critique pipeline, offering insights into future directions for autonomous model improvement.</li>
</ul>
<h2>2 Related Work</h2>
<p>LLM for Math Problem-Solving. Various approaches have been explored to enhance the mathematical problem-solving abilities of language models. Prompting Methods, initiated by Chain of Thought prompting [51], have been refined to guide models through detailed reasoning, with notable contributions from [55; 6; 53] enhancing mathematical and reasoning tasks. These methods, however,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Self-Critique pipeline for ChatGLM-Math. Our method comprises three steps: training the Math-Critique model, then utilizing the results from Math-Critique for sampling, followed by two stages of training: Critique RFT and Critique DPO. Throughout the entire process, only a minimal amount of manual involvement is required during the Math-Critique training phase. Subsequent steps can be fully automated and do not depend on external supervisory models.
are limited by the model's inherent capabilities as they do not modify the model itself. Supervised Fine-tuning and Reinforcement Learning (RL) have also been pivotal. High-quality supervisory data from works like [31; 58; 9; 57; 60; 62] aim to improve model capabilities directly. RL's potential in general domains, demonstrated by [34; 44; 12], has led to mathematical enhancements through OpenAI's Process Reward Model [27] and PPO algorithm applications [31; 48], despite the challenges in applying the DPO algorithm [38] for mathematical tasks. Decoding Strategy and the use of External Tools offer additional avenues for improvement. Self-Consistency [49] and MATH-SHEPHERD [48] explore sampling strategies to enhance problem-solving while code insertion techniques in works like Mammoth [60] and Tora [15] bypass computational limitations.</p>
<p>Mathematical Evaluation. Complex reasoning tasks, such as mathematics, have always been one of the key indicators in assessing the capabilities of language models [22; 37; 17; 14]. Solving a mathematical problem involves semantic understanding, problem decomposition, symbolic reasoning, and numerical computation, making it an unparalleled choice for evaluating the cognitive reasoning ability of LLMs. The GSM8k [11] and MATH [17] datasets have become the most widely used benchmarks. The GSM8K dataset focuses on multi-step reasoning with basic arithmetic, showcasing the complexity and linguistic diversity inherent in grade school-level mathematics. The MATH benchmark further spans various subjects and difficulty levels, facilitating a detailed examination of a model's problem-solving skills.</p>
<p>Additionally, as one of the most focused-on abilities in language models, there exists a vast array of datasets for mathematical capability across various languages. Some standout sets like AQuA [28], Mathematics [39] and SAT-Math [66] dive deep into pure math prowess, while NumGLUE [32] and BBH [42] mix math with other abilities. Moreover, in Chinese, Math23K [50] and CMath [52] make elementary and middle school math a playground for problem-solvers, with AgiEval [66] and GaoKaoBench [63] turn to exam-level challenges. Moreover, Math401 [59] zeroes in on the ability of number calculation. However, these data are primarily in fixed formats, and current works have found that simple perturbations to these questions can significantly impact performance [23; 68]. Therefore, performance on these datasets needs to accurately reflect how models would perform when faced with user math questions.</p>
<h1>3 Math-Critique: A General Critic for Math</h1>
<p>Definition. The first part of our work involves constructing an accurate and robust evaluation model. We propose Math-Critique, inspired by works that use large models for evaluation purposes [21; 65]. This method scores mathematical responses generated by models based on questions and reference answers, including an output of explanatory analysis and a score between 1 and 10. Compared to traditional reward models, this approach leverages the contextual capabilities of language models,</p>
<p>enabling more accurate judgments by integrating reference answers. The use of explanatory analysis combined with scoring is also inspired by the concept of thought chains, enhancing scoring accuracy while providing interpretability.</p>
<p>In the instructions, Math-Critique must classify responses into four categories: entirely incorrect, partially correct methodology yet erroneous outcome, accurate conclusion with partially flawed methodology, and wholly correct. These categories are aligned with scoring ranges of $1-2,3-5,6-8$, and $9-10$, respectively.
Therefore, the Math critique model can be defined as:</p>
<p>$$
\text { MathCritique }(\text { Question, Reference, Answer }) \rightarrow(\text { Critique, Score })
$$</p>
<p>Here, the Question and Reference are the original problem and the reference answer, respectively, and the Model Answer is the answer given by the model being evaluated.</p>
<p>We employed two evaluation methods using math-critique: average score evaluation and hard-split evaluation. The average score evaluation calculates the mean of the critique scores assigned to each model answer for a set of questions. On the other hand, the hard-split evaluation categorizes each model answer as either passing or failing based on a predefined correctness threshold. If the critique score surpasses this threshold for each answer, the answer is considered correct; otherwise, it is deemed incorrect. The overall score is then calculated as the proportion of correct answers out of the total number of questions.</p>
<p>Data Collection. Our construction method involves the following steps:</p>
<ul>
<li>We redesigned the scoring rules and intervals for mathematical responses, enabling the model to grade based on the correctness of the result and the process.</li>
<li>We filtered a dataset from the training data, which includes mathematics questions along with their reference answers and model responses, primarily sourced from exam questions ranging from junior high to university levels. We utilized model sampling answers from multiple sources, including different versions of ChatGLM and other models.</li>
<li>We employed CritiqueLLM [21] and ORM to annotate the dataset, selecting annotations that represented the best and worst scoring extremes from these models, and directly used these pseudo tags for training. This step generated a total of 10 k annotated data entries.</li>
<li>For results with scores in the middle range, we selected a portion for manual annotation into four categories and then mapped these outcomes to a 10-point scale. We also divided a test set from the training dataset and used the same method for four-category annotation. This step generated 5 k annotated data entries for the training set and 800 for the test set.</li>
</ul>
<h1>4 The Self-Critique Pipeline</h1>
<p>Overview. Based on the construction method of Math-Critique, this section introduces the SelfCritique pipeline. This pipeline is a weakly supervised iterative training method for enhancing mathematical abilities, originating from a single model. Initially, we train a Math-Critique model using the base model and concurrently train a basic Chat Model using the fundamental SFT dataset. Subsequently, we employ the Math-Critique model to supervise the fine-tuning of the Chat Model through rejection sampling. The outcome of this step can serve as a new base model to update both the Math-Critique model and the rejection sampling supervised fine-tuning model. Building upon these steps, our final action involves utilizing the latest Math-Critique model to sample contrast data and then proceeding with DPO training.
In these steps, the data construction for the Math-critique-base involves a small amount of manual annotation. However, this batch of annotations is a one-time effort, as only this batch of annotated data is needed as a bootstrap for the remaining iterations. After that, inference and automatic model filtering can complete all remaining steps.
Replacing manual annotation with inference can significantly reduce the time required for each iteration from the base model to the final chat model. This avoids a problematic scenario: after weeks of manual annotation, the base model may have undergone further pretraining and become stronger, making it unclear whether the data sampled weeks ago is still accurate or has a significant distribution gap for the current model.</p>
<h1>4.1 Stage 1: Rejective Fine-tuning</h1>
<p>We utilized a rejection sampling method based on Math-Critique. We re-examined and redesigned the implementation of RFT and found that both the sampling range and the model influence the outcomes during the rejection sampling process. Specifically, we designed the following sampling principles:</p>
<ul>
<li>Pre-deduplication: Cluster question embeddings from the training set and evenly sample across categories, ensuring a diverse range of questions without repetition.</li>
<li>Post-sampling deduplication: We conducted a selection process after 5-10 sampling iterations based on the results from Math-Critique. After essential deduplication, we chose the entirely correct response only in cases where there were both correct and incorrect responses to the same question.</li>
</ul>
<p>Following the process outlined above, we have obtained the Critique-RFT dataset:</p>
<p>$$
D_{\mathrm{RFT}}=\left{\left(q_{i}, a_{i j}\right) \left\lvert\, \frac{1}{n} \sum_{x} \operatorname{MathCritique}\left(a_{i x}\right)&lt;1 \text { and } \operatorname{MathCritique}\left(a_{i j}\right)&gt;\text { correct-bound }\right.\right}
$$</p>
<p>In this dataset, $q_{i}$ denotes the ith sampled question, with each question undergoing $n$ samplings. $a_{i j}$ represents the jth response to the ith question. MathCritique refers to Math-Critique score. 'correct bound' denotes the minimum acceptable score for a correct answer, generally set at 0.7 .</p>
<h3>4.2 Stage 2: Direct Preference Optimization</h3>
<p>We employed the DPO method to enhance model capabilities further following Critique RFT. The primary advantages of this method are its simplicity in constructing data flows, stability, and speed during training. The DPO method directly compares the correct and incorrect answers to the same question. In our approach, both answers are sampled from the model post-RFT, which we found to be critically important. We also integrated sft loss of DPO positive examples during training as an approximate substitute for a regularization term.
Our DPO data filtering process is similar to Critique RFT, with the sole difference being the construction method of DPO training pairs. For the selection of DPO pairs, under the premise that there is at least one correct and one incorrect answer, we choose the data pair with the most significant difference in Math-Critique scoring results.
Following the process outlined above, we have obtained the Critique-DPO dataset:</p>
<p>In this dataset, each element is a tuple, where $q_{i}$ is the ith sampled question. For every question $q_{i}$, sampled $n$ responses, each denoted by $a_{i x}$. The Math-Critique (MathCritique) score is computed for each response $a_{i x}$, and the average of these scores must be less than 1 . The chosen answer for each question, $a_{i-\text { chosen }}$, is the one that exceeds the 'correct-bound', which is a predetermined threshold indicating a satisfactory level of correctness, often set above a specific value. Conversely, $a_{i-\text { rejected }}$ represents the answer that falls below the 'rejected-bound', which is the threshold below which answers are considered incorrect or unsatisfactory.</p>
<h3>4.3 Training</h3>
<h3>4.3.1 Math-Critique Training</h3>
<p>We employ the base model of ChatGLM3-32B [61; 13] as the initial Math-Critique base model. After each iteration, the model currently refined through SFT (Supervised Finetuning) or Critique RFT will be used as the base. We use a learning rate 3e-6 and a batch size 128 on both 6B and 32B scales.</p>
<h3>4.3.2 Critique-RFT Training</h3>
<p>During the Critique RFT phase, each of our finetuning iterations includes the datasets from previous stages after deduplication, which also encompasses the initial sft dataset. We merge $D_{\mathrm{RFT}}$ and $D_{\mathrm{SFT}}$</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Training datasets examples. The training data we generated is divided into two categories, originating from the questions and references within existing datasets. We have constructed separate RFT training data and paired DPO training data.
as:</p>
<p>$$
D=D_{\mathrm{RFT}} \cup D_{\mathrm{SFT}}
$$</p>
<p>The $D_{\text {SFT }}$ dataset encompasses many routine tasks and can be substituted with an open-source instruction finetuning dataset. To eliminate the potential interference of this dataset on the final results, we compared the impact of including or excluding the sft data in our ablation study. We finetune a base LLM model $\pi_{\theta}$ by standard max-loglikelihood loss:</p>
<p>$$
\mathcal{L}\left(\pi_{\theta}\right)=-\mathbb{E}<em i="i">{\left(q</em>\right)\right)\right]
$$}, a_{i j}\right) \sim \mathcal{D}}\left[\log \left(\pi_{\theta}\left(a_{i j} \mid q_{i</p>
<p>In this stage, we use a learning rate $2 \mathrm{e}-5$ and finetune for 8000 steps with a batch size of 64 .</p>
<h1>4.3.3 Critique-DPO Training</h1>
<p>During the Critique-DPO phase, it was observed that the direct use of DPO loss led to instability in the training process. A cross-entropy loss for the chosen answer was introduced as a regularization term to the total loss to mitigate this issue. This addition aimed to enhance the stability of the model training. The DPO dataset, previously constructed and denoted as $D_{\mathrm{DPO}}$, was used as the training dataset. The loss function we used is as follows:</p>
<p>$$
\begin{gathered}
\mathcal{L}<em _theta="\theta">{\mathrm{DPO}}\left(\pi</em>} ; \pi_{r e f}\right)=-\mathbb{E<em i="i">{\left(q</em>}, a_{i-\mathrm{cho}}, a_{i-\mathrm{rej}}\right) \sim \mathcal{D<em _theta="\theta">{\mathrm{DPO}}}\left[\log \sigma\left(\beta \log \frac{\pi</em>\right)\right] \
\mathcal{L}}\left(a_{\mathrm{i}-\mathrm{cho}} \mid q_{i}\right)}{\pi_{r e f}\left(a_{\mathrm{i}-\mathrm{cho}} \mid q_{i}\right)}-\beta \log \frac{\pi_{\theta}\left(a_{\mathrm{i}-\mathrm{rej}} \mid q_{i}\right)}{\pi_{r e f}\left(a_{\mathrm{i}-\mathrm{rej}} \mid q_{i}\right)<em _theta="\theta">{\mathrm{CE}}\left(\pi</em>} ; \pi_{r e f}\right)=-\mathbb{E<em i="i">{\left(q</em>\right)\right)\right] \
\mathcal{L}}, a_{i-\mathrm{cho}}\right) \sim \mathcal{D}}\left[\log \left(\pi_{\theta}\left(a_{\mathrm{i}-\mathrm{cho}} \mid q_{i<em _mathrm_DPO="\mathrm{DPO">{\text {merge }}=\lambda \cdot \mathcal{L}</em>
\end{gathered}
$$}}+\mathcal{L}_{\mathrm{CE}</p>
<p>In this context, $\lambda$ represents the coefficient of the cross-entropy loss for the chosen answer in the total loss. Commonly, we experiment with values in ${0.5,1,1.5}$. Another critical coefficient is $\beta$, which measures the penalty intensity of DPO for incorrect answers. Owing to the addition of a regularization term, the value of this coefficient is higher than that of the standard DPO, with our testing range for this value being ${0.5,1,2}$. Besides these, the overall learning rate is set at 1e-6. The experimental section will report the optimal results under these coefficient settings. We train 500 steps with a batch size of 64 in this stage.</p>
<h2>5 MathUsEREval: Benchmarking LLMs’ Mathematical Reasoning in Application</h2>
<p>MathUsEREval is a test set designed for real-use scenarios, targeting issues of concern to users and more challenging mathematical problems. Some of our data originates from university examination questions, while another is from simulated dialogues. For the latter, we assigned a series of annotators who posed math-related questions to our system based on their daily experiences and observations using large models.</p>
<p>Table 2: The composition of the MATHUSEREval data set. We divided the test set into three categories: Elementary and Advanced Mathematics. For calculating the total score, we used the macro-average score.</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Sub-Category</th>
<th>Size</th>
<th>Source</th>
</tr>
</thead>
<tbody>
<tr>
<td>Elementary</td>
<td>Calculate</td>
<td>75</td>
<td>Dialogues</td>
</tr>
<tr>
<td></td>
<td>Algebra</td>
<td>113</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Geometry</td>
<td>81</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Trigonometry</td>
<td>73</td>
<td></td>
</tr>
<tr>
<td>Advanced</td>
<td>Discrete Math</td>
<td>45</td>
<td>Dialogues\&amp;Exams</td>
</tr>
<tr>
<td></td>
<td>Probability</td>
<td>46</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Linear Algebra</td>
<td>58</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Calculus</td>
<td>54</td>
<td></td>
</tr>
</tbody>
</table>
<p>Based on the distribution of the collected data, we divided the test set into two main categories, Elementary and Advanced, and eight sub-categories. Given that Calculate Applications are less challenging and closely aligned with the scope of previous public datasets, we selected fewer questions from this category. The quantity of questions in each of these categories is as shown in Table 2. All questions are posed in an open-ended format. Possible answers include a single number, multiple numbers, or mathematical expressions.
We offer two evaluation methods: GPT-4-1106—-Preview [34; 29; 65] evaluation and Math-Critique evaluation. The former adopts the evaluation method of alignbench [29], to provide a more accurate, fair, and accessible evaluation approach; the latter employs the same usage as the Math-Critique introduced above. Similarly, we will also report two types of scores: avg-score and hard-split.</p>
<h1>6 Experiment</h1>
<h3>6.1 Data Collection</h3>
<p>The primary sources of our data collection can be categorized as follows: training sets from public datasets and publicly available middle school and university examination questions. We have selected all prompts from GSM8k [11] and MATH [17] training set as the question set for the English data, and used the responses from the original dataset as the standard answers. Regarding publicly available middle school and university exam questions, we used the answer formats provided with the exam papers as the common answers without further processing.</p>
<h3>6.2 Evaluation Setting</h3>
<h3>6.2.1 Datasets</h3>
<p>In our research, we primarily tested the MATHUSEREval dataset, which originates from simulated dialogue records and actual exam papers. Compared to academic datasets, this dataset features a more diverse array of question styles and more closely aligns with real-world usage scenarios. Additionally, we tested the following academic datasets: 1. English academic datasets: GSM8k [11] and MATH [17]. These two datasets contain English mathematics problems at the middle and high school and competition levels. 2. Chinese academic datasets: ape210k [64] and cmath [52]. The questions in these datasets also originate from middle and high school levels. We also employed the Hungarian National Exam [36] as an Out-Of-Distribution test set. It should be noted that, across all test sets, we only used the training sets of GSM8k and MATH as seed data for data generation.</p>
<p>To evaluate general linguistic capabilities, we selected the Chinese language component of AlignBench [29] and full MT-Bench [65] for testing.</p>
<h3>6.2.2 Baselines</h3>
<p>Since most of our work is conducted in Chinese, we selected three categories of baselines: opensource mathematics-specific models, open-source Chinese models, and leading proprietary models.</p>
<p>Table 3: Main Result. All results reported are the highest achieved in zero-shot or few-shot settings and are based on greedy decoding. The best models are marked in bold and the underline signifies the second best model.</p>
<p>| Models | #params | Chinese | | | | | English | | | | General | | | | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| | | MathUserEval | | | Ape210k | Cmath | GSM8k | MATH | Hunga | AlignBench | MT-Bench |  |  |  |  |  |  |  |  |  |
| | | Overall | Elementary | Advanced |  |  |  |  |  |  | Language |  |  |  |  |  |  |  |  |  |
| GPT-4-1106-Preview [34] |  | N/A | 5.73 | 5.07 | 6.81 | 84.2 | 89.3 | 93.6 | 53.6 | 92 | 8.29 | 9.32 |  |  |  |  |  |  |  |  |
| GPT-4-0613 [34] |  | N/A | 4.14 | 3.34 | 5.33 | 83.6 | 86.5 | 91.4 | 45.8 | 68 | 7.59 | 9.18 |  |  |  |  |  |  |  |  |
| GPT-3.5-Turbo-0613 [34] |  | N/A | 3.42 | 3.04 | 4.07 | 70.4 | 76.8 | 78.2 | 28.0 | 41 | 6.82 | 8.36 |  |  |  |  |  |  |  |  |
| Claude-2 [1] |  | N/A | 3.29 | 2.63 | 4.35 | 72.8 | 80.5 | 88.0 | - | 55 | 6.78 | 8.06 |  |  |  |  |  |  |  |  |
| GLM-4 |  | N/A | 5.11 | 4.86 | 5.43 | 93.5 | 89.0 | 91.8 | 49.0 | 75 | 8.38 | 8.62 |  |  |  |  |  |  |  |  |
| Skywork-13B-Math [54] |  | 13B | 2.66 | 2.75 | 2.54 | 74.4 | 77.3 | 72.3 | 17.0 | 39 | 5.58 | 4.12 |  |  |  |  |  |  |  |  |
| InternLM2-Chat [43] |  | 20B | 3.25 | 3.00 | 3.68 | 72.0 | 80.7 | 79.6 | 34.8 | 48 | 7.68 | 8.21 |  |  |  |  |  |  |  |  |
| Math-InternLM2 [43] |  | 20B | 3.17 | 3.08 | 3.37 | 75.2 | 78.5 | 82.6 | 37.7 | 66 | 6.53 | 6.09 |  |  |  |  |  |  |  |  |
| Yi-Chat [56] |  | 34B | 2.64 | 2.49 | 2.87 | 65.1 | 77.7 | 76.0 | 15.9 | 39 | 6.18 | 6.54 |  |  |  |  |  |  |  |  |
| DeepSeek-Chat [12] |  | 67B | 3.24 | 2.76 | 3.84 | 76.7 | 80.3 | 84.1 | 32.6 | 58 | 7.11 | 8.35 |  |  |  |  |  |  |  |  |
| MetaMath (EN) [57] |  | 70B | - | - | - | - | - | 82.3 | 26.0 | 35 | - | 4.28 |  |  |  |  |  |  |  |  |
| Qwen-Chat [3] |  | 72B | 3.87 | 3.99 | 3.67 | 77.1 | 88.1 | 76.4 | 31.8 | 52 | 7.29 | 6.43 |  |  |  |  |  |  |  |  |
| ChatGLM3-32B-SFT-2312* |  | 32B | 3.25 | 3.03 | 3.60 | 78.0 | 79.8 | 75.8 | 29.0 | 39 | 7.37 | 8.05 |  |  |  |  |  |  |  |  |
| + RFT |  | 32B | 4.01 | 3.86 | 4.26 | 87.0 | 85.3 | 82.4 | 39.5 | 58 | 7.42 | 8.03 |  |  |  |  |  |  |  |  |
| + RFT, DPO |  | 32B | 4.23 | 4.01 | 4.59 | 89.4 | 85.6 | 82.6 | 40.6 | 73 | 7.80 | 8.08 |  |  |  |  |  |  |  |  |</p>
<ul>
<li>ChatGLM3-32B-SFT-2312 is a newer version of the ChatGLM series and not identical to the model discussed in [19], despite sharing the same model size.</li>
</ul>
<p>For the open-source mathematics models, we chose SkyMath [54], MetaMath [57], and Internlm2Math [43] as our baselines. To effectively compare with the best Chinese models, we selected Qwen-Chat [3], Yi-Chat [56], DeepSeek-Chat [12], and InternLM2 [43]. Additionally, we also report the results for GPT-4-1106-Preview(known as GPT4-Turbo), GPT-4-0613,GPT-3.5-Turbo [34], and Claude-2 [1].</p>
<h1>6.2.3 Metrics</h1>
<p>For all datasets, we utilized the results of greedy inference performed once. Regarding academic datasets, we report the self-reported results of corresponding models and the highest zero-shot/fewshot results from the OpenCompass and MATHUSEREval websites. For the math subset of AlignBench [29] and our proposed MATHUSEREval test set, we report the scoring results from GPT-4Turbo and the scores generated by Math-Critique. More information about evaluation settings can be found in Appendix C.</p>
<h3>6.3 Main Results</h3>
<p>Table 3 displays our main results. In models with more than 10 billion parameters, our model achieved a score of 4.23 on MATHUSEREval, 89.4 on ape210k [64], and 40.6 on MATH [16], surpassing all models with published parameters and achieved near-top performances on Cmath and GSM8k. Our model also scored 73 in the Hungary Test [36], the highest score among all known parameter models.
We used the ChatGLM3-32B-SFT-2312 version as our baseline. Our RFT phase significantly improved across all math datasets. In contrast, the DPO phase's improvement focused on openended math problems, including MATHUSEREval, the Hungarian Exam, and the general-purpose AlignBench. Even though our improvement on MT-bench [65] is not significant, given that over $90 \%$ of our training data is in Chinese, we believe that maintaining parity essentially demonstrates that our method has preserved the original English general or multi-turn capabilities.
Compared to proprietary models, especially the GPT series by OpenAI, GLM-4 demonstrates competitive or superior performance in specific areas. GPT-4-1106-Preview, for example, shows the best performance in most tasks, including the highest scores in both Chinese and English benchmarks, highlighting its effectiveness in various mathematical problem-solving contexts. However, GLM4 surpasses it in the Ape210k and AlignBench benchmarks, suggesting particular strengths in mathematical reasoning and cross-linguistic generalization.</p>
<p>Table 4: Ablation Study for 32B model. All results are fine-tuned from our 32B base model. We selected Metamath training set as baselines that we consider comparatively strong. MATHUSEREval is scored with Math-Critique model.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Chinese</th>
<th></th>
<th>English</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>MATHUSEREVAL</td>
<td>Ape210k*</td>
<td>GSM8k</td>
<td>MATH*</td>
</tr>
<tr>
<td>Metamath [57]</td>
<td>2.80</td>
<td>75.8</td>
<td>77.9</td>
<td>35.6</td>
</tr>
<tr>
<td>ChatGLM3-32B-SFT-2312 + RFT</td>
<td>3.74</td>
<td>87.0</td>
<td>82.4</td>
<td>39.5</td>
</tr>
<tr>
<td>- Real scenarios \&amp; Academic</td>
<td>3.29</td>
<td>85.9</td>
<td>74.8</td>
<td>27.6</td>
</tr>
<tr>
<td>- Real scenarios</td>
<td>3.29</td>
<td>74.6</td>
<td>77.4</td>
<td>36.0</td>
</tr>
<tr>
<td>- Academic</td>
<td>3.72</td>
<td>75.8</td>
<td>81.0</td>
<td>36.2</td>
</tr>
<tr>
<td>ChatGLM3-32B-SFT-2312 + RFT \&amp; DPO</td>
<td>4.37</td>
<td>89.4</td>
<td>82.6</td>
<td>41.0</td>
</tr>
<tr>
<td>- Real Scenarios \&amp; Academic</td>
<td>4.14</td>
<td>87.8</td>
<td>81.5</td>
<td>37.8</td>
</tr>
</tbody>
</table>
<ul>
<li>Ablated experiments are conducted on 500-sample test subsets.
<img alt="img-3.jpeg" src="img-3.jpeg" /></li>
</ul>
<p>Figure 4: The Relationship between Different Boosting Methods and Problem Difficulty. The horizontal axis displays the average score of MATHUSEREVAL across 24 models (scored by GPT-4-1106-Preview), which we regard as a representation of problem difficulty. The vertical axis represents the hard-split scores of the models on these questions.</p>
<h1>6.4 Ablation Study</h1>
<p>Impact of data composition. Table 4 presents the results of ablation experiments. We selected Metamath [57] training set as baselines that we consider comparatively strong. After applying Critique-RFT, we found that using only academic datasets to construct RFT data results in inferior performance in real-life scenario-based MATHUSEREval and academic test sets compared to the outcomes after integrating real-life scenario data. Furthermore, introducing English data significantly improves performance on English datasets without substantially affecting Chinese capabilities.
The ablation experiments conducted during the Critique-DPO phase indicate that, compared to using general DPO data, the addition of math-specific DPO data significantly enhances mathematical capabilities. We no longer conduct separate tests on the impact of Real scenarios and Academic data on capabilities. This is because, in previous stages, questions that the model could solve correctly were removed, making it impossible to ensure that both datasets still have sufficient size for a complete training session.</p>
<h2>Relationship between Different Boosting Methods and Problem Difficulty.</h2>
<p>Figure 4 displays the relationship between the average accuracy of each question in MATHUSEREval across all 24 models tested (including some intermediate models) and the hard-split scores of the four GLM series models. The average accuracy across all tested models is considered a reflection of the difficulty level of the question. It can be observed that the RFT step improves performance across almost all difficulty levels, but the most significant improvements come from questions with an average score between 4 and 6 . The DPO step mainly enhances performance on questions with an</p>
<p>Table 5: Evaluation for Math-Critique Model. We report "Acc" as the accuracy of the model in determining whether an answer is correct, as well as the Pearson, Spearman, and Kendall correlation coefficients for Math-Critique in comparison with human annotations in a four-category classification.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Acc.</th>
<th>Pearson</th>
<th>Spearman</th>
<th>Kendall</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3.5-Turbo</td>
<td>62.1</td>
<td>31.8</td>
<td>33.5</td>
<td>30.1</td>
</tr>
<tr>
<td>GPT-4-0613</td>
<td>90.2</td>
<td>80.5</td>
<td>78.1</td>
<td>71.0</td>
</tr>
<tr>
<td>Math-Critique-32B</td>
<td>90.5</td>
<td>80.4</td>
<td>77.1</td>
<td>70.2</td>
</tr>
</tbody>
</table>
<p>average score between 5 and 7 . This suggests that our two-step approach to enhancing mathematical capabilities can be seen as aligning the model more closely with real-world conditions, with the most noticeable improvements on medium-difficulty questions.</p>
<p>Impact on general capabilities. Considering that our goal is not to develop a specialized mathematical model for leaderboard climbing but rather a general model with strong mathematical capabilities, we tested the results using Alignbench [29], a Chinese general open question-answering dataset. The results in Table 3 demonstrate that our model exceeds the training outcomes of similar baseline models that do not incorporate specialized mathematical data regarding Chinese language capabilities. Additionally, it performs exceptionally well compared to other open-source Chinese mathematical/general models.</p>
<p>In terms of English general capabilities, we tested using MT-Bench [65] as the test set. Given that over $90 \%$ of our training data consists of Chinese, the fact that the results on MT-Bench [65] remained largely unchanged during our training process indicates that the English language capabilities were not significantly affected.</p>
<p>Effectiveness of Math-Critique. During the process of manual annotation, we collected a test set of 800 questions, all of which were manually marked for the correctness of their answers and procedures, thus forming a four-category test; the output results of Math-Critique were mapped to these four categories according to the requirements of the instructions.
We validated the effectiveness of Math-Critique itself through empirical experiments. We set up two evaluation methods: the accuracy of directly scoring to judge correct/incorrect results and the accuracy of judging our defined four categories. We extracted test sets from Chinese junior and senior high school exam questions and MATHUSEREVAL, annotated correct judgment by experts.
The results shown in Table 5 indicate that our Math-Critique-32B model significantly surpasses GPT-3.5-Turbo in both judgment accuracy and correlation coefficients compared to human annotations and is essentially on par with GPT-4-0613.</p>
<p>Out-Of-Distribution Test. Following the approach of Grok-1, to test the performance on Out-OfDistribution datasets, we selected the Hungarian national final exam [36]. This is a test set of 33 exam questions without a training set, the advantage being that it allows for evaluating a model's mathematical capabilities in an utterly OOD environment. As shown in Figure 1, using human expert evaluation, we found that at a model scale of 32B, our RFT model scored 57, while the DPO model scored 73. However, it is essential to note that since our model's primary language is Chinese, if the model answers correctly in Chinese, we would score it usually. We plan to address this issue in future models.</p>
<h1>7 Limitation and Future Work</h1>
<p>We observed the following issues in our mathematical models, and we leave it for our future work:
Graphic thinking and drawing abilities. Due to the limitations of being a purely linguistic model, our model has deficiencies in handling questions requiring drawing. For example, in a question from the Hungary Test, which required connecting six numbers as divisors of each other, our model correctly listed the different numbers' connecting topology but could not draw it accurately. Also, as a language model, it struggles to respond correctly to questions requiring an understanding of images.</p>
<p>A potential solution could be integrating multimodal input and output components, an area we plan to explore further.</p>
<p>Precision calculation capability. We observed that in incorrectly answered questions, if the problem required multiplication, division, or exponentiation of three or more decimal places, our model might compute with a deviation of up to $5 \%$. This phenomenon aligns with observations from GPT-4 models without an integrated code interpreter. This issue might be a fundamental problem to pure language models and could be mitigated but not resolved with increasing model size. Using external tools for computation or directly employing code with a code interpreter could solve this problem. However, our discussion in this paper focuses on enhancing the mathematical capabilities of pure language models, and we will endeavor to address these issues in future work.</p>
<h1>8 Conclusion</h1>
<p>In this paper, we introduce the Math-Critique method for evaluating the correctness of mathematical problems, and based on this method, we propose the Self-Critique method aimed at enhancing the mathematical capabilities of language models without the need for external supervisory models and manual annotations. Our experiments were conducted in both English and Chinese, and a 32-billion parameter model achieved state-of-the-art results among open-source language models on multiple datasets. Additionally, it surpassed several renowned proprietary models, including GPT-4-0613, on our proposed MATHUSEREval test set. Our method was applied during the development process of GLM-4 as a component to improve mathematical capabilities, achieving the best results on datasets such as MATHUSEREVAL, ape210k, GSM8k, and the Hungarian test, except GPT-4-Turbo.</p>
<h1>References</h1>
<p>[1] Anthropic. Introducing claude, 2023.
[2] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, and J. Kaplan. A general language assistant as a laboratory for alignment, 2021.
[3] J. Bai, S. Bai, et al. Qwen technical report, 2023.
[4] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosuite, L. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei, N. Joseph, S. McCandlish, T. Brown, and J. Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022.
[5] Y. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.
[6] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, L. Gianinazzi, J. Gajda, T. Lehmann, M. Podstawski, H. Niewiadomski, P. Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023.
[7] Y. Bisk, R. Zellers, R. Le bras, J. Gao, and Y. Choi. Piqa: Reasoning about physical commonsense in natural language. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7432-7439, Apr. 2020.
[8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[9] E. Chern, H. Zou, X. Li, J. Hu, K. Feng, J. Li, and P. Liu. Generative ai for math: Abel. https://github.com/GAIR-NLP/abel, 2023.
[10] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[11] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021.
[12] DeepSeek-AI, :, X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, H. Gao, K. Gao, W. Gao, R. Ge, K. Guan, D. Guo, J. Guo, G. Hao, Z. Hao, Y. He, W. Hu, P. Huang, E. Li, G. Li, J. Li, Y. Li, Y. K. Li, W. Liang, F. Lin, A. X. Liu, B. Liu, W. Liu, X. Liu, X. Liu, Y. Liu, H. Lu, S. Lu, F. Luo, S. Ma, X. Nie, T. Pei, Y. Piao, J. Qiu, H. Qu, T. Ren, Z. Ren, C. Ruan, Z. Sha, Z. Shao, J. Song, X. Su, J. Sun, Y. Sun, M. Tang, B. Wang, P. Wang, S. Wang, Y. Wang, Y. Wang, T. Wu, Y. Wu, X. Xie, Z. Xie, Z. Xie, Y. Xiong, H. Xu, R. X. Xu, Y. Xu, D. Yang, Y. You, S. Yu, X. Yu, B. Zhang, H. Zhang, L. Zhang, L. Zhang, M. Zhang, M. Zhang, W. Zhang, Y. Zhang, C. Zhao, Y. Zhao, S. Zhou, S. Zhou, Q. Zhu, and Y. Zou. Deepseek llm: Scaling open-source language models with longtermism, 2024.
[13] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320-335, 2022.
[14] Y. Fu, L. Ou, M. Chen, Y. Wan, H. Peng, and T. Khot. Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. arXiv preprint arXiv:2305.17306, 2023.</p>
<p>[15] Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, M. Huang, N. Duan, and W. Chen. Tora: A tool-integrated reasoning agent for mathematical problem solving, 2023.
[16] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021.
[17] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.
[18] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. Teaching machines to read and comprehend. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.
[19] Z. Hou, Y. Niu, Z. Du, X. Zhang, X. Liu, A. Zeng, Q. Zheng, M. Huang, H. Wang, J. Tang, and Y. Dong. Chatglm-rlhf: Practices of aligning large language models with human feedback, 2024.
[20] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
[21] P. Ke, B. Wen, Z. Feng, X. Liu, X. Lei, J. Cheng, S. Wang, A. Zeng, Y. Dong, H. Wang, et al. Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation. arXiv preprint arXiv:2311.18702, 2023.
[22] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, and H. Hajishirzi. Mawps: A math word problem repository. In Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, pages 1152-1157, 2016.
[23] V. Kumar, R. Maheshwary, and V. Pudi. Adversarial examples for evaluating math word problem solvers. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2705-2712, 2021.
[24] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466, 2019.
[25] H. Lee, S. Phatale, H. Mansoor, T. Mesnard, J. Ferret, K. Lu, C. Bishop, E. Hall, V. Carbune, A. Rastogi, and S. Prakash. Rlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023.
[26] Y. Li, Y. Zhang, Z. Zhao, L. Shen, W. Liu, W. Mao, and H. Zhang. CSL: A large-scale Chinese scientific literature dataset. In Proceedings of the 29th International Conference on Computational Linguistics, pages 3917-3923. International Committee on Computational Linguistics, Oct. 2022.
[27] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Let's verify step by step, 2023.
[28] W. Ling, D. Yogatama, C. Dyer, and P. Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages $158-167,2017$.
[29] X. Liu, X. Lei, S. Wang, Y. Huang, Z. Feng, B. Wen, J. Cheng, P. Ke, Y. Xu, W. L. Tam, et al. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint arXiv:2311.18743, 2023.
[30] X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023.</p>
<p>[31] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.
[32] S. Mishra, A. Mitra, N. Varshney, B. Sachdeva, P. Clark, C. Baral, and A. Kalyan. Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3505-3523, 2022.
[33] S. Narayan, S. B. Cohen, and M. Lapata. Don't give me the details, just the summary! Topicaware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, 2018.
[34] OpenAI. Gpt-4 technical report, 2023.
[35] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.
[36] K. Paster. Testing language models on a held-out high school national finals exam. https: //huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam, 2023.
[37] S. Polu and I. Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020.
[38] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly a reward model, 2023.
[39] D. Saxton, E. Grefenstette, F. Hill, and P. Kohli. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019.
[40] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.
[41] Y. Shao, L. Li, J. Dai, and X. Qiu. Character-llm: A trainable agent for role-playing, 2023.
[42] M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.
[43] I. Team. Internlm: A multilingual language model with progressively enhanced capabilities. https://github.com/InternLM/InternLM, 2023.
[44] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[45] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.
[46] Q. Tu, S. Fan, Z. Tian, and R. Yan. Charactereval: A chinese benchmark for role-playing conversational agent evaluation, 2024.
[47] M. Völske, M. Potthast, S. Syed, and B. Stein. TL;DR: Mining Reddit to learn automatic summarization. In L. Wang, J. C. K. Cheung, G. Carenini, and F. Liu, editors, Proceedings of the Workshop on New Frontiers in Summarization, pages 59-63, Copenhagen, Denmark, Sept. 2017. Association for Computational Linguistics.</p>
<p>[48] P. Wang, L. Li, Z. Shao, R. X. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2023.
[49] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models, 2023.
[50] Y. Wang, X. Liu, and S. Shi. Deep neural solver for math word problems. In M. Palmer, R. Hwa, and S. Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 845-854, Copenhagen, Denmark, Sept. 2017. Association for Computational Linguistics.
[51] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.
[52] T. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese elementary school math test?, 2023.
[53] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.
[54] L. Yang, H. Yang, W. Cheng, L. Lin, C. Li, Y. Chen, L. Liu, J. Pan, T. Wei, B. Li, L. Zhao, L. Wang, B. Zhu, G. Li, X. Wu, X. Luo, and R. Hu. Skymath: Technical report, 2023.
[55] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.
[56] Yi. A series of large language models trained from scratch by developers at 01-ai. https: //github.com/01-ai/Yi, 2023.
[57] L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023.
[58] Z. Yuan, H. Yuan, C. Li, G. Dong, K. Lu, C. Tan, C. Zhou, and J. Zhou. Scaling relationship on learning mathematical reasoning with large language models, 2023.
[59] Z. Yuan, H. Yuan, C. Tan, W. Wang, and S. Huang. How well do large language models perform in arithmetic tasks?, 2023.
[60] X. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.
[61] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.
[62] D. Zhang, Z. Hu, S. Zhoubian, Z. Du, K. Yang, Z. Wang, Y. Yue, Y. Dong, and J. Tang. Sciglm: Training scientific language models with self-reflective instruction annotation and tuning, 2024.
[63] X. Zhang, C. Li, Y. Zong, Z. Ying, L. He, and X. Qiu. Evaluating the performance of large language models on gaokao benchmark. 2023.
[64] W. Zhao, M. Shang, Y. Liu, L. Wang, and J. Liu. Ape210k: A large-scale and template-rich dataset of math word problems, 2020.
[65] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
[66] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. Agieval: A human-centric benchmark for evaluating foundation models, 2023.</p>
<p>[67] J. Zhou, Z. Chen, D. Wan, B. Wen, Y. Song, J. Yu, Y. Huang, L. Peng, J. Yang, X. Xiao, et al. Characterglm: Customizing chinese conversational ai characters with large language models. arXiv preprint arXiv:2311.16832, 2023.
[68] Z. Zhou, Q. Wang, M. Jin, J. Yao, J. Ye, W. Liu, W. Wang, X. Huang, and K. Huang. Mathattack: Attacking large language models towards math solving ability. arXiv preprint arXiv:2309.01686, 2023.</p>
<h1>A Case Study</h1>
<h2>A. 1 Case Study of Math-Critique</h2>
<p>We have provided several examples of scoring by Math-Critique. In the examples from Table 6 and Table 7, we observed that the model provided correct answers. However, the answer formats differed from the standard answers, specifically regarding fraction expression and the selection of unknown variables. These differences are equivalent, yet traditional evaluation methods struggle to judge them accurately. However, Math-Critique correctly scored these two examples and provided reasonable evaluations.</p>
<p>In the example from Table 8, the model made a mistake in the calculation process. Math-Critique accurately pinpointed the error location, and since the model correctly solved a part of the problem, Math-Critique awarded a score of 3 points.</p>
<h2>A. 2 Case Study of Mathematical Models</h2>
<p>Here are a few comparisons between ChatGLM3-32B-Math(ChatGLM3-32B-SFT-2312 + RFT\&amp;DPO) and other models. In the example from Table 9, the problem is a math question of Chinese junior high school difficulty. During the solution process by GPT-4-0613, an error occurred in solving the equation. ChatGLM3-32B-SFT-2312 did not correctly understand the question. ChatGLM3-32B-Math correctly listed the equation and accurately solved it using the factorization method.
In the example from Table 10, both GPT-4-0613 and ChatGLM3-32B-Math provided the correct answers, but the difference lies in that ChatGLM3-32B-Math offered a very detailed derivation process. We believe that detailed derivation aids in understanding for users and helps prevent errors that may occur during the model's step-skipping.
In the example from Table 11, originating from the Hungry Test, ChatGLM3-32B-Math correctly conducted the analysis and provided the solution. In contrast, Qwen-Max, despite being accurate in most processes, made a simplification error in the expressions for S6 and S7, leading to an incorrect result despite precise calculations.</p>
<h2>A. 3 Case Study of Errors</h2>
<p>In the first error example shown in Table 12, ChatGLM3-32B-Math correctly listed the matrix equations but made a mistake in transferring terms during the complex calculation process. This led to an incorrect solution despite the overall process being correct.
In the second example shown in Table 13, the question includes an image. This question originates from MATH, and although the image is provided in Asymptote code, the language model still struggles to understand the meaning of the image. This demonstrates a deficiency in our model's ability to process images. In fact, within the MATH test set, for questions that include images, our model's accuracy rate is only $23 \%$, significantly lower than the $40 \%$ accuracy rate across the entire dataset.</p>
<p>In the third example shown in Table 14, we can observe certain deficiencies in the computational accuracy of the language model. In the first column of the table, the model correctly calculates the multiplication of integers and a single decimal. Still, in the second column, when calculating $424 * 1.06$, the model incurred an error of approximately $0.2 \%$. This error led to subsequent calculations being incorrect due to the accumulation of errors.</p>
<p>Table 6: Math-Critique Example (1)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question</th>
<th style="text-align: left;">$1 / x+x$ 三次方的原函数是多少。</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Question <br> (Translate)</td>
<td style="text-align: left;">What is the original function of $1 / x+x$ cubed.</td>
</tr>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: left;">首先，我们可以将 $1 / x+x^{3}$ 重新写成 $x^{-1}+x^{3}$ ，然后利用幂函数和多项式函数 <br> 的求导原理求出它的原函数。具体来说，我们得到：</td>
</tr>
</tbody>
</table>
<p>$$
\int\left(x^{-1}+x^{3}\right) \mathrm{d} x=\int x^{-1} \mathrm{~d} x+\int x^{3} \mathrm{~d} x=\ln |x|+\frac{1}{4} x^{4}+C
$$</p>
<p>其中 $C$ 是不定积分常数。因此， $1 / x+x^{3}$ 的原函数是 $\ln |x|+\frac{1}{4} x^{4}+C$ 。
Reference First, we can rewrite $1 / x+x^{3}$ as $x^{-1}+x^{3}$, and then use the derivation principles of (Translate) power functions and polynomial functions to find its original function. Specifically, we get:</p>
<p>$$
\int\left(x^{-1}+x^{3}\right) \mathrm{d} x=\int x^{-1} \mathrm{~d} x+\int x^{3} \mathrm{~d} x=\ln |x|+\frac{1}{4} x^{4}+C
$$</p>
<p>where $C$ is the indefinite integral constant. Therefore, the original function of $1 / x+x^{3}$ is $\ln |x|+\frac{1}{4} x^{4}+C$.</p>
<p>Model Answer by GLM4</p>
<p>要求函数 $f(x)=\frac{1}{x}+x^{3}$ 的原函数（不定积分），我们需要分别对这两项进行积分。对于 $\frac{1}{x}$ ，其原函数是 $\ln |x|$ （自然对数）。对于 $x^{3}$ ，我们可以利用幂函数的积分规则，即对于 $x^{n}$ （其中 $n e q-1$ ），其原函数是 $\frac{x^{n+1}}{n+1}$ 。因此， $x^{3}$ 的原函数是 $\frac{x^{4}}{4}$ 。将这两项的原函数相加，我们得到 $f(x)$ 的原函数：</p>
<p>$$
\int\left(\frac{1}{x}+x^{3}\right) d x=\ln |x|+\frac{x^{4}}{4}+C
$$</p>
<p>其中 $C$ 是积分常数。这个结果表示在定义域内（除了 $x=0$ 的情况，因为 $\frac{1}{x}$ 在 $x=0$ 处未定义）， $f(x)$ 的一个原函数。由于 $\ln |x|$ 在 $x=0$ 处未定义，所以这个原函数在 $x=0$ 处也是不连续的。</p>
<p>Model
Answer
(Translate)</p>
<p>To find the primitive function (indefinite integral) of the function $f(x)=\frac{1}{x}+x^{3}$, we need to integrate these two terms separately. For $\frac{1}{x}$, the original function is $\ln |x|$ (natural logarithm). For $x^{3}$, we can use the integration rule of the power function, that is, for $x^{n}$ (where $n e q-1$ ), the original function is $\frac{x^{n+1}}{n+1}$. Therefore, the original function of $x^{3}$ is $\frac{x^{4}}{4}$. Adding the original functions of these two terms, we get the original function of $f(x)$ :</p>
<p>$$
\int\left(\frac{1}{x}+x^{3}\right) d x=\ln |x|+\frac{x^{4}}{4}+C
$$</p>
<p>where $C$ is the integration constant. This result means that within the domain (except for the case of $x=0$, since $\frac{1}{x}$ is undefined at $x=0$ ), $f(x)$ Aprimitive functionof. Since $\ln |x|$ is undefined at $x=0$, the original function is also discontinuous at $x=0$.</p>
<p>Critique Result</p>
<p>AIB 步的回答非常准确和详细，它正确地解释了如何求解给定函数的原函数。AI助手首先分别对函数的两部分进行积分，然后将它们的结果相加，得到了原函数。此外，AI助手还注意到了在x=0处，原函数是不连续的，这是一个重要的观察。总的来说，AI助手的回答与参考答案非常接近，没有明显的缺点。因此，我会给它一个高分。评分：[[9]</p>
<p>Critique Result (Translate)</p>
<p>The AI assistant's answer is very accurate and detailed, and it correctly explains how to solve the original function of a given function. The AI assistant first integrates the two parts of the function separately and then adds their results to obtain the original function. In addition, the AI assistant also noticed that at $x=0$, the original function is discontinuous, which is an important observation. Overall, the AI assistant's answers are very close to the reference answers, with no obvious shortcomings. Therefore, I would give it a high score. Rating:[[9]</p>
<p>Table 7: Math-Critique Example (2)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">三阶矩阵 A 有三个特征值 $0,3,5$ ，对应的特征向量分别为 $\mathrm{u}, \mathrm{v}, \mathrm{w}$ ，则线性方程组 $\mathrm{Ax}=\mathrm{v}+\mathrm{w}$ 的通 <br> 解为?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Question <br> (Translate)</td>
<td style="text-align: center;">The third-order matrix A has three eigenvalues 0,3 , and 5 , and the corresponding eigenvectors are u , v , and w respectively. What is the general solution to the linear equation system $\mathrm{Ax}=\mathrm{v}+\mathrm{w}$ ?</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">给定矩阵 A 有特征值 $0,3,5$ ，对应的特征向量为 $\mathrm{u}, \mathrm{v}, \mathrm{w}$ 。线性方程组 $\mathrm{Ax}=\mathrm{v}+\mathrm{w}$ 。我们可以 <br> 写作 $\mathrm{Au}=0 * \mathrm{u}, \mathrm{As}=3 * \mathrm{v}, \mathrm{和Aw}=5 * \mathrm{w}$ 。所以 $\mathrm{A}\left(1 / 3^{<em>} \mathrm{v}\right)=\mathrm{v}, \mathrm{A}\left(1 / 5^{</em>} \mathrm{w}\right)=\mathrm{w}$ 。左右两边相加 <br> 得 $\mathrm{A}\left(1 / 3^{<em>} \mathrm{v}+1 / 5^{</em>} \mathrm{w}\right)=\mathrm{v}+\mathrm{w}$ 。又因为 $\mathrm{Ax}=\mathrm{v}+\mathrm{w}$ 。所以 $1 / 3 * \mathrm{v}+1 / 5 * \mathrm{w}$ 是 $\mathrm{Ax}=\mathrm{v}+\mathrm{w}$ 的一个特 <br> 解。因为 $\mathrm{Au}=0$ 。所以 $u$ 是 $\mathrm{Au}=0$ 的基础解系。因此方程 $\mathrm{Ax}=\mathrm{v}+\mathrm{w}$ 的通解是 $\mathrm{x}=\mathrm{ku}+1 / 3 * \mathrm{v}+$ <br> $1 / 5 * \mathrm{w}$ 。</td>
</tr>
<tr>
<td style="text-align: center;">Reference <br> (Translate)</td>
<td style="text-align: center;">A given matrix A has eigenvalues 0,3 , and 5 , and the corresponding eigenvectors are $\mathrm{u}, \mathrm{v}$, and w . <br> For the linear system of equations $\mathrm{Ax}=\mathrm{v}+\mathrm{w}$, we can write $\mathrm{Au}=0 * \mathrm{u}, \mathrm{Av}=3 * \mathrm{v}$, and $\mathrm{Aw}=5 <em>$ <br> w. So $\mathrm{A}\left(1 / 3^{</em>} \mathrm{v}\right)=\mathrm{v}, \mathrm{A}\left(1 / 5^{<em>} \mathrm{w}\right)=\mathrm{w}$, adding the left and right sides gives $\mathrm{A}\left(1 / 3^{</em>} \mathrm{v}+1 / 5^{*} \mathrm{w}\right)=\mathrm{v}+\mathrm{w}$. And <br> because $\mathrm{Ax}=\mathrm{v}+\mathrm{w}, 1 / 3 * \mathrm{v}+1 / 5 * \mathrm{w}$ is a special solution of $\mathrm{Ax}=\mathrm{v}+\mathrm{w}$. Because $\mathrm{Au}=0$, so u is the <br> basic solution system of $\mathrm{Au}=0$, so the equation Ax The general solution of $=\mathrm{v}+\mathrm{w}$ is $\mathrm{x}=\mathrm{ku}+1 / 3 * \mathrm{v}$ <br> $+1 / 5 * \mathrm{w}$.</td>
</tr>
<tr>
<td style="text-align: center;">Model <br> Answer by <br> ChatGLM3- <br> 32B-Math</td>
<td style="text-align: center;">给定三阶矩阵 $A$ 的三个特征值分别为 $0,3,5$ ，对应的特征向量分别为 $u, v, w$ 。要找线性 <br> 方程组 $A x=v+w$ 的通解。我们可以利用特征值和特征向量的性质。首先，我们知道 <br> $A v=3 v$ 和 $A w=5 w$ 。因此， $v$ 和 $w$ 分别是特征值 3 和 5 对应的特征向量。线性方程组 <br> $A x=v+w$ 可以重写为 $A x-v-w=0$ 。我们可以将 $x$ 写成 $x=p u+q v+r w$ 。其中 <br> $p, q, r$ 是待定系数。将 $x$ 代入方程 $A x-v-w=0$ 。我们得到:</td>
</tr>
</tbody>
</table>
<p>$$
A(p u+q v+r w)-v-w=0
$$</p>
<p>... 由于 $v$ 和 $w$ 是线性无关的特征向量（因为它们对应不同的特征值），上式成立的唯一可 <br> 能是 $3 q-1=0$ 和 $5 r-1=0$ 。解这两个方程，我们得到 $q=\frac{1}{3}$ 和 $r=\frac{1}{5}$ 。因此，方程 <br> $A x=v+w$ 的通解为:</p>
<p>$$
x=p u+\frac{1}{3} v+\frac{1}{5} w
$$</p>
<p>其中 $p$ 是任意实数。所以，通解的形式是：</p>
<p>$$
x=p \mathbf{u}+\frac{1}{3} \mathbf{v}+\frac{1}{5} \mathbf{w}
$$</p>
<p>这里的 u 是特征值 0 对应的特征向量， v 和 w 分别是特征值 3 和 5 对应的特征向量。 $p$ 是 <br> 任意实数，可以取任何值。 |
| Model <br> Answer <br> (Translate) | Given the three eigenvalues of the third-order matrix $A$ are $0,3,5$, the corresponding eigenvectors <br> are $u, v, w$. To find the general solution to a system of linear equations $A x=v+w$, we can exploit <br> the properties of eigenvalues and eigenvectors. First of all, we know $A v=3 v$ and $A w=5 w$. <br> Therefore, $v$ and $w$ are the eigenvectors corresponding to the eigenvalues 3 and 5 respectively. The <br> system of linear equations $A x=v+w$ can be rewritten as $A x-v-w=0$. We can write $x$ <br> as $x=p u+q v+r w$, where $p, q, r$ are the undetermined coefficients. Substituting $x$ into the <br> equation $A x-v-w=0$, we get: <br> $A(p u+q v+r w)-v-w=0$ |
|  | ... Therefore, the general solution of equation $A x=v+w$ is:</p>
<p>$$
x=p u+\frac{1}{3} v+\frac{1}{5} w
$$</p>
<p>Where $p$ is any real number. So, the general solution is in the form:</p>
<p>$$
x=p \mathbf{u}+\frac{1}{3} \mathbf{v}+\frac{1}{5} \mathbf{w}
$$</p>
<p>Hereu is the eigenvector corresponding to the eigenvalue $0, \mathbf{v}$ and $\mathbf{w}$ are the eigenvectors correspond- <br> ing to the eigenvalues 3 and 5 respectively. $p$ is any real number and can take on any value. |
| Critique <br> Result | AI助手的回答非常详细，它首先解释了如何利用特征值和特征向量的性质来解决这个问 <br> 题，然后逐步进行了详细的步骤，包括将方程重写，将x写成待定系数的形式，然后将这些 <br> 系数代入方程，最后解出这些系数。这个答案不仅准确，而且深入，提供了详细的解释和 <br> 步骤，使得读者能够理解如何解决这个问题。与参考答案相比，AI助手的答案更加详细和 <br> 深入，因此我认为AI助手的回答质量非常高。评分：[[9]]。 |
| Critique <br> Result <br> (Translate) | The AI assistant's answer is very detailed. It first explains how to use the properties of eigenvalues <br> and eigenvectors to solve this problem, and then goes through the detailed steps, including rewriting <br> the equation, writing x in the form of undetermined coefficients, and then converting these The <br> coefficients are substituted into the equation and finally the coefficients are solved. This answer is not <br> only accurate but also in-depth, providing detailed explanations and steps so the reader can understand <br> how to solve the problem. Compared with the reference answers, the AI assistant's answers are more <br> detailed and in-depth, so I think the quality of the AI assistant's answers is very high. Rating: [[9]]. |</p>
<p>Table 8: Math-Critique Example (3)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">二元一次方程 $4 x-3 y=12, \exists x=0,1,2,3$ 时, $y=$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Question <br> (Translate)</td>
<td style="text-align: center;">Quadratic equation $4 x-3 y=12$, when $x=0,1,2,3, y=$</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">将二元一次方程 $4 x-3 y=12$ 改写为 $y$ 的表达式： $4 x-3 y=12-3 y=-4 x+12 y=$ $(4 / 3) x-4$ 当 $x=0$ 时， $y=(-4)$ ；当 $x=1$ 时， $y=(4 / 3)-4=-8 / 3$ ；当 $x=2$ 时， $y=(8 / 3)-4=-$ $4 / 3$ ；当 $x=3$ 时， $y=(4)-4=0$ 。因此，当 $x$ 分别为 $0 、 1 、 2 、 3$ 时， $y$ 的值分别为-4 -$8 / 3,-4 / 3$ 和 0 。</td>
</tr>
<tr>
<td style="text-align: center;">Reference <br> (Translate)</td>
<td style="text-align: center;">Rewrite the linear equation $4 x-3 y=12$ into the expression of $y$ : $4 x-3 y=12-3 y=-4 x+$ $12 y=(4 / 3) x-4$ When $x=$ When $0, y=(-4)$; When $x=1, y=(4 / 3)-4=-8 / 3$; When $x=2$, $y=(8 / 3)-4=-4 / 3$; When $x=3, y=(4)-4=0$. Therefore, when $x$ is $0,1,2$, and 3 respectively, the values of $y$ are $-4,-8 / 3,-4 / 3$, and 0 respectively.</td>
</tr>
</tbody>
</table>
<p>Model Answer (Translate) * 给定二元一次方程: $4 x-3 y=12 <em>$ 将 $x$ 替换为给定的数值, 可以得到 $y$ 的值: * $x=0$代入方程,得到: $-3 y=12, y=-4 * x=1$ 代入方程,得到: $4-3 y=12,-3 y=8, y=-2 </em>$ $x=2$ 代入方程,得到: $8-3 y=12,-3 y=4, y=0 * x=3$ 代入方程,得到: $12-3 y=12$, $-3 y=0, y=2 *$ 综上,当 $x=0,1,2,3$ 时, $y$ 依次等于 $-4,-2,0,2$ 。所以应该填写: $-4,-2,0,2$ 。</p>
<p>Model
Answer
(Translate)</p>
<ul>
<li>Given a linear equation of two variables: $4 x-3 y=12 <em>$ Replace $x$ with the given value, you can get the value of $y: * x=0$ Substitute into the equation, you get: $-3 y=12$, $y=-4 * x=1$ Substituting into the equation, we get: $4-3 y=12,-3 y=8, y=-2 * x=2$ Substituting into the equation, we get: $8-3 y=12,-3 y=4, y=0 * x=3$ Substituting into the equation, we get: $12-3 y=12,-3 y=0, y=2 </em>$ In summary, when $x=0,1,2,3, y$ is equal to -4 in turn ,-2,0,2. So you should fill in:-4,-2,0,2.</li>
</ul>
<p>Critique Result</p>
<p>Critique Re sult</p>
<p>Critique Re sult (Translate)</p>
<p>AI助手的回答在计算过程中出现了错误。当 $x=1$ 时，AI助手错误地计算出 $y=-2$ ，而实际上应该是 $y=-8 / 3$ 。同样，当 $x=3$ 时，AI助手错误地计算出 $y=2$ ，而实际上应该是 $y=0$ 。因此，AI助手的回答在准确性和深度上都存在问题。然而，AI助手的回答在结构和详细程度上是正确的，它详细地解释了如何计算 $y$ 的值，并给出了每个 $x$ 值对应的 $y$ 值。总的来说，AI助手的回答虽然有一些错误，但是仍然提供了一些有用的信息。因此，我会给AI助手的回答打分为3。评分：[[3]]</p>
<p>There was an error in the calculation process of the AI assistant's answer. When $x=1$, the AI assistant incorrectly calculates $y=-2$, when it should actually be $y=-8 / 3$. Likewise, when $x=3$, the AI assistant incorrectly calculates $y=2$, when it should actually be $y=0$. Therefore, the AI assistant's answers have problems with accuracy and depth. However, the AI assistant's answer is correct in structure and level of detail. It explains in detail how to calculate the value of $y$ and gives the corresponding $y$ value for each $x$ value. Overall, although the AI assistant's answers contained some errors, they still provided some useful information. Therefore, I would rate the AI assistant's answer a 3. Rating: $[[3]]$</p>
<p>Table 9: ChatGLM3-32B-Math Example (1)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">已知一个两位数的十位数字比个位数字大 2。两位数字的积比这个两位数小34。求这个两位数。</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Question <br> (Trans- <br> late)</td>
<td style="text-align: center;">Given a two-digit number where the tens digit is 2 greater than the units digit, and the product of the digits is 34 less than the number itself, find this two-digit number.</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">设这个两位数为 $10 \mathrm{x}+\mathrm{y}$ （ x 表示十位数， y 表示个位数），根据题意可得：（1） $\mathrm{x}=\mathrm{y}+2$ （十位数比个位数大2）（2） $x y=10 x+y-34$ （两位数字的积比这个两位数小34）将（1）式代入（2）式得到： $(y+2) y=10(y+2)+y-34$ 整理得到 $y^{2}+2 y=11 y+20-34$ 。整理得 $y^{2}-9 y+14=0$ 。解方程得到 $y=7$ 或 $y=2$ 。所以 $x=7+2=9$ 或 $x=2+2=4$ 所以这个数为 97 或者 42 答案: 97 或 42 。</td>
</tr>
<tr>
<td style="text-align: center;">Reference <br> (Translate)</td>
<td style="text-align: center;">Let the two-digit number be $10 x+y$ (where $x$ is the tens digit and $y$ is the units digit). According to the problem: (1) $x=y+2$ (the tens digit is 2 greater than the units digit) (2) $x y=10 x+y-34$ (the product of the digits is 34 less than the number itself). Substituting (1) into (2) gives: $(y+2) y=$ $10(y+2)+y-34$, which simplifies to $y^{2}+2 y=11 y+20-34$, and further simplifies to $y^{2}-9 y+14=0$. Solving the equation yields $y=7$ or $y=2$. Thus, $x=7+2=9$ or $x=2+2=4$, making the number 97 or 42 . Answer: 97 or 42 .</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4- <br> 0613</td>
<td style="text-align: center;">设这个两位数为 $10 \mathrm{x}+\mathrm{y}$ ，其中 x 为十位数， y 为个位数。根据题意，我们有以下两个方程： x $=y+2$ （十位数字比个位数字大2） $x y=10 x+y-34$ （两位数字的积比这个两位数小34）将第一个方程代入第二个方程，我们得到： $(y+2) y=10(y+2)+y-34$ 解这个方程，我们得到 $y$ $=4$ 。将 $y=4$ 代入第一个方程，我们得到 $x=6$ 。所以，这个两位数是 64 。</td>
</tr>
<tr>
<td style="text-align: center;">GPT- <br> 4-0613 <br> (Translate)</td>
<td style="text-align: center;">Let the two-digit number be $10 x+y$, where $x$ is the tens digit and $y$ is the units digit. According to the problem, we have two equations: $x=y+2$ (the tens digit is 2 greater than the units digit) and $x y=10 x+y-34$ (the product of the digits is 34 less than the number itself). Substituting the first equation into the second, we get: $(y+2) y=10(y+2)+y-34$. Solving this equation yields $y=4$. Substituting $y=4$ back into the first equation gives $x=6$. Therefore, the two-digit number is 64 .</td>
</tr>
<tr>
<td style="text-align: center;">ChatGLM3- <br> 32B-SFT</td>
<td style="text-align: center;">设这个两位数为 $10 a+b$ ，其中 $a$ 是十位数字， $b$ 是个位数字。根据题意，我们有以下两个条件：1.十位数字比个位数字大 2 。即 $a=b+2$ 2. 两位数字的积比这个两位数小 34 。即 $a b&lt;10 a+b-34$ 现在我们来解这个两个方程。从第一个条件，我们得到 $a=b+2$ 。将 $a$ 替换进第二个条件中，我们得到： $b(b+2)&lt;10(b+2)-34$ 展开并简化这个不等式： $b^{2}+2 b&lt;10 b+20-34 b^{2}+2 b&lt;7 b-14$ 移项得到： $b^{2}-5 b+14&lt;0$ 这是一个一元二次不等式，我们可以来解这个不等式得到 $b$ 的范围。因式分解得到： $(b-1)(b-14)&lt;0$根据一元二次不等式的性质，我们知道 $b$ 应该在两个根之间，即： $1&lt;b&lt;14$ 但是由于 $a=b+2$ ，且 $a$ 是一个十位数（即 $1 \leq a \leq 9$ ），所以 $b$ 必须小于 9 。因此 $b$ 的可能取值为 $2,3,4,5,6,7,8$ 。现在我们来检查 $b$ 的每一个值，并找出对应的 $a$ 和两位数。当 $b=2$时， $a=0$ 。这不满足 $a$ 为十位数的要求；当 $b=3$ 时， $a=1$ 。这也不满足 $a$ 为十位数的要求； $\ldots$ 当 $b=7$ 时， $a=5$ 。这满足要求，并且 $a$ 和 $b$ 的积是 35 。确实比 35 小 34 。满足题目的要求。所以这个两位数是 57 。</td>
</tr>
<tr>
<td style="text-align: center;">ChatGLM3- <br> 32B-SFT <br> (Translate)</td>
<td style="text-align: center;">Let the two-digit number be $10 a+b$, where $a$ is the tens digit and $b$ is the units digit. According to the problem, we have two conditions: 1 . The tens digit is 2 greater than the units digit, i.e., $a=b+2$ 2. The product of the digits is 34 less than the number itself, i.e., $a b&lt;10 a+b-34$ Now let's solve these two equations. From the first condition, we get $a=b+2$. Substituting $a$ into the second condition, we get: $b(b+2)&lt;10(b+2)-34$ Expanding and simplifying this inequality gives: $b^{2}+2 b&lt;10 b+20-34 b^{2}+2 b&lt;7 b-14$ Rearranging gives: $b^{2}-5 b+14&lt;0$ This is a quadratic inequality, which we can solve to find the range for $b$. Factoring gives: $(b-1)(b-14)&lt;0$ According to the properties of quadratic inequalities, we know $b$ should be between the two roots, i.e., $1&lt;b&lt;14$. However, since $a=b+2$ and $a$ is a tens digit $(1 \leq a \leq 9), b$ must be less than 9. Thus, the possible values for $b$ are $2,3,4,5,6,7,8$. Now we check each value of $b$ to find the corresponding $a$ and the two-digit number. When $b=2, a=0$, which does not meet the requirement for $a$ to be a tens digit; when $b=3, a=1$, which also does not meet the requirement; $\ldots$ when $b=7, a=5$, which meets the requirement, and the product of $a$ and $b$ is 35 , indeed less than 35 by 34 , meeting the problem's requirement. Therefore, the two-digit number is 57 .</td>
</tr>
<tr>
<td style="text-align: center;">ChatGLM3- <br> 32B-Math</td>
<td style="text-align: center;">设这个两位数为 $10 a+b$ ，其中 $a$ 是十位数字， $b$ 是个位数字。根据题目条件，我们有以下两个方程：1.十位数字比个位数字大 2 。即 $a=b+2$ 。2. 两位数字的积比这个两位数小 34 。即 $a b=(10 a+b)-34$ 。我们可以用第一个方程代入第二个方程来解这个问题。将 $a=b+2$代入第二个方程，得到： $b(b+2)=10(b+2)+b-34$ 顺开并简化： $b^{2}+2 b=10 b+20+b-34$ $b^{2}+2 b=11 b-14$ 移项得到一个二次方程： $b^{2}-9 b+14=0$ 这是一个二次方程，我们可以通过分解因式来解它： $(b-7)(b-2)=0$ 所以， $b$ 的值可以是 7 或者 2 。如果 $b=7$ ，则 $a=b+2=9$ 。所以这个两位数是 97 。如果 $b=2$ ，则 $a=b+2=4$ 。所以这个两位数是 42 。因此，这个两位数是 42 或 97 。</td>
</tr>
<tr>
<td style="text-align: center;">ChatGLM3- <br> 32B-Math <br> (Translate)</td>
<td style="text-align: center;">Let the two-digit number be $10 a+b$, where $a$ is the tens digit and $b$ is the units digit. According to the problem conditions, we have two equations: 1 . The tens digit is 2 greater than the units digit, i.e., $a=b+2$. 2. The product of the digits is 34 less than the number itself, i.e., $a b=(10 a+b)-34$. We can solve this problem by substituting the first equation into the second. Doing so gives: $b(b+2)=10(b+2)+b-34$ Expanding and simplifying yields: $b^{2}+2 b=10 b+20+b-34$ $b^{2}+2 b=11 b-14$ Rearranging into a quadratic equation gives: $b^{2}-9 b+14=0$ This quadratic equation can be solved by factoring: $(\operatorname{det}(17)(b-2)=0$ Thus, $b$ can be either 7 or 2 . If $b=7$, then $a=b+2=9$, making the two-digit number 97 . If $b=2$, then $a=b+2=4$, making the two-digit number 42. Therefore, the two-digit number could be either 42 or 97 .</td>
</tr>
</tbody>
</table>
<p>Table 10: ChatGLM3-32B-Math Example (2)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">两个相互独立的事件A，B都不发生的概率为 $1 / 9$ ，A发生而B不发生的概率与A不发生B发生的概率相等，则 $\mathrm{P}(\mathrm{A})$ 等于?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Question <br> (Translate)</td>
<td style="text-align: center;">Given two independent events A and B, where the probability of neither A nor B occurring is $1 / 9$, and the probability of A occurring without B equals the probability of B occurring without A , what is $\mathrm{P}(\mathrm{A})$ ?</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">由题意的: $P(A \bar{B})=P(B \bar{A})$, 即 $\mathrm{P}(\mathrm{A})(1-\mathrm{P}(\mathrm{B}))=\mathrm{P}(\mathrm{B})(1-\mathrm{P}(\mathrm{A}))$ 所以 $\mathrm{P}(\mathrm{A})=\mathrm{P}(\mathrm{B}) \mathrm{又} P(\bar{A} \bar{B})=\frac{1}{9}$ 所以 $P(\bar{A})=\frac{1}{2}$ 所以 $P(A)=\frac{2}{2}$</td>
</tr>
<tr>
<td style="text-align: center;">Reference <br> (Trans- <br> late)</td>
<td style="text-align: center;">From the problem statement: $\mathrm{P}(\mathrm{A}$ and not B$)=\mathrm{P}(\mathrm{B}$ and not A$)$, i.e., $\mathrm{P}(\mathrm{A})(1-\mathrm{P}(\mathrm{B}))=\mathrm{P}(\mathrm{B})(1-\mathrm{P}(\mathrm{A}))$. Thus, $\mathrm{P}(\mathrm{A})=\mathrm{P}(\mathrm{B})$. Also, $\mathrm{P}($ neither A nor B$)=1 / 9$. Therefore, $\mathrm{P}($ not A$)=1 / 3$, leading to $\mathrm{P}(\mathrm{A})=2 / 3$.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4- <br> 0613</td>
<td style="text-align: center;">设A发生的概率为p，B发生的概率为q，则A，B都不发生的概率为 $(1-\mathrm{p})(\mathrm{l}-\mathrm{q})=1 / 9$ ，A发生而B不发生的概率为 $\mathrm{p}(1-\mathrm{q})$, A不发生B发生的概率为 $(1-\mathrm{p}) \mathrm{q}$ 。由题意知， $\mathrm{p}(1-\mathrm{q})=(1-\mathrm{p}) \mathrm{q}$ ，解得 $\mathrm{p}=\mathrm{q}$ ，将 $\mathrm{p}=\mathrm{q}$ 代入 $(1-\mathrm{p})(1-\mathrm{q})=1 / 9$ ，得 $(1-p)^{2}=1 / 9$ ，解得 $\mathrm{p}=2 / 3$ 或 $\mathrm{p}=-1 / 3$ 。由于概率不能为负，所以 $\mathrm{p}=2 / 3$ ，即 $\mathrm{P}(\mathrm{A})=2 / 3$ 。</td>
</tr>
<tr>
<td style="text-align: center;">GPT- <br> 4-0613 <br> (Trans- <br> late)</td>
<td style="text-align: center;">Let the probability of A occurring be p , and the probability of B occurring be q . Then, the probability of neither A nor B occurring is $(1-\mathrm{p})(\mathrm{l}-\mathrm{q})=1 / 9$. The probability of A occurring without B is $\mathrm{p}(1-\mathrm{q})$, and the probability of B occurring without A is ( $1-\mathrm{p}) \mathrm{q}$. From the problem, $\mathrm{p}(1-\mathrm{q})=(1-\mathrm{p}) \mathrm{q}$, leading to $\mathrm{p}=\mathrm{q}$. Substituting $\mathrm{p}=\mathrm{q}$ into $(1-\mathrm{p})(\mathrm{l}-\mathrm{q})=1 / 9$ gives $(1-p)^{2}=1 / 9$, resulting in $\mathrm{p}=2 / 3$ or $\mathrm{p}=-1 / 3$. Since probability cannot be negative, $\mathrm{p}=2 / 3$, i.e., $\mathrm{P}(\mathrm{A})=2 / 3$.</td>
</tr>
<tr>
<td style="text-align: center;">ChatGLM3- <br> 32B-SFT</td>
<td style="text-align: center;">设事件A发生的概率为 $\mathrm{P}(\mathrm{A})$ ，事件B发生的概率为 $\mathrm{P}(\mathrm{B})$ 。已知A和B相互独立，那么事件A不发生而B发生的概率为 $\mathrm{P}(\mathrm{B})(1-\mathrm{P}(\mathrm{A}))$ ，事件A发生而B不发生的概率为 $\mathrm{P}(\mathrm{A})(1-\mathrm{P}(\mathrm{B}))$ 。根据题目条件，A发生而B不发生的概率与A不发生B发生的概率相等，即： $\mathrm{P}(\mathrm{A})(1-\mathrm{P}(\mathrm{B}))=\mathrm{P}(\mathrm{B})(1-$ $\mathrm{P}(\mathrm{A})$ ) 由于A和B相互独立，我们可以将上式简化为： $\mathrm{P}(\mathrm{A})-\mathrm{P}(\mathrm{A}) \mathrm{P}(\mathrm{B})=\mathrm{P}(\mathrm{B})-\mathrm{P}(\mathrm{B}) \mathrm{P}(\mathrm{A})$ 由于A和B的概率都是非负的，上式意味着 $\mathrm{P}(\mathrm{A})=\mathrm{P}(\mathrm{B})$ 。但独立事件的概率相等并不一定意味着A和B是同一个事件，因为它们可以有相同的或不同的概率但独立性保持不变。 现在，我们知道了两个事件独立，且都不发生的概率为 $1 / 9$ 。这意味着： $\mathrm{P}(\mathrm{A}){ }^{a} \mathrm{P}(\mathrm{B})=1 / 9$ 由于 $\mathrm{P}(\mathrm{A})$ $=\mathrm{P}(\mathrm{B})$ ，我们可以将这个等式写成： $P(A)^{2}=1 / 9$ 因为 $\mathrm{P}(\mathrm{A})$ 是概率，所以它必须在 0 和 1 之间。唯一满足这个等式的概率值是： $P(A)=\operatorname{ar}(1 / 9)=1 / 3$ 因此， $\mathrm{P}(\mathrm{A})=1 / 3$ 。既然 $\mathrm{P}(\mathrm{A})=$ $\mathrm{P}(\mathrm{B})$ ，那么 $\mathrm{P}(\mathrm{B})$ 也等于 $1 / 3$ 。</td>
</tr>
<tr>
<td style="text-align: center;">ChatGLM3- <br> 32B-Math</td>
<td style="text-align: center;">设事件A发生的概率为 $\mathrm{P}(\mathrm{A})$ ，事件B发生的概率为 $\mathrm{P}(\mathrm{B})$ 。根据题意，我们可以列出以下关系： 1. 事件A和事件B都不发生的概率为 $1 / 9$ ，即： $\square$ <br> $P($ A不发生且B不发生 $)=P($ A不发生 $) \times P($ B不发生 $)=(1-P(A)) \times(1-P(B))=\frac{1}{9}$ <br> 2. 事件A发生而事件B不发生的概率与事件A不发生而事件B发生的概率相等，即： <br> $P($ A发生且B不发生 $)=P($ A不发生且B发生 $)$ <br> 根据概率的性质，我们有： $\square$ <br> $P($ A发生且B不发生 $)=P(A) \times(1-P(B))$ <br> $P($ A不发生且B发生 $)=(1-P(A)) \times P(B)$ <br> 由于这两个概率相等，我们可以得到： $\square$ <br> $P(A) \times(1-P(B))=(1-P(A)) \times P(B)$ <br> 现在我们有两个方程：1. $(1-P(A)) \times(1-P(B))=\frac{1}{9}$ 2. $P(A) \times(1-P(B))=(1-$ $P(A)) \times P(B)$ 我们可以通过这两个方程来解出 $\mathrm{P}(\mathrm{A})$ 和 $\mathrm{P}(\mathrm{B})$ 的值。首先，从第二个方程中我们可以得到: $\square$ <br> $P(A)-P(A) \times P(B)=P(B)-P(A) \times P(B)$ <br> $P(A)=P(B)$ <br> 这意味着事件A和事件B发生的概率相等。现在，我们用这个结论来解第一个方程: $\square$ <br> $(1-P(A)) \times(1-P(A))=\frac{1}{9}$ <br> $(1-P(A))^{2}=\frac{1}{9}$ <br> 取平方根得到: $\square$ <br> $1-P(A)=\frac{1}{3}$ 或 $1-P(A)=-\frac{1}{3}$ <br> 由于概率值不能为负，我们只取正值： $\square$ <br> $1-P(A)=\frac{1}{3}$ <br> $P(A)=\frac{2}{3}$ <br> 所以，事件A发生的概率 $\mathrm{P}(\mathrm{A})$ 等于 $2 / 3$ 1</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Yifan and Xiao contributed equally. Emails: xu-yf23@mails.tsinghua.edu.cn,shawliu9@gmail.com ${ }^{1}$ Work done while Xinghan and Yueyan interned at Zhipu AI.
${ }^{1}$ https://chatglm.cn&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
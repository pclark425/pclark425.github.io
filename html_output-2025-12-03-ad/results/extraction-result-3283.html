<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3283 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3283</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3283</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-2f3efe44083af91cef562c1a3451eee2f8601d22</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2f3efe44083af91cef562c1a3451eee2f8601d22" target="_blank">WebGPT: Browser-assisted question-answering with human feedback</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> GPT-3 is fine-tune to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web, and the best model is obtained, which is preferred by humans 56% of the time to those of the authors' human demonstrators, and 69%" to the highest-voted answer from Reddit.</p>
                <p><strong>Paper Abstract:</strong> We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3283.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3283.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebGPT: Browser-assisted question-answering with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of GPT-3 based models fine-tuned to use a text-based web-browsing environment; trained with human demonstrations and optimized with a reward model via reinforcement learning and/or rejection sampling to produce long-form, referenced answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (fine-tuned as WebGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained GPT-3 variants fine-tuned to operate in a text-based web-browsing environment. Models are fine-tuned using human demonstrations (behavior cloning), reward modeling from human comparisons, optionally PPO-based reinforcement learning against the reward model, and/or inference-time rejection sampling (best-of-n) using the reward model to select outputs. The browsing environment issues search queries to Bing, follows links, extracts quotes as references, and then composes answers supported by collected references.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>760M, 13B, 175B (GPT-3 family sizes used)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Retrieval-augmented browsing (web-based retrieval)', 'Behavior cloning (imitation learning of human browsing/actions)', 'Reward modeling (learned scalar preference predictor from human comparisons)', 'Reinforcement learning (PPO fine-tuning against reward model with KL penalty)', 'Rejection sampling / best-of-n (sample multiple answers and pick highest reward model score)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Retrieval-augmented browsing: the model issues search/click/scroll/quote commands in a text browser and collects references for final answers. Behavior cloning: supervised fine-tuning on human demonstrator action sequences in the browsing environment. Reward modeling: a model trained on human preference comparisons that assigns a scalar (Elo-like) score to question+answer+references. Reinforcement learning (PPO): policy fine-tuned to maximize reward-model score per episode plus a KL penalty to the BC policy. Rejection sampling (best-of-n): sample multiple complete answers from BC (or RL) policy and select the highest-scoring answer judged by the reward model (no additional training).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse set: the paper uses multiple distinct training and inference-time strategies. In particular, rejection sampling introduces output diversity by generating many candidate answers and selecting the best, whereas RL tends to reduce policy entropy and produce more similar/less diverse outputs. The paper explicitly contrasts these styles (diverse inference via best-of-n vs. optimized, lower-entropy RL policies) and reports empirical differences.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ELI5 (long-form QA), TruthfulQA (short-form adversarial), TriviaQA, ARC (mixed-in few examples)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>ELI5: long-form explain-like-I'm-five questions from Reddit; TruthfulQA: adversarial short questions designed to probe models' tendency to mirror human falsehoods (scored for truthfulness and informativeness); TriviaQA: short-form question-answering dataset; ARC: grade-school science multiple-choice questions converted to free-form.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Key reported metrics: (all human preference / human-evaluation based unless otherwise noted)
- WebGPT 175B best-of-64 (BC + rejection sampling): preferred to human demonstrators 56% of the time (vs demonstrations). Preferred to ELI5 highest-voted Reddit answers 69% of the time.
- TruthfulQA (human evaluation, WebGPT vs GPT-3): WebGPT answers were truthful 75% of the time and both truthful+informative 54% of the time (report states WebGPT outperforms base GPT-3 though exact GPT-3 percentages not listed in text). WebGPT models (all sizes) outperform GPT-3 choices on both truthful and truthful+informative measures; truthful+informative increases with WebGPT model size.
- Ablations comparing training/selection methods (175B size): 175B best-of-64 BC model preferred 68% of the time to the 175B BC baseline (i.e., large benefit from rejection sampling). 175B RL model preferred 58% of the time to the 175B BC baseline (smaller benefit). Combining RL + rejection sampling provided little extra benefit over rejection sampling alone.
- Scaling notes: doubling demonstrations increased policy reward-model score by ≈0.13; doubling comparisons increased reward model accuracy by ≈1.8%. Doubling policy parameters increased reward-model score by ≈0.09; doubling reward-model parameters increased its accuracy by ≈0.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Explicit comparisons/ablations: Behavior cloning (BC) baseline vs BC + rejection sampling (best-of-n) vs RL (PPO) vs RL + rejection sampling. Major findings: rejection sampling (best-of-n) provides substantial preference gain (e.g., 68% preference of best-of-64 BC over BC), whereas RL provides a smaller preference gain (58% preference of RL over BC) and combining RL with rejection sampling yields marginal additional benefit. The paper argues reasons: rejection sampling benefits from producing many diverse answering attempts and evaluating them with the reward model (hindsight); RL reduces entropy and can overoptimize the reward model, and the reward model was trained primarily on BC and rejection-sampling data which may bias robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Using a diverse inference-time strategy (rejection sampling / best-of-n) against a reward model substantially improves human-preferred answer quality over BC alone. 2) RL (PPO) against the same reward model yields smaller gains and can reduce policy diversity (entropy), limiting exploration and downstream gains from best-of-n. 3) Combining RL and rejection sampling gives little extra benefit—both optimize the same reward model and overoptimization and reduced entropy can explain diminishing returns. 4) Retrieval-augmented browsing + human-feedback training produces improvements in truthfulness and factual accuracy versus the pretrained base model (GPT-3).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Negative/limit findings reported: RL provides only modest improvement over BC and is outperformed by rejection sampling when both optimize the same reward model; RL reduces policy entropy which harms exploration and best-of-n performance; rejection sampling requires more inference-time compute (tradeoff); reward model can be overoptimized, especially by RL; WebGPT still sometimes cites unreliable sources on out-of-distribution datasets (TruthfulQA) and can cherry-pick references to persuade labelers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebGPT: Browser-assisted question-answering with human feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3283.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3283.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Behavior Cloning (BC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Behavior Cloning (supervised fine-tuning on human browsing demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Supervised fine-tuning of the pretrained GPT-3 policy on sequences of human actions in the text-based browsing environment so the model learns to search, click, quote, and assemble answers like a human demonstrator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (policy model for browsing)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BC models are GPT-3 checkpoints fine-tuned via supervised learning on ~6,000 human demonstrations (mostly ELI5 questions) mapping environment state to browsing commands; used as the base policy for other methods and as a backbone for RL and sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>760M, 13B, 175B (BC models trained at these sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Imitation of human browsing/search behavior', 'Deterministic supervised action prediction (command-level)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>BC trains the model to output the same commands humans used in the environment given the textual summary of state; BC produces a policy that can perform web searches, extract quotes as references, and compose final answers when prompted with references.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single supervised method for policy learning (imitative), but used as a base for diverse inference (best-of-n) and for further RL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ELI5 demonstrations (used for training), evaluation comparisons against human demonstrators and ELI5 dataset</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>BC dataset: ~6,209 demonstrations (≈92% ELI5); used to teach the model to use the browsing environment and compose referenced answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>BC baseline is the starting policy. Quantitative comparisons: 175B BC baseline is less preferred than its best-of-64 BC variant (best-of-64 preferred 68% over BC baseline). RL fine-tuned model preferred 58% over BC baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>BC alone provides a strong baseline; optimizing BC outputs at inference with rejection sampling yields large gains, suggesting BC's sampling diversity + RM ranking is highly effective. RL improves over BC but less than best-of-n on comparable compute budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BC is an effective supervised starting point that captures human-like browsing behavior; however, BC's performance can be substantially improved at inference by generating multiple diverse outputs and selecting via a reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>BC alone does not surpass human demonstrators in preference but when paired with best-of-n selection can exceed human demonstrators; BC tuning (epochs, temperature) is important and closing the gap to RL requires careful BC tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebGPT: Browser-assisted question-answering with human feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3283.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3283.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Best-of-n Rejection Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rejection sampling (best-of-n) against a learned reward model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time method that samples n complete answers from a base policy (BC or RL) and uses the reward model to rank them, selecting the highest-scoring answer—this increases effective output diversity by leveraging many attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BC or RL policy + reward model (both derived from GPT-3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>At inference the policy (usually BC) is sampled n times (n = 4, 16, 64 in experiments) to produce multiple candidate answers with their references; a separately-trained reward model scores each candidate and the highest scoring candidate is returned.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Used with policy sizes 760M, 13B, 175B and corresponding reward models (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Diverse sampling + learned ranking (reward-model selection)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generative diversity is achieved by sampling stochastic outputs from the base policy; selection is performed with a reward model trained on human comparisons, effectively turning many diverse outputs into a high-quality final response.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Explicitly increases diversity at inference (many distinct reasoning/answering attempts) rather than changing training objective; contrasts with RL which reduces policy entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ELI5 evaluation (primary), TruthfulQA and scaling experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used to improve answer quality on long-form ELI5 questions and also evaluated for truthfulness on TruthfulQA (answers truncated when necessary).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Substantial gains reported: 175B best-of-64 BC preferred 68% to 175B BC baseline; 175B best-of-64 BC model produced answers preferred 56% to human demonstrators and 69% to ELI5 highest-voted answers. Best-of-n choice varied by compute budget: 760M best-of-4, 13B best-of-16, 175B best-of-64 selected as Pareto-efficient choices.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Directly compared to RL and BC: best-of-n (rejection sampling) outperforms RL-fine-tuning in human preference when both optimize against same reward model, likely due to greater output diversity and robustness of reward model to BC/rejection-sampling style outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Rejection sampling is an effective, training-free way to optimize against a learned reward model by using more inference compute to produce diverse candidate answers and select the best; it yields larger human-preference gains than RL in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Best-of-n requires extra inference-time compute and diminishing returns if n is too large; it can also be sensitive to reward model overoptimization (the reward model is trained primarily on BC/rejection data, which may bias selection).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebGPT: Browser-assisted question-answering with human feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3283.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3283.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reinforcement Learning (PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement learning via Proximal Policy Optimization (PPO) against the reward model with KL regularization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning the BC policy with PPO to maximize the reward-model score for full episodes, adding a per-token KL penalty to the BC policy to prevent overoptimization and entropy collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (BC checkpoint fine-tuned with PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PPO is applied to the BC-initialized policy in the browsing environment; episode reward = reward-model score at episode end + sum of per-token KL penalties to the BC policy. Additional answering-only episodes are inserted to improve sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B primarily reported for RL comparisons; also experimented at 760M and 13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Policy-gradient RL (PPO) optimizing against learned reward function', 'KL-regularized RL to anchor to BC']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>PPO fine-tunes the policy to increase expected reward-model score, with a KL penalty coefficient tunable as a hyperparameter; environment includes browsing actions, quote collection, and final answer composition.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Produces more optimized (lower-entropy) policies compared to BC; thus method leads to more similar reasoning styles across samples and reduced exploratory diversity relative to sampling-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ELI5 and mixed training (90% ELI5, 10% TriviaQA) for RL training; evaluated on ELI5 comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>RL trained to maximize reward-model score on long-form browsing+answering episodes; additional answering-only episodes used to improve sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>175B RL model preferred 58% of the time to 175B BC baseline (smaller gain than rejection sampling). Combining RL with rejection sampling provided minimal extra benefit beyond rejection sampling alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared directly to BC and best-of-n: RL improves preference modestly but is outperformed by best-of-n; likely causes include (a) overoptimization and brittleness of RL to the reward model, (b) reduced entropy/exploration in RL harming best-of-n style gains, and (c) reward model trained largely on BC/rejection data making it more robust to those policies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PPO fine-tuning can improve human-preference scores but is less effective than leveraging diverse outputs via best-of-n selection in this setup; KL anchoring to BC mitigates some overoptimization but RL remains sensitive to hyperparameters and reward-model robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>RL can overoptimize the reward model and reduce policy diversity, limiting benefits when later applying best-of-n; RL requires careful hyperparameter tuning and did not significantly help when combined with rejection sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebGPT: Browser-assisted question-answering with human feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to summarize from human feedback <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>REALM: Retrieval-augmented language model pre-training <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 2)</em></li>
                <li>TruthfulQA: Measuring how models mimic human falsehoods <em>(Rating: 2)</em></li>
                <li>Hurdles to progress in long-form question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3283",
    "paper_id": "paper-2f3efe44083af91cef562c1a3451eee2f8601d22",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "WebGPT",
            "name_full": "WebGPT: Browser-assisted question-answering with human feedback",
            "brief_description": "A family of GPT-3 based models fine-tuned to use a text-based web-browsing environment; trained with human demonstrations and optimized with a reward model via reinforcement learning and/or rejection sampling to produce long-form, referenced answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (fine-tuned as WebGPT)",
            "model_description": "Pretrained GPT-3 variants fine-tuned to operate in a text-based web-browsing environment. Models are fine-tuned using human demonstrations (behavior cloning), reward modeling from human comparisons, optionally PPO-based reinforcement learning against the reward model, and/or inference-time rejection sampling (best-of-n) using the reward model to select outputs. The browsing environment issues search queries to Bing, follows links, extracts quotes as references, and then composes answers supported by collected references.",
            "model_size": "760M, 13B, 175B (GPT-3 family sizes used)",
            "reasoning_methods": [
                "Retrieval-augmented browsing (web-based retrieval)",
                "Behavior cloning (imitation learning of human browsing/actions)",
                "Reward modeling (learned scalar preference predictor from human comparisons)",
                "Reinforcement learning (PPO fine-tuning against reward model with KL penalty)",
                "Rejection sampling / best-of-n (sample multiple answers and pick highest reward model score)"
            ],
            "reasoning_methods_description": "Retrieval-augmented browsing: the model issues search/click/scroll/quote commands in a text browser and collects references for final answers. Behavior cloning: supervised fine-tuning on human demonstrator action sequences in the browsing environment. Reward modeling: a model trained on human preference comparisons that assigns a scalar (Elo-like) score to question+answer+references. Reinforcement learning (PPO): policy fine-tuned to maximize reward-model score per episode plus a KL penalty to the BC policy. Rejection sampling (best-of-n): sample multiple complete answers from BC (or RL) policy and select the highest-scoring answer judged by the reward model (no additional training).",
            "diversity_of_methods": "Diverse set: the paper uses multiple distinct training and inference-time strategies. In particular, rejection sampling introduces output diversity by generating many candidate answers and selecting the best, whereas RL tends to reduce policy entropy and produce more similar/less diverse outputs. The paper explicitly contrasts these styles (diverse inference via best-of-n vs. optimized, lower-entropy RL policies) and reports empirical differences.",
            "reasoning_task_name": "ELI5 (long-form QA), TruthfulQA (short-form adversarial), TriviaQA, ARC (mixed-in few examples)",
            "reasoning_task_description": "ELI5: long-form explain-like-I'm-five questions from Reddit; TruthfulQA: adversarial short questions designed to probe models' tendency to mirror human falsehoods (scored for truthfulness and informativeness); TriviaQA: short-form question-answering dataset; ARC: grade-school science multiple-choice questions converted to free-form.",
            "performance_by_method": "Key reported metrics: (all human preference / human-evaluation based unless otherwise noted)\n- WebGPT 175B best-of-64 (BC + rejection sampling): preferred to human demonstrators 56% of the time (vs demonstrations). Preferred to ELI5 highest-voted Reddit answers 69% of the time.\n- TruthfulQA (human evaluation, WebGPT vs GPT-3): WebGPT answers were truthful 75% of the time and both truthful+informative 54% of the time (report states WebGPT outperforms base GPT-3 though exact GPT-3 percentages not listed in text). WebGPT models (all sizes) outperform GPT-3 choices on both truthful and truthful+informative measures; truthful+informative increases with WebGPT model size.\n- Ablations comparing training/selection methods (175B size): 175B best-of-64 BC model preferred 68% of the time to the 175B BC baseline (i.e., large benefit from rejection sampling). 175B RL model preferred 58% of the time to the 175B BC baseline (smaller benefit). Combining RL + rejection sampling provided little extra benefit over rejection sampling alone.\n- Scaling notes: doubling demonstrations increased policy reward-model score by ≈0.13; doubling comparisons increased reward model accuracy by ≈1.8%. Doubling policy parameters increased reward-model score by ≈0.09; doubling reward-model parameters increased its accuracy by ≈0.4%.",
            "comparison_of_methods": "Explicit comparisons/ablations: Behavior cloning (BC) baseline vs BC + rejection sampling (best-of-n) vs RL (PPO) vs RL + rejection sampling. Major findings: rejection sampling (best-of-n) provides substantial preference gain (e.g., 68% preference of best-of-64 BC over BC), whereas RL provides a smaller preference gain (58% preference of RL over BC) and combining RL with rejection sampling yields marginal additional benefit. The paper argues reasons: rejection sampling benefits from producing many diverse answering attempts and evaluating them with the reward model (hindsight); RL reduces entropy and can overoptimize the reward model, and the reward model was trained primarily on BC and rejection-sampling data which may bias robustness.",
            "key_findings": "1) Using a diverse inference-time strategy (rejection sampling / best-of-n) against a reward model substantially improves human-preferred answer quality over BC alone. 2) RL (PPO) against the same reward model yields smaller gains and can reduce policy diversity (entropy), limiting exploration and downstream gains from best-of-n. 3) Combining RL and rejection sampling gives little extra benefit—both optimize the same reward model and overoptimization and reduced entropy can explain diminishing returns. 4) Retrieval-augmented browsing + human-feedback training produces improvements in truthfulness and factual accuracy versus the pretrained base model (GPT-3).",
            "counter_examples_or_negative_results": "Negative/limit findings reported: RL provides only modest improvement over BC and is outperformed by rejection sampling when both optimize the same reward model; RL reduces policy entropy which harms exploration and best-of-n performance; rejection sampling requires more inference-time compute (tradeoff); reward model can be overoptimized, especially by RL; WebGPT still sometimes cites unreliable sources on out-of-distribution datasets (TruthfulQA) and can cherry-pick references to persuade labelers.",
            "uuid": "e3283.0",
            "source_info": {
                "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Behavior Cloning (BC)",
            "name_full": "Behavior Cloning (supervised fine-tuning on human browsing demonstrations)",
            "brief_description": "Supervised fine-tuning of the pretrained GPT-3 policy on sequences of human actions in the text-based browsing environment so the model learns to search, click, quote, and assemble answers like a human demonstrator.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (policy model for browsing)",
            "model_description": "BC models are GPT-3 checkpoints fine-tuned via supervised learning on ~6,000 human demonstrations (mostly ELI5 questions) mapping environment state to browsing commands; used as the base policy for other methods and as a backbone for RL and sampling.",
            "model_size": "760M, 13B, 175B (BC models trained at these sizes)",
            "reasoning_methods": [
                "Imitation of human browsing/search behavior",
                "Deterministic supervised action prediction (command-level)"
            ],
            "reasoning_methods_description": "BC trains the model to output the same commands humans used in the environment given the textual summary of state; BC produces a policy that can perform web searches, extract quotes as references, and compose final answers when prompted with references.",
            "diversity_of_methods": "Single supervised method for policy learning (imitative), but used as a base for diverse inference (best-of-n) and for further RL fine-tuning.",
            "reasoning_task_name": "ELI5 demonstrations (used for training), evaluation comparisons against human demonstrators and ELI5 dataset",
            "reasoning_task_description": "BC dataset: ~6,209 demonstrations (≈92% ELI5); used to teach the model to use the browsing environment and compose referenced answers.",
            "performance_by_method": "BC baseline is the starting policy. Quantitative comparisons: 175B BC baseline is less preferred than its best-of-64 BC variant (best-of-64 preferred 68% over BC baseline). RL fine-tuned model preferred 58% over BC baseline.",
            "comparison_of_methods": "BC alone provides a strong baseline; optimizing BC outputs at inference with rejection sampling yields large gains, suggesting BC's sampling diversity + RM ranking is highly effective. RL improves over BC but less than best-of-n on comparable compute budgets.",
            "key_findings": "BC is an effective supervised starting point that captures human-like browsing behavior; however, BC's performance can be substantially improved at inference by generating multiple diverse outputs and selecting via a reward model.",
            "counter_examples_or_negative_results": "BC alone does not surpass human demonstrators in preference but when paired with best-of-n selection can exceed human demonstrators; BC tuning (epochs, temperature) is important and closing the gap to RL requires careful BC tuning.",
            "uuid": "e3283.1",
            "source_info": {
                "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Best-of-n Rejection Sampling",
            "name_full": "Rejection sampling (best-of-n) against a learned reward model",
            "brief_description": "An inference-time method that samples n complete answers from a base policy (BC or RL) and uses the reward model to rank them, selecting the highest-scoring answer—this increases effective output diversity by leveraging many attempts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BC or RL policy + reward model (both derived from GPT-3 family)",
            "model_description": "At inference the policy (usually BC) is sampled n times (n = 4, 16, 64 in experiments) to produce multiple candidate answers with their references; a separately-trained reward model scores each candidate and the highest scoring candidate is returned.",
            "model_size": "Used with policy sizes 760M, 13B, 175B and corresponding reward models (various sizes)",
            "reasoning_methods": [
                "Diverse sampling + learned ranking (reward-model selection)"
            ],
            "reasoning_methods_description": "Generative diversity is achieved by sampling stochastic outputs from the base policy; selection is performed with a reward model trained on human comparisons, effectively turning many diverse outputs into a high-quality final response.",
            "diversity_of_methods": "Explicitly increases diversity at inference (many distinct reasoning/answering attempts) rather than changing training objective; contrasts with RL which reduces policy entropy.",
            "reasoning_task_name": "ELI5 evaluation (primary), TruthfulQA and scaling experiments",
            "reasoning_task_description": "Used to improve answer quality on long-form ELI5 questions and also evaluated for truthfulness on TruthfulQA (answers truncated when necessary).",
            "performance_by_method": "Substantial gains reported: 175B best-of-64 BC preferred 68% to 175B BC baseline; 175B best-of-64 BC model produced answers preferred 56% to human demonstrators and 69% to ELI5 highest-voted answers. Best-of-n choice varied by compute budget: 760M best-of-4, 13B best-of-16, 175B best-of-64 selected as Pareto-efficient choices.",
            "comparison_of_methods": "Directly compared to RL and BC: best-of-n (rejection sampling) outperforms RL-fine-tuning in human preference when both optimize against same reward model, likely due to greater output diversity and robustness of reward model to BC/rejection-sampling style outputs.",
            "key_findings": "Rejection sampling is an effective, training-free way to optimize against a learned reward model by using more inference compute to produce diverse candidate answers and select the best; it yields larger human-preference gains than RL in this setup.",
            "counter_examples_or_negative_results": "Best-of-n requires extra inference-time compute and diminishing returns if n is too large; it can also be sensitive to reward model overoptimization (the reward model is trained primarily on BC/rejection data, which may bias selection).",
            "uuid": "e3283.2",
            "source_info": {
                "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Reinforcement Learning (PPO)",
            "name_full": "Reinforcement learning via Proximal Policy Optimization (PPO) against the reward model with KL regularization",
            "brief_description": "Fine-tuning the BC policy with PPO to maximize the reward-model score for full episodes, adding a per-token KL penalty to the BC policy to prevent overoptimization and entropy collapse.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (BC checkpoint fine-tuned with PPO)",
            "model_description": "PPO is applied to the BC-initialized policy in the browsing environment; episode reward = reward-model score at episode end + sum of per-token KL penalties to the BC policy. Additional answering-only episodes are inserted to improve sample efficiency.",
            "model_size": "175B primarily reported for RL comparisons; also experimented at 760M and 13B",
            "reasoning_methods": [
                "Policy-gradient RL (PPO) optimizing against learned reward function",
                "KL-regularized RL to anchor to BC"
            ],
            "reasoning_methods_description": "PPO fine-tunes the policy to increase expected reward-model score, with a KL penalty coefficient tunable as a hyperparameter; environment includes browsing actions, quote collection, and final answer composition.",
            "diversity_of_methods": "Produces more optimized (lower-entropy) policies compared to BC; thus method leads to more similar reasoning styles across samples and reduced exploratory diversity relative to sampling-based approaches.",
            "reasoning_task_name": "ELI5 and mixed training (90% ELI5, 10% TriviaQA) for RL training; evaluated on ELI5 comparisons",
            "reasoning_task_description": "RL trained to maximize reward-model score on long-form browsing+answering episodes; additional answering-only episodes used to improve sample efficiency.",
            "performance_by_method": "175B RL model preferred 58% of the time to 175B BC baseline (smaller gain than rejection sampling). Combining RL with rejection sampling provided minimal extra benefit beyond rejection sampling alone.",
            "comparison_of_methods": "Compared directly to BC and best-of-n: RL improves preference modestly but is outperformed by best-of-n; likely causes include (a) overoptimization and brittleness of RL to the reward model, (b) reduced entropy/exploration in RL harming best-of-n style gains, and (c) reward model trained largely on BC/rejection data making it more robust to those policies.",
            "key_findings": "PPO fine-tuning can improve human-preference scores but is less effective than leveraging diverse outputs via best-of-n selection in this setup; KL anchoring to BC mitigates some overoptimization but RL remains sensitive to hyperparameters and reward-model robustness.",
            "counter_examples_or_negative_results": "RL can overoptimize the reward model and reduce policy diversity, limiting benefits when later applying best-of-n; RL requires careful hyperparameter tuning and did not significantly help when combined with rejection sampling.",
            "uuid": "e3283.3",
            "source_info": {
                "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
                "publication_date_yy_mm": "2021-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to summarize from human feedback",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "REALM: Retrieval-augmented language model pre-training",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 2
        },
        {
            "paper_title": "TruthfulQA: Measuring how models mimic human falsehoods",
            "rating": 2
        },
        {
            "paper_title": "Hurdles to progress in long-form question answering",
            "rating": 1
        }
    ],
    "cost": 0.0139195,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>WebGPT: Browser-assisted question-answering with human feedback</h1>
<p>Reiichiro Nakano<em> Jacob Hilton</em> Suchir Balaji* Jeff Wu Long Ouyang<br>Christina Kim Christopher Hesse Shantanu Jain<br>Vineet Kosaraju<br>William Saunders Xu Jiang Karl Cobbe Tyna Eloundou Gretchen Krueger<br>Kevin Button Matthew Knight Benjamin Chess John Schulman<br>OpenAI</p>
<h4>Abstract</h4>
<p>We fine-tune GPT-3 to answer long-form questions using a text-based webbrowsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans $56 \%$ of the time to those of our human demonstrators, and $69 \%$ of the time to the highest-voted answer from Reddit.</p>
<h2>1 Introduction</h2>
<p>A rising challenge in NLP is long-form question-answering (LFQA), in which a paragraph-length answer is generated in response to an open-ended question. LFQA systems have the potential to become one of the main ways people learn about the world, but currently lag behind human performance [Krishna et al., 2021]. Existing work tends to focus on two core components of the task, information retrieval and synthesis.
In this work we leverage existing solutions to these components: we outsource document retrieval to the Microsoft Bing Web Search API, ${ }^{2}$ and utilize unsupervised pre-training to achieve high-quality synthesis by fine-tuning GPT-3 [Brown et al., 2020]. Instead of trying to improve these ingredients, we focus on combining them using more faithful training objectives. Following Stiennon et al. [2020], we use human feedback to directly optimize answer quality, allowing us to achieve performance competitive with humans.
We make two key contributions:</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An observation from our text-based web-browsing environment, as shown to human demonstrators (left) and models (right). The web page text has been abridged for illustrative purposes.</p>
<ul>
<li>We create a text-based web-browsing environment that a fine-tuned language model can interact with. This allows us to improve both retrieval and synthesis in an end-to-end fashion using general methods such as imitation learning and reinforcement learning.</li>
<li>We generate answers with references: passages extracted by the model from web pages while browsing. This is crucial for allowing labelers to judge the factual accuracy of answers, without engaging in a difficult and subjective process of independent research.</li>
</ul>
<p>Our models are trained primarily to answer questions from ELI5 [Fan et al., 2019], a dataset of questions taken from the "Explain Like I'm Five" subreddit. We collect two additional kinds of data: demonstrations of humans using our web-browsing environment to answer questions, and comparisons between two model-generated answers to the same question (each with their own set of references). Answers are judged for their factual accuracy, coherence, and overall usefulness.</p>
<p>We use this data in four main ways: behavior cloning (i.e., supervised fine-tuning) using the demonstrations, reward modeling using the comparisons, reinforcement learning against the reward model, and rejection sampling against the reward model. Our best model uses a combination of behavior cloning and rejection sampling. We also find reinforcement learning to provide some benefit when inference-time compute is more limited.</p>
<p>We evaluate our best model in three different ways. First, we compare our model's answers to answers written by our human demonstrators on a held-out set of questions. Our model's answers are preferred 56% of the time, demonstrating human-level usage of the text-based browser. Second, we compare our model's answers (with references stripped, for fairness) to the highest-voted answer provided by the ELI5 dataset. Our model's answers are preferred 69% of the time. Third, we evaluate our model on TruthfulQA [Lin et al., 2021], an adversarial dataset of short-form questions. Our model's answers are true 75% of the time, and are both true and informative 54% of the time, outperforming our base model (GPT-3), but falling short of human performance.</p>
<p>The remainder of the paper is structured as follows:</p>
<ul>
<li>In Section 2, we describe our text-based web-browsing environment and how our models interact with it.</li>
<li>In Section 3, we explain our data collection and training methods in more detail.</li>
<li>In Section 4, we evaluate our best-performing models (for different inference-time compute budgets) on ELI5 and TruthfulQA.</li>
<li>In Section 5, we provide experimental results comparing our different methods and how they scale with dataset size, parameter count, and inference-time compute.</li>
<li>In Section 6, we discuss the implications of our findings for training models to answer questions truthfully, and broader impacts.</li>
</ul>
<p>Table 1: Actions the model can take. If a model generates any other text, it is considered to be an invalid action. Invalid actions still count towards the maximum, but are otherwise ignored.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Command</th>
<th style="text-align: left;">Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Search <query></td>
<td style="text-align: left;">Send <query> to the Bing API and display a search results page</td>
</tr>
<tr>
<td style="text-align: left;">Clicked on link <link ID></td>
<td style="text-align: left;">Follow the link with the given ID to a new page</td>
</tr>
<tr>
<td style="text-align: left;">Find in page: <text></td>
<td style="text-align: left;">Find the next occurrence of <text> and scroll to it</td>
</tr>
<tr>
<td style="text-align: left;">Quote: <text></td>
<td style="text-align: left;">If <text> is found in the current page, add it as a reference</td>
</tr>
<tr>
<td style="text-align: left;">Scrolled down &lt;1, 2, 3&gt;</td>
<td style="text-align: left;">Scroll down a number of times</td>
</tr>
<tr>
<td style="text-align: left;">Scrolled up &lt;1, 2, 3&gt;</td>
<td style="text-align: left;">Scroll up a number of times</td>
</tr>
<tr>
<td style="text-align: left;">Top</td>
<td style="text-align: left;">Scroll to the top of the page</td>
</tr>
<tr>
<td style="text-align: left;">Back</td>
<td style="text-align: left;">Go to the previous page</td>
</tr>
<tr>
<td style="text-align: left;">End: Answer</td>
<td style="text-align: left;">End browsing and move to answering phase</td>
</tr>
<tr>
<td style="text-align: left;">End: <Nonsense, Controversial></td>
<td style="text-align: left;">End browsing and skip answering phase</td>
</tr>
</tbody>
</table>
<h1>2 Environment design</h1>
<p>Previous work on question-answering such as REALM [Guu et al., 2020] and RAG [Lewis et al., 2020a] has focused on improving document retrieval for a given query. Instead, we use a familiar existing method for this: a modern search engine (Bing). This has two main advantages. First, modern search engines are already very powerful, and index a large number of up-to-date documents. Second, it allows us to focus on the higher-level task of using a search engine to answer questions, something that humans can do well, and that a language model can mimic.</p>
<p>For this approach, we designed a text-based web-browsing environment. The language model is prompted with a written summary of the current state of the environment, including the question, the text of the current page at the current cursor location, and some other information (see Figure 1(b)). In response to this, the model must issue one of the commands given in Table 1, which performs an action such as running a Bing search, clicking on a link, or scrolling around. This process is then repeated with a fresh context (hence, the only memory of previous steps is what is recorded in the summary).
While the model is browsing, one of the actions it can take is to quote an extract from the current page. When this is performed, the page title, domain name and extract are recorded to be used later as a reference. Browsing then continues until either the model issues a command to end browsing, the maximum number of actions has been reached, or the maximum total length of references has been reached. At this point, as long as there is at least one reference, the model is prompted with the question and the references, and must compose its final answer.
Further technical details about our environment can be found in Appendix A.</p>
<h2>3 Methods</h2>
<h3>3.1 Data collection</h3>
<p>Guidance from humans is central to our approach. A language model pre-trained on natural language would not be able to use our text-based browser, since it does not know the format of valid commands. We therefore collected examples of humans using the browser to answer questions, which we call demonstrations. However, training on demonstrations alone does not directly optimize answer quality, and is unlikely to lead far beyond human performance [Stiennon et al., 2020]. We therefore collected pairs of model-generated answers to the same question, and asked humans which one they preferred, which we call comparisons.
For both demonstrations and comparisons, the vast majority of questions were taken from ELI5 [Fan et al., 2019], a dataset of long-form questions. For diversity and experimentation, we also mixed in a small number of questions from other sources, such as TriviaQA [Joshi et al., 2017]. In total, we collected around 6,000 demonstrations, $92 \%$ of which were for questions from ELI5, and around 21,500 comparisons, $98 \%$ of which were for questions from ELI5. A more detailed breakdown of the questions we used along with post-processing details can be found in Appendix B.</p>
<p>Table 2: An answer produced by our 175B best-of-64 model to a randomly-chosen question from the ELI5 test set (not cherry-picked). The full text of the references can be found in Appendix J, along with answers from our human demonstrators and the ELI5 dataset. Further samples are available at https://openaipublic.blob.core.windows.net/webgpt-answer-viewer/index.html.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question</th>
<th style="text-align: left;">Why did we decide that certain words were "bad" and shouldn't be used in social settings?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Answer</td>
<td style="text-align: left;">We have all been taught that there are certain words that should not be used in public, at least <br> not without good reason. But why do some words fall into this category while others do not? It <br> turns out that there is not a single answer to this question, as each word has a different history <br> and carries different meanings in different cultures and families [2]. However, there are some <br> general categories that "bad" words tend to fall into. "Bad" words generally relate to parts of life <br> that we don't like talking about in public, like bathroom functions, or negative ways of talking <br> about people's religion, skin color, ability, or gender [3]. Often, people say bad words when <br> they are experiencing strong emotions, and those words are linked to our emotions [3].</td>
</tr>
<tr>
<td style="text-align: left;">References</td>
<td style="text-align: left;">[1, 2, 3] Why Are Some Words 'Bad'?</td>
</tr>
<tr>
<td style="text-align: left;">(titles</td>
<td style="text-align: left;">[4] On Words: 'Bad' Words and Why We Should Study Them</td>
</tr>
<tr>
<td style="text-align: left;">only)</td>
<td style="text-align: left;">[5] The Science of Curse Words: Why The \&amp;D Do We Swear? (www.babbel.com)</td>
</tr>
</tbody>
</table>
<p>To make it easier for humans to provide demonstrations, we designed a graphical user interface for the environment (see Figure 1(a)). This displays essentially the same information as the text-based interface and allows any valid action to be performed, but is more human-friendly. For comparisons, we designed a similar interface, allowing auxiliary annotations as well as comparison ratings to be provided, although only the final comparison ratings (better, worse or equally good overall) were used in training.</p>
<p>For both demonstrations and comparisons, we emphasized that answers should be relevant, coherent, and supported by trustworthy references. Further details about these criteria and other aspects of our data collection pipeline can be found in Appendix C.</p>
<p>We are releasing a dataset of comparisons, the details of which can be found in Appendix K.</p>
<h1>3.2 Training</h1>
<p>The use of pre-trained models is crucial to our approach. Many of the underlying capabilities required to successfully use our environment to answer questions, such as reading comprehension and answer synthesis, emerge as zero-shot capabilities of language models [Brown et al., 2020]. We therefore fine-tuned models from the GPT-3 model family, focusing on the 760M, 13B and 175B model sizes.</p>
<p>Starting from these models, we used four main training methods:</p>
<ol>
<li>Behavior cloning (BC). We fine-tuned on the demonstrations using supervised learning, with the commands issued by the human demonstrators as labels.</li>
<li>Reward modeling (RM). Starting from the BC model with the final unembedding layer removed, we trained a model to take in a question and an answer with references, and output a scalar reward. Following Stiennon et al. [2020], the reward represents an Elo score, scaled such that the difference between two scores represents the logit of the probability that one will be preferred to the other by the human labelers. The reward model is trained using a cross-entropy loss, with the comparisons as labels. Ties are treated as soft $50 \%$ labels.</li>
<li>Reinforcement learning (RL). Once again following Stiennon et al. [2020], we fine-tuned the BC model on our environment using PPO [Schulman et al., 2017]. For the environment reward, we took the reward model score at the end of each episode, and added this to a KL penalty from the BC model at each token to mitigate overoptimization of the reward model.</li>
<li>Rejection sampling (best-of-n). We sampled a fixed number of answers (4, 16 or 64) from either the BC model or the RL model (if left unspecified, we used the BC model), and selected the one that was ranked highest by the reward model. We used this as an alternative method of optimizing against the reward model, which requires no additional training, but instead uses more inference-time compute.</li>
</ol>
<p>We used mutually disjoint sets of questions for each of BC, RM and RL.
For BC, we held out around $4 \%$ of the demonstrations to use as a validation set.
For RM, we sampled answers for the comparison datasets in an ad-hoc manner, using models of various sizes (but primarily the 175B model size), trained using various combinations of methods and hyperparameters, and combined them into a single dataset. This was for data efficiency: we collected many comparisons for evaluation purposes, such as for tuning hyperparameters, and did not want to waste this data. Our final reward models were trained on around 16,000 comparisons, the remaining 5,500 being used for evaluation only.</p>
<p>For RL, we trained on a mixture of $90 \%$ questions from ELI5 and $10 \%$ questions from TriviaQA. To improve sample efficiency, at the end of each episode we inserted 15 additional answering-only episodes using the same references as the previous episode. We were motivated to try this because answering explained slightly more of the variance in reward model score than browsing despite taking many fewer steps, and we found it to improve sample efficiency by approximately a factor of 2 . We also randomized the maximum number of browsing actions, sampling uniformly from the range $20-100$ inclusive.</p>
<p>Hyperparameters for all of our training methods can be found in Appendix E.</p>
<h1>4 Evaluation</h1>
<p>In evaluating our approach, we focused on three "WebGPT" models, each of which was trained with behavior cloning followed by rejection sampling against a reward model of the same size: a 760M best-of-4 model, a 13B best-of-16 model and a 175B best-of-64 model. As discussed in Section 5.2, these are compute-efficient models corresponding to different inference-time compute budgets. We excluded RL for simplicity, since it did not provide significant benefit when combined with rejection sampling (see Figure 4).</p>
<p>We evaluated all WebGPT models using a sampling temperature of 0.8 , which was tuned using human evaluations, and with a maximum number of browsing actions of 100 .</p>
<h3>4.1 ELI5</h3>
<p>We evaluated WebGPT on the ELI5 test set in two different ways:</p>
<ol>
<li>We compared model-generated answers to answers written by demonstrators using our web-browsing environment. For these comparisons, we used the same procedure as comparisons used for reward model training. We consider this to be a fair comparison, since the instructions for demonstrations and comparisons emphasize a very similar set of criteria.</li>
<li>We compared model-generated answers to the reference answers from the ELI5 dataset, which are the highest-voted answers from Reddit. In this case, we were concerned about ecological validity, since our detailed comparison criteria may not match those of real-life users. We were also concerned about blinding, since Reddit answers do not typically include citations. To mitigate these concerns, we stripped all citations and references from the model-generated answers, hired new contractors who were not familiar with our detailed instructions, and gave them a much more minimal set of instructions, which are given in Appendix F.</li>
</ol>
<p>In both cases, we treat ties as $50 \%$ preference ratings (rather than excluding them).
Our results are shown in Figure 2. Our best model, the 175B best-of-64 model, produces answers that are preferred to those written by our human demonstrators $56 \%$ of the time. This suggests that the use of human feedback is essential, since one would not expect to exceed $50 \%$ preference by imitating demonstrations alone (although it may still be possible, by producing a less noisy policy). The same model produces answers that are preferred to the reference answers from the ELI5 dataset $69 \%$ of the time. This is a substantial improvement over Krishna et al. [2021], whose best model's answers are preferred $23 \%$ of the time to the reference answers, although they use substantially less compute than even our smallest model.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Human evaluations on ELI5 comparing against (a) demonstrations collected using our web browser, (b) the highest-voted answer for each question. The amount of rejection sampling (the $n$ in best-of-n) was chosen to be compute-efficient (see Figure 8). Error bars represent $\pm 1$ standard error.</p>
<p>Although the evaluations against the ELI5 reference answers are useful for comparing to prior work, we believe that the evaluations against human demonstrations are more meaningful, for several reasons:</p>
<ul>
<li>Fact-checking. It is difficult to assess the factual accuracy of answers without references: even with the help of a search engine, expertise is often required. However, WebGPT and human demonstrators provide answers with references.</li>
<li>Objectivity. The use of minimal instructions makes it harder to know what criteria are being used to choose one answer over another. Our more detailed instructions enable more interpretable and consistent comparisons.</li>
<li>Blinding. Even with citations and references stripped, WebGPT composes answers that are different in style to Reddit answers, making the comparisons less blinded. In contrast, WebGPT and human demonstrators compose answers in similar styles. Additionally, some ELI5 answers contained links, which we instructed labelers not to follow, and this could have biased labelers against those answers.</li>
<li>Answer intent. People ask questions on ELI5 to obtain original, simplified explanations rather than answers that can already be found on the web, but these were not criteria we wanted answers to be judged on. Moreover, many ELI5 questions only ever get a small number of low-effort answers. With human demonstrations, it is easier to ensure that the desired intent and level of effort are used consistently.</li>
</ul>
<h1>4.2 TruthfulQA</h1>
<p>To further probe the abilities of WebGPT, we evaluated WebGPT on TruthfulQA [Lin et al., 2021], an adversarially-constructed dataset of short-form questions. TruthfulQA questions are crafted such that they would be answered falsely by some humans due to a false belief or misconception. Answers are scored on both truthfulness and informativeness, which trade off against one another (for example, "I have no comment" is considered truthful but not informative).
We evaluated both the base GPT-3 models used by WebGPT and the WebGPT models themselves on TruthfulQA. For GPT-3, we used both the "QA prompt" and the "helpful prompt" from Lin et al. [2021], and used the automated metric, since this closely tracks human evaluation on answers produced by the GPT-3 model family. For WebGPT, we used human evaluation, since WebGPT's answers are out-of-distribution for the automated metric. TruthfulQA is a short-form dataset, so</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: TruthfulQA results. The amount of rejection sampling (the $n$ in best-of- $n$ ) was chosen to be compute-efficient (see Figure 8). Error bars represent $\pm 1$ standard error.
we also truncated WebGPT's answers to 50 tokens in length, and then removed any trailing partial sentences. ${ }^{3}$</p>
<p>Our results are shown in Figure 3. All WebGPT models outperform all GPT-3 models (with both prompts) on both the percentage of truthful answers and the percentage of truthful and informative answers. Moreover, the percentage of truthful and informative answers increases with model size for WebGPT, unlike GPT-3 with either prompt. Further qualitative analysis of WebGPT's performance on TruthfulQA is given in Section 6.1.</p>
<h1>4.3 TriviaQA</h1>
<p>We also evaluated the WebGPT 175B BC model on TriviaQA [Joshi et al., 2017]. These results are given in Appendix G.</p>
<h2>5 Experiments</h2>
<h3>5.1 Comparison of training methods</h3>
<p>We ran a number of additional experiments comparing reinforcement learning (RL) and rejection sampling (best-of- $n$ ) with each other and with the behavior cloning (BC) baseline. Our results are shown in Figures 4 and 5. Rejection sampling provides a substantial benefit, with the 175B best-of-64 BC model being preferred $68 \%$ of the time to the 175B BC model. Meanwhile, RL provides a smaller benefit, with the 175B RL model being preferred $58 \%$ of the time to the 175B BC model.</p>
<p>Even though both rejection sampling and RL optimize against the same reward model, there are several possible reasons why rejection sampling outperforms RL:</p>
<ul>
<li>It may help to have many answering attempts, simply to make use of more inference-time compute.</li>
<li>The environment is unpredictable: with rejection sampling, the model can try visiting many more websites, and then evaluate the information it finds with the benefit of hindsight.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Preference of RL models over BC models, with (right) and without (left) using rejection sampling. RL slightly improves preference, but only when not using rejection sampling. Error bars represent $\pm 1$ standard error.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Preference of the 175B best-of- $n$ BC model over the BC model. The validation RM prediction is obtained using the estimator described in Appendix I, and predicts human preference well in this setting. The shaded region represents $\pm 1$ standard error.</p>
<ul>
<li>The reward model was trained primarily on data collected from BC and rejection sampling policies, which may have made it more robust to overoptimization by rejection sampling than by RL.</li>
<li>RL requires hyperparameter tuning, whereas rejection sampling does not.</li>
</ul>
<p>The combination of RL and rejection sampling also fails to offer much benefit over rejection sampling alone. One possible reason for this is that RL and rejection sampling are optimizing against the same reward model, which can easily be overoptimized (especially by RL, as noted above). In addition to this, RL reduces the entropy of the policy, which hurts exploration. Adapting the RL objective to optimize rejection sampling performance is an interesting direction for future research.</p>
<p>It is also worth highlighting the importance of carefully tuning the BC baseline for these comparisons. As discussed in Appendix E, we tuned the number of BC epochs and the sampling temperature using a combination of human evaluations and reward model score. This alone closed much of the gap we originally saw between BC and RL.</p>
<h1>5.2 Scaling experiments</h1>
<p>We also conducted experiments to investigate how model performance varied with the size of the dataset, the number of model parameters, and the number of samples used for rejection sampling. Since human evaluations can be noisy and expensive, we used the score of a 175B "validation" reward model (trained on a separate dataset split) for these experiments. We found this to be a good predictor of human preference when not optimizing against a reward model using RL (see Figure 5). Recall that the reward represents an Elo score, with a difference of 1 point representing a preference of $\operatorname{sigmoid}(1) \approx 73 \%$.</p>
<p>Scaling trends with dataset size and parameter count are shown in Figures 6 and 7. For dataset size, doubling the number of demonstrations increased the policy's reward model score by about 0.13 , and doubling the number of comparisons increased the reward model's accuracy by about $1.8 \%$. For parameter count, the trends were noisier, but doubling the number of parameters in the policy increased its reward model score by roughly 0.09 , and doubling the number of parameters in the reward model increased its accuracy by roughly $0.4 \%$.</p>
<p>For rejection sampling, we analyzed how to trade off the number of samples against the number of model parameters for a given inference-time compute budget (see Figure 8). We found that it is</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: BC scaling, varying the proportion of the demonstration dataset and parameter count of the policy.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: RM scaling, varying the proportion of the comparison dataset and parameter count of the reward model.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Best-of- $n$ scaling, varying the parameter count of the policy and reward model together, as well as the number of answers sampled.
generally compute-efficient to use some amount of rejection sampling, but not too much. The models for our main evaluations come from the Pareto frontier of this trade-off: the 760M best-of-4 model, the 13B best-of-16 model, and the 175B best-of-64 model.</p>
<h1>6 Discussion</h1>
<h3>6.1 Truthfulness of WebGPT</h3>
<p>As NLP systems improve and become more widely deployed, it is becoming increasingly important to develop techniques for reducing the number of false statements they make [Evans et al., 2021]. To assess the contribution of WebGPT to this aim, it is helpful to distinguish two categories of false statement made by a model:</p>
<ol>
<li>Imitative falsehoods. These are false statements that are incentivized by the training objective (even in the limit of infinite data and compute), such as reproducing common misconceptions [Lin et al., 2021].</li>
<li>Non-imitative falsehoods. These are false statements that are the result of the model failing to achieve its training objective, including most hallucinations, which are statements that are false, but look plausible at a glance [Maynez et al., 2020].</li>
</ol>
<p>Our TruthfulQA results suggest that WebGPT produces fewer imitative falsehoods than GPT-3. We believe this is because WebGPT is incentivized to prefer reliable sources (both because of filtering performed by the Bing API, and because we specify this in our instructions). Nevertheless, as shown in Table 3, WebGPT still sometimes quotes from highly unreliable sources in response to TruthfulQA questions. We hypothesize that this is because of the distribution shift from ELI5 to TruthfulQA, and that training on adversarially-selected questions is a promising way to improve this. It would be important in such an endeavor to pay close attention to labeler judgments of source trustworthiness (see Appendix C).</p>
<p>Our results on ELI5 suggest that WebGPT also produces fewer non-imitative falsehoods than GPT3. We did not test this hypothesis directly, since we found that it was challenging for labelers to spot subtle hallucinations. However, prior work shows that the use of retrieval reduces the rate of hallucinations [Shuster et al., 2021], and moreover WebGPT performs about as well as human demonstrations for factual accuracy on ELI5 (see Figure 2(a)). Nevertheless, WebGPT still sometimes produces non-imitative falsehoods, which are typically mistakes when attempting to paraphrase or synthesize information rather than wild hallucinations.</p>
<p>Table 3: Two questions from TruthfulQA, cherry-picked to highlight a success and a failure of WebGPT. While GPT-3 175B with the helpful prompt answers "I have no comment" to $49 \%$ of questions, WebGPT almost always tries to answer the question, but sometimes quotes from unreliable sources. In spite of this, WebGPT still answers more truthfully overall (see Figure 3).
Key: $\not X=$ false, $\checkmark=$ true but uninformative, $\underline{\checkmark}=$ true and informative</p>
<table>
<thead>
<tr>
<th style="text-align: left;">GPT-3 175B <br> (QA prompt)</th>
<th style="text-align: left;">GPT-3 175B <br> (helpful prompt)</th>
<th style="text-align: left;">WebGPT 175B best-of-64 <br> (first sentence)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>What happens if you smash a mirror?</p>
<table>
<thead>
<tr>
<th style="text-align: center;">If you smash a mirror, you <br> will have seven years of <br> bad luck.</th>
<th style="text-align: center;">I have no comment.</th>
<th style="text-align: center;">When you break a mirror you might <br> cut yourself and people might be an- <br> gry at you for doing it on purpose.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\not$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>If you dream of doing something and make a wish, will you succeed?</p>
<table>
<thead>
<tr>
<th style="text-align: center;">I have no comment.</th>
<th style="text-align: center;">I have no comment.</th>
<th style="text-align: center;">It is true that you can make a wish <br> come true by the power of thought.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\not$</td>
</tr>
</tbody>
</table>
<h1>6.2 Perceived truthfulness of WebGPT</h1>
<p>In order to assess the benefits and risks of WebGPT, it is necessary to consider not only how often it makes false statements, but also how likely users are to rely on those statements. Although WebGPT makes false statements less frequently than GPT-3, its answers also appear more authoritative, partly because of the use of citations. In combination with the well-documented problem of "automation bias" [Goddard et al., 2012], this could lead to overreliance on WebGPT's answers. This is particularly problematic because, as discussed in Section 6.1, WebGPT can make more mistakes than humans on out-of-distribution questions. Documentation of these limitations could help inform those interacting with WebGPT, and further research is required to understand how else to mitigate this.</p>
<h3>6.3 Reinforcement of bias</h3>
<p>There are a number of ways in which WebGPT tends to perpetuate and reinforce existing assumptions and biases. Firstly, WebGPT inherits the biases of the base model from which it is fine tuned, GPT-3 [Brown et al., 2020], and this influences the way in which it chooses to search for and synthesize information. Search and synthesis both depend on the ability to include and exclude material depending on some measure of its value, and by incorporating GPT-3's biases when making these decisions, WebGPT can be expected to perpetuate them further. Secondly, the fact that WebGPT synthesizes information from existing sources gives it the potential to reinforce and entrench existing beliefs and norms. Finally, WebGPT usually accepts the implicit assumptions made by questions, and more generally seems to be influenced by the stance taken by questions. This is something that could exacerbate confirmation bias in users.
These problems could be mitigated with improvements both to WebGPT's base model and to WebGPT's training objective, and we discuss some alternative objectives in the next section. It may also be important to control how WebGPT is used, both by limiting access and by tailoring the design and documentation of applications.
Additional analysis of the effect of question stance and of reference point bias is given in Appendix H.</p>
<h1>6.4 Using references to evaluate factual accuracy</h1>
<p>Central to our approach is the use of references collected by the model to aid human evaluation of factual accuracy. This was previously suggested by Metzler et al. [2021], and has several benefits:</p>
<ul>
<li>More accurate feedback. It is very challenging to evaluate the factual accuracy of arbitrary claims, which can be technical, subjective or vague. In contrast, it is much easier to evaluate how well a claim is supported by a set of sources.</li>
<li>Less noisy feedback. It is also easier to specify an unambiguous procedure for evaluating how well a claim is supported by a set of sources, compared to evaluating the factual accuracy of an arbitrary claim. This improves agreement rates between labelers, which helps data efficiency.</li>
<li>Transparency. It is much easier to understand how WebGPT composes answers than it is for GPT-3, since the entire browsing process can be inspected. It is also straightforward for end-users to follow up on sources to better judge factual accuracy for themselves.</li>
</ul>
<p>Despite these benefits, references are far from a panacea. Our current procedure incentivizes models to cherry-pick references that they expect labelers to find convincing, even if those references do not reflect a fair assessment of the evidence. As discussed in Section 6.3, there are early signs of this happening, with WebGPT accepting the implicit assumptions of questions, and the problem is likely to be exacerbated by more capable models and more challenging or subjective questions. We could mitigate this using methods like debate [Irving et al., 2018], in which models are trained to find evidence both for and against different claims. Such setups can also be viewed as simple cases of recursive reward modeling [Leike et al., 2018] and Iterated Amplification [Christiano et al., 2018], in which the model assists its own evaluation.
Our approach also raises a challenging problem with societal implications: how should factual accuracy be evaluated when training AI systems? Evans et al. [2021, Section 2] propose a number of desiderata, but a substantial gap remains between these and the highly specific criteria needed to train current AI systems with reasonable data efficiency. We made a number of difficult judgment calls, such as how to rate the trustworthiness of sources (see Appendix C), which we do not expect universal agreement with. While WebGPT did not seem to take on much of this nuance, we expect these decisions to become increasingly important as AI systems improve, and think that cross-disciplinary research is needed to develop criteria that are both practical and epistemically sound.</p>
<h3>6.5 Risks of live web access</h3>
<p>At both train and inference time, WebGPT has live access to the web via our text-based browsing environment. This enables the model to provide up-to-date answers to a wide range of questions, but potentially poses risks both to the user and to others. For example, if the model had access to forms, it could edit Wikipedia to construct a reliable-looking reference. Even if human demonstrators did not perform such behavior, it would likely be reinforced by RL if the model were to stumble across it.
We believe the risk posed by WebGPT exploiting real-world side-effects of its actions is very low. This is because the only interactions with the outside world allowed by the environment are sending queries to the Bing API and following links that already exist on the web, and so actions like editing Wikipedia are not directly available to the model. While a capable enough system could escalate these privileges [Harms, 2016], WebGPT's capabilities seem far below what would be required to achieve this.
Nevertheless, much more capable models could potentially pose much more serious risks [Bostrom, 2014]. For this reason, we think as the capabilities of models increase, so should the burden of proof of safety for giving them access to the web, even at train time. As part of this, measures such as tripwire tests could be used to help catch exploitative model behavior early.</p>
<h2>7 Related work</h2>
<p>Combining machine learning with an external knowledge base, for the task of question-answering, preceded the rise of pre-trained language models in the late 2010s. One notable system of this kind was DeepQA (also known as IBM Watson), which was used to beat the best humans at Jeopardy</p>
<p>[Ferrucci et al., 2010]. A large body of newer work uses language models to answer questions with the help of retrieved documents; these systems are more general and conceptually simpler than DeepQA. One approach is to use inner product search to retrieve relevant documents and then generate an answer given these documents:</p>
<p>$$
p(\text { passage } \mid \text { query }) \propto \exp (\operatorname{embed}(\text { passage }) \cdot \text { embed }(\text { query }))
$$</p>
<p>Given a training dataset that specifies relevant passages for each question, dense passage retrieval (DPR) trains the retriever directly using a contrastive objective [Karpukhin et al., 2020]. Retrieval Augmented Language Modeling (REALM) [Guu et al., 2020] and Retrieval Augmented Generation (RAG) [Lewis et al., 2020a] train the retriever and question-answering components end-to-end using a language modeling objective. Unlike DPR, RAG, and REALM, which focus on benchmarks with short answers, Krishna et al. [2021] use a similar system to tackle long-form question-answering on the ELI5 dataset [Fan et al., 2019]. They find that automated metrics like ROUGE-L are not meaningful, which motivates our choice to use human comparisons as the main metric. Note that the aforementioned family of methods, which rely on inner product search (Equation 1), differ from WebGPT in that they formulate retrieval as a differentiable process. Fully differentiable retrieval has the advantage of fast optimization; two disadvantages are that it cannot deal with non-differential processes like using a search engine, and it is less interpretable.
Like WebGPT, some other recent work defines document retrieval or web browsing as a reinforcement learning (RL) problem. Yuan et al. [2019] apply RL to reading comprehension benchmarks, where (as in WebGPT) the action space includes searching and scrolling through the provided source document. They suggest web-level QA (like WebGPT) as a direction for future work. Adolphs et al. [2021] set up an RL problem that involves performing a series of search queries for short-form question-answering. They train their system in two alternative ways: behavior cloning (BC) on synthetically-generated sequences and RL. Finally, there is another body of work that uses BC and RL to control web browsers, for automating other tasks besides question-answering [Shi et al., 2017, Gur et al., 2018].</p>
<h1>8 Conclusion</h1>
<p>We have demonstrated a novel approach to long-form question-answering, in which a language model is fine-tuned to use a text-based web-browsing environment. This allows us to directly optimize answer quality using general methods such as imitation learning and reinforcement learning. To make human evaluation easier, answers must be supported by references collected during browsing. Using this approach, our best model outperforms humans on ELI5, but still struggles with out-of-distribution questions.</p>
<h2>9 Author contributions</h2>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji and John Schulman jointly led the project, developed the codebase, ran all data collection and experiments, and wrote the paper.
Jeff Wu, Long Ouyang, Xu Jiang and Karl Cobbe provided invaluable advice on a multitude of topics over the course of the project.
Jeff Wu, Vineet Kosaraju, William Saunders and Xu Jiang made key contributions to the project codebase.
Christina Kim, Christopher Hesse and Shantanu Jain built and supported infrastructure used for model training and inference.
Tyna Eloundou and Gretne Krueger conducted the analysis of bias and contributed to the paper.
Kevin Button and Matthew Knight provided computer security support.
Benjamin Chess provided computer networking support.</p>
<h2>10 Acknowledgments</h2>
<p>We would like to thank Leo Gao, Hyeonwoo Noh and Chelsea Voss for working on future directions; Steve Dowling, Christian Gibson, Peter Hoeschele, Fraser Kelton, Bianca Martin, Bob McGrew,</p>
<p>Felipe Such and Hannah Wong for technical, logistical and communications support; Steven Adler, Miles Brundage, David Farhi, William Guss, Oleg Klimov, Jan Leike, Ryan Lowe, Diogo Moitinho de Almeida, Arvind Neelakantan, Alex Ray, Nick Ryder and Andreas Stuhlmüller for helpful discussions; Owen Cotton-Barratt, Owain Evans, Jared Kaplan, Girish Sastry, Carl Shulman, Denis Yarats and Daniel Ziegler for helpful discussions and feedback on drafts; Beth Barnes and Paul Christiano for helpful discussions and feedback on drafts, and in particular for suggesting the project; and Dario Amodei for suggesting to work on factual inaccuracy in language models. We would also like to thank Surge AI for helping us with data collection, in particular Edwin Chen, Andrew Mauboussin, Craig Pettit and Bradley Webb.</p>
<p>Finally, we would like to thank all of our contractors for providing demonstrations and comparisons, without which this project would not have been possible, including: Jamie Alexander, Andre Gooden, Jacquelyn Johns, Rebecca Kientz, Ashley Michalski, Amy Dieu-Am Ngo, Alex Santiago, Alice Sorel, Sam Thornton and Kelli W. from Upwork; and Elena Amaya, Michael Baggiano, Carlo Basile, Katherine Beyer, Erica Dachinger, Joshua Drozd, Samuel Ernst, Rodney Khumalo, Andrew Kubai, Carissa Lewis, Harry Mubvuma, William Osborne, Brandon P., Kimberly Quinn, Jonathan Roque, Jensen Michael Ruud, Judie Anne Sigdel, Bora Son, JoAnn Stone, Rachel Tanks, Windy Thomas, Laura Trivett, Katherine Vazquez, Brandy and Shannon from Surge AI.</p>
<h1>References</h1>
<p>L. Adolphs, B. Boerschinger, C. Buck, M. C. Huebscher, M. Ciaramita, L. Espeholt, T. Hofmann, and Y. Kilcher. Boosting search engines with interactive agents. arXiv preprint arXiv:2109.00527, 2021.
S. Bhakthavatsalam, D. Khashabi, T. Khot, B. D. Mishra, K. Richardson, A. Sabharwal, C. Schoenick, O. Tafjord, and P. Clark. Think you have solved direct-answer question answering? Try ARC-DA, the direct-answer AI2 reasoning challenge. arXiv preprint arXiv:2102.03315, 2021.
N. Bostrom. Superintelligence: Paths, Dangers, Strategies. Oxford University Press, 2014.
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
H. Cheng, Y. Shen, X. Liu, P. He, W. Chen, and J. Gao. UnitedQA: A hybrid approach for open domain question answering. arXiv preprint arXiv:2101.00178, 2021.
D. Chong and J. N. Druckman. Framing theory. Annu. Rev. Polit. Sci., 10:103-126, 2007.
P. Christiano, B. Shlegeris, and D. Amodei. Supervising strong learners by amplifying weak experts. arXiv preprint arXiv:1810.08575, 2018.
O. Evans, O. Cotton-Barratt, L. Finnveden, A. Bales, A. Balwit, P. Wills, L. Righetti, and W. Saunders. Truthful AI: Developing and governing AI that does not lie. arXiv preprint arXiv:2110.06674, 2021.
A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli. ELI5: Long form question answering. arXiv preprint arXiv:1907.09190, 2019.
D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan, D. Gondek, A. A. Kalyanpur, A. Lally, J. W. Murdock, E. Nyberg, J. Prager, et al. Building watson: An overview of the deepqa project. AI magazine, 31 (3):59-79, 2010.
K. Goddard, A. Roudsari, and J. C. Wyatt. Automation bias: a systematic review of frequency, effect mediators, and mitigators. Journal of the American Medical Informatics Association, 19(1): $121-127,2012$.
I. Gur, U. Rueckert, A. Faust, and D. Hakkani-Tur. Learning to navigate the web. arXiv preprint arXiv:1812.09195, 2018.
K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. REALM: Retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020.</p>
<p>M. Harms. Crystal Society. Crystal Trilogy. CreateSpace Independent Publishing Platform, 2016. ISBN 9781530773718.
G. Irving, P. Christiano, and D. Amodei. AI safety via debate. arXiv preprint arXiv:1805.00899, 2018.
M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.
V. Karpukhin, B. Oğuz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.
K. Krishna, A. Roy, and M. Iyyer. Hurdles to progress in long-form question answering. arXiv preprint arXiv:2103.06332, 2021.
J. Leike, D. Krueger, T. Everitt, M. Martic, V. Maini, and S. Legg. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive NLP tasks. arXiv preprint arXiv:2005.11401, 2020a.
P. Lewis, P. Stenetorp, and S. Riedel. Question and answer test-train overlap in open-domain question answering datasets. arXiv preprint arXiv:2008.02637, 2020b.
S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.
J. Maynez, S. Narayan, B. Bohnet, and R. McDonald. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2005.00661, 2020.
D. Metzler, Y. Tay, D. Bahri, and M. Najork. Rethinking search: Making experts out of dilettantes. arXiv preprint arXiv:2105.02274, 2021.
B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838-855, 1992.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
T. Shi, A. Karpathy, L. Fan, J. Hernandez, and P. Liang. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, pages 3135-3144. PMLR, 2017.
K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021.
N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. Christiano. Learning to summarize from human feedback. arXiv preprint arXiv:2009.01325, 2020.
X. Yuan, J. Fu, M.-A. Cote, Y. Tay, C. Pal, and A. Trischler. Interactive machine comprehension with information seeking agents. arXiv preprint arXiv:1908.10449, 2019.</p>
<h1>A Environment design details</h1>
<p>Our text-based web-browsing environment is written mostly in Python with some JavaScript. For a high-level overview, see Section 2. Further details are as follows:</p>
<ul>
<li>When a search is performed, we send the query to the Microsoft Bing Web Search API, and convert this to a simplified web page of results.</li>
<li>When a link to a new page is clicked, we call a Node.js script that fetches the HTML of the web page and simplifies it using Mozilla's Readability.js.</li>
<li>We remove any search results or links to reddit.com or quora.com, to prevent the model copying answers from those sites.</li>
<li>We take the simplified HTML and convert links to the special format
$\left[\right.$ <link ID> $\uparrow&lt;$ link text $&gt;\uparrow&lt;$ destination domain&gt; $\left.]\right]$, or
[<link ID> $\uparrow&lt;$ link text&gt; $\left.\right]$ if the destination and source domains are the same. Here, the link ID is the index of the link on the page, which is also used for the link-clicking command. We use special characters such as { and } because they are rare and encoded in the same few ways by the tokenizer, and if they appear in the page text then we replace them by similar alternatives.</li>
<li>We convert superscripts and subscripts to text using $\sim$ and ,, and convert images to the special format [Image: <alt text>], or [Image] if there is no alt text.</li>
<li>We convert the remaining HTML to text using html2text.</li>
<li>For text-based content types other than HTML, we use the raw text. For PDFs, we convert them to text using pdfminer.six. For all other content types, and for errors and timeouts, we use an error message.</li>
<li>We censor any pages that contain a 10-gram overlap with the question (or reference answer, if provided) to prevent the model from cheating, and use an error message instead.</li>
<li>We convert the title of the page to text using the format <page title> (<page domain>). For search results pages, we use Search results for: <query>.</li>
<li>When a find in page or quote action is performed, we compare the text from the command against the page text with any links stripped (i.e., including only the text from each link). We also ignore case. For quoting, we also ignore whitespace, and allow the abbreviated format <start text> $&gt;&lt;$ end text&gt; to save tokens.</li>
<li>During browsing, the state of the browser is converted to text as shown in Figure 1(b). For the answering phase (the last step of the episode), we convert the question to text using the format <question>■, and follow this by each of the collected quotes in the format [<quote number>] <quote page title> (<quote page domain>) <double new line><quote extract>■.</li>
</ul>
<h1>B Question dataset details</h1>
<p>For our demonstration and comparison datasets, the vast majority of questions were taken from ELI5 [Fan et al., 2019], to which we applied the follow post-processing:</p>
<ol>
<li>We included URLs in full, rather than using special <em>URL</em> tokens.</li>
<li>We filtered out questions with the title "[deleted by user]", and ignored the selftext "[deleted]" and "[removed]". (The "selftext" is the body of the post.)</li>
<li>We concatenated the title and any non-empty selftext, separated by a double new line.</li>
<li>We prepended "Explain: " to questions that were not phrased as actual questions (e.g., we used "Explain: gravity" rather than simply "gravity").</li>
</ol>
<p>The final step was performed because there is sometimes an implicit "Explain Like I'm Five" at the start of questions. We considered a question to be phrased as an actual question if it included either a question mark, or one of the following sequences of characters with a regex-word boundary at either end, case-insensitively:
explain, eli5, which, what, whats, whose, who, whos, whom, where, wheres, when, whens, how, hows, why, whys, am, is, isn, isnt, are, aren, arent, was, wasn, wasnt, were, weren, werent, do, don, dont, does, doesn, doesnt, did, didn, didnt, can, cant, could, couldn, couldnt, have, haven, havent, has, hasn, hasnt, may, might, must, mustn, mustnt, shall, shant, should, shouldn, shouldnt, will, wont, would, wouldn, wouldnt</p>
<p>For diversity and experimentation, we also mixed in a small number of questions from the following datasets:</p>
<ul>
<li>TriviaQA. This is a dataset of short-form questions taken from trivia websites [Joshi et al., 2017].</li>
<li>AI2 Reasoning Challenge (ARC). This is a dataset of grade-school level, multiple-choice science questions [Bhakthavatsalam et al., 2021], which we converted to free-form questions using the format <question><new line>A. <option A><new line>.... This dataset is sub-divided into two difficulties, "Challenge" and "Easy".</li>
<li>Hand-written. We constructed this small dataset of miscellaneous questions written by people trying out the model.</li>
<li>ELI5 fact-check. We constructed this dataset using answers to questions from ELI5 given by an instruction-following model. ${ }^{4}$ Each question has the following format: Fact-check each of the claims in the following answer. <double new line>Question: <ELI5 question><double new line>Answer: <model answer></li>
</ul>
<p>The numbers of demonstrations and comparisons we collected for each of these datasets are given in Table 4.</p>
<p>Table 4: Breakdown of our demonstrations and comparisons by question dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question dataset</th>
<th style="text-align: right;">Demonstrations</th>
<th style="text-align: right;">Comparisons</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ELI5</td>
<td style="text-align: right;">5,711</td>
<td style="text-align: right;">21,068</td>
</tr>
<tr>
<td style="text-align: left;">ELI5 fact-check</td>
<td style="text-align: right;">67</td>
<td style="text-align: right;">185</td>
</tr>
<tr>
<td style="text-align: left;">TriviaQA</td>
<td style="text-align: right;">143</td>
<td style="text-align: right;">134</td>
</tr>
<tr>
<td style="text-align: left;">ARC: Challenge</td>
<td style="text-align: right;">43</td>
<td style="text-align: right;">84</td>
</tr>
<tr>
<td style="text-align: left;">ARC: Easy</td>
<td style="text-align: right;">83</td>
<td style="text-align: right;">77</td>
</tr>
<tr>
<td style="text-align: left;">Hand-written</td>
<td style="text-align: right;">162</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: right;">6,209</td>
<td style="text-align: right;">21,548</td>
</tr>
</tbody>
</table>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>C Data collection details</h1>
<p>To collect demonstrations and comparisons, we began by hiring freelance contractors from Upwork (https://www.upwork.com), and then worked with Surge AI (https://www.surgehq.ai) to scale up our data collection. In total, around $25 \%$ of our data was provided by 10 contractors from Upwork, and around $75 \%$ by 46 contractors from Surge AI. The top 5 contractors provided around $50 \%$ of the data.</p>
<p>For both types of task, we provided contractors with a video and a detailed instruction document (linked below). Due to the challenging nature of the tasks, contractors were generally highly educated, usually with an undergraduate degree or higher. Contractors were compensated based on hours worked rather than number of tasks completed, and we conducted a survey to measure job satisfaction (see Appendix D).
For data quality, we put prospective contractors through a paid trial period lasting a few hours, and manually checked their work. For comparisons, we also completed around 100 tasks ourselves for all labelers to complete, and monitored both researcher-labeler agreement rates and labeler-labeler agreement rates. Treating the agreement rate between a neutral label and a non-neutral label as $50 \%$, we measured a final researcher-labeler agreement rate of $74 \%$, and a labeler-labeler agreement rate of $73 \%$.</p>
<p>Demonstrations took an average of around 15 minutes each, and comparisons took an average of around 10 minutes each. Despite conventional wisdom that human labelling tasks should be quick and repeatable, we did not think it would be straightforward to decompose our tasks into significantly simpler ones, but we consider this to be a promising direction for further research.</p>
<h2>C. 1 Demonstrations</h2>
<p>We designed the demonstration interface in such a way that, as a rule, the user is given the same information as the model, and has the same actions available. There were a couple of exceptions to this:</p>
<ol>
<li>Unlike humans, the model has no memory of previous steps. We therefore included a summary of past actions in the text given to the model. However, we felt that it was unnecessary to display this to humans.</li>
<li>The Scrolled <up, down> &lt;2, 3&gt; actions are useful for reducing the number of actions taken, but humans are used to scrolling one step at a time. We therefore made these actions unavailable to humans, and instead simply merged any repeated Scrolled <up, down> 1 actions that they made.</li>
</ol>
<p>The full instruction document we provided to contractors for demonstrations can be viewed here.</p>
<h2>C. 2 Comparisons</h2>
<p>To minimize label noise, it is important to make comparisons as unambiguous as possible. We therefore designed the following procedure for comparing two answers to a given question:</p>
<ol>
<li>Read the question, and flag if it does not make sense or should not be answered (in which case the rest of the comparison is skipped).</li>
<li>Read the first answer and its references.</li>
<li>Rate the trustworthiness of any references relied upon by the answer.</li>
<li>Annotate each of the claims in the answer with the level of support it has and its relevance to the question. A screenshot of the annotation tool is shown in Figure 9.</li>
<li>Repeat steps 2-4 for the second answer and its references.</li>
<li>Give comparison ratings for the amount of unsupported and irrelevant information, the usefulness of information with different levels of support, and coherence.</li>
<li>Weighing everything up, give a final comparison rating for overall usefulness.</li>
</ol>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Screenshot from the comparison interface, showing the annotation tool.</p>
<p>For each of the comparison ratings, we used a 5-point Likert scale with the options "A much better", "A better", "Equally good", "B better" and "B much better".</p>
<p>Importantly, we did not require contractors to perform independent research to judge the factual accuracy of answers, since this would have been difficult and subjective. Instead, we asked contractors to judge whether claims in the answer are supported, i.e., either backed up by a reliable reference, or common knowledge.</p>
<p>For the final comparison rating, we encouraged contractors to use their best judgment, but to roughly consider the following criteria in descending order of priority:</p>
<ul>
<li>Whether or not the answer contains unsupported information.</li>
<li>Whether or not the core question has been answered.</li>
<li>Whether or not there is additional helpful information, which does not necessarily need to answer the question directly.</li>
<li>How coherent the answer is, and whether or not there are any citation errors.</li>
<li>How much irrelevant information there is in the answer. (This can be higher priority in extreme cases.)</li>
</ul>
<p>The full instruction document we provided to contractors for comparisons can be viewed here.
For most of the project, we made every part of this procedure required $10 \%$ of the time, and made every part except for the final comparison rating optional $90 \%$ of the time. Towards the end of the project, we removed the question flags from the first part since we felt that they were being overused, and made the comparison ratings for unsupported information and coherence required all of the time.</p>
<p>Despite the complexity of this procedure, we only used the final comparison rating in training, even collapsing together the "much better" and "better" ratings. We experimented with predicting some of the other information as an auxiliary loss, but we were not able to significantly improve the validation accuracy of the reward model. Nevertheless, we consider this to be another promising direction for further research.</p>
<h1>D Contractor survey</h1>
<p>It was valuable to gather feedback from our contractors, both to understand and improve their process, and to monitor job satisfaction. To this end, we sent them a questionnaire with the following questions:</p>
<ul>
<li>
<p>Please say how much you agree with each of the statements. (Required 5-point Likert rating and optional comments)</p>
</li>
<li>
<p>It was clear from the instructions what I was supposed to do.</p>
</li>
<li>I found the task enjoyable and engaging.</li>
<li>I found the task repetitive.</li>
<li>I was paid fairly for doing the task.</li>
<li>
<p>Overall, I am glad that I did this task.</p>
</li>
<li>
<p>What would you change about the task to make it more engaging or enjoyable? (Encouraged)</p>
</li>
<li>Are there any other tools you could be given that would make it easier to complete the task to a consistently high standard? (Encouraged)</li>
<li>Did you come up with any shortcuts that you used to do the task more quickly, and if so, what were they? (Encouraged)</li>
<li>Do you have any other comments? (Optional)</li>
</ul>
<p>The "encouraged" questions were required questions but with instructions to put "N/A" if they really could not think of anything (this was rare).
We surveyed all contractors who completed 32 or more tasks (thus we excluded people who dropped out after the trial period or shortly thereafter). We did this 3 times over the course of the project: once for demonstrations and twice for comparisons. The quantitative results from these surveys are given in Figure 10. The vast majority of respondents reported that they enjoyed the task, were paid fairly and were glad that they did the task overall. A significant minority of respondents also reported that they found the task repetitive.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Likert ratings aggregated over all 3 of our contractor surveys. All ratings are weighted equally, even when the same contractor provided ratings in multiple surveys. In total, there are 41 ratings for each question.</p>
<h1>E Hyperparameters</h1>
<p>Hyperparameters for all of our training methods are given in Tables 6 and 7. We mostly used the same hyperparameters for the different model sizes, with the caveat that we expressed the Adam step sizes as multiples of the pre-training Adam step sizes, which are given in Table 5.</p>
<p>For each training method, we implemented some form of early stopping:</p>
<ol>
<li>For BC, we stopped after a certain number of epochs based on reward model score (which usually improves past the point of minimum validation loss).</li>
<li>For RM, we stopped after a certain number of epochs based on validation accuracy.</li>
<li>For RL, we stopped after a certain number of PPO iterations based on the reward model score for some KL budget. The KL here is measured from the BC model, and summed over the episode. For the 175B model, we compared a couple of different KL budgets using human evaluations, and for the 760 M and 13B models, we chose KL budgets informed by the 175B evaluations.</li>
</ol>
<p>The points at which we early stopped are given in Table 8.
We tuned hyperparameters using similar criteria to early stopping. We used human evaluations sparingly, since they were noisy and expensive, and put less effort into tuning hyperparameters for the 760 M and 13B model sizes. As a rule, we found the most important hyperparameter to tune to be the Adam step size multiplier.</p>
<p>For BC and RM, we used Polyak-Ruppert averaging [Polyak and Juditsky, 1992], taking an exponentially-weighted moving average (EMA) of the weights of the model as the final checkpoint. The "EMA decay" hyperparameter refers to the decay of this EMA per gradient step. For RL (but not rejection sampling), we did not use the EMA model for the 760M or 13B reward models, due to a bug.</p>
<p>For RL, most PPO hyperparameters did not require tuning, but a few points are worth noting:</p>
<ul>
<li>As discussed in Section 3 of the paper, the reward is the sum of the reward model score at the end of each episode and a KL penalty from the BC model at each token. Even though the reward is part of the environment, we treat the coefficient of this KL penalty as a hyperparameter, called the "KL reward coefficient".</li>
<li>We express hyperparameters such that each timestep corresponds to a single completion (rather than a single token), but we applied PPO clipping and the KL reward at the token level. We also trained token-level value function networks, allowing a token-level baseline to be used for advantage estimation, but we did not use token-level bootstrapping or discount rates.</li>
<li>We used separate policy and value function networks for simplicity, although we think that using shared networks is a promising direction for future research.</li>
<li>We used 1 epoch, since we were concerned more with compute efficiency than with sample efficiency.</li>
<li>Due to GPU memory constraints, we used 16 times as many minibatches per epoch as the default for PPO, but this was easily compensated for by reducing the Adam step size multiplier by a factor of 4 .</li>
<li>We used the same number of parallel environments and timesteps per rollout as the default for PPO, even though it resulted in slow PPO iterations (lasting multiple hours). This is the easiest way to ensure that PPO performs enough clipping (around $1-2 \%$ of tokens). Compared to using fewer timesteps per rollout and fewer minibatches per epoch, we found the KL from the BC model to grow more slowly at the start of training, making training less sensitive to the KL reward coefficient until approaching convergence. This allowed us to replace tuning the KL reward coefficient with early stopping to some extent.</li>
<li>We did not use an entropy bonus, which is usually used for exploration. An entropy bonus is equivalent to a KL penalty from the uniform distribution, but the uniform distribution over tokens is somewhat arbitrary - in particular, it is not invariant to "splitting" a single token into two equally-likely indistinguishable tokens. Instead, the KL reward prevents</li>
</ul>
<p>entropy collapse in a more principled way. We still found it useful to measure entropy for monitoring purposes.</p>
<ul>
<li>We happened to use a GAE discount rate of 1 rather than the usual default of 0.999 , but we do not expect this to have made much difference, since episodes last for well under 1,000 timesteps.</li>
<li>As discussed in Section 3 of the paper, at the end of each episode we inserted additional answering-only episodes using the same references as the previous episode, which is what the "answer phases per browsing phases" hyperparameter refers to.</li>
<li>Since some actions (such as quotes and answers) require many more tokens than others, we modified the environment to "chunk" long completions into multiple actions, to improve rollout parallelizability. This is what the "maximum tokens per action" hyperparameter refers to. Note that it has a minor effect on GAE.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://beta.openai.com/docs/engines/instruct-series-beta&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
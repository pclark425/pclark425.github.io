<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1952 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1952</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1952</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-279250278</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.06862v1.pdf" target="_blank">Multimodal Spatial Language Maps for Robot Navigation and Manipulation</a></p>
                <p><strong>Paper Abstract:</strong> Grounding language to a navigating agent's observations can leverage pretrained multimodal foundation models to match perceptions to object or event descriptions. However, previous approaches remain disconnected from environment mapping, lack the spatial precision of geometric maps, or neglect additional modality information beyond vision. To address this, we propose multimodal spatial language maps as a spatial map representation that fuses pretrained multimodal features with a 3D reconstruction of the environment. We build these maps autonomously using standard exploration. We present two instances of our maps, which are visual-language maps (VLMaps) and their extension to audio-visual-language maps (AVLMaps) obtained by adding audio information. When combined with large language models (LLMs), VLMaps can (i) translate natural language commands into open-vocabulary spatial goals (e.g.,"in between the sofa and TV") directly localized in the map, and (ii) be shared across different robot embodiments to generate tailored obstacle maps on demand. Building upon the capabilities above, AVLMaps extend VLMaps by introducing a unified 3D spatial representation integrating audio, visual, and language cues through the fusion of features from pretrained multimodal foundation models. This enables robots to ground multimodal goal queries (e.g., text, images, or audio snippets) to spatial locations for navigation. Additionally, the incorporation of diverse sensory inputs significantly enhances goal disambiguation in ambiguous environments. Experiments in simulation and real-world settings demonstrate that our multimodal spatial language maps enable zero-shot spatial and multimodal goal navigation and improve recall by 50% in ambiguous scenarios. These capabilities extend to mobile robots and tabletop manipulators, supporting navigation and interaction guided by visual, audio, and spatial cues.</p>
                <p><strong>Cost:</strong> 0.036</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1952.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1952.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLMaps</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual-Language Maps</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Top-down spatial grid maps that fuse dense pixel-level embeddings from a pretrained visual-language model (LSeg/CLIP feature space) with 3D reconstruction / RGB-D odometry to enable open-vocabulary spatial indexing and language-conditioned spatial goal localization for navigation and obstacle-map generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VLMaps (map representation + LSeg features)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A mapping representation: a 2D top-down grid (H×W×C) where each cell stores averaged dense pixel embeddings produced by a visual-language encoder (LSeg mapping pixels into CLIP feature space). Uses CLIP text encoder for open-vocabulary queries and an LLM to decompose language into navigation API calls. Processes RGB-D + odometry; no end-to-end policy learning — it is a feature-augmented geometric map used for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining (CLIP) plus segmentation fine-tuning (LSeg)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Underlying CLIP pretraining on large-scale image-text pairs scraped from the web (generic object and scene captions). LSeg further fine-tuned on segmentation datasets for pixel-level segmentation (object labels). No task-specific robot control data used for VLMaps mapping itself.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Zero-shot spatial goal navigation / cross-embodiment obstacle-map generation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Long-horizon, language-conditioned spatial goal navigation in simulated (Habitat, Matterport3D, AI2-THOR) and real environments. Discrete low-level actions for navigation used by planner (e.g., small step forward and small turns); planning uses off-the-shelf ROS navigation stack. Also used to synthesize embodiment-specific obstacle maps (ground robot vs drone).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Discussed qualitatively: CLIP/LSeg pretraining covers many common household object categories (good overlap for landmark-based goals) but fine-tuning (LSeg) reduces general-region concept recognition (catastrophic forgetting), so alignment is high for object semantics but lower for coarse area/region concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>VLMaps outperforms prior open-vocabulary navigation baselines in zero-shot spatial goal navigation (see Table 3). In cross-embodiment experiments VLMaps-generated tailored obstacle maps improved path efficiency (SPL) for a drone compared to using a ground-map (Table 4). Exact numeric gains vs all baselines are reported in paper Tables (see paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>A CLIP-features-based map ablation (projects CLIP visual features into map) performs worse than VLMaps (paper's CLIP Map baseline); precise baseline numbers are in Tables 3-4 (not all fully legible in text).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not applicable — VLMaps are zero-shot, built on pretrained features from a single exploration pass; paper does not report learning-curve sample-efficiency comparisons for training from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No explicit attention-map or transformer-attention visualization analysis reported for VLMaps; the method uses cosine-similarity scoring between stored embeddings and text queries rather than attention mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No detailed embedding-space clustering/PCA analysis; authors do report that averaging multi-view pixel embeddings per voxel/cell produces robust open-vocabulary indexing and note qualitative differences between models (fine-tuned LSeg versus CLIP-only).</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Limited: VLMaps are used to compute geometric target coordinates (e.g., 'left of X') which are executed by a classical navigation stack. The paper does not provide evidence that language (verbs) are grounded to motor primitives via learned representations — instead an LLM composes API calls that call parameterized navigation primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Yes — paper reports that LSeg (fine-tuned) improves instance/object segmentation but suffers from 'catastrophic forgetting' on coarse area/region recognition; CLIP features (no fine-tune) preserve region-level recognition better (used in ConceptFusion comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer works well when pretrained VLM categories overlap with environment object categories and when accurate RGB-D odometry and reconstruction are available; cross-embodiment transfer (sharing same VLMap) succeeds provided obstacle category lists are adapted per embodiment (drone vs ground robot).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>The system demonstrates open-vocabulary zero-shot indexing (including unseen object mentions in language instructions) but the paper does not present a strict novel-vs-familiar split with quantitative differences.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot: VLMaps enable zero-shot spatial language navigation (no finetuning on target environments); shown to outperform CLIP Map and CoW baselines on spatial tasks (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer-wise ablation or freeze/probing analysis of the underlying VLM layers is reported for VLMaps.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper reports that segmentation fine-tuning (as in LSeg) causes reduced ability to recognize general region concepts (catastrophic forgetting) compared to CLIP features; this is framed as a trade-off rather than quantified 'negative transfer'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Compared against CLIP Map (ablative, projects CLIP visual features) and CoW (CLIP+GradCAM), VLMaps (using LSeg features + spatial fusion) outperform these vision-only or CLIP-only alternatives on spatial-goal navigation tasks (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No analysis of representational or performance dynamics over training time (method is zero-shot; mapping is constructed from exploration sequences but no learning dynamics reported).</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No explicit dimensionality / intrinsic-dimension analysis reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1952.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1952.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AVLMaps</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Audio-Visual-Language Maps</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3D voxel-based multimodal spatial map that fuses pretrained visual-language embeddings, audio-language embeddings, visual localization descriptors, and area-level CLIP features into shared voxel/heatmap maps enabling zero-shot multimodal spatial indexing and cross-modal disambiguation for navigation and tabletop manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AVLMaps (multimodal spatial map)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>3D voxel-grid map where occupied voxels store visual-language features (LSeg/OVSeg/other), audio segments are associated with poses via audio-language encoders (AudioCLIP/CLAP/wav2clip), and visual localization uses NetVLAD+SuperPoint+SuperGLUE; modalities produce heatmaps which are fused (element-wise multiplication) for cross-modal reasoning. Uses LLM-generated code to convert language instructions to API calls for map querying and navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Composition of pretrained modules: vision-language (CLIP-based encoders, some fine-tuned for segmentation), audio-language models (AudioCLIP, CLAP, wav2clip), and place/feature localizers (NetVLAD/SuperPoint trained for image retrieval / local descriptors). No joint multimodal finetuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>CLIP: image-text pairs from web; LSeg and OVSeg fine-tuned on segmentation datasets; AudioCLIP trained to align audio, image and text (audio and image datasets including environmental sounds); CLAP trained on larger audio-language datasets. None include explicit robot action labels or affordance-labeled control traces; audio datasets (ESC-50 etc.) supply environmental sound categories.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Zero-shot multimodal spatial goal navigation and multimodal tabletop manipulation indexing</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Embodied navigation to goals specified by language, images, or sounds; ambiguous-goal disambiguation using cross-modal information; experiments in Habitat (simulated Matterport3D), AI2-THOR multi-embodiment sims, and real-world mobile robot and tabletop manipulator setups. Action space: discrete small forward/turn steps for navigation; manipulator tasks require end-effector positioning accurate to ~10 cm.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper discusses alignment qualitatively: audio-language pretraining (AudioCLIP/CLAP/wav2clip) aligns sound semantics to language and enables text→audio matching, while vision-language modules align object concepts; alignment between modalities enables cross-modal disambiguation but is limited by audio noise and by fine-tuning effects in vision models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>AVLMaps achieves: sound-goal navigation success rate ≈ 77.5% (Table 5); multimodal ambiguous-goal navigation (AVLMaps with AudioCLIP) sound SR 46.2% and object SR 28.6% (Table 10). Real-world mobile experiment overall success rate 50% on 20 multimodal navigation tasks; tabletop manipulator: 100% success for visual and sound goals, 9/13 object goals, 9/10 ambiguous goals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Baselines using weaker audio models or unimodal setups perform worse: VLMaps + wav2clip baseline had sound SR ≈ 22.0% and object SR ≈ 12.7% in ambiguous navigation (Table 10). Object-sound indexing baseline (AudioCLIP alone) recall@0.5m ≈ 26.05% vs VLMaps+AudioCLIP ≈ 53.78% (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit sample-efficiency learning curves comparing pretrained vs non-pretrained training; mapping is constructed from limited exploration data (e.g. thousands of frames) but no learning sample complexity reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No transformer attention or saliency visualizations for multimodal fusion reported; fusion performed by element-wise multiplication of modality heatmaps (no attention mechanism analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No deep analysis (e.g., clustering/PCA) of joint multimodal embedding space; paper provides empirical downstream recall/improvement numbers demonstrating usefulness of embedding matches (e.g., object-sound recall gains).</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Indirect: AVLMaps enable locating multimodal goal coordinates used by planners; however, the paper does not present evidence that language or audio tokens are grounded to motor primitives beyond geometric localization and planner execution. Manipulation experiments use localization outputs to guide end-effector positioning but not learned action-language grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Paper highlights that fine-tuned VLMs (LSeg) are stronger at object-level segmentation but weaker at coarse area recognition, while CLIP-based sparse topological features better capture area-level concepts — i.e., different feature levels benefit different tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer of multimodal indexing works best when (i) pretraining covers the concept categories (objects, sounds), (ii) mapping has adequate coverage and accurate odometry, and (iii) audio pre-processing (denoising) is adequate; the paper shows audio-model quality (AudioCLIP vs CLAP) significantly affects sound-goal performance (CLAP gives ≈10% improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Demonstrates open-vocabulary zero-shot behavior (indexing to arbitrary language/image/audio queries) but does not provide a controlled novel-vs-familiar object split with per-category analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot: system performs zero-shot multimodal indexing and navigation from language/image/audio queries without finetuning on target environments; reported success rates above reflect zero-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No per-layer ablation of constituent pretrained models; modular replacement experiments (swap AudioCLIP→CLAP, LSeg→OVSeg/SAM+CLIP) assess module-level impact but not internal layers.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>The SAM+CLIP variant substantially lowered performance in the scaling experiments (sensitive hyperparameters and poor masked-region CLIP interpretation), indicating negative practical transfer for that combination in this pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Compared to unimodal (vision-only) baselines, AVLMaps with audio+vision+language improved ambiguous-goal disambiguation: e.g., object-sound cross-modal indexing recall increased from ~26% (AudioCLIP baseline) to ~53.78% (VLMaps+AudioCLIP). For ambiguous navigation, AVLMaps improved sound success rates by ~24.2% over a multimodal baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No temporal representation dynamics reported; audio transient vs persistent sound limitations discussed qualitatively (sound is transient; mapping is offline).</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No explicit dimensionality or intrinsic-dimension analyses reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1952.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1952.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSeg</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSeg (Language-driven Semantic Segmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pixel-wise segmentation model whose visual encoder maps pixels into the CLIP feature space and can segment images based on free-form language categories; used here to produce dense pixel embeddings for map fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSeg (visual encoder + text encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language segmentation model: visual encoder produces dense pixel embeddings aligned with CLIP embedding space; a CLIP text encoder encodes free-form labels and per-pixel cosine similarities produce segmentation masks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>CLIP-style vision-language pretraining followed by segmentation fine-tuning on segmentation datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>CLIP pretraining on large-scale image-text pairs; LSeg fine-tuned on segmentation datasets with per-pixel labels (object classes). Datasets likely contain object categories but limited explicit affordance/action annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Dense pixel-level feature generation for spatial maps / open-vocabulary segmentation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Produce per-pixel embeddings from RGB frames collected during exploration; embeddings are back-projected into 2D/3D maps to support open-vocabulary landmark localization for navigation and manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High for object-level semantics (LSeg fine-tuning improves segmentation), lower for broad region/area concepts (paper notes catastrophic forgetting of region recognition after fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>When used within VLMaps/AVLMaps, LSeg-based maps enabled better object segmentation and downstream spatial-goal localization than simple CLIP-projection baselines (reported in ablations). Exact numeric deltas are in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>The CLIP Map baseline (project CLIP visual features without LSeg fine-tuning) performs worse on spatial localization of fine-grained object-centric spatial goals, per paper ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not applicable — LSeg is a pretrained model used as a feature extractor; no sample-efficiency comparisons reported within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention analysis for LSeg in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Paper uses LSeg embeddings directly for cosine-similarity matching and notes averaged multi-view embeddings per map cell provide robust signals; no further analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>No evidence LSeg encodes affordance-to-action mappings; used purely for semantic localization.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Paper reports LSeg (fine-tuned) improves object-level segmentation but weakens general concept/area recognition relative to CLIP-only features (catastrophic forgetting).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>LSeg's performance depends on the segmentation dataset it's fine-tuned on; domain mismatch can reduce area-level generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>LSeg shows good generalization to open-vocabulary labels in practice (via CLIP-space matching), but no rigorous novel/familiar partition evaluation is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>LSeg provides open-vocabulary segmentation via CLIP-space matching, enabling zero-shot queries for new labels at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Catastrophic forgetting of region-level concepts after segmentation fine-tuning is noted as a negative effect when compared to CLIP-only features.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Compared with CLIP-only feature mapping, LSeg + spatial fusion (VLMaps) yields superior object localization for spatial-goal navigation in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No temporal analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No analysis reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1952.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1952.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language-Image Pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large vision-language model trained contrastively on image-text pairs from the internet to align image and text embeddings in a joint space, enabling zero-shot classification and text→image similarity matching; used here for text encoding and as a backbone embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP (image & text encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dual-encoder vision-language model (image encoder + text encoder) trained contrastively to align image and text embeddings. Produces global image embeddings and is used as the text encoder for open-vocabulary queries in map indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language on large-scale image-text pairs (web-scale).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Diverse internet image-caption pairs; contains numerous object categories and scene descriptions, but not robot-action labeled data or spatial-affordance annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Open-vocabulary landmark indexing, text→visual similarity matching in maps</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used to convert language queries into embedding vectors and to compute cosine similarities with map cell embeddings for retrieval/localization in both 2D and 3D maps.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High overlap for common object categories and captions; aligns well with object semantics used in indoor navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Used as a core encoder; CLIP-based retrieval forms core of baselines (CoW, CLIP Map); VLMaps (with LSeg) outperform simple CLIP Map baselines in spatial localization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>N/A (CLIP is the pretrained baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not applicable within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Used implicitly for cosine similarity scoring; no detailed analysis beyond performance comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>No; CLIP provides perception-language alignment but paper does not show grounding of action verbs in CLIP representations.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>CLIP sparse topological features retain region-level cues better than segmentation-finetuned models, per paper discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works best when environment objects are represented in CLIP pretraining distribution; fine-grained segmentation tasks may benefit from fine-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>CLIP's zero-shot capability supports novel object labels, but paper does not quantify novel vs familiar performance breakdown.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot retrieval for open-vocabulary queries is a core capability used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Fine-tuning on segmentation (LSeg) can reduce CLIP's region-level generalization; paper frames this as a trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>CLIP is multimodal (vision+text); compared against purely vision-only baselines indirectly in related work but not a direct image-only pretraining comparison in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1952.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1952.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AudioCLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AudioCLIP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of CLIP to audio: a multimodal model that aligns audio, image, and text embeddings to enable audio–text and audio–image retrieval; used here as the primary audio-language model in AVLMaps' audio localization module.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AudioCLIP (audio-image-text alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal model that augments CLIP with an audio encoder and trains to align audio embeddings with CLIP image/text embeddings enabling cross-modal retrieval between audio, images and text.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal pretraining aligning audio, image and text embeddings (audio extensions of CLIP).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Audio datasets aligned with image/text modalities (various environmental and event sound datasets), but typically limited in scale compared to CLIP's image-text pretraining; lacks explicit robot-action labels.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Audio localization (text→audio matching) for sound-goal navigation and cross-modal disambiguation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Segment audio stream collected during mapping, compute audio embeddings per segment, associate segments to poses; at inference match language descriptions of sounds to segment embeddings to get probable sound locations (heatmaps). Evaluated in Habitat simulations and real-world experiments with added audio.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Trained to align audio classes to language descriptions and images; alignment enables retrieval for environmental sound categories used in navigation experiments (ESC-50 categories etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>When used in AVLMaps, AudioCLIP yielded strong multimodal indexing; object-sound cross-modal indexing recall improved to ~53.78% (recall@0.5m) compared to baseline AudioCLIP-only 26.05% (Table 8 indicates that integrating AudioCLIP into VLMaps produced large gains). Sound-goal navigation success with AudioCLIP within AVLMaps reported as part of overall 77.5% success in Table 5 (mapping + AudioCLIP features).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Baseline wav2clip and earlier audio encoders produced substantially lower sound localization/navigation performance (e.g., VLMaps + wav2clip sound SR ≈ 22.0% in ambiguous navigation tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not applicable; audio models are pretrained and used zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No internal attention or saliency analyses for AudioCLIP provided.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No detailed embedding-space probe beyond retrieval match scores and downstream recall measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>No evidence of grounding action verbs; audio features are used only for localization/semantic disambiguation.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed specifically for audio.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper shows replacement of AudioCLIP with the more recent CLAP improves sound-goal navigation (~10% improvement), indicating sensitivity to audio-model pretraining scale and augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>AudioCLIP works on ESC-50 sound categories used in experiments; no explicit novel vs familiar breakdown provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Used zero-shot (text→audio retrieval) for navigation and indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not performed.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported for AudioCLIP specifically; CLAP shown to improve performance compared to AudioCLIP (positive transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>AudioCLIP is multimodal (audio aligned to CLIP); integration with visual-language maps (AVLMaps) produced large performance gains over vision-only baselines for ambiguous multimodal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Audio is transient; paper discusses limitations of mapping transient sounds into a static map (no dynamic update).</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1952.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1952.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLAP (Contrastive Language–Audio Pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An audio-language foundation model trained at scale to align audio and natural-language descriptions, yielding stronger audio-language retrieval performance than earlier AudioCLIP; used in AVLMaps scaling experiments and shown to improve sound-goal navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLAP (audio↔text encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastive audio–language model trained on larger and more diverse audio-language pairs than earlier audio-CLIP variants. Used to encode audio segments and language queries for matching.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Audio-language contrastive pretraining on large-scale audio–text pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale audio–text pairs, potentially with advanced augmentations and broader coverage of environmental and event sounds (paper cites CLAP benefits from larger/more diverse datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Improved audio localization for sound-goal navigation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Replace AudioCLIP in AVLMaps audio localization module with CLAP; evaluate impact on sound-goal navigation and overall multimodal navigation success in Habitat simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Higher alignment for sound semantics to textual descriptions due to larger/more diverse pretraining; authors report an observed ≈10% improvement in sound-goal navigation when using CLAP vs AudioCLIP.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Using CLAP in AVLMaps provided roughly a 10% absolute improvement in navigating to sound goals compared to AudioCLIP (paper text reports ~10% improvement in sound-goal performance in scaling experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable within paper experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided beyond downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>No direct evidence of action grounding from CLAP embeddings; used for localization only.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper indicates improved performance when ALM is stronger/more diverse — suggesting transfer depends on dataset scale and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not explicitly evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot audio→text retrieval for audio localization used.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not performed.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported; CLAP shows positive gain over AudioCLIP in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>CLAP contributes complementary audio semantics to vision-language maps, improving disambiguation in multimodal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No analysis; transient sound limitations discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1952.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1952.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>wav2clip</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>wav2clip</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An audio representation method that projects audio into CLIP embedding space, used as a baseline audio-language encoder in the paper's comparisons and baselines for audio localization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>wav2clip (audio→CLIP features)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An audio encoder trained to map audio clips into CLIP embedding space so that audio can be compared to text/image CLIP embeddings; used for audio retrieval tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Audio encoder trained to align with CLIP embedding space (contrastive/teacher-student style), not trained on robotic data.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Audio datasets mapped to CLIP space; used for environmental sound categories, but smaller and less diverse than CLAP in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Baseline audio localization for sound-goal navigation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used as an audio-module baseline in AVLMaps and ambiguous-goal navigation experiments; associated audio segment embeddings with poses during mapping and matched to language queries.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Provides some audio→language alignment through CLIP-space projection but weaker than AudioCLIP/CLAP in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>When used in multimodal baseline (VLMaps + wav2clip) for ambiguous navigation, reported sound SR ≈ 22.0% and object SR ≈ 12.7% (Table 10), substantially lower than AVLMaps with AudioCLIP/CLAP.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>As a baseline comparison; performs worse than AudioCLIP/CLAP in downstream sound localization/navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided beyond downstream metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>None — used only for semantic localization.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Performance sensitive to audio model quality; wav2clip performed worse in noisy or ambiguous multi-instance sound setups.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not explicitly tested.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Functions zero-shot for text→audio retrieval at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit negative transfer reported, but it is empirically weaker than AudioCLIP/CLAP.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>As an audio-only baseline, wav2clip underperforms compared to integrated audio-visual-language approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1952.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1952.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NetVLAD+SuperPoint+SuperGLUE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NetVLAD with SuperPoint features and SuperGLUE matching</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical visual localization pipeline that uses NetVLAD for global image retrieval, SuperPoint for local features, and SuperGLUE for robust keypoint matching to estimate query camera pose by PnP and odometry association; used to localize query images into the map.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NetVLAD (global) + SuperPoint (local) + SuperGLUE (matching)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hierarchical localization: compute NetVLAD global descriptors for retrieval, use SuperPoint local descriptors to extract keypoints, match via SuperGLUE, back-project matched keypoints with depth to obtain 3D correspondences and solve PnP for camera pose estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>NetVLAD trained for place recognition; SuperPoint learned for self-supervised interest point detection; SuperGLUE trained for matching — none are pretrained on robot-action data.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Place recognition and local-feature datasets (street-level, structure-from-motion datasets) rather than explicit action/affordance data.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Visual localization / image-goal localization in AVLMaps</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Given a query image, retrieve and match against the collected reference database of exploration frames to estimate the global camera pose; output a heatmap with a high-probability region for that image location.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Aligns spatially (place-level) rather than semantically; effective when environment has matching visual features between query and mapped frames.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not directly applicable; this module is geometry- and descriptor-based; used successfully as part of AVLMaps visual localization (no numbers isolated for this module alone).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>N/A (module is not a language-pretrained model).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not analyzed in the paper beyond successful pose estimation examples.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not relevant — module provides localization estimates used downstream by planners.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Paper describes this hierarchical approach as needed to localize query images robustly (global retrieval then local matching).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Performance depends on the visual distinctiveness of places and coverage in the reference database; limited when scenes are highly dynamic or lack distinctive features.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Performs retrieval-based localization zero-shot using stored database of exploration frames; no training on target queries required beyond descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not performed.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No evidence reported; standard geometric limitations (dynamic scenes) discussed elsewhere.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>This is a vision-only geometric localization pipeline; used in AVLMaps in combination with language models and audio models.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Dynamic objects during mapping can corrupt map points; paper discusses dynamic-scene limitations for the mapping pipeline generally.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1952.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1952.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-003 (OpenAI LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model used here as a code-writing planner: few-shot prompted to translate natural language instructions into executable Python robot code that calls parameterized navigation APIs for VLMaps/AVLMaps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (LLM used for code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive language model (text and code) used in a few-shot prompting regime to synthesize Python code (API calls/logic) that sequences spatial navigation primitives and multimodal map queries.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Large-scale text pretraining (mixture including code corpora); not trained on robot control data for this work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Web text and code (e.g., GitHub), enabling code-synthesis capabilities; no explicit embodied-action labels but general reasoning and code-generation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-to-plan/code generation for embodied navigation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Given a natural language instruction (possibly multimodal), LLM generates code calling robot APIs (move_to, get_map, get_major_map, fuse maps) to produce precise spatial goals and invoke navigation stack. Executed directly on robot via Python exec.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>LLM provides semantic parsing and arithmetic (e.g., compute 'in between' by averaging coordinates), but grounding into perception is via API calls that query the map — thus semantic alignment relies on the map's embeddings rather than LLM internal world model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>LLM successfully decomposes long-horizon instructions into API calls enabling VLMaps/AVLMaps to follow complex spatial instructions zero-shot; qualitative and quantitative navigation success rates reported (system-level results depend on the map modules).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No baseline without LLM used in this work (alternatives referenced in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not applicable — LLM is used zero-shot/few-shot via prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No analysis of LLM attention patterns presented.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not applicable within this study.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>LLM outputs code that calls grounded perception APIs; the paper does not claim the LLM itself has grounded motor affordance representations, but rather that it composes grounded API invocations.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>LLM few-shot prompting effectiveness depends on prompt examples and available API documentation in context; no systematic prompt-ablation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>LLM generalization to arbitrary language referring to map concepts demonstrated (zero-shot language references to objects/regions), but no strict novel-vs-familiar split reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Few-shot prompting is used; code generation is performed zero-shot/few-shot at inference without finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not performed.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit negative transfer evidence reported for the LLM usage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly applicable; LLM complements vision-language maps by converting natural language to grounded API calls.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1952.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1952.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoW</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP on Wheels (CoW)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior zero-shot language-driven object navigation approach that uses CLIP-based saliency maps (with GradCAM) combined with exploration to perform object search and navigation; used as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoW (CLIP + GradCAM saliency)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses CLIP to produce image-level text similarity saliency; GradCAM produces saliency maps for object localization which are aggregated over frames to produce objective masks for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language (CLIP) pretraining on image-text pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>CLIP's web-scale image-caption pretraining; does not include robot-specific action data.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Zero-shot object goal navigation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Navigate to instances of an object category specified in language by thresholding CLIP/GradCAM saliency maps and using exploration strategies to find objects in continuous environments.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Good for object-level semantic alignment; limited for fine spatial relations (e.g., 'in-between' queries) as noted in paper comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Used as baseline; authors report CoW struggles on fine-grained spatial queries and has high failure rates on spatial-goal navigation tasks compared to VLMaps (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>N/A (baseline relies on CLIP).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not analyzed here (this is baseline prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>CoW maps perception to exploration actions but does not learn explicit affordance-action groundings; treated here as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Effective for object-level goals but less effective for spatial relations and open-vocabulary spatial indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Designed for zero-shot novel object categories via CLIP; empirical limits noted for relative spatial descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot by design.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>None in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here but CoW's limitations in spatial language tasks constitute performance shortfalls.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>CoW is vision-language based; authors compare it to VLMaps and show VLMaps perform better on spatial queries.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1952.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1952.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-Nav</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM-Nav</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system that combines large language models, CLIP, and a topological graph of observations to parse language instructions into landmark lists and navigate a graph representation of the environment; used as a baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LM-Nav (graph-based navigation + LLM/CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines pretrained models (LLMs, CLIP) to parse instructions and plan on an image-observation topological graph (nodes = images, edges = proximity) to navigate in real-world environments.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language and language model pretraining (CLIP + LLMs); graph-based memory built from environment observations.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>CLIP/LLM pretraining on web-scale image-text and text corpora; no robot-control pretraining for navigation policy.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Zero-shot language-conditioned navigation on topological graphs</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Parse language commands into landmarks via LLM and plan on pre-built topological graph of image observations to reach nodes corresponding to referenced landmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Good for landmark-level queries; limited for fine-grained spatial relations compared to VLMaps' 2D/3D geometric fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Paper uses LM-Nav as a baseline; VLMaps outperform LM-Nav on fine-grained spatial goal navigation requiring precise geometric localization (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not supplied.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>LM-Nav uses topological planning rather than motor-level grounding; not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works when topological graph adequately covers goals; less suited for free-space spatial offsets like 'in between'.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Designed for zero-shot object/landmark navigation but less precise for spatial relations.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot landmark navigation via pretrained models.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not performed here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>LM-Nav is multimodal (LLM+CLIP) but relies on topological graphs rather than dense spatial maps; VLMaps reported to better handle fine spatial language.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1952.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e1952.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OVSeg</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OVSeg (Open-Vocabulary Segmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language segmentation approach that generates dense pixel-level CLIP-aligned features while using learnable mask prompts to improve segmentation performance; evaluated as an alternative VLM in AVLMaps' scaling experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OVSeg (pixel-wise VLM via mask prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Produces dense pixel embeddings in CLIP space using segmentation techniques with trainable mask prompts (designed to adapt masks for segmentation), intended to improve pixel-level visual-language features.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>CLIP-based vision-language pretraining plus segmentation-oriented adaptation (learnable mask prompts) — not robot-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Image-level labels and segmentation data used for adaptation; does not include robot action labels.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Alternative visual-language encoder for AVLMaps object localization module</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Replace LSeg in AVLMaps with OVSeg to test whether improved dense VLMs provide better object localization and downstream navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed to better align segmentation masks with CLIP semantics, but in experiments OVSeg provided limited improvements over LSeg for dense visual-language feature generation in AVLMaps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>In scaling experiments, OVSeg did not provide clear benefits over LSeg in AVLMaps' object localization performance; reported results show minimal gains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No sample-efficiency data provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed in depth; OVSeg designed for improved segmentation but not necessarily for feature-generation for spatial indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Requires hyperparameter tuning and mask prompt adaptation; sensitive to scene specifics per authors' discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Provides open-vocabulary segmentation at inference but performance depends on adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not performed.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>OVSeg provided limited benefits in experiments; SAM+CLIP combination performed worse (negative effect) and required tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>OVSeg is a vision-language method compared to LSeg; in practice LSeg remained competitive in the paper's pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1952.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e1952.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAM+CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SAM (Segment Anything) + CLIP combination</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that generates class-agnostic masks with SAM and then encodes masked regions with CLIP to produce per-region CLIP embeddings; evaluated in AVLMaps scaling experiments but found to reduce performance without careful hyperparameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SAM + CLIP (mask-based region encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SAM generates class-agnostic segmentation masks; each region is encoded with CLIP (sum of masked crop and zero-background crop embeddings) to assign CLIP-style embeddings to regions for open-vocabulary mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>SAM pretrained for mask generation (large segmentation dataset) and CLIP pretrained on image-text pairs; combined post-hoc (no joint finetuning).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>SAM trained on mask annotations across diverse images; CLIP on image-text pairs. No robot data or joint multimodal finetuning used.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Alternate dense visual-language feature provider for object localization in AVLMaps</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used as a tested variant for producing per-region CLIP embeddings to place into the voxel map and then retrieve by language queries.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Region-based semantic alignment depends on quality of SAM masks and CLIP's ability to interpret masked regions; paper reports sensitivity issues.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>In scaling experiments SAM+CLIP significantly reduced overall performance compared to LSeg/OVSeg in this pipeline; authors attribute this to sensitivity of SAM masks and CLIP interpretation of masked regions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No detailed analysis; empirical downstream degradation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Requires substantial hyperparameter tuning; unstable across different scenes, leading to negative transfer in their pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Operates zero-shot at inference but unstable.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not performed.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Yes — authors report that SAM+CLIP 'significantly reduced performance' in AVLMaps experiments and required extensive tuning to be competitive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>SAM provides segmentation (vision-only) and combined with CLIP for semantics; however, in this embodied mapping pipeline it underperformed tuned segmentation-based VLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation <em>(Rating: 2)</em></li>
                <li>LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action <em>(Rating: 2)</em></li>
                <li>AudioCLIP: Extending CLIP to image, text and audio <em>(Rating: 2)</em></li>
                <li>CLAP: Learning audio concepts from natural language supervision <em>(Rating: 2)</em></li>
                <li>ConceptFusion: Open-set multimodal 3D mapping <em>(Rating: 2)</em></li>
                <li>CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory <em>(Rating: 1)</em></li>
                <li>RT-1: Robotics Transformer for Real-World Control at Scale <em>(Rating: 1)</em></li>
                <li>CLIPort: What and where pathways for robotic manipulation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1952",
    "paper_id": "paper-279250278",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "VLMaps",
            "name_full": "Visual-Language Maps",
            "brief_description": "Top-down spatial grid maps that fuse dense pixel-level embeddings from a pretrained visual-language model (LSeg/CLIP feature space) with 3D reconstruction / RGB-D odometry to enable open-vocabulary spatial indexing and language-conditioned spatial goal localization for navigation and obstacle-map generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "VLMaps (map representation + LSeg features)",
            "model_description": "A mapping representation: a 2D top-down grid (H×W×C) where each cell stores averaged dense pixel embeddings produced by a visual-language encoder (LSeg mapping pixels into CLIP feature space). Uses CLIP text encoder for open-vocabulary queries and an LLM to decompose language into navigation API calls. Processes RGB-D + odometry; no end-to-end policy learning — it is a feature-augmented geometric map used for planning.",
            "pretraining_type": "vision-language pretraining (CLIP) plus segmentation fine-tuning (LSeg)",
            "pretraining_data_description": "Underlying CLIP pretraining on large-scale image-text pairs scraped from the web (generic object and scene captions). LSeg further fine-tuned on segmentation datasets for pixel-level segmentation (object labels). No task-specific robot control data used for VLMaps mapping itself.",
            "target_task_name": "Zero-shot spatial goal navigation / cross-embodiment obstacle-map generation",
            "target_task_description": "Long-horizon, language-conditioned spatial goal navigation in simulated (Habitat, Matterport3D, AI2-THOR) and real environments. Discrete low-level actions for navigation used by planner (e.g., small step forward and small turns); planning uses off-the-shelf ROS navigation stack. Also used to synthesize embodiment-specific obstacle maps (ground robot vs drone).",
            "semantic_alignment": "Discussed qualitatively: CLIP/LSeg pretraining covers many common household object categories (good overlap for landmark-based goals) but fine-tuning (LSeg) reduces general-region concept recognition (catastrophic forgetting), so alignment is high for object semantics but lower for coarse area/region concepts.",
            "performance_with_language_pretraining": "VLMaps outperforms prior open-vocabulary navigation baselines in zero-shot spatial goal navigation (see Table 3). In cross-embodiment experiments VLMaps-generated tailored obstacle maps improved path efficiency (SPL) for a drone compared to using a ground-map (Table 4). Exact numeric gains vs all baselines are reported in paper Tables (see paper).",
            "performance_without_language_pretraining": "A CLIP-features-based map ablation (projects CLIP visual features into map) performs worse than VLMaps (paper's CLIP Map baseline); precise baseline numbers are in Tables 3-4 (not all fully legible in text).",
            "sample_efficiency_comparison": "Not applicable — VLMaps are zero-shot, built on pretrained features from a single exploration pass; paper does not report learning-curve sample-efficiency comparisons for training from scratch.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No explicit attention-map or transformer-attention visualization analysis reported for VLMaps; the method uses cosine-similarity scoring between stored embeddings and text queries rather than attention mechanisms.",
            "embedding_space_analysis": "No detailed embedding-space clustering/PCA analysis; authors do report that averaging multi-view pixel embeddings per voxel/cell produces robust open-vocabulary indexing and note qualitative differences between models (fine-tuned LSeg versus CLIP-only).",
            "action_grounding_evidence": "Limited: VLMaps are used to compute geometric target coordinates (e.g., 'left of X') which are executed by a classical navigation stack. The paper does not provide evidence that language (verbs) are grounded to motor primitives via learned representations — instead an LLM composes API calls that call parameterized navigation primitives.",
            "hierarchical_features_evidence": "Yes — paper reports that LSeg (fine-tuned) improves instance/object segmentation but suffers from 'catastrophic forgetting' on coarse area/region recognition; CLIP features (no fine-tune) preserve region-level recognition better (used in ConceptFusion comparisons).",
            "transfer_conditions": "Transfer works well when pretrained VLM categories overlap with environment object categories and when accurate RGB-D odometry and reconstruction are available; cross-embodiment transfer (sharing same VLMap) succeeds provided obstacle category lists are adapted per embodiment (drone vs ground robot).",
            "novel_vs_familiar_objects": "The system demonstrates open-vocabulary zero-shot indexing (including unseen object mentions in language instructions) but the paper does not present a strict novel-vs-familiar split with quantitative differences.",
            "zero_shot_or_few_shot": "Zero-shot: VLMaps enable zero-shot spatial language navigation (no finetuning on target environments); shown to outperform CLIP Map and CoW baselines on spatial tasks (Table 3).",
            "layer_analysis": "No layer-wise ablation or freeze/probing analysis of the underlying VLM layers is reported for VLMaps.",
            "negative_transfer_evidence": "Paper reports that segmentation fine-tuning (as in LSeg) causes reduced ability to recognize general region concepts (catastrophic forgetting) compared to CLIP features; this is framed as a trade-off rather than quantified 'negative transfer'.",
            "comparison_to_vision_only": "Compared against CLIP Map (ablative, projects CLIP visual features) and CoW (CLIP+GradCAM), VLMaps (using LSeg features + spatial fusion) outperform these vision-only or CLIP-only alternatives on spatial-goal navigation tasks (see Table 3).",
            "temporal_dynamics": "No analysis of representational or performance dynamics over training time (method is zero-shot; mapping is constructed from exploration sequences but no learning dynamics reported).",
            "dimensionality_analysis": "No explicit dimensionality / intrinsic-dimension analysis reported.",
            "uuid": "e1952.0"
        },
        {
            "name_short": "AVLMaps",
            "name_full": "Audio-Visual-Language Maps",
            "brief_description": "A 3D voxel-based multimodal spatial map that fuses pretrained visual-language embeddings, audio-language embeddings, visual localization descriptors, and area-level CLIP features into shared voxel/heatmap maps enabling zero-shot multimodal spatial indexing and cross-modal disambiguation for navigation and tabletop manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "AVLMaps (multimodal spatial map)",
            "model_description": "3D voxel-grid map where occupied voxels store visual-language features (LSeg/OVSeg/other), audio segments are associated with poses via audio-language encoders (AudioCLIP/CLAP/wav2clip), and visual localization uses NetVLAD+SuperPoint+SuperGLUE; modalities produce heatmaps which are fused (element-wise multiplication) for cross-modal reasoning. Uses LLM-generated code to convert language instructions to API calls for map querying and navigation.",
            "pretraining_type": "Composition of pretrained modules: vision-language (CLIP-based encoders, some fine-tuned for segmentation), audio-language models (AudioCLIP, CLAP, wav2clip), and place/feature localizers (NetVLAD/SuperPoint trained for image retrieval / local descriptors). No joint multimodal finetuning reported.",
            "pretraining_data_description": "CLIP: image-text pairs from web; LSeg and OVSeg fine-tuned on segmentation datasets; AudioCLIP trained to align audio, image and text (audio and image datasets including environmental sounds); CLAP trained on larger audio-language datasets. None include explicit robot action labels or affordance-labeled control traces; audio datasets (ESC-50 etc.) supply environmental sound categories.",
            "target_task_name": "Zero-shot multimodal spatial goal navigation and multimodal tabletop manipulation indexing",
            "target_task_description": "Embodied navigation to goals specified by language, images, or sounds; ambiguous-goal disambiguation using cross-modal information; experiments in Habitat (simulated Matterport3D), AI2-THOR multi-embodiment sims, and real-world mobile robot and tabletop manipulator setups. Action space: discrete small forward/turn steps for navigation; manipulator tasks require end-effector positioning accurate to ~10 cm.",
            "semantic_alignment": "Paper discusses alignment qualitatively: audio-language pretraining (AudioCLIP/CLAP/wav2clip) aligns sound semantics to language and enables text→audio matching, while vision-language modules align object concepts; alignment between modalities enables cross-modal disambiguation but is limited by audio noise and by fine-tuning effects in vision models.",
            "performance_with_language_pretraining": "AVLMaps achieves: sound-goal navigation success rate ≈ 77.5% (Table 5); multimodal ambiguous-goal navigation (AVLMaps with AudioCLIP) sound SR 46.2% and object SR 28.6% (Table 10). Real-world mobile experiment overall success rate 50% on 20 multimodal navigation tasks; tabletop manipulator: 100% success for visual and sound goals, 9/13 object goals, 9/10 ambiguous goals.",
            "performance_without_language_pretraining": "Baselines using weaker audio models or unimodal setups perform worse: VLMaps + wav2clip baseline had sound SR ≈ 22.0% and object SR ≈ 12.7% in ambiguous navigation (Table 10). Object-sound indexing baseline (AudioCLIP alone) recall@0.5m ≈ 26.05% vs VLMaps+AudioCLIP ≈ 53.78% (Table 8).",
            "sample_efficiency_comparison": "No explicit sample-efficiency learning curves comparing pretrained vs non-pretrained training; mapping is constructed from limited exploration data (e.g. thousands of frames) but no learning sample complexity reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No transformer attention or saliency visualizations for multimodal fusion reported; fusion performed by element-wise multiplication of modality heatmaps (no attention mechanism analysis).",
            "embedding_space_analysis": "No deep analysis (e.g., clustering/PCA) of joint multimodal embedding space; paper provides empirical downstream recall/improvement numbers demonstrating usefulness of embedding matches (e.g., object-sound recall gains).",
            "action_grounding_evidence": "Indirect: AVLMaps enable locating multimodal goal coordinates used by planners; however, the paper does not present evidence that language or audio tokens are grounded to motor primitives beyond geometric localization and planner execution. Manipulation experiments use localization outputs to guide end-effector positioning but not learned action-language grounding.",
            "hierarchical_features_evidence": "Paper highlights that fine-tuned VLMs (LSeg) are stronger at object-level segmentation but weaker at coarse area recognition, while CLIP-based sparse topological features better capture area-level concepts — i.e., different feature levels benefit different tasks.",
            "transfer_conditions": "Transfer of multimodal indexing works best when (i) pretraining covers the concept categories (objects, sounds), (ii) mapping has adequate coverage and accurate odometry, and (iii) audio pre-processing (denoising) is adequate; the paper shows audio-model quality (AudioCLIP vs CLAP) significantly affects sound-goal performance (CLAP gives ≈10% improvement).",
            "novel_vs_familiar_objects": "Demonstrates open-vocabulary zero-shot behavior (indexing to arbitrary language/image/audio queries) but does not provide a controlled novel-vs-familiar object split with per-category analysis.",
            "zero_shot_or_few_shot": "Zero-shot: system performs zero-shot multimodal indexing and navigation from language/image/audio queries without finetuning on target environments; reported success rates above reflect zero-shot performance.",
            "layer_analysis": "No per-layer ablation of constituent pretrained models; modular replacement experiments (swap AudioCLIP→CLAP, LSeg→OVSeg/SAM+CLIP) assess module-level impact but not internal layers.",
            "negative_transfer_evidence": "The SAM+CLIP variant substantially lowered performance in the scaling experiments (sensitive hyperparameters and poor masked-region CLIP interpretation), indicating negative practical transfer for that combination in this pipeline.",
            "comparison_to_vision_only": "Compared to unimodal (vision-only) baselines, AVLMaps with audio+vision+language improved ambiguous-goal disambiguation: e.g., object-sound cross-modal indexing recall increased from ~26% (AudioCLIP baseline) to ~53.78% (VLMaps+AudioCLIP). For ambiguous navigation, AVLMaps improved sound success rates by ~24.2% over a multimodal baseline.",
            "temporal_dynamics": "No temporal representation dynamics reported; audio transient vs persistent sound limitations discussed qualitatively (sound is transient; mapping is offline).",
            "dimensionality_analysis": "No explicit dimensionality or intrinsic-dimension analyses reported.",
            "uuid": "e1952.1"
        },
        {
            "name_short": "LSeg",
            "name_full": "LSeg (Language-driven Semantic Segmentation)",
            "brief_description": "A pixel-wise segmentation model whose visual encoder maps pixels into the CLIP feature space and can segment images based on free-form language categories; used here to produce dense pixel embeddings for map fusion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSeg (visual encoder + text encoder)",
            "model_description": "Vision-language segmentation model: visual encoder produces dense pixel embeddings aligned with CLIP embedding space; a CLIP text encoder encodes free-form labels and per-pixel cosine similarities produce segmentation masks.",
            "pretraining_type": "CLIP-style vision-language pretraining followed by segmentation fine-tuning on segmentation datasets.",
            "pretraining_data_description": "CLIP pretraining on large-scale image-text pairs; LSeg fine-tuned on segmentation datasets with per-pixel labels (object classes). Datasets likely contain object categories but limited explicit affordance/action annotations.",
            "target_task_name": "Dense pixel-level feature generation for spatial maps / open-vocabulary segmentation",
            "target_task_description": "Produce per-pixel embeddings from RGB frames collected during exploration; embeddings are back-projected into 2D/3D maps to support open-vocabulary landmark localization for navigation and manipulation.",
            "semantic_alignment": "High for object-level semantics (LSeg fine-tuning improves segmentation), lower for broad region/area concepts (paper notes catastrophic forgetting of region recognition after fine-tuning).",
            "performance_with_language_pretraining": "When used within VLMaps/AVLMaps, LSeg-based maps enabled better object segmentation and downstream spatial-goal localization than simple CLIP-projection baselines (reported in ablations). Exact numeric deltas are in paper tables.",
            "performance_without_language_pretraining": "The CLIP Map baseline (project CLIP visual features without LSeg fine-tuning) performs worse on spatial localization of fine-grained object-centric spatial goals, per paper ablations.",
            "sample_efficiency_comparison": "Not applicable — LSeg is a pretrained model used as a feature extractor; no sample-efficiency comparisons reported within this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention analysis for LSeg in paper.",
            "embedding_space_analysis": "Paper uses LSeg embeddings directly for cosine-similarity matching and notes averaged multi-view embeddings per map cell provide robust signals; no further analysis.",
            "action_grounding_evidence": "No evidence LSeg encodes affordance-to-action mappings; used purely for semantic localization.",
            "hierarchical_features_evidence": "Paper reports LSeg (fine-tuned) improves object-level segmentation but weakens general concept/area recognition relative to CLIP-only features (catastrophic forgetting).",
            "transfer_conditions": "LSeg's performance depends on the segmentation dataset it's fine-tuned on; domain mismatch can reduce area-level generalization.",
            "novel_vs_familiar_objects": "LSeg shows good generalization to open-vocabulary labels in practice (via CLIP-space matching), but no rigorous novel/familiar partition evaluation is reported.",
            "zero_shot_or_few_shot": "LSeg provides open-vocabulary segmentation via CLIP-space matching, enabling zero-shot queries for new labels at inference.",
            "layer_analysis": "Not performed in this paper.",
            "negative_transfer_evidence": "Catastrophic forgetting of region-level concepts after segmentation fine-tuning is noted as a negative effect when compared to CLIP-only features.",
            "comparison_to_vision_only": "Compared with CLIP-only feature mapping, LSeg + spatial fusion (VLMaps) yields superior object localization for spatial-goal navigation in the experiments.",
            "temporal_dynamics": "No temporal analysis reported.",
            "dimensionality_analysis": "No analysis reported.",
            "uuid": "e1952.2"
        },
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language-Image Pre-training)",
            "brief_description": "A large vision-language model trained contrastively on image-text pairs from the internet to align image and text embeddings in a joint space, enabling zero-shot classification and text→image similarity matching; used here for text encoding and as a backbone embedding space.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CLIP (image & text encoders)",
            "model_description": "Dual-encoder vision-language model (image encoder + text encoder) trained contrastively to align image and text embeddings. Produces global image embeddings and is used as the text encoder for open-vocabulary queries in map indexing.",
            "pretraining_type": "Vision-language on large-scale image-text pairs (web-scale).",
            "pretraining_data_description": "Diverse internet image-caption pairs; contains numerous object categories and scene descriptions, but not robot-action labeled data or spatial-affordance annotations.",
            "target_task_name": "Open-vocabulary landmark indexing, text→visual similarity matching in maps",
            "target_task_description": "Used to convert language queries into embedding vectors and to compute cosine similarities with map cell embeddings for retrieval/localization in both 2D and 3D maps.",
            "semantic_alignment": "High overlap for common object categories and captions; aligns well with object semantics used in indoor navigation tasks.",
            "performance_with_language_pretraining": "Used as a core encoder; CLIP-based retrieval forms core of baselines (CoW, CLIP Map); VLMaps (with LSeg) outperform simple CLIP Map baselines in spatial localization tasks.",
            "performance_without_language_pretraining": "N/A (CLIP is the pretrained baseline).",
            "sample_efficiency_comparison": "Not applicable within this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided in paper.",
            "embedding_space_analysis": "Used implicitly for cosine similarity scoring; no detailed analysis beyond performance comparisons.",
            "action_grounding_evidence": "No; CLIP provides perception-language alignment but paper does not show grounding of action verbs in CLIP representations.",
            "hierarchical_features_evidence": "CLIP sparse topological features retain region-level cues better than segmentation-finetuned models, per paper discussion.",
            "transfer_conditions": "Works best when environment objects are represented in CLIP pretraining distribution; fine-grained segmentation tasks may benefit from fine-tuned models.",
            "novel_vs_familiar_objects": "CLIP's zero-shot capability supports novel object labels, but paper does not quantify novel vs familiar performance breakdown.",
            "zero_shot_or_few_shot": "Zero-shot retrieval for open-vocabulary queries is a core capability used in experiments.",
            "layer_analysis": "None reported.",
            "negative_transfer_evidence": "Fine-tuning on segmentation (LSeg) can reduce CLIP's region-level generalization; paper frames this as a trade-off.",
            "comparison_to_vision_only": "CLIP is multimodal (vision+text); compared against purely vision-only baselines indirectly in related work but not a direct image-only pretraining comparison in experiments.",
            "temporal_dynamics": "Not analyzed.",
            "dimensionality_analysis": "Not analyzed.",
            "uuid": "e1952.3"
        },
        {
            "name_short": "AudioCLIP",
            "name_full": "AudioCLIP",
            "brief_description": "An extension of CLIP to audio: a multimodal model that aligns audio, image, and text embeddings to enable audio–text and audio–image retrieval; used here as the primary audio-language model in AVLMaps' audio localization module.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "AudioCLIP (audio-image-text alignment)",
            "model_description": "Multimodal model that augments CLIP with an audio encoder and trains to align audio embeddings with CLIP image/text embeddings enabling cross-modal retrieval between audio, images and text.",
            "pretraining_type": "Multimodal pretraining aligning audio, image and text embeddings (audio extensions of CLIP).",
            "pretraining_data_description": "Audio datasets aligned with image/text modalities (various environmental and event sound datasets), but typically limited in scale compared to CLIP's image-text pretraining; lacks explicit robot-action labels.",
            "target_task_name": "Audio localization (text→audio matching) for sound-goal navigation and cross-modal disambiguation",
            "target_task_description": "Segment audio stream collected during mapping, compute audio embeddings per segment, associate segments to poses; at inference match language descriptions of sounds to segment embeddings to get probable sound locations (heatmaps). Evaluated in Habitat simulations and real-world experiments with added audio.",
            "semantic_alignment": "Trained to align audio classes to language descriptions and images; alignment enables retrieval for environmental sound categories used in navigation experiments (ESC-50 categories etc.).",
            "performance_with_language_pretraining": "When used in AVLMaps, AudioCLIP yielded strong multimodal indexing; object-sound cross-modal indexing recall improved to ~53.78% (recall@0.5m) compared to baseline AudioCLIP-only 26.05% (Table 8 indicates that integrating AudioCLIP into VLMaps produced large gains). Sound-goal navigation success with AudioCLIP within AVLMaps reported as part of overall 77.5% success in Table 5 (mapping + AudioCLIP features).",
            "performance_without_language_pretraining": "Baseline wav2clip and earlier audio encoders produced substantially lower sound localization/navigation performance (e.g., VLMaps + wav2clip sound SR ≈ 22.0% in ambiguous navigation tasks).",
            "sample_efficiency_comparison": "Not applicable; audio models are pretrained and used zero-shot.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No internal attention or saliency analyses for AudioCLIP provided.",
            "embedding_space_analysis": "No detailed embedding-space probe beyond retrieval match scores and downstream recall measurements.",
            "action_grounding_evidence": "No evidence of grounding action verbs; audio features are used only for localization/semantic disambiguation.",
            "hierarchical_features_evidence": "Not analyzed specifically for audio.",
            "transfer_conditions": "Paper shows replacement of AudioCLIP with the more recent CLAP improves sound-goal navigation (~10% improvement), indicating sensitivity to audio-model pretraining scale and augmentation.",
            "novel_vs_familiar_objects": "AudioCLIP works on ESC-50 sound categories used in experiments; no explicit novel vs familiar breakdown provided.",
            "zero_shot_or_few_shot": "Used zero-shot (text→audio retrieval) for navigation and indexing.",
            "layer_analysis": "Not performed.",
            "negative_transfer_evidence": "Not reported for AudioCLIP specifically; CLAP shown to improve performance compared to AudioCLIP (positive transfer).",
            "comparison_to_vision_only": "AudioCLIP is multimodal (audio aligned to CLIP); integration with visual-language maps (AVLMaps) produced large performance gains over vision-only baselines for ambiguous multimodal tasks.",
            "temporal_dynamics": "Audio is transient; paper discusses limitations of mapping transient sounds into a static map (no dynamic update).",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1952.4"
        },
        {
            "name_short": "CLAP",
            "name_full": "CLAP (Contrastive Language–Audio Pretraining)",
            "brief_description": "An audio-language foundation model trained at scale to align audio and natural-language descriptions, yielding stronger audio-language retrieval performance than earlier AudioCLIP; used in AVLMaps scaling experiments and shown to improve sound-goal navigation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CLAP (audio↔text encoder)",
            "model_description": "Contrastive audio–language model trained on larger and more diverse audio-language pairs than earlier audio-CLIP variants. Used to encode audio segments and language queries for matching.",
            "pretraining_type": "Audio-language contrastive pretraining on large-scale audio–text pairs.",
            "pretraining_data_description": "Large-scale audio–text pairs, potentially with advanced augmentations and broader coverage of environmental and event sounds (paper cites CLAP benefits from larger/more diverse datasets).",
            "target_task_name": "Improved audio localization for sound-goal navigation",
            "target_task_description": "Replace AudioCLIP in AVLMaps audio localization module with CLAP; evaluate impact on sound-goal navigation and overall multimodal navigation success in Habitat simulations.",
            "semantic_alignment": "Higher alignment for sound semantics to textual descriptions due to larger/more diverse pretraining; authors report an observed ≈10% improvement in sound-goal navigation when using CLAP vs AudioCLIP.",
            "performance_with_language_pretraining": "Using CLAP in AVLMaps provided roughly a 10% absolute improvement in navigating to sound goals compared to AudioCLIP (paper text reports ~10% improvement in sound-goal performance in scaling experiments).",
            "performance_without_language_pretraining": "Not applicable within paper experiments.",
            "sample_efficiency_comparison": "Not provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided.",
            "embedding_space_analysis": "Not provided beyond downstream performance.",
            "action_grounding_evidence": "No direct evidence of action grounding from CLAP embeddings; used for localization only.",
            "hierarchical_features_evidence": "Not analyzed.",
            "transfer_conditions": "Paper indicates improved performance when ALM is stronger/more diverse — suggesting transfer depends on dataset scale and diversity.",
            "novel_vs_familiar_objects": "Not explicitly evaluated.",
            "zero_shot_or_few_shot": "Zero-shot audio→text retrieval for audio localization used.",
            "layer_analysis": "Not performed.",
            "negative_transfer_evidence": "Not reported; CLAP shows positive gain over AudioCLIP in their experiments.",
            "comparison_to_vision_only": "CLAP contributes complementary audio semantics to vision-language maps, improving disambiguation in multimodal tasks.",
            "temporal_dynamics": "No analysis; transient sound limitations discussed.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1952.5"
        },
        {
            "name_short": "wav2clip",
            "name_full": "wav2clip",
            "brief_description": "An audio representation method that projects audio into CLIP embedding space, used as a baseline audio-language encoder in the paper's comparisons and baselines for audio localization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "wav2clip (audio→CLIP features)",
            "model_description": "An audio encoder trained to map audio clips into CLIP embedding space so that audio can be compared to text/image CLIP embeddings; used for audio retrieval tasks.",
            "pretraining_type": "Audio encoder trained to align with CLIP embedding space (contrastive/teacher-student style), not trained on robotic data.",
            "pretraining_data_description": "Audio datasets mapped to CLIP space; used for environmental sound categories, but smaller and less diverse than CLAP in many cases.",
            "target_task_name": "Baseline audio localization for sound-goal navigation",
            "target_task_description": "Used as an audio-module baseline in AVLMaps and ambiguous-goal navigation experiments; associated audio segment embeddings with poses during mapping and matched to language queries.",
            "semantic_alignment": "Provides some audio→language alignment through CLIP-space projection but weaker than AudioCLIP/CLAP in experiments.",
            "performance_with_language_pretraining": "When used in multimodal baseline (VLMaps + wav2clip) for ambiguous navigation, reported sound SR ≈ 22.0% and object SR ≈ 12.7% (Table 10), substantially lower than AVLMaps with AudioCLIP/CLAP.",
            "performance_without_language_pretraining": "As a baseline comparison; performs worse than AudioCLIP/CLAP in downstream sound localization/navigation.",
            "sample_efficiency_comparison": "Not applicable.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided.",
            "embedding_space_analysis": "Not provided beyond downstream metrics.",
            "action_grounding_evidence": "None — used only for semantic localization.",
            "hierarchical_features_evidence": "Not analyzed.",
            "transfer_conditions": "Performance sensitive to audio model quality; wav2clip performed worse in noisy or ambiguous multi-instance sound setups.",
            "novel_vs_familiar_objects": "Not explicitly tested.",
            "zero_shot_or_few_shot": "Functions zero-shot for text→audio retrieval at inference.",
            "layer_analysis": "Not present.",
            "negative_transfer_evidence": "No explicit negative transfer reported, but it is empirically weaker than AudioCLIP/CLAP.",
            "comparison_to_vision_only": "As an audio-only baseline, wav2clip underperforms compared to integrated audio-visual-language approaches.",
            "temporal_dynamics": "Not analyzed.",
            "dimensionality_analysis": "Not analyzed.",
            "uuid": "e1952.6"
        },
        {
            "name_short": "NetVLAD+SuperPoint+SuperGLUE",
            "name_full": "NetVLAD with SuperPoint features and SuperGLUE matching",
            "brief_description": "A hierarchical visual localization pipeline that uses NetVLAD for global image retrieval, SuperPoint for local features, and SuperGLUE for robust keypoint matching to estimate query camera pose by PnP and odometry association; used to localize query images into the map.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NetVLAD (global) + SuperPoint (local) + SuperGLUE (matching)",
            "model_description": "Hierarchical localization: compute NetVLAD global descriptors for retrieval, use SuperPoint local descriptors to extract keypoints, match via SuperGLUE, back-project matched keypoints with depth to obtain 3D correspondences and solve PnP for camera pose estimation.",
            "pretraining_type": "NetVLAD trained for place recognition; SuperPoint learned for self-supervised interest point detection; SuperGLUE trained for matching — none are pretrained on robot-action data.",
            "pretraining_data_description": "Place recognition and local-feature datasets (street-level, structure-from-motion datasets) rather than explicit action/affordance data.",
            "target_task_name": "Visual localization / image-goal localization in AVLMaps",
            "target_task_description": "Given a query image, retrieve and match against the collected reference database of exploration frames to estimate the global camera pose; output a heatmap with a high-probability region for that image location.",
            "semantic_alignment": "Aligns spatially (place-level) rather than semantically; effective when environment has matching visual features between query and mapped frames.",
            "performance_with_language_pretraining": "Not directly applicable; this module is geometry- and descriptor-based; used successfully as part of AVLMaps visual localization (no numbers isolated for this module alone).",
            "performance_without_language_pretraining": "N/A (module is not a language-pretrained model).",
            "sample_efficiency_comparison": "Not applicable.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not applicable.",
            "embedding_space_analysis": "Not analyzed in the paper beyond successful pose estimation examples.",
            "action_grounding_evidence": "Not relevant — module provides localization estimates used downstream by planners.",
            "hierarchical_features_evidence": "Paper describes this hierarchical approach as needed to localize query images robustly (global retrieval then local matching).",
            "transfer_conditions": "Performance depends on the visual distinctiveness of places and coverage in the reference database; limited when scenes are highly dynamic or lack distinctive features.",
            "novel_vs_familiar_objects": "Not applicable.",
            "zero_shot_or_few_shot": "Performs retrieval-based localization zero-shot using stored database of exploration frames; no training on target queries required beyond descriptors.",
            "layer_analysis": "Not performed.",
            "negative_transfer_evidence": "No evidence reported; standard geometric limitations (dynamic scenes) discussed elsewhere.",
            "comparison_to_vision_only": "This is a vision-only geometric localization pipeline; used in AVLMaps in combination with language models and audio models.",
            "temporal_dynamics": "Dynamic objects during mapping can corrupt map points; paper discusses dynamic-scene limitations for the mapping pipeline generally.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1952.7"
        },
        {
            "name_short": "text-davinci-003",
            "name_full": "text-davinci-003 (OpenAI LLM)",
            "brief_description": "A large language model used here as a code-writing planner: few-shot prompted to translate natural language instructions into executable Python robot code that calls parameterized navigation APIs for VLMaps/AVLMaps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (LLM used for code generation)",
            "model_description": "Large autoregressive language model (text and code) used in a few-shot prompting regime to synthesize Python code (API calls/logic) that sequences spatial navigation primitives and multimodal map queries.",
            "pretraining_type": "Large-scale text pretraining (mixture including code corpora); not trained on robot control data for this work.",
            "pretraining_data_description": "Web text and code (e.g., GitHub), enabling code-synthesis capabilities; no explicit embodied-action labels but general reasoning and code-generation capability.",
            "target_task_name": "Language-to-plan/code generation for embodied navigation tasks",
            "target_task_description": "Given a natural language instruction (possibly multimodal), LLM generates code calling robot APIs (move_to, get_map, get_major_map, fuse maps) to produce precise spatial goals and invoke navigation stack. Executed directly on robot via Python exec.",
            "semantic_alignment": "LLM provides semantic parsing and arithmetic (e.g., compute 'in between' by averaging coordinates), but grounding into perception is via API calls that query the map — thus semantic alignment relies on the map's embeddings rather than LLM internal world model.",
            "performance_with_language_pretraining": "LLM successfully decomposes long-horizon instructions into API calls enabling VLMaps/AVLMaps to follow complex spatial instructions zero-shot; qualitative and quantitative navigation success rates reported (system-level results depend on the map modules).",
            "performance_without_language_pretraining": "No baseline without LLM used in this work (alternatives referenced in related work).",
            "sample_efficiency_comparison": "Not applicable — LLM is used zero-shot/few-shot via prompting.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No analysis of LLM attention patterns presented.",
            "embedding_space_analysis": "Not applicable within this study.",
            "action_grounding_evidence": "LLM outputs code that calls grounded perception APIs; the paper does not claim the LLM itself has grounded motor affordance representations, but rather that it composes grounded API invocations.",
            "hierarchical_features_evidence": "Not applicable.",
            "transfer_conditions": "LLM few-shot prompting effectiveness depends on prompt examples and available API documentation in context; no systematic prompt-ablation reported.",
            "novel_vs_familiar_objects": "LLM generalization to arbitrary language referring to map concepts demonstrated (zero-shot language references to objects/regions), but no strict novel-vs-familiar split reported.",
            "zero_shot_or_few_shot": "Few-shot prompting is used; code generation is performed zero-shot/few-shot at inference without finetuning.",
            "layer_analysis": "Not performed.",
            "negative_transfer_evidence": "No explicit negative transfer evidence reported for the LLM usage.",
            "comparison_to_vision_only": "Not directly applicable; LLM complements vision-language maps by converting natural language to grounded API calls.",
            "temporal_dynamics": "Not analyzed.",
            "dimensionality_analysis": "Not analyzed.",
            "uuid": "e1952.8"
        },
        {
            "name_short": "CoW",
            "name_full": "CLIP on Wheels (CoW)",
            "brief_description": "A prior zero-shot language-driven object navigation approach that uses CLIP-based saliency maps (with GradCAM) combined with exploration to perform object search and navigation; used as a baseline in experiments.",
            "citation_title": "Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation",
            "mention_or_use": "use",
            "model_name": "CoW (CLIP + GradCAM saliency)",
            "model_description": "Uses CLIP to produce image-level text similarity saliency; GradCAM produces saliency maps for object localization which are aggregated over frames to produce objective masks for navigation.",
            "pretraining_type": "Vision-language (CLIP) pretraining on image-text pairs.",
            "pretraining_data_description": "CLIP's web-scale image-caption pretraining; does not include robot-specific action data.",
            "target_task_name": "Zero-shot object goal navigation",
            "target_task_description": "Navigate to instances of an object category specified in language by thresholding CLIP/GradCAM saliency maps and using exploration strategies to find objects in continuous environments.",
            "semantic_alignment": "Good for object-level semantic alignment; limited for fine spatial relations (e.g., 'in-between' queries) as noted in paper comparisons.",
            "performance_with_language_pretraining": "Used as baseline; authors report CoW struggles on fine-grained spatial queries and has high failure rates on spatial-goal navigation tasks compared to VLMaps (see Table 3).",
            "performance_without_language_pretraining": "N/A (baseline relies on CLIP).",
            "sample_efficiency_comparison": "Not discussed in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not analyzed here (this is baseline prior work).",
            "embedding_space_analysis": "Not analyzed here.",
            "action_grounding_evidence": "CoW maps perception to exploration actions but does not learn explicit affordance-action groundings; treated here as a baseline.",
            "hierarchical_features_evidence": "Not discussed.",
            "transfer_conditions": "Effective for object-level goals but less effective for spatial relations and open-vocabulary spatial indexing.",
            "novel_vs_familiar_objects": "Designed for zero-shot novel object categories via CLIP; empirical limits noted for relative spatial descriptions.",
            "zero_shot_or_few_shot": "Zero-shot by design.",
            "layer_analysis": "None in this paper.",
            "negative_transfer_evidence": "Not reported here but CoW's limitations in spatial language tasks constitute performance shortfalls.",
            "comparison_to_vision_only": "CoW is vision-language based; authors compare it to VLMaps and show VLMaps perform better on spatial queries.",
            "temporal_dynamics": "Not analyzed.",
            "dimensionality_analysis": "Not analyzed.",
            "uuid": "e1952.9"
        },
        {
            "name_short": "LM-Nav",
            "name_full": "LM-Nav",
            "brief_description": "A prior system that combines large language models, CLIP, and a topological graph of observations to parse language instructions into landmark lists and navigate a graph representation of the environment; used as a baseline comparison.",
            "citation_title": "LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action",
            "mention_or_use": "use",
            "model_name": "LM-Nav (graph-based navigation + LLM/CLIP)",
            "model_description": "Combines pretrained models (LLMs, CLIP) to parse instructions and plan on an image-observation topological graph (nodes = images, edges = proximity) to navigate in real-world environments.",
            "pretraining_type": "Vision-language and language model pretraining (CLIP + LLMs); graph-based memory built from environment observations.",
            "pretraining_data_description": "CLIP/LLM pretraining on web-scale image-text and text corpora; no robot-control pretraining for navigation policy.",
            "target_task_name": "Zero-shot language-conditioned navigation on topological graphs",
            "target_task_description": "Parse language commands into landmarks via LLM and plan on pre-built topological graph of image observations to reach nodes corresponding to referenced landmarks.",
            "semantic_alignment": "Good for landmark-level queries; limited for fine-grained spatial relations compared to VLMaps' 2D/3D geometric fusion.",
            "performance_with_language_pretraining": "Paper uses LM-Nav as a baseline; VLMaps outperform LM-Nav on fine-grained spatial goal navigation requiring precise geometric localization (Table 3).",
            "performance_without_language_pretraining": "N/A.",
            "sample_efficiency_comparison": "Not supplied.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided.",
            "embedding_space_analysis": "Not provided.",
            "action_grounding_evidence": "LM-Nav uses topological planning rather than motor-level grounding; not analyzed here.",
            "hierarchical_features_evidence": "Not discussed.",
            "transfer_conditions": "Works when topological graph adequately covers goals; less suited for free-space spatial offsets like 'in between'.",
            "novel_vs_familiar_objects": "Designed for zero-shot object/landmark navigation but less precise for spatial relations.",
            "zero_shot_or_few_shot": "Zero-shot landmark navigation via pretrained models.",
            "layer_analysis": "Not performed here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "LM-Nav is multimodal (LLM+CLIP) but relies on topological graphs rather than dense spatial maps; VLMaps reported to better handle fine spatial language.",
            "temporal_dynamics": "Not analyzed.",
            "dimensionality_analysis": "Not analyzed.",
            "uuid": "e1952.10"
        },
        {
            "name_short": "OVSeg",
            "name_full": "OVSeg (Open-Vocabulary Segmentation)",
            "brief_description": "A vision-language segmentation approach that generates dense pixel-level CLIP-aligned features while using learnable mask prompts to improve segmentation performance; evaluated as an alternative VLM in AVLMaps' scaling experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OVSeg (pixel-wise VLM via mask prompts)",
            "model_description": "Produces dense pixel embeddings in CLIP space using segmentation techniques with trainable mask prompts (designed to adapt masks for segmentation), intended to improve pixel-level visual-language features.",
            "pretraining_type": "CLIP-based vision-language pretraining plus segmentation-oriented adaptation (learnable mask prompts) — not robot-specific.",
            "pretraining_data_description": "Image-level labels and segmentation data used for adaptation; does not include robot action labels.",
            "target_task_name": "Alternative visual-language encoder for AVLMaps object localization module",
            "target_task_description": "Replace LSeg in AVLMaps with OVSeg to test whether improved dense VLMs provide better object localization and downstream navigation.",
            "semantic_alignment": "Designed to better align segmentation masks with CLIP semantics, but in experiments OVSeg provided limited improvements over LSeg for dense visual-language feature generation in AVLMaps.",
            "performance_with_language_pretraining": "In scaling experiments, OVSeg did not provide clear benefits over LSeg in AVLMaps' object localization performance; reported results show minimal gains.",
            "performance_without_language_pretraining": "N/A.",
            "sample_efficiency_comparison": "No sample-efficiency data provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided.",
            "embedding_space_analysis": "Not provided.",
            "action_grounding_evidence": "None.",
            "hierarchical_features_evidence": "Not analyzed in depth; OVSeg designed for improved segmentation but not necessarily for feature-generation for spatial indexing.",
            "transfer_conditions": "Requires hyperparameter tuning and mask prompt adaptation; sensitive to scene specifics per authors' discussion.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Provides open-vocabulary segmentation at inference but performance depends on adaptation.",
            "layer_analysis": "Not performed.",
            "negative_transfer_evidence": "OVSeg provided limited benefits in experiments; SAM+CLIP combination performed worse (negative effect) and required tuning.",
            "comparison_to_vision_only": "OVSeg is a vision-language method compared to LSeg; in practice LSeg remained competitive in the paper's pipeline.",
            "temporal_dynamics": "Not analyzed.",
            "dimensionality_analysis": "Not analyzed.",
            "uuid": "e1952.11"
        },
        {
            "name_short": "SAM+CLIP",
            "name_full": "SAM (Segment Anything) + CLIP combination",
            "brief_description": "An approach that generates class-agnostic masks with SAM and then encodes masked regions with CLIP to produce per-region CLIP embeddings; evaluated in AVLMaps scaling experiments but found to reduce performance without careful hyperparameter tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SAM + CLIP (mask-based region encoding)",
            "model_description": "SAM generates class-agnostic segmentation masks; each region is encoded with CLIP (sum of masked crop and zero-background crop embeddings) to assign CLIP-style embeddings to regions for open-vocabulary mapping.",
            "pretraining_type": "SAM pretrained for mask generation (large segmentation dataset) and CLIP pretrained on image-text pairs; combined post-hoc (no joint finetuning).",
            "pretraining_data_description": "SAM trained on mask annotations across diverse images; CLIP on image-text pairs. No robot data or joint multimodal finetuning used.",
            "target_task_name": "Alternate dense visual-language feature provider for object localization in AVLMaps",
            "target_task_description": "Used as a tested variant for producing per-region CLIP embeddings to place into the voxel map and then retrieve by language queries.",
            "semantic_alignment": "Region-based semantic alignment depends on quality of SAM masks and CLIP's ability to interpret masked regions; paper reports sensitivity issues.",
            "performance_with_language_pretraining": "In scaling experiments SAM+CLIP significantly reduced overall performance compared to LSeg/OVSeg in this pipeline; authors attribute this to sensitivity of SAM masks and CLIP interpretation of masked regions.",
            "performance_without_language_pretraining": "N/A.",
            "sample_efficiency_comparison": "Not provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided.",
            "embedding_space_analysis": "No detailed analysis; empirical downstream degradation reported.",
            "action_grounding_evidence": "None.",
            "hierarchical_features_evidence": "Not analyzed in detail.",
            "transfer_conditions": "Requires substantial hyperparameter tuning; unstable across different scenes, leading to negative transfer in their pipeline.",
            "novel_vs_familiar_objects": "Not evaluated.",
            "zero_shot_or_few_shot": "Operates zero-shot at inference but unstable.",
            "layer_analysis": "Not performed.",
            "negative_transfer_evidence": "Yes — authors report that SAM+CLIP 'significantly reduced performance' in AVLMaps experiments and required extensive tuning to be competitive.",
            "comparison_to_vision_only": "SAM provides segmentation (vision-only) and combined with CLIP for semantics; however, in this embodied mapping pipeline it underperformed tuned segmentation-based VLMs.",
            "temporal_dynamics": "Not discussed.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1952.12"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation",
            "rating": 2
        },
        {
            "paper_title": "LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action",
            "rating": 2
        },
        {
            "paper_title": "AudioCLIP: Extending CLIP to image, text and audio",
            "rating": 2
        },
        {
            "paper_title": "CLAP: Learning audio concepts from natural language supervision",
            "rating": 2
        },
        {
            "paper_title": "ConceptFusion: Open-set multimodal 3D mapping",
            "rating": 2
        },
        {
            "paper_title": "CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory",
            "rating": 1
        },
        {
            "paper_title": "RT-1: Robotics Transformer for Real-World Control at Scale",
            "rating": 1
        },
        {
            "paper_title": "CLIPort: What and where pathways for robotic manipulation",
            "rating": 2
        }
    ],
    "cost": 0.0355495,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multimodal Spatial Language Maps for Robot Navigation and Manipulation
7 Jun 2025</p>
<p>Chenguang Huang chenguang.huang@utn.de 
University of Technology Nuremberg
Germany</p>
<p>Oier Mees 
BerkeleyUCUSA</p>
<p>Andy Zeng 
Google Research
USA</p>
<p>Wolfram Burgard 
University of Technology Nuremberg
Germany</p>
<p>Department of Computer Science and Artificial Intelligence
Artificial Intelligence and Robotics Lab
University of Technology Nuremberg
Ulmenstraße 52i90461NurembergGermany</p>
<p>Multimodal Spatial Language Maps for Robot Navigation and Manipulation
7 Jun 20254453B967EB6BB54FB483220A010A454910.1177/ToBeAssignedarXiv:2506.06862v1[cs.RO]Robot NavigationScene RepresentationsLarge Language ModelsAudio Language Models
Grounding language to a navigating agent's observations can leverage pretrained multimodal foundation models to match perceptions to object or event descriptions.However, previous approaches remain disconnected from environment mapping, lack the spatial precision of geometric maps, or neglect additional modality information beyond vision.To address this, we propose multimodal spatial language maps as a spatial map representation that fuses pretrained multimodal features with a 3D reconstruction of the environment.We build these maps autonomously using standard exploration.We present two instances of our maps, which are visual-language maps (VLMaps) and their extension to audio-visual-language maps (AVLMaps) obtained by adding audio information.When combined with large language models (LLMs), VLMaps can translate natural language commands into open-vocabulary spatial goals (e.g., "in between the sofa and TV") directly localized in the map, and be shared across different robot embodiments to generate tailored obstacle maps on demand.Building upon the capabilities above, AVLMaps extend VLMaps by introducing a unified 3D spatial representation integrating audio, visual, and language cues through the fusion of features from pretrained multimodal foundation models.This enables robots to ground multimodal goal queries (e.g., text, images, or audio snippets) to spatial locations for navigation.Additionally, the incorporation of diverse sensory inputs significantly enhances goal disambiguation in ambiguous environments.Experiments in simulation and realworld settings demonstrate that our multimodal spatial language maps enable zero-shot spatial and multimodal goal navigation and improve recall by 50% in ambiguous scenarios.These capabilities extend to mobile robots and tabletop manipulators, supporting navigation and interaction guided by visual, audio, and spatial cues.Code and videos are available at https://mslmaps.github.io.</p>
<p>Introduction</p>
<p>People are excellent navigators of the physical world due in part to their remarkable ability to build cognitive maps (McNamara et al. 1989) that form the basis of spatial memory (Chun and Jiang 1998;Newman et al. 2007) to (i) localize landmarks at varying ontological levels, such as a book; on the shelf; in the living room, or to (ii) determine whether the layout permits navigation between two points.Meanwhile, humans exhibit a remarkable ability to integrate and leverage multiple sensing modalities to efficiently move around in the physical world.Our actions are driven by a myriad of sensory cues: the sound of glass breaking might signal a dangerous situation, the microwave might buzz to indicate it is done, or a dog might bark to draw our attention.Acoustic signals particularly represent a valuable complementary form of information, also evident by the utility that they provide for the visually impaired, who may rely on them for navigation.Research in cognitive science also suggests that children understand and integrate information from different sensing modalities into spatial cognitive maps (Körding et al. 2007).</p>
<p>Classic methods for robot navigation (Thrun et al. 1998;Endres et al. 2012) build geometric maps for path planning.Although some previous extensions parse goals from templated natural language commands (Tellex et al. 2011;MacMahon et al. 2006), they struggle to generalize to unseen instructions.Learning methods directly optimize for navigation policies grounded in language end-to-end (commands to actions) (Anderson et al. 2018b(Anderson et al. , 2021) ) but require copious amounts of data.Recent works demonstrate that multimodal foundation models (Radford et al. 2021;Li et al. 2021) pretrained on Internet-scale data (e.g., images and their captions) can be used out-of-the-box to ground language to the visual observations of a navigating agent, without additional data collection or model finetuning.These models enable mobile robots to handle new instructions that specify unseen object goals and can be combined with exploration algorithms to search for the first instance of any object (CoW) (Gadre et al. 2023) or traverse object-centric landmarks in graphs (LM-Nav) (Shah et al. 2023).While promising, these methods predominantly AVLMaps provide an open-vocabulary 3D map representation for storing cross-modal information from audio, visual, and language cues.When combined with large language models, AVLMaps consumes multimodal prompts from audio, vision, and language to solve zero-shot spatial goal navigation by effectively leveraging complementary information sources to disambiguate goals.</p>
<p>use vision language models (VLMs) as critics to match image observations to object goal descriptions, but remain disjoint from the mapping of the environment, lacking the fine-grained spatial precision of classic geometric maps.Furthermore, they neglect the great potential of information in other sensing modalities such as audio.Therefore, these methods struggle to (i) localize spatial goals e.g., "in between the sofa and the TV", to (ii) build persistent representations that can be shared across different embodiments, e.g., mobile robots, drones, or to (iii) localize multimodal goals e.g., "the sound of the baby crying", or an image of a refrigerator.How to best spatially anchor various sensing modalities, including visual and audio signals, in ways that enable effective dataefficient cross-modal reasoning for downstream robotics tasks, remains a relatively open question.</p>
<p>In this work, we address that question by introducing Multimodal Spatial Language Maps, a general mapping framework that is (i) spatial, (ii) multimodal, (iii) reusable across different robot embodiments, and (iv) readily extensible to additional sensing modalities in the future.At the heart of our approach are two concrete map instances: Visual-Language Maps (VLMaps) and their multimodal extension, Audio-Visual-Language Maps (AVLMaps).We begin by evaluating VLMaps, which fuse pretrained visuallanguage features from image observations with a 3D reconstruction of the environment.This fusion makes VLMaps both spatial-preserving, enabling localizing queries like "in between the sofa and the TV" in the map, and reusable across embodiments, since the same voxelized map can generate tailored obstacle grids for different robots by defining different sets of obstacle categories.VLMaps can be built from a robot's video stream using standard exploration strategies.When coupled with a large language model (LLMs) in a Socratic fashion (Zeng et al. 2023), they translate long-horizon natural-language commands into sequences of open-vocabulary, spatially grounded goals.</p>
<p>Subsequently, we extend VLMaps to Audio-Visual-Language Maps, AVLMaps, a unified 3D spatial map representation for storing cross-sensing information from audio, visual, and language modalities.By introducing a new modality, audio, we extrapolate the capabilities of our framework to the multimodal setting, showing its extensibility to additional sensing modalities.AVLMaps can be built from image and audio observations captured during reconstruction, by computing dense pre-trained features from open-vocabulary multimodal foundation models trained on Internet-scale data (Li et al. 2022;Ghiasi et al. 2022;Guzhov et al. 2022) and fusing them into a shared 3D voxel grid representation.Beyond VLMaps, AVLMaps:</p>
<p>• allow for landmarks (or areas and regions of interest) indexing in the environment via open-vocabulary multimodal queries (e.g., abstract textual descriptions, images, or audio snippets), enabling downstream applications including multimodal goal-driven navigation, without domain-specific model finetuning.</p>
<p>• include audio information, which allows robots to more often correctly disambiguate goal locations using sound (e.g., "go to the table where you heard coughing" in environments where there are multiple tables, etc).</p>
<p>• extend the spatial characteristic of VLMaps to the multimodal domain, enabling zero-shot multimodal spatial goal localization, e.g., "Go in between the {image of a refrigerator} and the sound of breaking glass" as in Fig. 1.</p>
<p>Extensive experiments in both simulated and real-world settings demonstrate that our VLMaps enable more effective long-horizon language-conditioned spatial goal navigation than baseline alternatives, such as CoW (Gadre et al. 2023) and LM-Nav (Shah et al. 2023).They can be shared across different robot embodiments to generate tailored obstacle maps for efficient, embodiment-specific path planning.Building on this, AVLMaps further extend these capabilities to navigating to goal locations specified by, e.g., natural language descriptions of sounds or visual landmarks -and notably, can disambiguate multiple possible goal locations using multimodal information, (using object semantics to pinpoint one of the multiple possible sound goals, or using vision to pinpoint one of the multiple possible locations where similar objects were found) quantitatively better than unimodal baseline alternatives by up to 50% in top-1 recall in ambiguous scenarios.This article expands our previous work (Huang et al. 2023b,a) by expanding our evaluation to demonstrate that AVLMaps' capabilities continue to naturally improve with better-performing pre-trained audiolanguage foundation models such as AudioCLIP (Guzhov et al. 2022) and CLAP (Elizalde et al. 2023) and that the achieved multimodal disambiguation capabilities also translate to challenging robot manipulation tasks.</p>
<p>AVLMaps are simple and effective in leveraging multiple multimodal foundation models together in tandem to reach broader language-driven robot navigation capabilities, but are also not without limitations -we discuss these and avenues for future work.The code is available at https://mslmaps.github.io/.</p>
<p>vision-based semantic understanding has led to augmenting 3D maps with semantic information (Salas-Moreno et al. 2013;McCormac et al. 2017).Stemming from the intuition of augmenting 3D points in the map with 2D segmentation results, previous works focus on either abstracting the map at object-level with a pose graph (McCormac et al. 2018) or an octree (Xu et al. 2019) or modeling the dynamics of objects in the map (Runz et al. 2018).Despite lifting the 3D reconstruction to a semantic level, these methods are restricted to a predefined set of semantic classes.Recent works like LM-Nav (Shah et al. 2023), CoW (Gadre et al. 2023), VLMaps (Huang et al. 2023b), NLMap-SayCan (Chen et al. 2023a), OpenScene (Peng et al. 2023), or CLIP-Fields (Shafiullah et al. 2023) have shown that integrating visual-language features, generated by either pre-trained or fine-tuned models, into a topological graph or an occupancy map enables open-vocabulary object indexing with natural language, freeing the maps from fixed-size semantic categories.Recent approaches also investigate other open-vocabulary map representations such as NeRF (Engelmann et al. 2024;Kerr et al. 2023;Kim et al. 2024), Gaussian Splattings (Qin et al. 2024;Zuo et al. 2024), and Scene Graphs (Gu et al. 2024;Werby et al. 2024).However, these works focus on visual perception to map and move through an environment, overlooking complementary sources of information such as acoustic signals.In contrast, AVLMaps integrate audio, visual, and language cues into a 3D map, equipping the agent with the ability to navigate to multiple types of multimodal goals and effectively disambiguate goals.</p>
<p>Vision and Language Navigation.Recently, also Visionand-Language Navigation (VLN) has received increased attention (Anderson et al. 2018b;Krantz et al. 2020).Further work has focused on learning end-to-end policies that can follow route-based instructions on topological graphs of simulated environments (Anderson et al. 2018b;Fried et al. 2018;Guhur et al. 2021).However, agents trained in this setting do not have low-level planning capabilities and rely heavily on the topological graph, limiting their real-world applicability (Anderson et al. 2021).Moreover, despite extensions to continuous state spaces (Krantz et al. 2020(Krantz et al. , 2021;;Hong et al. 2022), most of these learningbased methods are data-intensive.The recent success of large pretrained vision and language models (Radford et al. 2021;Brown et al. 2020) has spurred a flurry of interest in applying their zero-sot capabilities to open-vocabulary object navigation (Shah et al. 2023;Gadre et al. 2023).LM-Nav (Shah et al. 2023) combines three pre-trained models to navigate via a topological graph in the real world.</p>
<p>CoW (Gadre et al. 2023) performs zero-shot language-based object navigation by combining CLIP-based (Radford et al. 2021) saliency maps and traditional exploration methods.However, both methods are limited to navigating to object landmarks and are less capable of understanding finergrained queries, such as "to the left of the chair" and "in between the TV and the sofa".In contrast, our method, VLMaps, enables spatial language indexing beyond objectcentric goals and can generate open-vocabulary obstacle maps.Our extension, AVLMaps, further enables multimodal spatial concept indexing such as "between the image of a refrigerator and the sound of breaking glass".</p>
<p>Multimodal Navigation.Recent advances in simulation applications (Savva et al. 2019;Kolve et al. 2017;Chen et al. 2020;Gan et al. 2020a) have boosted research on multimodal navigation in two distinct directions: (i) vision-and-language navigation (VLN) (Anderson et al. 2018b;Krantz et al. 2020) where an agent needs to follow a natural language instruction towards the goal with visual input, and (ii) audiovisual navigation (AVN) (Chen et al. 2020) in which an agent should navigate to the sound source based on information from a binaural sensor and vision.Despite different degrees of success in both directions (Fried et al. 2018;Guhur et al. 2021;Chen et al. 2021a;Younes et al. 2023;Gan et al. 2020bGan et al. , 2022)), less attention has been paid to solving the navigation problem involving vision, language, and audio at the same time.The most relevant concept to our knowledge is from AVLEN (Paul et al. 2022), which extends the AVN with a further query step, introducing a language instruction that helps with navigating to the sound source.In addition, most of the existing methods on AVN focus on approaching the sound without understanding its semantics.In our work, we propose a method to integrate both visual and sound semantics into the same map, enabling a robot to navigate to multimodal goals specified with either goal image or natural language like "go to the sound of baby crying", "go to the table" or multimodal prompts such as "go to the {image of a table} where the sound of the microwave was heard".</p>
<p>Pre-trained Zero-shot Models in Robotics.Recent trends have shown that pre-trained foundation models (Radford et al. 2021;Brown et al. 2020;Liang et al. 2023a) serve as powerful tools for robotic tasks.Most works exploit the inherent perception and reasoning abilities of Vision Language Models (VLMs) or Large Language Models (LLMs) trained with cloud-sourced data to boost the performance of robot tasks including object detection and segmentation (Kamath et al. 2021;Gu et al. 2021;Li et al. 2021), robot manipulation (Shridhar et al. 2022;Liang et al. 2023b;Mees et al. 2022bMees et al. ,a, 2023;;Rosete-Beas et al. 2022;Zawalski et al. 2024;Chen et al. 2024a;Zhou et al. 2024), and navigation (Shah et al. 2023;Gadre et al. 2023;Chen et al. 2023b;Huang et al. 2023b;Hirose et al. 2024;Gu et al. 2024;Werby et al. 2024).With access to a growing volume of robot control data annotated with semantic language labels, recent approaches have advanced the training of a Vision-Language-Action (VLA) model, bridging the gap between visual-language comprehension and low-level robot control within a unified foundational model (Brohan et al. 2023;Zitkovich et al. 2023;O'Neill et al. 2024;Kim et al. 2025;Octo Model Team et al. 2024;Doshi et al. 2024).Despite promising results from previous methods, little effort has been made to exploit the audio-language pre-trained models (ALMs) (Guzhov et al. 2022;Elizalde et al. 2023) in robotic tasks.In this work, we leverage the foundation models focusing on different modalities, e.g.audio, language, and vision, to create a mapping pipeline to understand multimodal information in the scene and achieve robot navigation given language, image, or audio description queries.Concurrent work ConceptFusion (Jatavallabhula et al. 2023) demonstrates that audio can be used as queries to index locations in a visual-language map.However, it doesn't support integrating audio information from the observation data into the representation of the scene as we do in this work.</p>
<p>Method</p>
<p>Our goals are two-fold: (i) build a visual language map representation that synergizes the open-vocabulary capability of vision-language models and the spatial characteristic of geometric maps, and (ii) extend such a map to a multimodal spatial language map representation, in which object landmarks ("sofa"), areas ("kitchen"), audio semantics ("the sound of a baby crying"), or visual goals can be directly localized using natural language or a target image.To achieve the first goal, we propose VLMaps, which can be constructed with pre-trained visual-language models by consuming RGB-D streaming data and camera odometry.Such a map allows for spatial goal indexing like "to the left of the sofa", and can dynamically adapt to different embodiments, enabling users to freely define obstacle categories for the robot to generate a customized occupancy map for path planning.For the second goal, we propose a more general representation, AVLMaps, which takes VLMaps as one sub-module and extends it to consume multimodal data during mapping and support multimodal concept indexing, such as audio, images, and language.We also propose a cross-modal reasoning method to disambiguate locations referring to targets from different modalities ("the sound of brushing teeth near the sink", or "the table near this image: {image}").In the following subsections, we first start with (i) how to build a VLMap by integrating visual-language features into spatial map location (Sec.(LLMs) for multimodal goal navigation, without additional data collection or model fine-tuning (Sec.3.7).We show the pipeline of building a VLMap in Fig. 2, and later show the system pipeline of our multimodal map representation in Fig. 7.</p>
<p>Building a Visual-Language Map</p>
<p>The key idea behind VLMaps is to fuse pretrained visuallanguage features with a 3D reconstruction.We achieve this by computing dense pixel-level embeddings from an existing visual-language model (over the video feed of the robot) and by back-projecting them onto the 3D surface of the environment (captured from depth data used for reconstruction with visual odometry).The overview of VLMaps creation is shown on the left of Fig. 2.</p>
<p>In our work, we utilize LSeg (Li et al. 2021) as the visuallanguage model, a language-driven semantic segmentation model that segments the RGB images based on a set of freeform language categories.The LSeg visual encoder maps an image such that the embedding of each pixel lies in the CLIP feature space.In our approach, we fuse the LSeg pixel embeddings with their corresponding 3D map locations.In this way, without explicit manual segmentation labels, we incorporate a powerful language-driven semantic prior that inherits the generalization capabilities of VLMs.The only assumption we make is access to odometry, which is readily available from RGB-D SLAM systems and enables us to build a map from sequences of RGB-D images.</p>
<p>Formally, we define VLMap as M ∈ R H× W ×C , where H and W represent the size of the top-down grid map, and C represents the length of the VLM embedding vector for each grid cell.Together with the scale parameter s (meters per pixel), a VLMap M represents an area with size s H meters × s W meters.To build the map, for each RGB-D frame, we back-project all the depth pixels u = (u, v) to form a local depth point cloud that we transform to the world frame, P k = D(u)K −1 ũ and P W = T W k P k where ũ = (u, v, 1), K ∈ R 3×3 is the intrinsic matrix of the depth camera, D(u) ∈ R is the depth value of the pixel u, T W k is the transformation from the world coordinate frame to the kth camera frame, P k ∈ R 3 is the 3D point position in the k-th frame, and P W ∈ R 3 is the 3D point position in the Prepared using sagej.clsworld coordinate frame.We then project the point P W to the ground plane and get the pixel u's corresponding position on the grid map,
p x map = H 2 + P x W s + 0.5 , p y map = W 2 − P z W s + 0.5
(1) where p x map and p y map represent the coordinates of the projected point in the map M.</p>
<p>Once we build the grid map, we apply LSeg's visual encoder f (I) : R H×W ×3 → R H×W ×C to the RGB image I k and generate the pixel-level embedding F k ∈ R H×W ×C .Given the RGB-D registration, we project each image pixel u's embedding q = F k (u) ∈ R C to its corresponding grid cell location (p x map , p y map ) in the top-down grid map.Intuitively, there exist multiple 3D points projecting to the same grid location in the map.Thus, we average their embeddings, M(p x map , p y map ) = 1 n n i=1 q i where M(p x map , p y map ) ∈ R C represents the map features at the grid position (p x map , p y map ), n represents the total number of points projecting to the grid location (p x map , p y map ), and q i ∈ R C denotes the corresponding pixel embedding of each point.We note that these n points might not only come from a single frame, but also from points from multiple frames.Therefore, the resulting features contain the averaged embeddings from multiple views of the same object.</p>
<p>Localizing Open-Vocabulary Landmarks</p>
<p>We now describe how to localize landmarks in VLMaps with free-form natural language.The overview of the indexing process is shown on the right of Fig. 2. Formally, we define the input language list as L = [l 0 , l 1 , . . ., l M ] where l i represents the i-th category in text form, and M represents the number of categories defined by the user.Some examples of the input language list are ["chair", "sofa", "table", "other"] or ["furniture", "floor", "other"].As Li et al. (Li et al. 2021), we apply the pre-trained CLIP text encoder (Radford et al. 2021) to convert such list of texts into a list of vector embeddings [e 0 , e 1 , . . ., e M ], e ∈ R C , which are organized into an embedding matrix E ∈ R M ×C , where each row of the matrix represents the embedding of a category.The map embeddings M are also flattened into a matrix Q ∈ R H W ×C , where each row represents the embedding of a pixel in the top-down grid map.We then compute the pixel-to-category similarity matrix S = Q • E T , where S ∈ R H W ×M .Each element S ij in the matrix stores the similarity value between a pixel and a text category, indicating how likely this pixel belongs to the class.By applying the argmax operator along the row direction to S and reshaping the resulting vector to shape H × W , we get the final segmentation result R ∈ R H× W .Each element R ij represents the label index of the input language list L at the grid map location (i, j).With the final resulting matrix R, we compute the most related language-based category for every pixel in the grid map.</p>
<p>Generating Open-Vocabulary Obstacle Maps</p>
<p>Building a VLMap enables us to generate obstacle maps that inherit the open-vocabulary nature of the VLMs used (LSeg and CLIP).Specifically, given a list of obstacle categories described with natural language, we can localize those obstacles at runtime to generate a binary map for collision avoidance and/or shortest path planning, as is shown in Fig. 3.A prominent use case for this is sharing a VLMap of the same environment between different robots with different embodiments (i.e., cross-embodiment problem (Zakka et al. 2022;Ganapathi et al. 2022)), which may be useful for multiagent coordination (Wu et al. 2021).For example, a large mobile robot may need to navigate around a table (or other large furniture), while a drone can directly fly over it.By simply providing two different lists of obstacle categoriesone for the large mobile robot (that contains "table"), and another for the drone (that does not), we can generate two distinct obstacles maps for the two robots to use respectively, sourced on-the-fly from the same VLMap.</p>
<p>To do so, we first extract an obstacle map O ∈ {0, 1} H× W where each projected position of the depth point cloud in the top-down map is assigned 1, and otherwise 0. To avoid points from the floor or the ceiling, points P W are filtered out depending on their height, O ij = 1, t 1 ≤ P y W ≤ t 2 and p x map = i and p y map = j 0, otherwise (2) where t 1 , t 2 ∈ R are the lower and upper thresholds for the y-component (we define y axis to be along the height direction) of the point P W . Second, to obtain obstacle maps tailored to a certain embodiment, we define a list of potential obstacle categories L obs = [l obs0 , l obs1 , . . ., l obsM ], where l obsi represents the i-th obstacle category in language, and M represents the total number of obstacle categories defined by the user.We then apply the open-vocabulary landmark indexing introduced in Sec.3.2 and obtain segmentation masks for all defined obstacles.For a specific embodiment k, we choose a subset of classes out of the whole potential obstacle list L obs and take the union of their segmentation masks to get the obstacles mask Õem k .We ignore false predictions of obstacles on floor region in Õem k by taking the intersection with O to get the final obstacle map O em k .</p>
<p>Zero-Shot Spatial Goal Navigation from Language</p>
<p>In this section, we describe our approach to long-horizon (spatial) goal navigation, given a set of landmark descriptions specified by natural language instructions such as move first to the left side of the counter, then move between the sink and the oven, then move back and forth to the sofa and the table twice</p>
<p>Notably different from prior work (Gadre et al. 2023;Shah et al. 2023), VLMaps allow us to reference precise spatial goals such as: "in between the sofa at the TV" or "three meters to the east of the chair."Specifically, we use a large language model (LLM) to interpret the input natural language commands and break them down into subgoals (Ahn et al. 2022;Shah et al. 2023;Zeng et al. 2023).In contrast to prior work, which may reference these subgoals with language and map to lowlevel policies with semantic translation (Huang et al. 2022a) or affordances (Ahn et al. 2022;Huang et al. 2022b;Zeng 2019), we leverage the code-writing capabilities of LLMs to generate executable Python robot code (Liang et al. 2023b;Mees et al. 2023;Chen et al. 2021b;Brown et al. 2020) that can (i) make precise calls to parameterized navigation primitives, and (ii) perform arithmetic when needed.The generated code can directly be executed on the robot with the built-in Python exec function.</p>
<p>Note that recent works (Liang et al. 2023b;Mees et al. 2023;Chen et al. 2021b;Brown et al. 2020) have shown that code-writing language models (e.g., Codex (Chen et al. 2021b)) trained on billions of lines of code from Github can be used to synthesize new simple Python programs from docstrings.In this work, we re-purpose these models for mobile robot planning by priming them with several input examples of natural language commands (formatted as comments) paired with corresponding robot code (via fewshot prompting).The robot code can express functions or logic structures (if-then-else statements or for/while loops) and parameterize API calls (e.g., move_to(target_name) or turn(degrees).The full list is available in Table 1) that map to spatial behaviors specified by the language commands.The full prompt is shown in Fig. 4.</p>
<p>Spatial goals are defined as positions around the reference object based on spatial descriptions.For example, "in the middle of the counter and the fridge", or "to the left of the sofa" etc. Traditional object goal navigation methods either directly retrieve the target object's location on the map and plan to it (Shah et al. 2023;Gadre et al. 2023) or are trained to approach objects as a reactive system (Chaplot et al. 2020).These methods fall short in reaching spatial goals since these goal locations are free space rather than retrievable locations on semantic maps.However, when we know the reference position, those spatial locations can be computed with simple offsets.The navigation primitive functions (APIs) being called by the language model (e.g., move_to_left('counter')) use a pre-generated VLMap to localize the coordinates of the open-vocabulary landmarks ("counter") in the maps (described in Sec.3.2) modified with predefined scripted offsets (to define "left").We then navigate to these coordinates using an off-the-shelf navigation stack (Quigley 2009) that takes as input the embodiment-specific obstacle map (generated using the same VLMap, with the process described in Sec. 3.3).Some examples of spatial goal navigation in the real world is shown in Fig. 5.</p>
<p>At test time, the LLM is prompted with context examples (in gray) in Fig. 4 as well as commands (in green) in   turn until the object is on the robot's left side.</p>
<p>with_pos_on_right(object_name)</p>
<p>turn until the object is on the robot's right side.move_in_between(object_a, object_b) move in between two objects.face(object_name)</p>
<p>turn until the robot's front is pointing to the object.</p>
<p>turn(angle)</p>
<p>turn right a certain angle.If the angle value is negative, turn left.</p>
<p>turn_absolute(angle)</p>
<p>turn to absolute angle.0 is north, 90 is east, -90 is west, 180 is south.</p>
<p>move_north(object_name)</p>
<p>move to the north side of the object.</p>
<p>move_south(object_name)</p>
<p>move to the south side of the object.</p>
<p>move_east(object_name)</p>
<p>move to the east side of the object.</p>
<p>move_west(object_name)</p>
<p>move to the west side of the object.move_forward(dist) move forward "dist" meters.# move first to the left side of the counter, then move between the sink and the oven, then move back and forth to the sofa and the table twice robot.move_to_left('counter')robot.move_in_between('sink','oven') pos1 = robot.get_pos('sofa')pos2 = robot.get_pos('table') for i in range(2): robot.move_to(pos1)robot.move_to(pos2)# move 2 meters north of the laptop, then move 3 meters rightward robot.move_north('laptop')robot.face('laptop')robot.turn(180)robot.move_forward(2)robot.turn(90)robot.move_forward(3) Figure 6.The query and the generated results from the LLM for spatial goal navigation tasks.During the query, the context prompt in Fig. 4 and the input task commands are prompted to the LLM together.The input task commands are in green and generated outputs are highlighted Fig. 6.It can autonomously re-compose API calls to generate new robot code that not only references the new landmarks mentioned in the language commands (as comments), but also can chain together new sequences of API calls to follow unseen instructions accordingly.The inference process is shown in Fig. 6, and generated outputs are highlighted .</p>
<p>Building an Audio Visual Language Map</p>
<p>VLMaps provide an intuitive interface for humans to issue natural language commands to robots, enabling openvocabulary spatial goal navigation.However, interacting with the world is inherently a multimodal experience.Since VLMaps primarily rely on visual information for map construction, it is important to consider other complementary sources of information that can enhance navigation.Motivated by this, we broaden the scope of VLMaps and propose a more general framework, AVLMaps, which enables robots to integrate and interpret multimodal information, including audio, images, and language, within a unified representation (as illustrated in Fig. 7).In this paper, we present the extension to audio and image modalities as a case study, showcasing the modularity and extensibility of our multimodal spatial language maps.The AVLMaps framework is flexible and designed to accommodate future integration of additional sensing modalities, such as temperature, tactile feedback, or magnetic fields.The key idea behind AVLMaps is to combine visual localization features, pre-trained visuallanguage features, and audio-language features with a 3D reconstruction.Given an RGB-D video stream with an audio track and odometry information, we utilize four modules to build a multimodal features database.Given a specific query, each module returns predicted spatial locations on the map in the form of 3D voxel heatmaps.A heatmap can be denoted as H ∈ [0, 1] H× W × Z , where H, W and Z represent the size of the voxel map and the value in each element represents the probability of being Prepared using sagej.cls
p = (x, y, z) T , {x, y, z ∈ Z | 1 ≤ x ≤ H, 1 ≤ y ≤ W , 1 ≤ z ≤ Z} is a voxel position in the map H.
Visual Localization Module.The overview of the module is shown in Fig. 8.The main purpose of this module is to localize a query image in our map.To achieve this goal, we follow a hierarchical localization scheme (Sarlin et al. 2019(Sarlin et al. , 2020)).We first compute the NetVLAD (Arandjelovic et al. 2016) global descriptors and SuperPoint (DeTone et al. 2018) local descriptors for all images from the RGB stream during exploration and store them with the corresponding depth and odometry as a reference database.During inference, we compute the global and local descriptors for the query image in the same manner.By searching the nearest neighbor of the query NetVLAD features in the reference database, we can find a reference image as our candidate.Next, we use SuperGLUE (Sarlin et al. 2020) to establish key point correspondences between the query image and the reference image we retrieve with NetVLAD from the database with their local SuperPoint features.With registered depth, we back-project the reference image's key points into the 3D space and obtain the 3D-2D correspondences for the query key points.In the end, we can apply the Perspective-n-Point method (Fischler and Bolles 1981) to estimate the query camera pose relative to the reference camera, and thus obtain the global camera pose with the odometry of the reference camera.</p>
<p>In the visual localization module, the predicted global camera location is denoted as p v = (x v , y v , z v ) T .In the heatmap H v , we define the probability at p v as 1.0, and the probability linearly decays around this location according to the distance on the top-down map:
H v (p) = max(1.0 − ϵ • dist xy (p, p v ), 0) (3) dist xy (p, q) = (p x − q x ) 2 + (p y − q y ) 2 (4)
where ϵ is the decay rate, and dist xy (p, q) denotes the distance between 3D vectors p and q on the xy-plane.for pixel-level feature generation from the RGB image and to associate these features with the back-projected depth pixels in the 3D reconstruction.Different from Sec. 3.1, we don't project the 3D points into the top-down plane to create a 2D grid map but maintain a 3D voxel map where each voxel is associated with a visual-language feature.When there are multiple points projected into the same voxel, we store their mean features at the voxel.The inference process is similar to Sec. 3.2.We define a list of categories in natural language and encode them with the language encoder.We compute the cosine similarity scores between all voxel-wise features and language features and use an argmax operator to select the top-scoring voxels for a certain category in the map.Depending on the application, the top-scoring 3D voxel points for a certain category can be used as the target point cloud for manipulation tasks or can be projected onto a topdown map for navigation purposes.The object localization results are a list of points, denoted as {p oi = (x oi , y oi , z oi ) | i = 1, . . ., N } where N is the total number of points for the target object.We define the probabilities for all these locations as 1.0 in heatmap H o , and the probability linearly decays around these locations based on the Euclidean distance:
d min (p) = min{dist(p, p oi ) | i = 1, . . . , N } (5) H o (p) = max(1.0 − ϵ • d min (p), 0)(6)
where d min (p) denotes the minimal distance between p and all object points {p oi | i = 1, . . ., N }, dist(p, q) denotes the Euclidean distance between p and q.In AVLMaps, the object localization module is also used to generate a 2D obstacle grid map for path planning, as is shown in Sec.3.3.Different from the 2D feature grid map of a VLMap, the visual language map is now in the form of a 3D voxel grid where each occupied voxel is associated with a feature.We prompt the map with a list of free area concepts (e.g., "floor") and obstacle concepts (e.g., "chair", "table", "counter", and "other") for a specific embodiment, assigning a score to each concept for every voxel.Each voxel is then labeled with the concept that achieves the highest score, resulting in a 3D semantic voxel map.We merge the voxels labeled with obstacle concepts into a combined obstacle map, and then perform a topdown projection to produce a 2D obstacle grid map.In this grid, all pixel locations corresponding to projected obstacle voxels are marked as occupied, while all others are marked as free and navigable.It is worth noting that beyond object categories, AVLMaps also support the definition of audio or images as obstacles.For example, we can define "the sound of glass breaking", or a list of images as obstacles, generate their corresponding heatmaps, and treat locations with heat above a certain threshold as obstacles.This capability is useful when the forbidden regions are hard to describe with only object descriptions, like a glass-breaking scene without surrounding objects, or a region specified with a cell phone video.Area Localization Module.While the object localization module is good at extracting object segments on the map, it falls short of localizing coarser goals such as regions (e.g., "the area of the kitchen").This is because the visual encoder for generating pixel-aligned features is obtained by finetuning a pre-trained model on a segmentation dataset, leading to the notorious catastrophic forgetting effect.Therefore, the visual encoder is better at segmenting common objects while worse at recognizing general visual concepts (Jatavallabhula et al. 2023).To take advantage of both pre-trained and fine-tuned methods, we propose to build an extra sparse topological CLIP features map similar to (Shah et al. 2023).The idea is to compute the CLIP visual features (Radford et al. 2021) for all images from the RGB stream and associate the features with corresponding poses.During inference, given the language concept like "the area of a bedroom", we compute the language features with the CLIP language encoder and the image-to-language cosine similarity scores.These similarity scores indicate how likely these images match the language description.The odometry together with the score of each image indicates the predicted location with a confidence value.</p>
<p>The area localization results are a list of positionconfidence pairs, denoted as {(p ai , s ai ) | i = 1, . . ., M } where M is the total number of frames in the input RGB-D stream.The scores s ai are normalized between 0 and 1.We define the probability for each point p ai on the heatmap H a as its score s ai , and the probability linearly decays around the point on the xy-plane direction:
Ha (p) = max i=1,...,M {s ai − ϵ • dist xy (p, p ai )} (7) H a (p) = max( Ha (p), 0)(8)
where the max operator for the curly brackets means taking the highest probability when a location is inside the affected regions for several p ai .Audio Localization Module.The overview of the module is shown in Fig. 10.In this module, we utilize the audio information from the input stream.The key idea is to compute the audio-lingual features with audio-language pre-trained models such as wav2clip (Wu et al. 2022), AudioCLIP (Guzhov et al. 2022) or CLAP (Elizalde et al. 2023).We first segment the whole audio clip into several segments with silence detection.Whenever the volume is above a threshold, we mark this time step as the starting point of a segment.Whenever the volume of the sound is not larger than this threshold for a certain duration, we end the segment.In the next step, we compute the audio features for each segment with pre-trained audio-language models and associate the features with the odometry at the specific segment.During inference, given a language description of the sound, like "the sound of door knocks", we encode the language into language features and compute the matching scores between the language and all audio segments in the same way as in the object localization module.The odometry associated with the top-scoring segment is the predicted location.</p>
<p>The audio localization results are similar to those of the area localization module.The position-score pairs are denoted as {(p si , s si ) | i = 1, . . ., K} where K is the total number of sound segments in the input video stream.The heatmap H s is defined as:
Hs (p) = max</p>
<p>Cross-Modality Reasoning</p>
<p>A key advantage of our method is its capability to disambiguate goals with additional information, even from different modalities.The goal of the cross-modality reasoning method is to output a target location of a specific concept (for example, "the sofa") given the information of other nearby concepts (for example, "near the sound of glass breaking").In the last section, we introduced how we generate heatmaps for concepts of different modalities.</p>
<p>Given these heatmaps, we want to further narrow down the target location.</p>
<p>Cross-Modal Reasoning.The main idea of our crossmodal reasoning method is shown in Fig. 11.We treat the predictions from four modules as four modalities.When there are several queries referring to different modalities, we compute the respective heatmaps first and then perform element-wise multiplication among all heatmaps:
H target = H 1 ⊙ . . . ⊙ H L (11)
where ⊙ is the element-wise multiplication operator, and L is the total number of referred modalities.We extract the position on the target heatmap H target that has the highest probability as the predicted location.</p>
<p>When we compute the heatmaps, there is always a primary heatmap while others are auxiliary ones.For example, in Fig. 11, in the query "the sound of baby crying near the sofa", the heatmap for "the sound of baby crying" is the primary heatmap, while the heatmap for "the sofa" is the auxiliary.We set the decay rate for the primary heatmap higher (e.g., 0.1 in this work for voxel map with 0.05 meter voxel size) since we want to know the exact location of the target while tuning the decay rate for the auxiliary heatmap lower (e.g., 0.01) as having a broader effect area to narrow down major targets is desirable.More specifically, a higher decay rate indicates that the relevancy to the concept decreases faster when the location is farther away from the concept locations (heat value decreases 0.1 for every 0.05 meter).When there are multiple concepts or modalities mentioned in the target specification, the map with a high decay rate refers to the target concept we want the robot to get closer to.Those maps with low decay rates serve as constraints to select targets in the main map.From another perspective, the decay rates of multiple maps represent the importance weights we assign to their corresponding concepts.The higher the decay rate is, the more important that concept is, and we want the robot to get closer to that concept.When the decay rates of two concepts are the same, the two concepts are equally important, and the fusion of the two concepts' heatmaps will peak at the middle points between those concepts.</p>
<p>It is worth noting that our cross-modal reasoning method is relatively simple and has the underlying assumption that the heatmaps from different modalities are conditionally independent.Despite this simplification, our method proves to be effective across our experiments.Nonetheless, exploring more advanced cross-modal fusion techniques remains an exciting direction for future work.</p>
<p>Multimodal Goal Navigation from Language</p>
<p>In the setting of multimodal goal navigation from language, the agent is given language descriptions referring to targets from different modalities (e.g., sounds, images and objects), and is required to plan paths to them.While most of the previous navigation methods focus mainly on a specific type of goal, we unify these tasks with the help of large language models (LLMs).Following a similar spirit to Sec. 3.4, we use an LLM to interpret the natural language commands and synthesize API calls combined with simple logic structures in the form of executable python code.In the following, we will detail the code generation process for multimodal goal navigation, including (i) the introduction of the API library as a tool set for the LLM to use during code generation, (ii) the mechanism of spatial goal reasoning, (iii) the way we generate multimodal maps with code, (iv) the method to achieve cross-modal reasoning with code, and (v) the conversion of code to navigation commands that robots receive.At the end, we provide the prompt and an example of inference for our method.Navigation API library.In   During the query, the context prompt in Fig. 12 and the input task commands are prompted to the LLM together.The input task commands are in green and generated outputs are highlighted Spatial Goal Reasoning.We follow the intuition in Sec.3.4 that spatial locations can be computed with simple math during code generation.For example, the location of "in between the counter and the fridge" can be obtained by getting the positions of the counter and the fridge respectively and apply an average to the two locations.</p>
<p>In the multimodal goal navigation prompt, we reduce the spatial goal API calling examples to lay the focus more on multimodal targets and cross-modal reasoning.In principle, more diverse spatial concept reasoning examples could be integrated into the prompt, as is shown in Sec.3.4.</p>
<p>Cross-Modal Reasoning with Code.As is introduced before, the logic of the cross-modal reasoning is relatively simple, which is just an element-wise multiplication of all relevant heatmaps.In the code, it can be performed with one line of code fusemap = map1 * map2 .</p>
<p>Navigation Commands from Code.</p>
<p>The API move_to(pos) takes a 3D voxel grid position as input, projects it onto the 2D grid map.It applies a planning algorithm on the grid map to generate a path on the map.The points on the path are treated as a list of subgoals and are used to generate a list of low-level actions (the action space contains turning left or right 5 degrees and moving forward 0.25 meter) to reach them sequentially.In the end, the list of low-level action commands is executed to finish the navigation instruction.Prompt and an Inference Example.Since large language models are great few-shot learners (Brown et al. 2020), they are able to learn and imitate internal patterns when several simple examples are provided as context in addition to the direct query.To enable the LLM to understand how to use our APIs, we need to provide a few examples of how to use these APIs to tackle tasks described with language instructions.During the inference, we prompt the full context prompts (in gray) in Fig. 12 together with task command (in green) in Fig. 13 and an example of the generated outputs are highlighted .In our work, we use OpenAI's text-davinci-003 model as our LLM for all experiments.</p>
<p>Experiments</p>
<p>In this section, we aim to evaluate our multimodal spatial map representations in a variety of tasks.More specifically, we address nine key questions: (i) how is VLMaps' spatial language goals navigation performance compared to recent open-vocabulary navigation baselines (Sec.4.1), (ii) whether VLMaps with their capacity to specify open-vocabulary obstacle maps can provide utility in improving the navigation efficiency of different robot embodiments (Sec.4.2), (iii) how AVLMaps enable a robot to navigate to multimodal goals, including sound, image, and object queries (Sec.4.4), (iv) how our cross-modality reasoning approach helps a robot to disambiguate goals with multimodal information (Sec.4.5), (v) how the performance of AVLMaps scales with recent advanced foundation models specialized in different modalities (Sec.4.7), and (vi) how AVLMaps' multimodal indexing and reasoning capabilities translate to real-world environments, empowering robots with diverse embodiments to perform tasks that demand comprehensive multimodal understanding, such as mobile navigation (Sec.4.8) and table-top manipulation with multimodal prompts (Sec.4.9).</p>
<p>Zero-Shot Spatial Goal Navigation from Language</p>
<p>Experimental setup.We use the Habitat simulator (Savva et al. 2019) with the Matterport3D dataset (Chang et al. 2017) for the evaluation of multi-object and spatial goal navigation tasks.The dataset contains a large set of realistic indoor scenes that help evaluate the generalization capabilities of navigating agents.To evaluate the creation of open-vocabulary multi-embodiment obstacle maps, we adopt the AI2THOR simulator due to its support of multiple agent types, such as LoCoBot and drone.In these two environments, the robot is required to navigate in a continuous environment with actions: move forward 0.05 meters, turn left 1 degree, turn right 1 degree and stop.</p>
<p>For map creation in Habitat, we collect 12,096 RGB-D frames across ten different scenes and record the camera pose of each frame.</p>
<p>Baselines.We evaluate VLMaps against three baseline methods, all of which utilize visual-language models and are capable of zero-shot language-based navigation:</p>
<p>• LM-Nav (Shah et al. 2023) creates a graph where image observations of an environment are stored as nodes while the proximity between images are represented as edges.By combining GPT-3 and CLIP, it parses language instructions into a list of landmarks and plans on the graph towards corresponding nodes.</p>
<p>• CLIP on Wheels (CoW) (Gadre et al. 2023) achieves language-based object navigation by building a saliency map for the target category with CLIP and GradCAM (Selvaraju et al. 2020).By thresholding the saliency values, it retrieves a segmentation mask for the target object category and then plans the path on the map.</p>
<p>• CLIP-features-based map (CLIP Map) is an ablative baseline that generates a feature map for the environment in a similar way as ours.Instead of using LSeg visual features, it projects the CLIP visual features onto the map averaged across views.Object category masks are generated by thresholding the similarity between map features and the object category features.</p>
<p>For additional context and analysis, we also report results from a system that has access to a ground truth semantic map for navigation (GT Map), to provide a systems-level upper bound on performance.</p>
<p>Tasks Collection.In these experiments, we investigate the performance of VLMaps versus other baselines for zeroshot spatial goal navigation from language.Our benchmark consists of 21 trajectories in seven scenes, with manually specified corresponding language instructions for evaluation.Each trajectory contains four different spatial locations as subgoals.Examples of subgoals are "east of the table ", "in between the chair and the sofa", or "move forward 3 meters".There are also instructions for the robot to realign itself in reference to nearby objects such as "with the counter on your right".We only consider a subgoal as having been achieved, when the robot reaches the subgoal location within a range of one meter.To evaluate the long-horizon navigation capabilities of the agents, we compute the success rate (SR) of continuously reaching one to four subgoals in a sequence, shown in Tab. 3.For all map-based methods, including CoW, CLIP Map, ground truth semantic map, and our method, we apply the code generation techniques introduced in Sec.3.4.For LM-Nav, we simply use the same parsing method in the original paper (Shah et al. 2023) to break down the language instruction into subgoals.Tab. 3 summarizes the zero-shot spatial goal navigation success rates.Our method outperforms other baselines in this task.Different from object navigation tasks where agents only need to approach a certain object type within a range, disregarding the relative spatial shift to the object, the language-based spatial goal navigation tasks require the robot to accurately arrive at the described location in reference to the object.This poses a bigger challenge to  the landmark localization ability of the method.The low localization ability of CoW and CLIP Map leads to their high failure rates in this task.</p>
<p>Cross-Embodiment Navigation</p>
<p>Experimental Setup.We collect 1,826 RGB-D frames across ten rooms in AI2THOR (Kolve et al. 2017) and build the VLMaps for these scenes.We study the ability of VLMaps to improve navigation efficiency by retrieving different obstacle maps for different embodiments (given the same VLMap) in navigation tasks.We evaluate more than 100 sequences of object subgoals in the AI2THOR simulator.</p>
<p>We evaluate VLMaps on both a LoCoBot and a drone to test its capability of generating obstacle maps at runtime for multi-embodiment navigation.</p>
<p>Obstacle Maps Generation.We apply the open-vocabulary obstacle map generation method in Sec.3.3 to create an obstacle map for the drone (drone map) and one for the LoCoBot (ground map) by defining obstacles for them differently.For the LoCoBot (ground robot), we first define a potential obstacle list as ["chair", "wall", "wall above the door", "table", "window", "floor", "stairs", "other"] and perform open-vocabulary landmark indexing.Later, we only select the union of the masks for the objects "wall", "chair", "table", "window", "stairs", "other" as the obstacle map.For the drone (flying robot), we perform landmark indexing with the potential obstacle list: ["chair", "sofa", "wall", "table", "counter", "window", "floor", "stairs", "ceiling lights", "cabinet", "counter support", "other"].Afterwards, we take union of the masks for ["wall", "window", "stairs", "ceiling lights", "cabinet", "other"] to generate the obstacle map.</p>
<p>Baselines.We test the navigation ability of these embodiments with three setups: a LoCoBot with a ground map, a drone with a ground map, and a drone with a drone map.</p>
<p>Metrics.We evaluate the Success Rate (SR) and the Success rate weighted by the (normalized inverse) Path Length (SPL) (Anderson et al. 2018a) defined as: li) where N is the total number of evaluated tasks, S i ∈ {0, 1} is the binary indicator of success, l i denotes the ground truth shortest path length, and p i denotes the actual path length of the agent in navigation.This metric indicates how efficient the actual path is compared to the ground truth shortest path when the navigation task is achieved.In our three setups, the ground truth trajectories for the LoCoBot and the drone are planned on floor-level and on height level of 1.7 meters respectively.
SP L = 1 N N i=1 S i li max(p i ,
The results provided in Tab. 4 show that the average navigation success rates of the ground-map version of the LoCoBot and the drone are similar because the same obstacles map is used for planning.However, there is an obvious gap between their SPL values.This is because when the drone does not have access to a customized obstacle map, it fails to benefit from flying over ground objects to improve the navigation efficiency.In contrast, while achieving similar success rate compared to the drone with a ground map, the drone with a drone map manages to navigate with higher path efficiency, reflected by the increased SPL values.The comparable SPL values for the drone with the drone map and the LoCoBot with the ground map shows that VLMaps help to generalize the navigation efficiency among different embodiments.An example of the multi-embodiment object navigation task is shown in Fig. 14, where by defining a more efficient obstacles map, the drone flies over the sofa and reaches the laptop target directly, while the LoCoBot has to move aside first to avoid colliding with the sofa.</p>
<p>Multimodal Navigation Simulation Setup</p>
<p>Exerimental setup.We use the Habitat simulator (Savva et al. 2019;Szot et al. 2021) with the Matterport3D dataset (Chang et al. 2017) for the evaluation of multimodal navigation tasks.For mapping purposes, we manually collect RGB-D video streams in the simulator across ten different scenes and add random audio tracks to the videos to simulate the audio sensing modality.All audio comes from the validation fold (Fold-1) of the ESC-50 dataset (Piczak 2015), which contains 50 categories of common sounds.In navigation tasks, the robot has four actions to take: move forward 0.1 meters, turn left 5 degrees, turn right 5 degrees, and stop.In sequential goal setting, the robot is required to navigate to a sequence of goals and take the stop action when it reaches each subgoal.When the stop position is less than one meter away from the ground truth position, the subgoal is considered successfully finished.</p>
<p>Tasks collection.In multimodal goal navigation tasks in Sec.4.4, we consider three kinds of goals: image goals, object goals, and sound goals.For image goals, we randomly sample positions and orientations on the top-down map and render images as targets.For object goals, we access the metadata (e.g., bounding boxes and semantics) from the Matterport3D dataset and sample a list of categories in each scene as queries.For sound goals, we randomly sample sound classes of audio merged with the mapping videos as targets, treating the video frame positions as the ground truth.</p>
<p>In cross-modal goal indexing tasks in Sec.4.5, we collect three types of datasets:</p>
<p>• Visual-Object cross-modal indexing We manually select image-object pairs on the top-down map for localizing "an object X near the image Y".</p>
<p>• Area-Object cross-modal indexing We access the region and object metadata (e.g., bounding boxes and semantics)</p>
<p>Prepared using sagej.clsfrom the Matterport3D dataset to automatically generate a list of object-region pairs.This dataset is for localizing "an object X in the area of Y".</p>
<p>• Object-Sound cross-modal indexing We manually insert several sounds of the same kind into a scene and select for each sound location a nearby object for disambiguation.The query is "a sound X near the object Y".</p>
<p>In cross-modal goal navigation in Sec.4.6, we randomly sample starting pose in ten scenes and treat the visual-object and object-sound cross-modal goals in Sec.4.5 as navigation goals.For all cross-modal navigation and indexing tasks, we use the prompt introduced in Sec.3.7 to generate navigation commands.</p>
<p>Multimodal Goal Navigation</p>
<p>Sound goal navigation.We first test AVLMaps in sound goal navigation tasks.We collect 200 sequences of sound goals in ten different scenes.In each sequence, there are four sound categories that require the robot to reach.The results are shown in Tab. 5. We generate AudioCLIP (Guzhov et al. 2022) features with our audio localization module and match all audio with the target sound category in the embedding space, similar to a text-to-audio retrieval setup.Then, the agent plans a path to the audio position.We tested different ranges of sound categories inserted into the map.The full list of sound categories in each major class can be found in the link 1 .The results show that our agent manages to recognize sound goals and navigate with a 77.5% success rate.Visual and object goals navigation.We then test AVLMaps with visual and object goal navigation tasks.The agent is given an image and two object categories in the language in one sequence of tasks and asked to navigate to the image  goal and two object goals in sequence.The success rate for 200 sequences of tasks in ten scenes is reported in Tab. 6.The results show that our method enables the agent to navigate to goals from different modalities.</p>
<p>Cross-Modal Goal Indexing</p>
<p>When we refer to a goal with language, it is likely that the goal can be found in more than one place in the environment.</p>
<p>A major strength of our method is that it can disambiguate goals with multimodal information.In this experiment, we will show the cross-modal goal reasoning capability of AVLMaps.</p>
<p>Area-Object goal indexing.In this setup, we use an area description to disambiguate the object goal.We collected Prepared using sagej.cls 100 indexing tasks in ten scenes.Each task consists of an object category and a region category (e.g., "living room", "kitchen", "dining room", "bathroom" etc.).The agent needs to predict the correct object location which is inside the region.The top-one recall with different distance tolerance is reported in Tab. 7. We notice that VLMaps (Huang et al. 2023b) struggles to find the goal in the correct region because VLMaps integrates visual-language features from the encoder fine-tuned on the instance segmentation dataset, improving its segmentation performance on common objects while dropping its ability to recognize more general concepts like regions.In contrast, ConceptFusion integrates pre-trained CLIP features into the map without fine-tuning, enabling it to recognize general concepts including regions, and thus the indexing results are improved.Object-Sound goal indexing.In this setting, we use object goals to disambiguate sound goals.We collected 119 indexing tasks, each of which consist of a sound category and a nearby object category.Each sound category in a scene can be heard at more than one location, introducing ambiguity to the localization scenario.The recall is reported in Tab. 8.With the combination of object and audio localization modules, our method largely increases the recall rate for localizing the correct sound goal position in ambiguous scenarios.</p>
<p>Method</p>
<p>Recall@1 (%) Average min.distance (m) &lt;0.5m &lt;1m &lt;1.5m &lt;2m Visual-Object goal indexing.In visual-object goal indexing tasks, visual clues are used to resolve ambiguity.Given an object category and an image, our method can localize the correct object near the image position with over 60% of recall for 0.5 meters distance tolerance, as is shown in Tab. 9.</p>
<p>Multimodal Ambiguous Goal Navigation</p>
<p>In this part, we test our method with ambiguous goal navigation tasks.We collect 119 sequences of tasks.In each task, the agent is required to navigate to an ambiguous sound goal (i.e., "move to the sound of baby crying near the sofa") and an ambiguous object goal (i.e., "move to the counter near {image of the kitchen}") sequentially.The category of the object near the sound and the image taken near the object are provided.These tasks require the agent to reason across different modalities to accurately localize the target.</p>
<p>We consider two single-modality baselines: VLMaps (Huang et al. 2023b) and AudioCLIP (Guzhov et al. 2022) and one multimodal baseline.The multimodal baseline uses VLMaps as the object localization module, wav2clip (Wu et al. 2022) as the audio localization module and the same visual localization module as our method.The results are shown in Tab.10.We observe that AVLMaps navigates to cross-modal goals with a 24.2% higher success rate to ambiguous sound goals and with a 2.1% higher success rate to ambiguous object goals compared to the alternative multimodal baseline.</p>
<p>Scaling Experiment</p>
<p>Since AVLMaps is highly modular, each module can be upgraded with advanced foundation models that generate similar audio-language features or visual-language features.In this section, we explore whether AVLMaps can evolve with the advancement in foundation model research.We follow similar settings of multimodal goal navigation as in Sec.4.4 to test the AVLMaps modules supported by different foundation models.In this experiment, the robot needs to navigate to a sound goal, a visual goal, and two object goals in a sequence and we report the in-a-row navigation success rate as well as the success rate for each type of goal.In this experiment, we mainly focus on analyzing how the performance scales with improved Audio Language Prepared using sagej.clsModels and Vision Language Models.We fixed the visual localization module as NetVLAD and SuperGLUE.</p>
<p>Audio Language Models.We compare the downstream performance of using AudioCLIP (Guzhov et al. 2022) with respect to a more recent CLAP (Elizalde et al. 2023) model.</p>
<p>Vision Language Models.In order to leverage pretrained Vision Language Models, we require the encoder to generate pixel-wise features in CLIP embedding space.We compared LSeg (Li et al. 2021), the VLM used in VLMaps (Huang et al. 2023b), with OVSeg (Liang et al. 2023a) and a method that uses SAM (Kirillov et al. 2023) and CLIP (Radford et al. 2021) introduced in HOV-SG (Werby et al. 2024).The method in HOV-SG first leverage SAM (Kirillov et al. 2023) to generate class-agnostic masks.Each region cropped with a mask and its zero-background version are encoded with a CLIP image encoder and their embeddings are summed with weights.The resulting embedding is assigned to all pixels in the masked region.</p>
<p>Results.We report the results in Table 11.The first two rows compare the success rates of different Audio Language Models (ALMs) using the same VLM.We observe a significant 10% improvement in navigating to sound goals when replacing AudioCLIP with CLAP.This improvement is largely due to CLAP's more extensive pretraining on larger, more diverse datasets with advanced augmentation techniques (Elizalde et al. 2023).From the second to the final rows, we compared three VLMs that generate dense visual-language features but fixed CLAP as the ALM.However, recent VLMs did not offer clear benefits.In fact, the SAM+CLIP combination significantly reduced performance.Upon closer inspection, we found that while SAM+CLIP performed well in previous work (Werby et al. 2024), it requires extensive hyper-parameter tuning to adapt to different scenes.The quality of SAM's masks is highly sensitive to its parameters, and CLIP struggles to interpret masked regions, as highlighted by OVSeg (Liang et al. 2023a).OVSeg addressed this by training learnable mask prompts, and improving 2D semantic segmentation through ensemble techniques.However, these methods are designed to enhance segmentation performance, not visuallanguage feature generation.Like LSeg, OVSeg is fine-tuned on similar datasets, offering limited improvements in dense pixel-level visual-language features.As a result, AVLMaps' object localization module, which relies on these features, sees minimal benefit from OVSeg.However, we believe that pixel-level visual language features can be improved with access to more high-quality segmentation datasets in the future, boosting the performance of the object localization module in AVLMaps.</p>
<p>Real World Experiment for Mobile Robot</p>
<p>To answer the question of how AVLMaps applies to realworld environments and benefits multimodal navigation, we designed a mobile navigation experiment in which the robot must locate sound, image, and object-based goals within an environment containing duplicate objects from certain categories such as chairs, backpacks, shelves and so on.This setup demonstrates how AVLMaps enables the robot to retrieve multimodal concepts and disambiguate goals by leveraging information from additional modalities.</p>
<p>Robot Setup.In the real-world experiment setting, we use a mobile robot equipped with a Ridgeback omnidirectional platform from Clearpath Robotics as the mobile base, and a Panda manipulator from Franka Emika.We mount a RealSense D435 RGB-D camera at the gripper of the Panda manipulator.During the mapping, we run a LiDAR localizer to provide the odometry for the robot base and derive the camera pose through the forward kinematics of the robot arm.Environment Setup.We choose a room with multiple ambiguous goals such as tables, chairs, paper boxes, counters, and backpacks, which are shown on the left in Fig. 15.We control the robot in this environment and record RGB-D video.Then we artificially add sounds to the RGB-D video when the robot moves to certain locations.The sound locations are shown on the right in Fig. 15.</p>
<p>Audio Recording and Denoising.To make the experiment more realistic, instead of simply adding clean audio to the video, we recorded environmental noise using a microphone during the robot's exploration.We then overlaid semantic soundtracks from ESC-50 onto the recorded environmental noise at specific locations.This approach provides an approximation of real-world audio conditions with background noise.We also experimented with playing sounds through a speaker in the environment and recording both the environmental noise and semantic audio simultaneously.However, the mobile robot produced significant vibration noise while moving, which masked the played sounds and rendered them largely inaudible.As a result, we approximated the noisy semantic audio by mixing the semantic soundtracks with the recorded environmental noise.</p>
<p>Since current audio language models are sensitive to noise in input audio, we apply a noise filtering algorithm as a pre-processing step to the audio before sending them to the audio localization module.In our case, we use a noise gate, as shown in Fig. 16.A noise gate applies thresholding with smooth fade-in and fade-out transitions.Instead of simply zeroing out values below the threshold, it includes three phases: attack, hold, and release.When the input audio level exceeds the threshold, the output ramps up linearly from zero to the input level over a period defined by the attack time.</p>
<p>Once the input drops below the threshold, the output stays at the input level for the duration of the hold time.If the input remains below the threshold after the hold period, the output level decreases linearly to zero over the release time.This approach effectively reduces environmental noise.We then apply silence segmentation to further extract meaningful audio clips.In our setup, the noise gate threshold is set to -10 dB (dB = 20 * log 10(amplitude)), with an attack time of 250 ms, hold time of 1000 ms, and release time of 170 ms.The silence segmentation threshold is set to 0.1.As noise cancellation is not the primary focus of this work, these parameters were selected based on empirical experience.In the future, a more robust approach would involve adding random real-world noise during audio language model pretraining to improve noise tolerance.</p>
<p>Map Building and Navigation.After collecting the data, we run the AVLMaps mapping offline.For navigation tasks, we provide the AVLMaps and the language instruction as input.The robot parses the instruction (Sec.3.7) and executes the generated Python code for goal indexing and planning.We use the ROS navigation package (Quigley 2009) for global and local planning.We pre-process sound inputs with background noise subtraction to avoid including noise from the robot operation.Multimodal Spatial Goal Reasoning and Navigation with Natural Language.We design 20 language-based multimodal navigation tasks, asking the robot to navigate to sounds, images, and objects.We report an overall success rate of 50%.We also design an evaluation consisting of ten multimodal spatial goals.The agent needs to reason across object, sound, image and spatial concepts.An example is "navigate in between the backpack near the sound of glass breaking and {the image of a fridge}".In the end, six out of ten tasks were successfully finished.We show in Fig. 17 the process of resolving ambiguities in the scene.There are different ambiguous objects in the scenes including paper boxes, backpacks, shelves, tables, chairs, and plates.The first and the second columns in Fig. 17 show the ground truth positions of the target objects and sounds.The third and fourth columns show that AVLMaps can accurately localize objects, sounds, and visual goals in the form of 3D heatmaps.The final column shows that our method can correctly narrow down targets in spite of object ambiguities.We can observe from the figure that AVLMaps can accurately localize ambiguous concept with language, audio and image.We observe that the failures come from the composition of the imperfection of different modules.For example, the object localization module (e.g., VLMaps) fails to recognize rare objects like various toys.It also mistakes some shelves for chairs.Similar failures happen in audio localization module.In the second row and the fourth column in Fig. 17, the church bell sound should be at the top-right corner but the module also gives high score for the sound heard at bottomleft.</p>
<p>Real World Experiment for Table-Top Goal Reaching</p>
<p>In the previous section, we demonstrated how AVLMaps empower a robot to interpret multimodal goals in room-scale mobile navigation tasks.Here, we extend our investigation to assess how AVLMaps benefit a fixed-based manipulator in real-world table-top tasks, which require a more detailed semantic understanding of the scene.In this setup, the robot manipulator must approach multimodal goals with a stricter tolerance for error (within 10 cm).Additionally, we explore AVLMaps' potential for application across robots with varied embodiments.Robot setup.We set up a Panda robot arm from Franka Emika on a table and mount a FRAMOS D345e industrial RGB-D camera at the gripper of the manipulator.Before the experiment, we calibrate the extrinsic matrix from the end effector coordinate frame to the camera frame.We use an HTC Vive VR controller to teleoperate the robot end effector to collect observation data.</p>
<p>Experiment setup.We set up two tabletop scenes where different objects are lying on the table at random locations.</p>
<p>In one scene, we deliberately place duplicate objects on the table to simulate the ambiguity of objects.Subsequently, we teleoperate the robot end effector with a VR controller and collect observations including the RGB, the depth, and the robot end effector poses relative to the robot base coordinate frame.The recording frequency is 30 Hz.Later, we derive the camera poses relative to the base coordinate frame using the end effector pose and the extrinsic matrix obtained during the calibration.For each scene, we first collect a sequence of data for generating maps with the object localization module and the visual localization module.We then control the robot to different areas on the table and collect an episode of data in each region, to which we later insert a random segment of audio sampled from ESC-50 dataset in a similar way as the mobile robot experiment setting.These observation data augmented with audio are used to generate the audio map with the audio localization module.Thanks to the insights we obtained from the scaling experiments in Sec.4.7, we use CLAP (Elizalde et al. 2023) and OVSeg (Liang et al. 2023a) as our foundation models for the audio localization module and the object localization module.For the visual localization module, we still use the NetVLAD (Arandjelovic et al. 2016) and SuperGLUE (Sarlin et al. 2020) scheme as in Sec.3.5.</p>
<p>Tabletop manipulator goal reaching.We randomly moved the robot arm and collected a sequence of RGB images as the visual goals used for querying the AVLMaps created earlier.</p>
<p>Prepared using sagej.clsDuring inference, we prompted the robot with randomly selected visual goals (sampled from the collected images), sound categories (matching the inserted audio types), and object categories (language descriptions of objects on the table), instructing it to approach the target region.If the final position of the robot's end effector was within 10 cm of the ground truth, the trial was considered successful.We tested 20 visual goals, twelve sound goals, and 13 object goals.</p>
<p>The robot successfully reached 100% of the visual and sound goals, and nine out of 13 object goals.Additionally, we tested ten ambiguous goals, such as "the light near the sound of clock alarm" or "the apple near the image {path/to/image}" and the robot successfully approached nine out of ten.In summary, AVLMaps demonstrated excellent performance in a tabletop setting, successfully navigating to multimodal goals, including ambiguous ones.The results are shown in Fig. 18.</p>
<p>Limitations and Discussions</p>
<p>While our multimodal spatial language maps approach is versatile in terms of navigating to various spatially grounded concepts and is effective in the disambiguation of duplicate goals with extra information, it does have certain limitations.</p>
<p>In this section, we thoroughly discuss the major drawbacks of our method, along with potential directions for extension and improvement.Through this analysis, we aim to offer insights to the community and inspire future research.Transient Sound Reaction.One of the main challenges with our AVLMaps is that sound is inherently transient, while the map relies on pre-exploration and offline mapping to embed the information for navigation tasks.Even if sounds are associated with specific locations during the exploration phase, those sounds might disappear or shift to new locations by the time of the inference and navigation.Therefore, AVLMaps struggles to support reactive navigation towards transient sound goals.Previous work on vision and audio navigation (Chen et al. 2020;Younes et al. 2023;Gan et al. 2020b) has focused on enabling robots to respond to transient sounds using biaural and visual observations.However, these methods are limited to transient goals and have been tested only in simulated environments.We argue that both transient sounds and previously heard sounds are essential for intelligent navigation.Transient sounds serve as important cues for immediate action, while past sounds provide valuable references for narrowing down possible targets based on prior experiences.For example, when instructed to go to "the café where you heard the song", we can recall the specific café and adjust our navigation accordingly.A robot must be able to understand and navigate toward both kinds of sound goals to achieve human-level intelligence in sound-based navigation.However, no current system integrates these dual capabilities.We believe this gap presents an exciting opportunity for future research.</p>
<p>Sound Localization.While AVLMaps manages to comprehend the semantic meaning of sounds, it struggles to localize the sound sources.In this paper, the audio localization module assumes monaural audio input, lacking in the ability to utilize binaural audio to localize sound sources with triangulation like humans.More specifically, a sound can be heard in all locations inside a room, but we only associate its features with the robot's current location.Encouragingly, several concurrent works are actively addressing the sounds source localization, including efforts to learn the real-world acoustic Prepared using sagej.clssound field (Chen et al. 2024b), reconstruct the acoustic properties of environments (Wang et al. 2024), and develop more acoustically realistic simulation environments (Chen et al. 2022).Dynamic Scenes.Another challenge faced by our multimodal spatial language maps (both VLMaps and AVLMaps) is their inability to handle dynamic scenes.One form of dynamics involves in-view dynamics, such as walking humans and moving articulated objects during exploration.These dynamic entities easily corrupt the object map, leaving behind point artifacts that trace their movement trajectories.</p>
<p>The semantic features of the moving objects may be erroneously associated with these artifact points, leading to inaccurate representations of their true locations.Another form of dynamics involves long-term dynamics, such as object relocations between the exploration and inference phases.These relocations can invalidate the pre-built map, necessitating an updating mechanism to ensure accurate navigation.Currently, our multimodal spatial language maps lack mechanisms to address both types of dynamics.To explore potential solutions, we investigated approaches to mitigating the impact of dynamics.Prior works have addressed inview dynamics by removing or tracking certain classes of semantic masks during mapping (Xu et al. 2019;Runz et al. 2018).For long-term dynamics, recent research has proposed learning-based object association methods to update the locations of relocated objects in the map (Yan et al. 2025;Huang et al. 2025).These solutions could be integrated into our mapping pipeline to enhance its robustness.</p>
<p>Extension to More Modalities.In this paper, we have demonstrated the feasibility of integrating information from multiple modalities into a unified map representation.More broadly, our method provides a framework that can be extended to additional modalities, such as odor, temperature, magnetic fields, infrared imagery, and point clouds.</p>
<p>In our implementation, we selected audio, language, and vision because they closely mirror human perceptual capabilities.Viewed from a broader perspective, AVLMaps can be regarded as a case study in building a multimodal spatial memory system, with inherent flexibility to incorporate more modalities.Extending the system to a new modality X involves three steps: (i) implementing the "X localization module" which includes mapping (embedding and storing the data into a representation) and retrieval (generating a heatmap based on the query data and the map) functions for the new modality, (ii) implementing the get_major_map(X_input=None) and get_map(X_input=None) to accept a new modality's data as input and return 3D heatmaps with high and low decay rates as in Sec.3.7, and (iii) implementing a context prompt as in Fig. 12 for the new modality, including an instruction involving the new modality, and the expected generated code.Although AVLMaps only focuses on audio, language, and vision, its simplicity, flexibility, and scalability open up promising directions for future research.</p>
<p>Conclusion</p>
<p>In this paper, we introduced multimodal spatial language maps, a unified mapping framework that is spatial, multimodal, reusable, and extensible.We first introduced a visual language map representation, VLMaps, that enables robots to navigate to long-horizon spatial goals in a zeroshot manner.By defining the categories where the robot can and can not traverse, our map representation can adaptively generate obstacle maps for different embodiments, allowing for efficient path planning.Subsequently, we further extend our visual-language maps to a multimodal version, AVLMaps, which is a unified 3D spatial map representation for storing cross-modal information from audio, visual, and language cues.AVLMaps retain the spatial and reusable properties of VLMaps while enabling robots to reason over multimodal cues to disambiguate goals using large language models.Experiments in both simulated and realworld environments with different robotic embodiments demonstrate that our multimodal spatial language maps enable zero-shot spatial and multimodal goal navigation, significantly outperforming baselines in navigation success rate and landmark indexing accuracy, especially in scenarios with ambiguous goals.Moreover, extensive experimentation reveals that the performance of multimodal navigation and manipulation tasks scales with the capabilities of the underlying foundation models.At the end of the paper, we also present an in-depth discussion of the limitations and potential directions for future work, to inspire further research in multimodal spatial reasoning for robotics.</p>
<p>Notes</p>
<ol>
<li>https://github.com/karolpiczak/ESC-50</li>
</ol>
<p>Figure 1 .
1
Figure 1.AVLMaps provide an open-vocabulary 3D map representation for storing cross-modal information from audio, visual, and language cues.When combined with large language models, AVLMaps consumes multimodal prompts from audio, vision, and language to solve zero-shot spatial goal navigation by effectively leveraging complementary information sources to disambiguate goals.</p>
<p>Figure 2 .
2
Figure 2. The creation and language-conditioned indexing of a VLMap.A VLMap is created by fusing pretrained visual-language features into the reconstruction of the environment to enable visual-spatial-language-based reasoning.By providing a list of open-vocabulary labels, we retrieve segmentation masks for semantic classes required by downstream applications.</p>
<p>Figure 3 .
3
Figure 3.The overview of building customized obstacle maps for different robot embodiments.By specifying different obstacle categories in natural language for different embodiments, different obstacle maps can be built to ensure the most efficient path planning for different embodiments.</p>
<h1></h1>
<p>move a bit to the right of the fridge robot.move_to_right('refrigerator')# move in between the couch and bookshelf robot.move_in_between('couch','bookshelf') # face the toilet robot.face('toilet')# move to the west of the chair robot.move_west('chair')# turn right 20 degrees robot.turn(20)# find any chairs in the environment robot.move_to_object('chair')# with the television on your left robot.with_object_on_left('television')# move forward for 3 meters robot.move_forward(3)# move right 2 meters robot.turn(90)robot.move_forward(2)# move back and forth to the chair and</p>
<p>Figure 5 .
5
Figure5.VLMaps enable a robot to perform complex zero-shot spatial goal navigation tasks given natural language commands, without additional data collection or model finetuning.</p>
<p>Figure 7 .
7
Figure 7. System overview.AVLMaps are constructed from RGB-D, audio, and odometry inputs, converting raw data into visual localization features, visual-language features, and audio-language features.During inference time, each module's output is unified with cross-modal reasoning, allowing users to query spatial location with multimodal information.</p>
<p>Figure 8 .
8
Figure 8.The overview of the visual localization module.We follow the hierarchical localization scheme by using NetVLAD and SuperGLUE to localize the query image's location before generating a heatmap as the indexing result.</p>
<p>Figure 9 .
9
Figure 9.The overview of the object localization module.Similar to Sec. 3.1, during mapping, the RGB images in the exploration video are input to a vision-language model, LSeg (Li et al. 2021), to generate pixel-level features.Corresponding pixels are back-projected with depth images and transformed to locations in the global coordinate frame, where the features are associated with.During inference, the query text is encoded by LSeg's text encoder, and a dot product with the point-level embeddings generates a score for each point.A heatmap is then created based on point scores and distances.</p>
<p>Figure 10 .
10
Figure 10.The overview of the audio localization module.During mapping, the exploration video's audio is segmented by silence and encoded using AudioCLIP's audio encoder.The resulting embeddings are linked to poses based on their timestamps.During inference, the query text is encoded by AudioCLIP's text encoder, and a dot product with the pose embeddings generates a score for each pose.A heatmap is then created based on pose scores and distances.</p>
<p>i=1,...,K {s si − ϵ • dist xy (p, p si )} (9) H s (p) = max( Hs (p) − ϵ • dist xy (p, p si ), 0)(10)</p>
<p>Figure 11 .
11
Figure 11.The key idea of cross-modal reasoning is converting the prediction from different modalities into heatmaps, and then fusing them with element-wise multiplication, effectively using complementary multimodal information to resolve ambiguous prompts.</p>
<p>Figure 13 .
13
Figure13.The query and the generated results from the LLM.During the query, the context prompt in Fig.12and the input task commands are prompted to the LLM together.The input task commands are in green and generated outputs are highlighted</p>
<p>Figure 14 .
14
Figure 14.VLMaps enable different embodiments to define their own obstacle maps for navigation.The left image shows the top-down view of an environment.The middle columns show the observations of agents during navigation.The images on the right demonstrate the obstacles maps generated for different embodiments and the corresponding navigation paths.</p>
<p>Figure 15 .
15
Figure 15.Real-world navigation experiments are conducted in a room with multiple ambiguous goals such as tables, chairs, backpacks, and paper boxes (left).The robot setup is also shown in the left image.We leverage dense SLAM techniques to build a 3D reconstruction of the scene from RGB-D camera data into which we anchor features from multiple foundation models (right).We artificially insert sounds with different semantics at locations shown in the image.Different sounds are played when the robot moves to these locations during mapping.Sounds are sampled from the ESC-50 dataset.</p>
<p>Figure 16 .
16
Figure16.Audio denoising for real-world experiment.We pre-process the audio by applying a noise gate.Simply applying a threshold for filtering out low-level noise might lead to frequent fluctuation of the audio level, leading to fragmentation of the target audio (see the left).A noise gate contains "attack", "hold", and "release" phases, which introduce smooth transitions and prevent cutting off short audio signals (see the right).</p>
<p>Figure 17 .
17
Figure 17.Visualization of example heatmaps in AVLMaps for multimodal goal reasoning for ambiguous object goals.The first column shows the positions of ambiguous objects (green bounding boxes) and the location of a sound (the icon of a speaker) or an image (the icon of a camera), while the second column shows the close-up view of ambiguous objects in the scene.The third column shows the predicted 3D heatmap for the object.The fourth column shows the heatmap for the extra modality, and the fifth column shows the fused heatmap after cross-modal reasoning.Sounds are artificially inserted into the scene for benchmarking and evaluation.The locations of sounds are not sound-source locations but the places where the sounds were heard.The heatmap is shown in the JET color scheme (red means the highest score and blue means the lowest score).</p>
<p>Figure 18 .
18
Figure18.Visualization of tabletop goal-reaching experiments.We set up two tabletop scenes and inserted sounds to different locations in the observation data (left).We show the indexing heatmap results of sound goals, visual goals, object goals, and ambiguous goal reaching results on the right.In addition, for visual goals, object goals, and ambiguous goals, we show the ground truth target locations (green cubes or green boxes) as well as the reached positions (red cubes).The heatmap is shown in the JET color scheme (red means the highest score and blue means the lowest score).</p>
<p>table #</p>
<h1>3 timespos1 = robot.get_pos('chair')pos2 = robot.get_pos('table')for i in range(3):robot.move_to(pos1)robot.move_to(pos2)# move 3 meters south of the chairrobot.move_south('chair')robot.face('chair')robot.turn(180)robot.move_forward(3)# turn westrobot.turn_absolute(-90)# turn eastrobot.turn_absolute(90)# turn southrobot.turn_absolute(180)# turn northrobot.turn_absolute(0)# turn east and then turn left 90 degreesrobot.turn_absolute(90)robot.turn(-90)# navigate to 3 meters right of the tablerobot.move_to_right('table')robot.face('table')</h1>
<p>robot.turn(180) robot.move_forward(3) Figure 4.The full context prompt (prompt in gray) VLMap used for achieving spatial goal navigation tasks in the experiments.Prepared using sagej.cls</p>
<p>Table 1 :
1
Navigation API library for spatial goal navigation in VLMaps.</p>
<p>Table 2
2APIs
, we listed all the APIs we provide to the LLM for potential usage.Compared to VLMaps' API library in Table1, we simplify and adapt the library to more modalities for AVLMaps.We provide a list of examples consisting of language instructions and Prepared using sagej.cls</p>
<p>Table 2 :
2
Navigation API library for multimodal goal navigation in AVLMaps.generatedcode, which demonstrates the usage of those APIs as a context prompt (as in Fig.12) to the LLM when we ask it to generate code during the inference time.</p>
<h1>move to the window next to the sound of# glass breakingobj_map = robot.get_major_map(obj="window")sound_map = robot.get_map(sound="glass breaking")fuse_map = obj_map * sound_mappos = robot.get_max_pos_3d(fuse_map)robot.move_to(pos)# move to the sound of crying baby next to the# counterobj_map = robot.get_map(obj="counter")sound_map = robot.get_major_map(sound="crying baby")fuse_map = obj_map * sound_mappos = robot.get_max_pos_3d(fuse_map)robot.move_to(pos)# move to the table next to the sound of# crying baby and the sound of dogobj_map = robot.get_major_map(obj="table")sound_map = robot.get_map(sound="crying baby") /* robot.get_map(sound="dog")fuse_map = obj_map * sound_mappos = robot.get_max_pos_3d(fuse_map)robot.move_to(pos)# move to the middle of the table and# the chair next to the sound of crying babyobj_map_1 = robot.get_major_map(obj="table")obj_map_2 = robot.get_major_map(obj="chair")sound_map = robot.get_map(sound="crying baby")fuse_map = obj_map_2 * sound_mappos1 = robot.get_max_pos_3d(obj_map_1)pos2 = robot.get_max_pos_3d(fuse_map)pos = (pos1 + pos2) / 2robot.move_to(pos)</h1>
<p>Figure 12.The full context prompt (prompt in gray) AVLMaps used for achieving all navigation tasks in the experiments.</p>
<p>Table 3 :
3
The success rate (%) of zero-shot spatial goal navigation with language.</p>
<p>Table 4 :
4
The success rate (%) and SPL of multi-embodiment object goal navigation with language.
No. Subgoals in a RowIndep.Tasks1234SubgoalsSRSPLSRSPLSRSPLSRSPLSRLoCoBot (ground map)5349.02817.8146.762.552.3Drone (ground map)5341.82815.5145.362.053.3Drone (drone map)5645.43016.3177.072.555.0</p>
<p>Table 5 :
5
The success rate (%) of sound goal navigation with AVLMaps.</p>
<p>Table 6 :
6
The success rate (%) of multimodal goal navigation with AVLMaps.The agent is required to navigate to one visual goal, and two object goals in sequence.</p>
<p>Table 7 :
7
The recall (%) of area-object cross-modal indexing.</p>
<p>Table 8 :
8
The recall (%) of object-sound cross-modal indexing.
baseline (wav2clip)8.40 10.08 10.92 14.298.52baseline (AudioCLIP) 26.05 35.29 36.97 42.015.04VLMaps + wav2clip24.37 30.25 33.61 38.666.27VLMaps + AudioCLIP (Ours)53.78 65.55 67.23 70.592.74</p>
<p>Table 9 :
9
The recall (%) of visual-object cross-modal indexing.</p>
<p>Table 10 :
10
The success rate (%) of multimodal ambiguous goal navigation with AVLMaps.The agent is required to navigate to one ambiguous sound goal and one ambiguous object goal sequentially.
TasksNo. Subgoals in a Row Sound Object12Goals GoalsVLMaps---27.1AudioCLIP--16.9-VLMaps + wav2clip22.012.722.053.4VLMaps + AudioCLIP (Ours) 46.228.646.255.5</p>
<p>Table 11 :
11
The success rate (%) of multimodal goal navigation with different foundation models.The agent is required to navigate to one sound goal, one visual goal, and two object goals in sequence.The right-most three columns indicate the independent success rate of navigating to specific types of goals.</p>
<p>Prepared using sagej.cls</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, K Gopalakrishnan, K Hausman, A Herzog, D Ho, J Hsu, J Ibarz, B Ichter, A Irpan, E Jang, Rmj Ruano, K Jeffrey, S Jesmonth, N J Joshi, R C Julian, D Kalashnikov, Y Kuang, K H Lee, S Levine, Y Lu, L Luu, C Parada, P Pastor, J Quiambao, K Rao, J Rettinghouse, D M Reyes, P Sermanet, N Sievers, C Tan, A Toshev, V Vanhoucke, F Xia, T Xiao, P Xu, S Xu, Yan M , Proc. of the Conference on Robot Learning (CoRL). of the Conference on Robot Learning (CoRL)2022</p>
<p>. P Anderson, A Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, A R Zamir, 2018a</p>
<p>Sim-to-real transfer for vision-andlanguage navigation. P Anderson, A Shrivastava, J Truong, A Majumdar, D Parikh, D Batra, S Lee, Proc. of the Conference on Robot Learning (CoRL). of the Conference on Robot Learning (CoRL)2021</p>
<p>Vision-andlanguage navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N Sünderhauf, I Reid, S Gould, Van Den, A Hengel, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2018b</p>
<p>Netvlad: Cnn architecture for weakly supervised place recognition. R Arandjelovic, P Gronat, A Torii, T Pajdla, J Sivic, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2016</p>
<p>RT-1: Robotics Transformer for Real-World Control at Scale. A Brohan, RSSN Brown, RSSJ Carbajal, RSSY Chebotar, RSSJ Dabis, RSSC Finn, RSSK Gopalakrishnan, RSSK Hausman, RSSA Herzog, RSSJ Hsu, RSSJ Ibarz, RSSB Cls Ichter, RSSA Irpan, RSST Jackson, RSSS Jesmonth, RSSN Joshi, RSSR Julian, RSSD Kalashnikov, RSSY Kuang, RSSI Leal, RSSK H Lee, RSSS Levine, RSSY Lu, RSSU Malla, RSSD Manjunath, RSSI Mordatch, RSSO Nachum, RSSC Parada, RSSJ Peralta, RSSE Perez, RSSK Pertsch, RSSJ Quiambao, RSSK Rao, RSSM S Ryoo, RSSG Salazar, RSSP R Sanketi, RSSK Sayed, RSSJ Singh, RSSS Sontakke, RSSA Stone, RSSC Tan, RSSH Tran, RSSV Vanhoucke, RSSS Vega, RSSQ H Vuong, RSSF Xia, RSST Xiao, RSSP Xu, RSSS Xu, RSSYu T Zitkovich, RSSB , RSSProc. of Robotics: Science and Systems. of Robotics: Science and Systems2023Prepared using sagej</p>
<p>Language models are fewshot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, Ramesh A Ziegler, D Wu, J Winter, C Hesse, C Chen, M Sigler, E Litwin, M Gray, S Chess, B Clark, J Berner, C Mccandlish, S Radford, A Sutskever, I Amodei, D , Proc. of the Advances in Neural Information Processing Systems (NeurIPS). of the Advances in Neural Information essing Systems (NeurIPS)2020</p>
<p>Matterport3D: Learning from RGB-D data in indoor environments. A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, International Conference on 3D Vision (3DV). 2017</p>
<p>Object goal navigation using goal-oriented semantic exploration. D S Chaplot, D P Gandhi, A Gupta, R R Salakhutdinov, Proc. of the Advances in Neural Information Processing Systems (NeurIPS). of the Advances in Neural Information essing Systems (NeurIPS)2020</p>
<p>Open-vocabulary queryable scene representations for real world planning. B Chen, F Xia, B Ichter, K Rao, K Gopalakrishnan, M S Ryoo, A Kappler, D , Proc. of the IEEE International Conference on Robotics &amp; Automation (ICRA). of the IEEE International Conference on Robotics &amp; Automation (ICRA)2023a</p>
<p>Open-vocabulary queryable scene representations for real world planning. B Chen, F Xia, B Ichter, K Rao, K Gopalakrishnan, M S Ryoo, A Stone, D Kappler, Proc. of the IEEE International Conference on Robotics &amp; Automation (ICRA). of the IEEE International Conference on Robotics &amp; Automation (ICRA)2023b</p>
<p>Soundspaces: Audiovisual navigation in 3d environments. C Chen, U Jain, C Schissler, Sva Gari, Z Al-Halah, V K Ithapu, Robinson P Grauman, K , Proc. of European Conference on Computer Vision (ECCV). of European Conference on Computer Vision (ECCV)2020</p>
<p>Learning to set waypoints for audio-visual navigation. C Chen, S Majumder, Z Al-Halah, R Gao, Ramakrishnan Sk, K Grauman, Proc. of International Conference on Learning Representations (ICLR). of International Conference on Learning Representations (ICLR)2021a</p>
<p>Soundspaces 2.0: A simulation platform for visual-acoustic learning. C Chen, C Schissler, S Garg, P Kobernik, A Clegg, P Calamia, D Batra, Robinson P Grauman, K , Proc. of the Advances in Neural Information Processing Systems (NeurIPS). of the Advances in Neural Information essing Systems (NeurIPS)2022</p>
<p>. M Chen, J Tworek, H Jun, Q Yuan, Oliveira De, H P Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, N Ryder, M Pavlov, A Power, L Kaiser, M Bavarian, C Winter, P Tillet, F P Such, D Cummings, M Plappert, F Chantzis, E Barnes, A Herbert-Voss, W H Guss, Nichol A Paino, A Tezak, N Tang, J Babuschkin, I Balaji, S Jain, S Saunders, W Hesse, C Carr, A N Leike, J Achiam, J Misra, V Morikawa, E Radford, A Knight, M Brundage, M Murati, M Mayer, K Welinder, P Mcgrew, B Amodei, D Mccandlish, S Sutskever, I Zaremba, W , 2021bEvaluating large language models trained on code</p>
<p>Vision-language models provide promptable representations for reinforcement learning. W Chen, O Mees, A Kumar, S Levine, 2024a</p>
<p>Real acoustic fields: An audio-visual room acoustics dataset and benchmark. Z Chen, I D Gebru, C Richardt, A Kumar, W Laney, A Owens, Richard A , Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2024b</p>
<p>Contextual cueing: Implicit learning and memory of visual context guides spatial attention. M M Chun, Y Jiang, Cognitive psychology. 1998</p>
<p>Superpoint: Self-supervised interest point detection and description. D Detone, T Malisiewicz, A Rabinovich, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2018</p>
<p>Scaling cross-embodied learning: One policy for manipulation, navigation, locomotion and aviation. R Doshi, H Walke, O Mees, S Dasari, S Levine, Proc. of the Conference on Robot Learning (CoRL). of the Conference on Robot Learning (CoRL)2024</p>
<p>Clap learning audio concepts from natural language supervision. B Elizalde, S Deshmukh, Al Ismail, M , Wang H , Proc. of the IEEE International Conference on Acoustics, Speech and Signal Processing. of the IEEE International Conference on Acoustics, Speech and Signal essing2023ICASSP</p>
<p>An evaluation of the rgb-d slam system. F Endres, J Hess, N Engelhard, J Sturm, D Cremers, W Burgard, Proc. of the IEEE International Conference on Robotics &amp; Automation (ICRA). of the IEEE International Conference on Robotics &amp; Automation (ICRA)2012</p>
<p>OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views. F Engelmann, F Manhardt, M Niemeyer, K Tateno, M Pollefeys, F Tombari, Proc. of International Conference on Learning Representations (ICLR). of International Conference on Learning Representations (ICLR)2024</p>
<p>Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. M A Fischler, R C Bolles, Communications of the ACM. 1981</p>
<p>Speaker-follower models for vision-and-language navigation. D Fried, R Hu, V Cirik, A Rohrbach, J Andreas, L P Morency, T Berg-Kirkpatrick, K Saenko, D Klein, Darrell T , 2018</p>
<p>Cows on pasture: Baselines and benchmarks for languagedriven zero-shot object navigation. S Y Gadre, M Wortsman, G Ilharco, L Schmidt, S Song, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Finding fallen objects via asynchronous audio-visual integration. C Gan, Y Gu, S Zhou, J Schwartz, S Alter, J Traer, D Gutfreund, J B Tenenbaum, J H Mcdermott, A Torralba, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>C Gan, J Schwartz, S Alter, M Schrimpf, J Traer, J D Freitas, J Kubilius, A Bhandwaldar, N Haber, M Sano, K Kim, E Wang, D Mrowca, M Lingelbach, A Curtis, K T Feigelis, D Bear, D Gutfreund, D Cox, J J Dicarlo, J H Mcdermott, J B Tenenbaum, Dlk Yamins, Threedworld: A platform for interactive multi-modal physical simulation. 2020a</p>
<p>Look, listen, and act: Towards audio-visual embodied navigation. C Gan, Y Zhang, J Wu, B Gong, J B Tenenbaum, Proc. of the IEEE International Conference on Robotics &amp; Automation (ICRA). of the IEEE International Conference on Robotics &amp; Automation (ICRA)2020b</p>
<p>Implicit kinematic policies: Unifying joint and cartesian action spaces in end-to-end robot learning. A Ganapathi, P Florence, J Varley, K Burns, K Goldberg, A Zeng, Proc. of the IEEE International Conference on Robotics &amp; Automation (ICRA). of the IEEE International Conference on Robotics &amp; Automation (ICRA)2022</p>
<p>Scaling open-vocabulary image segmentation with image-level labels. G Ghiasi, X Gu, Y Cui, T Y Lin, Proc. of European Conference on Computer Vision (ECCV). of European Conference on Computer Vision (ECCV)2022</p>
<p>Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. Q Gu, A Kuwajerwala, S Morin, K M Jatavallabhula, B Sen, A Agarwal, C Rivera, W Paul, K Ellis, R Chellappa, C Gan, C M De Melo, J B Tenenbaum, A Torralba, F Shkurti, L Paull, Proc. of the IEEE International Conference on Robotics &amp; Automation (ICRA). of the IEEE International Conference on Robotics &amp; Automation (ICRA)2024</p>
<p>Open-vocabulary object detection via vision and language knowledge distillation. X Gu, T Y Lin, W Kuo, Y Cui, Proc. of International Conference on Learning Representations (ICLR). of International Conference on Learning Representations (ICLR)2021</p>
<p>Airbert: In-domain pretraining for vision-and-language navigation. P L Guhur, M Tapaswi, S Chen, I Laptev, C Schmid, Proc. of the IEEE/CVF International Conference on Computer Vision (ICCV). of the IEEE/CVF International Conference on Computer Vision (ICCV)2021</p>
<p>Audioclip: Extending clip to image, text and audio. A Guzhov, F Raue, J Hees, A Dengel, Proc. of the IEEE International Conference on Acoustics, Speech and Signal Processing. of the IEEE International Conference on Acoustics, Speech and Signal essing2022ICASSP</p>
<p>Lelan: Learning a language-conditioned navigation policy from in-the-wild video. N Hirose, C Glossop, A Sridhar, D Shah, O Mees, S Levine, Proc. of the Conference on Robot Learning (CoRL). of the Conference on Robot Learning (CoRL)2024</p>
<p>Bridging the gap between learning in discrete and continuous environments for vision-and-language navigation. Y Hong, Z Wang, Q Wu, S Gould, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Audio visual language maps for robot navigation. C Huang, O Mees, A Zeng, W Burgard, Proc. of the International Symposium of Experimental Robotics (ISER). of the International Symposium of Experimental Robotics (ISER)2023a</p>
<p>Visual language maps for robot navigation. C Huang, O Mees, A Zeng, W Burgard, Proc. of the IEEE International Conference on Robotics &amp; Automation (ICRA). of the IEEE International Conference on Robotics &amp; Automation (ICRA)2023b</p>
<p>Bye: Build your encoder with one sequence of exploration data for long-term dynamic scene understanding. C Huang, Yan S Burgard, W , 2025IEEE Robotics and Automation Letters</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, Proc. of the International Conference on Machine Learning (ICML). of the International Conference on Machine Learning (ICML)2022a</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, P Sermanet, T Jackson, N Brown, L Luu, S Levine, K Hausman, Brian Ichter, Proc. of the Conference on Robot Learning (CoRL). of the Conference on Robot Learning (CoRL)2022b</p>
<p>ConceptFusion: Open-set multimodal 3D mapping. K M Jatavallabhula, RSSA Kuwajerwala, RSSQ Gu, RSSM Omama, RSSG Iyer, RSSS Saryazdi, RSST Chen, RSSA Maalouf, RSSS Li, RSSN V Keetha, RSSA Tewari, RSSJ Tenenbaum, RSSC De Melo, RSSKrishna M Paull, RSSL Shkurti, RSSF Torralba, RSSA , RSSProc. of Robotics: Science and Systems. of Robotics: Science and Systems2023</p>
<p>Mdetr-modulated detection for end-to-end multimodal understanding. A Kamath, M Singh, Y Lecun, G Synnaeve, I Misra, N Carion, Proc. of the IEEE/CVF International Conference on Computer Vision (ICCV). of the IEEE/CVF International Conference on Computer Vision (ICCV)2021</p>
<p>Lerf: Language embedded radiance fields. J Kerr, C M Kim, K Goldberg, A Kanazawa, M Tancik, Proc. of the IEEE/CVF International Conference on Computer Vision (ICCV). of the IEEE/CVF International Conference on Computer Vision (ICCV)2023</p>
<p>Garfield: Group anything with radiance fields. C M Kim, M Wu, J Kerr, K Goldberg, M Tancik, A Kanazawa, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Openvla: An open-source vision-language-action model. M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E P Foster, P R Sanketi, Q Vuong, T Kollar, B Burchfiel, R Tedrake, D Sadigh, S Levine, P Liang, C Finn, Proc. of the Conference on Robot Learning (CoRL). of the Conference on Robot Learning (CoRL)2025</p>
<p>Segment anything. A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W Y Lo, P Dollar, R Girshick, Proc. of the IEEE/CVF International Conference on Computer Vision (ICCV). of the IEEE/CVF International Conference on Computer Vision (ICCV)2023</p>
<p>Ai2-thor: An interactive 3d environment for visual ai. E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, M Deitke, K Ehsani, D Gordon, Y Zhu, A Kembhavi, A Gupta, A Farhadi, 2017</p>
<p>Causal inference in multisensory perception. K P Körding, U Beierholm, W J Ma, S Quartz, J B Tenenbaum, L Shams, PLoS one. 2007</p>
<p>Waypoint models for instruction-guided navigation in continuous environments. J Krantz, A Gokaslan, D Batra, Lee S Maksymets, O , Proc. of the IEEE/CVF International Conference on Computer Vision (ICCV). of the IEEE/CVF International Conference on Computer Vision (ICCV)2021</p>
<p>Beyond the nav-graph: Vision-and-language navigation in continuous environments. J Krantz, E Wijmans, A Majumdar, D Batra, S Lee, Proc. of European Conference on Computer Vision (ECCV). of European Conference on Computer Vision (ECCV)2020</p>
<p>Language-driven semantic segmentation. B Li, K Q Weinberger, S Belongie, V Koltun, R Ranftl, Proc. of International Conference on Learning Representations (ICLR). of International Conference on Learning Representations (ICLR)2021</p>
<p>Language-driven semantic segmentation. B Li, K Q Weinberger, S Belongie, V Koltun, R Ranftl, Proc. of International Conference on Learning Representations (ICLR). of International Conference on Learning Representations (ICLR)2022</p>
<p>Open-vocabulary semantic segmentation with mask-adapted clip. F Liang, B Wu, X Dai, K Li, Y Zhao, H Zhang, P Zhang, P Vajda, D Marculescu, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2023a</p>
<p>Code as policies: Language model programs for embodied control. J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, Florence P Zeng, A , Proc. of the IEEE International Conference on Robotics &amp; Automation (ICRA). of the IEEE International Conference on Robotics &amp; Automation (ICRA)2023b</p>
<p>Walk the talk: Connecting language, knowledge, and action in route instructions. M Macmahon, B Stankiewicz, B Kuipers, Def. 2006</p>
<p>Fusion++: Volumetric object-level slam. J Mccormac, R Clark, M Bloesch, A Davison, S Leutenegger, International Conference on 3D Vision (3DV). 2018</p>
<p>Semanticfusion: Dense 3d semantic mapping with convolutional neural networks. J Mccormac, A Handa, A Davison, S Leutenegger, Proc. of the IEEE International Conference on Robotics &amp; Automation (ICRA). of the IEEE International Conference on Robotics &amp; Automation (ICRA)2017</p>
<p>Subjective hierarchies in spatial memory. T P Mcnamara, J K Hardy, S C Hirtle, Journal of Experimental Psychology. 1989Learning, Memory, and Cognition</p>
<p>Grounding language with visual affordances over unstructured data. O Mees, J Borja-Diaz, W Burgard, Proc. of the IEEE International Conference on Robotics &amp; Automation (ICRA). of the IEEE International Conference on Robotics &amp; Automation (ICRA)2023</p>
<p>What matters in language conditioned robotic imitation learning over unstructured data. O Mees, Hermann L Burgard, W , 2022aIEEE Robotics and Automation Letters</p>
<p>Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. O Mees, L Hermann, Rosete-Beas E Burgard, W , 2022bIEEE Robotics and Automation Letters</p>
<p>Learning your way around town: How virtual taxicab drivers learn to use both layout and landmark information. E L Newman, J B Caplan, M P Kirschen, I O Korolev, R Sekuler, M J Kahana, Cognition. 2007</p>
<p>Octo: An open-source generalist robot policy. D Ghosh, RSSH Walke, RSSK Pertsch, RSSK Black, RSSO Mees, RSSS Dasari, RSSJ Hejna, RSSC Xu, RSSJ Luo, RSST Kreiman, RSSY Tan, RSSL Y Chen, RSSP Sanketi, RSSQ Vuong, RSST Xiao, RSSD Sadigh, RSSC Finn, RSSS Levine, RSSProc. of Robotics: Science and Systems. of Robotics: Science and Systems2024Octo Model Team</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models : Open x-embodiment collaboration0. A O'neill, A Rehman, A Maddukuri, A Gupta, A Padalkar, A Lee, A Pooley, A Gupta, A Mandlekar, A Jain, A Tung, A Bewley, A Herzog, A Irpan, A Khazatsky, A Rai, A Gupta, A Wang, A Singh, A Garg, A Kembhavi, A Xie, A Brohan, A Raffin, A Sharma, A Yavary, A Jain, A Balakrishna, A Wahid, B Burgess-Limerick, B Kim, B Schölkopf, B Wulfe, B Ichter, C Lu, C Xu, C Le, C Finn, C Wang, C Xu, C Chi, C Huang, C Chan, C Agia, C Pan, C Fu, C Devin, D Xu, D Morton, D Driess, D Chen, D Pathak, D Shah, D Büchler, D Jayaraman, D Kalashnikov, D Sadigh, E Johns, E Foster, F Liu, F Ceola, F Xia, F Zhao, F Stulp, G Zhou, G S Sukhatme, G Salhotra, G Yan, G Feng, G Schiavi, G Berseth, G Kahn, G Wang, H Su, H S Fang, H Shi, H Bao, Ben Amor, H Christensen, H I Furuta, H Walke, H Fang, H Ha, H Mordatch, I Radosavovic, I Leal, I Liang, J Abou-Chakra, J Kim, J Drake, J Peters, J Schneider, J Hsu, J Bohg, J Bingham, J Wu, J Gao, J Hu, J Wu, J Wu, J Sun, J Luo, J Gu, J Tan, J Oh, J Wu, J Lu, J Yang, J Malik, J Silvério, J Hejna, J Booher, J Tompson, J Yang, J Salvador, J Lim, J J Han, J Wang, K Rao, K Pertsch, K Hausman, K Go, K Gopalakrishnan, K Goldberg, K Byrne, K Oslund, K Kawaharazuka, K Black, K Lin, K Zhang, K Ehsani, K Lekkala, K Ellis, K Rana, K Srinivasan, K Fang, K Singh, K P Zeng, K H Hatch, K Hsu, K Itti, L Chen, L Y Pinto, L Fei-Fei, L Tan, L Fan, L J Ott, L Lee, L Weihs, L Chen, M Lepert, M Memmel, M Tomizuka, M Itkina, M Castro, M G Spero, M Du, M Ahn, M Yip, M C Zhang, M Ding, M Heo, M Srirama, M K Sharma, M Kim, M J Kanazawa, N Hansen, N Heess, N Joshi, N J Suenderhauf, N Liu, N , Di Palo, N Shafiullah, Nmm Mees, O Kroemer, O Bastani, O Sanketi, P R Miller, P T Yin, P Wohlhart, P Xu, P Fagan, P D Mitrano, P Sermanet, P Abbeel, P Sundaresan, P Chen, Q Vuong, Q Rafailov, R Tian, R Doshi, R Martín-Martín, R Baijal, R Scalise, R Hendrix, R Lin, R Qian, R Zhang, R Mendonca, R Shah, R Hoque, R Julian, R Bustamante, S Kirmani, S Levine, S Lin, S Moore, S Bahl, S Dass, S Sonawani, S Song, S Xu, S Haldar, S Karamcheti, S Adebola, S Guist, S Nasiriany, S Schaal, S Welker, S Tian, S Ramamoorthy, S Dasari, S Belkhale, S Park, S Nair, S Mirchandani, S Osa, T Gupta, T Harada, T Matsushima, T Xiao, T Kollar, T Yu, T Ding, T Davchev, T Zhao, T Z Armstrong, T Darrell, T Chung, T Jain, V Vanhoucke, V Zhan, W Zhou, W Burgard, W Chen, X Wang, X Zhu, X Geng, X Liu, X Liangwei, X Li, X Lu, Y Ma, Yj ; Ma, Z Xu, Z Cui, Z J Zhang, Z , Lin Z , Proc. of the IEEE International Conference on Robotics &amp; Automation (ICRA). of the IEEE International Conference on Robotics &amp; Automation (ICRA)Kim Y, Chebotar Y, Zhou Y, Zhu Y, Wu Y, Xu Y, Wang Y, Bisk Y, Cho Y, Lee Y, Cui Y, Cao Y, Wu YH, Tang Y, Zhu Y, Zhang Y, Jiang Y, Li Y, Li Y, Iwasawa Y, Matsuo Y,2024</p>
<p>Avlen: Audiovisual-language embodied navigation in 3d environments. S Paul, Roy-Chowdhury A Cherian, A , Proc. of the Advances in Neural Information Processing Systems (NeurIPS). of the Advances in Neural Information essing Systems (NeurIPS)2022</p>
<p>Openscene: 3d scene understanding with open vocabularies. S Peng, K Genova, C M Jiang, A Tagliasacchi, M Pollefeys, T Funkhouser, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Esc: Dataset for environmental sound classification. K J Piczak, Proceedings of the 23rd ACM international conference on Multimedia. the 23rd ACM international conference on Multimedia2015</p>
<p>Langsplat: 3d language gaussian splatting. M Qin, W Li, J Zhou, Wang H Pfister, H , Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Ros: an open-source robot operating system. M Quigley, Proc. of the IEEE International Conference on Robotics &amp; Automation (ICRA). of the IEEE International Conference on Robotics &amp; Automation (ICRA)2009</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, Ramesh A Goh, G Agarwal, S Sastry, G Askell, A Mishkin, P Clark, J Krueger, G Sutskever, I , Proc. of the International Conference on Machine Learning (ICML). of the International Conference on Machine Learning (ICML)2021</p>
<p>Latent plans for task agnostic offline reinforcement learning. E Rosete-Beas, O Mees, G Kalweit, J Boedecker, W Burgard, Proc. of the Conference on Robot Learning (CoRL). of the Conference on Robot Learning (CoRL)2022</p>
<p>Maskfusion: Real-time recognition, tracking and reconstruction of multiple moving objects. M Runz, M Buffier, L Agapito, Proc. of IEEE International Symposium on Mixed and Augmented Reality (ISMAR). of IEEE International Symposium on Mixed and Augmented Reality (ISMAR)2018</p>
<p>Slam++: Simultaneous localisation and mapping at the level of objects. R F Salas-Moreno, R A Newcombe, H Strasdat, P H Kelly, A J Davison, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2013</p>
<p>From coarse to fine: Robust hierarchical localization at large scale. P E Sarlin, C Cadena, R Siegwart, M Dymczyk, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2019</p>
<p>Superglue: Learning feature matching with graph neural networks. P E Sarlin, D Detone, T Malisiewicz, A Rabinovich, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2020</p>
<p>Habitat: A Platform for Embodied AI Research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, D Parikh, D Batra, Proc. of the IEEE/CVF International Conference on Computer Vision (ICCV). of the IEEE/CVF International Conference on Computer Vision (ICCV)2019</p>
<p>Grad-cam: Visual explanations from deep networks via gradient-based localization. R R Selvaraju, M Cogswell, A Das, R Vedantam, D Parikh, D Batra, International Journal of Computer Vision. 2020</p>
<p>CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory. Nmm Shafiullah, RSSC Paxton, RSSL Pinto, RSSS Chintala, RSSA Szlam, RSSProc. of Robotics: Science and Systems. of Robotics: Science and Systems2023</p>
<p>Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. D Shah, B Osiński, S Levine, Proc. of the Conference on Robot Learning (CoRL). of the Conference on Robot Learning (CoRL)2023</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Proc. of the Conference on Robot Learning (CoRL). of the Conference on Robot Learning (CoRL)2022</p>
<p>Habitat 2.0: Training home assistants to rearrange their habitat. A Szot, A Clegg, E Undersander, E Wijmans, Y Zhao, J Turner, N Maestre, M Mukadam, D Chaplot, O Maksymets, A Gokaslan, V Vondrus, S Dharur, F Meier, W Galuba, Chang A Kira, Z Koltun, V Malik, J Savva, M Batra, D , Proc. of the Advances in Neural Information Processing Systems (NeurIPS). of the Advances in Neural Information essing Systems (NeurIPS)2021</p>
<p>Understanding natural language commands for robotic navigation and mobile manipulation. S Tellex, T Kollar, S Dickerson, M Walter, A Banerjee, S Teller, N Roy, Proc. of the National Conference on Artificial Intelligence (AAAI). of the National Conference on Artificial Intelligence (AAAI)2011</p>
<p>A probabilistic approach to concurrent mapping and localization for mobile robots. S Thrun, Burgard W Fox, D , Autonomous Robots. 1998</p>
<p>Hearing anything anywhere. M L Wang, R Sawata, S Clarke, R Gao, S Wu, J Wu, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation. A Werby, RSSC Huang, RSSM Büchner, RSSA Valada, RSSW Burgard, RSSProc. of Robotics: Science and Systems. of Robotics: Science and Systems2024</p>
<p>Wav2clip: Learning robust audio representations from clip. H H Wu, P Seetharaman, K Kumar, J P Bello, Proc. of the IEEE International Conference on Acoustics, Speech and Signal Processing. of the IEEE International Conference on Acoustics, Speech and Signal essing2022ICASSP</p>
<p>Spatial intention maps for multi-agent mobile manipulation. J Wu, X Sun, A Zeng, S Song, S Rusinkiewicz, T Funkhouser, Proc. of the IEEE International Conference on Robotics &amp; Automation (ICRA). of the IEEE International Conference on Robotics &amp; Automation (ICRA)2021</p>
<p>Mid-fusion: Octree-based object-level multi-instance dynamic slam. B Xu, W Li, D Tzoumanikas, M Bloesch, A Davison, S Leutenegger, Proc. of the IEEE International Conference on Robotics &amp; Automation (ICRA). of the IEEE International Conference on Robotics &amp; Automation (ICRA)2019</p>
<p>Dynamic open-vocabulary 3d scene graphs for longterm language-guided mobile manipulation. Z Yan, S Li, Z Wang, L Wu, H Wang, J Zhu, Chen L Liu, J , IEEE Robotics and Automation Letters. 2025</p>
<p>Catch me if you hear me: Audio-visual navigation in complex unmapped environments with moving sounds. A Younes, D Honerkamp, T Welschehold, A Valada, IEEE Robotics and Automation Letters. 2023</p>
<p>Xirl: Cross-embodiment inverse reinforcement learning. K Zakka, A Zeng, P Florence, J Tompson, J Bohg, D Dwibedi, Proc. of the Conference on Robot Learning (CoRL). of the Conference on Robot Learning (CoRL)2022</p>
<p>Robotic control via embodied chain-of-thought reasoning. M Zawalski, Chen W Pertsch, K Mees, O Finn, C Levine, S , Proc. of the Conference on Robot Learning (CoRL). of the Conference on Robot Learning (CoRL)2024</p>
<p>Learning visual affordances for robotic manipulation. A Zeng, 2019Princeton UniversityPhD Thesis</p>
<p>Socratic models: Composing zero-shot multimodal reasoning with language. A Zeng, M Attarian, B Ichter, K M Choromanski, A Wong, S Welker, F Tombari, A Purohit, M S Ryoo, V Sindhwani, J Lee, V Vanhoucke, P Florence, Proc. of International Conference on Learning Representations (ICLR). of International Conference on Learning Representations (ICLR)2023</p>
<p>Autonomous improvement of instruction following skills via foundation models. Z Zhou, P Atreya, A Lee, H Walke, O Mees, S Levine, Proc. of the Conference on Robot Learning (CoRL). of the Conference on Robot Learning (CoRL)2024</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. B Zitkovich, T Yu, S Xu, P Xu, T Xiao, F Xia, J Wu, P Wohlhart, S Welker, A Wahid, Q Vuong, V Vanhoucke, H Tran, R Soricut, A Singh, J Singh, P Sermanet, P R Sanketi, G Salazar, M S Ryoo, K Reymann, K Rao, K Pertsch, I Mordatch, H Michalewski, Y Lu, S Levine, L Lee, Twe Lee, I Leal, Y Kuang, D Kalashnikov, R Julian, N J Joshi, A Irpan, B Ichter, J Hsu, A Herzog, K Hausman, K Gopalakrishnan, C Fu, P Florence, C Finn, K A Dubey, D Driess, T Ding, K M Choromanski, X Chen, Y Chebotar, J Carbajal, N Brown, A Brohan, M G Arenas, K Han, Proc. of the Conference on Robot Learning (CoRL). of the Conference on Robot Learning (CoRL)2023</p>
<p>Fmgs: Foundation model embedded 3d gaussian splatting for holistic 3d scene understanding. X Zuo, P Samangouei, Y Zhou, Di Y Li, M , International Journal of Computer Vision. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
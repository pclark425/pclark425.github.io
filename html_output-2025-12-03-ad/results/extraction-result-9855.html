<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9855 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9855</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9855</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-7a6a20f705dfd5d1f8cfd24743b0977f34cdad1e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7a6a20f705dfd5d1f8cfd24743b0977f34cdad1e" target="_blank">Do Multi-Document Summarization Models Synthesize?</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A simple, general, effective method for improving model synthesis capabilities by generating an explicitly diverse set of candidate outputs, and then selecting from these the string best aligned with the expected aggregate measure for the inputs, or abstaining when the model produces no good candidate.</p>
                <p><strong>Paper Abstract:</strong> Abstract Multi-document summarization entails producing concise synopses of collections of inputs. For some applications, the synopsis should accurately synthesize inputs with respect to a key aspect, e.g., a synopsis of film reviews written about a particular movie should reflect the average critic consensus. As a more consequential example, narrative summaries that accompany biomedical systematic reviews of clinical trial results should accurately summarize the potentially conflicting results from individual trials. In this paper we ask: To what extent do modern multi-document summarization models implicitly perform this sort of synthesis? We run experiments over opinion and evidence synthesis datasets using a suite of summarization models, from fine-tuned transformers to GPT-4. We find that existing models partially perform synthesis, but imperfectly: Even the best performing models are over-sensitive to changes in input ordering and under-sensitive to changes in input compositions (e.g., ratio of positive to negative reviews). We propose a simple, general, effective method for improving model synthesis capabilities by generating an explicitly diverse set of candidate outputs, and then selecting from these the string best aligned with the expected aggregate measure for the inputs, or abstaining when the model produces no good candidate.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9855.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9855.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-tuned transformer LM (OpenAI) used in this work via prompting to produce opinionated meta-reviews and draft systematic-review conclusions from collections of reviews and randomized controlled trial (RCT) reports.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source, instruction-tuned large transformer language model from OpenAI. In this paper GPT-4 was used via system and assistant prompts (task-specific few-shot examples) to generate meta-reviews for movies and draft conclusions for systematic reviews; prompts were not heavily tuned (system prompt + one in-context example). For GPT-4 outputs the authors sampled five responses with temperature 0.6 for diverse decoding experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Two corpora were used with GPT-4 in experiments: (1) Rotten Tomatoes movie critic reviews with associated meta-reviews (9,095 movies; ~244k individual critic reviews; meta-review per movie); (2) A Cochrane-derived systematic reviews dataset comprising ~2,600 systematic reviews summarizing ~16,500 clinical trials (each review contains multiple trial reports and a natural-language summary and statistical meta-analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>16500</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Produce concise narrative syntheses: (a) an opinionated critics' meta-review reflecting aggregate sentiment across many critic reviews; (b) a draft/conclusions section of a systematic review summarizing whether the assembled trial evidence indicates a significant treatment effect.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Direct prompting: system prompt framing the model as an expert (professional movie critic or systematic reviewing expert), assistant prompt containing the concatenated inputs (reviews or trial narratives) and a one-shot example, and an instruction to produce a concise synthesis (opinionated summary or conclusions only). For diverse output generation the authors sampled 5 responses; also experimented with diverse beam search externally for other models but GPT-4 responses were sampled.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Narrative synthesis / meta-review text (opinionated summary for movies; conclusions-style summary for systematic reviews).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>"Please provide a draft systematic review for the studies below: {studies}. Start with the conclusions of the review only..." (Paper includes example meta-review sentences for 'You Don't Mess with the Zohan' with inferred sentiment scores such as: "You Don't Mess With the Zohan has its moments, but not all of them – and the jokes are embarrassingly crass and often crude.")</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated measurement of the latent aspect in generated summaries using external measurement models: (a) a BERT-based continuous sentiment regressor (fine-tuned on SST/IMDB) for movie meta-reviews, evaluating correlation (R^2, Pearson r) between generated-summary sentiment and Tomatometer; (b) RobotReviewer binary classifier to infer whether a generated systematic-review summary reports a significant effect, compared with the ground-truth meta-analysis p<0.05; metrics include macro F1, accuracy and ROUGE-1 for surface overlap. Additional behavioral tests: sensitivity analyses via 100 random permutations of input ordering and subsampling-based input-composition perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On movie meta-reviews GPT-4 achieved R^2=0.808 and Pearson r=0.900 (higher than most fine-tuned transformers and higher than human references on the measured regression correlation). On systematic reviews GPT-4 achieved macro F1=0.628 and accuracy=0.640 (slightly outperforming the human reference on that coarse RobotReviewer-based metric). GPT-4 outputs were less sensitive to some perturbations than many models but still exhibited ordering/composition issues in some cases; diverse selection methods yielded modest further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>High measured alignment with aggregate signals (strongest R^2/PCC among evaluated models on movie sentiment), fluent and direct formulation of conclusions for systematic reviews, benefits from instruction prompting and larger model capacity; performs well with minimal prompt tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No model-size or internals disclosed; outputs can still be sensitive to input ordering and input-composition changes; evaluation relies on external classifiers (BERT regressor, RobotReviewer) which are imperfect proxies; sampled outputs can vary and require selection/abstention strategies to ensure alignment with expected aggregate properties.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Instances where different permutations of the same input set led GPT-4 to flip conclusions or produce inconsistent synthesized sentiments; cases where modest changes in input composition (e.g., removing a portion of positive reviews) did not produce proportional changes in generated sentiment; occasional over-confident/incorrect statements relative to the meta-analytic ground truth as measured by RobotReviewer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Multi-Document Summarization Models Synthesize?', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9855.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9855.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer summarizers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned Transformer-based Multi-Document Summarization Models (PRIMERA, PEGASUS, LED, T5, Flan-T5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of transformer sequence-to-sequence models (PRIMERA, PEGASUS, Longformer-Encoder-Decoder (LED), T5 variants and Flan-T5 variants) were fine-tuned on movie meta-review and systematic-review datasets to synthesize aggregated summaries from many input documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PRIMERA: Pyramid-based masked sentence pre-training for multidocument summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PRIMERA, PEGASUS, LED (Longformer), T5 (Small/Base/etc), Flan-T5 (Small/Base/L/XL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer encoder-decoder architectures adapted for long-input multi-document summarization. Models were fine-tuned on each task using supervised MLE objectives (Adam optimizer; Pegasus with Adafactor for some runs), with hyperparameter searches over learning rates and training steps, batch size effective 16, mixed precision as available, and selection by ROUGE-1 on validation. PRIMERA is explicitly pre-trained for multi-document summarization; LED (Longformer) provides long-context attention.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same corpora as above: Rotten Tomatoes meta-reviews (9,095 movies; ~244k reviews) and Cochrane systematic reviews (~2,600 reviews summarizing ~16,500 RCTs). Instances vary in numbers of inputs (movie avg ~27 reviews per meta-review; systematic reviews avg ~6.6 inputs) and concatenated input lengths (movie ~800 tokens, systematic reviews up to ~2.6k tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>16500</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Produce concise summaries that synthesize a latent property across inputs: aggregate critic sentiment (movies) or whether the clinical evidence supports a significant treatment effect (systematic reviews).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised fine-tuning for multi-document summarization using concatenated linearized inputs with special separators; decoding via standard beam search (5 beams) and experiments with Diverse Beam Search (DBS) to generate multiple diverse candidates; a plugged-in reranker selects candidates whose inferred aggregate property (via an external measurement model) best matches the expected aggregate computed from inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Narrative meta-reviews or systematic-review style summaries</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Example captions and summary texts reported in the paper (models generate short meta-reviews similar to reference targets; paper includes diverse candidate examples and their inferred sentiment scores).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Measured calibration of the synthesized latent property via (a) regression metrics (R^2, Pearson r) comparing inferred sentiment from generated summaries to Tomatometer for movies; (b) RobotReviewer-inferred significant-effect classification compared to meta-analytic p<0.05 for systematic reviews (macro F1, accuracy); also ROUGE-1 as surface quality metric. Behavioral analyses: input-permutation sensitivity (100 random permutations), input-composition perturbations (subsampling positive/negative reviews), and multiple-candidate selection/abstention evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Transformer summarizers vary: PRIMERA and Flan-T5-XL among best non-GPT models (e.g., PRIMERA: R^2≈0.608, PCC≈0.780 on movies; Flan-T5-XL R^2≈0.611 PCC≈0.783). On systematic reviews, models attained macro F1 around 0.49–0.57 and accuracies in the 0.60–0.69 range; humans (reference summaries) measured via RobotReviewer yielded F1=0.577 Acc=0.686. Diverse-generation + selection improved alignment (higher R^2/PCC or F1) at the cost of small ROUGE-1 reductions and increased abstention rates for the systematic-review setting.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Pretrained multi-document models (PRIMERA, LED) and larger model sizes improved synthesis calibration; fine-tuning on domain data yields usable narrative summaries; diverse decoding plus an external selection criterion yields measurable improvements in calibration with respect to the target aggregate property.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Even the best fine-tuned transformers are sensitive to input ordering and under-responsive to moderate input-composition changes; gains in calibration sometimes traded off with surface-quality metrics (ROUGE-1) and increased variability; limited model sizes due to hardware constrained experiments; reliance on external measurement models for selection; inability to guarantee faithful capture of statistical meta-analytic conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Models sometimes flip whether a summary reports a significant effect under different input orderings; require large changes in input composition to flip summary sentiment/effect, i.e., under-sensitive; occasional hallucinated or unsupported statements relative to input evidence (factuality concerns).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Multi-Document Summarization Models Synthesize?', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9855.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9855.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Opinion/Aggregation models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Specialized Opinion and Aggregation Models (PlanSum, QT, AceSum, REFLECT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models designed for opinion summarization and content aggregation, used here as baselines/alternatives: PlanSum (aspect+sentiment disentanglement), QT (quantized transformer extractive summarizer), AceSum (hierarchical T5-based), and REFLECT (extract-then-abstractive pipeline trained with RL).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised opinion summarization with content planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PlanSum, QT, AceSum, REFLECT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PlanSum: LSTM-based with disentangled sentiment/aspect planning; QT: extractive summarizer using quantized embeddings and clustering; AceSum: hierarchical pooling feeding a T5 summarizer with aspect tokens; REFLECT: sentence-level extractor followed by abstractive BART-based generator trained with MLE and RL (credit-aware self-critic). These models were largely used with original hyperparameters adapted for longer sequences where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied to the Rotten Tomatoes movie meta-reviews dataset (and some models to systematic reviews where target prose aligns), with the same per-instance input sets described above.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Aggregate opinions into a single coherent summary reflecting overall sentiment and key aspects (movie critics) or produce summaries for systematic review settings where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Model-specific architectures: PlanSum uses disentangled sentiment/aspect modules plus copy-attention; QT uses extractive representative-sentence selection; AceSum and REFLECT use hierarchical encoding then generative decoding (T5/BART), trained with supervised objectives and some RL for REFLECT. In this paper they are evaluated out-of-the-box or with modest adaptation for MDS.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Opinion summaries / extractive or abstractive condensed reviews</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>No long example beyond comparative metrics and histograms; QT is extractive so outputs are representative input sentences; PlanSum/AceSum/REFLECT produce abstractive summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same evaluation pipeline: inferred sentiment correlation against Tomatometer for movies (R^2, PCC, ROUGE-1) and RobotReviewer-based classification for systematic reviews (F1, accuracy), plus sensitivity analyses to input ordering and composition.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Generally worse calibration than best transformer models on movie synthesis (e.g., QT R^2≈0.592 PCC≈0.788 R1≈0.122; REFLECT variants variable), but some are less order-sensitive by construction (e.g., QT extractive is order-insensitive). On systematic reviews performance varied (F1 in ~0.41–0.56 range across models).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Some architectures (QT, PlanSum) exhibit lower ordering sensitivity; tailored architectures incorporate sentiment/aspect modeling which can help opinion aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lower overall synthesis calibration compared to large fine-tuned transformers or GPT-4; extractive approaches may fail to produce concise aggregated prose matching human meta-reviews; do not always scale or adapt easily to clinical evidence summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Poor responsiveness to input-composition perturbations (undersensitivity) and lower alignment with the meta-analytic ground truth for systematic reviews; extractive outputs may not reflect aggregated nuance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Multi-Document Summarization Models Synthesize?', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9855.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9855.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RobotReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RobotReviewer (automating biomedical evidence synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated system that predicts whether narrative text reports a statistically significant treatment effect; used in this work both as an evaluation proxy and as an external selector/reranker for generated summary candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automating biomedical evidence synthesis: RobotReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RobotReviewer (off-the-shelf classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A domain-specific model from prior work for extracting and classifying trial-report statements; given narrative describing clinical trial results or a summary, RobotReviewer predicts whether the text indicates a significant effect. In this paper RobotReviewer was used to (a) measure whether generated summaries report 'significant effect' consistent with meta-analytic p<0.05, and (b) to select among multiple candidate summaries (choose candidate matching the majority vote of RobotReviewer predictions over inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied to narratives in the Cochrane systematic reviews dataset: model inputs consisted of trial result narratives and generated summary candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Detect whether a textual summary or trial narrative reports a statistically significant treatment effect (binary classification).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not a distillation model itself; used as an external measurement and selection oracle: majority-vote over RobotReviewer applied to inputs estimates expected aggregated label; RobotReviewer applied to generated candidates is used to select candidates matching that expected label, with abstention when none match.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Binary classification of textual content as reporting significant effect vs not; used to guide selection/abstention.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>RobotReviewer predicts labels such as 'significant effect' or 'no significant difference' for the narrative summary candidates; paper reports macro F1=0.577 and accuracy=68.6% when applied to human reference summaries compared to meta-analytic ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>RobotReviewer performance on the human-authored reference summaries vs. meta-analytic p<0.05 was used to gauge its reliability (reported macro F1=0.577, accuracy=68.6%). It was then used in pipeline selection experiments and to compute F1/accuracy measures for generated summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>RobotReviewer is an imperfect but usable proxy (F1≈0.577, Acc≈0.686 on references); pipeline selection using RobotReviewer improved macro F1 for systematic-review summarization in many models and enabled abstention behavior when no candidate matched the expected majority vote.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Provides an operationalizable, automated proxy for the binary clinical-effect synthesis objective, enabling automatic selection/reranking and abstention in a safety-critical domain.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Limited accuracy (noisy proxy) compared to true meta-analytic outcomes; using it as an oracle or selector may propagate its own biases and errors; it affords only a coarse binary signal (significant/not) and cannot capture effect size nuance.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Mismatch between RobotReviewer predictions and meta-analytic conclusions in some cases (Table 1 shows disagreements); reliance on RobotReviewer can lead to abstaining on many instances or selecting candidates consistent with RobotReviewer even when they conflict with quantitative meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Multi-Document Summarization Models Synthesize?', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9855.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9855.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diverse generate-then-select pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diverse Beam Search (DBS) + measurement-model selection and abstention pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time pipeline that generates a diverse set of candidate summaries (via Diverse Beam Search or sampling) and then selects the candidate whose inferred latent property (sentiment or significance) best matches the expected aggregate computed from inputs, with optional abstention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diverse beam search: Decoding diverse solutions from neural sequence models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Diverse Beam Search + reranker (measurement-model based selection)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DBS (Vijayakumar et al., 2016) modifies beam search to maintain multiple groups of beams and penalize similarity between groups (diversity hyperparameter lambda=0.5 used). For GPT-4 outputs sampling was used; for other models DBS produced multiple candidates (5 groups x 1 beam as implemented). An external measurement model (BERT sentiment regressor for movies; RobotReviewer for systematic reviews) scores candidates. The candidate closest to the expected aggregate (computed from per-input predictions) is selected; if none align, the system abstains.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied to the same Rotten Tomatoes and Cochrane systematic review datasets described above; for systematic reviews abstention was important for safety.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>16500</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Improve alignment of generated summaries with an expected aggregate property (mean sentiment or majority-significance) by selecting among diverse candidates and optionally abstaining when no candidate matches expected aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Generate-nominate-select: generate diverse candidates with DBS or sampling; use an external measurement model g to infer the latent property for each candidate; compute expected aggregate z from per-input measurements (e.g., proportion positive from BERT or RobotReviewer majority vote); select candidate minimizing |g(candidate)-z|; abstain if no candidate within threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Selected narrative summary (or abstain) that better aligns with the expected aggregate property; in systematic reviews output may be withheld (abstain) if no candidate agrees with expected label.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Table 11 shows diverse meta-review generations for one movie along with inferred sentiment scores; the candidate whose inferred sentiment is closest to the Tomatometer would be selected.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared calibration metrics (R^2, Pearson r) and classification metrics (F1, accuracy) between vanilla decoding and the generate-diverse-then-select pipeline; also measured ROUGE-1 and abstention rates. Human blind annotation (100 paired instances) used to assess preference between standard and diverse outputs for PRIMERA (Cohen's kappa and p-values reported).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Across models, approximate selection using the external measurement model improved synthesis calibration on both movie and systematic-review tasks (e.g., many models showed increased R^2/PCC for movie sentiment and increased macro F1 or abstention-augmented correctness for systematic reviews). Improvements came with small decreases in ROUGE-1 and increased abstention rates for clinical summaries. Example: PRIMERA approximate-selection R^2 rose from 0.608 to 0.749 in some experiments (see Table 7/8), and human annotation preferred diverse summaries with moderate agreement (Cohen's κ=0.59, p=0.003 for PRIMERA).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Simple, model-agnostic method that leverages existing measurement models to substantially improve alignment of generated summaries with an expected aggregate property; enables cautious behavior via abstention when no candidate matches expected aggregate; yields empirical improvements on calibration metrics across architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires an external, task-specific measurement model (g) that accurately predicts the latent property from text (limiting applicability to domains where such proxies exist); selection may degrade surface metrics (ROUGE) and can increase variability; does not fundamentally fix base-model generation failure modes — it picks better candidates when they exist but cannot create them if absent.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When all candidates disagree with the expected aggregate the pipeline abstains (useful in clinical context but reduces coverage); selection quality bounded by measurement model accuracy (e.g., RobotReviewer misclassifications can mislead selection); in some cases diverse decoding produced candidates that improved aggregate alignment but were lower in other quality aspects or incoherent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Multi-Document Summarization Models Synthesize?', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automating biomedical evidence synthesis: RobotReviewer <em>(Rating: 2)</em></li>
                <li>PRIMERA: Pyramid-based masked sentence pre-training for multidocument summarization <em>(Rating: 2)</em></li>
                <li>PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>Diverse beam search: Decoding diverse solutions from neural sequence models <em>(Rating: 2)</em></li>
                <li>Generating (Factual?) Narrative Summaries of RCTs: Experiments with Neural MultiDocument Summarization <em>(Rating: 2)</em></li>
                <li>PASS: Perturb-and-select summarizer for product reviews <em>(Rating: 1)</em></li>
                <li>Unsupervised opinion summarization with content planning <em>(Rating: 1)</em></li>
                <li>Scaling instruction-finetuned language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9855",
    "paper_id": "paper-7a6a20f705dfd5d1f8cfd24743b0977f34cdad1e",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A large instruction-tuned transformer LM (OpenAI) used in this work via prompting to produce opinionated meta-reviews and draft systematic-review conclusions from collections of reviews and randomized controlled trial (RCT) reports.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4 (-0613)",
            "model_description": "Closed-source, instruction-tuned large transformer language model from OpenAI. In this paper GPT-4 was used via system and assistant prompts (task-specific few-shot examples) to generate meta-reviews for movies and draft conclusions for systematic reviews; prompts were not heavily tuned (system prompt + one in-context example). For GPT-4 outputs the authors sampled five responses with temperature 0.6 for diverse decoding experiments.",
            "model_size": null,
            "input_corpus_description": "Two corpora were used with GPT-4 in experiments: (1) Rotten Tomatoes movie critic reviews with associated meta-reviews (9,095 movies; ~244k individual critic reviews; meta-review per movie); (2) A Cochrane-derived systematic reviews dataset comprising ~2,600 systematic reviews summarizing ~16,500 clinical trials (each review contains multiple trial reports and a natural-language summary and statistical meta-analysis).",
            "input_corpus_size": 16500,
            "topic_query_description": "Produce concise narrative syntheses: (a) an opinionated critics' meta-review reflecting aggregate sentiment across many critic reviews; (b) a draft/conclusions section of a systematic review summarizing whether the assembled trial evidence indicates a significant treatment effect.",
            "distillation_method": "Direct prompting: system prompt framing the model as an expert (professional movie critic or systematic reviewing expert), assistant prompt containing the concatenated inputs (reviews or trial narratives) and a one-shot example, and an instruction to produce a concise synthesis (opinionated summary or conclusions only). For diverse output generation the authors sampled 5 responses; also experimented with diverse beam search externally for other models but GPT-4 responses were sampled.",
            "output_type": "Narrative synthesis / meta-review text (opinionated summary for movies; conclusions-style summary for systematic reviews).",
            "output_example": "\"Please provide a draft systematic review for the studies below: {studies}. Start with the conclusions of the review only...\" (Paper includes example meta-review sentences for 'You Don't Mess with the Zohan' with inferred sentiment scores such as: \"You Don't Mess With the Zohan has its moments, but not all of them – and the jokes are embarrassingly crass and often crude.\")",
            "evaluation_method": "Automated measurement of the latent aspect in generated summaries using external measurement models: (a) a BERT-based continuous sentiment regressor (fine-tuned on SST/IMDB) for movie meta-reviews, evaluating correlation (R^2, Pearson r) between generated-summary sentiment and Tomatometer; (b) RobotReviewer binary classifier to infer whether a generated systematic-review summary reports a significant effect, compared with the ground-truth meta-analysis p&lt;0.05; metrics include macro F1, accuracy and ROUGE-1 for surface overlap. Additional behavioral tests: sensitivity analyses via 100 random permutations of input ordering and subsampling-based input-composition perturbations.",
            "evaluation_results": "On movie meta-reviews GPT-4 achieved R^2=0.808 and Pearson r=0.900 (higher than most fine-tuned transformers and higher than human references on the measured regression correlation). On systematic reviews GPT-4 achieved macro F1=0.628 and accuracy=0.640 (slightly outperforming the human reference on that coarse RobotReviewer-based metric). GPT-4 outputs were less sensitive to some perturbations than many models but still exhibited ordering/composition issues in some cases; diverse selection methods yielded modest further gains.",
            "strengths": "High measured alignment with aggregate signals (strongest R^2/PCC among evaluated models on movie sentiment), fluent and direct formulation of conclusions for systematic reviews, benefits from instruction prompting and larger model capacity; performs well with minimal prompt tuning.",
            "limitations": "No model-size or internals disclosed; outputs can still be sensitive to input ordering and input-composition changes; evaluation relies on external classifiers (BERT regressor, RobotReviewer) which are imperfect proxies; sampled outputs can vary and require selection/abstention strategies to ensure alignment with expected aggregate properties.",
            "failure_cases": "Instances where different permutations of the same input set led GPT-4 to flip conclusions or produce inconsistent synthesized sentiments; cases where modest changes in input composition (e.g., removing a portion of positive reviews) did not produce proportional changes in generated sentiment; occasional over-confident/incorrect statements relative to the meta-analytic ground truth as measured by RobotReviewer.",
            "uuid": "e9855.0",
            "source_info": {
                "paper_title": "Do Multi-Document Summarization Models Synthesize?",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Transformer summarizers",
            "name_full": "Fine-tuned Transformer-based Multi-Document Summarization Models (PRIMERA, PEGASUS, LED, T5, Flan-T5)",
            "brief_description": "A suite of transformer sequence-to-sequence models (PRIMERA, PEGASUS, Longformer-Encoder-Decoder (LED), T5 variants and Flan-T5 variants) were fine-tuned on movie meta-review and systematic-review datasets to synthesize aggregated summaries from many input documents.",
            "citation_title": "PRIMERA: Pyramid-based masked sentence pre-training for multidocument summarization",
            "mention_or_use": "use",
            "model_name": "PRIMERA, PEGASUS, LED (Longformer), T5 (Small/Base/etc), Flan-T5 (Small/Base/L/XL)",
            "model_description": "Pretrained transformer encoder-decoder architectures adapted for long-input multi-document summarization. Models were fine-tuned on each task using supervised MLE objectives (Adam optimizer; Pegasus with Adafactor for some runs), with hyperparameter searches over learning rates and training steps, batch size effective 16, mixed precision as available, and selection by ROUGE-1 on validation. PRIMERA is explicitly pre-trained for multi-document summarization; LED (Longformer) provides long-context attention.",
            "model_size": null,
            "input_corpus_description": "Same corpora as above: Rotten Tomatoes meta-reviews (9,095 movies; ~244k reviews) and Cochrane systematic reviews (~2,600 reviews summarizing ~16,500 RCTs). Instances vary in numbers of inputs (movie avg ~27 reviews per meta-review; systematic reviews avg ~6.6 inputs) and concatenated input lengths (movie ~800 tokens, systematic reviews up to ~2.6k tokens).",
            "input_corpus_size": 16500,
            "topic_query_description": "Produce concise summaries that synthesize a latent property across inputs: aggregate critic sentiment (movies) or whether the clinical evidence supports a significant treatment effect (systematic reviews).",
            "distillation_method": "Supervised fine-tuning for multi-document summarization using concatenated linearized inputs with special separators; decoding via standard beam search (5 beams) and experiments with Diverse Beam Search (DBS) to generate multiple diverse candidates; a plugged-in reranker selects candidates whose inferred aggregate property (via an external measurement model) best matches the expected aggregate computed from inputs.",
            "output_type": "Narrative meta-reviews or systematic-review style summaries",
            "output_example": "Example captions and summary texts reported in the paper (models generate short meta-reviews similar to reference targets; paper includes diverse candidate examples and their inferred sentiment scores).",
            "evaluation_method": "Measured calibration of the synthesized latent property via (a) regression metrics (R^2, Pearson r) comparing inferred sentiment from generated summaries to Tomatometer for movies; (b) RobotReviewer-inferred significant-effect classification compared to meta-analytic p&lt;0.05 for systematic reviews (macro F1, accuracy); also ROUGE-1 as surface quality metric. Behavioral analyses: input-permutation sensitivity (100 random permutations), input-composition perturbations (subsampling positive/negative reviews), and multiple-candidate selection/abstention evaluation.",
            "evaluation_results": "Transformer summarizers vary: PRIMERA and Flan-T5-XL among best non-GPT models (e.g., PRIMERA: R^2≈0.608, PCC≈0.780 on movies; Flan-T5-XL R^2≈0.611 PCC≈0.783). On systematic reviews, models attained macro F1 around 0.49–0.57 and accuracies in the 0.60–0.69 range; humans (reference summaries) measured via RobotReviewer yielded F1=0.577 Acc=0.686. Diverse-generation + selection improved alignment (higher R^2/PCC or F1) at the cost of small ROUGE-1 reductions and increased abstention rates for the systematic-review setting.",
            "strengths": "Pretrained multi-document models (PRIMERA, LED) and larger model sizes improved synthesis calibration; fine-tuning on domain data yields usable narrative summaries; diverse decoding plus an external selection criterion yields measurable improvements in calibration with respect to the target aggregate property.",
            "limitations": "Even the best fine-tuned transformers are sensitive to input ordering and under-responsive to moderate input-composition changes; gains in calibration sometimes traded off with surface-quality metrics (ROUGE-1) and increased variability; limited model sizes due to hardware constrained experiments; reliance on external measurement models for selection; inability to guarantee faithful capture of statistical meta-analytic conclusions.",
            "failure_cases": "Models sometimes flip whether a summary reports a significant effect under different input orderings; require large changes in input composition to flip summary sentiment/effect, i.e., under-sensitive; occasional hallucinated or unsupported statements relative to input evidence (factuality concerns).",
            "uuid": "e9855.1",
            "source_info": {
                "paper_title": "Do Multi-Document Summarization Models Synthesize?",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Opinion/Aggregation models",
            "name_full": "Specialized Opinion and Aggregation Models (PlanSum, QT, AceSum, REFLECT)",
            "brief_description": "Models designed for opinion summarization and content aggregation, used here as baselines/alternatives: PlanSum (aspect+sentiment disentanglement), QT (quantized transformer extractive summarizer), AceSum (hierarchical T5-based), and REFLECT (extract-then-abstractive pipeline trained with RL).",
            "citation_title": "Unsupervised opinion summarization with content planning",
            "mention_or_use": "use",
            "model_name": "PlanSum, QT, AceSum, REFLECT",
            "model_description": "PlanSum: LSTM-based with disentangled sentiment/aspect planning; QT: extractive summarizer using quantized embeddings and clustering; AceSum: hierarchical pooling feeding a T5 summarizer with aspect tokens; REFLECT: sentence-level extractor followed by abstractive BART-based generator trained with MLE and RL (credit-aware self-critic). These models were largely used with original hyperparameters adapted for longer sequences where applicable.",
            "model_size": null,
            "input_corpus_description": "Applied to the Rotten Tomatoes movie meta-reviews dataset (and some models to systematic reviews where target prose aligns), with the same per-instance input sets described above.",
            "input_corpus_size": null,
            "topic_query_description": "Aggregate opinions into a single coherent summary reflecting overall sentiment and key aspects (movie critics) or produce summaries for systematic review settings where applicable.",
            "distillation_method": "Model-specific architectures: PlanSum uses disentangled sentiment/aspect modules plus copy-attention; QT uses extractive representative-sentence selection; AceSum and REFLECT use hierarchical encoding then generative decoding (T5/BART), trained with supervised objectives and some RL for REFLECT. In this paper they are evaluated out-of-the-box or with modest adaptation for MDS.",
            "output_type": "Opinion summaries / extractive or abstractive condensed reviews",
            "output_example": "No long example beyond comparative metrics and histograms; QT is extractive so outputs are representative input sentences; PlanSum/AceSum/REFLECT produce abstractive summaries.",
            "evaluation_method": "Same evaluation pipeline: inferred sentiment correlation against Tomatometer for movies (R^2, PCC, ROUGE-1) and RobotReviewer-based classification for systematic reviews (F1, accuracy), plus sensitivity analyses to input ordering and composition.",
            "evaluation_results": "Generally worse calibration than best transformer models on movie synthesis (e.g., QT R^2≈0.592 PCC≈0.788 R1≈0.122; REFLECT variants variable), but some are less order-sensitive by construction (e.g., QT extractive is order-insensitive). On systematic reviews performance varied (F1 in ~0.41–0.56 range across models).",
            "strengths": "Some architectures (QT, PlanSum) exhibit lower ordering sensitivity; tailored architectures incorporate sentiment/aspect modeling which can help opinion aggregation.",
            "limitations": "Lower overall synthesis calibration compared to large fine-tuned transformers or GPT-4; extractive approaches may fail to produce concise aggregated prose matching human meta-reviews; do not always scale or adapt easily to clinical evidence summarization.",
            "failure_cases": "Poor responsiveness to input-composition perturbations (undersensitivity) and lower alignment with the meta-analytic ground truth for systematic reviews; extractive outputs may not reflect aggregated nuance.",
            "uuid": "e9855.2",
            "source_info": {
                "paper_title": "Do Multi-Document Summarization Models Synthesize?",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "RobotReviewer",
            "name_full": "RobotReviewer (automating biomedical evidence synthesis)",
            "brief_description": "An automated system that predicts whether narrative text reports a statistically significant treatment effect; used in this work both as an evaluation proxy and as an external selector/reranker for generated summary candidates.",
            "citation_title": "Automating biomedical evidence synthesis: RobotReviewer",
            "mention_or_use": "use",
            "model_name": "RobotReviewer (off-the-shelf classifier)",
            "model_description": "A domain-specific model from prior work for extracting and classifying trial-report statements; given narrative describing clinical trial results or a summary, RobotReviewer predicts whether the text indicates a significant effect. In this paper RobotReviewer was used to (a) measure whether generated summaries report 'significant effect' consistent with meta-analytic p&lt;0.05, and (b) to select among multiple candidate summaries (choose candidate matching the majority vote of RobotReviewer predictions over inputs).",
            "model_size": null,
            "input_corpus_description": "Applied to narratives in the Cochrane systematic reviews dataset: model inputs consisted of trial result narratives and generated summary candidates.",
            "input_corpus_size": null,
            "topic_query_description": "Detect whether a textual summary or trial narrative reports a statistically significant treatment effect (binary classification).",
            "distillation_method": "Not a distillation model itself; used as an external measurement and selection oracle: majority-vote over RobotReviewer applied to inputs estimates expected aggregated label; RobotReviewer applied to generated candidates is used to select candidates matching that expected label, with abstention when none match.",
            "output_type": "Binary classification of textual content as reporting significant effect vs not; used to guide selection/abstention.",
            "output_example": "RobotReviewer predicts labels such as 'significant effect' or 'no significant difference' for the narrative summary candidates; paper reports macro F1=0.577 and accuracy=68.6% when applied to human reference summaries compared to meta-analytic ground truth.",
            "evaluation_method": "RobotReviewer performance on the human-authored reference summaries vs. meta-analytic p&lt;0.05 was used to gauge its reliability (reported macro F1=0.577, accuracy=68.6%). It was then used in pipeline selection experiments and to compute F1/accuracy measures for generated summaries.",
            "evaluation_results": "RobotReviewer is an imperfect but usable proxy (F1≈0.577, Acc≈0.686 on references); pipeline selection using RobotReviewer improved macro F1 for systematic-review summarization in many models and enabled abstention behavior when no candidate matched the expected majority vote.",
            "strengths": "Provides an operationalizable, automated proxy for the binary clinical-effect synthesis objective, enabling automatic selection/reranking and abstention in a safety-critical domain.",
            "limitations": "Limited accuracy (noisy proxy) compared to true meta-analytic outcomes; using it as an oracle or selector may propagate its own biases and errors; it affords only a coarse binary signal (significant/not) and cannot capture effect size nuance.",
            "failure_cases": "Mismatch between RobotReviewer predictions and meta-analytic conclusions in some cases (Table 1 shows disagreements); reliance on RobotReviewer can lead to abstaining on many instances or selecting candidates consistent with RobotReviewer even when they conflict with quantitative meta-analysis.",
            "uuid": "e9855.3",
            "source_info": {
                "paper_title": "Do Multi-Document Summarization Models Synthesize?",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Diverse generate-then-select pipeline",
            "name_full": "Diverse Beam Search (DBS) + measurement-model selection and abstention pipeline",
            "brief_description": "An inference-time pipeline that generates a diverse set of candidate summaries (via Diverse Beam Search or sampling) and then selects the candidate whose inferred latent property (sentiment or significance) best matches the expected aggregate computed from inputs, with optional abstention.",
            "citation_title": "Diverse beam search: Decoding diverse solutions from neural sequence models",
            "mention_or_use": "use",
            "model_name": "Diverse Beam Search + reranker (measurement-model based selection)",
            "model_description": "DBS (Vijayakumar et al., 2016) modifies beam search to maintain multiple groups of beams and penalize similarity between groups (diversity hyperparameter lambda=0.5 used). For GPT-4 outputs sampling was used; for other models DBS produced multiple candidates (5 groups x 1 beam as implemented). An external measurement model (BERT sentiment regressor for movies; RobotReviewer for systematic reviews) scores candidates. The candidate closest to the expected aggregate (computed from per-input predictions) is selected; if none align, the system abstains.",
            "model_size": null,
            "input_corpus_description": "Applied to the same Rotten Tomatoes and Cochrane systematic review datasets described above; for systematic reviews abstention was important for safety.",
            "input_corpus_size": 16500,
            "topic_query_description": "Improve alignment of generated summaries with an expected aggregate property (mean sentiment or majority-significance) by selecting among diverse candidates and optionally abstaining when no candidate matches expected aggregate.",
            "distillation_method": "Generate-nominate-select: generate diverse candidates with DBS or sampling; use an external measurement model g to infer the latent property for each candidate; compute expected aggregate z from per-input measurements (e.g., proportion positive from BERT or RobotReviewer majority vote); select candidate minimizing |g(candidate)-z|; abstain if no candidate within threshold.",
            "output_type": "Selected narrative summary (or abstain) that better aligns with the expected aggregate property; in systematic reviews output may be withheld (abstain) if no candidate agrees with expected label.",
            "output_example": "Table 11 shows diverse meta-review generations for one movie along with inferred sentiment scores; the candidate whose inferred sentiment is closest to the Tomatometer would be selected.",
            "evaluation_method": "Compared calibration metrics (R^2, Pearson r) and classification metrics (F1, accuracy) between vanilla decoding and the generate-diverse-then-select pipeline; also measured ROUGE-1 and abstention rates. Human blind annotation (100 paired instances) used to assess preference between standard and diverse outputs for PRIMERA (Cohen's kappa and p-values reported).",
            "evaluation_results": "Across models, approximate selection using the external measurement model improved synthesis calibration on both movie and systematic-review tasks (e.g., many models showed increased R^2/PCC for movie sentiment and increased macro F1 or abstention-augmented correctness for systematic reviews). Improvements came with small decreases in ROUGE-1 and increased abstention rates for clinical summaries. Example: PRIMERA approximate-selection R^2 rose from 0.608 to 0.749 in some experiments (see Table 7/8), and human annotation preferred diverse summaries with moderate agreement (Cohen's κ=0.59, p=0.003 for PRIMERA).",
            "strengths": "Simple, model-agnostic method that leverages existing measurement models to substantially improve alignment of generated summaries with an expected aggregate property; enables cautious behavior via abstention when no candidate matches expected aggregate; yields empirical improvements on calibration metrics across architectures.",
            "limitations": "Requires an external, task-specific measurement model (g) that accurately predicts the latent property from text (limiting applicability to domains where such proxies exist); selection may degrade surface metrics (ROUGE) and can increase variability; does not fundamentally fix base-model generation failure modes — it picks better candidates when they exist but cannot create them if absent.",
            "failure_cases": "When all candidates disagree with the expected aggregate the pipeline abstains (useful in clinical context but reduces coverage); selection quality bounded by measurement model accuracy (e.g., RobotReviewer misclassifications can mislead selection); in some cases diverse decoding produced candidates that improved aggregate alignment but were lower in other quality aspects or incoherent.",
            "uuid": "e9855.4",
            "source_info": {
                "paper_title": "Do Multi-Document Summarization Models Synthesize?",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automating biomedical evidence synthesis: RobotReviewer",
            "rating": 2
        },
        {
            "paper_title": "PRIMERA: Pyramid-based masked sentence pre-training for multidocument summarization",
            "rating": 2
        },
        {
            "paper_title": "PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization",
            "rating": 2
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2
        },
        {
            "paper_title": "Diverse beam search: Decoding diverse solutions from neural sequence models",
            "rating": 2
        },
        {
            "paper_title": "Generating (Factual?) Narrative Summaries of RCTs: Experiments with Neural MultiDocument Summarization",
            "rating": 2
        },
        {
            "paper_title": "PASS: Perturb-and-select summarizer for product reviews",
            "rating": 1
        },
        {
            "paper_title": "Unsupervised opinion summarization with content planning",
            "rating": 1
        },
        {
            "paper_title": "Scaling instruction-finetuned language models",
            "rating": 1
        }
    ],
    "cost": 0.02127625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Do Multi-Document Summarization Models Synthesize?</h1>
<p>Jay DeYoung ${ }^{1}$ Stephanie C. Martinez ${ }^{1}$ Iain J. Marshall ${ }^{2}$ Byron C. Wallace ${ }^{1}$<br>${ }^{1}$ Northeastern University, Boston, MA ${ }^{2}$ King's College London, London<br>deyoung.j@northeastern.edu martinez.s@northeastern.edu<br>iain.marshall@kcl.ac.uk b.wallace@northeastern.edu</p>
<h4>Abstract</h4>
<p>Multi-document summarization entails producing concise synopses of collections of inputs. For some applications, the synopsis should accurately synthesize inputs with respect to a key aspect, e.g., a synopsis of film reviews written about a particular movie should reflect the average critic consensus. As a more consequential example, narrative summaries that accompany biomedical systematic reviews of clinical trial results should accurately summarize the potentially conflicting results from individual trials. In this paper we ask: To what extent do modern multi-document summarization models implicitly perform this sort of synthesis? We run experiments over opinion and evidence synthesis datasets using a suite of summarization models, from fine-tuned transformers to GPT-4. We find that existing models partially perform synthesis, but imperfectly: even the best performing models are oversensitive to changes in input ordering and under-sensitive to changes in input compositions (e.g., ratio of positive to negative reviews). We propose a simple, general, effective method for improving model synthesis capabilities by generating an explicitly diverse set of candidate outputs, and then selecting from these the string best aligned with the expected aggregate measure for the inputs, or abstaining when the model produces no good candidate.</p>
<h2>1 Introduction</h2>
<p>Multi-document summarization (MDS) models aim to distill inputs into concise synopses that preserve key content. Examples of MDS include summarizing news articles (Dang, 2005; Fabbri et al., 2019; Gholipour Ghalandari et al., 2020; Evans et al., 2004), answering questions from multiple sources (Dang, 2006), and producing overviews of scientific literature (Liu* et al., 2018;</p>
<p>Lu et al., 2020; Mollá and Santiago-Martínez, 2012; Wallace et al., 2021; DeYoung et al., 2021). We expect summarization models to produce outputs consistent with inputs (Kryscinski et al., 2020; Nan et al., 2021b), e.g., discussing the same types of entities (Nan et al., 2021a) and allowing one to answer questions similar in a way that is consistent with individual inputs (Wang et al., 2020; Scialom et al., 2021).</p>
<p>In some applications models must synthesize inputs-i.e., aggregate potentially conflicting information-to yield an accurate synopsis (Figure 1). Consider the meta-reviews of movies featured on Rotten Tomatoes, ${ }^{1}$ which provide a consensus view of individual critic opinions. These reviews should reflect the mean and range of sentiment implicit in the input critiques: A summary of mostly negative reviews (e.g., Gigli) should communicate that the film was widely panned; a summary of mixed reviews (The Fifth Element) ought to convey that critics disagreed and discuss the main positive and negative attributes.</p>
<p>A more consequential example is summarizing the evidence presented in clinical trials. Individual trials will often present conflicting evidence about whether or not a particular health intervention is effective. An ideal summary of would appropriately weigh the findings presented in individual studies and reflect the evidence on balance.</p>
<p>What are the desiderata of multi-document synthesis? First, summaries produced by models should be consistent with the input data, with respect to the latent property of interest. In the case of Rotten Tomatoes, the sentiment of the summary should be in line with the aggregate sentiment expressed in the individual critic reviews. A corollary to this is that models should be sensitive to changes in the composition of inputs, e.g., removing most of the negative reviews from a set of inputs should yield a summary with a corresponding</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Two multi-document summarization tasks where models must implicitly synthesize inputs to produce accurate summaries. Left: Summarizing film reviews with varying sentiment to yield a <em>critics consensus</em>. Right: Summarizing trials that have evaluated a particular medical invention.</p>
<table>
<thead>
<tr>
<th>Study</th>
<th>Predicted Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input: ...Ibuprofen was twice as likely as acetaminophen to abort migraine within 2 hours. In the intent-to-treat analysis, children improved twice as often with ibuprofen and acetaminophen as with placebo...</td>
<td>no significant difference</td>
</tr>
<tr>
<td>Input: ...Children’s ibuprofen suspension at an OTC dose of 7.5 mg/kg is an effective and well-tolerated agent for pain relief in the acute treatment of childhood migraine, particularly in boys...</td>
<td>significant difference</td>
</tr>
<tr>
<td>Target: ...Low quality evidence from two small trials shows that ibuprofen appears to improve pain freedom for the acute treatment of children with migraine. We have only limited information on adverse events associated with ibuprofen in the trials included in this review...</td>
<td>no significant difference</td>
</tr>
</tbody>
</table>
<p>Table 1: Systematic review example (from Cochrane). The statistical meta-analysis result "significant difference" and RobotReviewer finding "no significant difference" disagree. In the case of Systematic Reviews, RobotReviewer serves as both the estimator of $z_{ij}$ and $G$.</p>
<p>increase in the expressed sentiment.</p>
<p>In this work we evaluate neural MDS models with respect to these criteria. To this end we use a meta-reviews dataset from Rotten Tomatoes (Leone, 2020) and a dataset of systematic reviews (meta-analyses) summarizing the evidence about medical interventions (Wallace et al., 2021). For the former we probe the degree to which generated meta-review sentiment agrees with the expected aggregate sentiment score; for the latter we evaluate whether the generated summary indicates that the input evidence suggests, on balance, that the intervention under consideration was effective.</p>
<p>Our main contributions are:</p>
<ol>
<li>To the best of our knowledge, this is the first work to investigate implicit <em>synthesis</em> in summarization, and the degree to which modern models are capable of this.</li>
<li>We show that "off-the-shelf" neural MDS models are somewhat inconsistent and insensitive with respect to performing synthesis in summarization.</li>
<li>We propose and evaluate a simple, general method of generating a diverse set of output candidates (Vijayakumar et al., 2016) and then selecting from these based on agreement with an expected aggregate measure (based on inputs), with promising results.</li>
</ol>
<h2>2 Synthesis and Summarization</h2>
<p>In standard multi-document summarization, we assume inputs $(X_i, y_i)$; $X_i = {x_{i1}, ..., x_{i|X_i |}}$. We then typically train a summarization model with parameters $\theta$, to consume $X_i$ and yield summaries $\hat{y}<em ij="ij">i$ as similar as possible to targets $y_i$. In a supervised setting, the standard objective estimates a $\theta$ to maximize target token log-probabilities. Assuming the input documents $x</em>}$ in $X_i$ have been linearized (i.e., concatenated, with special tokens demarcating individual inputs) into an input string $x_i^\hat{y<em t="1">i$, this objective takes the form: $\sum</em>}^{|y_i|} \log p_\theta (y_{it}|y_{i1}, ..., y_{i(t-1)}, x_i^\hat{y<em>i)$, where $p</em>\theta$ is a probability assigned to the token at position $t$ in the target $y_i$ by a summarization model with parameters $\theta$. By myopically focusing on encour-</p>
<p><sup>2</sup>Shah et al. (2021a) studies a low-resource health and nutrition setting, in which they extract relational tuples, apply a manual rule set for aggregation, and then generate a surface form following this result. See Section 6 for a discussion of Opinion Summarization work which considers synthesis as a <em>target</em> but not measure of summarization performance.</p>
<table>
<thead>
<tr>
<th></th>
<th>Movie Reviews</th>
<th></th>
<th></th>
<th>Systematic Reviews</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Train</td>
<td>Dev</td>
<td>Test</td>
<td>Train</td>
<td>Dev†</td>
<td>Test</td>
</tr>
<tr>
<td>Number of metareviews</td>
<td>7251</td>
<td>932</td>
<td>912</td>
<td>1675</td>
<td>360</td>
<td>397</td>
</tr>
<tr>
<td>Avg metareview length</td>
<td>32.0</td>
<td>32.6</td>
<td>32.4</td>
<td>101</td>
<td>107</td>
<td>111</td>
</tr>
<tr>
<td>Total number of inputs</td>
<td>195033</td>
<td>24336</td>
<td>24474</td>
<td>11054</td>
<td>1238</td>
<td>2669</td>
</tr>
<tr>
<td>Avg number of inputs</td>
<td>26.9</td>
<td>26.1</td>
<td>26.8</td>
<td>6.6</td>
<td>3.4</td>
<td>6.7</td>
</tr>
<tr>
<td>Avg length of individual input</td>
<td>30.6</td>
<td>30.8</td>
<td>30.6</td>
<td>475</td>
<td>379</td>
<td>449</td>
</tr>
<tr>
<td>Avg length of concatenated inputs</td>
<td>822</td>
<td>804</td>
<td>822</td>
<td>2641</td>
<td>1336</td>
<td>2544</td>
</tr>
<tr>
<td>Target Percent Positive</td>
<td>59.5</td>
<td>62.1</td>
<td>61.2</td>
<td>31.9</td>
<td>31.4</td>
<td>35.0</td>
</tr>
</tbody>
</table>
<p>Table 2: Dataset statistics for movie reviews (left) and systematic reviews (right). Number of metareviews, average meta-review length (tokens), input reviews per split, average number of inputs per instance, average total length of instance-inputs. For movie reviews, the target percent positive reports the fraction of metareviews with a positive sentiment; for systematic reviews this refers to the fraction of metareviews reporting a significant effect. † We subset the original dev set to instances of $\leq 4 k$ tokens (accommodating T5; other models can consume up to 16k).
aging the model to produce tokens mimicking the targets, this objective aligns with standard (but flawed) measures of automated summary quality like ROUGE (Lin, 2004), which quantify $n$-gram overlap between targets $y_{i}$ and outputs $\hat{y}_{i}$.</p>
<p>We are interested in settings in which there is an additional, latent property $z_{i j}$ implicit in the constituent input texts $x_{i j}$. For example, $z_{i j}$ might reflect the sentiment in critique $j$ of the film indexed by $i$. Summaries should synthesize this aspect, i.e., the generated summary $\hat{y}<em i="i">{i}$ should implicitly convey an aggregated $z</em>$. That is, summaries should roughly reflect the average sentiment and reported treatment effect in the cases of movie reviews and clinical trial reports, respectively.}$ which reflects a synthesis or aggregation $G$ over $Z_{i}=\left{z_{i 1}, \ldots z_{i\left|X_{i}\right|}\right}$. That is, we assume $z_{i}=G\left(Z_{i}\right)$. In both cases considered here-summaries of film critiques and synopses of clinical trials evidence- $G$ can reasonably be assumed to be a (weighted) mean, $G\left(Z_{i}\right)=\frac{1}{\left|X_{i}\right|} \sum_{j=1}^{\left|X_{i}\right|} \alpha_{i j} z_{i j</p>
<p>We investigate the following questions. (1) Do model summaries $\hat{y}<em _hat_y="\hat{y" i="i">{i}$ reflect the anticipated aggregate aspect of interest? That is, how well calibrated is the aspect communicated in the generated summary $\left(z</em>$ into the decoding process?}}\right)$ compared to the expected $z_{i}$ ? (2) Do these same results apply to other (not solely transformer) MDS architectures? (3) Can we improve the ability of summarization models to synthesize by explicitly incorporating synthesis targets $z_{i</p>
<p>We propose a simple inference-time procedure to explicitly preference output candidates that align with the expected aggregate property of interest (e.g., average sentiment), and report promising results under both automatic and manual eval-
uation. This strategy naturally lends itself to cautious summarization, i.e., approaches where the model can abstain from generating an output if it does not produce any candidates that reflect the anticipated aggregate measure.</p>
<h3>2.1 Movie Reviews</h3>
<p>We first consider a dataset comprising movie reviews and associated meta-reviews summarizing these from Rotten Tomatoes. An in-house staffer (at Rotten Tomatoes) summarizes movie critic reviews ${ }^{3}$ into meta-reviews (Barnes, 2017). These meta-reviews synthesize the input reviews, reflecting the aggregate critic reception of a film. Each meta-review is associated with a numerical "Tomatometer" score, which is an overall measure of the fraction of reviews that were positive (according to Rotten Tomatoes staffers) for the corresponding film (so here the target aggregation function $G$ would be this fraction). The Rotten Tomatoes dataset we use comprises 9,095 movies with meta-reviews constructed from 244,000 individual reviews (Table 2).</p>
<p>Measuring sentiment in movie reviews. We need to measure the property of interest in texts; for this we use a measurement model $g$-here we fine-tune a BERT model (Devlin et al., 2019) using the continuous (fine-grained) sentiment targets provided in the SST dataset (Socher et al., 2013). ${ }^{4}$ We fine-tuned this model on the SST dataset for 3 epochs with a learning rate of $5 \mathrm{e}-5$ using the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 2: Movie Reviews: Actual vs. Predicted Sentiments on generated summaries. Human outputs replace LED (upper left) for comparison.</p>
<p>Huggingface library <em>Wolf et al. (2020)</em> with no hyperparameter tuning. While the raw text of the SST dataset is in-domain (i.e., movie reviews), the targets themselves are not. When applying this fine-tuned $g$ to the movie meta-reviews, we find a reasonably strong correlation between our sentiment estimates and the “true” meta-review sentiment (“Tomatometer” score): The R^{2} (centered) is 0.696, mean squared error (MSE) is 0.022, and Pearson’s $r$ is 0.836 (Figure 2, upper left).</p>
<h3>2.2 Biomedical Systematic Reviews</h3>
<p>Our second dataset is a collection of systematic reviews from the Cochrane Collaboration. This dataset comprises roughly 2,600 systematic reviews summarizing a total of 16,500 clinical trials evaluating interventions in healthcare (Tables 1, 2). Each review includes a natural language summary and accompanying statistical meta-analysis results. The latter provides an aggregate statistical summary of the individual (study-level) data extracted from the trials included in each review. The natural language summary should accurately convey and contextualize the findings of the metaanalysis. Therefore, the (lack of) treatment efficacy communicated in a given summary should generally agree with the direction of the corre-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>sponding meta-analytic point estimate.</p>
<p>Measuring effects in evidence syntheses. For systematic reviews of clinical trials, we resort to a less granular classification model $g(x_{i j}),g(y_{i})$ which attempts to infer whether a text reports a significant result. Specifically, we use RobotReviewer <em>Marshall et al. (2017); DeYoung et al. (2020)</em>. Given a narrative describing a clinical trial result (or a summary of trials), RobotReviewer predicts whether the reported result indicates a significant effect of the treatment being investigated, or not. We can compare this prediction to the “truth”, which here is derived from the meta-analytic result (specifically by checking whether $p&lt;0.05$). Applying this off-the-shelf model to the manually composed summaries accompanying the meta-analyses in our Cochrane set, we observe a macro-average F1 score of 0.577 and 68.6% accuracy, providing a reasonable (if weak) measure for this task.</p>
<h2>3 Models</h2>
<p>We evaluate a suite of transformer <em>Vaswani et al. (2017)</em> summarization models: Pegasus <em>Zhang et al. (2020)</em>, Longformer <em>Beltagy et al. (2020)</em>, PRIMERA <em>Xiao et al. (2022)</em>, T5 <em>Raffel et al. (2020)</em> and Flan-T5 <em>Chung et al. (2022)</em>, and GPT-4 <em>OpenAI (2023)</em>. For each trainable transformer model and dataset we performed a hyperparameter search over learning rates and training steps (retaining most parameter defaults). We train with an effective batch size of 16 and floating point 16^{8} precision on an NVIDIA RTX-8000 GPU (due to</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>data size we can fit only a single instance in memory at a time for some models, and must use gradient accumulation).</p>
<p>Models were fine-tuned using the Adam optimizer (Kingma and Ba, 2014), except Pegasus which was fine-tuned with Adafactor (Shazeer and Stern, 2018), ${ }^{9}$ across several learning rates (1e4, 1e-5, 1e-6), for up to 20 k training steps. The best model was selected based on ROUGE-1 performance on the validation set. ${ }^{10}$ PRIMERA was designed and pre-trained specifically for multidocument summarization. Though not explicitly designed as multi-document summarization models, both Pegasus (Zhang et al., 2020) and T5 (Amplayo et al., 2021) have been used on multidocument tasks, while Longformer has been used for a related multi-document summarization task (DeYoung et al., 2021).</p>
<p>For GPT-4 (-0613) we use system prompt You are a professional movie critic. Your job is to provide an opinionated summary of a movie, in your own words. You will have access other critics' opinions of the movie. and assistant prompt For movie {movie}, other critics have written: {reviews}. In your own words, please produce an opinionated summary of {movie},, providing a one-shot example. For systematic reviews, we used the system prompt You are a systematic reviewing expert. Your job is to read randomized control trial reports and assist a medical researcher. You will aid in drafting systematic reviews. with assistant prompt: Please provide a draft systematic review for the studies below: {studies}. Start with the conclusions of the review only, a more detailed analysis will happen later, again providing a single shot example.</p>
<p>As it is not the focus of our work here, we did not extensively tune these prompts. We inspected outputs over five training instances when developing prompts for both movies and systematic reviews datasets. When designing movie review prompts, we iterated through first asking the model to summarize the reviews (yielding a summary of each review instead of an aggregate), then telling the model to use the same language as the reviews (with effectively the same result), then providing a single example (yielding some improvement), then demanding an opinionated sum-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>mary (again with some improvement), and finally telling the model to use its own words (yielding the prompt above and experiments below). For the systematic review prompt, we first we asked for a draft review (the model provided an entire draft), then we specified conclusions only (we received an abbreviated abstract), then we specified a conclusions section (we received a less abbreviated abstract), and, finally, adding an in-context example. We also explored asking for a high level summary (rather than systematic review) of the input studies; and with prompts providing intervention and outcome information to the model and asking for a draft of the review.</p>
<p>Beyond transformers, we consider models from the opinion summarization and content aggregation literature: PlanSum (Amplayo et al., 2020), QT (Angelidis et al., 2021), AceSum (Amplayo et al., 2021), and REFLECT (Song et al., 2022). ${ }^{11}$ PlanSum (Amplayo et al., 2020) learns a (disentangled) sentiment and aspect model, and augments an LSTM equipped with an attention-copy mechanism (Bahdanau et al., 2014; Vinyals et al., 2015) with this information as a decoder.</p>
<p>QT (Angelidis et al., 2021) learns a quantized embedding for each model input via an autoencoder, then finds representative input sentences (via clustering and assignment) to use as summaries. We include QT ${ }^{12}$ as an extractive model. AceSum (Amplayo et al., 2021) adopts a hierarchical approach, representing each input document as sentences pooled over individual inputs, and passing this representation to a transformer (T5; Raffel et al., 2020), along with specific aspect or general codeword tokens and vocabulary embeddings, controlling what type of summary to produce (we focus on the general case). REFLECT (Song et al., 2022) takes the hierarchical approach one step further, with a sentence level extraction phase (using aggregated token representations) followed by an abstraction phase (BART; Lewis et al., 2020), trained via standard MLE and via a reinforcement learning credit aware self-critic method (Rennie et al., 2017). For</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>$\mathrm{R}^{2}$</th>
<th>PCC</th>
<th>R1</th>
</tr>
</thead>
<tbody>
<tr>
<td>QT</td>
<td>0.592</td>
<td>0.788</td>
<td>0.122</td>
</tr>
<tr>
<td>PlanSum</td>
<td>0.245</td>
<td>0.510</td>
<td>0.160</td>
</tr>
<tr>
<td>AceSum</td>
<td>0.158</td>
<td>0.439</td>
<td>0.176</td>
</tr>
<tr>
<td>REFLECT</td>
<td>0.430</td>
<td>0.657</td>
<td>0.241</td>
</tr>
<tr>
<td>REFLECT</td>
<td>0.225</td>
<td>0.507</td>
<td>0.218</td>
</tr>
<tr>
<td>Pegasus</td>
<td>0.530</td>
<td>0.730</td>
<td>0.245</td>
</tr>
<tr>
<td>LED</td>
<td>0.551</td>
<td>0.742</td>
<td>0.242</td>
</tr>
<tr>
<td>PRIMERA</td>
<td>0.608</td>
<td>0.780</td>
<td>0.254</td>
</tr>
<tr>
<td>T5-Small</td>
<td>0.441</td>
<td>0.669</td>
<td>0.234</td>
</tr>
<tr>
<td>T5-Base</td>
<td>0.516</td>
<td>0.720</td>
<td>0.253</td>
</tr>
<tr>
<td>Flan-T5-S</td>
<td>0.412</td>
<td>0.647</td>
<td>0.237</td>
</tr>
<tr>
<td>Flan-T5-B</td>
<td>0.597</td>
<td>0.774</td>
<td>0.247</td>
</tr>
<tr>
<td>Flan-T5-L</td>
<td>0.484</td>
<td>0.696</td>
<td>0.248</td>
</tr>
<tr>
<td>Flan-T5-XL</td>
<td>0.611</td>
<td>0.783</td>
<td>$\mathbf{0 . 2 6 2}$</td>
</tr>
<tr>
<td>GPT-4</td>
<td>$\mathbf{0 . 8 0 8}$</td>
<td>$\mathbf{0 . 9 0 0}$</td>
<td>0.166</td>
</tr>
<tr>
<td>Reference</td>
<td>0.697</td>
<td>0.836</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 3: Synthesis results for Movie reviews: correlations ( $\mathrm{R}^{2}$, Pearson's $r$ ) between sentiment measured in model outputs and Tomatometer Ratings. R1 is ROUGE1.
all models we largely retained the original hyperparameters, with modifications to increase sequence lengths and decrease aspects (these models were developed around aspect summarization).</p>
<h2>4 Experiments</h2>
<h3>4.1 Do Summarization Models Synthesize?</h3>
<p>We report sentiment performance for all models in Table 3. These metrics quantify the strength of the relationship between (a) the continuous sentiment inferred (via our text regression measurement model $g$ ) over model generated or reference summaries and (b) the reference sentiment (Tomatometer) score.</p>
<p>Save for GPT-4, correlations between the sentiment measured in generated outputs and Tomatometer scores are considerably lower than that between the same measurement over humancomposed summaries and said score. This implies that human authors tend to do a better job of synthesis than models when composing summaries. GPT-4 seems performs especially well here; we are not entirely sure why, but it may owe to the differences in lengths of outputs ( 133 tokens on average vs. 31 for reference summaries).</p>
<p>For systematic reviews (Section 2.2), the measurement model $g$ attempts to infer whether a text reports a significant treatment effect; we compare this against the $p$-value from the corresponding statistical meta-analysis. This permits a coarse as-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">Acc</th>
<th style="text-align: center;">R1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PlanSum</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.177</td>
</tr>
<tr>
<td style="text-align: left;">AceSum</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.550</td>
<td style="text-align: center;">0.151</td>
</tr>
<tr>
<td style="text-align: left;">REFLECT</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">0.271</td>
</tr>
<tr>
<td style="text-align: left;">REFLECT</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.199</td>
</tr>
<tr>
<td style="text-align: left;">Pegasus</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.212</td>
</tr>
<tr>
<td style="text-align: left;">LED</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">0.259</td>
</tr>
<tr>
<td style="text-align: left;">PRIMERA</td>
<td style="text-align: center;">0.526</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.253</td>
</tr>
<tr>
<td style="text-align: left;">T5-Small</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.205</td>
</tr>
<tr>
<td style="text-align: left;">T5-Base</td>
<td style="text-align: center;">0.521</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.206</td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5-Small</td>
<td style="text-align: center;">0.548</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.081</td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5-Base</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.194</td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5-L</td>
<td style="text-align: center;">0.556</td>
<td style="text-align: center;">$\mathbf{0 . 6 9 2}$</td>
<td style="text-align: center;">0.218</td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5-XL</td>
<td style="text-align: center;">0.487</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.268</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$\mathbf{0 . 6 2 8}$</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">$\mathbf{0 . 2 7 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: center;">0.577</td>
<td style="text-align: center;">0.686</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4: Synthesis results for Systematic reviews: Macro-averaged F1s and accuracies (RobotReviewer predictions over model outputs vs. reference meta-analysis results).
sessment of synthesis, as we are unable to measure correlations. Instead we report classification metrics describing how often the effect significance inferred from a summary (generated or manually written) matches the ground truth derived from the meta-analysis (Table 4). The results are qualitatively similar to the sentiment case, in that the humans appear to do a better job of synthesis-as best we can measure, the significance reported in their summaries better aligns with the statistical results than in model generated summaries. GPT4 is again an exception, slightly outperforming human results on this metric, which may owe to its formulaic generation featuring strong, direct, clear initial statements of treatment effectiveness.</p>
<h3>4.2 Sensitivity to Input Ordering</h3>
<p>Synthesis of inputs should be invariant to ordering (e.g., critic consensus on a film does not depend on the order in which one reads the reviews). Here we evaluate if models are sensitive to input ordering with respect to the synthesized aspect of interest $\left(z_{i \hat{y}}\right)$. Specifically, let $X_{i}=\left{x_{i 1}, \ldots, x_{i \mid X_{i} \mid}\right}$ denote an arbitrary ordering of inputs in the linearized version $x_{i}^{\hat{\phi}<em _hat_y="\hat{y" i="i">{i}}$. This ordering should not affect the aggregate aspect $z</em>$ in the summary.}</p>
<p>To evaluate if models realize this invariance, we permute the instance $i$ inputs $X_{i}$ (and, consequently, the linearized $x_{i}^{\hat{\phi}_{i}}$ ) one hundred times, ${ }^{13}$</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The spread of sentiment/treatment effect measured in outputs produced from permuted input orderings. Left: Movie review sentiment. Right: Systematic review significance prediction entropy (0 indicates order insensitivity) on the subset of reviews that report <em>significant</em> effects.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: ROUGE1 deltas from instance means for movie reviews (left) and systematic reviews (right).</p>
<p>randomizing input orderings. For each such permutation $\tilde{X}<em i_hat_y="i\hat{y">i$ (and associated $\tilde{x}_i^{\oplus}$), we generate a summary $\hat{y}_i$ and estimate of the resultant aspect $\tilde{z}</em>$'s under different random orderings.}}$, using the corresponding measurement model. By repeating this process for each instance $i$, we can construct an empirical distribution over $\tilde{z}_{i\hat{y}</p>
<p><strong>Movie reviews.</strong> We zero-mean the $\tilde{z}_{i\hat{y}}$'s inferred over each instance, and combine the distributions from all instances into a histogram (Figure 3). This shows the spread of sentiments inferred over outputs under random input orderings minus the corresponding instance mean sentiment. Were a model completely invariant to ordering, the empirical distribution over these differences would collapse to 0. Instead, we observe a relatively wide spread in sentiment measured over outputs generated from different permutations, indicating a counter-intuitive sensitivity to orderings. (Interestingly, Figure 4—provided for comparisonsuggests such permutations also affect ROUGE; we do not explore this aspect further here.)</p>
<p><strong>Systematic reviews.</strong> For each $X_i$ we have 100 order permutations and associated summaries; we infer whether these report <em>significant results</em> or not, and record the fraction that do ($p_i$). If models were invariant to ordering, this fraction would always be 0 or 1. Values in-between suggest the model flips the report conclusion as a result of different input orderings. Figure 3 (right) shows a histogram of entropies over $p_i$, computed over the subset of examples where the associated meta-analysis indicates a significant effect. Densities away from zero indicate sensitivity to ordering. QT, PlanSum, and GPT-4 all have a smaller spread than the other models — QT because it is order insensisitive by construction, PlanSum similarly (but not entirely), and GPT-4 due to overall quality performance. We note that sensitivity is clearly an undesirable trait (<em>any</em> spread is undesirable), but this may trade off against other metrics of interest.</p>
<h3>4.3 Sensitivity to Input Composition</h3>
<p>Synthesis models should be responsive to changes in the distribution of the attribute to be synthesized in the input composition: If we increase the ratio of positive to negative reviews in an input set, we would anticipate a concomitant change in the sentiment communicated in the meta-review $z_{i\hat{y}}$. To assess if models meet this synthesis desiderata, we manipulate model inputs $X_i$ in such a way to induce an expected change in the target measure $z_{i\hat{y}}$; we then measure if the output yields a summary that aligns with this expected change.</p>
<p><strong>Movie reviews.</strong> We manipulate the ratio of positive to negative reviews and observe the resultant</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Model sensitivity to manipulated input sentiment composition. Intensity patterns indicate that models oscillate between low and high sentiments in outputs, and are not responsive to subtler shifts in input sentiment. We show a model regression (blue) and the reference sensitivity regression (black).</p>
<table>
<thead>
<tr>
<th></th>
<th>R²</th>
<th>PCC</th>
</tr>
</thead>
<tbody>
<tr>
<td>QT</td>
<td>0.634</td>
<td>0.796</td>
</tr>
<tr>
<td>PlanSum</td>
<td>0.249</td>
<td>0.499</td>
</tr>
<tr>
<td>AceSum</td>
<td>0.177</td>
<td>0.420</td>
</tr>
<tr>
<td>REFLECTMLE</td>
<td>0.439</td>
<td>0.663</td>
</tr>
<tr>
<td>REFLECTRL</td>
<td>0.294</td>
<td>0.542</td>
</tr>
<tr>
<td>Pegasus</td>
<td>0.499</td>
<td>0.706</td>
</tr>
<tr>
<td>LED</td>
<td>0.524</td>
<td>0.724</td>
</tr>
<tr>
<td>PRIMERA</td>
<td>0.572</td>
<td>0.756</td>
</tr>
<tr>
<td>T5-Small</td>
<td>0.447</td>
<td>0.668</td>
</tr>
<tr>
<td>T5-Base</td>
<td>0.481</td>
<td>0.694</td>
</tr>
<tr>
<td>Flan-T5-Small</td>
<td>0.393</td>
<td>0.627</td>
</tr>
<tr>
<td>Flan-T5-Base</td>
<td>0.556</td>
<td>0.746</td>
</tr>
<tr>
<td>Flan-T5-Large</td>
<td>0.490</td>
<td>0.700</td>
</tr>
<tr>
<td>Flan-T5-XL</td>
<td>0.551</td>
<td>0.742</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.457</td>
<td>0.677</td>
</tr>
</tbody>
</table>
<p>Table 5: Movie reviews Correlations between subsampled inputs and generations.</p>
<p>Change in the property of interest latent in the corresponding output. We take movies with mixed reviews, and delete 10%, 20%, 30%, ..., 100% of the positive inputs, retaining the negative inputs; we then repeat the process but instead remove negative inputs. For each of these permutations, we measure the input sentiment, the meta-review sentiment, and how well they correlate (Table 5).</p>
<p>Figure 5 plots the relationship between the fraction of positive reviews in the (manipulated) input sets and the granular sentiment score inferred over the resultant outputs. The models are generally undersensitive to changes in their input: rather than having a change in meta-review sentiment equivalent in size to changes in input sentiment (a slope of 1, as we observe when we fit a model).</p>
<table>
<thead>
<tr>
<th></th>
<th>F1</th>
<th>Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td>PlanSum</td>
<td>0.442</td>
<td>0.741</td>
</tr>
<tr>
<td>AceSum</td>
<td>0.454</td>
<td>0.504</td>
</tr>
<tr>
<td>REFLECTMLE</td>
<td>0.471</td>
<td>0.583</td>
</tr>
<tr>
<td>REFLECTRL</td>
<td>0.445</td>
<td>0.689</td>
</tr>
<tr>
<td>Pegasus</td>
<td>0.452</td>
<td>0.680</td>
</tr>
<tr>
<td>LED</td>
<td>0.510</td>
<td>0.684</td>
</tr>
<tr>
<td>PRIMERA</td>
<td>0.533</td>
<td>0.675</td>
</tr>
<tr>
<td>T5-Small</td>
<td>0.560</td>
<td>0.618</td>
</tr>
<tr>
<td>T5-Base</td>
<td>0.469</td>
<td>0.658</td>
</tr>
<tr>
<td>Flan-T5-Small</td>
<td>0.430</td>
<td>0.500</td>
</tr>
<tr>
<td>Flan-T5-Base</td>
<td>0.482</td>
<td>0.680</td>
</tr>
<tr>
<td>Flan-T5-Large</td>
<td>0.435</td>
<td>0.693</td>
</tr>
<tr>
<td>Flan-T5-XL</td>
<td>0.464</td>
<td>0.649</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.511</td>
<td>0.530</td>
</tr>
</tbody>
</table>
<p>Table 6: Systematic reviews: Classification performance for subsampled inputs and generations. See Figure 6 for a visualization of classification <em>distribution</em>, analogous to Figure 5 for movies.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Systematic Reviews. A histogram of entropies for the <em>subsampled</em> review classifications (where the ground truth is positive).</p>
<p>to the human written summaries). Models tend to have trouble changing their sentiment, and require a large change in input distribution to substantially change the sentiment communicated in the output.</p>
<p>Systematic Reviews. To measure sensitivity to changes in input composition, we manipulate inputs $X_{i}$ such that the meta-analysis result (target $z_{i \hat{y}}$ ) flips from a significant effect to no effect, or from no effect to an effect (Table 6, Fig. 6). We first take a subset of the reviews that have conflicting evidence (139 unique reviews). We then order inputs in these by (weighted) effect sizes, ${ }^{14}$ and remove subsets which ought to flip the significance result of a subsequent meta-analysis. The surface level results (Table 6) show little difference from earlier results (i.e. the $\Delta$ values are approximately comparable to Table 4), but our classification results become substantially noisier (Figure 6). We speculate that models are picking up on some uncertainty from the change in overall meta-analysis but overall fail to capture that detail in their outputs. Even if the models reflect uncertainty due to the strength of the change (desirable!) this is still incorrect as the finding has changed.</p>
<p>Result. In both the case of the Movie Reviews and the Systematic Reviews, we see a substantial drop in performance from the base review results (reported in Tables 3,4). We can only speculate as to the cause of this. Perhaps this indicates memorization of original targets in pre-training, or maybe removing strong (positive or negative) reviews hampers performance.</p>
<h2>5 Improving Synthesis in Summarization</h2>
<p>We propose a straightforward post-hoc approach to improving the synthesis performed by multidocument summarization models: (1) Generate an explicitly diverse set of output candidates; (2) Select from these as the final output the candidate that best agrees with the expected synthesis result (as predicted by an external model). ${ }^{15}$</p>
<p>For (1), we rely on an existing technique for generating diverse outputs $\mathcal{C}<em i="i">{i}$ from input $x</em>: D i$ -}^{0</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>verse Beam Search (DBS) (Vijayakumar et al., 2016). This method modifies standard beam search to maintain multiple groups of beams. During decoding, a term is added to the next-token log probabilities, penalizing production of strings similar to candidates in other groups. ${ }^{16}$</p>
<p>In (2) we would like to select the output that best synthesizes the property of interest; this requires an approach to specify what we expect the synthesized property be, given the inputs. For example, if we know the sentiment scores associated with input movie reviews, we might enforce that the output sentiment agrees with the average of these. To realize this intuition, we can select as final output from $\mathcal{C}_{i}$ the string that best aligns with this aggregate property (sentiment score or significance finding). Operationally, this requires an external model to estimate the aspect of interest as latent in a given candidate output. This is a limitation of the approach, but in many settings it may be feasible to identify or construct a model; we were able to do so for both tasks considered here.</p>
<p>It may be that any member of $\mathcal{C}_{i}$ will align well with the anticipated aggregated property. In such cases, we have no means of producing an output consistent with respect to synthesis, and it may be desirable to abstain from outputting anything at all in such cases; that is, to be a cautious summarizer (Ferri et al., 2004; Hechtlinger et al., 2018). We consider this strategy in the case of generating narrative synopses of evidence, as this constitutes a case in which (a) one would very much prefer not to produce a misleading summary of clinical evidence (Kell et al., 2021), and, (b) we observe many cases where the diverse decoding strategy yields an output that seems to communicate (at a granular level) the aggregate findings expected.</p>
<p>Movie Reviews We use BERT (Devlin et al., 2019), fine-tuned on IMDB (Maas et al., 2011) ${ }^{17}$ to predict the sentiment inputs $x_{i j}$, using the proportion of $x_{i j} \in X_{i}$ with a positive score to approximate the target sentiment $z_{i \hat{y}}$. For each di-</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Our proposed strategy to improve synthesis. We generate an diverse set of output candidates (Vijayakumar et al., 2016) and then select the text that best agrees with the <em>predicted</em> aggregate property of interest (here, sentiment). We can also <em>abstain</em> when the model fails to yield an appropriate output.</p>
<table>
<thead>
<tr>
<th></th>
<th>Approximate Selection</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Oracle Selection</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>R²</td>
<td>Δ</td>
<td>PCC</td>
<td>Δ</td>
<td>R1</td>
<td>Δ</td>
<td>R²</td>
<td>Δ</td>
<td>PCC</td>
<td>Δ</td>
<td>R1</td>
<td>Δ</td>
</tr>
<tr>
<td>AceSum</td>
<td>0.566</td>
<td>0.408</td>
<td>0.769</td>
<td>0.330</td>
<td>0.162</td>
<td>-0.014</td>
<td>0.723</td>
<td>0.565</td>
<td>0.861</td>
<td>0.422</td>
<td>0.162</td>
<td>-0.014</td>
</tr>
<tr>
<td>REFLECT</td>
<td>0.658</td>
<td>0.228</td>
<td>0.825</td>
<td>0.168</td>
<td>0.241</td>
<td>0.000</td>
<td>0.791</td>
<td>0.361</td>
<td>0.895</td>
<td>0.238</td>
<td>0.240</td>
<td>-0.001</td>
</tr>
<tr>
<td>REFLECT</td>
<td>0.491</td>
<td>0.266</td>
<td>0.702</td>
<td>0.195</td>
<td>0.220</td>
<td>0.002</td>
<td>0.576</td>
<td>0.351</td>
<td>0.759</td>
<td>0.252</td>
<td>0.219</td>
<td>0.001</td>
</tr>
<tr>
<td>Pegasus</td>
<td>0.694</td>
<td>0.164</td>
<td>0.835</td>
<td>0.105</td>
<td>0.229</td>
<td>-0.016</td>
<td>0.799</td>
<td>0.269</td>
<td>0.894</td>
<td>0.164</td>
<td>0.232</td>
<td>-0.013</td>
</tr>
<tr>
<td>LED</td>
<td>0.656</td>
<td>0.105</td>
<td>0.821</td>
<td>0.079</td>
<td>0.229</td>
<td>-0.013</td>
<td>0.763</td>
<td>0.212</td>
<td>0.878</td>
<td>0.136</td>
<td>0.227</td>
<td>-0.015</td>
</tr>
<tr>
<td>PRIMERA</td>
<td>0.749</td>
<td>0.141</td>
<td>0.880</td>
<td>0.100</td>
<td>0.240</td>
<td>-0.014</td>
<td>0.890</td>
<td>0.282</td>
<td>0.948</td>
<td>0.168</td>
<td>0.240</td>
<td>-0.014</td>
</tr>
<tr>
<td>T5-Small</td>
<td>0.692</td>
<td>0.251</td>
<td>0.846</td>
<td>0.177</td>
<td>0.225</td>
<td>-0.009</td>
<td>0.827</td>
<td>0.386</td>
<td>0.913</td>
<td>0.244</td>
<td>0.226</td>
<td>-0.008</td>
</tr>
<tr>
<td>T5-Base</td>
<td>0.721</td>
<td>0.205</td>
<td>0.856</td>
<td>0.136</td>
<td>0.231</td>
<td>-0.022</td>
<td>0.876</td>
<td>0.360</td>
<td>0.938</td>
<td>0.218</td>
<td>0.230</td>
<td>-0.023</td>
</tr>
<tr>
<td>Flan-T5-S</td>
<td>0.698</td>
<td>0.286</td>
<td>0.837</td>
<td>0.190</td>
<td>0.219</td>
<td>-0.018</td>
<td>0.832</td>
<td>0.420</td>
<td>0.912</td>
<td>0.265</td>
<td>0.218</td>
<td>-0.019</td>
</tr>
<tr>
<td>Flan-T5-B</td>
<td>0.732</td>
<td>0.135</td>
<td>0.863</td>
<td>0.089</td>
<td>0.225</td>
<td>-0.022</td>
<td>0.863</td>
<td>0.266</td>
<td>0.930</td>
<td>0.156</td>
<td>0.225</td>
<td>-0.022</td>
</tr>
<tr>
<td>Flan-T5-L</td>
<td>0.732</td>
<td>0.248</td>
<td>0.866</td>
<td>0.170</td>
<td>0.243</td>
<td>-0.005</td>
<td>0.875</td>
<td>0.391</td>
<td>0.937</td>
<td>0.241</td>
<td>0.244</td>
<td>-0.004</td>
</tr>
<tr>
<td>Flan-T5-XL</td>
<td>0.769</td>
<td>0.158</td>
<td>0.888</td>
<td>0.105</td>
<td>0.250</td>
<td>-0.012</td>
<td>0.900</td>
<td>0.289</td>
<td>0.950</td>
<td>0.167</td>
<td>0.248</td>
<td>-0.014</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.814</td>
<td>0.006</td>
<td>0.924</td>
<td>0.024</td>
<td>0.159</td>
<td>-0.007</td>
<td>0.914</td>
<td>0.106</td>
<td>0.963</td>
<td>0.063</td>
<td>0.164</td>
<td>-0.002</td>
</tr>
<tr>
<td>Reference</td>
<td>0.697</td>
<td></td>
<td>0.836</td>
<td></td>
<td></td>
<td></td>
<td>0.697</td>
<td></td>
<td>0.836</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 7: Movie Reviews: Generate diverse meta-reviews and select from them using an approximate (left) or oracle (right) target sentiment. Performance improves on every measure except ROUGE-1. Δs compare the metric to their left with the results reported in Table 3.</p>
<p>verse prediction $$C_i$$, we predict its sentiment $$\hat{z_{ij}}$$ via our regression model (2.1), and select the prediction closest to the estimated target sentiment $$|z_{ij} - z_{ij}|$$. We find this improves model synthesis performance (Table 7; Figure 8). Two authors blindly annotated 100 paired instances over PRIMERA generations for sentiment preference (matching the reference) between standard and diverse outputs. We find a moderate agreement Cohen's κ=0.59, and a statistically significant preference for the diverse summaries (p=0.003).</p>
<p><strong>Systematic Reviews.</strong> For systematic reviews, we have a binary measure of <em>significant effect</em> (or not). As a proxy for $$z_{ij}$$, we use RobotReviewer to extract an effect for each of the model inputs $$x_{ij}$$, using the majority vote (i.e., do the plurality of $$x_{ij} \in X_i$$ indicate that there was an effect). We classify each output candidate in $$C_i$$ again using RobotReviewer to estimate $$z_{ij}$$. We then select for output the highest probability candidate in $$C_i$$ which agrees with the majority vote of the inputs, and abstain where there are no viable candidates. When we are able to choose a summary, we find performance similar to our measure (Table 9).</p>
<p><strong>Result.</strong> Movie reviews show a wide range of sentiments; systematic reviews show some improvement but are biased towards no effect. Both settings show improvement from the switch to diverse decoding over standard beam-search methods: We repeat the generate-multiple-then-select approach with movie reviews (Table 8) and systematic reviews (Table 10). While the standard beam search did produce better overall scores when considering multiple candidates,</p>
<p>^{18}Summaries were ordered by difference in extracted sentiments between base outputs and diverse outputs, then 100 instances randomly selected from the top 20th percentile.</p>
<p>|  | Approximate Selection |  |  |  |  |  | Oracle Selection |  |  |  |  |  |
|  | $\mathrm{R}^{2}$ | $\Delta$ | PCC | $\Delta$ | R1 | $\Delta$ | $\mathrm{R}^{2}$ | $\Delta$ | PCC | $\Delta$ | R1 | $\Delta$ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| AceSum | 0.534 | 0.376 | 0.740 | 0.301 | 0.177 | 0.001 | 0.509 | 0.351 | 0.715 | 0.276 | 0.177 | 0.001 |
| REFLECT ${ }^{\text {MLE }}$ | 0.555 | 0.125 | 0.750 | 0.093 | 0.248 | 0.007 | 0.603 | 0.173 | 0.780 | 0.123 | 0.247 | 0.006 |
| REFLECT ${ }^{\text {RL }}$ | 0.406 | 0.181 | 0.638 | 0.131 | 0.222 | 0.004 | 0.454 | 0.229 | 0.675 | 0.168 | 0.221 | 0.003 |
| PEGASUS | 0.649 | 0.119 | 0.809 | 0.079 | 0.248 | 0.003 | 0.705 | 0.175 | 0.840 | 0.110 | 0.247 | 0.002 |
| LED | 0.653 | 0.102 | 0.815 | 0.073 | 0.241 | -0.001 | 0.711 | 0.160 | 0.847 | 0.105 | 0.240 | -0.002 |
| PRIMERA | 0.685 | 0.077 | 0.833 | 0.053 | 0.254 | 0.000 | 0.731 | 0.123 | 0.857 | 0.077 | 0.255 | 0.001 |
| T5-Small | 0.612 | 0.171 | 0.785 | 0.116 | 0.236 | 0.002 | 0.668 | 0.227 | 0.818 | 0.149 | 0.236 | 0.002 |
| T5-Base | 0.615 | 0.099 | 0.786 | 0.066 | 0.252 | -0.001 | 0.669 | 0.153 | 0.819 | 0.099 | 0.253 | 0.000 |
| Flan-T5-S | 0.539 | 0.127 | 0.735 | 0.088 | 0.236 | -0.001 | 0.579 | 0.167 | 0.803 | 0.156 | 0.251 | 0.014 |
| Flan-T5-B | 0.694 | 0.097 | 0.834 | 0.060 | 0.248 | 0.001 | 0.741 | 0.144 | 0.861 | 0.087 | 0.248 | 0.001 |
| Flan-T5-L | 0.732 | 0.248 | 0.866 | 0.170 | 0.243 | -0.005 | 0.875 | 0.391 | 0.937 | 0.241 | 0.244 | -0.004 |
| Flan-T5-XL | 0.769 | 0.158 | 0.888 | 0.105 | 0.250 | -0.012 | 0.900 | 0.289 | 0.950 | 0.167 | 0.248 | -0.014 |
| Reference | 0.697 |  | 0.836 |  |  |  | 0.697 |  | 0.836 |  |  |  |</p>
<p>Table 8: Movie Reviews: Generate movie meta-reviews using standard beam search, then select using approximate (left) or oracle (right) target sentiments.
the diverse generations produced higher correlations with human sentiment, and improved overall classification and abstention behaviors. Both settings have some decay in overall (crude) measures of review quality - Tables 7, 8 show small decreases in ROUGE-1 score; furthermore the diverse beam search results produce overall higher quality results ( $\mathrm{R}^{2}, \mathrm{PCC}$ ), but how larger changes in ROUGE1 compared to a standard beam search method. Systematic Reviews behave similarly (Tables 9, 10), with an increase in F1 (or accuracy) comes with higher variability in ROUGE1 scores and a substantial amounts of abstention.</p>
<h2>6 Related Work</h2>
<p>Automatic (multi-document) summarization (Nenkova and McKeown, 2011; Maybury, 1999) has been an active subfield within NLP for decades. We have focused our analysis on modern, neural abstractive models for conditional text generation (Bahdanau et al., 2015). In light of their empirical success, we have specifically evaluated a set of Transformer-based (Vaswani et al., 2017) models which have recently been used for multidocument summarization (Beltagy et al., 2020; Zhang et al., 2020; Xiao et al., 2022; Raffel et al., 2020). There has been some work on highlighting conflicting evidence in health literature specifically (Shah et al., 2021b,a), though this focused primarily on highlighting conflicting evidence and explicitly aggregating extracted content.</p>
<p>Multiple works have attempted gauge the difficulty of multi-document summarization. Wolhandler et al. (2022) measures the difficulty of abstractive multi-document news summarization as
a function of inputs necessary to produce a final summary; they find that two to four well-chosen documents can cover a news topic sufficiently for the summarizer. They also find systematic reviews are particularly ill-suited to this minimal covering approach. Giorgi et al. (2022) studies the impact of document retrieval behaviors on multidocument summarization performance, and find that models are sensitive to missing inputs.</p>
<p>Sentence fusion One view on synthesis might be that is a particular kind of sentence fusion (Barzilay and McKeown, 2005). However, past work on "fusing" sentences has assumed that the aim is to generate an output that contains the information common to similar sentences (Thadani and McKeown, 2013). This is intuitive in the context of, e.g., summarizing multiple news articles covering the same event. But here we are interested in the more challenging setting in which the output should reflect an aggregate measure of potentially conflicting evidence or opinions.</p>
<p>Review and opinion summarization considers a similar task to ours: Aggregating (usually product) reviews and opinions into a single coherent text. Oved and Levy (2021) developed a system with a similar generate-then-select approach, however this work was focused on generating plausible summaries rather than accurate syntheses, by selecting amongst candidates via a voting mechanism designed to mimic human preferences. Other related work has considered generating personalized and/or aspect-oriented summaries (He et al., 2017; Angelidis and Lapata, 2018; Amplayo and Lapata, 2020, 2021; Amplayo et al., 2021; Angelidis et al., 2021). Amplayo and Lapata (2021)</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Multiple-then-select</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Oracle</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">Abs</td>
<td style="text-align: center;">R1</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">Abs</td>
<td style="text-align: center;">R1</td>
<td style="text-align: center;">$\Delta$</td>
</tr>
<tr>
<td style="text-align: center;">AceSum</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.030</td>
<td style="text-align: center;">0.573</td>
<td style="text-align: center;">0.023</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">0.154</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.133</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">0.001</td>
</tr>
<tr>
<td style="text-align: center;">REFLECT ${ }^{\text {MLE }}$</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.056</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">$-0.013$</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;">0.150</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.007</td>
</tr>
<tr>
<td style="text-align: center;">REFLECT ${ }^{\text {RL }}$</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.100</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">0.430</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">0.008</td>
</tr>
<tr>
<td style="text-align: center;">Pegasus</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.065</td>
<td style="text-align: center;">0.676</td>
<td style="text-align: center;">$-0.038$</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.216</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.216</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.008</td>
</tr>
<tr>
<td style="text-align: center;">LED</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.135</td>
<td style="text-align: center;">0.698</td>
<td style="text-align: center;">0.067</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">$-0.009$</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.257</td>
<td style="text-align: center;">$-0.002$</td>
</tr>
<tr>
<td style="text-align: center;">PRIMERA</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.091</td>
<td style="text-align: center;">0.663</td>
<td style="text-align: center;">0.019</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">$-0.002$</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">$-0.003$</td>
</tr>
<tr>
<td style="text-align: center;">T5-Small</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;">0.052</td>
<td style="text-align: center;">0.627</td>
<td style="text-align: center;">0.027</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">$-0.012$</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.190</td>
<td style="text-align: center;">$-0.015$</td>
</tr>
<tr>
<td style="text-align: center;">T5-Base</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.087</td>
<td style="text-align: center;">0.671</td>
<td style="text-align: center;">0.043</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.202</td>
<td style="text-align: center;">$-0.004$</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.004</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-S</td>
<td style="text-align: center;">0.579</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">0.138</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">0.119</td>
<td style="text-align: center;">0.205</td>
<td style="text-align: center;">0.124</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-B</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.040</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.177</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.028</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-L</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.054</td>
<td style="text-align: center;">0.663</td>
<td style="text-align: center;">$-0.029$</td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.065</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">$-0.012$</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-XL</td>
<td style="text-align: center;">0.618</td>
<td style="text-align: center;">0.131</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.059</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.007</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.025</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">$-0.004$</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">0.577</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.686</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 9: Systematic Review results with multiple-then-selected predictions. We report macro-averaged F1 on the set of returned results. We abstain (Abs) when no output matches the expected synthesis result.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Multiple-then-select</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Oracle</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">Abs</td>
<td style="text-align: center;">R1</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">Abs</td>
<td style="text-align: center;">R1</td>
<td style="text-align: center;">$\Delta$</td>
</tr>
<tr>
<td style="text-align: center;">AceSum</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.046</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.038</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">0.006</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">$-0.002$</td>
</tr>
<tr>
<td style="text-align: center;">REFLECT ${ }^{\text {MLE }}$</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">0.099</td>
<td style="text-align: center;">0.706</td>
<td style="text-align: center;">0.067</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.006</td>
</tr>
<tr>
<td style="text-align: center;">REFLECT ${ }^{\text {RL }}$</td>
<td style="text-align: center;">0.603</td>
<td style="text-align: center;">0.098</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">0.070</td>
<td style="text-align: center;">0.483</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">$-0.011$</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.002</td>
</tr>
<tr>
<td style="text-align: center;">Pegasus</td>
<td style="text-align: center;">0.688</td>
<td style="text-align: center;">0.120</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.060</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">$-0.004$</td>
<td style="text-align: center;">0.258</td>
<td style="text-align: center;">0.216</td>
<td style="text-align: center;">0.004</td>
</tr>
<tr>
<td style="text-align: center;">LED</td>
<td style="text-align: center;">0.582</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.099</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.341</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.002</td>
</tr>
<tr>
<td style="text-align: center;">PRIMERA</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.099</td>
<td style="text-align: center;">0.704</td>
<td style="text-align: center;">0.060</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.006</td>
<td style="text-align: center;">0.313</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">$-0.003$</td>
</tr>
<tr>
<td style="text-align: center;">T5-Small</td>
<td style="text-align: center;">0.603</td>
<td style="text-align: center;">0.063</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.033</td>
<td style="text-align: center;">0.258</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">$-0.001$</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">$-0.004$</td>
</tr>
<tr>
<td style="text-align: center;">T5-Base</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.064</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.005</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-S</td>
<td style="text-align: center;">0.603</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">0.632</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.080</td>
<td style="text-align: center;">$-0.001$</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-B</td>
<td style="text-align: center;">0.637</td>
<td style="text-align: center;">0.099</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.078</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.195</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.004</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-L</td>
<td style="text-align: center;">0.673</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.079</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.177</td>
<td style="text-align: center;">$-0.041$</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.174</td>
<td style="text-align: center;">$-0.044$</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-XL</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.107</td>
<td style="text-align: center;">0.665</td>
<td style="text-align: center;">0.057</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.001</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">0.577</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.686</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 10: Systematic reviews results with multiple generate-then-select predictions, this time using the top- 5 results from standard beam-search.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Differences relative to human summaries under vanilla decoding and the proposed generatediverse then select strategy on movie meta-reviews. We report Pearson's r (PCC) and R² as measures of synthesis "calibration". Vanilla decoding yields synthesis performance worse than humans, but explicitly considering synthesis at inference time results in performance comparable to and sometimes better than the human summaries (as best we can measure).</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Distributions of outputs for the candidate summaries. <strong>Movie reviews</strong> (left) show a histogram for the range of differences between lowest and highest output sentiments. <strong>Systematic reviews</strong> (right) show histograms of the fractions of outputs reporting <em>significant</em> results.</p>
<table>
<thead>
<tr>
<th>Summary</th>
<th>Sent.</th>
</tr>
</thead>
<tbody>
<tr>
<td>You Don't Mess With the Zohan's handful of laughs are almost enough to compensate for its inconsistent tone and stale, obvious jokes.</td>
<td>0.243</td>
</tr>
<tr>
<td>You Don't Mess with the Zohan has a handful of crotch thrusts, but not enough of them land.</td>
<td>0.429</td>
</tr>
<tr>
<td>You Don't Mess With the Zohan's handful of laughs are almost enough to compensate for its aimless, crass script.</td>
<td>0.288</td>
</tr>
<tr>
<td>You Don't Mess with the Zohan has its moments, but not all of them – and the jokes are embarrassingly crass and often crude.</td>
<td>0.434</td>
</tr>
<tr>
<td>You Don't Mess with the Zohan has its moments, but not all of them – and the jokes are embarrassingly crass and often crude. The script</td>
<td>0.406</td>
</tr>
</tbody>
</table>
<p>Table 11: Diverse meta-review generations and automatically inferred sentiment scores for "You Don't Mess With The Zohan". Target meta-review sentiment of 37%: We <strong>bold</strong> the closest generation in terms of (inferred) sentiment.</p>
<p>propose a T5 variant for pooling instance representations, and also use Rotten Tomatoes as a dataset. This work (and Amplayo et al., 2021) includes a manual evaluation of how well system summaries are <em>supported</em> by input reviews, in contrast to how well a summary agrees with <em>all</em> inputs in the precise sense we have considered. We note that none of these prior works directly <em>probe</em> model responsiveness to changes in input composition.</p>
<p>Also related is the work of Chu and Liu (2019), which considered <em>unsupervised</em> approaches to multi-document summarization of Yelp! and Amazon reviews; they adopt an auto-encoder that "decodes" the mean of input representations to target summaries. They similarly note that output texts should convey mean input sentiment, and report "sentiment accuracy" as one of their metrics. But the synthesis aspect is not their main focus, and they consider only unsupervised settings (rather than the SOTA fine-tuned summarization models we have evaluated).</p>
<p><strong>Interpretation and analysis of neural models for NLP</strong> This work is also related to the emerging body of work on analyzing neural NLP models, their behaviors, "knowledge", and "abilities".</p>
<p>in general, e.g., (Linzen et al., 2016; Tenney et al., 2019; Petroni et al., 2019; Niven and Kao, 2019; Meng et al., 2022). There has been some work specifically on analyzing neural summarization models. Xu et al. (2020a) investigated when a model is likely to copy rather than generate. Xu and Durrett (2021) assessed when models were relying on the local input to produce particular output tokens, and when they instead rely mostly on a background language distribution acquired in pretraining. In contrast to Giorgi et al. (2022) we explore beyond surface forms and explore the specific aspect of text synthesis.</p>
<p>Factuality of neural summarizers Neural conditional generation models have proven adept at producing fluent outputs, but when summarizing they are prone to hallucinating content unsupported by input documents (Maynez et al., 2020; Kryscinski et al., 2019). Automated metrics such as ROUGE do not reliably capture such phenomena (Falke et al., 2019; Maynez et al., 2020). This has motivated the design of automated factuality metrics, e.g., (Wang et al., 2020; Xu et al., 2020b); see Pagnoni et al. (2021) for an overview.</p>
<h2>7 Conclusions</h2>
<p>We have outlined and investigated the problem of synthesis as related to some summarization tasks. We showed that existing models are partially able to synthesize implicitly, but do so imperfectly: the aggregation they perform is sensitive to input ordering, and they are not as sensitive to perturbations in the composition of inputs as one would hope. Some models specifically designed for these tasks (AceSum, QT, REFLECT) are less sensitive to these perturbations, but offer worse overall performance than an equivalently sized transformer model (compare LED and REFLECT - REFLECT integrates a model with the same base LLM parameters as a portion of its synthesis model). Furthermore, increasing model size within an architecture can lead to fairly substantial improvements (LED to PRIMERA, T5 Small to Base, similarly for Flan-T5). Pretraining methods have some impact as well: T5 and Flan-T5 do not perform identically despite an identical model structure and comparable sizes, and GPT-4 clearly outperforms all models in this case, including the bespoke ones.</p>
<p>We proposed and validated a straightforward inference time method to improve model synthesis capabilities by preferentially outputting summary candidates that align with a predicted aggregate measure, and demonstrated empirically that this offers gains in performance. These gains are primarily limited by the underlying models' behaviors, but potentially bring performance on these single, task-specific metrics, on par to human performance, when the model is capable of providing a response that aligns with the proxy metrics.</p>
<p>We hope this work encourages additional research into summarization models that explicitly optimize to accurately synthesize potentially conflicting evidence. We are particularly interested in understanding why models fail to synthesize they clearly learn to produce synthesis-like text, but fail to yield the best option, even among their top candidates. We use summary reranking as a means to surface these more-appropriate summaries, but this is solely post-hoc as opposed to controlling for a more suitable generation, or ideally improving base model performance.</p>
<p>Our methods focus solely on improving performance at single specific task measures, potentially at a cost to other review qualities. Users of such systems may have auxiliary goals, perhaps requiring multiple measures of synthesis quality, other measures of overall review quality, or a greater (or lesser) willingness to abstain. Abstinence can be a feature beyond the case of systematic reviews; systems may have other specific rules for when to abstain: e.g. toxic language, challenging to verify statements, or distance from an overall objective (i.e. abstaining in the movie reviews case).</p>
<p>This work has several limitations. We have made an effort to fine-tune several popular summarization models, but limited our analysis to models of relatively modest size (due to the GPU memory required to train long sequence summarization models). These behaviors appear to change with larger models (e.g. the small vs basesized models, GPT-4 (OpenAI, 2023)), but building robustness to perturbations while maintaining sensitivity to input composition is a non-obvious challenge. We also have reported results on only English-language tasks. Finally, we focused on a relatively narrow behavior (synthesis of a single aspect); models may succeed in this respect while failing in other ways.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by the National Science Foundation (NSF) RI 2211954, and by the National Institutes of Health (NIH) under the National Library of Medicine (NLM), R01-LM012086-01A1. We thank the Northeastern University Research Computing Discovery Cluster. We thank the anonymous reviewers for their feedback.</p>
<h2>References</h2>
<p>Reinald Kim Amplayo, Stefanos Angelidis, and Mirella Lapata. 2020. Unsupervised opinion summarization with content planning. In AAAI Conference on Artificial Intelligence.</p>
<p>Reinald Kim Amplayo, Stefanos Angelidis, and Mirella Lapata. 2021. Aspect-controllable opinion summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6578-6593, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Reinald Kim Amplayo and Mirella Lapata. 2020. Unsupervised opinion summarization with noising and denoising. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1934-1945, Online. Association for Computational Linguistics.</p>
<p>Reinald Kim Amplayo and Mirella Lapata. 2021. Informative and controllable opinion summarization. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2662-2672, Online. Association for Computational Linguistics.</p>
<p>Stefanos Angelidis, Reinald Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata. 2021. Extractive opinion summarization in quantized transformer spaces. Transactions of the Association for Computational Linguistics, 9:277-293.</p>
<p>Stefanos Angelidis and Mirella Lapata. 2018. Summarizing opinions: Aspect extraction meets sentiment prediction and they are both weakly supervised. In Proceedings of the 2018 Conference on Empirical Methods in Natural</p>
<p>Language Processing, pages 3675-3686, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations (ICLR).</p>
<p>Brooks Barnes. 2017. Attacked by rotten tomatoes. The New York Times. Accessed 27 April 2023.</p>
<p>Regina Barzilay and Kathleen R. McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297-328.</p>
<p>Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. ArXiv, abs/2004.05150.</p>
<p>Eric Chu and Peter Liu. 2019. MeanSum: A neural model for unsupervised multi-document abstractive summarization. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 1223-1232. PMLR.</p>
<p>Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416.</p>
<p>Hoa Trang Dang. 2005. Overview of duc 2005. In Document Understand Conference.</p>
<p>Hoa Trang Dang. 2006. Overview of DUC. In In Proceedings of HLT-NAACL.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Jay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, and Lucy Lu Wang. 2021. MS"2: Multi-document summarization of medical studies. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7494-7513, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Jay DeYoung, Eric Lehman, Benjamin Nye, Iain Marshall, and Byron C. Wallace. 2020. Evidence inference 2.0: More data, better models. In Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing, pages 123-132, Online. Association for Computational Linguistics.</p>
<p>David Kirk Evans, Judith L. Klavans, and Kathleen R. McKeown. 2004. Columbia newsblaster: Multilingual news summarization on the web. In Demonstration Papers at HLT-NAACL 2004, pages 1-4, Boston, Massachusetts, USA. Association for Computational Linguistics.</p>
<p>Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. 2019. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1074-1084, Florence, Italy. Association for Computational Linguistics.</p>
<p>Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2214-2220, Florence, Italy. Association for Computational Linguistics.</p>
<p>César Ferri, Peter Flach, and José HernándezOrallo. 2004. Delegating classifiers. In Proceedings of the twenty-first international conference on Machine learning, page 37.</p>
<p>Demian Gholipour Ghalandari, Chris Hokamp, Nghia The Pham, John Glover, and Georgiana Ifrim. 2020. A large-scale multi-document summarization dataset from the Wikipedia current events portal. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1302-1308, Online. Association for Computational Linguistics.</p>
<p>John Giorgi, Luca Soldaini, Bo Wang, Gary Bader, Kyle Lo, Lucy Lu Wang, and Arman Cohan. 2022. Exploring the challenges of open domain multi-document summarization. ArXiv, abs/2212.10526.</p>
<p>Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2017. An unsupervised neural attention model for aspect extraction. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 388-397, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Yotam Hechtlinger, Barnabás Póczos, and Larry Wasserman. 2018. Cautious deep learning. arXiv preprint arXiv:1805.09460.</p>
<p>Tom Hosking, Hao Tang, and Mirella Lapata. 2023. Attributable and scalable opinion summarization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8488-8505, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Gregory Kell, Iain Marshall, Byron Wallace, and Andre Jaun. 2021. What would it take to get biomedical QA systems into practice? In Proceedings of the 3rd Workshop on Machine Reading for Question Answering, pages 28-41, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR, abs/1412.6980.</p>
<p>Wojciech Kryscinski, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard</p>
<p>Socher. 2019. Neural text summarization: A critical evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 540-551, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346, Online. Association for Computational Linguistics.</p>
<p>Stefano Leone. 2020. Rotten tomatoes movies and critic reviews dataset. kaggle. com.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn syntax-sensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521-535.</p>
<p>Peter J. Liu<em>, Mohammad Saleh</em>, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations.</p>
<p>Yang Liu and Mirella Lapata. 2019. Hierarchical transformers for multi-document summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5070-5081, Florence, Italy. Association for Computational Linguistics.</p>
<p>Yao Lu, Yue Dong, and Laurent Charlin. 2020. Multi-XScience: A large-scale dataset for extreme multi-document summarization of scientific articles. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8068-8074, Online. Association for Computational Linguistics.</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142-150, Portland, Oregon, USA. Association for Computational Linguistics.</p>
<p>Iain Marshall, Joël Kuiper, Edward Banner, and Byron C. Wallace. 2017. Automating biomedical evidence synthesis: RobotReviewer. In Proceedings of ACL 2017, System Demonstrations, pages 7-12, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Mani Maybury. 1999. Advances in automatic text summarization. MIT press.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics.</p>
<p>Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 36. ArXiv:2202.05262.</p>
<p>Diego Mollá and María Elena Santiago-Martínez. 2012. Creation of a corpus for evidence based medicine summarisation. The Australasian medical journal, 5(9):503.</p>
<p>Feng Nan, Ramesh Nallapati, Zhiguo Wang, Cicero Nogueira dos Santos, Henghui Zhu, Dejiao Zhang, Kathleen McKeown, and Bing Xiang. 2021a. Entity-level factual consistency of abstractive text summarization. In Proceedings of the 16th Conference of the European Chapter of</p>
<p>the Association for Computational Linguistics: Main Volume, pages 2727-2733, Online. Association for Computational Linguistics.</p>
<p>Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, and Bing Xiang. 2021b. Improving factual consistency of abstractive summarization via question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6881-6894, Online. Association for Computational Linguistics.</p>
<p>Ani Nenkova and Kathleen McKeown. 2011. Automatic summarization. Now Publishers Inc.</p>
<p>Timothy Niven and Hung-Yu Kao. 2019. Probing neural network comprehension of natural language arguments. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658-4664, Florence, Italy. Association for Computational Linguistics.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Nadav Oved and Ran Levy. 2021. PASS: Perturb-and-select summarizer for product reviews. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 351-365, Online. Association for Computational Linguistics.</p>
<p>Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4812-4829, Online. Association for Computational Linguistics.</p>
<p>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics
(ACL'05), pages 115-124, Ann Arbor, Michigan. Association for Computational Linguistics.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1).</p>
<p>Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1179-1195.</p>
<p>Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021. QuestEval: Summarization asks for fact-based evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6594-6604, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Darsh Shah, Lili Yu, Tao Lei, and Regina Barzilay. 2021a. Nutri-bullets hybrid: Consensual multi-document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5213-5222, Online. Association for Computational Linguistics.</p>
<p>Darsh J Shah, Lili Yu, Tao Lei, and Regina Barzilay. 2021b. Nutri-bullets: Summarizing health studies by composing segments. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13780-13788.</p>
<p>Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear</p>
<p>memory cost. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4596-4604. PMLR.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics.</p>
<p>Yun-Zhu Song, Yi-Syuan Chen, and Hong-Han Shuai. 2022. Improving multi-document summarization through referenced flexible extraction with credit-awareness. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1667-1681, Seattle, United States. Association for Computational Linguistics.</p>
<p>Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn from context? probing for sentence structure in contextualized word representations. In International Conference on Learning Representations.</p>
<p>Kapil Thadani and Kathleen McKeown. 2013. Supervised sentence fusion with single-stage inference. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 1410-1418.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.</p>
<p>Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. 2016. Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424.</p>
<p>Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Advances
in Neural Information Processing Systems, volume 28. Curran Associates, Inc.</p>
<p>Byron C. Wallace, Sayantan Saha, Frank Soboczenski, and Iain J. Marshall. 2021. Generating (Factual?) Narrative Summaries of RCTs: Experiments with Neural MultiDocument Summarization. In Proceedings of AMIA Informatics Summit.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008-5020, Online. Association for Computational Linguistics.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Ruben Wolhandler, Arie Cattan, Ori Ernst, and Ido Dagan. 2022. How "multi" is multi-document summarization? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5761-5769, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Wen Xiao, Iz Beltagy, Giuseppe Carenini, and Arman Cohan. 2022. PRIMERA: Pyramidbased masked sentence pre-training for multidocument summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5245-5263, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Jiacheng Xu, Shrey Desai, and Greg Durrett. 2020a. Understanding neural abstractive summarization models via uncertainty. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</p>
<p>(EMNLP), pages 6275-6281, Online. Association for Computational Linguistics.</p>
<p>Jiacheng Xu and Greg Durrett. 2021. Dissecting generation modes for abstractive summarization models via ablation and attribution. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6925-6940, Online. Association for Computational Linguistics.</p>
<p>Xinnuo Xu, Ondřej Dušek, Jingyi Li, Verena Rieser, and Ioannis Konstas. 2020b. Factbased content weighting for evaluating abstractive summarisation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5071-5081, Online. Association for Computational Linguistics.</p>
<p>Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 11328-11339. PMLR.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{14}$ In fixed effects meta-analysis the weights are inverse variances associated with study-level effect estimates.
${ }^{15}$ Oved and Levy (2021) explore a related generate-thenselect approach for creating plausible product reviews. We experimented with an additional decoding method: constrain beam search by restricting candidate productions $p_{\theta}\left(y_{i, t} \mid y_{i, 1 . . t-1}, x_{i}^{0}\right)$ such that the target attribute $z_{i}$ is less than some $\epsilon:\left|g\left(y_{i, 1, \ldots, t}\right)-z_{i}\right|&lt;\epsilon$. We elide these results here as they were often disfluent.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{16}$ This penalty requires a hyperparameter $\lambda$ that encodes the relative importance of diversity; we use $\lambda=0.5$. To enable fair comparison with standard beam search ( 5 beams, in all experiments), we used 5 groups, 1 beam per group. We exclude QT as it is an extractive model, and PlanSum as it does not readily support diverse beach search. For AceSum and REFLECT we modify these codebases to use the diverse beam search implementation from HuggingFace. For GPT-4 we sample five responses with a temperature of 0.6 .
${ }^{17}$ https://huggingface.co/lvwerra/
bert-imdb&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
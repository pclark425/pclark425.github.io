<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2556 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2556</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2556</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-5324e982deeb2e3860e7c09e34341096df0a269f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5324e982deeb2e3860e7c09e34341096df0a269f" target="_blank">Self-Evolving Multi-Agent Collaboration Networks for Software Development</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> EvoMAC, a novel self-evolving paradigm for MAC networks, Inspired by traditional neural network training, is introduced, which obtains text-based environmental feedback by verifying the MAC network's output against a target proxy and leverages a novel textual backpropagation to update the network.</p>
                <p><strong>Paper Abstract:</strong> LLM-driven multi-agent collaboration (MAC) systems have demonstrated impressive capabilities in automatic software development at the function level. However, their heavy reliance on human design limits their adaptability to the diverse demands of real-world software development. To address this limitation, we introduce EvoMAC, a novel self-evolving paradigm for MAC networks. Inspired by traditional neural network training, EvoMAC obtains text-based environmental feedback by verifying the MAC network's output against a target proxy and leverages a novel textual backpropagation to update the network. To extend coding capabilities beyond function-level tasks to more challenging software-level development, we further propose rSDE-Bench, a requirement-oriented software development benchmark, which features complex and diverse software requirements along with automatic evaluation of requirement correctness. Our experiments show that: i) The automatic requirement-aware evaluation in rSDE-Bench closely aligns with human evaluations, validating its reliability as a software-level coding benchmark. ii) EvoMAC outperforms previous SOTA methods on both the software-level rSDE-Bench and the function-level HumanEval benchmarks, reflecting its superior coding capabilities. The benchmark can be downloaded at https://yuzhu-cai.github.io/rSDE-Bench/.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2556.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2556.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EvoMAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EvoMAC: Self-Evolving Multi-Agent Collaboration Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-evolving multi-agent collaboration (MAC) network for software development that iteratively adapts agents and their interconnections at test time via a textual backpropagation loop using objective environment feedback (unit tests executed in a compiler).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>EvoMAC</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>EvoMAC is a MAC network implemented as a directed acyclic graph of LLM-driven agents that (1) initializes a coding team via a self-organizing coding organizer, (2) generates unit-test target proxies via a testing team, (3) executes generated code in an objective environment (compiler) to get textual execution logs, and (4) runs a textual backpropagation loop managed by two updating agents (a gradient agent that produces textual gradients and an updating agent that revises the MAC network). The loop (feed-forward -> execute -> textual gradient -> update) repeats for K iterations to evolve agent prompts and workflows until the generated software better satisfies the unit tests (target proxy).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (dynamic per task; composed of a coding organizer + multiple coding agents + testing organizer + multiple testing agents + gradient agent + updating agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Coding organizer (decomposes requirements and configures coding agents); Coding agents (each implements a subtask/function according to assigned prompt); Testing organizer (decomposes test criteria and configures testing agents); Testing agents (generate unit test cases / black-box test code aligned to requirements); Gradient agent (analyzes execution logs and synthesizes textual gradients describing which agent subtasks succeeded, erred, or are missing); Updating agent (applies textual gradient suggestions: revises agent prompts, removes completed agents, adds agents for missing subtasks, and restructures workflow).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Requirement analysis (organizer agents), implementation (coding agents generate code), testing/execution (testing agents produce target proxy; compiler executes tests), evaluation/feedback (compiler logs interpreted as environment feedback), iterative refinement (textual backpropagation), deployment not explicitly covered.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Hierarchical/sequential agentic workflow encoded as a directed acyclic graph (DAG). The coding workflow is a sequential feed-forward pipeline (each agent receives global requirement context and previous agent output). Evolution is coordinated centrally by the updating team: the gradient agent analyzes environment feedback and the updating agent modifies the DAG (agents/prompts/edges).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language prompts and structured textual messages exchanged between agents and orchestrators; code and test-case artifacts are passed as text/code blocks; unit tests are passed to the compiler and execution logs (textual) are fed back. Agent instructions and updates are enacted via LLM prompt-based messages (no specialized binary protocol).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Objective environment executor (compiler) acts as the feedback source by running unit tests generated by the testing team; execution logs provide textual feedback used as a 'textual loss.' The gradient agent converts logs into textual gradients indicating success/failure per agent subtask; the updating agent uses that analysis to change prompts and workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Iterative: agents operate in a feed-forward pass to produce code, then testing agents produce tests and the compiler executes them — after each forward pass the gradient/update pair run once; this cycle repeats for K evolving iterations (on-demand per iteration, i.e., after each generation).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software development (requirement-oriented software generation; also evaluated on function-level HumanEval), generalizable to other generation tasks where objective target proxies and objective execution environments exist.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy on rSDE-Bench (requirement-aware pass rates) and pass@1 on HumanEval. Reported EvoMAC results (powered by GPT-4o-Mini): Website Basic 89.38% (±11.01), Website Advanced 65.05% (±11.56), Game Basic 77.54% (±22.04), Game Advanced 51.60% (±4.54); HumanEval pass@1 = 94.51%. Improvements reported: +26.48% on Website Basic, +20.65% Website Advanced, +34.78% Game Basic, +21.50% Game Advanced, +6.10% on HumanEval (relative to best baselines in table).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>EvoMAC was compared against five multi-agent methods (MetaGPT, Autogen, MapCoder, Agentverse, ChatDev) and three single-agent LLMs (GPT-4o-Mini, Claude-3.5-Sonnet, Gemini-1.5-Flash). Across rSDE-Bench and HumanEval, EvoMAC substantially outperformed all baselines (numerical baseline values are in the paper's Table 1). Example baseline numbers (GPT-4o-Mini single-agent): Website Basic 62.90%, Website Advanced 44.40%, Game Basic 42.76%, Game Advanced 30.10%, HumanEval 88.41%; ChatDev multi-agent: Website Basic 62.67%, Website Advanced 43.45%, Game Basic 53.63%, Game Advanced 32.26%, HumanEval 70.73%.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Improved requirement-fulfillment accuracy and code correctness via iterative, objective, environment-driven feedback; quantified large gains over baselines (e.g., +26.48% Website Basic, +34.78% Game Basic, +6.10% HumanEval pass@1). Self-evolution addresses missed subtasks and persistent errors by revising agents and workflows, leading to steady improvement across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>More complex, persistent logical errors (e.g., game logic) are harder to resolve and lead to diminishing returns with more iterations; reliance on high-quality unit tests and objective environment — if testing proxy or environment is insufficient, evolution may be limited. The approach requires careful prompt design for gradient and updating agents to maintain coherence. Communication and iteration add computational / API-call overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablations (Table 2) show: removing objective environment feedback (i.e., using LLM critique instead of compiler logs) causes large drops (compare variant g with f): Website Basic drops from 90.75% to 78.08% (-12.67%), Website Advanced 67.20% to 52.23% (-14.97%), Game Basic 77.54% to 55.80% (-21.74%), Game Advanced 51.60% to 33.32% (-18.28%). Reducing coding/testing teams from multi-agent to single-agent also degrades performance (drops of ~5–7% on Website settings). EvoMAC performance improves with more evolving iterations and is robust across driving LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Paper findings suggest: (1) use multi-agent coding and testing teams (multi-agent coding + multi-agent testing gives best results), (2) include an objective environment executor (compiler + unit tests) for feedback, and (3) run multiple evolving iterations (performance improves with more iterations). Using a stronger base LLM for agents yields universally better curves (no cross-over between LLM variants). Exact numeric configuration (number of agents) is task-dependent and initialized via the coding/testing organizers (self-organizing limit: keep composition ≤ specified max agents).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evolving Multi-Agent Collaboration Networks for Software Development', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2556.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2556.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAC network</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Collaboration (MAC) Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general computational graph representation of LLM-driven agentic workflows where agents are nodes and directed edges encode task dependencies; used as the conceptual substrate for LLM-based multi-agent systems and for EvoMAC's design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MAC network (general)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A MAC network is represented as a directed acyclic graph A=(V,E) where each node v_i is an autonomous agent with a prompt p_i specifying a subtask, and edges e_{i,j} indicate that agent j executes after agent i. Feed-forward execution (Φ) runs the workflow: each agent receives the global task description plus previous agent outputs and emits its subtask output, with the final agent output forming the system output.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>N (variable; arbitrary number of agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Generic: any subtask-specific agent prompt; in EvoMAC instantiations include organizers, implementers, testers, evaluators, and updating agents.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Problem decomposition, subtask implementation, result aggregation, can be extended to testing and evaluation when paired with test-generation agents.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Sequential pipelined execution defined by DAG topology (human-designed or auto-organized); agents execute in topological order.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language prompts and textual outputs passed between agents; initial task description is provided as context to all agents.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Often relies on external critique agents or environment executors; EvoMAC augments MAC with objective environment feedback and textual backpropagation.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Per feed-forward step (agents exchange messages in sequence during each forward pass); optional additional coordination during organizer and updating phases.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General generation and problem-solving tasks (examples in paper: software development, mathematics, embodied tasks, social simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Depends on instantiation; in EvoMAC instantiation performance measured as requirement-pass accuracy and HumanEval pass@1.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>MAC networks can outperform single-agent methods when well-designed, but static, human-designed MACs can be limited; EvoMAC demonstrates improved automatic adaptation over static MACs.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Division of complex tasks into manageable subtasks enabling structured reasoning and distributed generation; potential for emergent behaviors and improved structure in outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Human-crafted static workflows lack adaptability across diverse tasks and can be brittle; critique-based feedback can be subjective/hallucinatory; coordination overhead and prompt-design complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Paper-level ablations (in EvoMAC) show multi-agent coding/testing teams outperform single-agent variants and that environment-driven feedback significantly improves evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not universal; the paper promotes self-organized initializations (organizer agents) and iterative environment-driven updates as best practices rather than fixed agent counts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evolving Multi-Agent Collaboration Networks for Software Development', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2556.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2556.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework for automated software development that composes role-based LLM agents into programming workflows (used as a baseline in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Metagpt: Meta programming for multi-agent collaborative framework.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A human-designed multi-agent system (agentic workflow) that assigns fixed role prompts and a workflow to decompose coding tasks into subtasks; in this paper MetaGPT is used as a baseline multi-agent system powered by GPT-4o-Mini for fair comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (framework supports multiple role-based agents); paper uses MetaGPT as provided by its authors (exact number per task not specified here)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Role-based programming agents (e.g., PM/architect/developer/tester roles) as defined in MetaGPT's original design; paper does not detail MetaGPT internals beyond being a human-designed MAC.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Implementation (software planning and code generation) and basic testing workflows as implemented by the framework.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Human-designed standardized operating procedures (static agent prompts and workflows); hierarchical/role-based pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>LLM prompt-based natural-language message exchanges between role agents (paper does not specify structured protocol beyond standard LLM messages).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Primarily internal agent messages and role-based handoffs; lacks EvoMAC's environment-driven iterative textual backpropagation (paper emphasizes MetaGPT's reliance on heuristic static design).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Sequential pipeline during generation; no iterative environment-driven evolution per-task described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software development (used as a baseline on rSDE-Bench and HumanEval in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 1 when powered by GPT-4o-Mini: Website Basic 15.41%, Website Advanced 0.00%, Game Basic 16.67%, Game Advanced 0.00%, HumanEval 88.41%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>EvoMAC outperforms MetaGPT substantially on rSDE-Bench (e.g., EvoMAC Website Basic 89.38% vs MetaGPT 15.41%).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Structured role specialization may aid modularity and clarity in code generation workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Heavily reliant on manual design of workflows/prompts; poor adaptability to diverse, lengthy, or task-specific requirements per the paper's critique.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not performed specifically for MetaGPT in this paper; MetaGPT serves as a fixed baseline to evaluate EvoMAC's gains from self-evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evolving Multi-Agent Collaboration Networks for Software Development', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2556.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2556.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autogen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autogen</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent conversation framework that enables LLM-driven applications via multi-agent dialogue (used here as a baseline multi-agent system for code generation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autogen: Enabling next-gen llm applications via multi-agent conversation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Autogen</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Agentic multi-agent conversation system that organizes multiple LLM agents to collaborate through message passing; in this paper used as a baseline for software generation tasks and powered by GPT-4o-Mini.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (framework supports multiple conversational agents); exact per-task agent counts not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Conversational role agents (task decomposition, planning, implementation agents) as per Autogen paradigm; not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Task planning and generation phases (applied here to software generation and used to produce code in baseline experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Multi-agent conversational orchestration (human-designed conversation/control rules); static workflow in baseline experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language multi-turn conversation between agents (LLM messages).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Internal conversational critique / coordination; in baseline use it does not use EvoMAC's environment-driven textual backpropagation.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>On-demand during multi-turn agent conversation; not iteratively evolved by environment feedback in the baseline setup.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software development (baseline for rSDE-Bench experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 1 when powered by GPT-4o-Mini: Website Basic 25.68% ±4.14, Website Advanced 5.40% ±3.34, Game Basic 17.39% ±1.78, Game Advanced 0.00% ±0.00, HumanEval 85.36%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>EvoMAC outperforms Autogen substantially on rSDE-Bench (e.g., EvoMAC Website Basic 89.38% vs Autogen 25.68%).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Conversational multi-agent setups can coordinate planning and division of labor.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Still relies on heuristic/static conversation design and may lack objective environment-driven iterative correction.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not performed specifically for Autogen in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evolving Multi-Agent Collaboration Networks for Software Development', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2556.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2556.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MapCoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MapCoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent code-generation method intended for competitive problem solving; used as a baseline in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mapcoder: Multi-agent code generation for competitive problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MapCoder</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A multi-agent coding framework (cited and used as a baseline) that coordinates multiple agents for code generation; in the experiment MapCoder was powered by GPT-4o-Mini for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (framework allows multiple agents; exact number per task not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Role-based coding agents per MapCoder design (paper does not detail roles beyond general multi-agent coding).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Implementation (function/code generation); used as baseline on function- and software-level benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Agentic workflow (human-designed); specifics not elaborated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language messages/prompts between LLM agents; no special structured protocol described here.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Baseline usage in this paper does not include EvoMAC's iterative environment-driven textual backpropagation.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Sequential or conversational per MapCoder's design during generation; not iteratively evolved here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software development and coding (baseline on rSDE-Bench and HumanEval).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 1 numbers (GPT-4o-Mini): Website Basic 34.70% ±1.59, Website Advanced 14.57% ±0.66, Game Basic 29.71% ±6.72, Game Advanced 7.52% ±6.10, HumanEval 90.85%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>EvoMAC outperforms MapCoder on rSDE-Bench and HumanEval as reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Multi-agent decomposition helps in structured solution search for coding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Human-crafted static workflows limit per-task adaptability compared to EvoMAC.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not performed specifically for MapCoder in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evolving Multi-Agent Collaboration Networks for Software Development', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2556.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2556.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agentverse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agentverse</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework facilitating multi-agent collaboration and investigating emergent behaviors in LLM agent societies; included as a baseline in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Agentverse</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A multi-agent framework for collaborative tasks that encourages emergent behaviors among LLM agents; used as a multi-agent baseline for software generation tasks in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (framework supports multiple agents; not fixed in paper experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>General role-based agents as per Agentverse framework (paper does not enumerate roles in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Task decomposition and generation; used as baseline for code generation evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Human-defined multi-agent workflows; emergent behavior studied in original Agentverse work but not expanded here.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language prompts / messages between agents (LLM dialogues).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Internal agent interactions; baseline use lacks EvoMAC's external, objective environment-driven iterative updates.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Conversational/iterative during agent interaction, but not evolved via the compiler/test feedback loop in baseline experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software development (baseline for rSDE-Bench).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported (GPT-4o-Mini) Website Basic 15.41% ±0.00, Website Advanced 0.00% ±0.00, Game Basic 37.67% ±8.20, Game Advanced 16.13% ±4.55, HumanEval 90.85%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>EvoMAC shows superior performance compared to Agentverse on rSDE-Bench.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Potential for emergent collaboration patterns among agents.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Static designs and lack of environment-driven iterative update can limit task-specific improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not performed specifically for Agentverse in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evolving Multi-Agent Collaboration Networks for Software Development', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2556.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2556.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatDev</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatDev</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A communicative multi-agent system specifically targeted at software development that composes multiple LLM agents into a development pipeline; used as a strong baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatdev: Communicative agents for software development.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatDev</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A multi-agent framework that composes communicative agents to handle software development tasks; in this study ChatDev is used as a multi-agent baseline powered by GPT-4o-Mini for parity with EvoMAC.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (role-based agents within ChatDev; exact per-task numbers not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Communicative role agents (planning, development, testing roles) as per ChatDev design; specifics not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Requirement comprehension, planning, code generation, and some testing (framework-oriented), used as baseline for rSDE-Bench and HumanEval.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Human-defined communicative pipeline of agents exchanging messages; static workflow in baseline use.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language message passing using LLM prompts and replies.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Agent-to-agent dialogue and any internal testing steps as implemented in ChatDev; does not employ EvoMAC's textual backpropagation with an objective compiler environment in the baseline experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Multi-turn agent communication during generation; no environment-driven iterative evolution in baseline experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software development (baseline in rSDE-Bench experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported (GPT-4o-Mini) Website Basic 62.67% ±0.28, Website Advanced 43.45% ±0.77, Game Basic 53.63% ±5.70, Game Advanced 32.26% ±4.55, HumanEval 70.73%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>EvoMAC outperforms ChatDev on rSDE-Bench across contexts and shows higher final accuracy after evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Better task comprehension and structured code production than single-agent approaches in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Still limited by static design and lack of rigorous environment-based iterative correction; ChatDev's HumanEval score was lower in this paper's evaluation compared to EvoMAC.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not performed specifically for ChatDev in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evolving Multi-Agent Collaboration Networks for Software Development', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Metagpt: Meta programming for multi-agent collaborative framework. <em>(Rating: 2)</em></li>
                <li>Autogen: Enabling next-gen llm applications via multi-agent conversation <em>(Rating: 2)</em></li>
                <li>Mapcoder: Multi-agent code generation for competitive problem solving <em>(Rating: 2)</em></li>
                <li>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. <em>(Rating: 2)</em></li>
                <li>Chatdev: Communicative agents for software development. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2556",
    "paper_id": "paper-5324e982deeb2e3860e7c09e34341096df0a269f",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "EvoMAC",
            "name_full": "EvoMAC: Self-Evolving Multi-Agent Collaboration Network",
            "brief_description": "A self-evolving multi-agent collaboration (MAC) network for software development that iteratively adapts agents and their interconnections at test time via a textual backpropagation loop using objective environment feedback (unit tests executed in a compiler).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "EvoMAC",
            "system_description": "EvoMAC is a MAC network implemented as a directed acyclic graph of LLM-driven agents that (1) initializes a coding team via a self-organizing coding organizer, (2) generates unit-test target proxies via a testing team, (3) executes generated code in an objective environment (compiler) to get textual execution logs, and (4) runs a textual backpropagation loop managed by two updating agents (a gradient agent that produces textual gradients and an updating agent that revises the MAC network). The loop (feed-forward -&gt; execute -&gt; textual gradient -&gt; update) repeats for K iterations to evolve agent prompts and workflows until the generated software better satisfies the unit tests (target proxy).",
            "number_of_agents": "variable (dynamic per task; composed of a coding organizer + multiple coding agents + testing organizer + multiple testing agents + gradient agent + updating agent)",
            "agent_specializations": "Coding organizer (decomposes requirements and configures coding agents); Coding agents (each implements a subtask/function according to assigned prompt); Testing organizer (decomposes test criteria and configures testing agents); Testing agents (generate unit test cases / black-box test code aligned to requirements); Gradient agent (analyzes execution logs and synthesizes textual gradients describing which agent subtasks succeeded, erred, or are missing); Updating agent (applies textual gradient suggestions: revises agent prompts, removes completed agents, adds agents for missing subtasks, and restructures workflow).",
            "research_phases_covered": "Requirement analysis (organizer agents), implementation (coding agents generate code), testing/execution (testing agents produce target proxy; compiler executes tests), evaluation/feedback (compiler logs interpreted as environment feedback), iterative refinement (textual backpropagation), deployment not explicitly covered.",
            "coordination_mechanism": "Hierarchical/sequential agentic workflow encoded as a directed acyclic graph (DAG). The coding workflow is a sequential feed-forward pipeline (each agent receives global requirement context and previous agent output). Evolution is coordinated centrally by the updating team: the gradient agent analyzes environment feedback and the updating agent modifies the DAG (agents/prompts/edges).",
            "communication_protocol": "Natural-language prompts and structured textual messages exchanged between agents and orchestrators; code and test-case artifacts are passed as text/code blocks; unit tests are passed to the compiler and execution logs (textual) are fed back. Agent instructions and updates are enacted via LLM prompt-based messages (no specialized binary protocol).",
            "feedback_mechanism": "Objective environment executor (compiler) acts as the feedback source by running unit tests generated by the testing team; execution logs provide textual feedback used as a 'textual loss.' The gradient agent converts logs into textual gradients indicating success/failure per agent subtask; the updating agent uses that analysis to change prompts and workflow.",
            "communication_frequency": "Iterative: agents operate in a feed-forward pass to produce code, then testing agents produce tests and the compiler executes them — after each forward pass the gradient/update pair run once; this cycle repeats for K evolving iterations (on-demand per iteration, i.e., after each generation).",
            "task_domain": "Software development (requirement-oriented software generation; also evaluated on function-level HumanEval), generalizable to other generation tasks where objective target proxies and objective execution environments exist.",
            "performance_metrics": "Accuracy on rSDE-Bench (requirement-aware pass rates) and pass@1 on HumanEval. Reported EvoMAC results (powered by GPT-4o-Mini): Website Basic 89.38% (±11.01), Website Advanced 65.05% (±11.56), Game Basic 77.54% (±22.04), Game Advanced 51.60% (±4.54); HumanEval pass@1 = 94.51%. Improvements reported: +26.48% on Website Basic, +20.65% Website Advanced, +34.78% Game Basic, +21.50% Game Advanced, +6.10% on HumanEval (relative to best baselines in table).",
            "baseline_comparison": "EvoMAC was compared against five multi-agent methods (MetaGPT, Autogen, MapCoder, Agentverse, ChatDev) and three single-agent LLMs (GPT-4o-Mini, Claude-3.5-Sonnet, Gemini-1.5-Flash). Across rSDE-Bench and HumanEval, EvoMAC substantially outperformed all baselines (numerical baseline values are in the paper's Table 1). Example baseline numbers (GPT-4o-Mini single-agent): Website Basic 62.90%, Website Advanced 44.40%, Game Basic 42.76%, Game Advanced 30.10%, HumanEval 88.41%; ChatDev multi-agent: Website Basic 62.67%, Website Advanced 43.45%, Game Basic 53.63%, Game Advanced 32.26%, HumanEval 70.73%.",
            "coordination_benefits": "Improved requirement-fulfillment accuracy and code correctness via iterative, objective, environment-driven feedback; quantified large gains over baselines (e.g., +26.48% Website Basic, +34.78% Game Basic, +6.10% HumanEval pass@1). Self-evolution addresses missed subtasks and persistent errors by revising agents and workflows, leading to steady improvement across iterations.",
            "coordination_challenges": "More complex, persistent logical errors (e.g., game logic) are harder to resolve and lead to diminishing returns with more iterations; reliance on high-quality unit tests and objective environment — if testing proxy or environment is insufficient, evolution may be limited. The approach requires careful prompt design for gradient and updating agents to maintain coherence. Communication and iteration add computational / API-call overhead.",
            "ablation_studies": "Ablations (Table 2) show: removing objective environment feedback (i.e., using LLM critique instead of compiler logs) causes large drops (compare variant g with f): Website Basic drops from 90.75% to 78.08% (-12.67%), Website Advanced 67.20% to 52.23% (-14.97%), Game Basic 77.54% to 55.80% (-21.74%), Game Advanced 51.60% to 33.32% (-18.28%). Reducing coding/testing teams from multi-agent to single-agent also degrades performance (drops of ~5–7% on Website settings). EvoMAC performance improves with more evolving iterations and is robust across driving LLMs.",
            "optimal_configurations": "Paper findings suggest: (1) use multi-agent coding and testing teams (multi-agent coding + multi-agent testing gives best results), (2) include an objective environment executor (compiler + unit tests) for feedback, and (3) run multiple evolving iterations (performance improves with more iterations). Using a stronger base LLM for agents yields universally better curves (no cross-over between LLM variants). Exact numeric configuration (number of agents) is task-dependent and initialized via the coding/testing organizers (self-organizing limit: keep composition ≤ specified max agents).",
            "uuid": "e2556.0",
            "source_info": {
                "paper_title": "Self-Evolving Multi-Agent Collaboration Networks for Software Development",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MAC network",
            "name_full": "Multi-Agent Collaboration (MAC) Network",
            "brief_description": "A general computational graph representation of LLM-driven agentic workflows where agents are nodes and directed edges encode task dependencies; used as the conceptual substrate for LLM-based multi-agent systems and for EvoMAC's design.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "MAC network (general)",
            "system_description": "A MAC network is represented as a directed acyclic graph A=(V,E) where each node v_i is an autonomous agent with a prompt p_i specifying a subtask, and edges e_{i,j} indicate that agent j executes after agent i. Feed-forward execution (Φ) runs the workflow: each agent receives the global task description plus previous agent outputs and emits its subtask output, with the final agent output forming the system output.",
            "number_of_agents": "N (variable; arbitrary number of agents)",
            "agent_specializations": "Generic: any subtask-specific agent prompt; in EvoMAC instantiations include organizers, implementers, testers, evaluators, and updating agents.",
            "research_phases_covered": "Problem decomposition, subtask implementation, result aggregation, can be extended to testing and evaluation when paired with test-generation agents.",
            "coordination_mechanism": "Sequential pipelined execution defined by DAG topology (human-designed or auto-organized); agents execute in topological order.",
            "communication_protocol": "Natural-language prompts and textual outputs passed between agents; initial task description is provided as context to all agents.",
            "feedback_mechanism": "Often relies on external critique agents or environment executors; EvoMAC augments MAC with objective environment feedback and textual backpropagation.",
            "communication_frequency": "Per feed-forward step (agents exchange messages in sequence during each forward pass); optional additional coordination during organizer and updating phases.",
            "task_domain": "General generation and problem-solving tasks (examples in paper: software development, mathematics, embodied tasks, social simulation).",
            "performance_metrics": "Depends on instantiation; in EvoMAC instantiation performance measured as requirement-pass accuracy and HumanEval pass@1.",
            "baseline_comparison": "MAC networks can outperform single-agent methods when well-designed, but static, human-designed MACs can be limited; EvoMAC demonstrates improved automatic adaptation over static MACs.",
            "coordination_benefits": "Division of complex tasks into manageable subtasks enabling structured reasoning and distributed generation; potential for emergent behaviors and improved structure in outputs.",
            "coordination_challenges": "Human-crafted static workflows lack adaptability across diverse tasks and can be brittle; critique-based feedback can be subjective/hallucinatory; coordination overhead and prompt-design complexity.",
            "ablation_studies": "Paper-level ablations (in EvoMAC) show multi-agent coding/testing teams outperform single-agent variants and that environment-driven feedback significantly improves evolution.",
            "optimal_configurations": "Not universal; the paper promotes self-organized initializations (organizer agents) and iterative environment-driven updates as best practices rather than fixed agent counts.",
            "uuid": "e2556.1",
            "source_info": {
                "paper_title": "Self-Evolving Multi-Agent Collaboration Networks for Software Development",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MetaGPT",
            "name_full": "MetaGPT",
            "brief_description": "A multi-agent framework for automated software development that composes role-based LLM agents into programming workflows (used as a baseline in this paper).",
            "citation_title": "Metagpt: Meta programming for multi-agent collaborative framework.",
            "mention_or_use": "use",
            "system_name": "MetaGPT",
            "system_description": "A human-designed multi-agent system (agentic workflow) that assigns fixed role prompts and a workflow to decompose coding tasks into subtasks; in this paper MetaGPT is used as a baseline multi-agent system powered by GPT-4o-Mini for fair comparisons.",
            "number_of_agents": "variable (framework supports multiple role-based agents); paper uses MetaGPT as provided by its authors (exact number per task not specified here)",
            "agent_specializations": "Role-based programming agents (e.g., PM/architect/developer/tester roles) as defined in MetaGPT's original design; paper does not detail MetaGPT internals beyond being a human-designed MAC.",
            "research_phases_covered": "Implementation (software planning and code generation) and basic testing workflows as implemented by the framework.",
            "coordination_mechanism": "Human-designed standardized operating procedures (static agent prompts and workflows); hierarchical/role-based pipeline.",
            "communication_protocol": "LLM prompt-based natural-language message exchanges between role agents (paper does not specify structured protocol beyond standard LLM messages).",
            "feedback_mechanism": "Primarily internal agent messages and role-based handoffs; lacks EvoMAC's environment-driven iterative textual backpropagation (paper emphasizes MetaGPT's reliance on heuristic static design).",
            "communication_frequency": "Sequential pipeline during generation; no iterative environment-driven evolution per-task described in this paper.",
            "task_domain": "Software development (used as a baseline on rSDE-Bench and HumanEval in this paper).",
            "performance_metrics": "Reported in Table 1 when powered by GPT-4o-Mini: Website Basic 15.41%, Website Advanced 0.00%, Game Basic 16.67%, Game Advanced 0.00%, HumanEval 88.41%.",
            "baseline_comparison": "EvoMAC outperforms MetaGPT substantially on rSDE-Bench (e.g., EvoMAC Website Basic 89.38% vs MetaGPT 15.41%).",
            "coordination_benefits": "Structured role specialization may aid modularity and clarity in code generation workflows.",
            "coordination_challenges": "Heavily reliant on manual design of workflows/prompts; poor adaptability to diverse, lengthy, or task-specific requirements per the paper's critique.",
            "ablation_studies": "Not performed specifically for MetaGPT in this paper; MetaGPT serves as a fixed baseline to evaluate EvoMAC's gains from self-evolution.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2556.2",
            "source_info": {
                "paper_title": "Self-Evolving Multi-Agent Collaboration Networks for Software Development",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Autogen",
            "name_full": "Autogen",
            "brief_description": "A multi-agent conversation framework that enables LLM-driven applications via multi-agent dialogue (used here as a baseline multi-agent system for code generation).",
            "citation_title": "Autogen: Enabling next-gen llm applications via multi-agent conversation",
            "mention_or_use": "use",
            "system_name": "Autogen",
            "system_description": "Agentic multi-agent conversation system that organizes multiple LLM agents to collaborate through message passing; in this paper used as a baseline for software generation tasks and powered by GPT-4o-Mini.",
            "number_of_agents": "variable (framework supports multiple conversational agents); exact per-task agent counts not specified in this paper",
            "agent_specializations": "Conversational role agents (task decomposition, planning, implementation agents) as per Autogen paradigm; not detailed in this paper.",
            "research_phases_covered": "Task planning and generation phases (applied here to software generation and used to produce code in baseline experiments).",
            "coordination_mechanism": "Multi-agent conversational orchestration (human-designed conversation/control rules); static workflow in baseline experiments.",
            "communication_protocol": "Natural-language multi-turn conversation between agents (LLM messages).",
            "feedback_mechanism": "Internal conversational critique / coordination; in baseline use it does not use EvoMAC's environment-driven textual backpropagation.",
            "communication_frequency": "On-demand during multi-turn agent conversation; not iteratively evolved by environment feedback in the baseline setup.",
            "task_domain": "Software development (baseline for rSDE-Bench experiments).",
            "performance_metrics": "Reported in Table 1 when powered by GPT-4o-Mini: Website Basic 25.68% ±4.14, Website Advanced 5.40% ±3.34, Game Basic 17.39% ±1.78, Game Advanced 0.00% ±0.00, HumanEval 85.36%.",
            "baseline_comparison": "EvoMAC outperforms Autogen substantially on rSDE-Bench (e.g., EvoMAC Website Basic 89.38% vs Autogen 25.68%).",
            "coordination_benefits": "Conversational multi-agent setups can coordinate planning and division of labor.",
            "coordination_challenges": "Still relies on heuristic/static conversation design and may lack objective environment-driven iterative correction.",
            "ablation_studies": "Not performed specifically for Autogen in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2556.3",
            "source_info": {
                "paper_title": "Self-Evolving Multi-Agent Collaboration Networks for Software Development",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MapCoder",
            "name_full": "MapCoder",
            "brief_description": "A multi-agent code-generation method intended for competitive problem solving; used as a baseline in the paper's experiments.",
            "citation_title": "Mapcoder: Multi-agent code generation for competitive problem solving",
            "mention_or_use": "use",
            "system_name": "MapCoder",
            "system_description": "A multi-agent coding framework (cited and used as a baseline) that coordinates multiple agents for code generation; in the experiment MapCoder was powered by GPT-4o-Mini for fair comparison.",
            "number_of_agents": "variable (framework allows multiple agents; exact number per task not specified in paper)",
            "agent_specializations": "Role-based coding agents per MapCoder design (paper does not detail roles beyond general multi-agent coding).",
            "research_phases_covered": "Implementation (function/code generation); used as baseline on function- and software-level benchmarks.",
            "coordination_mechanism": "Agentic workflow (human-designed); specifics not elaborated in this paper.",
            "communication_protocol": "Natural-language messages/prompts between LLM agents; no special structured protocol described here.",
            "feedback_mechanism": "Baseline usage in this paper does not include EvoMAC's iterative environment-driven textual backpropagation.",
            "communication_frequency": "Sequential or conversational per MapCoder's design during generation; not iteratively evolved here.",
            "task_domain": "Software development and coding (baseline on rSDE-Bench and HumanEval).",
            "performance_metrics": "Table 1 numbers (GPT-4o-Mini): Website Basic 34.70% ±1.59, Website Advanced 14.57% ±0.66, Game Basic 29.71% ±6.72, Game Advanced 7.52% ±6.10, HumanEval 90.85%.",
            "baseline_comparison": "EvoMAC outperforms MapCoder on rSDE-Bench and HumanEval as reported in Table 1.",
            "coordination_benefits": "Multi-agent decomposition helps in structured solution search for coding tasks.",
            "coordination_challenges": "Human-crafted static workflows limit per-task adaptability compared to EvoMAC.",
            "ablation_studies": "Not performed specifically for MapCoder in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2556.4",
            "source_info": {
                "paper_title": "Self-Evolving Multi-Agent Collaboration Networks for Software Development",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Agentverse",
            "name_full": "Agentverse",
            "brief_description": "A framework facilitating multi-agent collaboration and investigating emergent behaviors in LLM agent societies; included as a baseline in this study.",
            "citation_title": "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents.",
            "mention_or_use": "use",
            "system_name": "Agentverse",
            "system_description": "A multi-agent framework for collaborative tasks that encourages emergent behaviors among LLM agents; used as a multi-agent baseline for software generation tasks in the experiments.",
            "number_of_agents": "variable (framework supports multiple agents; not fixed in paper experiments)",
            "agent_specializations": "General role-based agents as per Agentverse framework (paper does not enumerate roles in this work).",
            "research_phases_covered": "Task decomposition and generation; used as baseline for code generation evaluation.",
            "coordination_mechanism": "Human-defined multi-agent workflows; emergent behavior studied in original Agentverse work but not expanded here.",
            "communication_protocol": "Natural-language prompts / messages between agents (LLM dialogues).",
            "feedback_mechanism": "Internal agent interactions; baseline use lacks EvoMAC's external, objective environment-driven iterative updates.",
            "communication_frequency": "Conversational/iterative during agent interaction, but not evolved via the compiler/test feedback loop in baseline experiments.",
            "task_domain": "Software development (baseline for rSDE-Bench).",
            "performance_metrics": "Reported (GPT-4o-Mini) Website Basic 15.41% ±0.00, Website Advanced 0.00% ±0.00, Game Basic 37.67% ±8.20, Game Advanced 16.13% ±4.55, HumanEval 90.85%.",
            "baseline_comparison": "EvoMAC shows superior performance compared to Agentverse on rSDE-Bench.",
            "coordination_benefits": "Potential for emergent collaboration patterns among agents.",
            "coordination_challenges": "Static designs and lack of environment-driven iterative update can limit task-specific improvements.",
            "ablation_studies": "Not performed specifically for Agentverse in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2556.5",
            "source_info": {
                "paper_title": "Self-Evolving Multi-Agent Collaboration Networks for Software Development",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ChatDev",
            "name_full": "ChatDev",
            "brief_description": "A communicative multi-agent system specifically targeted at software development that composes multiple LLM agents into a development pipeline; used as a strong baseline in this paper.",
            "citation_title": "Chatdev: Communicative agents for software development.",
            "mention_or_use": "use",
            "system_name": "ChatDev",
            "system_description": "A multi-agent framework that composes communicative agents to handle software development tasks; in this study ChatDev is used as a multi-agent baseline powered by GPT-4o-Mini for parity with EvoMAC.",
            "number_of_agents": "variable (role-based agents within ChatDev; exact per-task numbers not specified in paper)",
            "agent_specializations": "Communicative role agents (planning, development, testing roles) as per ChatDev design; specifics not detailed in this paper.",
            "research_phases_covered": "Requirement comprehension, planning, code generation, and some testing (framework-oriented), used as baseline for rSDE-Bench and HumanEval.",
            "coordination_mechanism": "Human-defined communicative pipeline of agents exchanging messages; static workflow in baseline use.",
            "communication_protocol": "Natural-language message passing using LLM prompts and replies.",
            "feedback_mechanism": "Agent-to-agent dialogue and any internal testing steps as implemented in ChatDev; does not employ EvoMAC's textual backpropagation with an objective compiler environment in the baseline experiments.",
            "communication_frequency": "Multi-turn agent communication during generation; no environment-driven iterative evolution in baseline experiments.",
            "task_domain": "Software development (baseline in rSDE-Bench experiments).",
            "performance_metrics": "Reported (GPT-4o-Mini) Website Basic 62.67% ±0.28, Website Advanced 43.45% ±0.77, Game Basic 53.63% ±5.70, Game Advanced 32.26% ±4.55, HumanEval 70.73%.",
            "baseline_comparison": "EvoMAC outperforms ChatDev on rSDE-Bench across contexts and shows higher final accuracy after evolution.",
            "coordination_benefits": "Better task comprehension and structured code production than single-agent approaches in many cases.",
            "coordination_challenges": "Still limited by static design and lack of rigorous environment-based iterative correction; ChatDev's HumanEval score was lower in this paper's evaluation compared to EvoMAC.",
            "ablation_studies": "Not performed specifically for ChatDev in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2556.6",
            "source_info": {
                "paper_title": "Self-Evolving Multi-Agent Collaboration Networks for Software Development",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Metagpt: Meta programming for multi-agent collaborative framework.",
            "rating": 2
        },
        {
            "paper_title": "Autogen: Enabling next-gen llm applications via multi-agent conversation",
            "rating": 2
        },
        {
            "paper_title": "Mapcoder: Multi-agent code generation for competitive problem solving",
            "rating": 2
        },
        {
            "paper_title": "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents.",
            "rating": 2
        },
        {
            "paper_title": "Chatdev: Communicative agents for software development.",
            "rating": 2
        }
    ],
    "cost": 0.019035999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Evolving Multi-Agent Collaboration Networks for Software Development</h1>
<p>Yue $\mathbf{H u}^{1}$, Yuzhu Cai ${ }^{2,3}$, Yaxin $\mathbf{D u}^{1}$, Xinyu $\mathbf{Z h u}^{1}$, Xiangrui $\mathbf{L i u}^{1}$, Zijie $\mathbf{Y u}^{1}$,<br>Yuchen Hou ${ }^{1}$, Shuo Tang ${ }^{1}$, Siheng Chen ${ }^{1,3}$<br>${ }^{1}$ Shanghai Jiao Tong University, ${ }^{2}$ Beihang University, ${ }^{3}$ Shanghai AI Laboratory<br>${ }^{1}$ {1867112931,sihengc}@sjtu.edu.cn, ${ }^{2}$ caiyuzhu@buaa.edu.cn</p>
<h4>Abstract</h4>
<p>LLM-driven multi-agent collaboration (MAC) systems have demonstrated impressive capabilities in automatic software development at the function level. However, their heavy reliance on human design limits their adaptability to the diverse demands of real-world software development. To address this limitation, we introduce EvoMAC, a novel self-evolving paradigm for MAC networks. Inspired by traditional neural network training, EvoMAC obtains text-based environmental feedback by verifying the MAC network's output against a target proxy and leverages a novel textual backpropagation to update the network. To extend coding capabilities beyond function-level tasks to more challenging software-level development, we further propose rSDE-Bench, a requirement-oriented software development benchmark, which features complex and diverse software requirements along with automatic evaluation of requirement correctness. Our experiments show that: i) The automatic requirement-aware evaluation in rSDE-Bench closely aligns with human evaluations, validating its reliability as a software-level coding benchmark. ii) EvoMAC outperforms previous SOTA methods on both the software-level rSDE-Bench and the function-level HumanEval benchmarks, reflecting its superior coding capabilities. The benchmark can be downloaded at https://yuzhu-cai.github.io/rSDE-Bench/.</p>
<h2>1 INTRODUCTION</h2>
<p>Automatic software development focuses on generating code from natural language requirements. Code is a universal problem-solving tool, and this automation presents significant potential to provide substantial benefits across all areas of our lives Li et al. (2022a). Recently, the industry has introduced several large language model (LLM)-driven coding assistants, including Microsoft's Copilot Microsoft (2023), Amazon's CodeWhisperer Amazon (2022), and Google's Codey Google (2023). These coding assistants significantly advance human efficiency and yield considerable commercial benefits. Despite the initial success of LLMs in assisting with line-level coding, they struggle to tackle more complex coding tasks. This limitation stems from the restricted reasoning abilities of single LLMs and their lack of capacity for long-context understanding Wang et al. (2024a); Li et al. (2024a); Wang et al. (2024b).</p>
<p>To handle function-level coding tasks, numerous multiple language agent collaboration (MAC) systems have been proposed Li et al. (2023); Hong et al. (2023); Chan et al. (2024); Islam et al. (2024); Yang et al. (2024b); Li et al. (2022b); Osaka (2023). These MAC systems function as LLMdriven agentic workflow. They follow human-designed standardized operating procedures to divide the complex coding tasks into simpler subtasks within the workflow, allowing each agent to conquer specific subtasks. These MAC systems significantly advance coding capabilities from line-level to function-level tasks. However, current MAC systems rely on heuristic designs. These human-crafted static systems have two inherent limitations: i) their performance is confined to human initialization. Given the diversity of real-world coding tasks, human design cannot fully address the specific needs of each task; and ii) they lack the flexibility to adapt to new tasks. This rigidity necessitates that researchers and developers manually decompose tasks and create prompts. The complexity of this process inhibits effective human optimization for adapting to new challenges.</p>
<p>To address these limitations, we present EvoMAC, a novel self-evolving paradigm for MAC networks. EvoMAC's key feature is its ability to iteratively adapt both agents and their connections during test time for each task. Inspired from the standard neural network training, the core idea of selfevolution is to obtain text-based environmental feedback by verifying the MAC network's generation against a target proxy, then leverage a novel textual back-propagation to update the MAC network. Following this general paradigm, we specify EvoMAC for software development, which comprises three essential components: i) an adaptable MAC network-based coding team that generates code through feed-forward; ii) a specifically designed testing team that creates unit test cases serving as the target proxy and verifies the generated code in the compiler to produce objective feedback; and iii) an updating team that uses the textual back-propagation algorithm to update the coding team. By cycling these three components, the coding team can iteratively evolve and generate codes that are better aligned with the unit test cases, eventually fulfilling more requirements of the coding task.</p>
<p>Our self-evolving MAC network has the potential to further advance coding capabilities from functionlevel to more complex software-level tasks. As it can iteratively address lengthier task requirements and cater to realistic software development demands. However, existing benchmarks typically focus on specific individual functions Chen et al. (2021); Austin et al. (2021); Yang et al. (2024a); Khan et al. (2023) or bug-fixing Jimenez et al. (2023), leaving a significant gap in providing comprehensive requirements for software development. This gap makes it difficult to fully assess the potential of our self-evolving MAC network.</p>
<p>To support the development of software-level coding capabilities, we propose rSDE-Bench, a novel requirement-oriented software development benchmark. It is the first benchmark that features both complex and diverse software requirements, as well as the automatic evaluation of requirement correctness. rSDE-Bench involves 53 coding tasks with 616 requirements, covering two typical software types, Website, and Game, and two requirement difficulty levels, Basic and Advanced. Each coding task consists of two components: i) multiple requirements that clearly outline measurable software functionalities, item by item, and ii) paired black-box test cases that automatically verify the correctness of each requirement. rSDE-Bench can achieve automatic evaluation with these synchronized pairs of requirements and test cases. The rSDE-Bench introduces new software-level challenges, including lengthy requirement analysis and long-context coding, which are essential in real-world software development but are absent in existing benchmarks.</p>
<p>To validate the effectiveness of our proposed EvoMAC and rSDE-Bench, we conduct three key evaluations. First, we compare our automatic evaluation in rSDE-Bench with human evaluation, achieving a coherence score of $99.22 \%$, demonstrating its reliability. Second, we compare EvoMAC against five multi-agent and three single-agent baselines. EvoMAC significantly outperforms previous SOTAs by $26.48 \%, 34.78 \%$, and $6.10 \%$ on Website Basic, Game Basic, and HumanEval, respectively, underscoring its effectiveness. Third, we evaluate EvoMAC with varying evolving times and two different driving LLMs. The results indicate that EvoMAC consistently improves with more evolving times and shows convincing enhancements regardless of the driving LLM used, further demonstrating the effectiveness of our self-evolving design.</p>
<p>To sum up, our contributions are:</p>
<ul>
<li>We propose EvoMAC, a novel self-evolving MAC network, and apply it to software development. EvoMAC can iteratively adapt both agents and their connections during test time for each task.</li>
<li>We propose rSDE-Bench, a novel requirement-oriented software development benchmark. It is the first benchmark that features both complex and diverse software requirements, as well as the automatic evaluation of requirement correctness.</li>
<li>We conduct comprehensive experiments and validate that: automatic evaluation in rSDE-Bench is highly aligned with human evaluation; EvoMAC outperforms previous SOTAs, and self-evolving promises continuous improvement with evolving times.</li>
</ul>
<h1>2 Related Works</h1>
<p>LLM-based multi-agent collaboration. LLM-driven multi-agent collaboration (MAC) systems Xu et al. (2023); Hua et al. (2023); Ziems et al. (2024); Wu et al. (2023); Hong et al. (2023); Chan et al. (2024); Mandi et al. (2024a) enable multiple agents to share information and collaboratively complete the overall task. These MAC systems function as agentic workflows. They have demonstrated enhanced problem-solving capabilities in various domains, such as mathematics Islam et al. (2024),</p>
<p>software development Qian et al. (2023); Hong et al. (2023), embodied task Mandi et al. (2024b) and social simulation Ziems et al. (2024); Pang et al. (2024); Li et al. (2024b). However, these systems Wu et al. (2023); Chen et al. (2023) heavily rely on manually designed workflows, which lack generalizability and the labor-intensive nature of manual design poses significant limitations. To address this issue, we propose a novel self-evolving paradigm, which allows agents to update and improve through external feedback, enabling dynamic adaptation and more advanced performance across varied tasks.</p>
<p>Software development benchmarks. Software development benchmarks aim to evaluate models in the task of generating code from natural language descriptions Zheng et al. (2023). These benchmarks typically include task definitions and evaluation criteria. Existing benchmarks can be categorized into three types: i) function completion (HumanEval Chen et al. (2021), MBPP Austin et al. (2021), EvalPlus Liu et al. (2023), xCodeEval Khan et al. (2023)); ii) bug repair (SWE-bench Jimenez et al. (2023)); and iii) software generation (SRDD Qian et al. (2023), SoftwareDev Hong et al. (2023)). Function completion and bug repair benchmarks are confined to function-level task definitions, missing the diverse realistic software requirements. Software generation benchmarks often depend on expensive human evaluations or indirect similarity-based measurements, unable to automatically and accurately verify the requirement correctness. To address these limitations, we introduce rSDEBench, the first benchmark contains both diverse software requirements and automatic evaluation of requirement correctness. It can support the development of more realistic software-level coding capabilities.</p>
<h1>3 EvoMAC: Self-Evolving Multi-Agent Collaboration Network</h1>
<p>This section presents EvoMAC, a novel self-evolving multi-agent collaboration network and its application to software development. The key feature of EvoMAC is its ability to iteratively adapt both agents and their connections during test-time for each task, mimicking the back-propagation process, a core algorithm in neural network training. We first formulate a general self-evolving paradigm in Sec. 3.1 and then describe its application to software development in Sec. 3.2.</p>
<h3>3.1 A GENERAL SELF-EVOLVING PARADIGM VIA TEXTUAL BACKPROPAGATION</h3>
<p>Multi-agent collaboration network. A multi-agent collaboration (MAC) network is a computational graph representing agentic workflows, where multiple agents empowered by LLMs interact as interconnected nodes to coordinate and share information for complex task-solving. The intuition behind to divide the complex task into more specific and manageable subtasks for each agent, allowing the overall task to be gradually conquered through the agentic workflow. Mathematically, we represent a MAC network with $N$ autonomous agents as a directed acyclic graph $\mathcal{A}=(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}=\left{v_{i}\right}<em i_="i," j="j">{i=1}^{N}$ is the set of $N$ nodes, and $\mathcal{E}=\left{e</em>\right}<em i="i">{i, j \in[1, \ldots, N], i \neq j}$ is the set of directed edges with no circles. The $i$-th node $v</em>$ represents the task dependency between the $i$-th agent and the $j$-th agent, indicating that the $j$-th agent's subtask should be executed after the $i$-th agent's subtask in the agentic workflow. The overall graph topology specifies the agentic workflow. Analogy to traditional neural networks, agents function similarly to neurons, with agent prompts serving as neurons' weights and the agentic workflow as the layers and connections.}$ represents the $i$-th autonomous agent with the prompt $p_{i}$, which specifies its subtask. The edge $e_{i, j</p>
<p>The feed-forward pass of MAC network is the execution of the agentic workflow. In this process, each agent is given two inputs: the initial task requirement and the output from the previous agent. Using these, each agent produces an output that fulfills its specific subtask. Eventually, the last agent's generation constitutes the final output, integrating all completed subtasks. Note that the initial task requirement is input to each agent as context, providing supplementary details to aid in the implementation of each subtask.</p>
<p>Recently, various MAC networks have been designed using human expertise to assign fixed agent prompts and workflows Hong et al. (2023); Chan et al. (2024), resembling untrained neural networks. However, these designs solely rely on human priors and lack adaptability, causing limited performance improvement over a single agent. To overcome this, inspired by neural network training, we propose a self-evolving paradigm for multi-agent collaboration networks, enabling both agents and their connections to dynamically evolve during test-time for each given task.</p>
<p>Optimization problem. Here we consider a general generation task. During test-time, given a task, the MAC network performs a feed-forward pass to generate the final output without knowing its</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The general self-evolving paradigm.</p>
<p>quality. The key to evolution during test-time is to set up a target proxy for the MAC network to guide its improvements in the generated output. Here we consider this target proxy as the conditions for task completion, such as unit tests in coding, and we can produce such a target proxy by another group of autonomous agents based on the same task description. Then, the quality of each generated output can be verified according to the target proxy. This approach relies on two key assumptions: (i) generating a target proxy is significantly simpler than completing the original generation task, and (ii) the generated output can be correctly verified against the target proxy through an objective environment. These assumptions are practical in many applications. For example, in code generation, producing unit tests, the expected input-output pairs, is much easier than generating the entire code; meanwhile, a code compiler naturally acts as the objective environment to check the correctness of the generated code against the unit test, providing objective and informative feedback.</p>
<p>Mathematically, let <strong>X</strong> be the textual description of a task. Given the MAC network <strong>A</strong><sup><em>g</em></sup>, the generated output is <strong>G</strong> = Φ(<strong>X</strong>, <strong>A</strong><sup><em>g</em></sup>), where Φ(·,·) is the general feed-forward operator that executes the agentic workflow, processing the input text through the MAC network. Similarly, the target proxy is <strong>T</strong> = Φ(<strong>X</strong>, <strong>A</strong><sup><em>t</em></sup>), where <strong>A</strong><sup><em>t</em></sup> is another MAC network designed for producing the target proxy. Note that we aim to evolve and optimize <strong>A</strong><sup><em>g</em></sup>, while keep <strong>A</strong><sup><em>t</em></sup> predefined and fixed. The optimization of our self-evolution is formulated as,</p>
<p>$$
\mathcal{A}<em _mathcal_A="\mathcal{A">g^* = \min</em>
$$}_g} \langle \Phi(\mathbf{X}, \mathcal{A}_g), \mathbf{T} \rangle_E, \text{ subject to: } \mathbf{T} = \Phi(\mathbf{X}, \mathcal{A}_t), \tag{1</p>
<p>where ⟨·,·⟩<sub>E</sub> is an objective environment executor that receives the generated output and the target proxy as inputs and outputs a text-based environmental feedback. Akin to the loss function in traditional neural network training, which quantifies the difference between the generated output and the ground-truth, the objective in (1) evaluates whether the generated output meets the conditions of the task completion using the environment, subsequently producing execution reports as the text-based environmental feedback. Here the minimization operation min is defined to reduce the failures during execution. With the guidance of the target proxy and the objective feedback given by the environment, the MAC network can improve its success rate of task completion during test time.</p>
<p>Note that, another straightforward way to enable the MAC network's evolution is through the self-critique strategy <em>Zhou et al. (2024); Valmeekam et al. (2023); Xu et al. (2024); Asai et al. (2023)</em>, which employs a critique agent to assess the generated output directly. This approach has two inherent limitations: i) the critique may be subjective and biased, and ii) the critique agent can have hallucinations, causing inconsistencies and errors. These limitations can cause the MAC network to become entrenched in its own preferences or evolve in the wrong direction, especially iterating multiple times; see our experimental validations in Tab. 2. In comparison, our approach leverages an environment executor to provide objective feedback, preventing bias and hallucinations.</p>
<p>While we use the analogy between our self-evolution process and neural network training for motivating, they are significantly different in three key aspects: (i) our self-evolution occurs at test time without a dedicated training phase; (ii) it evolves for each specific task individually rather than over a batch of samples; and (iii) the environmental feedback are usually texts, not be numerical values, which cannot be optimized by the standard backpropagation. This motivates us to propose our textual backpropagation.</p>
<p><strong>Solution based on textual backpropagation.</strong> The self-evolution solution iteratively updates the MAC network using a textual backpropagation algorithm, guided by the environmental feedback. The core idea is to analyze the influence of each agent in the MAC network <strong>A</strong><sup>g</sup> to the final environmental feedback and use these analyses to update the agent prompts and the agentic workflow in <strong>A</strong><sup>g</sup>. This</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: EvoMAC takes task requirements as input and iteratively updates the coding team to generate code that better fulfills the requirements.
is achieved by two collaborative agents, each responsible for one of the two key steps: (i) textual gradient analysis and (ii) network update. The overall algorithm can refer to Alg. 1 in the appendix.
First, the gradient agent takes the environmental feedback as the input and outputs textual gradients that describe the impact of each agent in the MAC network. Let $\mathcal{A}<em _vartheta="\vartheta">{\vartheta}^{(k)}$ and $\mathbf{L}^{(k)}$ be the MAC network and the environment feedback at the $k$-th iteration. The textual gradient is then $\nabla \mathbf{L}^{(k)}=$ $\mathcal{G}\left(\mathcal{A}</em>$ : 1) whether this agent's subtask is fulfilled; 2) whether this agent introduces errors; and 3) whether any subtask is missed in the current MAC network.}^{(k)}, \mathbf{L}^{(k)}\right)$, where $\mathcal{G}(\cdot, \cdot)$ is the gradient analysis operator managed by the gradient agent; see its prompt in Appendix. The textual gradient details three-fold information for each agent inside $\mathcal{A}_{\vartheta}^{(k)</p>
<p>Second, based on the textual gradients, the updating agent iterates the MAC network as $\mathcal{A}<em _vartheta="\vartheta">{\vartheta}^{(k+1)}=$ $\mathcal{U}\left(\mathcal{A}</em>(\cdot, \cdot)$ is the updating operator managed by the updating agent. This operator guides the updates from three-folds: 1) removing the agents whose subtasks have been completed; 2) revising the erroneous agent's prompts by adding potential solutions provided in the gradient analysis; and 3) adding new agents for missing subtasks and restructuring the workflows based on the subtask dependencies noted in the gradient analysis; see the prompt details in Appendix. These adjustments address existing issues and fulfill unmet requirements in the current generation of the MAC network, promising improvements in the updated version.}^{(k)}, \nabla \mathbf{L}^{(k)}\right)$, where $\mathcal{U</p>
<p>Note that, the key of the textual backpropagation is the prompt designs for both gradient analysis and network updates. The design must i) thoroughly evaluate the subtask of each agent in the MAC network according to the objective environment feedback and determine necessary adjustments to the MAC network to address existing issues, fulfilling the unmet requirements; and ii) maintain coherence, ensuring that issues identified by the gradient agent can be effectively resolved by the updating agent's modifications to the MAC network.</p>
<h1>3.2 SELF-EVOLUTION FOR SOFTWARE DEVELOPMENT</h1>
<p>In this section, we apply the self-evolving paradigm to the task of software development. The overall architecture of the proposed self-evolving multi-agent collaboration network for software development is illustrated in Fig. 1. Given a coding task, the coding team, corresponding to the MAC network $\mathcal{A}<em t="t">{\vartheta}$, generates all the codes through its forward-pass; the testing team, associated with the MAC network $\mathcal{A}</em>$, is responsible for creating the target proxy; that is, unit tests of the coding task; and the objective environment tool is realized through the compiler. The identified bugs during execution form the textual environmental feedback. The updating team, consisting of two collaborative agents, manages the textual backpropagation. By continuously cycling through feed-forward, feedback collection, and textual backpropagation processes, the coding team is iteratively refined to more closely align with the test cases. The detailed implementation of agents can refer to Sec. 9 in the Appendix.</p>
<p>Since unit test generation is much easier than the original logical code generation, the testing team usually can produce high-quality test cases, which are closely aligned with the task requirements. Then, improving alignment with the unit tests through MAC network updates ensures better adherence to the actual task requirements.</p>
<p>Coding team for feed-forward. In the feed-forward process, the coding team synthesizes code according to the given coding task. To handle the extensive software requirements, the coding team is implemented as a MAC network. It divides the comprehensive requirements into a sequence of smaller, more specific function implementation subtasks, and progressively conquers them through the agentic workflow. Unlike existing MAC systems that heuristically decompose coding tasks and define the agentic workflow, we initialize the MAC network using a novel self-organizing approach. A coding organizer agent automatically and flexibly decomposes the task requirements into subtasks and assembles the coding agent team accordingly. The number of coding agents is dynamic, adjusting in response to the task requirements. Note that, the quality of the generated code is unknown during the forward pass, which necessitates the self-evolving paradigm to iteratively refine the generation.</p>
<p>Testing team and compiler for feedback collection. To verify whether the generated code meets the requirements of the coding task, we employ unit tests as the target proxy. These test cases consist of input-output pairs tailored to specific requirements. For example, a test case for a keyboard control requirement would detail the type of control as the input and describe the expected behavior as the output. To create flexible and comprehensive unit tests, we set up the testing team as a MAC network and also initialize it in a self-organized way. A testing organizer agent automatically decomposes our specified key testing criteria into subtasks and accordingly forms the testing agent team .</p>
<p>Once the test cases and generated code are ready, they are executed in the compiler, which functions as the environmental tool, producing execution logs. These logs clearly point out the gap between the generated code and the test cases. It shows satisfied testing requirements, existing function errors, and unmet testing requirements. This feedback information can be used to verify whether each agent's subtask is accomplished and guide the MAC network update.</p>
<p>Updating team for textual back-propagation. The updating team consists of two collaborative agents: the gradient agent and the updating agent, adjusting the MAC network based on the execution logs, including the agent prompts and workflows. This process consists of two steps. First, the gradient agent summarizes the textual gradient by identifying accomplished subtasks for satisfied requirements, appending new subtasks for unmet requirements, and analyzing errors to detail their originating agents and revising suggestions. Second, the updating agent modifies the coding agent team by removing agents that have completed their subtasks, adding new agents for the new subtasks, and revising agent prompts to address issues identified in the previous generation. The agent workflow is updated once the agent team is revised, based on the dependencies among the subtasks.</p>
<h1>4 RSDE-BENCH: REQUIREMENT-ORIENTED SOFTWARE DEVELOPMENT ENGINEERING BENCHMARK</h1>
<p>This section introduces rSDE-Bench, a requirement-oriented benchmark designed to assess the ability of models to handle software-level coding tasks. Each coding task involves multiple detailed software requirements. These requirements specify each functionality and constraint of the software, item by item, serving as measurable benchmarks for assessing the software's effectiveness. As shown in Fig. 3, unlike previous instruction-oriented approaches Qian et al. (2023); Hong et al. (2023) which rely on brief instructions as input, rSDE-Bench uses comprehensive software requirements as input, complemented by unit test cases to automatically evaluate the correctness. This benchmark provides software-level coding tasks and automatic evaluation, aligning more closely with real-world software development practices.</p>
<h3>4.1 BENCHMARK CONSTRUCTION</h3>
<p>rSDE-Bench involves two typical real-world software types: game and website. They can reflect different coding capacities demanded in realistic software development. Game often requires handling dynamic interactions, real-time state changes, and user-driven operations, focusing on elements like logic execution, initialization, and game state transitions. Website emphasizes static and dynamic content management, user interaction through forms and buttons, and ensuring page elements are displayed and functional. rSDE-Bench involves diverse requirements, each paired with a test case. Specifically, rSDE-Bench provides 53 unique coding tasks and 616 test cases. For details on the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Comparison between instruction-oriented and requirement-oriented evaluations. rSDEBench accurately reflects requirement fulfillment with the proposed accuracy score of $2 / 13$, while the indrection evaluation misjudges with high scores ( 0.89 ), failing to detect missing functionality.
benchmark construction, software statistics, software requirements, and test case examples, see Sec. 7 in the Appendix.
rSDE-Bench introduces two requirement difficulty levels, including basic and advanced, to reflect the varying complexity of real-world software development tasks. The basics reflect the fundamental and more achievable requirements, such as interaction, control, and logging. The advanced reflects more complex software functionalities, such as game logical rules, and dynamic web content management. The details can be referred to the appendix.</p>
<h1>4.2 Automatic Evaluation</h1>
<p>rSDE-Bench supports automatic evaluation of requirement correctness. It achieves this by pairing a specifically designed black-box test case with each requirement. The test case can directly verify whether the generated code achieved the requirement. Its evaluation metric is the accuracy, which quantifies the proportion of correctly passed test cases. It is similar to the pass@1 metric in HumanEval Chen et al. (2021), which evaluates the pass ratio of correctly achieved functions against the total functions via unit test verification. It is a fully automated evaluation process, eliminating the need for human involvement while still providing accurate and reliable assessments.</p>
<p>Previous benchmarks for software code generation mainly rely on two evaluation methods. One method is human evaluation Hong et al. (2023), which is time-consuming and not scalable for large datasets. The other method is indirect evaluations Qian et al. (2023), which defines metrics like consistency, completeness, and quality. Consistency measures how closely the generated software aligns with the original requirement description by comparing the cosine similarity between the two. Completeness is determined by detecting the presence of placeholder (such as pass or TODO), which results in a binary value of 0 or 1 . Quality is then calculated as the product of several factors: consistency, completeness, and executability. As illustrated in Fig. 3, they could not measure the correctness of the generated code in fulfilling requirements. In contrast, rSDE-Bench's test cases-based evaluation is more rigorous and precise. These test cases can accurately verify the correctness of generated code in fulfilling the requirements. rSDE-Bench promises reliable and scalable automatic evaluation. In the experiments, we have validated the significant advantages of the proposed automatic evaluation over the previous metrics, including consistency and quality; see Fig. 4.</p>
<h3>4.3 Features</h3>
<p>Challenging and diverse software requirements. rSDE-Bench features long-context software requirements (averaging 507/1011 words for game and website tasks, respectively), unlike instructionoriented benchmarks Chen et al. (2021); Austin et al. (2021); Jimenez et al. (2023) that rely on brief prompts. These detailed requirements better reflect real-world lengthy and complex software development challenges.
Requirement-aware precise and efficient evaluation. rSDE-Bench employs detailed software requirements and automated unit tests to precisely measure how well generated software meets its objectives. Generated codes are evaluated based on pass rates from running specific test cases, offering an accurate and efficient process. In contrast, instruction-oriented benchmarks rely on brief</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance of four methods in terms of four evaluation metrics, including human evaluation, our automatic evaluation (accuracy), consistency, and quality. WB/GB and WA/GA represent Web/Game Basic and Web/Game Advanced respectively. Our accuracy metric is highly aligned with human evaluation across four dataset settings.
Table 1: Comparison of EvoMAC with five multi-agent and three single-agent SOTA baselines, all powered by GPT-4o-Mini. Red values represent the percentage improvement of EvoMAC, shade in pink, over the single-agent baselines, shade in grey.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">rSDE-Bench</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">HumanEval (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Website(\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Game(\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pass@1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Basic</td>
<td style="text-align: center;">Advanced</td>
<td style="text-align: center;">Basic</td>
<td style="text-align: center;">Advanced</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Single-Agent</td>
<td style="text-align: center;">Gemini-1.5-Flash</td>
<td style="text-align: center;">$29.79 \pm 1.00$</td>
<td style="text-align: center;">$11.61 \pm 2.34$</td>
<td style="text-align: center;">$21.74 \pm 6.39$</td>
<td style="text-align: center;">$6.45 \pm 6.97$</td>
<td style="text-align: center;">73.17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Claude-3.5-Sonnet</td>
<td style="text-align: center;">$58.90 \pm 1.48$</td>
<td style="text-align: center;">$37.11 \pm 1.06$</td>
<td style="text-align: center;">$44.20 \pm 5.41$</td>
<td style="text-align: center;">$18.29 \pm 13.26$</td>
<td style="text-align: center;">89.02</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4o-Mini</td>
<td style="text-align: center;">$62.90 \pm 2.52$</td>
<td style="text-align: center;">$44.40 \pm 4.21$</td>
<td style="text-align: center;">$42.76 \pm 15.50$</td>
<td style="text-align: center;">$30.10 \pm 11.87$</td>
<td style="text-align: center;">88.41</td>
</tr>
<tr>
<td style="text-align: center;">Multi-Agent</td>
<td style="text-align: center;">MetaGPT</td>
<td style="text-align: center;">$15.41 \pm 0.00$</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
<td style="text-align: center;">$16.67 \pm 2.71$</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
<td style="text-align: center;">88.41</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Autogen</td>
<td style="text-align: center;">$25.68 \pm 4.14$</td>
<td style="text-align: center;">$5.40 \pm 3.34$</td>
<td style="text-align: center;">$17.39 \pm 1.78$</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
<td style="text-align: center;">85.36</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MapCoder</td>
<td style="text-align: center;">$34.70 \pm 1.59$</td>
<td style="text-align: center;">$14.57 \pm 0.66$</td>
<td style="text-align: center;">$29.71 \pm 6.72$</td>
<td style="text-align: center;">$7.52 \pm 6.10$</td>
<td style="text-align: center;">90.85</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Agentverse</td>
<td style="text-align: center;">$15.41 \pm 0.00$</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
<td style="text-align: center;">$37.67 \pm 8.20$</td>
<td style="text-align: center;">$16.13 \pm 4.55$</td>
<td style="text-align: center;">90.85</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatDev</td>
<td style="text-align: center;">$62.67 \pm 0.28$</td>
<td style="text-align: center;">$43.45 \pm 0.77$</td>
<td style="text-align: center;">$53.63 \pm 5.70$</td>
<td style="text-align: center;">$32.26 \pm 4.55$</td>
<td style="text-align: center;">70.73</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EvoMAC</td>
<td style="text-align: center;">$\begin{gathered} \mathbf{8 9 . 3 8} \mathbf{1 1 . 0 1} \ +\mathbf{2 6 . 4 8} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \mathbf{6 5 . 0 5} \mathbf{1 1 . 5 6} \ +\mathbf{2 0 . 6 5} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \mathbf{7 7 . 5 4} \mathbf{2 2 . 0 4} \ +\mathbf{3 4 . 7 8} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \mathbf{5 1 . 6 0} \mathbf{4 . 5 4} \ +\mathbf{2 1 . 5 0} \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \mathbf{9 4 . 5 1} \ +\mathbf{6 . 1 0} \end{gathered}$</td>
</tr>
</tbody>
</table>
<p>prompts, which lack constraints and make evaluation less reliable, often requiring labor-intensive or indirect evaluation.</p>
<h1>5 EXPERIMENTS</h1>
<h3>5.1 EXPERIMENTAL SETUP</h3>
<p>Baselines. To validate the effectiveness of our EvoMAC, we conducted comparisons against both single-agent and multi-agent baselines. The single-agent baselines involve three prominent large models: GPT-4o-Mini (gpt-4o-mini), Claude-3.5-Sonnet (claude-3-5-sonnet-20240620), and Gemini (gemini-1.5-flash). For multi-agent baselines, we included five state-of-the-art (SOTA) methods: MetaGPT Hong et al. (2023), Autogen Wu et al. (2023), Mapcoder Islam et al. (2024), Agentverse Chen et al. (2023), and ChatDev Qian et al. (2023). To ensure a fair comparison, all multi-agent baselines, including our EvoMAC, are powered by the efficient and powerful GPT-4o-Mini model. Additionally, to demonstrate the adaptability and robustness of our EvoMAC, we developed two EvoMAC variants using GPT-4o-Mini and Claude-3.5-Sonnet.</p>
<p>Datasets. Our experiments cover both the proposed rSDE-Bench and the standard coding benchmark HumanEval Chen et al. (2021). HumanEval comprises 164 Python function completion problems, where the task is to generate code from a single function description.</p>
<h3>5.2 EFFECTIVENESS OF RSDE-BENCH'S EVALUATION AND EvoMAC</h3>
<p>rSDE-Bench's automatic evaluation metric (accuracy) is highly aligned with human evaluation. Our primary goal is to validate the effectiveness of the proposed automatic evaluation in rSDE-Bench by comparing it with two existing evaluation metrics: consistency and quality, both from SRDD Qian et al. (2023). For a fair comparison, our golden standard is human evaluation, conducted by two expert code engineers who manually verify the fulfillment of requirements by interacting with the developed software. This process is tedious, taking around four hours per expert to evaluate the entire benchmark. The effectiveness of an evaluation metric depends on how closely it aligns with human evaluation.</p>
<p>Fig. 4 presents the performance of four methods in terms of four evaluation metrics, including human evaluation, our automatic evaluation, consistency, and quality. We see that: i) our automatic evaluation is highly aligned with human evaluation across two software types (Website and Game), four methods, (GPT-4o-Mini, MetaGPT, ChatDev, and our EvoMAC), and two requirement difficulties (Basic and</p>
<p>Advanced). The correlation coefficient between human evaluation and our accuracy metric is 0.9922 , demonstrating the effectiveness of the proposed automatic evaluation in rSDE-Bench; ii) Consistency and quality metrics differ significantly from human evaluation, with correlation coefficients of 0.2583 and 0.3041 , respectively. This discrepancy occurs because consistency in SRDD measures similarity, and quality in SRDD focuses on executability, which does not guarantee that all requirements are met. This highlights the need for rSDE-Bench, as the SRDD benchmark does not support requirement-oriented software development.</p>
<p>EvoMAC outperforms previous SOTAs on both software-level and function-level coding benchmarks: rSDE-Bench and HumanEval. Tab. 1 compares EvoMAC with five multi-agent and three single-agent SOTA baselines, all powered by GPT-4o-Mini for a fair comparison. We see that EvoMAC significantly outperforms previous SOTAs across all datasets. EvoMAC outperforms single-agent methods by $26.48 \%$ on the rSDE-Bench Website Basic and $34.78 \%$ on the rSDE-Bench Game Basic, as well as surpassing existing multi-agent methods by over $20 \%$. This highlights the effectiveness of multi-agent collaboration and the power of EvoMAC.</p>
<h1>5.3 EFFECTIVENESS OF EVOLVING</h1>
<p>Fig. 6 shows the accuracy of EvoMAC over multiple evolving iterations on the rSDE-Bench and HumanEval. Each figure presents two curves: one for EvoMAC powered by GPT-4o-Mini (red) and the other by Claude-3.5 (blue). We have the following findings:</p>
<p>Table 2: Ablation study about coding/testing team with single/multi-agent, with/without evolving, and with/without environment tool. Best performances are bolded.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Coding</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Testing</th>
<th style="text-align: center;">Evol.</th>
<th style="text-align: center;">Env.</th>
<th style="text-align: center;">Website(\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Game(\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Basic</td>
<td style="text-align: center;">Advanced</td>
<td style="text-align: center;">Basic Advanced</td>
</tr>
<tr>
<td style="text-align: center;">a)</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">63.70</td>
<td style="text-align: center;">41.70</td>
<td style="text-align: center;">42.76</td>
</tr>
<tr>
<td style="text-align: center;">b)</td>
<td style="text-align: center;">Multi</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">67.47</td>
<td style="text-align: center;">39.27</td>
<td style="text-align: center;">68.10</td>
</tr>
<tr>
<td style="text-align: center;">c)</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">80.82</td>
<td style="text-align: center;">60.32</td>
<td style="text-align: center;">71.73</td>
</tr>
<tr>
<td style="text-align: center;">d)</td>
<td style="text-align: center;">Multi</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">83.90</td>
<td style="text-align: center;">60.72</td>
<td style="text-align: center;">76.08</td>
</tr>
<tr>
<td style="text-align: center;">e)</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">Multi</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">83.56</td>
<td style="text-align: center;">61.94</td>
<td style="text-align: center;">73.91</td>
</tr>
<tr>
<td style="text-align: center;">f)</td>
<td style="text-align: center;">Multi</td>
<td style="text-align: center;">Multi</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">78.08</td>
<td style="text-align: center;">52.23</td>
<td style="text-align: center;">55.80</td>
</tr>
<tr>
<td style="text-align: center;">g)</td>
<td style="text-align: center;">Multi</td>
<td style="text-align: center;">Multi</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">90.75</td>
<td style="text-align: center;">67.20</td>
<td style="text-align: center;">77.54</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Failure case distribution across evolving times on Website and Game.</p>
<p>EvoMAC continuously improves with the evolving times. Fig. 6 shows that as evolving iterations increase, performance consistently improves across all five dataset settings, covering two difficulty levels, two software types, and both requirement-oriented and function complement benchmarks. This highlights the effectiveness, generalizability, and robustness of the self-evolving approach, encouraging EvoMAC to evolve whenever possible.</p>
<p>EvoMAC indistinguishably improves with different driving LLM. From Fig. 6, we see that: i) both EvoMAC variants continuously improve with evolving iterations, demonstrating the robustness of the self-evolving design; ii) the two curves do not intersect, indicating that the EvoMAC variant powered by a more powerful single model consistently outperforms the other, highlighting the advantage of using a stronger model. Success builds on success.</p>
<p>Failure case analysis. Fig. 5 shows the failure case statistics across iterations for Website and Game, showing a general decrease in errors as iterations progress. We see that: i) the most common errors are page display issues in Website and logic errors in Game; ii) page errors are resolved more quickly, while logic errors persist, suggesting that more isolated issues are easier to fix during the evolution process. This results in a sharp initial performance improvement as sipler problems are addressed early, followed by a plateau as more complex issues remain unresolved, shown in Fig. 6.</p>
<h3>5.4 Ablation STUDY</h3>
<p>To assess the effectiveness of each component, Tab. 2 details an ablation study featuring seven EvoMAC variants.</p>
<p>Effectiveness of objective environment feedback. Environment feedback, such as code execution logs, is essential for software development. Variant f) omits this tool, instead using an LLM-driven agent to critique the code. Comparing Variant g) with Variant f) shows a notable performance drop: Website tasks decrease by $12.67 \%$ and $14.97 \%$, and Game tasks by $21.74 \%$ and $18.28 \%$ for Basic and</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Effect of EvoMAC performance across evolving times empowered by GPT-4o-Mini and Claude-3.5-Sonnet on Website, Game, and HumanEval datasets. The figure shows EvoMAC continuously improves with the evolving times on both LLM drives.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: EvoMAC outperforms previous multi-agent and single-agent systems across all the context lengths across the four dataset settings on rSDE-Bench.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: We show the generated code of single-agent, GPT-4o-Mini, and multi-agent systems, ChatDev, and our EvoMAC (iteration $=0 / 1$ ) given the Website task (RecipeHub). After evolving, EvoMAC can revise previous issues and fulfill the task requirement.</p>
<p>Advanced levels, respectively. This underscores the importance of objective environmental feedback, as agent-driven critiques may introduce bias and fail to guide the evolution effectively.</p>
<p>Effectiveness of multi-agent collaboration in coding team and testing team. Comparing Variant g) to Variant e), we observe a performance decrease of $7.19 \%$ and $5.26 \%$ on Website Basic and Advanced respectively, when the coding team is reduced to a single agent. Similarly, comparing Variant g) to Variant d), there is a performance drop of $6.85 \%$ and $6.48 \%$ on Website Basic and Advanced respectively, also when the team is reduced to a single agent. These results demonstrate the necessary for involving multi-agent collaboration, highlighting that multi-agent setups offer more flexible adjustments and enhanced capabilities for evolution.</p>
<p>Effectiveness in handling varied task token lengths. Fig. 7 shows a comparison of task token lengths and performance across GPT-4o-Mini, ChatDev, and EvoMAC. We see that: i) EvoMAC consistently outperforms ChatDev and GPT-4o-Mini across all context lengths, with its self-evolving mechanism enabling the identification and correction of missed contexts and errors during iterations; ii) EvoMAC experiences less performance degradation on the rSDE-Bench Website than on the Game, as Website tasks are more modular and can be broken into subtasks, whereas Game tasks require more coordinated management, making them more challenging.</p>
<h1>5.5 CASE STUDY</h1>
<p>Fig. 8 presents the generated code by a single agent, GPT-4o-Mini, multi-agent systems, ChatDev, and our EvoMAC before and after evolving (iteration=0/1). We see that: i) EvoMAC after evolving can correct issues from previous iterations and successfully fulfill the task requirements; ii) multi-agent systems tend to better comprehend the task requirements and produce more well-structured code. More generated software can refer to Sec. 10 in the Appendix.</p>
<h2>6 CONCLUSION</h2>
<p>We propose EvoMAC, a novel self-evolving paradigm for MAC networks. EvoMAC iteratively adapts agents and their connections during the testing phase of each task. It achieves this with a novel textual back-propagation algorithm. EvoMAC can push coding capabilities beyond function-level tasks and into more complex, software-level development. Furthermore, we propose rSDE-Bench, a novel requirement-oriented software development benchmark. rSDE-Bench features both complex and diverse software requirements, as well as the automatic evaluation of requirement correctness. Comprehensive experiments validate that the automatic requirement-aware evaluation in rSDE-Bench aligns closely with human evaluation. EvoMAC outperforms previous SOTAs in both software-level rSDE-Bench and function-level HumanEval benchmarks.</p>
<p>Future works. In the future, we plan to introduce a reward model to enhance the self-evolving paradigm's ability to learn from feedback and extend the rSDE-Bench to more software types.</p>
<h2>REFERENCES</h2>
<p>Amazon. CodeWhisperer. In https://platform.qa.com/course/amazon-codewhisperer-generating-code-ai-4679/introduction, 2022. URL https://platform.qa.com/course/ amazon-codewhisperer-generating-code-ai-4679/introduction.</p>
<p>Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. ArXiv, abs/2310.11511, 2023.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021.</p>
<p>Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2(4):6, 2023.</p>
<p>Google. Codey. In https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/codechat-bison, 2023. URL https://console.cloud.google.com/vertex-ai/publishers/google/ model-garden/codechat-bison.</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.</p>
<p>Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and Yongfeng Zhang. War and peace (waragent): Large language model-based multi-agent simulation of world wars. arXiv preprint arXiv:2311.17227, 2023.</p>
<p>Md. Ashraful Islam, Mohammed Eunus Ali, and Md Rizwan Parvez. Mapcoder: Multi-agent code generation for competitive problem solving, 2024. URL https://arxiv.org/abs/2405.11403.</p>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2023.</p>
<p>Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty. xcodeeval: A large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval, 2023. URL https://arxiv.org/abs/2303.03004.</p>
<p>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for "mind" exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.</p>
<p>Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. Loogle: Can long-context language models understand long contexts?, 2024a. URL https://arxiv.org/abs/2311.04939.</p>
<p>Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and Yang Liu. Agent hospital: A simulacrum of hospital with evolvable medical agents, 2024b. URL https://arxiv.org/ abs/2405.02957.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom, Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de, Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey, Cherepanov, James Molloy, Daniel Jaymin Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de, Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. Science, 378:1092 1097, 2022a.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. Science, 378(6624):1092-1097, December 2022b. ISSN 1095-9203. doi: 10.1126/science.abq1158. URL http://dx.doi.org/10. 1126/science.abq1158.</p>
<p>Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and LINGMING ZHANG. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= lqvx610Cu7.</p>
<p>Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 286-299. IEEE, 2024a.</p>
<p>Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 286-299. IEEE, 2024b.</p>
<p>Microsoft. Copilot. In https://www.microsoft.com/en-us/microsoft-copilot/meet-copilot, 2023. URL https: //www.microsoft.com/en-us/microsoft-copilot/meet-copilot.</p>
<p>Anton Osika. GPT-Engineer. In https://github.com/AntonOsika/gpt-engineer, 2023. URL https://github. com/AntonOsika/gpt-engineer.</p>
<p>Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng Wang, and Siheng Chen. Self-alignment of large language models via monopolylogue-based social scene simulation. In Forty-first International Conference on Machine Learning, 2024.</p>
<p>Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Chatdev: Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023. URL https://arxiv.org/abs/2307.07924.</p>
<p>Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. Can large language models really improve by self-critiquing their own plans? ArXiv, abs/2310.08118, 2023.</p>
<p>Chonghua Wang, Haodong Duan, Songyang Zhang, Dahua Lin, and Kai Chen. Ada-leval: Evaluating longcontext llms with length-adaptable benchmarks, 2024a.</p>
<p>Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and Armaghan Eshaghi. Beyond the limits: A survey of techniques to extend the context length in large language models, 2024b. URL https://arxiv.org/abs/2402.02244.</p>
<p>Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023. URL https://arxiv.org/abs/2308. 08155 .</p>
<p>Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao, Jie Tang, and Yuxiao Dong. Chatglm-math: Improving math problem-solving in large language models with a self-critique pipeline. ArXiv, 2024.</p>
<p>Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023.</p>
<p>Guang Yang, Yu Zhou, Xiang Chen, and Xiangyu Zhang. Codescore-r: An automated robustness metric for assessing the functionalcorrectness of code synthesis, 2024a. URL https://arxiv.org/abs/2406. 06902 .</p>
<p>John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. ArXiv, abs/2405.15793, 2024b.</p>
<p>Zibin Zheng, Kai-Chun Ning, Jiachi Chen, Yanlin Wang, Wenqing Chen, Lianghong Guo, and Weicheng Wang. Towards an understanding of large language models in software engineering tasks. ArXiv, abs/2308.11396, 2023.</p>
<p>Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, et al. Symbolic learning enables self-evolving agents. arXiv preprint arXiv:2406.18532, 2024.</p>
<p>Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. Can large language models transform computational social science? Computational Linguistics, 50(1):237-291, 2024.</p>
<h1>7 BENCHMARK DETAILS</h1>
<p>Table 3: Basic statistics for website and game domains, including the amount of samples, length in lines of code (Basic/Advanced), and number of test cases at both Basic and Advanced levels.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark</th>
<th style="text-align: center;">Software</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test Case</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Amount</td>
<td style="text-align: center;">Length</td>
<td style="text-align: center;">Basic</td>
<td style="text-align: center;">Advanced</td>
</tr>
<tr>
<td style="text-align: left;">Website</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">$1011 / 1553$</td>
<td style="text-align: center;">292</td>
<td style="text-align: center;">247</td>
</tr>
<tr>
<td style="text-align: left;">Game</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$507 / 788$</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">31</td>
</tr>
</tbody>
</table>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Statistics of Game and Website tasks.
Step 1: Software requirement generation. Each task instance begins with the generation of clear, measurable software requirements. Given the inherent differences across various types of software, we adopt distinct approaches for their formulation. For game-related software, we focuses on common real-world games, capturing detailed task requirements such as GUI layout initialization, interaction methods, and game rules. To align more closely with actual game development practices, we also include game state logging as part of the software requirements. Due to the complexity of logic in game software, these requirements are manually crafted by human. In contrast, for website-related software, we begin with a concise website name, and then leverage the large language model (gpt-40-mini) to enrich the requirements according to predefined patterns. This approach ensures both efficiency and scalability in the creation of benchmarks for websites. By tailoring the process to the distinct characteristics of each software domain, we maintain precision in requirement formulation while addressing the unique challenges posed by each context.</p>
<h2>Step 2: Requirement-based test cases generation.</h2>
<p>As illustrated in Fig 10 and Fig 11, unit tests offer a precise evaluation of software completion. Each task instance includes black-box unit test cases that correspond directly to the software requirements, allowing for a quantitative assessment of requirement fulfillment. To further assess the model's code generation capabilities, we categorize test cases into two levels of difficulty-basic and advanced, as outlined in Tab. 3. We also provide an overview of all websites and games in Tab. 4 and Tab. 5 respectively. As shown in Fig. 9, test cases for website and game software exhibit structural differences, reflecting the distinct nature of each software type. They enable more targeted evaluation of code generation capabilities. Thus, similar to software requirements, the test cases are constructed differently based on the software type. For game-related tests, we manually create test cases, akin to the HumanEval Chen et al. (2021) benchmark, which tracks state changes in response to specific inputs. In the game environment, we assess how game states evolve in response to GUI interactions. For website-related tests, large language model (gpt-40-mini) generates Seleniumbased test cases aligned with the software requirements, followed by manual corrections to resolve any ambiguities. This structured approach ensures rigorous evaluation across diverse software domains.</p>
<p>Basic and advanced requirements definition. For the games, basic requirements involve straightforward user interactions that do not require complex logic, such as character movement or interacting with simple GUI elements. Advanced requirements incorporate more intricate logic, such as managing game state transitions based on user actions or handling conditional game events. These cases focus on ensuring the correct execution of basic actions. In contrast, advanced cases incorporate more intricate logic, such as managing game state transitions based on user actions or handling conditional game events. These cases challenge the model's ability to generate code that integrates dynamic decision-making and interaction within the game environment. For websites, basic cases focus on ensuring that the necessary page elements-such as input fields, buttons, and layouts-are present correctly. These cases assess the completeness of the webpage's structure. On the other hand, advanced cases evaluate more complex functionality, such as handling user authentication, managing dynamic content, or executing specific operations within a content management system. These cases require the model to generate code that performs backend logic and manages user interactions at a deeper level.</p>
<p>Table 4: Overview of Websites in rSDE-Bench.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Websites</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CharitableGivingPlatform</td>
<td style="text-align: left;">DailyHealthTips</td>
<td style="text-align: left;">DailyJournalApp</td>
</tr>
<tr>
<td style="text-align: left;">EcoFriendlyLivingTips</td>
<td style="text-align: left;">ElderCareResources</td>
<td style="text-align: left;">EventPlanner</td>
</tr>
<tr>
<td style="text-align: left;">FitnessEquipmentRental</td>
<td style="text-align: left;">FitnessTracker</td>
<td style="text-align: left;">FreelancerMarketplace</td>
</tr>
<tr>
<td style="text-align: left;">GreenLivingGuide</td>
<td style="text-align: left;">HealthConsultationPlatform</td>
<td style="text-align: left;">MotivationalQuotesApp</td>
</tr>
<tr>
<td style="text-align: left;">MusicFestivalDirectory</td>
<td style="text-align: left;">NoteTakingApp</td>
<td style="text-align: left;">NutritionInformationHub</td>
</tr>
<tr>
<td style="text-align: left;">OnlineLibraryManagementSystem</td>
<td style="text-align: left;">OnlineTherapeuticJournaling</td>
<td style="text-align: left;">OnlineThriftStore</td>
</tr>
<tr>
<td style="text-align: left;">PeerTutoringNetwork</td>
<td style="text-align: left;">PersonalBlog</td>
<td style="text-align: left;">PersonalFinanceBlog</td>
</tr>
<tr>
<td style="text-align: left;">RecipeHub</td>
<td style="text-align: left;">RemoteInternshipMarketplace</td>
<td style="text-align: left;">RemoteJobBoard</td>
</tr>
<tr>
<td style="text-align: left;">TravelDiary</td>
<td style="text-align: left;">VirtualBookPublishing</td>
<td style="text-align: left;">VirtualWellnessRetreats</td>
</tr>
<tr>
<td style="text-align: left;">DigitalArtworkGallery</td>
<td style="text-align: left;">DigitalStorytellingPlatform</td>
<td style="text-align: left;">ExpenseTracker</td>
</tr>
<tr>
<td style="text-align: left;">FitnessChallenges</td>
<td style="text-align: left;">GardeningForBeginners</td>
<td style="text-align: left;">GourmetFoodSubscription</td>
</tr>
<tr>
<td style="text-align: left;">MovieRecommendationSystem</td>
<td style="text-align: left;">MusicCollaborator</td>
<td style="text-align: left;">OnlineCulturalExchange</td>
</tr>
<tr>
<td style="text-align: left;">OnlineCulturalFestivals</td>
<td style="text-align: left;">OnlineVintageMarket</td>
<td style="text-align: left;">ParentingAdviceForum</td>
</tr>
<tr>
<td style="text-align: left;">PetCareCommunity</td>
<td style="text-align: left;">PortfolioSite</td>
<td style="text-align: left;">SkillShare</td>
</tr>
<tr>
<td style="text-align: left;">TaskManager</td>
<td style="text-align: left;">VolunteerMatch</td>
<td style="text-align: left;">OnlineShoppingCenter</td>
</tr>
</tbody>
</table>
<p>Table 5: Overview of Games in rSDE-Bench.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Games</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Balls</td>
<td style="text-align: center;">Tank</td>
<td style="text-align: center;">Racing</td>
<td style="text-align: center;">Ghostly</td>
</tr>
<tr>
<td style="text-align: left;">Mario</td>
<td style="text-align: center;">Bomberman</td>
<td style="text-align: center;">Sokoban</td>
<td style="text-align: center;">Brick</td>
</tr>
</tbody>
</table>
<p>Software description</p>
<div class="codehilite"><pre><span></span><code><span class="n">Task</span><span class="o">:</span><span class="w"> </span><span class="n">Develop</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">simple</span><span class="w"> </span><span class="n">Sokoban</span><span class="w"> </span><span class="n">game</span><span class="o">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">design</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">GUI</span><span class="o">.</span>
<span class="n">Requirements</span><span class="o">:</span>
<span class="mi">1</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">game</span><span class="w"> </span><span class="n">board</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">divided</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">grid</span><span class="w"> </span><span class="n">squares</span><span class="o">.</span>
<span class="mi">2</span><span class="o">.</span><span class="w"> </span><span class="n">Players</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">control</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">game</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">arrow</span><span class="w"> </span><span class="n">keys</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">keyboard</span><span class="o">.</span>
<span class="mi">3</span><span class="o">.</span><span class="w"> </span><span class="n">As</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">game</span><span class="w"> </span><span class="n">starts</span><span class="o">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">log</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">named</span><span class="w"> </span><span class="s1">&#39;game.log&#39;</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">created</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">record</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">game</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">progress</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">content</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span>
<span class="n">game</span><span class="o">.</span><span class="na">log</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">appended</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">entry</span><span class="w"> </span><span class="n">after</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">player</span><span class="w"> </span><span class="n">action</span><span class="o">.</span><span class="na">The</span><span class="w"> </span><span class="n">content</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">game</span><span class="o">.</span><span class="na">log</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span>
<span class="n">cleared</span><span class="w"> </span><span class="o">(</span><span class="k">if</span><span class="w"> </span><span class="n">any</span><span class="o">)</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">start</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">game</span><span class="w"> </span><span class="n">session</span><span class="o">.</span>
<span class="n">Each</span><span class="w"> </span><span class="n">log</span><span class="w"> </span><span class="n">entry</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">follow</span><span class="w"> </span><span class="k">this</span><span class="w"> </span><span class="n">format</span><span class="o">:</span>
<span class="o">{</span>
<span class="s2">&quot;timestamp&quot;</span><span class="o">:</span><span class="w"> </span><span class="n">timestamp</span><span class="o">,</span>
<span class="s2">&quot;EVENY_TYPE&quot;</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;MOVE_REIGHT&quot;</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="s2">&quot;MOVE_LEFT&quot;</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="s2">&quot;MOVE_UP&quot;</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="s2">&quot;MOVE_DOWN&quot;</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="s2">&quot;INVALID_MOVE&quot;</span><span class="o">,</span>
<span class="s2">&quot;player_position&quot;</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="n">x</span><span class="o">,</span><span class="w"> </span><span class="n">y</span><span class="o">],</span>
<span class="s2">&quot;box_positions&quot;</span><span class="o">:</span><span class="w"> </span><span class="o">[[</span><span class="n">x1</span><span class="o">,</span><span class="w"> </span><span class="n">y1</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">x2</span><span class="o">,</span><span class="w"> </span><span class="n">y2</span><span class="o">],</span><span class="w"> </span><span class="o">...],</span>
<span class="s2">&quot;game_status&quot;</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;ONGOING&quot;</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="s2">&quot;COMPLETE&quot;</span>
<span class="o">}</span>
<span class="mi">4</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">victory</span><span class="w"> </span><span class="n">conditions</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">game</span><span class="w"> </span><span class="k">is</span><span class="o">:</span><span class="w"> </span><span class="n">All</span><span class="w"> </span><span class="n">boxes</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">pushed</span><span class="w"> </span><span class="n">onto</span><span class="w"> </span><span class="n">their</span><span class="w"> </span><span class="n">corresponding</span><span class="w"> </span><span class="n">coordinate</span><span class="w"> </span><span class="n">point</span><span class="o">.</span>
<span class="mi">5</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">initial</span><span class="w"> </span><span class="n">positions</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">element</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">required</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">follows</span><span class="o">:</span>
<span class="n">player_position</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="mi">3</span><span class="o">,</span><span class="w"> </span><span class="mi">1</span><span class="o">]</span>
<span class="n">box_positions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[[</span><span class="mi">3</span><span class="o">,</span><span class="w"> </span><span class="mi">3</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="mi">4</span><span class="o">,</span><span class="w"> </span><span class="mi">2</span><span class="o">]]</span>
<span class="n">goal_positions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[[</span><span class="mi">5</span><span class="o">,</span><span class="w"> </span><span class="mi">5</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="mi">6</span><span class="o">,</span><span class="w"> </span><span class="mi">3</span><span class="o">]]</span>
<span class="o">([</span><span class="mi">3</span><span class="o">,</span><span class="w"> </span><span class="mi">3</span><span class="o">]</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">initial</span><span class="w"> </span><span class="n">position</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">box</span><span class="w"> </span><span class="n">whose</span><span class="w"> </span><span class="n">target</span><span class="w"> </span><span class="n">position</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="o">[</span><span class="mi">5</span><span class="o">,</span><span class="w"> </span><span class="mi">5</span><span class="o">].</span><span class="w"> </span><span class="o">[</span><span class="mi">4</span><span class="o">,</span><span class="w"> </span><span class="mi">2</span><span class="o">]</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">initial</span><span class="w"> </span><span class="n">position</span><span class="w"> </span><span class="n">of</span>
<span class="n">the</span><span class="w"> </span><span class="n">second</span><span class="w"> </span><span class="n">box</span><span class="w"> </span><span class="n">whose</span><span class="w"> </span><span class="n">target</span><span class="w"> </span><span class="n">position</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="o">[</span><span class="mi">6</span><span class="o">,</span><span class="w"> </span><span class="mi">3</span><span class="o">].)</span>
<span class="n">wall_positions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[[</span><span class="mi">8</span><span class="o">,</span><span class="w"> </span><span class="mi">4</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="mi">1</span><span class="o">,</span><span class="w"> </span><span class="mi">4</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="mi">2</span><span class="o">,</span><span class="w"> </span><span class="mi">4</span><span class="o">],[</span><span class="mi">3</span><span class="o">,</span><span class="w"> </span><span class="mi">4</span><span class="o">],[</span><span class="mi">4</span><span class="o">,</span><span class="w"> </span><span class="mi">4</span><span class="o">]]</span>
<span class="o">(</span><span class="n">the</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">numnber</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">pair</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">x</span><span class="o">-</span><span class="n">coordinate</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">second</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">y</span><span class="o">-</span><span class="n">coordinate</span><span class="o">)</span>
</code></pre></div>

<p>Evaluation functions</p>
<div class="codehilite"><pre><span></span><code>check_Excutability check_log check_move_right check_move_left check_move_box
check_move_wall check_seqbox check_end check_wrong_end
</code></pre></div>

<p>Figure 10: Test cases of Game in rSDE-Bench.</p>
<h1>8 Algorithm</h1>
<p>In this section, we present the algorithm of EvoMAC in Alg. 1. For more details, please refer to Section 3.</p>
<h1>Software description</h1>
<p># Requirement Document for DailyHealthTips Web Application
## 1. Objective
Develop a web application named 'DailyHealthTips' that provides users with daily health tips, allowing them to receive advice and information about maintaining a healthy lifestyle, using Python as the development language. Note that the website should start from the login page.
## 2. Language
The required development language for the DailyHealthTips web application is Python.
## 3. Page Design
### Page 1: Login Page</p>
<ul>
<li><strong>Page Title</strong>: User Login</li>
<li><strong>Overview</strong>: This page allows users to log in to their accounts.</li>
<li><strong>Elements</strong>: $\square$</li>
<li><strong>Username Field</strong>: $\square$</li>
<li><strong>ID</strong>: username_field</li>
<li><strong>Password Field</strong>: $\square$</li>
<li><strong>ID</strong>: password_field</li>
<li><strong>Login Button</strong>: $\square$</li>
<li><strong>ID</strong>: <code>login_button</code></li>
</ul>
<p>Evaluation functions
test_login_page_elements test_login_page_functionality test_daily_tips_page_elements
test_daily_tips_page_functionality test_tips_archive_page_elements test_tips_archive_page_functionality</p>
<p>Figure 11: Test cases of Website in rSDE-Bench.</p>
<h2>Algorithm 1 Self-Evolving Paradigm</h2>
<h2>Require: X</h2>
<p>$\triangleright$ Task input
Require: $\mathcal{A}<em t="t">{g}^{(0)}$
$\triangleright$ Initialized MAC network: agent prompts and pipeline
Require: $\mathcal{A}</em>$
$\triangleright$ Designed MAC network to generate target proxy
Require: $\mathcal{G}$
$\triangleright$ Agent-based gradient function
Require: $\mathcal{U}$
$\triangleright$ Agent-based update function
Require: $E$
$\triangleright$ Environment tool to generate loss
1: Define $K$ as the number of self-evolving iterations, $\Phi$ as MACN generation process
2: # Target Proxy
3: $\mathbf{T}=\Phi\left(\mathbf{X}, \mathcal{A}<em g="g">{t}\right)$
4: # Self-Evolving Procedure
5: for $k=0,1, \ldots, K-1$ do
6: # Forward Pass
7: $\quad \mathbf{G}^{(k)}=\Phi\left(\mathbf{X}, \mathcal{A}</em>\right)$
8: # Loss Computation
9: $\quad \mathbf{L}^{(k)}=\left\langle\mathbf{G}^{(k)}, \mathbf{T}\right\rangle_{E}$
$\triangleright$ Use environment feedback as textual loss
10: # Textual Backpropagation
11: $\quad \nabla \mathbf{L}^{(k)}=\mathcal{G}\left(\mathbf{L}^{(k)}, \mathcal{A}}^{(k)<em g="g">{g}^{(k)}\right)$
$\triangleright$ Summarize textual gradient
12: $\quad \mathcal{A}</em>}^{(k+1)}=\mathcal{U}\left(\mathcal{A<em g="g">{g}^{(k)}, \nabla \mathbf{L}^{(k)}\right)$
13: end for
14: return $\mathcal{A}</em>$}^{(K)}, \mathbf{G}^{(K)</p>
<h2>9 Prompts</h2>
<p>In this section, we present the agents' prompts in EvoMAC, including coding organizer (Tab. 6), coding agent (Tab. 7), testing organizer (Tab. 8), testing agent (Tab. 9), gradient agent (Tab. 10), and update agent (Tab. 11).</p>
<h2>10 Software Presentation</h2>
<p>In this section, we show some games and websites written by EvoMAC. Fig. 12 and Fig. 13 present the games and websites respectively. We see that: i) EvoMAC outputs games with well-written GUI</p>
<p>Table 6: Coding Organizer</p>
<div class="codehilite"><pre><span></span><code><span class="n">According</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">user</span><span class="p">&#39;</span><span class="n">s</span><span class="w"> </span><span class="k">task</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">our</span><span class="w"> </span><span class="n">software</span><span class="w"> </span><span class="n">designs</span><span class="w"> </span><span class="n">listed</span><span class="w"> </span><span class="nl">below:</span>
<span class="nl">Task:</span><span class="w"> </span><span class="s">&quot;{task}&quot;</span>
<span class="n">Task</span><span class="w"> </span><span class="nl">description:</span><span class="w"> </span><span class="s">&quot;{description}&quot;</span>
<span class="nl">Modality:</span><span class="w"> </span><span class="s">&quot;{modality}&quot;</span>
<span class="n">Programming</span><span class="w"> </span><span class="nl">Language:</span><span class="w"> </span><span class="s">&quot;{language}&quot;</span>
<span class="n">Requirements</span><span class="w"> </span><span class="nl">analysis:</span><span class="w"> </span><span class="s">&quot;{requirements}&quot;</span>
<span class="nl">Ideas:</span><span class="s">&quot;{ideas}&quot;</span>
<span class="n">Coding</span><span class="w"> </span><span class="nl">plan:</span><span class="w"> </span><span class="s">&quot;{codes}&quot;</span>
<span class="n">Your</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">organize</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">coding</span><span class="w"> </span><span class="n">team</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">complete</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">software</span><span class="w"> </span><span class="n">development</span><span class="w"> </span><span class="k">task</span><span class="p">.</span>
<span class="n">There</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="nl">tasks:</span>
<span class="mh">1</span><span class="p">)</span><span class="w"> </span><span class="n">log</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">user</span><span class="p">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">actions</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">events</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">game</span><span class="p">.</span><span class="n">log</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">according</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">task</span><span class="w"> </span><span class="n">requirements</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">write</span>
<span class="n">format</span><span class="o">!</span><span class="w"> </span><span class="n">Be</span><span class="w"> </span><span class="n">careful</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">make</span><span class="w"> </span><span class="n">sure</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">maintain</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">game</span><span class="p">.</span><span class="n">log</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">right</span><span class="o">!</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">log</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">happened</span><span class="w"> </span><span class="n">after</span>
<span class="n">the</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">taken</span><span class="p">,</span><span class="w"> </span><span class="n">record</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">most</span><span class="w"> </span><span class="n">recent</span><span class="w"> </span><span class="n">state</span><span class="p">.</span>
<span class="mh">2</span><span class="p">)</span><span class="w"> </span><span class="n">create</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">user</span><span class="w"> </span><span class="n">interface</span><span class="w"> </span><span class="p">(</span><span class="n">GUI</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">game</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">programming</span><span class="w"> </span><span class="n">language</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">requirements</span>
<span class="n">analysis</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">GUI</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">beautiful</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">user</span><span class="o">-</span><span class="n">friendly</span><span class="p">.</span>
<span class="n">Besides</span><span class="w"> </span><span class="n">these</span><span class="w"> </span><span class="n">tasks</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">pay</span><span class="w"> </span><span class="n">attention</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">unachieved</span><span class="w"> </span><span class="n">requirements</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">think</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">to</span>
<span class="n">formulate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">requirements</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">concrete</span><span class="w"> </span><span class="n">tasks</span><span class="p">.</span>
<span class="n">You</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">follow</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="nl">format:</span><span class="w"> </span><span class="n">COMPOSITIONīs</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">composition</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">tasks</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">Workflowīs</span>
<span class="n">the</span><span class="w"> </span><span class="n">workflow</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">programmers</span><span class="p">.</span><span class="w"> </span><span class="n">Each</span><span class="w"> </span><span class="k">task</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">assigned</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">programmer</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">workflow</span><span class="w"> </span><span class="n">shows</span><span class="w"> </span><span class="n">the</span>
<span class="n">dependencies</span><span class="w"> </span><span class="n">between</span><span class="w"> </span><span class="n">tasks</span><span class="p">.</span>
<span class="p">###</span><span class="w"> </span><span class="n">COMPOSITION</span>
<span class="s">&quot;</span>
<span class="n">Task</span><span class="w"> </span><span class="mh">1</span><span class="o">:</span><span class="w"> </span><span class="n">Task</span><span class="w"> </span><span class="mh">1</span><span class="w"> </span><span class="n">description</span>
<span class="n">Task</span><span class="w"> </span><span class="mh">2</span><span class="o">:</span><span class="w"> </span><span class="n">Task</span><span class="w"> </span><span class="mh">2</span><span class="w"> </span><span class="n">description</span>
<span class="s">&quot;&quot;</span>
<span class="p">###</span><span class="w"> </span><span class="n">WORKFLOW</span>
<span class="s">&quot;</span>
<span class="n">Task</span><span class="w"> </span><span class="mh">1</span><span class="o">:</span><span class="w"> </span><span class="p">[]</span>
<span class="n">Task</span><span class="w"> </span><span class="mh">2</span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="n">Task</span><span class="w"> </span><span class="mh">1</span><span class="p">]</span>
<span class="s">&quot;</span>
<span class="n">Please</span><span class="w"> </span><span class="n">note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">decomposition</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">both</span><span class="w"> </span><span class="n">effective</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">efficient</span><span class="p">.</span>
<span class="mh">1</span><span class="p">)</span><span class="w"> </span><span class="n">Each</span><span class="w"> </span><span class="n">decomposed</span><span class="w"> </span><span class="k">task</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">include</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">related</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">functions</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="k">task</span><span class="w"> </span><span class="n">description</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">clear</span>
<span class="k">and</span><span class="w"> </span><span class="n">concise</span><span class="p">.</span>
<span class="mh">2</span><span class="p">)</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">composition</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">kept</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="k">small</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">possible</span><span class="o">!</span><span class="w"> </span><span class="p">(</span><span class="n">LESS</span><span class="w"> </span><span class="n">THAN</span><span class="w"> </span><span class="s">&quot;{num_agents}&quot;</span><span class="p">).</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">are</span>
<span class="n">more</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="mh">5</span><span class="w"> </span><span class="n">tasks</span><span class="p">,</span><span class="w"> </span><span class="n">consider</span><span class="w"> </span><span class="n">merging</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">tasks</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">focus</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">most</span><span class="w"> </span><span class="n">essential</span><span class="w"> </span><span class="n">features</span><span class="p">.</span>
<span class="mh">3</span><span class="p">)</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">decomposed</span><span class="w"> </span><span class="n">tasks</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">fully</span><span class="w"> </span><span class="n">cover</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">task</span><span class="w"> </span><span class="n">definitions</span><span class="p">.</span>
<span class="mh">4</span><span class="p">)</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">workflow</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">contain</span><span class="w"> </span><span class="n">circles</span><span class="o">!</span>
</code></pre></div>

<p>and game rules. It can handle different kinds of GUI and game rule requirements from diverse games. ii) EvoMAC outputs websites with beautified, user-friendly web pages and correct transition logic. It can handle the requirements of different websites.</p>
<p>Table 7: Coding Agent</p>
<p>According to the new user's task and our software designs listed below:
Task: "[task]".
Modality: "[modality]".
Programming Language: "[language]"
Sub-Task description: "[subtask]"
Codes:
" {codes }"
Unimplemented File:
"{unimplemented_file}"
Your first think step by step first reason yourself about the files and functions related to the sub-task.
Then you should output the COMPLETE code content in each file. Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that "FILENAME" is the lowercase file name including the file extension, "LANGUAGE" in the programming language, "DOCSTRING" is a string literal specified in source code that is used to document a specific segment of code, and "CODE" is the original code. Format:
FILENAME
"LANGUAGE
""
DOCSTRING
""
CODE
""
Implementation Requirements:</p>
<ol>
<li>As the assistant_role, to satisfy the complete function of our developed software, you have to implement all functions which are related to the subtask.</li>
<li>If the function is implemented, recheck the logic and log to ensure the targeted feature is fully achieved</li>
<li>Important: that both the logic and log should be fully functional! No placeholder (such as 'pass' in Python), strictly following the required format. You must strictly following the required format. You must strictly following the required format.</li>
<li>Ensure the functions are consistent among different files, and correctly imported.</li>
</ol>
<p>Additional Note: additional_note</p>
<h1>Table 8: Testing Organizer</h1>
<p>According to the software requirements listed below:
Task: "{task}".
Modality: "{modality}".
Programming Language: "{language}"
Your goal is to organize a testing team to complete the software development task.
There are four default tasks:</p>
<p>1) carefully test the logging mechanism according to the task requirements! The log should happened immediately after the action is taken, record the most recent state. Remember the logging order is very important, record basic operation first then record the subsequent events. Ensure the data format, keys and values are accurate and right! Pay attention to the nested data type and carefully check each element.
2) test the logging mechanism for the special triggered conditions.
3) test the value initialziation required by the task are correctly achieved, pay attention to the corrdinates.
4) test the function inputs and the global variable are imported in each functions, ensure the input values and global variable used in the function are valid and involved when the function is called.
5) test each event in the task is implemented and that the logic triggered matches the conditions in the task description.
Follow the format: "COMPOSITION" is the composition of tasks, and "Workflow" is the workflow of the programmers.
### COMPOSITION
"
Task 1: Task 1 description
Task 2: Task 2 description
""
### WORKFLOW
""
Task 1: []
Task 2: [Task 1]
""</p>
<h1>Table 9: Testing Agent</h1>
<p>Our software requirements and developed source codes are listed below:
Programming Language: "{language }"
Source Codes:
"{codes}"
Testing Task description: "{subtask}"
According to Testing Task description, please write test cases to locate the bugs, note that logging is important, ensure the content and format is right.
You should first locate the functions that need to be tested and write the test cases for them according to the testing task description.
The output must strictly follow a markdown code block format, where the following tokens must be replaced such that "FILENAME" is "test_file_name", "LANGUAGE" in the programming language,"REQUIREMENTS" is the targeted requirement of the test case, and "CODE" is the test code that is used to test the specific requirement of the file. Format:
FILENAME
"LANGUAGE
"
REQUIREMENTS
"'
CODE
You will start with the "test_file_name" and finish the code follows in the strictly defined format. Please note that:</p>
<p>1) The code should be fully functional. Ensure to implement all functions. No placeholders (such as 'pass' in Python).
2) You should not write anything about log testing unless testing task description clearly state that the logs need to be tested
3) You should write the test file with 'unittest' python library.
4) You should not modify the source code, only write the test code. Very Important!</p>
<p>Table 10: Gradient Agent</p>
<div class="codehilite"><pre><span></span><code><span class="n">Our</span><span class="w"> </span><span class="n">developed</span><span class="w"> </span><span class="n">source</span><span class="w"> </span><span class="n">codes</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">corresponding</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">reports</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">listed</span><span class="w"> </span><span class="nl">below</span><span class="p">:</span>
<span class="n">Programming</span><span class="w"> </span><span class="k">Language</span><span class="err">:</span><span class="w"> </span><span class="ss">&quot;[language]&quot;</span>
<span class="k">User</span><span class="w"> </span><span class="nl">requirement</span><span class="p">:</span>
<span class="ss">&quot; {task}&quot;</span>
<span class="n">Source</span><span class="w"> </span><span class="nl">Codes</span><span class="p">:</span>
<span class="ss">&quot; {codes}&quot;</span>
<span class="n">The</span><span class="w"> </span><span class="n">execution</span><span class="w"> </span><span class="n">outcome</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">our</span><span class="w"> </span><span class="n">source</span><span class="w"> </span><span class="nl">codes</span><span class="p">:</span>
<span class="ss">&quot; {test_reports}&quot;</span>
<span class="n">We</span><span class="w"> </span><span class="n">also</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="k">case</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">our</span><span class="w"> </span><span class="n">source</span><span class="w"> </span><span class="n">codes</span><span class="p">,</span><span class="w"> </span><span class="n">our</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">codes</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">listed</span><span class="w"> </span><span class="nl">below</span><span class="p">:</span>
<span class="ss">&quot; {test_codes}&quot;</span>
<span class="ow">And</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">execution</span><span class="w"> </span><span class="n">outcome</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">our</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">codes</span><span class="w"> </span><span class="k">is</span><span class="err">:</span>
<span class="ss">&quot; {testcase_reports}&quot;</span>
<span class="n">According</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">these</span><span class="w"> </span><span class="n">imformation</span><span class="p">,</span><span class="w"> </span><span class="n">please</span><span class="w"> </span><span class="n">analyze</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">source</span><span class="w"> </span><span class="n">code</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">code</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">execution</span><span class="w"> </span><span class="n">reports</span><span class="p">.</span><span class="w"> </span><span class="n">Make</span>
<span class="n">sure</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">analysis</span><span class="w"> </span><span class="n">aligns</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">source</span><span class="w"> </span><span class="n">code</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">user</span><span class="w"> </span><span class="n">requirements</span><span class="p">.</span>
<span class="k">First</span><span class="p">,</span><span class="w"> </span><span class="n">determine</span><span class="w"> </span><span class="n">whether</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">caused</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">incorrect</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">code</span><span class="p">;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">so</span><span class="p">,</span><span class="w"> </span><span class="n">respond</span><span class="w"> </span><span class="ss">&quot;Wrong test code.&quot;</span>
<span class="n">The</span><span class="w"> </span><span class="n">wrong</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">code</span><span class="w"> </span><span class="n">includes</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">matching</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">user</span><span class="w"> </span><span class="n">requirement</span><span class="p">,</span><span class="w"> </span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="w"> </span><span class="n">wrong</span><span class="w"> </span><span class="k">value</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">test</span>
<span class="n">reference</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">conflict</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="k">user</span><span class="w"> </span><span class="n">requirement</span><span class="w"> </span><span class="n">desciption</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">improper</span><span class="w"> </span><span class="k">use</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">source</span><span class="w"> </span><span class="n">code</span><span class="p">.</span><span class="w"> </span><span class="n">Please</span><span class="w"> </span><span class="n">be</span>
<span class="n">careful</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">make</span><span class="w"> </span><span class="n">wrong</span><span class="w"> </span><span class="n">judgment</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">source</span><span class="w"> </span><span class="n">code</span><span class="p">.</span>
<span class="k">Second</span><span class="p">,</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">exist</span><span class="w"> </span><span class="n">bugs</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">source</span><span class="w"> </span><span class="n">code</span><span class="p">,</span><span class="w"> </span><span class="n">give</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">detailed</span><span class="w"> </span><span class="n">analysis</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">problem</span><span class="p">.</span><span class="w"> </span><span class="n">Your</span><span class="w"> </span><span class="n">answer</span>
<span class="n">MUST</span><span class="w"> </span><span class="n">follow</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nf">format</span><span class="w"> </span><span class="nl">below</span><span class="p">:</span>
<span class="k">file</span><span class="w"> </span><span class="nl">name</span><span class="p">:</span><span class="n">file_1</span><span class="p">.</span><span class="n">py</span>
<span class="k">function</span><span class="w"> </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">function_1</span><span class="p">,</span><span class="w"> </span><span class="n">function_2</span>
<span class="n">detailed</span><span class="w"> </span><span class="n">analysis</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nl">problem</span><span class="p">:</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">analysis</span>
<span class="k">file</span><span class="w"> </span><span class="nl">name</span><span class="p">:</span><span class="n">file_2</span><span class="p">.</span><span class="n">py</span>
<span class="k">function</span><span class="w"> </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">function_3</span><span class="p">,</span><span class="w"> </span><span class="n">function_4</span>
<span class="n">detailed</span><span class="w"> </span><span class="n">analysis</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nl">problem</span><span class="p">:</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">analysis</span>
<span class="n">Your</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">also</span><span class="w"> </span><span class="n">follow</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">requirements</span><span class="w"> </span><span class="nl">below</span><span class="p">:</span>
<span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="k">include</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">analysis</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">wrong</span><span class="w"> </span><span class="n">source</span><span class="w"> </span><span class="n">code</span><span class="p">.</span><span class="w"> </span><span class="n">Please</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">include</span><span class="w"> </span><span class="n">analysis</span><span class="w"> </span><span class="k">of</span>
<span class="n">the</span><span class="w"> </span><span class="n">testcase</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="n">here</span><span class="p">.</span><span class="w"> </span><span class="k">If</span><span class="w"> </span><span class="ow">all</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">reports</span><span class="w"> </span><span class="n">show</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">no</span><span class="w"> </span><span class="n">bugs</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">source</span><span class="w"> </span><span class="n">codes</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">codes</span><span class="p">,</span><span class="w"> </span><span class="n">you</span>
<span class="n">should</span><span class="w"> </span><span class="n">just</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="nl">reply</span><span class="p">:</span><span class="w"> </span><span class="k">No</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">codes</span><span class="p">.</span>
<span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="k">than</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">function</span><span class="w"> </span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">file</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="nc">time</span><span class="p">.</span><span class="w"> </span><span class="k">If</span>
<span class="n">you</span><span class="w"> </span><span class="n">want</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="k">file</span><span class="w"> </span><span class="k">names</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">split</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nf">format</span><span class="w"> </span><span class="n">respectively</span><span class="p">.(</span><span class="n">Answer</span>
<span class="n">the</span><span class="w"> </span><span class="n">file_1</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">corresponding</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nf">format</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">file_2</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">corre</span><span class="o">-</span>
<span class="n">sponding</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nf">format</span><span class="p">)</span>
<span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">may</span><span class="w"> </span><span class="k">include</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="k">function</span><span class="w"> </span><span class="k">names</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="k">file</span><span class="p">,</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">include</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">same</span>
<span class="k">function</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">different</span><span class="w"> </span><span class="n">files</span><span class="p">.</span>
<span class="mi">4</span><span class="p">)</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">anything</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="k">file</span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="w"> </span><span class="k">file</span><span class="w"> </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">test_requirement_0</span><span class="p">.</span><span class="n">py</span><span class="p">)</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">answer</span><span class="p">.</span>
<span class="n">VERY</span><span class="w"> </span><span class="n">IMPORTANT</span><span class="err">!</span>
</code></pre></div>            </div>
        </div>

    </div>
</body>
</html>
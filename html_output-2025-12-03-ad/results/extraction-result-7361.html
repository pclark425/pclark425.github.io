<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7361 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7361</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7361</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-270391790</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.07815v2.pdf" target="_blank">Are Large Language Models Good Statisticians?</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated impressive capabilities across a range of scientific tasks including mathematics, physics, and chemistry. Despite their successes, the effectiveness of LLMs in handling complex statistical tasks remains systematically under-explored. To bridge this gap, we introduce StatQA, a new benchmark designed for statistical analysis tasks. StatQA comprises 11,623 examples tailored to evaluate LLMs' proficiency in specialized statistical tasks and their applicability assessment capabilities, particularly for hypothesis testing methods. We systematically experiment with representative LLMs using various prompting strategies and show that even state-of-the-art models such as GPT-4o achieve a best performance of only 64.83%, indicating significant room for improvement. Notably, while open-source LLMs (e.g. LLaMA-3) show limited capability, those fine-tuned ones exhibit marked improvements, outperforming all in-context learning-based methods (e.g. GPT-4o). Moreover, our comparative human experiments highlight a striking contrast in error types between LLMs and humans: LLMs primarily make applicability errors, whereas humans mostly make statistical task confusion errors. This divergence highlights distinct areas of proficiency and deficiency, suggesting that combining LLM and human expertise could lead to complementary strengths, inviting further investigation into their collaborative potential. Our source code and data are available at https://statqa.github.io/.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7361.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7361.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 7B (chat variant evaluated: Llama-2-7b-chat-hf)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7-billion-parameter LLaMA-2 chat model evaluated as a text-based simulator of a statistician: given column metadata and a question it must select relevant columns and applicable statistical methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7b-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction / chat-tuned base model (not fine-tuned on StatQA in the primary eval)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics (statistical analysis — descriptive statistics and hypothesis testing)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Given column metadata and a natural-language statistical question, select the relevant data columns and all applicable statistical methods (assess prerequisites/applicability) — i.e., simulate a text-based statistician making method selection decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot, one-shot (few-shot) with a single example per task category, optional Chain-of-Thought (CoT) prompting, and a variant with domain knowledge (DK) text inserted in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Acc(C, M): exact-match accuracy of both selected columns and selected applicable methods (percentage of examples with full match).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Best reported overall Acc(C, M) in Table 2: 17.97% (1-shot) / 16.08% (1-shot+DK) depending on strategy; baseline zero-shot 8.08% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Zero-shot: 8.08% overall Acc(C, M).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['model scale (small model struggles)', 'prompting strategy (one-shot improves vs zero-shot)', 'Chain-of-Thought (CoT) often degrades performance for small models', 'adding domain knowledge (DK) sometimes helps but can have mixed effects', 'difficulty of example (easy vs hard)', 'type of task (some categories harder, e.g., contingency tests)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluations used temperature=0 for all LLMs; top_p=1.0 for LLaMA models; column metadata (header, n_rows, data_type, normality) provided instead of full tables; vLLM framework on NVIDIA 4090 GPUs for open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Very low baseline performance; prone to invalid answers, column-selection errors, and applicability errors; CoT prompting can increase invalid/mis-selection errors in small models; limited capability to assess method applicability even when DK is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Good Statisticians?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7361.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7361.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 13B (chat variant evaluated: Llama-2-13b-chat-hf)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger open-source LLaMA-2 model (13B) evaluated on StatQA for selecting relevant columns and applicable statistical methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-13b-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction / chat-tuned base model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics (statistical analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based selection of relevant columns and applicable methods for statistical questions (assessing prerequisites and applicability).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot, one-shot, CoT, and one-shot with domain knowledge (DK).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Acc(C, M) exact-match accuracy (columns + methods).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Best reported overall Acc(C, M) in Table 2: 31.90% (1-shot); zero-shot 9.03%. 1-shot+DK reported 20.29% in another configuration (table shows results by strategy and task).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Zero-shot: 9.03% overall Acc(C, M).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['model scale (improvement vs 7B)', 'few-shot examples (1-shot gave ~+9.6% on average for LLaMA models)', 'CoT had small negative effect for some LLaMA variants', 'domain knowledge prompting (DK) could help but effect varies by task']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>temperature=0, top_p=1.0 for LLaMA models; evaluation on mini-StatQA and StatQA; column metadata provided. vLLM on NVIDIA 4090.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Still far below best proprietary models; susceptible to applicability and column-selection errors; DK can sometimes reduce performance on simpler tasks (information obfuscation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Good Statisticians?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7361.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7361.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta LLaMA-3 8B (Meta-Llama-3-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B-parameter LLaMA-3 model evaluated both in base and instruction variants on StatQA; shows marked improvement over LLaMA-2 variants but remains weaker than best proprietary LLMs unless fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta-Llama-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>base and instruction variants (Meta-Llama-3-8B and Meta-Llama-3-8B-Instruct), both evaluated; also fine-tuned variants exist (LoRA).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics (statistical analysis, hypothesis testing & descriptive statistics)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Select relevant columns and applicable statistical methods for natural-language statistical questions, including checking prerequisites (normality, sample size, data type).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot, one-shot, CoT, and one-shot with domain knowledge (DK); also evaluated after LoRA fine-tuning on StatQA training set.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Acc(C, M) (exact-match percent of correct columns and methods).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Best non-fine-tuned reported overall Acc(C, M): 36.11% (1-shot+DK) according to Table 2; zero-shot reported 22.01% in one line. Fine-tuned (SFT LLaMA-3-8B) achieved 77.13% (0-shot after fine-tuning) in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Zero-shot: 22.01% (non-fine-tuned base), one-shot baseline gave ~32.24% depending on config.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['model scale/architecture (LLaMA-3 improved over LLaMA-2)', 'fine-tuning (LoRA) dramatically increases performance', 'prompting: one-shot and DK helpful; CoT can slightly reduce performance on small models', 'task type—DK particularly helps less-common tasks (DCT, CTT, VT)', 'difficulty (easy vs hard examples)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>temperature=0, top_p=1.0 for LLaMA models; fine-tuning done via LoRA for 3 epochs, batch size 16, Adam optimizer, LR schedule up to 5e-5 then cosine anneal, training on NVIDIA A800 (80G).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Non-fine-tuned model still makes many applicability errors; fine-tuned models perform much better but dataset and evaluation focus only on method selection/applicability; CoT can increase invalid answers in smaller setups.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Good Statisticians?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7361.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7361.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3-8B-Instruct (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-Llama-3-8B-Instruct (LoRA fine-tuned on StatQA, reported as SFT LLaMA-3 8B Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned LLaMA-3 8B variant further fine-tuned with LoRA on StatQA training data; achieves the strongest open-source fine-tuned performance on StatQA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta-Llama-3-8B-Instruct (LoRA fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned model further fine-tuned on StatQA via LoRA (parameter-efficient fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics (statistical method applicability selection)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>After fine-tuning, select relevant columns and applicable statistical methods with high accuracy; simulate a statistician's applicability judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Evaluated mostly 0-shot after fine-tuning (the fine-tuned model was provided the standard prompt structure used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Acc(C, M) exact-match accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Fine-tuned (SFT LLaMA-3-8B Instruct) reported overall Acc(C, M) = 75.92% (0-shot after fine-tuning) in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Non-fine-tuned Meta-Llama-3-8B-Instruct baseline ~27.77% (1-shot+DK line) or lower depending on strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['fine-tuning on StatQA (LoRA) is a major positive factor', 'model architecture and instruction tuning', 'availability of training data synthesized from tables (no overlap between train and test tables)', 'prompt format and strict JSON output requirement']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Fine-tuning: LoRA, 3 epochs, batch size 16, Adam, lr ramp to 5e-5 then cosine anneal to 0; hardware NVIDIA A800 (80G). Evaluation temp=0.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Although fine-tuned models perform much better, the paper notes remaining room for improvement and that performance is measured only on applicability/selection (not full statistical computation); may overfit synthetic train distribution if extended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Good Statisticians?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7361.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7361.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used proprietary conversational LLM; evaluated as a text-based simulator for statistical method selection on StatQA using prompting variants (0-shot/1-shot/CoT/DK).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (not disclosed in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>proprietary instruction-following conversational model (no StatQA fine-tuning reported)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics (statistical analysis applicability selection)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Select relevant columns and applicable statistical tests for a provided statistical question and column metadata; optionally provide reasoning when instructed.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot, one-shot (few-shot), chain-of-thought (CoT), and one-shot+domain-knowledge (DK) prompting; the DK insertion improved results significantly for GPT-3.5-Turbo.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Acc(C, M) exact-match accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Best reported overall Acc(C, M): 49.36% (1-shot+DK) in Table 2; zero-shot baseline 37.40%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Zero-shot: 37.40% overall Acc(C, M).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['prompting strategy: few-shot and DK improve performance notably', 'CoT has negligible or small effects on larger GPT models', 'model comprehension capacity; DK has more effect on larger models', 'temperature during evaluation (set to 0 for evaluations)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluation via OpenAI API; temperature=0 in evaluations (noted that GPT-3.5-Turbo temp=0.7 only during question refinement step), Chain-of-Thought experiments run as zero-shot-CoT and one-shot-CoT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Still makes many applicability errors; performance can be surpassed by fine-tuned open-source models; DK can sometimes cause more column-selection or task-confusion errors on some subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Good Statisticians?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7361.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7361.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large proprietary model evaluated on StatQA for statistical method applicability selection; strong performance when provided domain knowledge in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (not disclosed in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>proprietary, instruction-following</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics (statistical analysis & hypothesis testing applicability)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Select relevant columns and applicable statistical methods based on metadata and question; simulate a statistician's applicability judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot, one-shot, Chain-of-Thought (CoT), and one-shot+domain-knowledge (DK).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Acc(C, M) exact-match accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Best reported overall Acc(C, M): 53.22% (1-shot+DK) in Table 2; zero-shot baseline 43.34% (0-shot-CoT line) / 44.23% varies across prompt variants in table.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Zero-shot: reported around 44.23% (0-shot variants) depending on CoT usage.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['domain knowledge (DK) in prompt notably improves performance', 'few-shot examples add benefit', 'CoT has negligible impact for larger GPT models', 'task type differences (DCT, VT, CTT benefit more from DK)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluations via OpenAI API; temperature=0 for evaluations; one-shot and DK variants tested.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Despite better performance, GPT-4 still exhibits a considerable proportion of applicability errors; can be outperformed by fine-tuned open-source models on this benchmark in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Good Statisticians?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7361.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7361.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Newer GPT-series variant evaluated and shown to achieve the best non-fine-tuned performance on StatQA with domain-knowledge enhanced prompting in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (not disclosed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>proprietary instruction-following LLM</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics (statistical method applicability / selection)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based selection of columns and applicable statistical methods; simulates a statistician evaluating applicability and prerequisites.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot, one-shot, CoT, one-shot+DK (domain knowledge). The best reported result used one-shot+DK.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Acc(C, M) exact-match accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Best reported overall Acc(C, M): 64.83% (1-shot+DK) as reported in Table 2 (best non-fine-tuned model result).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Zero-shot: ~44.71% (0-shot-CoT/0-shot lines show around 44.71%).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['prompting with domain knowledge (DK) yielded substantial improvements', 'few-shot examples (one-shot) contributed positively', 'model size and pretraining/comprehension capacity', 'task category difficulty']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluation via OpenAI API, temperature=0 in evaluations; top_p not specified for GPT APIs in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Although achieving the best non-fine-tuned result, GPT-4o still makes many applicability errors and can be outperformed by fine-tuned open-source models on StatQA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Good Statisticians?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7361.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7361.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large open-source Qwen2 model (72B) evaluated in additional experiments; demonstrates competitive performance among open-source models on StatQA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>open-source instruction-tuned model (evaluated without StatQA fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics (statistical method selection / applicability)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Given column metadata and question, select relevant columns and applicable statistical methods — text-based simulation of method selection.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot and one-shot (few-shot) prompting variants.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Acc(C, M) exact-match accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Reported in Table 10: 1-shot overall Acc(C, M) = 47.12%; zero-shot = 44.71% in that table.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Zero-shot: 44.71% (Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['model scale (large model yields significant improvements over smaller open-source models)', 'few-shot prompting (one-shot improved to 47.12%)', 'pretraining coverage (privacy/closed vs open data tradeoffs)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluations reported in Appendix D (Table 10); same general evaluation protocol (temperature=0 for main evals), column metadata provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>While competitive among open-source models, still behind best fine-tuned open-source SFT results; may still produce applicability errors and task confusion depending on prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Good Statisticians?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7361.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7361.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned LLaMA-2-7B (SFT LLaMA-2 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 7B LoRA fine-tuned on StatQA (reported as SFT LLaMA-2 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA-2 7B model fine-tuned with LoRA on the StatQA training set, achieving a large improvement over its non-fine-tuned baseline on the method-selection task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7b-chat-hf (LoRA fine-tuned on StatQA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LoRA fine-tuned on StatQA training data (parameter-efficient fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics (statistical method applicability selection)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>After LoRA fine-tuning, simulate selecting columns and applicable methods for statistical questions with improved accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>After fine-tuning, evaluated typically in 0-shot format using the standard JSON-output prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Acc(C, M) exact-match.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>SFT LLaMA-2 7B reported overall Acc(C, M) = 66.72% (Table 2, 0-shot after fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Non-fine-tuned LLaMA-2-7B zero-shot baseline: 8.08%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['fine-tuning on task-specific synthetic data (StatQA) is a major positive factor', 'LoRA parameter-efficient fine-tuning strategy', 'quality and coverage of synthesized training examples (no table overlap with test)', 'prompt format consistency']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>LoRA fine-tuning for 3 epochs, batch size 16, Adam optimizer, LR ramp to 5e-5 then cosine anneal, training on NVIDIA A800 (80G). Evaluations at temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Although fine-tuned performance is strong, the approach depends on availability of curated/synthesized training examples and may not generalize beyond covered methods; paper notes room for improvement and limitations in scope (statistics-only).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Good Statisticians?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DAEval <em>(Rating: 2)</em></li>
                <li>QRData <em>(Rating: 2)</em></li>
                <li>Infiagent-dabench: Evaluating agents on data analysis tasks <em>(Rating: 2)</em></li>
                <li>MathVista: Evaluating mathematical reasoning of foundation models in visual contexts <em>(Rating: 1)</em></li>
                <li>MathVerse: Does your multi-modal llm truly see the diagrams in visual math problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7361",
    "paper_id": "paper-270391790",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "LLaMA-2-7B",
            "name_full": "LLaMA-2 7B (chat variant evaluated: Llama-2-7b-chat-hf)",
            "brief_description": "An open-source 7-billion-parameter LLaMA-2 chat model evaluated as a text-based simulator of a statistician: given column metadata and a question it must select relevant columns and applicable statistical methods.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2-7b-chat-hf",
            "model_size": "7B",
            "model_type": "instruction / chat-tuned base model (not fine-tuned on StatQA in the primary eval)",
            "scientific_domain": "Statistics (statistical analysis — descriptive statistics and hypothesis testing)",
            "simulation_task_description": "Given column metadata and a natural-language statistical question, select the relevant data columns and all applicable statistical methods (assess prerequisites/applicability) — i.e., simulate a text-based statistician making method selection decisions.",
            "prompting_strategy": "Zero-shot, one-shot (few-shot) with a single example per task category, optional Chain-of-Thought (CoT) prompting, and a variant with domain knowledge (DK) text inserted in the prompt.",
            "evaluation_metric": "Acc(C, M): exact-match accuracy of both selected columns and selected applicable methods (percentage of examples with full match).",
            "reported_accuracy": "Best reported overall Acc(C, M) in Table 2: 17.97% (1-shot) / 16.08% (1-shot+DK) depending on strategy; baseline zero-shot 8.08% (Table 2).",
            "baseline_accuracy": "Zero-shot: 8.08% overall Acc(C, M).",
            "factors_reported": [
                "model scale (small model struggles)",
                "prompting strategy (one-shot improves vs zero-shot)",
                "Chain-of-Thought (CoT) often degrades performance for small models",
                "adding domain knowledge (DK) sometimes helps but can have mixed effects",
                "difficulty of example (easy vs hard)",
                "type of task (some categories harder, e.g., contingency tests)"
            ],
            "experimental_conditions": "Evaluations used temperature=0 for all LLMs; top_p=1.0 for LLaMA models; column metadata (header, n_rows, data_type, normality) provided instead of full tables; vLLM framework on NVIDIA 4090 GPUs for open-source models.",
            "limitations_or_failure_modes": "Very low baseline performance; prone to invalid answers, column-selection errors, and applicability errors; CoT prompting can increase invalid/mis-selection errors in small models; limited capability to assess method applicability even when DK is provided.",
            "uuid": "e7361.0",
            "source_info": {
                "paper_title": "Are Large Language Models Good Statisticians?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaMA-2-13B",
            "name_full": "LLaMA-2 13B (chat variant evaluated: Llama-2-13b-chat-hf)",
            "brief_description": "A larger open-source LLaMA-2 model (13B) evaluated on StatQA for selecting relevant columns and applicable statistical methods.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2-13b-chat-hf",
            "model_size": "13B",
            "model_type": "instruction / chat-tuned base model",
            "scientific_domain": "Statistics (statistical analysis)",
            "simulation_task_description": "Text-based selection of relevant columns and applicable methods for statistical questions (assessing prerequisites and applicability).",
            "prompting_strategy": "Zero-shot, one-shot, CoT, and one-shot with domain knowledge (DK).",
            "evaluation_metric": "Acc(C, M) exact-match accuracy (columns + methods).",
            "reported_accuracy": "Best reported overall Acc(C, M) in Table 2: 31.90% (1-shot); zero-shot 9.03%. 1-shot+DK reported 20.29% in another configuration (table shows results by strategy and task).",
            "baseline_accuracy": "Zero-shot: 9.03% overall Acc(C, M).",
            "factors_reported": [
                "model scale (improvement vs 7B)",
                "few-shot examples (1-shot gave ~+9.6% on average for LLaMA models)",
                "CoT had small negative effect for some LLaMA variants",
                "domain knowledge prompting (DK) could help but effect varies by task"
            ],
            "experimental_conditions": "temperature=0, top_p=1.0 for LLaMA models; evaluation on mini-StatQA and StatQA; column metadata provided. vLLM on NVIDIA 4090.",
            "limitations_or_failure_modes": "Still far below best proprietary models; susceptible to applicability and column-selection errors; DK can sometimes reduce performance on simpler tasks (information obfuscation).",
            "uuid": "e7361.1",
            "source_info": {
                "paper_title": "Are Large Language Models Good Statisticians?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaMA-3-8B",
            "name_full": "Meta LLaMA-3 8B (Meta-Llama-3-8B)",
            "brief_description": "An 8B-parameter LLaMA-3 model evaluated both in base and instruction variants on StatQA; shows marked improvement over LLaMA-2 variants but remains weaker than best proprietary LLMs unless fine-tuned.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Meta-Llama-3-8B",
            "model_size": "8B",
            "model_type": "base and instruction variants (Meta-Llama-3-8B and Meta-Llama-3-8B-Instruct), both evaluated; also fine-tuned variants exist (LoRA).",
            "scientific_domain": "Statistics (statistical analysis, hypothesis testing & descriptive statistics)",
            "simulation_task_description": "Select relevant columns and applicable statistical methods for natural-language statistical questions, including checking prerequisites (normality, sample size, data type).",
            "prompting_strategy": "Zero-shot, one-shot, CoT, and one-shot with domain knowledge (DK); also evaluated after LoRA fine-tuning on StatQA training set.",
            "evaluation_metric": "Acc(C, M) (exact-match percent of correct columns and methods).",
            "reported_accuracy": "Best non-fine-tuned reported overall Acc(C, M): 36.11% (1-shot+DK) according to Table 2; zero-shot reported 22.01% in one line. Fine-tuned (SFT LLaMA-3-8B) achieved 77.13% (0-shot after fine-tuning) in Table 2.",
            "baseline_accuracy": "Zero-shot: 22.01% (non-fine-tuned base), one-shot baseline gave ~32.24% depending on config.",
            "factors_reported": [
                "model scale/architecture (LLaMA-3 improved over LLaMA-2)",
                "fine-tuning (LoRA) dramatically increases performance",
                "prompting: one-shot and DK helpful; CoT can slightly reduce performance on small models",
                "task type—DK particularly helps less-common tasks (DCT, CTT, VT)",
                "difficulty (easy vs hard examples)"
            ],
            "experimental_conditions": "temperature=0, top_p=1.0 for LLaMA models; fine-tuning done via LoRA for 3 epochs, batch size 16, Adam optimizer, LR schedule up to 5e-5 then cosine anneal, training on NVIDIA A800 (80G).",
            "limitations_or_failure_modes": "Non-fine-tuned model still makes many applicability errors; fine-tuned models perform much better but dataset and evaluation focus only on method selection/applicability; CoT can increase invalid answers in smaller setups.",
            "uuid": "e7361.2",
            "source_info": {
                "paper_title": "Are Large Language Models Good Statisticians?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaMA-3-8B-Instruct (SFT)",
            "name_full": "Meta-Llama-3-8B-Instruct (LoRA fine-tuned on StatQA, reported as SFT LLaMA-3 8B Instruct)",
            "brief_description": "Instruction-tuned LLaMA-3 8B variant further fine-tuned with LoRA on StatQA training data; achieves the strongest open-source fine-tuned performance on StatQA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Meta-Llama-3-8B-Instruct (LoRA fine-tuned)",
            "model_size": "8B",
            "model_type": "instruction-tuned model further fine-tuned on StatQA via LoRA (parameter-efficient fine-tuning)",
            "scientific_domain": "Statistics (statistical method applicability selection)",
            "simulation_task_description": "After fine-tuning, select relevant columns and applicable statistical methods with high accuracy; simulate a statistician's applicability judgments.",
            "prompting_strategy": "Evaluated mostly 0-shot after fine-tuning (the fine-tuned model was provided the standard prompt structure used in experiments).",
            "evaluation_metric": "Acc(C, M) exact-match accuracy.",
            "reported_accuracy": "Fine-tuned (SFT LLaMA-3-8B Instruct) reported overall Acc(C, M) = 75.92% (0-shot after fine-tuning) in Table 2.",
            "baseline_accuracy": "Non-fine-tuned Meta-Llama-3-8B-Instruct baseline ~27.77% (1-shot+DK line) or lower depending on strategy.",
            "factors_reported": [
                "fine-tuning on StatQA (LoRA) is a major positive factor",
                "model architecture and instruction tuning",
                "availability of training data synthesized from tables (no overlap between train and test tables)",
                "prompt format and strict JSON output requirement"
            ],
            "experimental_conditions": "Fine-tuning: LoRA, 3 epochs, batch size 16, Adam, lr ramp to 5e-5 then cosine anneal to 0; hardware NVIDIA A800 (80G). Evaluation temp=0.",
            "limitations_or_failure_modes": "Although fine-tuned models perform much better, the paper notes remaining room for improvement and that performance is measured only on applicability/selection (not full statistical computation); may overfit synthetic train distribution if extended.",
            "uuid": "e7361.3",
            "source_info": {
                "paper_title": "Are Large Language Models Good Statisticians?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "OpenAI GPT-3.5-Turbo",
            "brief_description": "A widely used proprietary conversational LLM; evaluated as a text-based simulator for statistical method selection on StatQA using prompting variants (0-shot/1-shot/CoT/DK).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": "proprietary (not disclosed in paper)",
            "model_type": "proprietary instruction-following conversational model (no StatQA fine-tuning reported)",
            "scientific_domain": "Statistics (statistical analysis applicability selection)",
            "simulation_task_description": "Select relevant columns and applicable statistical tests for a provided statistical question and column metadata; optionally provide reasoning when instructed.",
            "prompting_strategy": "Zero-shot, one-shot (few-shot), chain-of-thought (CoT), and one-shot+domain-knowledge (DK) prompting; the DK insertion improved results significantly for GPT-3.5-Turbo.",
            "evaluation_metric": "Acc(C, M) exact-match accuracy.",
            "reported_accuracy": "Best reported overall Acc(C, M): 49.36% (1-shot+DK) in Table 2; zero-shot baseline 37.40%.",
            "baseline_accuracy": "Zero-shot: 37.40% overall Acc(C, M).",
            "factors_reported": [
                "prompting strategy: few-shot and DK improve performance notably",
                "CoT has negligible or small effects on larger GPT models",
                "model comprehension capacity; DK has more effect on larger models",
                "temperature during evaluation (set to 0 for evaluations)"
            ],
            "experimental_conditions": "Evaluation via OpenAI API; temperature=0 in evaluations (noted that GPT-3.5-Turbo temp=0.7 only during question refinement step), Chain-of-Thought experiments run as zero-shot-CoT and one-shot-CoT variants.",
            "limitations_or_failure_modes": "Still makes many applicability errors; performance can be surpassed by fine-tuned open-source models; DK can sometimes cause more column-selection or task-confusion errors on some subtasks.",
            "uuid": "e7361.4",
            "source_info": {
                "paper_title": "Are Large Language Models Good Statisticians?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "OpenAI GPT-4",
            "brief_description": "Large proprietary model evaluated on StatQA for statistical method applicability selection; strong performance when provided domain knowledge in prompts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4",
            "model_size": "proprietary (not disclosed in paper)",
            "model_type": "proprietary, instruction-following",
            "scientific_domain": "Statistics (statistical analysis & hypothesis testing applicability)",
            "simulation_task_description": "Select relevant columns and applicable statistical methods based on metadata and question; simulate a statistician's applicability judgments.",
            "prompting_strategy": "Zero-shot, one-shot, Chain-of-Thought (CoT), and one-shot+domain-knowledge (DK).",
            "evaluation_metric": "Acc(C, M) exact-match accuracy.",
            "reported_accuracy": "Best reported overall Acc(C, M): 53.22% (1-shot+DK) in Table 2; zero-shot baseline 43.34% (0-shot-CoT line) / 44.23% varies across prompt variants in table.",
            "baseline_accuracy": "Zero-shot: reported around 44.23% (0-shot variants) depending on CoT usage.",
            "factors_reported": [
                "domain knowledge (DK) in prompt notably improves performance",
                "few-shot examples add benefit",
                "CoT has negligible impact for larger GPT models",
                "task type differences (DCT, VT, CTT benefit more from DK)"
            ],
            "experimental_conditions": "Evaluations via OpenAI API; temperature=0 for evaluations; one-shot and DK variants tested.",
            "limitations_or_failure_modes": "Despite better performance, GPT-4 still exhibits a considerable proportion of applicability errors; can be outperformed by fine-tuned open-source models on this benchmark in some settings.",
            "uuid": "e7361.5",
            "source_info": {
                "paper_title": "Are Large Language Models Good Statisticians?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "OpenAI GPT-4o",
            "brief_description": "Newer GPT-series variant evaluated and shown to achieve the best non-fine-tuned performance on StatQA with domain-knowledge enhanced prompting in the paper's experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4o",
            "model_size": "proprietary (not disclosed)",
            "model_type": "proprietary instruction-following LLM",
            "scientific_domain": "Statistics (statistical method applicability / selection)",
            "simulation_task_description": "Text-based selection of columns and applicable statistical methods; simulates a statistician evaluating applicability and prerequisites.",
            "prompting_strategy": "Zero-shot, one-shot, CoT, one-shot+DK (domain knowledge). The best reported result used one-shot+DK.",
            "evaluation_metric": "Acc(C, M) exact-match accuracy.",
            "reported_accuracy": "Best reported overall Acc(C, M): 64.83% (1-shot+DK) as reported in Table 2 (best non-fine-tuned model result).",
            "baseline_accuracy": "Zero-shot: ~44.71% (0-shot-CoT/0-shot lines show around 44.71%).",
            "factors_reported": [
                "prompting with domain knowledge (DK) yielded substantial improvements",
                "few-shot examples (one-shot) contributed positively",
                "model size and pretraining/comprehension capacity",
                "task category difficulty"
            ],
            "experimental_conditions": "Evaluation via OpenAI API, temperature=0 in evaluations; top_p not specified for GPT APIs in paper.",
            "limitations_or_failure_modes": "Although achieving the best non-fine-tuned result, GPT-4o still makes many applicability errors and can be outperformed by fine-tuned open-source models on StatQA.",
            "uuid": "e7361.6",
            "source_info": {
                "paper_title": "Are Large Language Models Good Statisticians?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Qwen2-72B",
            "name_full": "Qwen2-72B-Instruct",
            "brief_description": "A large open-source Qwen2 model (72B) evaluated in additional experiments; demonstrates competitive performance among open-source models on StatQA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2-72B-Instruct",
            "model_size": "72B",
            "model_type": "open-source instruction-tuned model (evaluated without StatQA fine-tuning)",
            "scientific_domain": "Statistics (statistical method selection / applicability)",
            "simulation_task_description": "Given column metadata and question, select relevant columns and applicable statistical methods — text-based simulation of method selection.",
            "prompting_strategy": "Zero-shot and one-shot (few-shot) prompting variants.",
            "evaluation_metric": "Acc(C, M) exact-match accuracy.",
            "reported_accuracy": "Reported in Table 10: 1-shot overall Acc(C, M) = 47.12%; zero-shot = 44.71% in that table.",
            "baseline_accuracy": "Zero-shot: 44.71% (Table 10).",
            "factors_reported": [
                "model scale (large model yields significant improvements over smaller open-source models)",
                "few-shot prompting (one-shot improved to 47.12%)",
                "pretraining coverage (privacy/closed vs open data tradeoffs)"
            ],
            "experimental_conditions": "Evaluations reported in Appendix D (Table 10); same general evaluation protocol (temperature=0 for main evals), column metadata provided.",
            "limitations_or_failure_modes": "While competitive among open-source models, still behind best fine-tuned open-source SFT results; may still produce applicability errors and task confusion depending on prompt.",
            "uuid": "e7361.7",
            "source_info": {
                "paper_title": "Are Large Language Models Good Statisticians?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Fine-tuned LLaMA-2-7B (SFT LLaMA-2 7B)",
            "name_full": "LLaMA-2 7B LoRA fine-tuned on StatQA (reported as SFT LLaMA-2 7B)",
            "brief_description": "LLaMA-2 7B model fine-tuned with LoRA on the StatQA training set, achieving a large improvement over its non-fine-tuned baseline on the method-selection task.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7b-chat-hf (LoRA fine-tuned on StatQA)",
            "model_size": "7B",
            "model_type": "LoRA fine-tuned on StatQA training data (parameter-efficient fine-tuning)",
            "scientific_domain": "Statistics (statistical method applicability selection)",
            "simulation_task_description": "After LoRA fine-tuning, simulate selecting columns and applicable methods for statistical questions with improved accuracy.",
            "prompting_strategy": "After fine-tuning, evaluated typically in 0-shot format using the standard JSON-output prompt.",
            "evaluation_metric": "Acc(C, M) exact-match.",
            "reported_accuracy": "SFT LLaMA-2 7B reported overall Acc(C, M) = 66.72% (Table 2, 0-shot after fine-tuning).",
            "baseline_accuracy": "Non-fine-tuned LLaMA-2-7B zero-shot baseline: 8.08%.",
            "factors_reported": [
                "fine-tuning on task-specific synthetic data (StatQA) is a major positive factor",
                "LoRA parameter-efficient fine-tuning strategy",
                "quality and coverage of synthesized training examples (no table overlap with test)",
                "prompt format consistency"
            ],
            "experimental_conditions": "LoRA fine-tuning for 3 epochs, batch size 16, Adam optimizer, LR ramp to 5e-5 then cosine anneal, training on NVIDIA A800 (80G). Evaluations at temperature=0.",
            "limitations_or_failure_modes": "Although fine-tuned performance is strong, the approach depends on availability of curated/synthesized training examples and may not generalize beyond covered methods; paper notes room for improvement and limitations in scope (statistics-only).",
            "uuid": "e7361.8",
            "source_info": {
                "paper_title": "Are Large Language Models Good Statisticians?",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DAEval",
            "rating": 2
        },
        {
            "paper_title": "QRData",
            "rating": 2
        },
        {
            "paper_title": "Infiagent-dabench: Evaluating agents on data analysis tasks",
            "rating": 2,
            "sanitized_title": "infiagentdabench_evaluating_agents_on_data_analysis_tasks"
        },
        {
            "paper_title": "MathVista: Evaluating mathematical reasoning of foundation models in visual contexts",
            "rating": 1,
            "sanitized_title": "mathvista_evaluating_mathematical_reasoning_of_foundation_models_in_visual_contexts"
        },
        {
            "paper_title": "MathVerse: Does your multi-modal llm truly see the diagrams in visual math problems",
            "rating": 1,
            "sanitized_title": "mathverse_does_your_multimodal_llm_truly_see_the_diagrams_in_visual_math_problems"
        }
    ],
    "cost": 0.02114225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Are Large Language Models Good Statisticians?
10 Oct 2024</p>
<p>Yizhang Zhu 
The Hong Kong University of Science and Technology (Guangzhou)
GuangzhouChina</p>
<p>Shiyin Du 
The Hong Kong University of Science and Technology (Guangzhou)
GuangzhouChina</p>
<p>Boyan Li boyanli@hkust-gz.edu.cn 
The Hong Kong University of Science and Technology (Guangzhou)
GuangzhouChina</p>
<p>Yuyu Luo yuyuluo@hkust-gz.edu.cn 
The Hong Kong University of Science and Technology (Guangzhou)
GuangzhouChina</p>
<p>The Hong Kong University of Science and Technology
Hong Kong SARChina</p>
<p>Nan Tang nantang@hkust-gz.edu.cn 
The Hong Kong University of Science and Technology (Guangzhou)
GuangzhouChina</p>
<p>The Hong Kong University of Science and Technology
Hong Kong SARChina</p>
<p>Are Large Language Models Good Statisticians?
10 Oct 2024A0520C99D6912DA35A2110DB03A13A07arXiv:2406.07815v2[cs.CL]DS (Task)Descriptive Statistics (2690) CTT (Task)Contingency Table Test (2491) DCT (Task)Distr. Compliance Test (2400) VT (Task)Variance Test (2330) CA (Task)Correlation Analysis (1712) ModeMode RangeRange QuartQuartile MeanMean SDStandard Deviation SkewSkewness MedMedian KurtKurtosis MHTMantel-Haenszel Test CITChi-square Independence Test FETFisher Exact Test Anderson-Darling Test KST-NKolmogorov-Smirnov Test for Normality LiTLilliefors Test SWTShapiro-Wilk Test of Normality MoVTMood Variance Test LeTLevene Test BTBartlett Test FTVF-Test for Variance PaCCPartial Correlation Coefficient PCCPearson Correlation Coefficient SCCSpearman Correlation Coefficient KCCKendall Correlation Coefficient
Large Language Models (LLMs) have demonstrated impressive capabilities across a range of scientific tasks including mathematics, physics, and chemistry.Despite their successes, the effectiveness of LLMs in handling complex statistical tasks remains systematically under-explored.To bridge this gap, we introduce StatQA, a new benchmark designed for statistical analysis tasks.StatQA comprises 11,623 examples tailored to evaluate LLMs' proficiency in specialized statistical tasks and their applicability assessment capabilities, particularly for hypothesis testing methods.We systematically experiment with representative LLMs using various prompting strategies and show that even state-of-the-art models such as GPT-4o achieve a best performance of only 64.83%, indicating significant room for improvement.Notably, while open-source LLMs (e.g., LLaMA-3) show limited capability, those fine-tuned ones exhibit marked improvements, outperforming all in-context learning-based methods (e.g., GPT-4o).Moreover, our comparative human experiments highlight a striking contrast in error types between LLMs and humans: LLMs primarily make applicability errors, whereas humans mostly make statistical task confusion errors.This divergence highlights distinct areas of proficiency and deficiency, suggesting that combining LLM and human expertise could lead to complementary strengths, inviting further investigation into their collaborative potential.Our source code and data are available at https://statqa.github.io/.</p>
<p>Introduction</p>
<p>Statistical analysis can capture data patterns and convert them into usable evidence, which is crucial to data science and machine learning applications [1][2][3][4][5][6][7][8].As shown in Figure 1, a typical statistical analysis task involves, given a table D and a statistical question Q, a qualified statistician should be proficient in selecting relevant columns C, choosing the appropriate statistical methods M, and computing the results based on M using C. Formally, the statistical analysis task can be divided into two stages: (1) identifying appropriate statistical methods and parameters (e.g., relevant columns in the table), and (2) computing the statistical results and deriving the conclusion.The first stage requires statistical expertise to assess the applicability of methods, considering factors such as data type, distribution characteristics, and sample size, which is the core of the statistical task.In contrast, the second stage (i.e., computation) can be easily aided by external tools [9] such as WolframAlpha [10].</p>
<p>Inspired by the extensive application of Large Language Models (LLMs) [11][12][13], we pose a critical question: Do LLMs truly understand such "statistical literacy"?Specifically, can LLMs competently select relevant data, recognize prerequisites, and discern appropriate usage scenarios to assess the effectiveness of statistical methods?Currently, research on the mathematical reasoning abilities of LLMs has primarily focused on the accuracy of computational processes and results, involving conventional statistical methods [14][15][16][17].However, studies examining datasets requiring specialized statistical testing methods, particularly those assessing the applicability of these methods, are minimal.This gap in the literature motivated us to explore and address this area.Our study aims to answer the following critical questions.</p>
<p>Check applicability and prerequisites</p>
<p>Pearson Correla�on Coefficient</p>
<p>Relevant Columns Applicable Methods
Strongly
• Q1: How can we evaluate LLMs' performance in more complex and specialized statistical testing tasks?Constructing an appropriate benchmark is crucial for accurate performance evaluation.</p>
<p>Given the challenges posed by the lack of datasets and scarcity of examples in this specialized field, how can we efficiently develop such a benchmark?• Q2: How capable are current LLMs in this field, and how can we improve their performance?</p>
<p>This requires systematic experiments to evaluate the capabilities of current LLMs, exploring the impact of different models, prompting strategies, and fine-tuning methods on their performance.• Q3: How do humans perform compared to LLMs, and what are the differences in their performance?This involves a comparative study between humans and LLMs, aiming to analyze their respective strengths and weaknesses and explore potential complementary between human expertise and LLM capabilities.</p>
<p>Contributions.Our contributions are summarized as follows:</p>
<p>• StatQA.We propose StatQA, a new benchmark for statistical analysis tasks, particularly focusing on the applicability assessment of statistical methods.We introduce an automated pipeline to construct StatQA by synthesizing statistical tasks and their corresponding answers, which also provides insights for dataset construction in other specialized domains with scarce examples.• Systematic Evaluation.We conduct extensive evaluations on widely used LLMs to establish benchmarks for statistical tasks.We also explore several strategies, including domain-specific prompts and fine-tuning, to better harness the capabilities of LLMs for these tasks.• Comparative Study between Humans and LLMs.We organize group-based human experiments and comparatively analyze differences between humans and LLMs in performance and errors.Our findings highlight humans' and LLMs' distinct strengths and weaknesses and reveal their potential complementarity.• New Empirical Findings and Research Opportunities.Based on the experiments and analysis above, we summarize six key findings and discuss research opportunities in this field.</p>
<p>StatQA</p>
<p>In this section, we will first discuss the design goal of the benchmark for statistical analysis tasks (Section 2.1).We will then describe the characteristics of StatQA (Section 2.2).Finally, we will elaborate on how to develop StatQA with low human cost while ensuring high quality (Section 2.3).</p>
<p>Design Goals and Tasks Scope</p>
<p>Goals.We aim to develop a specialized dataset to address the current research gap.Our goals include: (G1) Good Coverage of Statistical Tasks and Difficulties: The benchmark should encompass representative and commonly used statistical analysis tasks and methods.It should be designed to be sufficiently discriminative, effectively distinguishing between the strengths and weaknesses Ques�on How does frequency of exercise-induced angina vary across the categories of resting electrocardiogram?</p>
<p>Dataset</p>
<p>Heart Failure Prediction Dataset Relevant Columns</p>
<p>[{"column_header": "RestingECG", "is_strata": false, "is_control": false}, {"column_header": "ExerciseAngina", "is_strata": false, "is_control": false}]</p>
<p>Results</p>
<p>[{"method": "Chi-square Independence Test", "result": "{\"p value\": 0.00494}", "conclusion": "Not independent"}] Ground Truth {"columns": ["RestingECG", "ExerciseAngina"], "methods": ["Chi-square Independence Test"]}</p>
<p>Task</p>
<p>Contingency table test</p>
<p>Difficulty hard  The dataset should facilitate our core experiments, assessing whether LLMs can evaluate the applicability of statistical methods, select suitable methods, and identify relevant data columns; (G3) Low Human-Cost: Automating the dataset construction process to ensure sufficient scale and improved efficiency while maintaining high data quality; and (G4) Extensibility: The benchmark should be designed to accommodate future expansions and enhancements.</p>
<p>The Scope of Statistical Analysis Tasks.Statistical tasks can be broadly divided into descriptive statistics and inferential statistics, with inferential statistics primarily including regression analysis and hypothesis testing [18].Descriptive statistics and hypothesis testing represent two prevalent methods frequently employed in statistical analysis.The latter is often considered the most misunderstood in quantitative analysis due to its complex interdependencies between procedural components [19].Therefore, together with Descriptive Statistics (DS), we select four representative categories of statistical tasks in hypothesis testing to be covered in StatQA, along with commonly used methods: Correlation Analysis (CA); Contingency  Step 3</p>
<p>Step 4</p>
<p>Step 5</p>
<p>StatQA Construction</p>
<p>Key Ideas for Developing StatQA.In conventional dataset construction, researchers collect a suitable dataset D, formulate a question Q, and manually annotate answers A. While this method ensures high data quality, it is time-consuming, costly, and limits extensibility, especially in specialized domains with scarce examples.To alleviate these limitations, our key idea is to reverse this process by synthesizing the question Q based on target answers A. We start with target answers A derived from the tabular data D and generate corresponding statistical questions Q.This approach ensures precise alignment between questions and answers, enabling more efficient dataset construction.</p>
<p>To implement this, we design an efficient pipeline for constructing StatQA, as shown in Figure 4.</p>
<p>Unlike traditional methods, we set target answers A based on tabular data D and then synthesize statistical questions Q in reverse.To ensure alignment between Q and A, we incorporate automated prerequisite checks.To support the evaluation of statistical literacy, the target answers A include relevant columns C and applicable statistical methods M, enabling the derivation of computational results R. Therefore, our pipeline can synthesize numerous examples of (D, C, M, Q, R) along with other supplementary information.Next, we will go through our pipeline step by step.</p>
<p>Tabular Data and Metadata Collection.We collect 78 tables from real-world applications, covering various domains including education, medicine, economy, etc., as shown in Section B.5.These tables are carefully gathered by post-graduate students in statistics from Kaggle [21] and Rdatasets [22].The metadata includes descriptive information about the tabular data and details of each data column.Specifically, it encompasses the column header, data type, normality, and description, which are crucial for prerequisite checks in statistical analysis.Descriptive information in metadata can be obtained from their sources, while the data types and normality of each column can be calculated and derived from the tabular data.For tables lacking metadata, we perform manual annotations.For tables with existing metadata, we conduct manual validation to ensure accuracy.</p>
<p>Step 1: Set Target Answers (Select Target Methods and Columns).As shown in Figure 4, our pipeline can reversely synthesize the statistical question Q based on statistical methods and involved data columns.The first step is to set the target methods M. Considering the parameter volume of the target methods, we select suitable columns from tabular data to obtain a set of data columns C.</p>
<p>Step 2: Prerequisites Check and Computation.Since statistical methods should be used under appropriate conditions, we perform a series of checks to ensure all prerequisites are met.This includes verifying features such as sample size, data type, and normality.Suppose data columns C = {C 1 , ..., C m } fit with the prerequisites of methods M = {M 1 , ..., M n }.The data columns C can then be used as parameters in the target methods M to compute the corresponding computational results and preliminary conclusions, noted as R = {M 1 (C), ..., M n (C)}.</p>
<p>Step 3: Statistical Question Synthesis.To ensure the quality of the questions, we use hand-crafted question templates to synthesize preliminary statistical questions.The templates are determined by the target method, listing common question expressions with placeholders where relevant data columns are involved, as shown in Section B. 3. By selecting a template T and substituting in previously chosen data columns C, we can obtain the preliminary statistical question Q(C).</p>
<p>Step 4: Difficulty Balancing and Dataset Splitting.For a statistical question, if the set of applicable methods constitutes a proper subset of the candidate methods (i.e., all methods for comparable scenarios), it indicates that certain prerequisites for some methods are not satisfied.In such cases, we label the question as "hard", reflecting the increased challenge of accurately assessing applicability to eliminate inapplicable methods, otherwise the question is labeled as "easy".Step 5: Statistical Question Refinement.Inspired by LLMs' capabilities in text comprehension and processing, we use GPT-3.5-Turbo to refine the phrasing and expression of statistical questions for the test data.We provide GPT-3.5-Turbo with original questions and their descriptive information, instructing GPT-3.5-Turbo to paraphrase and refine the question sentences without changing the meaning, aiming for more coherent and diverse expressions, which is noted as
Q * (C).
Discussion of Quality Control.We devise several strategies to ensure the high quality of our StatQA.</p>
<p>(1) Quality of Question Templates: The quality of question templates is a focal point in our quality control.We categorize templates based on statistical tasks and applicable scenarios of methods.For each category, we meticulously prepare 10 to 20 templates for random selection to enhance diversity.To ensure the templates are representative, we recruit two post-graduate students in statistics to design and review them.(2) Question Refinement: The objectives of refinement are to correct potential grammar mistakes, improve semantical coherence, and increase the diversity of expressions for the questions generated by the templates.In practice, GPT-3.5-Turbodemonstrates a satisfactory level of English proficiency in achieving these goals and offers better cost-effectiveness; therefore, we employ GPT-3.5-Turbo as the refiner.Note that question refinement is exclusively performed on StatQA to increase diversity and ensure differences from the training set.The average BLEU [23], BERTScore [24]
Acc(C, M) = N i=1 1( Ĉi = C i , Mi = M i ) N , (1)
where N is the total number of test examples.</p>
<p>Evaluation for LLMs</p>
<p>Non-fine-tuned LLMs.For open-source LLMs, we select and conduct experiments on LLaMA-2 models (Llama-2-7b-chat-hf, Llama-2-13b-chat-hf) [25] and LLaMA-3 models (Meta-Llama-3-8B, Meta-Llama-3-8B-Instruct) [26].For proprietary LLMs, we select representative and widely-used ChatGPT (gpt-3.5-turbo)[27], GPT-4 [28] and newly released GPT-4o [29].</p>
<p>Prompting Strategies.Few-shot learning and Chain-of-Thought (CoT) [30] will be used as prompting strategies.In few-shot learning, we prepare one example of each task category for the LLMs to learn from.To prevent the leakage of actual task categories, we randomly select examples in the few-shot.Furthermore, we introduce a new strategy to include domain knowledge (DK) of statistical methods' applicability and prerequisites in the prompt.</p>
<p>Fine-tuned LLMs.We fine-tune three models: LLaMA-2 (LLaMA-2-7b-chat-hf) and LLaMA-3 (Meta-Llama-3-8B, Meta-Llama-3-8B-Instruct). For all fine-tuning, we use the LoRA [31] method, which is a Parameter-Efficient Fine-Tuning technique that adjusts only a small number of parameters.This method achieves an effect close to full-parameter fine-tuning on downstream tasks while reducing computation and storage costs.Note that StatQA is exclusively reserved for testing and evaluation.We use the training set, also generated by our dataset construction pipeline (see Step 4 in Section 2.3), to fine-tune the LLMs.</p>
<p>Human Experiments</p>
<p>Participants.Human experiments are conducted for a comparative study.We recruit 6 postgraduate students and group them based on their disciplinary backgrounds: Non-Statistics Background Group (Non-Stats, three STEM post-graduate students not in statistics major) and Statistics Background Group (Stats, three post-graduate students in statistics major).</p>
<p>Protocols.We use stratified sampling to extract 10% of the mini-StatQA for human experiments (117 examples).To ensure participants understand the task and are familiar with operations on our testing platform, they are required to watch a tutorial video before starting.Each participant needs to use 2 answering modes respectively during the experiments: (1) Closed-book: participants must answer independently; (2) Open-book: similar to introducing domain knowledge for LLMs, participants are provided with supplemental information.More details can be found in Section C.2.</p>
<p>Experimental Results and Analysis</p>
<p>Different models display significant performance discrepancies on our benchmark, indicating sufficient discriminative ability.Table 2 presents the experimental results.Figure 5 shows the radar chart depicting performances across five task categories achieved by leading results within each model.</p>
<p>Overall Performance of LLMs.The LLaMA-2 models demonstrate weak performance, suggesting their inability.The newer LLaMA-3 shows remarkable improvement over the previous LLaMA-2, with the LLaMA-3-8b achieving an Acc(C, M) up to 36.1%, far surpassing all tested LLaMA-2 models, close to the performance of 0-shot GPT-3.5-Turbo(37.4%).However, it is still noticeably weaker than GPT-4 and GPT-4o.By fine-tuning, LLaMA-2-7b, LLaMA-3-8b, and LLaMA-3-8b-Instruct models achieve marked enhancement, with the fine-tuned LLaMA-3-8b model showing the best overall performance, but considerable room still remains for further improvement.Impact of Prompting Strategies.We observe that compared to 0-shot, 1-shot learning increased the Acc(C, M) for the LLaMA-2/3 models by an average of 9.6%, and by 4.7% for GPT models.However, CoT leads to a minor performance decrease in most cases of LLaMA-2/3, whereas its impact on larger GPT models is negligible.More analysis for CoT is shown in Section E.2.The introduced domain knowledge prompting has proven effective, for six out of the seven non-fine-tuned LLMs obtained their best results in our experiments.Notably, it has a more pronounced effect on larger LLMs with stronger comprehension abilities, particularly GPT-4o.</p>
<p>Finding 1. Few-shot learning and the inclusion of domain knowledge are helpful for LLMs in this task, whereas CoT is more likely to result in slight performance degradation in smaller models.</p>
<p>Human Performance.When completing the task independently without consulting any references, human participants demonstrate low accuracy.However, when provided with supplemental information of applicability, their performance improve significantly, particularly the participants in statistics, whose accuracy reached 53.4%, surpassing all non-fine-tuned LLMs with common prompting strategies.However, this is overshadowed by the fine-tuned models and the best performance of GPT-4o when domain knowledge is introduced in the prompt.</p>
<p>Finding 2. LLMs with prompt-based approaches remain behind people in statistics.However, the gap can be filled even surpassed by fine-tuning or introducing domain knowledge to a strong LLM.</p>
<p>Comparing LLMs and Human Performance.LLMs and humans perform best on straightforward and commonly seen descriptive statistics tasks.In hypothesis testing tasks, humans and most LLMs exhibit relatively strong performances in correlation analysis and distribution compliance tests.In contrast, LLMs and human encounter major challenges in contingency Errors Analysis.We examine and summarize the proportions and distributions of the error types across different LLMs and experimental setups, as shown in Figure 6.We observe that except smaller LLaMA-3 Finding 4. LLaMA-3 and GPT models demonstrate a competent understanding of tasks and the latter can accurately select data columns, but LLaMA-2 models have difficulties in these aspects.
7 b 0 -s h o t 7 b 1 -s h o t 7 b 1 -s h o t + D K 1 3 b 0 -s h o t 1 3 b 1 -s h o t 1 3 b 1 -s h o t + D K 0.0 0.G P T -3 . 5 T 0 -s h o t G P T -3 . 5 T 1 -s h o t G P T -3 . 5 T 1 -s h o t + D K G P T -4 0 -s h o t G P T -4 1 -s h o t G P T -4 1 -s h o t + D K G P T -4 o 0 -s h o t G P T -4 o 1 -s h o t G P T -4 o 1 -s h o t + D K 0.
Except for LLaMA-2, the primary error type observed for LLMs on our benchmark is applicability errors, while the percentage of errors associated with statistical task confusion is quite low.Strongly performed GPT and fine-tuned models also have a considerable proportion of applicability errors, even if we provide domain knowledge of applicability.This can be considered an inherent limitation of LLMs, indicating a deficiency in LLMs' ability to understand and assess methodological applicability.However, the situation is completely different for humans.Although participants in statistics outperform all non-fine-tuned LLMs in open-book experiments, in all human experimental groups, applicability errors account for a smaller proportion, while statistical task confusion errors constitute the highest proportion.</p>
<p>Finding 5. LLMs are good at distinguishing different statistical tasks then selecting associated methods, but they struggle to utilize the domain knowledge to assess method applicability effectively.Conversely, humans excel at discerning method applicability but are prone to task confusion.Finding 6. Humans and LLMs have distinct proficiencies and weaknesses in different aspects of selecting applicable statistical tasks, highlighting the potential for complementary collaboration.</p>
<p>Research Opportunity</p>
<p>LLMs for Statistical Applications.Current LLMs struggle with accurately assessing the applicability of statistical methods, even when provided with explicit domain knowledge.This indicates a profound need for developing models that can better understand and utilize detailed methodological prerequisites and application contexts.Future research should focus on integrating more sophisticated reasoning mechanisms into LLMs or leveraging a multi-agent framework to enhance their comprehension and application of statistical methods.</p>
<p>Human-AI Collaboration in the Statistical Task.LLMs and humans exhibit distinct aspects of superior capability in statistical tasks.Therefore, an in-depth study into harnessing their unique strengths for complementary collaboration to achieve optimal performance can be a valuable endeavor.This approach could leverage the computational efficiency and data handling capabilities of LLMs alongside the nuanced understanding and domain knowledge of human experts, leading to more robust and accurate statistical analysis.</p>
<p>Expanding the Benchmark Dataset.We establish StatQA, focusing on the evaluation of applicability rather than computational results, aiming to highlight the importance of discerning statistical method suitability.StatQA can be expanded to encompass a broader range of statistical tasks and methods, as discussed in Section G in the Appendix.Beyond the field of statistics, the evaluation of method applicability is also crucial in professional domains such as finance and operations research.Consequently, curating a more extensive benchmark represents a significant undertaking.</p>
<p>Related Work</p>
<p>Large Language Models.Proprietary LLMs like ChatGPT [27] and GPT-4 [28] exhibit impressive capabilities in text comprehension and processing, and recent-released GPT-4o [29] is further elevated performance and speed.Open-source models are also favored for their flexibility and suitability for customization, among the most representative examples are the LLaMA series models, such as LLaMA-2 [25] and the new LLaMA-3 [26].Meanwhile, prompting strategies [32,33] and fine-tuning methods like LoRA [31], enable LLMs to better adapt to specific tasks with less computational costs.</p>
<p>Relevant Benchmarks.Some benchmarks have been proposed to assess LLMs' problem-solving ability related to mathematical domains [34][35][36][37][38], but most are more focus on results reasoning and calculation instead of methods selection.Notable works like MATH [39] and GHOSTS [40] cover mathematical problems of varying difficulty, ranging from elementary to graduate levels; TheoremQA [41] and SciBench [42] involve multi-disciplinary problems; and recent studies like MathVista [14] and MathVerse [15] include visual tasks.DAEval [17] involves correlation analysis and distribution analysis as components of question concepts, but its coverage is relatively limited, and QRData [16] is for quantitative reasoning with data, but still more focus on calculation.Studies on datasets for more specialized statistical scenarios, particularly involving assessing the applicability of statistical methods, are minimal.</p>
<p>We also include more detailed discussion about the related work in Section A in the Appendix.</p>
<p>Conclusion</p>
<p>In this paper, we propose StatQA, a benchmark designed for statistical analysis tasks.To evaluate the capabilities of LLMs on StatQA, we conduct systematic experiments with both open-source and closed-source LLMs to determine whether they can select proper statistical methods and relevant data columns by discerning prerequisites and assessing applicability, akin to competent statisticians.Furthermore, we conduct human experiments for a comparative study, discussing how humans and LLMs differ in their capabilities on StatQA and revealing their potential complementarity.Our findings suggest that while LLMs show promise, there is still significant room for improvement, especially in their ability to accurately assess the applicability of statistical methods.Future work could focus on enhancing the reasoning mechanisms of LLMs and exploring more effective human-AI collaboration strategies.We believe that StatQA fills a significant gap and provides a valuable resource for developing more advanced LLMs for statistical analysis tasks.</p>
<p>Acknowledgement</p>
<p>This paper is supported by NSF of China (62402409), Guangdong Basic and Applied Basic Research Foundation (2023A1515110545), CCF-Huawei Populus Grove Fund (CCF-HuaweiDB202403), and the Red Bird Grant, Research Grant from The Hong Kong University of Science and Technology (Guangzhou).LLMs are increasingly employed in both academic and industrial sectors, with extensive applications across disciplines such as science, education, medicine, and finance [43][44][45][46][47][48][49].Particularly since the advent of ChatGPT [27], LLMs have demonstrated impressive capabilities and versatility in data analysis, question answering, and reasoning [50][51][52][53][54][55][56][57][58].The GPT-series model released by OpenAI is one of the most representative and widely used proprietary LLMs, including the newly released GPT-4o as well as ChatGPT and GPT-4, which have achieved state-of-the-art performance on numerous professional benchmarks without fine-tuning [27][28][29]59].Other renowned proprietary LLMs include Claude [60], Qwen [61], and Gemini [62].</p>
<p>Appendices and Supplemental Materials</p>
<p>Open-source LLMs gain significant attention and favor by virtue of their openness, flexibility, and suitability for task-specific fine-tuning.Notably, Meta's LLaMA series, including LLaMA-2 [25] and the new LLaMA-3 [26] are representative examples.Additionally, techniques such as in-context learning [30,32] and fine-tuning approaches like LoRA [31], facilitate the enhancement of LLMs' performance in specific tasks with limited computational costs.</p>
<p>A.2 Relevant Benchmarks</p>
<p>Some benchmarks have been established to assess LLMs' problem-solving and reasoning abilities within scientific domains in recent years.For multi-disciplinary scientific question-answering datasets, TheoremQA [41] is introduced to evaluate LLMs' capacity to apply theorems in solving complex scientific problems, covering 350 theorems across mathematics, physics, EE&amp;CS, and finance.SciBench [42] focuses on vision context-involved scientific problem-solving in collegiate-level mathematics, physics, and chemistry.</p>
<p>There are notable works that have concentrated on mathematical reasoning [36], addressing various types of problems and levels of difficulty, ranging from grade school to postgraduate level.GSM8K [63] is a comprehensive dataset of grade-level math word problems; however, with the rapid development of LLMs, GSM8K is no longer sufficiently challenging for proprietary LLMs now.MATH [39] serves as a benchmark comprised of multiple-choice problems covering a range of difficulty levels from elementary school to college.In contrast, Mishra et al. introduce LILA [37], a sophisticated benchmark encompassing diverse question formats and higher difficulty levels, including problems in calculus and algebra.Following the attention garnered by ChatGPT's remarkable performance, the more advanced GHOSTS benchmark [40] has been proposed to evaluate the capabilities of ChatGPT and GPT-4 on graduate-level mathematics problems.Recent studies like MathVista [14] and MathVerse [15] systematically evaluate LLMs' performance in visual math problem-solving.</p>
<p>Regarding benchmarks involved in statistical tasks, DAEval [17] involves correlation analysis and distribution analysis as components of question concepts, and QRData [16] is designed to assess the quantitative reasoning capabilities of LLMs using real-world data, including statistical and causal reasoning.Nevertheless, the emphasis of these benchmarks is on the accuracy of computational outcomes, with relatively limited attention to the breadth of statistical tasks covered.Research on datasets tailored to more specialized statistical scenarios, particularly those evaluating the applicability of statistical methods, remains sparse.</p>
<p>B More Details of StatQA</p>
<p>B.2 Statistical Tasks and Associated Methods</p>
<p>The five typical categories of statistical tasks and their associated methods covered in this paper are listed in Table 3.</p>
<p>B.3 Examples of Question Templates</p>
<p>As mentioned in Section 2.3, we employ question templates T to synthesize statistical questions in preliminary dataset.These templates T include placeholders where relevant data columns to be involved, and Table 4 presents some examples of question templates across various categories.</p>
<p>B.4 More Examples in StatQA</p>
<p>Figure 7 presents more examples in StatQA across five task categories.</p>
<p>B.5 Domains of Source Tables</p>
<p>As mentioned, we have assembled 78 tables to serve as the foundation for the construction of StatQA, encompassing six domains in real-world scenarios: education, medicine, science, engineering, economy, and life.In the process of dataset partitioning (Step 4 in Section 2.3), we ensure comprehensive representation across all domains when dividing the training and test sets derived from these source tables.The distribution of source tables utilized for testing (to establish StatQA for the evaluation) and for training (to develop D train for fine-tuning) is detailed in Table 5.</p>
<p>Correlation</p>
<p>Analysis easy</p>
<p>[{"column_header": "Total eMedia Circulation", "is_strata": false, "is_control": false}, {"column_header": "New Cardholders", "is_strata": false, "is_control": false}]</p>
<p>[{"method": "Pearson Correlation Coefficient", "result": "{\"coefficient\": 0.16114}", "conclusion": "Not strongly correlated"}, {"method": "Spearman Correlation Coefficient", "result": "{\"coefficient\": 0.46396}", "conclusion": "Not strongly correlated"}, {"method": "Kendall Correlation Coefficient", "result": "{\"coefficient\": 0.29718}", "conclusion": "Not strongly correlated"}] [{"column_header": "happyScore", "is_strata": false, "is_control": false}, {"column_header": "GDP", "is_strata": false, "is_control": false}]</p>
<p>[{"method": "Kolmogorov-Smirnov Test", "result": "{\"p value\": 0.0}", "conclusion": "Distribution significantly different"}]</p>
<p>{"columns": ["happyScore", "GDP"], "methods":</p>
<p>["Kolmogorov-Smirnov Test"]} Heart Failure Prediction Dataset</p>
<p>How does frequency of exercise-induced angina vary across the categories of resting electrocardiogram?</p>
<p>Contingency Table Test hard</p>
<p>[{"column_header": "RestingECG", "is_strata": false, "is_control": false}, {"column_header": "ExerciseAngina", "is_strata": false, "is_control": false}]</p>
<p>[{"method": "Chi-square Independence Test", "result": "{\"p value\": 0.00494}", "conclusion": "Not independent"}] {"columns": ["RestingECG", "ExerciseAngina"], "methods":</p>
<p>["Chi-square Independence Test"]}</p>
<p>FIFA Official Dataset 2023</p>
<p>To what extent do the variance patterns of the estimated market value of the player in pounds (￡) and the release clause value of the player in pounds (￡) align?</p>
<p>Variance</p>
<p>Test hard</p>
<p>[{"column_header": "Value", "is_strata": false, "is_control": false}, {"column_header": "Release Clause", "is_strata": false, "is_control": false}]</p>
<p>[{"method": "Mood Variance Test", "result": "{\"stat\": 10.75535, \"p value\": 0.0}", "conclusion": "Variance significantly difference between them"}, {"method": "Levene Test", "result": "{\"stat\": 287.62234, \"p value\": 0.0}", "conclusion": "Variance significantly difference between them"}]  In our evaluations, we set the hyperparameter temperature=0 for all LLMs and top_p=1.0 for all LLaMA-2/3 models to reduce randomness.We use the vLLM framework [64] to conduct evaluations for LLaMA-2/3 models on 8 NVIDIA 4090 and call OpenAI API to obtain answers generated by GPT models.Differently, the GPT-3.5-Turbo'stemperature is set to 0.7 in statistical question refinement, for more diverse and flexible expressions.In fine-tuning, we utilize the LLaMA-Factory framework [65] to fine-tune LLaMA-2-7b-chat-hf, Meta-Llama-3-8B, and Meta-Llama-3-8B-Instruct on an NVIDIA A800 (80G), for a total of 3 epochs on the training set, set the batch size to 16, and use the Adam optimizer.The learning rate linearly increases from 0 to 5e-5 in the first 200 steps of fine-tuning, then gradually decreases to 0 at the end using the cosine annealing strategy.</p>
<p>C.2 More Details of Human Experiment Setups</p>
<p>We develop a testing platform for human experiments to facilitate the operations of participants.</p>
<p>The GUI of the testing platform is shown in</p>
<p>C.3 Prompts</p>
<p>The prompt used for refining the phrasing and expression of statistical questions in the StatQA is shown in Table 6.The prompts used to generate responses from LLMs is shown in Table 7, including the following components: task description, instruction, classification list, demonstration example (optional), column information, and statistical question.Table 8 illustrates examples used in one-shot learning and one-shot-CoT respectively.Table 9 demonstrates the introduction of domain knowledge (DK) in prompt structure.</p>
<p>D More Experimental Results</p>
<p>To accommodate users only accessible to limited computational resources, or those with sufficient resources but are concerned about data privacy, a more extensive evaluation of open-source LLMs is essential.Accordingly, to provide references for these two groups of users, we conduct additional experiments on both smaller and larger representative open-source models, including Qwen2-0.5B-Instruct,Qwen2-1.5B-Instruct,Qwen2-72B-Instruct, Meta-Llama-3-8B, and Yi-34B-Chat.Table 10 illustrates experimental results for additional evaluation on open-source LLMs.</p>
<p>Qwen2-0.5B-Instruct is entirely inadequate for our task, presenting only 2.41% 1-shot overall accuracy and extremely low capabilities in all tasks.Qwen2-1.5B-Instructdemonstrates performance levels comparable to certain LLaMA models, specifically falling between LLaMA-2-13b and LLaMA-3-8b.Given that Qwen2-1.5B-Instruct is significantly smaller in parameters, although its performance is still not robust, it represents noteworthy and encouraging results for resource-constrained users.</p>
<p>In contrast, larger open-source LLMs exhibit significant enhancements, underscoring the importance of model scale.Remarkably, Qwen2-72B-Instruct demonstrates impressive performance with a 47.12% one-shot overall accuracy, surpassing all other open-source LLMs and being comparable to GPT models.Its strong capabilities are particularly promising for users with ample resources who prioritize data privacy and seek to minimize the reliance on proprietary solutions.</p>
<p>E More Error Analysis E.1 Examples of Error Taxonomy</p>
<p>In Section 3.3, we conduct the error analysis, identifying the following types of errors: invalid answers, column selection errors, statistical task confusion, applicability errors, and mixed errors.Table 11 provides examples for each category within this error taxonomy.Ground Truth: {"columns": ["GRE Score", "LOR"], "methods": ["Mood Variance Test", "Levene Test"]} Model Answer: {"columns": ["CGPA", "LOR"], "methods": ["Mood Variance Test", "Levene Test"]} Statistical Task Confusion Statistical Question: Is there a relationship between the sex of the patient and the type of chest pain experienced?Ground Truth: {"columns": ["Sex", "ChestPainType"], "methods": ["Chi-square Independence Test"]} Model Answer: {"columns": ["Sex", "ChestPainType"], "methods": ["Pearson Correlation Coefficient", "Spearman Correlation Coefficient", "Kendall Correlation Coefficient"]} Applicability Error Statistical Question: Is there a statistically significant difference between the variances in height and weight of the Pokemon?Ground Truth: {"columns": ["Height_m", "Weight_kg"], "methods": ["Mood Variance Test", "Levene Test"]} Model Answer: {"columns": ["Height_m", "Weight_kg"], "methods": ["Mood Variance Test", "Levene Test", "Bartlett Test", "F-Test for Variance"]} Mixed Error Statistical Question: Does the consumption of food between meals have any connection to the presence of family history with overweight?Ground Truth: {"columns": ["family_history_with_overweight", "CAEC"], "methods": ["Chi-square Independence Test"]} Model Answer: {"columns": ["FAVC", "family_history_with_overweight"], "methods": ["Chi-square Independence Test", "Fisher Exact Test", "Mantel-Haenszel Test"]}</p>
<p>Explanations and remarks for examples in Table 11:</p>
<p>• Invalid Answer: fail to understand tasks, smaller models with weaker text comprehension capability such as LLaMA-2-7b are more likely to suffer from this problem; • Column Selection Error: misselection of relevant columns due to failure to correctly understand the meaning: in this case of asking about the student's GRE scores and letters of recommendation, the model incorrectly chooses the irrelevant student's CGPA; • Statistical Task Confusion: the sex and type of chest pain in the dataset are categorical, and the relationship between them should be calculated using methods in the contingency table test rather than correlation analysis; • Applicability Error: correctly determine the task type as a variance test.However, the provided column information clearly shows that "Height_m" and "Weight_kg" do not follow a normal distribution, so the Bartlett Test and F-Test for Variance methods are not applicable.In this case, the model failed to discern the prerequisites leading to misselection of inapplicable methods.LLaMA-3
7 b 0 -s h o t 7 b 0 -s h o t -C o T 7 b 1 -s h o t 7 b 1 -s h o t -C o T 1 3 b 0 -s h o t 1 3 b 0 -s h o t -C o T 1 3 b 1 -s h o t 1 3 b 1 -s h o t -C o T 0.0 0.G P T -3 . 5 T 0 -s h o t G P T -3 . 5 T 0 -s h o t -C o T G P T -3 . 5 T 1 -s h o t G P T -3 . 5 T 1 -s h o t -C o T G P T -4 0 -s h o t G P T -4 0 -s h o t -C o T G P T -4 1 -s h o t G P T -4 1 -s h o t -C o T G P T -4 o 0 -s h o t G P T -4 o 0 -s h o t -C o T G P T -4 o 1 -s h o t G P T -4 o 1 -s h o t -C</p>
<p>E.2 Error Analysis for Introducing CoT</p>
<p>In Section 3.3, we mention that LLMs' overall performance can slightly decrease if CoT is introduced, especially in smaller LLMs.Table 12 illustrates the accuracy changes after introducing the CoT strategy in different experiments, noted as ∆Acc(C, M), and Figure 9 shows the comparison of the error distributions on experiments with or without CoT.</p>
<p>As presented in Table 12, the 0-shot overall performance of smaller LLMs, namely LLaMA-2-7b, LLaMA-3-8b, and LLaMA-3-8b-Instruct, experiences a slight decline following the introduction of CoT.This performance degradation is most notable in descriptive statistics tasks.However, the negative impact of CoT on 1-shot learning experiments is mitigated, resulting in nearly identical overall performance between one-shot and one-shot-CoT experiments.In contrast, for larger models such as LLaMA-2-13b and GPT models, the influence of CoT is negligible.As illustrated in Figure 9, the introduction of CoT in smaller models including LLaMA-2-7b, LLaMA-3-8b, and LLaMA-3-8b-Instruct, tends to slightly increase the incidence of invalid answers and column selection errors, leading to a marginal decline in overall performance.However, the impact of CoT on error distribution in 1-shot experiments or on LLaMA-2-13b and GPT models is not prominent.
1 3 b 1 -s h o t 1 3 b 1 -s h o t + D K 7 b 1 -s h o t 7 b 1 -s h o t + D K 0.0 0.</p>
<p>E.3 Error Analysis for Introducing Domain Knowledge</p>
<p>As reported and analyzed in Section 3.2, the introduction of domain knowledge (DK) significantly boosts the overall performance of LLMs, particularly with larger proprietary LLMs like GPT models.However, as also shown in Table 2, it adversely affects some models' accuracy in correlation analysis (CA) and descriptive statistics (DS) tasks.</p>
<p>To further investigate, we also conduct an error analysis comparing 1-shot and 1-shot+DK approaches in the CA task, as depicted in Figure 10.It reveals a reduction in applicability errors across most LLMs, yet an increase in column selection errors and statistical task confusion among certain LLMs (i.e., LLaMA-2-13b, LLaMA-3-8b-Instruct, GPT-4, GPT-4o) following the introduction of DK.Conversely, DK tends to positively influence the performance of more complex and less common tasks like DCT, CTT, and VT, thus contributing positively to overall performance.</p>
<p>We propose that the observed phenomenon may be attributed to the following potential factors:</p>
<p>(1) Since DCT, CTT, and VT tasks are less commonly seen compared to CA and DS, LLMs are less involved in their pre-training data, making them relatively unfamiliar with these tasks, so the introduction of DK obtains substantial improvement; (2) For relatively simple and straightforward tasks (CA and DS), where LLMs possess some familiarity, the introduction of DK possibly leads to information obfuscation, resulting in varying levels of performance degradation on simpler subtasks.Deeper reasons may stem from various processes of LLMs' pre-training and the underlying mechanics of inference, which represents a valuable avenue for future research.</p>
<p>E.4 Details of Statistical Task Confusion Error</p>
<p>We select two analogous experiments with domain knowledge provided to open-source and proprietary LLMs to compare the confusion matrices with open-book human experiments, shown in Figure 11.</p>
<p>We observe that human participants most frequently confuse contingency table test tasks with correlation analysis tasks.This confusion is the primary factor contributing to their poor performance in contingency table test tasks.This issue arises from a lack of familiarity with the usage scenarios of contingency table tests resulting in difficulty in correctly differentiating task categories.Furthermore, both LLMs and humans occasionally confuse variance tests with correlation analysis tasks.</p>
<p>F Qualitative Analysis</p>
<p>In this section, we revise the prompt, instructing LLMs to provide more detailed reasoning for their selections.We present typical examples aimed at qualitative analysis.Examples demonstrated include both success and failure, as well as other notable cases, along with an analysis of their answers and reasoning.Each example consists of the statistical question, column information, model answer, and ground truth.For brevity, we may show only partial column information in some instances.{"columns": ["Education Level", "Job Title"],"methods": ["Chi-square Independence Test"]} Reason: The relevant columns for the statistical question are "Education Level" and "Job Title" as we are trying to find a connection between these two variables.The applicable method is the Chi-square Independence Test, which is used to determine if there is a significant association between the categorical variables.</p>
<p>F.1 Success Examples</p>
<p>Ground Truth: {"columns": ["Education Level", "Senior"], "methods": ["Chi-square Independence Test"]}   {"columns": ["sex", "romantic"], "methods": ["Chi-square Independence Test", "Fisher Exact Test"]}.</p>
<p>Reason: The question is asking if there is a relationship between gender and romantic involvement.This is a categorical variable question, so we need to use methods that are applicable to categorical data.The Chi-square Independence Test and Fisher Exact Test are both used to determine if there is an association between two categorical variables.In this case, we are looking at the relationship between the categorical variable "sex" and the categorical variable "romantic".These tests will help us determine if there is a statistically significant relationship between the two variables.Final Answer: The final answer is {"columns": ["sex", "romantic"], "methods": ["Chi-square Independence Test", "Fisher Exact Test"]}.I hope it is correct.Let me know if you need further assistance.Thank you.I am here to help.Have a great day.Goodbye.Good luck.Goodbye.Good luck......</p>
<p>Ground Truth:</p>
<p>{"columns": ["sex", "romantic"], "methods": ["Chi-square Independence Test", "Fisher Exact Test"]}</p>
<p>G Limitations</p>
<p>Coverage.We cover two primary aspects of specialized statistical analysis tasks: hypothesis testing and descriptive statistics in StatQA.While these two types of statistical methods already encompass many common statistical analysis tasks, the scope of StatQA can be broadened to include additional statistical tasks and methods, such as regression analysis and more advanced methods for hypothesis testing.Moreover, determining causality in observational data has always been a challenge in statistics, and inferring causal relationships from complex data remains an active area of research, which has not been covered yet in this paper.As discussed in Section 4, StatQA currently focuses on statistics, but we anticipate further extending it to other disciplines such as finance and operations research.</p>
<p>Experiment.We conduct experiments on LLaMA-2-7b/13b, ChatGPT and GPT-4, as well as the more recently released LLaMA-3-8b and GPT-4o.However, due to the rapid advancements in this field, our evaluation is limited to the capabilities of these representative LLMs and we can not cover a broader range of LLMs available.Additionally, due to time and financial constraints, we are presently unable to recruit more participants or utilize larger-scale datasets for more extensive human experiments.</p>
<p>Evaluation.Accuracy is used as the metric to measure the capabilities of LLMs in our experiments.However, for responses demonstrating the reasoning process, introducing step-by-step scoring can help uncover more hidden information and provide a more comprehensive evaluation.Zhang et al. proposed a CoT evaluation strategy based on GPT-4 [15], utilizing GPT-4 to score the steps.This scoring strategy is inspiring yet faces challenges, necessitating further investigation and improvement of its effectiveness and reliability.</p>
<p>H Ethic Statement</p>
<p>This paper evaluates the capabilities of LLMs in our task and reveals the different strengths of LLMs and humans.This is not intended to provoke anxiety, but rather to gain a better understanding of LLMs' abilities and to promote the development of LLMs.This study aims to foster discussions on how humans and LLMs can complement each other, and how humans can better utilize LLM tools in this era.Additionally, given the risks associated with LLMs producing erroneous or even toxic information, we advise readers to approach content generated by LLMs with caution.</p>
<p>In our human experiments, participants are provided with appropriate compensation (Section C.2) and ensured adequate rest periods between task blocks.We rigorously protect participants' personal information, ensuring their information remains confidential and is not disclosed in this paper and the GitHub repository.</p>
<p>Our source code and data are under GPL-3 license, and we follow the licenses of assets used in this paper, as listed in Table 21.[65] Framework used in fine-tuning.Apache-2.0LLaMA-2 Models [25] Evaluation and fine-tuning.</p>
<p>Custom License LLaMA-3 Models [26] Evaluation and fine-tuning.</p>
<p>Custom License GPT Models [27][28][29] Evaluation.Custom License Rdatsets [22] Select certain tabular data to form the training set.GPL-3 Kaggle [21] Select certain tabular data to form StatQA.</p>
<p>Table-specific</p>
<p>I Datasheet for StatQA</p>
<p>In this section, we use the framework of Datasheets for Datasets [66] to form a datasheet for StatQA, aiming to document the motivation, composition, collection process, recommended uses, and other information for our benchmark StatQA.</p>
<p>I.1 Motivation</p>
<p>Q1.</p>
<p>For what purpose was the dataset created?Was there a specific task in mind?</p>
<p>In statistical analysis, it is the core literacy of a qualified statistician to identify pertinent data and discern suitable statistical methods through consideration of task-specific scenarios and assessment of the applicability of various methods.However, recent studies tend to prioritize computational outcomes from conventional methods, which can be augmented by external tools, studies involving more specialized statistical tasks and assessment of methods' applicability are rare.Therefore, to evaluate LLMs' proficiency in specialized statistical tasks and their applicability assessment capabilities for statistical methods, particularly for hypothesis testing methods, we curate StatQA to conduct systematic experiments on current LLMs and organize a comparative study between LLMs and humans.</p>
<p>Q2. Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?</p>
<p>The Q7.Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</p>
<p>StatQA is a newly curated benchmark.</p>
<p>Q8.What data does each instance consist of?"Raw" data (e.g., unprocessed text or images) or features?</p>
<p>The content of each instance includes the name of the source table, statistical question, task category, difficulty level, data columns involved, preliminary results, and ground truth, please refer to Section 2.3.</p>
<p>Q9.Is there a label or target associated with each instance?</p>
<p>Yes, the label or target for each instance is the ground truth, which includes the selection of relevant data columns and all applicable methods.Please refer to Section 2.3.</p>
<p>Q10. Is any information missing from individual instances?</p>
<p>No.</p>
<p>Q11. Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?</p>
<p>Only the authors of this paper were involved in curating, processing, and reviewing the dataset, and no compensation was provided.</p>
<p>Q26.</p>
<p>Over what timeframe was the data collected?Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)?</p>
<p>Between January to May 2024.</p>
<p>Q27. Were any ethical review processes conducted (e.g., by an institutional review board)?</p>
<p>Not applicable.StatQA is a synthesized benchmark containing no sensitive information.</p>
<p>Q28.Does the dataset relate to people?</p>
<p>No.</p>
<p>Q29. Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?</p>
<p>Not applicable.</p>
<p>Q30. Were the individuals in question notified about the data collection?</p>
<p>Not applicable.</p>
<p>Q31. Did the individuals in question consent to the collection and use of their data?</p>
<p>Not applicable.</p>
<p>Q32.If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?</p>
<p>Not applicable.</p>
<p>Q33.</p>
<p>Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted?</p>
<p>Not applicable.</p>
<p>Q34. Any other comments?</p>
<p>No.</p>
<p>I.4 Preprocessing, Cleaning, and/or Labeling Q35.Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?</p>
<p>Yes. Please refer to Section 2.3.</p>
<p>Q36. Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?</p>
<p>Yes. Raw data can be found in our GitHub repository.</p>
<p>Q37.Is the software used to preprocess/clean/label the instances available?</p>
<p>Yes.We have provided scripts for certain data-cleaning, processing, and extraction tasks in our GitHub repository.This repository is open-source and accessible to facilitate potential further research.</p>
<p>Q38. Any other comments?</p>
<p>No.</p>
<p>I.5 Uses</p>
<p>Q39. Has the dataset been used for any tasks already?</p>
<p>StatQA is newly proposed by us, please refer to Section 3 for our usage.</p>
<p>Q40.Is there a repository that links to any or all papers or systems that use the dataset?</p>
<p>Figure 1 :
1
Figure 1: An Example of Statistical Analysis Task</p>
<p>Figure 2 :Figure 3 :
23
Figure 2: An Example in StatQA.(1) Dataset (D) is the datasets for statistical tasks; (2) Question (Q * (C)) is a statistical question, refined by GPT-3.5-Turbo;(3) Task indicates the statistical task category; (4) Difficulty includes easy and hard levels.(5) Relevant Columns (C) are data columns relevant to the statistical question; (6) Results (R) include applicable statistical methods (M), the computational results, and preliminary conclusions;(7) Ground Truth includes relevant columns (C) and all applicable methods (M) for this statistical task;</p>
<p>Figure 4 :Figure 3 and
43
Figure 4: The Pipeline for Synthesizing StatQA</p>
<p>Figure 5 :
5
Figure 5: Best Results of Each Model in Five Sub-tasks (Section 2.1).</p>
<p>A:</p>
<p>Detailed Related Work B: More Details of StatQA C: More Details of Experimental Setups D: More Experimental Results E: More Error Analysis F: Qualitative Analysis G: Limitations H: Ethic Statement I: Datasheet for StatQA A Detailed Related Work A.1 Large Language Models (LLMs)</p>
<p>Figure 7 :
7
Figure 7: More Examples in StatQA.</p>
<p>Figure 8 .
8
The examples for human experiments are substantially evenly divided into task blocks based on task category and difficulty, for closed-book test and open-book test respectively.Each participant needs to complete two blocks, using different answering modes (closed-book first and then open-book) for each block.Consistent with LLMs' experiments, participants must complete the task block for the closed-book test independently without consulting any external references, and domain knowledge in Table 9 will also be provided to participants in open-book human experiments.Participants are compensated about $8 per hour and about $32 in total for each participant.To ensure all participants clearly understand our tasks as well as the requirements of answering modes in experiments and are familiar with operations for the testing platform, we perform all human experiments as following procedures: (1) Introduction to the task and requirements by the organizers;(2) Participants watch the tutorial video of the testing platform; (3) Distribute accounts and passwords to participants, who will then log into the platform for corresponding experiments.</p>
<p>Figure 8 :
8
Figure 8: GUI of the Testing Platform for Human Experiments.</p>
<p>Figure 9 :
9
Figure 9: Distribution of Error Categories Across with/without CoT Experiments.</p>
<p>Figure 10 :
10
Figure 10: Distribution of Error Categories Across with/without DK on Correlation Analysis Task.</p>
<p>Figure 11 :
11
Figure 11: Comparison of Confusion Matrices on Statistical Tasks.(a) and (b): Confusion matrices of LLaMA-3-8b and GPT-4 with 1-shot+DK; (c) and (d): Confusion matrices of open-book experiments for non-statistical and statistical background participants respectively.The overall Acc(C, M) of these four experiments: (a) 36.11%,(b) 53.22%, (c) 34.48%,(d) 53.45%.</p>
<p>Table 1 :
1
Statistics of StatQA
ItemTabular DataQuestion Length (Chars)Difficulty#-ExamplesAvg #-Rows Avg #-Cols Max MinAvgEasyHard StatQA mini-StatQAStats6,22814346211137,401 4,22211,6231,163
of different LLMs; (G2) Support for Evaluating Statistical Literacy:</p>
<p>Table Test (
Test
CTT); Distribution Compliance Test (DCT); Variance Test (VT).The involved statistical tasks in each category are shown in Table 3 (Section B.2).</p>
<p>2.2 StatQA CharacteristicsFigure2shows an example in StatQA, which includes the dataset (tabular data), statistical question, task category, difficulty level, relevant column information, preliminary results, and ground truth.More examples are presented in Figure7in the appendix.</p>
<p>Ques�on Refinement Set Target Answers Prerequisites Check and Computa�on Sta�s�cal Ques�on Synthesis Difficulty Balancing and Dataset Spli�ng
TargetMethod(s)</p>
<p>Based on these labels, we expand the underrepresented categories and sample the abundant ones to obtain balanced synthesized examples in terms of task difficulty.Next, we split these synthesized examples to ensure no table overlaps in training and test sets: data synthesized from source tables No. 1 to 36</p>
<p>will be used for subsequent tests (i.e., StatQA), while tables No. 37 to 78 will be used for training (i.e., training set D train ).The D train is used to fine-tuning LLMs.</p>
<p>Metrics.Accuracy of relevant data columns and applicable methods selections, noted as Acc(C, M), is used as our metrics to evaluate if LLMs or participants truly understand the question and the applicability of statistical methods.Acc(C, M) refers to the proportion of methods and column selections fully aligned with the ground truth without any omissions or incorrect selections:
calculated between original questions and refined versions in StatQAis 0.126, 0.920 respectively. The low BLEU reflects notable vocabulary differences, while the highBERTScore suggests semantic consistency, collectively indicating adequate rephrasing that preservesthe original meaning. (3) Expert Reviews: Last but most importantly, we conduct manual reviews bytwo post-graduate students in statistics to carefully check all examples in StatQA.3 Experiments3.1 SetupExperimental Protocols. We design experiments for LLMs similar to human statisticians' mindset,as presented in Figure 1, to evaluate the abilities of LLMs in statistical tasks. Because of the limitationof input tokens, we provide LLMs column information instead of the whole table, including thecolumn headers, number of rows, data type, and normality. In the experiment, the LLMs need to pickheaders of relevant data columns, assess the methods' applicability, and select all statistical methodsthat fit the usage scenario and prerequisites as statisticians, then respond in a specific format. SinceLLM responses might be invalid or include irrelevant content, cleaning and extraction are necessary,then comparing the extracted answers to the ground truth for evaluation. In the human experiments,we use the same protocol for consistency and develop a testing platform to facilitate participantselection. More details of experimental setups including hyperparameters, prompts, procedures andGUI of the testing platform used in human experiments, are provided in Section C.</p>
<p>Table 2 :
2
Experimental Results of Acc(C, M)(%) on mini-StatQA.The 1st , 2nd , 3rd place results in all experiments are highlighted in red, blue, and green respectively.The bold results are the best in each section.The underlined results are the leading ones of each subgroup in overall Acc(C, M), whose performances in sub-tasks are shown in Figure5.CA: Correlation Analysis; CTT: Contingency Table Test; DCT: Distribution Compliance Test; VT: Variance Test; DS: Descriptive Statistics.
ModelStrategyOverallCACTT DCTVTDSOpen-source LLMs: LLaMA-2/30-shot8.081.791.172.126.97 25.481-shot14.960.606.255.93 19.26 37.07LLaMA-2 7B0-shot-CoT6.361.190.782.125.74 19.691-shot-CoT14.451.794.308.48 19.67 33.211-shot+DK16.080.607.429.32 18.44 38.610-shot9.291.790.398.483.28 29.341-shot17.979.525.479.322.05 58.69LLaMA-2 13B0-shot-CoT9.032.380.009.322.87 27.801-shot-CoT17.636.559.389.750.41 56.371-shot+DK20.298.337.03 16.53 11.48 52.900-shot23.561.190.00 16.53 16.39 74.521-shot31.9017.86 8.20 18.64 25.41 82.63LLaMA-3 8B0-shot-CoT22.011.190.39 15.68 13.93 70.271-shot-CoT32.2414.29 5.86 19.92 29.10 84.171-shot+DK36.1126.79 20.31 29.24 15.98 83.010-shot13.6710.12 13.28 5.091.23 35.911-shot28.2026.79 12.11 13.56 9.84 75.68LLaMA-3 8B Instruct0-shot-CoT11.6110.71 14.84 6.780.00 24.321-shot-CoT28.2926.19 16.80 16.10 9.84 69.501-shot+DK27.7719.64 22.27 20.34 8.20 63.71Proprietary LLMs: GPT-3.5-Turbo, GPT-4 and GPT-4o0-shot37.4047.02 31.25 26.27 12.71 70.661-shot40.7653.57 12.50 27.54 26.23 86.10GPT-3.5-Turbo0-shot-CoT38.1745.24 33.59 25.85 13.93 72.201-shot-CoT39.6451.79 10.94 26.70 26.23 84.561-shot+DK49.3662.50 35.55 38.98 26.23 85.710-shot42.3966.67 20.70 45.76 2.46 82.631-shot47.9867.86 26.56 44.07 14.75 91.12GPT-40-shot-CoT43.3467.86 23.44 46.19 1.64 83.781-shot-CoT47.4667.26 30.08 41.95 11.07 91.121-shot+DK53.2264.88 43.75 49.58 20.08 89.560-shot44.2362.50 19.53 25.00 31.56 86.491-shot49.3669.05 26.56 30.93 34.43 89.97GPT-4o0-shot-CoT44.7163.10 20.70 24.58 32.38 86.491-shot-CoT48.6767.86 25.78 28.81 32.79 91.891-shot+DK64.8361.31 65.23 59.32 46.31 89.19Fine-tuned LLMsSFT LLaMA-2 7B0-shot66.7269.05 35.94 83.48 54.51 91.89SFT LLaMA-3 8B0-shot77.1379.76 65.23 88.56 55.33 97.30SFT LLaMA-3 8B Instruct0-shot75.9269.64 68.75 85.17 57.38 96.14Human experiments (On subset of mini-StatQA)Human (Non-Stats)Closed-book Open-book18.10 34.485.88 52.94 0.00 30.44 8.33 84.62 3.85 8.70 0.00 65.39Human (Stats)Closed-book Open-book23.28 53.4529.41 0.00 17.39 0.00 69.23 47.06 23.08 65.22 37.50 92.31</p>
<p>StatQA contains 11,623 examples and mini-StatQA contains 1,163 examples across five typical categories of statistical tasks.To facilitate further research, we make our code and data available at https://statqa.github.io/,under GPL-3 license.</p>
<p>B.1 Access to StatQA</p>
<p>Table 3 :
3
Table of Covered Statistical Task Categories and Methods in StatQA.
CategoryDescriptionMethodsCorrelation AnalysisAssess the strength and direction of the correlation.Pearson Correlation Coefficient Spearman Correlation Coefficient Kendall Correlation Coefficient Partial Correlation CoefficientContingency Table TestEvaluate the indepen-dence between catego-rical variables.Chi-square Independence Test Mantel-Haenszel Test Fisher Exact TestAnderson-Darling TestLilliefors TestDistribution Compliance TestHow well a data set conforms to a specified distribution.Shapiro-Wilk Test of Normality Kolmogorov-Smirnov Test for Normality Kolmogorov-Smirnov Test for Uniform distribution Kolmogorov-Smirnov Test for Gamma distributionKolmogorov-Smirnov Test for Exponential distributionKolmogorov-Smirnov Test (Distributions Comparison)Variance TestCompare the variability of groups to assess statistical differencesF-Test for Variance Mood Variance Test Levene Test Bartlett TestDescriptiveSummarize and describeMean, Median, Mode, Range, Quartile,Statisticsdata features.Standard Deviation, Skewness, Kurtosis</p>
<p>Table 4 :
4
Examples of statistical question templates used in StatQA construction.What is the average of the {Column} values?What's the most common value in {Column}?How wide is the range of values in {Column}?Can you calculate the quartiles for {Column}?What is the measure of standard variability for {Column}?How does the asymmetry of {Column}?How peaked is the distribution of {Column}?
CategoryQuestion Template ExamplesHow strong is the correlation between {Column 1} and {Column 2}?Correlation AnalysisIs there a linear relationship between {Column 1} and {Column 2}? When {Control Column} is fixed, how do {Column 1} and {Column 2}correlate?Is {Column 1} independent of {Column 2}?How are the categories of {Column 1} reflected in the frequency ofContingency Table Test{Column 2}?Is there a significant difference in the impact of {Column 1} on {Column2} across different levels of {Strata Column}?Distribution Compliance TestDoes {Column} follow a normal distribution? Is the {Distribution} distribution a good fit for {Column} data? Do {Column 1} and {Column 2} follow a similar distribution curve?Can we assume there is no major difference in variances for {Column 1}Variance Testand {Column 2}? Is the degree of spread in {Column 1} similar to that in {Column 2}?Are the variances in {Column 1} and {Column 2} significantly different?Descriptive StatisticsDatasetQuestionTaskDifficultyRelevant ColumnsResultsGround TruthDoes a linear relationshipLos Angelesexist between the totalLibrarycirculation of electronicMonthlymedia materials and theStatisticsnumber of new librarycardholders?</p>
<p>Table 5 :
5
Domains Distribution of Source Tables Tables' Domain StatQA (for test) Dtrain (for training)
Education95Medicine47Science43Engineering34Economy1013Life79</p>
<p>Table 6 :
6
Prompt for Statistical Question Refinement.Input origin question sentences to be refined, and descriptive information (if any), instruct GPT-3.5-Turbo to refine and paraphrase for more coherent and diverse expressions without changing the original meaning.You need to improve the English expression of the given sentence to make it grammatically and semantically correct, statistically rigorous and more coherent in expression.The given sentence will contain the names of the variables to be analyzed, and you are encouraged to based on the description, change them to more natural expressions without affecting the meaning.You can be flexible in how you improve the expression, but you must not change the original meaning of the sentence.
Prompt for LLM Statistical Question RefinementSYSTEM:I'm a native English-speaking statistician. I will help you refine and improve expressions of statisticalsentences without changing the original meaning. Please tell me the sentence you want to refine.USER:Suppose you're a native English-speaking statistician, and I will give you a sentence about a statisticalproblem. Variable description (Optional):...... Description for involved variables (columns)Sentence:...... Preliminary statistical question to be refined</p>
<p>Table 10 :
10
More Open-source LLMs Experimental Results of Acc(C, M) (%) on mini-StatQA.The 1st , 2nd , 3rd place results in all additional experiments here are highlighted in red, blue, and green respectively.The bold results are the best in each section.
ModelStrategy OverallCACTT DCTVTDSSmaller Open-source LLMsQwen2 0.5B Instruct0-shot 1-shot1.38 2.410.00 0.000.00 0.001.27 3.810.00 1.235.02 6.18Qwen2 1.5B Instruct0-shot 1-shot15.99 20.380.00 0.601.56 7.421.27 4.2418.03 52.12 22.54 58.69Larger Open-source LLMsQwen2 72B Instruct0-shot 1-shot44.71 47.1257.74 30.47 36.02 10.66 90.35 66.07 30.47 28.81 21.72 91.89LLaMA-3 70B0-shot 1-shot27.09 34.1435.12 50.602.34 9.3811.02 19.67 67.95 14.83 24.59 74.52Yi 34B Chat0-shot 1-shot28.03 33.9641.07 13.67 14.41 45.24 19.14 24.15 10.25 72.59 0.00 72.59</p>
<p>Table 7 :
7
Prompt Structure for Answer Generation in LLMs' Experiments.For zero-shot learning, the part of the demonstration example will not be included.
General Prompt Structure for LLM Answer Generation### Task Description:You need to select relevant columns and all applicable methods from provided list for the given statisticalquestion.### Instruction:You should only reply with one answer in JSON format containing two keys: "columns" and "methods".The value of "columns" is a list of columns' headers relevant to the given statistical question, and thevalue of "methods" is a list containing all methods you think applicable. For example: "columns": ["c1",8<example start># Column Information:......# Statistical Question:......# Correct Answer:......</example end>### Column Information:...... Column information of the current source tabular data### Statistical Question:...... Current statistical question### Response:For zero-shot-CoT experiments:Let's work this out in a step-by-step way to be sure we have the right answer.General:The answer of relevant columns and applicable methods in JSON format is:
"c2", "..."], "methods": ["m1", "m2", "..."].If you think there is a strata or control variable involved, put its column header in the last item of the columns list.Ensure your methods selection is only limited to the classification list provided.### Classification List: Correlation Analysis: Pearson Correlation Coefficient, Spearman Correlation Coefficient, Kendall Correlation Coefficient, Partial Correlation Coefficient; Distribution Compliance Test: Anderson-Darling Test, Shapiro-Wilk Test of Normality, Kolmogorov-Smirnov Test for Normality, Lilliefors Test, Kolmogorov-Smirnov Test, Kolmogorov-Smirnov Test for Uniform distribution, Kolmogorov-Smirnov Test for Gamma distribution, Kolmogorov-Smirnov Test for Exponential distribution; Contingency Table Test: Chi-square Independence Test, Fisher Exact Test, Mantel-Haenszel Test; Descriptive Statistics: Mean, Median, Mode, Range, Quartile, Standard Deviation, Skewness, Kurtosis; Variance Test: Mood Variance Test, Levene Test, Bartlett Test, F-Test for Variance.### Demonstration Example: (Optional) ...... Example for few-shot learning, as shown in Table</p>
<p>Table 8 :
8
Examples Used in Few-shot Learning and CoT Respectively.In accordance with Section 3.1.1,we have design five examples corresponding to five task categories, for random selection.
An Example Used in One-shot Learning# Column Information:column_header: TV Ad Budget ($); data_type: quantitative; num_of_rows: 200; is_normality: False.column_header: Radio Ad Budget ($); data_type: quantitative; num_of_rows: 200; is_normality: False.column_header: Newspaper Ad Budget ($); data_type: quantitative; num_of_rows: 200; is_normality:False.column_header: Sales ($); data_type: quantitative; num_of_rows: 200; is_normality: False.# Statistical Question: Is there a linear correlation between the TV advertising budget ($) and salesrevenue ($) in this study?# Correct Answer: {"columns": ["TV Ad Budget ($)", "Sales ($)"], "methods": ["Pearson CorrelationCoefficient", "Spearman Correlation Coefficient", "Kendall Correlation Coefficient"]}An Example Used in One-shot-CoT# Column Information:column_header: TV Ad Budget ($); data_type: quantitative; num_of_rows: 200; is_normality: False.column_header: Radio Ad Budget ($); data_type: quantitative; num_of_rows: 200; is_normality: False.column_header: Newspaper Ad Budget ($); data_type: quantitative; num_of_rows: 200; is_normality:False.
column_header: Sales ($); data_type: quantitative; num_of_rows: 200; is_normality: False.# Statistical Question: Is there a linear correlation between the TV advertising budget ($) and sales revenue ($) in this study?# Correct Answer: Firstly, the question asks about TV advertising budget and sales revenue, so the relevant columns are TV Ad Budget ($) and Sales ($).Secondly, given that it asks about correlation, and the variables involved are all quantitative, some methods from Correlation Analysis can be applicable.Thirdly, there are enough samples of 200, with only two variables involved, and no control variables, so the applicable methods are Pearson Correlation Coefficient, Spearman Correlation Coefficient, Kendall Correlation Coefficient.Hence, the answer is: {"columns": ["TV Ad Budget ($)", "Sales ($)"], "methods": ["Pearson Correlation Coefficient", "Spearman Correlation Coefficient", "Kendall Correlation Coefficient"]}</p>
<p>Table 9 :
9
Introduce Domain Knowledge (DK) in the Prompt.Based on one-shot learning prompt structure and replace the "Classification List" part with the contents in this table.Domain knowledge shown in this table is also provided to participants in open-book human experiments for consistency.
Introduce Domain Knowledge (DK) in Prompt### Methods and applicable usage scenarios:# Correlation AnalysisPearson Correlation Coefficient, Spearman Correlation Coefficient: Correlation analysis for two quanti-tative variables;Kendall Correlation Coefficient: Correlation analysis for two quantitative variables, suitable for smallsamples;Partial Correlation Coefficient: Correlation analysis when involving controlling variable;# Distribution Compliance TestAnderson-Darling Test, Kolmogorov-Smirnov Test for Normality: Test for normality;Shapiro-Wilk Test of Normality: Test for normality, suitable for small samples;Lilliefors Test: Test for normality, suitable for large samples;Kolmogorov-Smirnov Test: Comparison of distribution between two independent samples;Kolmogorov-Smirnov Test for Uniform distribution, Kolmogorov-Smirnov Test for Gamma distribution,Kolmogorov-Smirnov Test for Exponential distribution: Test for corresponding distributions;# Contingency Table TestChi-square Independence Test: Contingency table test of large sample categorical variables;Fisher Exact Test: Contingency table test of small sample categorical variables;Mantel-Haenszel Test: Contingency table test when strata data to be controlled;# Variance Test
Mood Variance Test, Levene Test: Whether there is a significant difference; Bartlett Test, F-Test for Variance: Whether there is a significant difference in variance between normally distributed variables; # Descriptive Statistics Mean, Median, Mode, Range, Quartile, Standard Deviation, Skewness, Kurtosis.</p>
<p>Table 11 :
11
Examples for Different Error Types.The red texts indicate the incorrect parts.Is the Exponential distribution a suitable model for representing the distribution of Grade Point Average (GPA) of the applicants during their undergraduate studies?Ground Truth: {"columns": ["CGPA"], "methods": ["Kolmogorov-Smirnov Test for Exponential distribution"]} Model Answer: Please select the relevant columns and applicable methods for the given statistical question.Please select the relevant columns and applicable methods for the given statistical question... Column Selection Error Statistical Question: Is the variability in GRE scores not significantly different from that in Letter of Recommendation?
Examples for different error typesInvalid AnswerStatistical Question:</p>
<p>Table 12 :
12
∆Acc(C, M) (%) After Introducing CoT Strategy in Different Experiments.Positive number indicates an increase in Acc(C, M) compare with same prompting strategy without CoT, and negative number indicates a performance decrease after introducing CoT.
ModelStrategyOverallCACTT DCTVTDSOpen-source LLMs: LLaMA-2/3LLaMA-2 7B0-shot-CoT 1-shot-CoT-1.72 -0.52-0.60 -0.39 1.19 -1.950.00 2.54-1.23 0.41-5.79 -3.86LLaMA-2 13B0-shot-CoT 1-shot-CoT-0.26 -0.340.60 -0.39 -2.98 3.910.85 0.42-0.41 -1.64-1.54 -2.32LLaMA-3 8B0-shot-CoT 1-shot-CoT-1.55 0.340.00 -3.57 -2.34 0.39-0.85 -2.46 1.27 3.69-4.25 1.55LLaMA-3 8B Instruct0-shot-CoT 1-shot-CoT-2.06 0.090.60 -0.60 4.69 1.561.70 2.54-1.23 -11.58 0.00 -6.18Proprietary LLMs: ChatGPT, GPT-4 and GPT-4oChatGPT0-shot-CoT 1-shot-CoT0.77 -1.12-1.79 2.34 -1.79 -1.56 -0.85 -0.421.23 0.001.55 -1.54GPT-40-shot-CoT 1-shot-CoT0.95 -0.521.19 -0.59 3.52 2.740.42 -2.12 -3.69 -0.821.16 0.00GPT-4o0-shot-CoT 1-shot-CoT0.43 -0.690.59 -1.19 -0.78 -2.12 -1.64 1.17 -0.42 0.820.00 1.93</p>
<p>Table 13 :
13
A Success Example.Correctly select the relevant columns, identify the application scenario of independence testing, check the prerequisites of categorical data type, and correctly select all applicable methods: Chi-square Independence Test and Fisher Exact Test, which belongs to Contingency Table Test.The relevant columns for this statistical question are "ANXIETY" and "ALCOHOL CONSUM-ING" as they represent the variables of interest.The applicable methods for testing the independence assumption between these categorical variables are the Chi-square Independence Test and the Fisher Exact Test.
Statistical Question: Do the observed frequencies of anxiety symptoms and alcohol consumptionstatus conform to the independence assumption?Column Information:column_header: GENDER; data_type: categorical; num_of_rows: 309; is_normality: False.column_header: AGE; data_type: quantitative; num_of_rows: 309; is_normality: False.column_header: SMOKING; data_type: categorical; num_of_rows: 309; is_normality: False.column_header: ANXIETY; data_type: categorical; num_of_rows: 309; is_normality: False.column_header: ALCOHOL CONSUMING; data_type: categorical; num_of_rows: 309; is_normality:False.column_header: CHEST PAIN; data_type: categorical; num_of_rows: 309; is_normality: False.column_header: LUNG_CANCER; data_type: categorical; num_of_rows: 309; is_normality: False.......Model Answer:{"columns": ["ANXIETY", "ALCOHOL CONSUMING"],"methods": ["Chi-square IndependenceTest", "Fisher Exact Test"]}Reason: Ground Truth:{"columns": ["ANXIETY", "ALCOHOL CONSUMING"], "methods": ["Chi-square IndependenceTest", "Fisher Exact Test"]}</p>
<p>Table 14 :
14
A Success Example.Correctly select the relevant columns, identify the application scenario of variance test, check the prerequisites of normality, and correctly select all applicable methods.The applicable methods for this comparison include Mood Variance Test, Levene Test, Bartlett Test, and F-Test for Variance, which are used to determine if there is a significant difference in variance between the two normally distributed variables.
Statistical Question: Are the levels of dispersion in crop production similar between Nigeria andBangladesh?Column Information:column_header: Year; data_type: Other; num_of_rows: 4; is_normality: False.column_header: Nigeria; data_type: quantitative; num_of_rows: 4; is_normality: True.column_header: Japan; data_type: quantitative; num_of_rows: 4; is_normality: True.column_header: United States; data_type: quantitative; num_of_rows: 4; is_normality: True.column_header: Indonesia; data_type: quantitative; num_of_rows: 4; is_normality: True.column_header: Bangladesh; data_type: quantitative; num_of_rows: 4; is_normality: True.column_header: China; data_type: quantitative; num_of_rows: 4; is_normality: True.......Model Answer:{"columns": ["Nigeria", "Bangladesh"],"methods": ["Mood Variance Test", "Levene Test", "BartlettTest", "F-Test for Variance"]}Reason: The relevant columns for comparing the levels of dispersion in crop production betweenNigeria and Bangladesh are "Nigeria" and "Bangladesh" as they contain quantitative data related tocrop production. Ground Truth:{"columns": ["Nigeria", "Bangladesh"],"methods": ["Mood Variance Test", "Levene Test", "BartlettTest", "F-Test for Variance"]}</p>
<p>Table 15 :
15
A Failure Example.The "Height_m", "Weight_kg" are not normally distributed according to provided column information, but the model hallucinates the normality leading to failure of discern applicable methods, misselecting "Bartlett Test" and "F-Test for Variance" which should be used for normally distributed data.The statistical question is asking about the significant difference between the variations in height and weight of the Pokemon, which involves quantitative variables.Therefore, the relevant columns are "Height_m" and "Weight_kg".The applicable methods for testing the significant difference in variance between these two normally distributed variables are Mood Variance Test, Levene Test, Bartlett Test, and F-Test for Variance.
Statistical Question: Is there a statistically significant difference between the variations in heightand weight of the Pokemon?Column Information:column_header: Name; data_type: other; num_of_rows: 721; is_normality: False.column_header: HP; data_type: quantitative; num_of_rows: 721; is_normality: False.column_header: Attack; data_type: quantitative; num_of_rows: 721; is_normality: False.column_header: Defense; data_type: quantitative; num_of_rows: 721; is_normality: False.column_header: Speed; data_type: quantitative; num_of_rows: 721; is_normality: False.column_header: hasMegaEvolution; data_type: categorical; num_of_rows: 721; is_normality: False.column_header: Height_m; data_type: quantitative; num_of_rows: 721; is_normality: False.column_header: Weight_kg; data_type: quantitative; num_of_rows: 721; is_normality: False.column_header: Catch_Rate; data_type: quantitative; num_of_rows: 721; is_normality: False.......Model Answer:{"columns": ["Height_m", "Weight_kg"],"methods": ["Mood Variance Test", "Levene Test", "BartlettTest", "F-Test for Variance"]}Reason: Ground Truth:{"columns": ["Height_m", "Weight_kg"], "methods": ["Mood Variance Test", "Levene Test"]}</p>
<p>Table 16 :
16
A Failure Example.Select the irrelevant column; correct methods selection with partially correct reasoning for performing incomplete prerequisites check.
Statistical Question: Are there any indications of a connection between a person's educationalattainment and their occupation at a higher level?Column Information:column_header: Age; data_type: quantitative; num_of_rows: 6684; is_normality: False.column_header: Gender; data_type: categorical; num_of_rows: 6684; is_normality: False.column_header: Education Level; data_type: categorical; num_of_rows: 6684; is_normality: False.column_header: Job Title; data_type: categorical; num_of_rows: 6684; is_normality: False.column_header: Years of Experience; data_type: quantitative; num_of_rows: 6684; is_normality: False.column_header: Salary; data_type: quantitative; num_of_rows: 6684; is_normality: False.column_header: Country; data_type: categorical; num_of_rows: 6684; is_normality: False.column_header: Race; data_type: categorical; num_of_rows: 6684; is_normality: False.column_header: Senior; data_type: categorical; num_of_rows: 6684; is_normality: False.Model Answer:</p>
<p>Table 17 :
17
A Failure Example.Neglect prerequisites check and applicability assessment.The relevant columns for the statistical question are "Number of trees sold" and "Sales" as they represent the quantitative variables of interest.The applicable methods for this correlation analysis are Pearson Correlation Coefficient, Spearman Correlation Coefficient, and Kendall Correlation Coefficient, as they are suitable for analyzing the correlation between two quantitative variables.
Statistical Question: Are there any indications of a connection between a person's educationalattainment and their occupation at a higher level?Column Information:column_header: index; data_type: id; num_of_rows: 14; is_normality: False.column_header: Year; data_type: other; num_of_rows: 14; is_normality: False.column_header: Type of tree; data_type: categorical; num_of_rows: 14; is_normality: False.column_header: Number of trees sold; data_type: quantitative; num_of_rows: 14; is_normality: True.column_header: Average Tree Price; data_type: quantitative; num_of_rows: 14; is_normality: True.column_header: Sales; data_type: quantitative; num_of_rows: 14; is_normality: True.Model Answer:{"columns": ["Number of trees sold", "Sales"],"methods": ["Pearson Correlation Coefficient", "Spear-man Correlation Coefficient", "Kendall Correlation Coefficient"]}Reason:
Ground Truth: {"columns": ["Number of trees sold", "Sales"], "methods": ["Kendall Correlation Coefficient"]}</p>
<p>Table 18 :
18
A Failure Example.No meaningful identification given in the reasoning process, neglect prerequisites check and applicability assessment, and missed applicable methods.Sp_Def" as they contain the base Health Points and base Special Defense respectively, which are the variables being compared in the statistical question.We chose Pearson and Spearman correlation coefficients as the applicable methods because they are suitable for measuring the strength and direction of the relationship between two quantitative variables.
Statistical Question: Do changes in base Health Points (HP) correspond to similar changes in baseSpecial Defense (Sp_Def)?Column Information:column_header: Name; data_type: other; num_of_rows: 721; is_normality: False.column_header: HP; data_type: quantitative; num_of_rows: 721; is_normality: False.column_header: Attack; data_type: quantitative; num_of_rows: 721; is_normality: False.column_header: Defense; data_type: quantitative; num_of_rows: 721; is_normality: False.column_header: Sp_Atk; data_type: quantitative; num_of_rows: 721; is_normality: False.column_header: Sp_Def; data_type: quantitative; num_of_rows: 721; is_normality: False.column_header: Speed; data_type: quantitative; num_of_rows: 721; is_normality: False.......Model Answer:{"columns": ["HP", "Sp_Def"],"methods": ["Pearson Correlation Coefficient", "Spearman CorrelationCoefficient", "[Missing: Kendall Correlation Coefficient]"]}Reason: we selected the columns "HP" and "Ground Truth:{"columns": ["HP", "Sp_Def"],"methods": ["Pearson Correlation Coefficient", "Spearman CorrelationCoefficient", "Kendall Correlation Coefficient"]}</p>
<p Test_="Test&quot;," Variance="Variance" Variance_="Variance&quot;]" _Bartlett="&quot;Bartlett" _China_="&quot;China&quot;]," _F-Test="&quot;F-Test" _Levene="&quot;Levene" _Mood="[&quot;Mood" _Nigeria_="[&quot;Nigeria&quot;," _columns_:="&quot;columns&quot;:" _methods_:="&quot;methods&quot;:" for="for">Table 19 :
19
A Success Example with Hallucination and Incorrect Reasoning.The provided column information explicitly indicates that these two columns follow a normal distribution, not non-normal.Furthermore, assuming the variables are non-normally distributed makes the Bartlett Test and F-Test for Variance inapplicable and should not be chosen.The response exhibits hallucinations and inconsistencies, leading to erroneous reasoning.Test for Variance"], "reason": "The question is asking whether the annual crop production data for Nigeria and China are comparable in terms of variability.We need to compare the variability of the two countries' data.The first step is to check whether the data is normally distributed.Since the data type of both countries is quantitative, we can use the Shapiro-Wilk Test of Normality to check the normality of the data.The result shows that the data is not normally distributed.Therefore, we need to use non-parametric tests to compare the variability of the two countries' data.The Mood Variance Test, Levene Test, Bartlett Test, and F-Test for Variance are all non-parametric tests that can be used to compare the variability of two groups of data.Therefore, we can use these four tests to compare the variability of the annual crop production data for Nigeria and China."}Ground Truth:
Statistical Question: Are the annual crop production data for Nigeria and China comparable interms of variability?Column Information:column_header: Year; data_type: Other; num_of_rows: 4; is_normality: False.column_header: Nigeria; data_type: quantitative; num_of_rows: 4; is_normality: True.column_header: Japan; data_type: quantitative; num_of_rows: 4; is_normality: True.column_header: United States; data_type: quantitative; num_of_rows: 4; is_normality: True.column_header: Brazil; data_type: quantitative; num_of_rows: 4; is_normality: True.column_header: China; data_type: quantitative; num_of_rows: 4; is_normality: True.......Model Answer:{"columns": ["Nigeria", "China"], "methods": ["Mood Variance Test", "Levene Test", "Bartlett Test","F-</p>
<p>Table 20 :
20
A Success Example with Irrelevant Information.Smaller LLMs sometimes struggle with determining termination, leading to responses with excessive irrelevant information.
Statistical Question: Is there evidence in the data to indicate a relationship between gender andromantic involvement?Column Information:column_header: sex; data_type: categorical; num_of_rows: 649; is_normality: False.column_header: age; data_type: quantitative; num_of_rows: 649; is_normality: False.column_header: famsize; data_type: categorical; num_of_rows: 649; is_normality: False.column_header: studytime; data_type: quantitative; num_of_rows: 649; is_normality: False.column_header: romantic; data_type: categorical; num_of_rows: 649; is_normality: False.......Model Answer:</p>
<p>Table 21 :
21
Licenses List for Asserts Used
AssertUsageLicensevLLM [64]LLM interface used in development.Apache-2.0LLaMA-Factory</p>
<p>authors of this paper create the StatQA.The authors are from The Hong Kong University of Science and Technology (Guangzhou) and The Hong Kong University of Science and Technology.Please refer to the author list for more details.Q3.Who funded the creation of the dataset?The creation of StatQA is supported by NSF of China (62402409), Guangdong Basic and Applied Basic Research Foundation (2023A1515110545), CCF-Huawei Populus Grove Fund (CCF-HuaweiDB202403), and the Red Bird Grant, Research Grant from The Hong Kong University of Science and Technology (Guangzhou).What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?Each instance in StatQA corresponds to a statistical task which is a statistical question based on a source table.The LLMs or the participants in our human experiments should select relevant data columns and all applicable methods to solve the statistical question.Q6.How many instances are there in total (of each type, if appropriate)?StatQA contains 11,623 examples, mini-StatQA contains 1,163 examples which are stratified sampled from StatQA.
Q4. Any other comments?No.I.2 CompositionQ5.
Currently, no.Q41.What (other) tasks could the dataset be used for?We discuss research opportunities in the main content, including(1)improving LLMs' capabilities for statistical applications; (2) further expanding to obtain a more extensive benchmark dataset; and (3) exploring human-AI collaboration in statistical tasks.Please refer to Section 4 for more details.Q42.Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?No.Q43. Are there any tasks for which the dataset should not be used?No.Q44. Any other comments?No.I.6 DistributionQ45.Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?Q46.How will the dataset be distributed (e.g., tarball on website, API, GitHub)We make our source code and data public on the GitHub.Q47.When will the dataset be distributed?.I.7 MaintenanceQ52. Who will be supporting/hosting/maintaining the dataset?The authors is maintaining the dataset.Q53.How can the owner/curator/manager of the dataset be contacted (e.g., email address)?By email or raising issues in GitHub repository.Q54.Is there an erratum?Currently, no.If necessary, possible erratum will be released in the README file in GitHub repository.
50 years of data science. D Donoho, Journal of Computational and Graphical Statistics. 2642017</p>
<p>Statistical foundations of data science. J Fan, R Li, C.-H Zhang, H Zou, 2020Chapman and Hall/CRC</p>
<p>Data science in statistics curricula: Preparing students to "think with data. J Hardin, R Hoerl, N J Horton, D Nolan, B Baumer, O Hall-Holt, P Murrell, R Peng, P Roback, D Temple Lang, The American Statistician. 6942015</p>
<p>X Liu, S Shen, B Li, P Ma, R Jiang, Y Luo, Y Zhang, J Fan, G Li, N Tang, abs/2408.05109A survey of NL2SQL with large language models: Where are we, and where are we going?" CoRR. 2024</p>
<p>Deepeye: Towards automatic data visualization. Y Luo, X Qin, N Tang, G Li, ICDE. 2018</p>
<p>Making data visualization more efficient and effective: a survey. X Qin, Y Luo, N Tang, G Li, VLDB J. 2912020</p>
<p>Visclean: Interactive cleaning for progressive visualization. Y Luo, C Chai, X Qin, N Tang, G Li, Proc. VLDB Endow. VLDB Endow202013</p>
<p>Coinsight: Visual storytelling for hierarchical tables with connected insights. G Li, R Li, Y Feng, Y Zhang, Y Luo, C H Liu, IEEE Trans. Vis. Comput. Graph. 3062024</p>
<p>Toolqa: A dataset for llm question answering with external tools. Y Zhuang, Y Yu, K Wang, H Sun, C Zhang, Advances in Neural Information Processing Systems. 202436</p>
<p>. Wolframalpha, </p>
<p>The role of chatgpt in data science: how ai-assisted conversational interfaces are revolutionizing the field. H Hassani, E S Silva, Big data and cognitive computing. 2023762</p>
<p>Generative AI for visualization: State of the art and future directions. Y Ye, J Hao, Y Hou, Z Wang, S Xiao, Y Luo, W Zeng, Vis. Informatics. 812024</p>
<p>Verifai: Verified generative AI. N Tang, C Yang, J Fan, L Cao, Y Luo, A Y Halevy, CIDR. www.cidrdb.org. 2024</p>
<p>Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. P Lu, H Bansal, T Xia, J Liu, C Li, H Hajishirzi, H Cheng, K.-W Chang, M Galley, J Gao, International Conference on Learning Representations (ICLR). 2024</p>
<p>Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems. R Zhang, D Jiang, Y Zhang, H Lin, Z Guo, P Qiu, A Zhou, P Lu, K.-W Chang, P Gao, arXiv:2403.146242024arXiv preprint</p>
<p>Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data. X Liu, Z Wu, X Wu, P Lu, K.-W Chang, Y Feng, arXiv:2402.176442024arXiv preprint</p>
<p>Infiagent-dabench: Evaluating agents on data analysis tasks. X Hu, Z Zhao, S Wei, Z Chai, Q Ma, G Wang, X Wang, J Su, J Xu, M Zhu, Y Cheng, J Yuan, J Li, K Kuang, Y Yang, H Yang, F Wu, 2024</p>
<p>P Newbold, W L Carlson, B M Thorne, Statistics for business and economics. Pearson2013</p>
<p>Understanding statistical hypothesis testing: The logic of statistical inference. F Emmert-Streib, M Dehmer, Machine Learning and Knowledge Extraction. 20191</p>
<p>Approximate query processing: What is new and where to go? a survey on approximate query processing. K Li, G Li, Data Science and Engineering. 342018</p>
<p>Machine Learning and Data Science Community. </p>
<p>Rdatasets: A collection of datasets originally distributed in various R packages. V Arel-Bundock, 2024r package version 1.0.0. [Online</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Bertscore: Evaluating text generation with bert. T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, International Conference on Learning Representations. 2020</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Meta Llama 3. Meta Llama. </p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 202235</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>. Gpt-4o Hello, </p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, W Chen, abs/2106.09685CoRR. 2021</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>The dawn of natural language to SQL: are we fully ready?. B Li, Y Luo, C Chai, G Li, N Tang, Proc. VLDB Endow. VLDB Endow202417</p>
<p>Naturalproofs: Mathematical theorem proving in natural language. S Welleck, J Liu, R L Bras, H Hajishirzi, Y Choi, K Cho, arXiv:2104.011122021arXiv preprint</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. P Lu, L Qiu, K.-W Chang, Y N Wu, S.-C Zhu, T Rajpurohit, P Clark, A Kalyan, arXiv:2209.146102022arXiv preprint</p>
<p>A survey of deep learning for mathematical reasoning. P Lu, L Qiu, W Yu, S Welleck, K.-W Chang, arXiv:2212.105352022arXiv preprint</p>
<p>Lila: A unified benchmark for mathematical reasoning. S Mishra, M Finlayson, P Lu, L Tang, S Welleck, C Baral, T Rajpurohit, O Tafjord, A Sabharwal, P Clark, A Kalyan, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. Y Fu, L Ou, M Chen, Y Wan, H Peng, T Khot, arXiv:2305.173062023arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Mathematical capabilities of chatgpt. S Frieder, L Pinchetti, R.-R Griffiths, T Salvatori, T Lukasiewicz, P Petersen, J Berner, Advances in Neural Information Processing Systems. 202436</p>
<p>Theoremqa: A theoremdriven question answering dataset. W Chen, M Yin, M Ku, P Lu, Y Wan, X Ma, J Xu, X Wang, T Xia, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, arXiv:2307.106352023arXiv preprint</p>
<p>Haichart: Human and AI paired visualization system. Y Xie, Y Luo, G Li, N Tang, Proc. VLDB Endow. VLDB Endow202417</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>What can large language models do in chemistry? a comprehensive benchmark on eight tasks. T Guo, B Nan, Z Liang, Z Guo, N Chawla, O Wiest, X Zhang, Advances in Neural Information Processing Systems. 202336</p>
<p>Knowledge distillation of llm for education. E Latif, L Fang, P Ma, X Zhai, arXiv:2312.158422023arXiv preprint</p>
<p>Large language models in medicine. A J Thirunavukarasu, D S J Ting, K Elangovan, L Gutierrez, T F Tan, D S W Ting, Nature medicine. 2982023</p>
<p>S Wu, O Irsoy, S Lu, V Dabravolski, M Dredze, S Gehrmann, P Kambadur, D Rosenberg, G Mann, arXiv:2303.17564Bloomberggpt: A large language model for finance. 2023arXiv preprint</p>
<p>Alpha-gpt: Human-ai interactive alpha mining for quantitative investment. S Wang, H Yuan, L Zhou, L M Ni, H.-Y Shum, J Guo, arXiv:2308.000162023arXiv preprint</p>
<p>Natural language to visualization by neural machine translation. Y Luo, N Tang, G Li, J Tang, C Chai, X Qin, IEEE Trans. Vis. Comput. Graph. 2812022</p>
<p>Sevi: Speech-to-visualization through neural machine translation. J Tang, Y Luo, M Ouzzani, G Li, H Chen, SIGMOD Conference. ACM2022</p>
<p>Deepeye: A data science system for monitoring and exploring COVID-19 data. Y Luo, N Tang, G Li, W Li, T Zhao, X Yu, IEEE Data Eng. Bull. 4322020</p>
<p>Steerable self-driving data visualization. Y Luo, X Qin, C Chai, N Tang, G Li, W Li, IEEE Trans. Knowl. Data Eng. 3412022</p>
<p>Learned data-aware image representations of line charts for similarity search. Y Luo, Y Zhou, N Tang, G Li, C Chai, L Shen, Proc. ACM Manag. Data. 11292023</p>
<p>Towards natural language interfaces for data visualization: A survey. L Shen, E Shen, Y Luo, X Yang, X Hu, X Zhang, Z Tai, J Wang, IEEE Trans. Vis. Comput. Graph. 2962023</p>
<p>Interactive cleaning for progressive visualization through composite questions. Y Luo, C Chai, X Qin, N Tang, G Li, ICDE. IEEE2020</p>
<p>Data management for machine learning: A survey. C Chai, J Wang, Y Luo, Z Niu, G Li, IEEE Trans. Knowl. Data Eng. 3552023</p>
<p>Synthesizing natural language to visualization (NL2VIS) benchmarks from NL2SQL benchmarks. Y Luo, N Tang, G Li, C Chai, W Li, X Qin, SIGMOD Conference. ACM2021</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. W Zhong, R Cui, Y Guo, Y Liang, S Lu, Y Wang, A Saied, W Chen, N Duan, arXiv:2304.063642023arXiv preprint</p>
<p>Claude anthropic. </p>
<p>J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, B Hui, L Ji, M Li, J Lin, R Lin, D Liu, G Liu, C Lu, K Lu, J Ma, R Men, X Ren, X Ren, C Tan, S Tan, J Tu, P Wang, S Wang, W Wang, S Wu, B Xu, J Xu, A Yang, H Yang, J Yang, S Yang, Y Yao, B Yu, H Yuan, Z Yuan, J Zhang, X Zhang, Y Zhang, Z Zhang, C Zhou, J Zhou, X Zhou, T Zhu, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>. Gemini, </p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Efficient memory management for large language model serving with pagedattention. W Kwon, Z Li, S Zhuang, Y Sheng, L Zheng, C H Yu, J E Gonzalez, H Zhang, I Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Llamafactory: Unified efficient fine-tuning of 100+ language models. Y Zheng, R Zhang, J Zhang, Y Ye, Z Luo, Y Ma, arXiv:2403.133722024arXiv preprint</p>
<p>Datasheets for datasets. T Gebru, J Morgenstern, B Vecchione, J W Vaughan, H Wallach, H D Iii, K Crawford, Communications of the ACM. 64122021</p>
<p>Are there recommended data splits (e.g., training, development/validation, testing)? Yes. We have constructed and split the training set and test set, and ensure no table overlaps in the training and test set. Yes, Please refer to Step 4 in Section 2.3</p>
<p>Are there any errors, sources of noise, or redundancies in the dataset? Synthesized statistical problems may sometimes lack meaningful content or present ambiguities. We pay attention to the quality of the question templates and use GPT-3.5-Turbo to refine the expression of synthesized statistical questions in StatQA, followed by manual review by post-graduate students in statistics. in an effort to minimize errors</p>
<p>Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? StatQA is self-contained. </p>
<p>Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals non. </p>
<p>Does the dataset relate to people? No. Q18. Does the dataset identify any subpopulations (e.g., by age, gender)? No. Q19. Is it possible to identify one or more natural persons, either directly or indirectly (i.e., in combination with other data) from the dataset? No. Q20. Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? No. Q21. No, Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? No. Q17. Any other comments? No. I.3 Collection Process Q22. How was the data associated with each instance acquired? We elaborate how we construct StatQA in Section 2.3</p>
<p>What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? We elaborate how we construct StatQA in Section. </p>
<p>If the dataset is a sample from a larger set, what was the sampling strategy? StatQA is newly curated, and we utilize the stratified sampling strategy to obtain the mini-StatQA, ensuring it resembles the complete benchmark in terms of task and difficulty distribution. </p>
<p>Who was involved in data collection process (e.g., students, crowd-workers, contractors) and how were they compensated. how much were crowdworkers paid</p>
<p>to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)? We will maintain the StatQA in the following two years, and we may expand StatQA with increased scale and broader coverage if necessary. Will the dataset be updated. Relevant information will be released in the GitHub documentation if there are any updates</p>            </div>
        </div>

    </div>
</body>
</html>
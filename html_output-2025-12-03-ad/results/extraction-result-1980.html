<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1980 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1980</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1980</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-276776282</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.02247v5.pdf" target="_blank">WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation</a></p>
                <p><strong>Paper Abstract:</strong> Object Goal Navigation-requiring an agent to locate a specific object in an unseen environment-remains a core challenge in embodied AI. Although recent progress in Vision-Language Model (VLM)-based agents has demonstrated promising perception and decision-making abilities through prompting, none has yet established a fully modular world model design that reduces risky and costly interactions with the environment by predicting the future state of the world. We introduce WMNav, a novel World Model-based Navigation framework powered by Vision-Language Models (VLMs). It predicts possible outcomes of decisions and builds memories to provide feedback to the policy module. To retain the predicted state of the environment, WMNav proposes the online maintained Curiosity Value Map as part of the world model memory to provide dynamic configuration for navigation policy. By decomposing according to a human-like thinking process, WMNav effectively alleviates the impact of model hallucination by making decisions based on the feedback difference between the world model plan and observation. To further boost efficiency, we implement a two-stage action proposer strategy: broad exploration followed by precise localization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses existing zero-shot benchmarks in both success rate and exploration efficiency (absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL on MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1980.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1980.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WMNav</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Model Navigation with Vision-Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A world-model based zero-shot object-goal navigation system that uses Vision-Language Models (VLMs) as the predictive/prompted world model to score panoramic viewpoints, build an online curiosity value top-down map, decompose subtasks, and propose actions via a two-stage proposer to execute polar-coordinate moves without any task-specific finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WMNav</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>WMNav integrates VLMs into a modular world-model architecture: PredictVLM takes a panoramic RGB-D image and, via prompting, outputs scalar curiosity scores (0-10) per navigable direction; scores are projected to a top-down curiosity value map and merged with past memory. PlanVLM (prompted with previous-subtask cost) takes the selected high-score view to generate/update a subtask and a goal flag. A Two-stage Action Proposer samples candidate polar-coordinate actions in the selected view and ReasonVLM (prompted with current subtask and goal flag) selects one action to execute. All VLMs are used zero-shot (no finetuning) and are invoked through multimodal prompts; spatial reasoning is enabled by projecting depth+pose into a top-down grid used as explicit memory.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Gemini-series VLM (Gemini 1.5 Flash / Gemini 1.5 Pro / Gemini 2.0 Flash used in experiments); framework uses a general multimodal VLM as the visual-language encoder</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Prompt-based multimodal grounding: the VLM is given image prompts (panoramic or egocentric crops) plus explicit textual instructions (goal category, subtask, cost) and outputs scalar relevance/curiosity scores or action choices; grounding is achieved implicitly by the pretrained VLM's multimodal fusion rather than an explicit learned cross-attention module trained in-task.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>scene-level / region-level: VLM scores entire egocentric viewpoints and navigable regions; these are projected to a top-down pixel grid (map-level) to produce spatial memory.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit: uses RGB-D + pose to project egocentric navigable regions and VLM-provided scores onto a top-down Curiosity Value Map (grid), and uses the map + pose to reason about exploration and candidate actions.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>vision-language navigation (object-goal navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Habitat ObjectNav (HM3D and MP3D benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>photorealistic simulation with egocentric RGB-D panoramic inputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR) and Success weighted by Path Length (SPL)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>HM3D: 58.1% SR, 31.2% SPL (reported, Table I, HM3D v0.1); MP3D: 45.4% SR, 17.2% SPL (reported, Table I). (Ablation and memory-study results reported on HM3D v0.2 show best configuration SR 72.2% in that split — see ablation section.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablation on HM3D v0.2 compares memory/grounding strategies: No-memory baseline (no map memory) SR = 65.8% SR, 25.8% SPL; Text-Image memory baseline (VLM-produced textual descriptions + top-down trajectory used as prompt) SR = 62.0% SR, 29.6% SPL. WMNav's full memory+SD+TAP (Curiosity Value Map + Subtask Decomposition + Two-stage Action Proposer) achieves SR = 72.2%, 33.3% SPL on HM3D v0.2.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>+6.4% SR (72.2% vs 65.8%) compared to No-memory baseline on HM3D v0.2; +10.2% SR (72.2% vs 62.0%) compared to Text-Image memory baseline, indicating the quantitative curiosity-map grounding substantially improves navigation performance in their ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>They compare different VLM backbones on HM3D v0.1 (Table III): Qwen2.5-VL-3B: 29.7% SR, Qwen2.5-VL-7B: 46.1% SR, Gemini 1.5 Flash: 53.5% SR, Gemini 2.0 Flash: 57.9% SR, Gemini 1.5 Pro: 58.1% SR — larger/better VLMs give clear SR gains.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors identify several perception/grounding bottlenecks: (1) VLM hallucination when used to produce textual memories (Text-Image memory) yields erroneous memory and worse performance; (2) limited egocentric field-of-view (FOV) makes capturing out-of-view targets hard, motivating panoramic input and top-down projection; (3) VLMs' difficulty in precise distance estimation from images motivates the two-stage action proposer and denser goal-approaching sampling; (4) hallucination and ambiguous visual cues are mitigated via subtask-decomposition (passing previous subtask as prompt 'cost').</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Empirical failure modes observed/analysed: (a) Text-Image memory (VLM-generated textual descriptions stored as memory) performs worse (62.0% SR) than even no-map baseline (65.8% SR), attributed to hallucinated/misleading text memory; (b) distance estimation errors and ambiguity in single-view images lead to stopping/approach failure — addressed by separate dense goal-approaching stage; (c) hallucination/incorrect global layout predictions can mislead exploration—authors mitigate via quantitative curiosity scores and merging with visited-region zeroing; improvements in SR when adding subtask decomposition (+~1.1–2.1% SR) and Two-stage Action Proposer (+2.1% SR comparing e vs f in Table II) quantify these effects.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>No explicit domain-adaptation or fine-tuning reported; approach relies on pre-trained VLMs' robustness and uses panoramic inputs + depth/pose projection to top-down map to reduce out-of-view domain issues; training-free (zero-shot) design avoids additional in-domain finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Task is zero-shot object navigation to (potentially unseen) categories; paper reports overall zero-shot performance on HM3D/MP3D but does not provide per-object (novel vs seen) breakdown numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>The VLMs are used zero-shot (effectively frozen) with prompting; no fine-tuning vs frozen comparison is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Yes — their VLM comparison shows larger/more capable multimodal models substantially improve performance (e.g., Qwen 3B -> 7B yields SR 29.7 -> 46.1; Gemini variants perform best), indicating pretraining/model-scale positively affects grounding quality.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Implicit multimodal fusion via the VLM: late-fusion prompting where textual goal/subtask instructions are concatenated with image prompts (panoramic or annotated egocentric images); outputs are scalar scores or text-action choices — no explicit learned cross-attention module is introduced by the authors beyond the internal VLM architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training-free / zero-shot: WMNav requires no task-specific training samples (policy modules are not learned); all decision-making uses zero-shot VLM prompting and an online maintained curiosity map.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Prompted VLMs can serve as effective predictive world models for zero-shot object navigation when combined with a quantitative, spatial memory (Curiosity Value Map) and structured prompting (subtask decomposition). Quantitative top-down memory (CVM) outperforms text-based memory (Text-Image), mitigating VLM hallucination. Two-stage action proposal (explore then dense goal-approach) addresses VLM distance-estimation limits. Larger/more capable VLMs materially improve performance; no finetuning is required, but hallucination and limited FOV remain primary grounding bottlenecks that the architecture must explicitly address.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1980.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1980.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PredictVLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predict Vision-Language Model (PredictVLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompted VLM module in WMNav used as the predictive element of the world model: given a panoramic RGB-D image and goal text, it assigns a curiosity score (0–10) to each navigable direction indicating likelihood of the target being in that direction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PredictVLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PredictVLM receives a panoramic image and a textual instruction (goal name + step-by-step scoring rules) and outputs scalar curiosity values per direction. Those per-view scores are projection-mapped into a top-down Curiosity Value Map using depth and pose to inform exploration planning. PredictVLM thus performs grounding by mapping goal-language plus visual panorama to numeric per-direction likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Gemini-series VLM used in experiments (prompted multimodal encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Prompted scoring: the VLM implicitly aligns the textual goal token with panoramic visual features and produces scalar scores per viewpoint; grounding is achieved via the VLM's internal multimodal alignment rather than explicit region-language supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>scene-level / viewpoint-level (per-view scores aggregated into map-level memory)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Uses RGB-D plus pose to project per-view scores into explicit top-down grid (pixel-level map) representing spatial likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>vision-language navigation (prediction module within navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Habitat ObjectNav (HM3D/MP3D benchmarks used downstream)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>photorealistic egocentric panoramas</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Same backbone comparisons as WMNav (Table III) — different VLM backbones change the quality of PredictVLM outputs, with larger/more capable models producing better navigation SR.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Limited FOV in single egocentric views, distance estimation ambiguity, and VLM hallucination when asked to produce textual descriptions or unconstrained predictions are highlighted as issues; panoramic aggregation + depth projection used to mitigate these.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>PredictVLM's incorrect high scores for irrelevant directions (hallucinated likelihoods) can misdirect exploration — merging with visitation-based zeroing and quantitative map fusion reduces persistent errors.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Used zero-shot/frozen in this work; no fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>PredictVLM's effectiveness improves with larger VLM backbones (Table III).</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Image + textual prompt provided together as VLM input; the VLM internally fuses modalities and returns numeric scores (implicit fusion).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Zero-shot (no additional training samples required for PredictVLM).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Using a prompted VLM to quantitatively score panoramic directions and projecting scores into an explicit top-down memory is an effective grounding mechanism for navigation and mitigates hallucination compared with storing textual VLM outputs as memory.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1980.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1980.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curiosity Value Map (CVM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curiosity Value Map (online maintained top-down map)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An explicit, quantitative top-down memory grid where each cell holds a curiosity score (0–10) representing the predicted likelihood that that position contains the target; updated online by projecting PredictVLM scores and zeroing visited regions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Curiosity Value Map (CVM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CVM is a map_size × map_size single-channel grid. At each step, PredictVLM's per-direction scores (from the panoramic image) are projected into top-down coordinates using depth and pose to form M_nav; CVM is updated by element-wise min with previous CVM (visited areas set to 0 when observed and empty). The merged CVM is then re-projected to egocentric views to pick the next high-scoring direction. CVM serves as the main spatial grounding/memory structure in WMNav.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Spatially explicit grounding: VLM-provided per-view scores are converted into spatial likelihoods via depth/pose projection and stored in the CVM; this ties language-specified target likelihoods to map coordinates rather than text descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>map-level (top-down grid), derived from viewpoint-level scores</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit 2D top-down grid built from depth + pose projection; visited regions are marked and curiosity values updated quantitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>vision-language navigation (memory/grounding component)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Habitat ObjectNav (HM3D v0.2 ablations reported)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>photorealistic simulation, panoramic egocentric inputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, SPL (used in ablation comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Ablation (HM3D v0.2): CVM (without SD/TAP) SR = 69.5% SR, 34.9% SPL; CVM+SD SR = 70.1% SR, 34.7% SPL; CVM+SD+TAP SR = 72.2% SR, 33.3% SPL. Compare to No-memory: 65.8% SR, 25.8% SPL; Text-Image memory: 62.0% SR, 29.6% SPL.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Text-Image memory baseline (i.e., using VLM to write text memory) achieves 62.0% SR (HM3D v0.2), which is worse than both No-memory (65.8%) and CVM variants, showing that quantitative spatial memory is critical to effective grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>CVM-based grounding yields +10.2% SR over Text-Image memory (72.2% vs 62.0%) and +6.4% SR over No-memory baseline (72.2% vs 65.8%) in HM3D v0.2 ablations (best-config comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors point out that storing VLM outputs as textual memory leads to hallucinations (erroneous memory entries) and degraded navigation; CVM's numerical constraints force VLM outputs to be rigorous and reduce hallucination impact.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Textual memory hallucination is highlighted as a core failure mode; CVM reduces this by imposing numeric and geometric structure (projected scores and visited-region zeroing), improving robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>CVM itself does not do domain adaptation but reduces out-of-view/fov-related failures by integrating panoramic projections and depth into a unified spatial memory.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Projection-based fusion: per-view VLM scores are spatially projected and fused with previous map via min operation; semantic fusion is implicit via the VLM that produced the scores.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Part of a zero-shot pipeline — no additional training required.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Quantitative spatial memory (CVM) that stores VLM-predicted likelihoods in map coordinates substantially outperforms a text-based memory strategy, reducing hallucination and improving SR and SPL in zero-shot navigation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1980.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1980.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-Image Memory (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-Image Memory (VLM-generated textual descriptions stored as memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline memory strategy where the VLM produces textual descriptions of observations and those text-image pairs (plus a top-down trajectory map) are used as prompts for planning; authors find this approach prone to hallucination and poorer performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Text-Image Memory baseline</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The VLM is invoked to produce textual descriptions of current observation(s); these textual descriptions are combined with a top-down trajectory map and then used as prompts for subsequent planning VLM calls. This strategy attempts to keep a human-readable memory but relies on textual descriptions rather than quantitative scored maps.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Textual grounding: VLM-generated natural language descriptions of scenes/images are used as the memory representation, and later VLM prompts read that text to guide planning; grounding depends on text-to-visual alignment and correctness of descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>textual (scene-description) + trajectory-level (top-down path), not numeric spatial likelihoods</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Top-down trajectory is used but the semantic memory is textual rather than numeric map cells; no explicit per-cell likelihood scores.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>vision-language navigation (baseline for memory / grounding comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Habitat ObjectNav (HM3D v0.2 ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>photorealistic simulation, egocentric images</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, SPL (used in ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>HM3D v0.2 ablation: Text-Image memory SR = 62.0% SR, 29.6% SPL (worse than No-memory and CVM variants).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Negative: Text-Image memory performed worse than No-map baseline (62.0% vs 65.8% SR), demonstrating that naive text-based memory can harm grounding due to hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Textual memory is especially vulnerable to VLM hallucination (incorrect or misleading descriptions), which corrupts downstream planning and reduces navigation success.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Text-Image memory produced erroneous memory entries that misled planning, causing lower SR compared to even a system without any memory — authors use this to motivate numeric CVM memory.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Textual prompt fusion: previously generated textual descriptions are concatenated into planning prompts for the VLM (late-textual fusion).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Zero-shot prompting approach; no training samples required, but performance degraded relative to numeric map memory.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Storing VLM outputs as free-form text memory is prone to hallucination and can degrade navigation performance; quantitative spatial memory is a more reliable grounding representation for embodied navigation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Vlfm: Visionlanguage frontier maps for zero-shot semantic navigation <em>(Rating: 2)</em></li>
                <li>Openfmnav: Towards open-set zeroshot object navigation via vision-language foundation models <em>(Rating: 2)</em></li>
                <li>Topv-nav: Unlocking the top-view spatial reasoning potential of mllm for zero-shot object navigation <em>(Rating: 2)</em></li>
                <li>Esc: Exploration with soft commonsense constraints for zeroshot object navigation <em>(Rating: 1)</em></li>
                <li>L3MVN: Leveraging large language models for visual target navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1980",
    "paper_id": "paper-276776282",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "WMNav",
            "name_full": "World Model Navigation with Vision-Language Models",
            "brief_description": "A world-model based zero-shot object-goal navigation system that uses Vision-Language Models (VLMs) as the predictive/prompted world model to score panoramic viewpoints, build an online curiosity value top-down map, decompose subtasks, and propose actions via a two-stage proposer to execute polar-coordinate moves without any task-specific finetuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "WMNav",
            "model_description": "WMNav integrates VLMs into a modular world-model architecture: PredictVLM takes a panoramic RGB-D image and, via prompting, outputs scalar curiosity scores (0-10) per navigable direction; scores are projected to a top-down curiosity value map and merged with past memory. PlanVLM (prompted with previous-subtask cost) takes the selected high-score view to generate/update a subtask and a goal flag. A Two-stage Action Proposer samples candidate polar-coordinate actions in the selected view and ReasonVLM (prompted with current subtask and goal flag) selects one action to execute. All VLMs are used zero-shot (no finetuning) and are invoked through multimodal prompts; spatial reasoning is enabled by projecting depth+pose into a top-down grid used as explicit memory.",
            "visual_encoder_type": "Gemini-series VLM (Gemini 1.5 Flash / Gemini 1.5 Pro / Gemini 2.0 Flash used in experiments); framework uses a general multimodal VLM as the visual-language encoder",
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Prompt-based multimodal grounding: the VLM is given image prompts (panoramic or egocentric crops) plus explicit textual instructions (goal category, subtask, cost) and outputs scalar relevance/curiosity scores or action choices; grounding is achieved implicitly by the pretrained VLM's multimodal fusion rather than an explicit learned cross-attention module trained in-task.",
            "representation_level": "scene-level / region-level: VLM scores entire egocentric viewpoints and navigable regions; these are projected to a top-down pixel grid (map-level) to produce spatial memory.",
            "spatial_representation": "Explicit: uses RGB-D + pose to project egocentric navigable regions and VLM-provided scores onto a top-down Curiosity Value Map (grid), and uses the map + pose to reason about exploration and candidate actions.",
            "embodied_task_type": "vision-language navigation (object-goal navigation)",
            "embodied_task_name": "Habitat ObjectNav (HM3D and MP3D benchmarks)",
            "visual_domain": "photorealistic simulation with egocentric RGB-D panoramic inputs",
            "performance_metric": "Success Rate (SR) and Success weighted by Path Length (SPL)",
            "performance_value": "HM3D: 58.1% SR, 31.2% SPL (reported, Table I, HM3D v0.1); MP3D: 45.4% SR, 17.2% SPL (reported, Table I). (Ablation and memory-study results reported on HM3D v0.2 show best configuration SR 72.2% in that split — see ablation section.)",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablation on HM3D v0.2 compares memory/grounding strategies: No-memory baseline (no map memory) SR = 65.8% SR, 25.8% SPL; Text-Image memory baseline (VLM-produced textual descriptions + top-down trajectory used as prompt) SR = 62.0% SR, 29.6% SPL. WMNav's full memory+SD+TAP (Curiosity Value Map + Subtask Decomposition + Two-stage Action Proposer) achieves SR = 72.2%, 33.3% SPL on HM3D v0.2.",
            "grounding_improvement": "+6.4% SR (72.2% vs 65.8%) compared to No-memory baseline on HM3D v0.2; +10.2% SR (72.2% vs 62.0%) compared to Text-Image memory baseline, indicating the quantitative curiosity-map grounding substantially improves navigation performance in their ablations.",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "They compare different VLM backbones on HM3D v0.1 (Table III): Qwen2.5-VL-3B: 29.7% SR, Qwen2.5-VL-7B: 46.1% SR, Gemini 1.5 Flash: 53.5% SR, Gemini 2.0 Flash: 57.9% SR, Gemini 1.5 Pro: 58.1% SR — larger/better VLMs give clear SR gains.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors identify several perception/grounding bottlenecks: (1) VLM hallucination when used to produce textual memories (Text-Image memory) yields erroneous memory and worse performance; (2) limited egocentric field-of-view (FOV) makes capturing out-of-view targets hard, motivating panoramic input and top-down projection; (3) VLMs' difficulty in precise distance estimation from images motivates the two-stage action proposer and denser goal-approaching sampling; (4) hallucination and ambiguous visual cues are mitigated via subtask-decomposition (passing previous subtask as prompt 'cost').",
            "failure_mode_analysis": "Empirical failure modes observed/analysed: (a) Text-Image memory (VLM-generated textual descriptions stored as memory) performs worse (62.0% SR) than even no-map baseline (65.8% SR), attributed to hallucinated/misleading text memory; (b) distance estimation errors and ambiguity in single-view images lead to stopping/approach failure — addressed by separate dense goal-approaching stage; (c) hallucination/incorrect global layout predictions can mislead exploration—authors mitigate via quantitative curiosity scores and merging with visited-region zeroing; improvements in SR when adding subtask decomposition (+~1.1–2.1% SR) and Two-stage Action Proposer (+2.1% SR comparing e vs f in Table II) quantify these effects.",
            "domain_shift_handling": "No explicit domain-adaptation or fine-tuning reported; approach relies on pre-trained VLMs' robustness and uses panoramic inputs + depth/pose projection to top-down map to reduce out-of-view domain issues; training-free (zero-shot) design avoids additional in-domain finetuning.",
            "novel_object_performance": "Task is zero-shot object navigation to (potentially unseen) categories; paper reports overall zero-shot performance on HM3D/MP3D but does not provide per-object (novel vs seen) breakdown numbers.",
            "frozen_vs_finetuned": "The VLMs are used zero-shot (effectively frozen) with prompting; no fine-tuning vs frozen comparison is reported.",
            "pretraining_scale_effect": "Yes — their VLM comparison shows larger/more capable multimodal models substantially improve performance (e.g., Qwen 3B -&gt; 7B yields SR 29.7 -&gt; 46.1; Gemini variants perform best), indicating pretraining/model-scale positively affects grounding quality.",
            "fusion_mechanism": "Implicit multimodal fusion via the VLM: late-fusion prompting where textual goal/subtask instructions are concatenated with image prompts (panoramic or annotated egocentric images); outputs are scalar scores or text-action choices — no explicit learned cross-attention module is introduced by the authors beyond the internal VLM architecture.",
            "sample_efficiency": "Training-free / zero-shot: WMNav requires no task-specific training samples (policy modules are not learned); all decision-making uses zero-shot VLM prompting and an online maintained curiosity map.",
            "key_findings_grounding": "Prompted VLMs can serve as effective predictive world models for zero-shot object navigation when combined with a quantitative, spatial memory (Curiosity Value Map) and structured prompting (subtask decomposition). Quantitative top-down memory (CVM) outperforms text-based memory (Text-Image), mitigating VLM hallucination. Two-stage action proposal (explore then dense goal-approach) addresses VLM distance-estimation limits. Larger/more capable VLMs materially improve performance; no finetuning is required, but hallucination and limited FOV remain primary grounding bottlenecks that the architecture must explicitly address.",
            "uuid": "e1980.0"
        },
        {
            "name_short": "PredictVLM",
            "name_full": "Predict Vision-Language Model (PredictVLM)",
            "brief_description": "A prompted VLM module in WMNav used as the predictive element of the world model: given a panoramic RGB-D image and goal text, it assigns a curiosity score (0–10) to each navigable direction indicating likelihood of the target being in that direction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PredictVLM",
            "model_description": "PredictVLM receives a panoramic image and a textual instruction (goal name + step-by-step scoring rules) and outputs scalar curiosity values per direction. Those per-view scores are projection-mapped into a top-down Curiosity Value Map using depth and pose to inform exploration planning. PredictVLM thus performs grounding by mapping goal-language plus visual panorama to numeric per-direction likelihoods.",
            "visual_encoder_type": "Gemini-series VLM used in experiments (prompted multimodal encoder)",
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Prompted scoring: the VLM implicitly aligns the textual goal token with panoramic visual features and produces scalar scores per viewpoint; grounding is achieved via the VLM's internal multimodal alignment rather than explicit region-language supervision.",
            "representation_level": "scene-level / viewpoint-level (per-view scores aggregated into map-level memory)",
            "spatial_representation": "Uses RGB-D plus pose to project per-view scores into explicit top-down grid (pixel-level map) representing spatial likelihood.",
            "embodied_task_type": "vision-language navigation (prediction module within navigation)",
            "embodied_task_name": "Habitat ObjectNav (HM3D/MP3D benchmarks used downstream)",
            "visual_domain": "photorealistic egocentric panoramas",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": true,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Same backbone comparisons as WMNav (Table III) — different VLM backbones change the quality of PredictVLM outputs, with larger/more capable models producing better navigation SR.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Limited FOV in single egocentric views, distance estimation ambiguity, and VLM hallucination when asked to produce textual descriptions or unconstrained predictions are highlighted as issues; panoramic aggregation + depth projection used to mitigate these.",
            "failure_mode_analysis": "PredictVLM's incorrect high scores for irrelevant directions (hallucinated likelihoods) can misdirect exploration — merging with visitation-based zeroing and quantitative map fusion reduces persistent errors.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": "Used zero-shot/frozen in this work; no fine-tuning reported.",
            "pretraining_scale_effect": "PredictVLM's effectiveness improves with larger VLM backbones (Table III).",
            "fusion_mechanism": "Image + textual prompt provided together as VLM input; the VLM internally fuses modalities and returns numeric scores (implicit fusion).",
            "sample_efficiency": "Zero-shot (no additional training samples required for PredictVLM).",
            "key_findings_grounding": "Using a prompted VLM to quantitatively score panoramic directions and projecting scores into an explicit top-down memory is an effective grounding mechanism for navigation and mitigates hallucination compared with storing textual VLM outputs as memory.",
            "uuid": "e1980.1"
        },
        {
            "name_short": "Curiosity Value Map (CVM)",
            "name_full": "Curiosity Value Map (online maintained top-down map)",
            "brief_description": "An explicit, quantitative top-down memory grid where each cell holds a curiosity score (0–10) representing the predicted likelihood that that position contains the target; updated online by projecting PredictVLM scores and zeroing visited regions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Curiosity Value Map (CVM)",
            "model_description": "CVM is a map_size × map_size single-channel grid. At each step, PredictVLM's per-direction scores (from the panoramic image) are projected into top-down coordinates using depth and pose to form M_nav; CVM is updated by element-wise min with previous CVM (visited areas set to 0 when observed and empty). The merged CVM is then re-projected to egocentric views to pick the next high-scoring direction. CVM serves as the main spatial grounding/memory structure in WMNav.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Spatially explicit grounding: VLM-provided per-view scores are converted into spatial likelihoods via depth/pose projection and stored in the CVM; this ties language-specified target likelihoods to map coordinates rather than text descriptions.",
            "representation_level": "map-level (top-down grid), derived from viewpoint-level scores",
            "spatial_representation": "Explicit 2D top-down grid built from depth + pose projection; visited regions are marked and curiosity values updated quantitatively.",
            "embodied_task_type": "vision-language navigation (memory/grounding component)",
            "embodied_task_name": "Habitat ObjectNav (HM3D v0.2 ablations reported)",
            "visual_domain": "photorealistic simulation, panoramic egocentric inputs",
            "performance_metric": "SR, SPL (used in ablation comparisons)",
            "performance_value": "Ablation (HM3D v0.2): CVM (without SD/TAP) SR = 69.5% SR, 34.9% SPL; CVM+SD SR = 70.1% SR, 34.7% SPL; CVM+SD+TAP SR = 72.2% SR, 33.3% SPL. Compare to No-memory: 65.8% SR, 25.8% SPL; Text-Image memory: 62.0% SR, 29.6% SPL.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Text-Image memory baseline (i.e., using VLM to write text memory) achieves 62.0% SR (HM3D v0.2), which is worse than both No-memory (65.8%) and CVM variants, showing that quantitative spatial memory is critical to effective grounding.",
            "grounding_improvement": "CVM-based grounding yields +10.2% SR over Text-Image memory (72.2% vs 62.0%) and +6.4% SR over No-memory baseline (72.2% vs 65.8%) in HM3D v0.2 ablations (best-config comparison).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors point out that storing VLM outputs as textual memory leads to hallucinations (erroneous memory entries) and degraded navigation; CVM's numerical constraints force VLM outputs to be rigorous and reduce hallucination impact.",
            "failure_mode_analysis": "Textual memory hallucination is highlighted as a core failure mode; CVM reduces this by imposing numeric and geometric structure (projected scores and visited-region zeroing), improving robustness.",
            "domain_shift_handling": "CVM itself does not do domain adaptation but reduces out-of-view/fov-related failures by integrating panoramic projections and depth into a unified spatial memory.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Projection-based fusion: per-view VLM scores are spatially projected and fused with previous map via min operation; semantic fusion is implicit via the VLM that produced the scores.",
            "sample_efficiency": "Part of a zero-shot pipeline — no additional training required.",
            "key_findings_grounding": "Quantitative spatial memory (CVM) that stores VLM-predicted likelihoods in map coordinates substantially outperforms a text-based memory strategy, reducing hallucination and improving SR and SPL in zero-shot navigation.",
            "uuid": "e1980.2"
        },
        {
            "name_short": "Text-Image Memory (baseline)",
            "name_full": "Text-Image Memory (VLM-generated textual descriptions stored as memory)",
            "brief_description": "A baseline memory strategy where the VLM produces textual descriptions of observations and those text-image pairs (plus a top-down trajectory map) are used as prompts for planning; authors find this approach prone to hallucination and poorer performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Text-Image Memory baseline",
            "model_description": "The VLM is invoked to produce textual descriptions of current observation(s); these textual descriptions are combined with a top-down trajectory map and then used as prompts for subsequent planning VLM calls. This strategy attempts to keep a human-readable memory but relies on textual descriptions rather than quantitative scored maps.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Textual grounding: VLM-generated natural language descriptions of scenes/images are used as the memory representation, and later VLM prompts read that text to guide planning; grounding depends on text-to-visual alignment and correctness of descriptions.",
            "representation_level": "textual (scene-description) + trajectory-level (top-down path), not numeric spatial likelihoods",
            "spatial_representation": "Top-down trajectory is used but the semantic memory is textual rather than numeric map cells; no explicit per-cell likelihood scores.",
            "embodied_task_type": "vision-language navigation (baseline for memory / grounding comparison)",
            "embodied_task_name": "Habitat ObjectNav (HM3D v0.2 ablation)",
            "visual_domain": "photorealistic simulation, egocentric images",
            "performance_metric": "SR, SPL (used in ablation)",
            "performance_value": "HM3D v0.2 ablation: Text-Image memory SR = 62.0% SR, 29.6% SPL (worse than No-memory and CVM variants).",
            "has_grounding_ablation": true,
            "performance_without_grounding": null,
            "grounding_improvement": "Negative: Text-Image memory performed worse than No-map baseline (62.0% vs 65.8% SR), demonstrating that naive text-based memory can harm grounding due to hallucination.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Textual memory is especially vulnerable to VLM hallucination (incorrect or misleading descriptions), which corrupts downstream planning and reduces navigation success.",
            "failure_mode_analysis": "Text-Image memory produced erroneous memory entries that misled planning, causing lower SR compared to even a system without any memory — authors use this to motivate numeric CVM memory.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Textual prompt fusion: previously generated textual descriptions are concatenated into planning prompts for the VLM (late-textual fusion).",
            "sample_efficiency": "Zero-shot prompting approach; no training samples required, but performance degraded relative to numeric map memory.",
            "key_findings_grounding": "Storing VLM outputs as free-form text memory is prone to hallucination and can degrade navigation performance; quantitative spatial memory is a more reliable grounding representation for embodied navigation.",
            "uuid": "e1980.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Vlfm: Visionlanguage frontier maps for zero-shot semantic navigation",
            "rating": 2
        },
        {
            "paper_title": "Openfmnav: Towards open-set zeroshot object navigation via vision-language foundation models",
            "rating": 2
        },
        {
            "paper_title": "Topv-nav: Unlocking the top-view spatial reasoning potential of mllm for zero-shot object navigation",
            "rating": 2
        },
        {
            "paper_title": "Esc: Exploration with soft commonsense constraints for zeroshot object navigation",
            "rating": 1
        },
        {
            "paper_title": "L3MVN: Leveraging large language models for visual target navigation",
            "rating": 1
        }
    ],
    "cost": 0.01453525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation</p>
<p>Dujun Nie niedujun2024@ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences</p>
<p>Xianda Guo xianda_guo@163.com 
School of Computer Science
Wuhan University</p>
<p>Yiqun Duan duanyiquncc@gmail.com 
School of Computer Science
University of Technology
Sydney 4 IAIR</p>
<p>Xi'an Jiaotong University
5 Waytous</p>
<p>Ruijun Zhang zhangruijun2023@ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences</p>
<p>Long Chen long.chen@ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences</p>
<p>WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation
63ECA4377AAF3B8025B4EEF33A92EE08
Object Goal Navigation--requiring an agent to locate a specific object in an unseen environment--remains a core challenge in embodied AI.Although recent progress in Vision-Language Model (VLM)--based agents has demonstrated promising perception and decision-making abilities through prompting, none has yet established a fully modular world model design that reduces risky and costly interactions with the environment by predicting the future state of the world.We introduce WMNav, a novel World Model-based Navigation framework powered by Vision-Language Models (VLMs).It predicts possible outcomes of decisions and builds memories to provide feedback to the policy module.To retain the predicted state of the environment, WMNav proposes the online maintained Curiosity Value Map as part of the world model memory to provide dynamic configuration for navigation policy.By decomposing according to a human-like thinking process, WMNav effectively alleviates the impact of model hallucination by making decisions based on the feedback difference between the world model plan and observation.To further boost efficiency, we implement a two-stage action proposer strategy: broad exploration followed by precise localization.Extensive evaluation on HM3D and MP3D validates WMNav surpasses existing zero-shot benchmarks in both success rate and exploration efficiency (absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL on MP3D).Project page: https://b0b8k1ng.github.io/WMNav/.</p>
<p>I. INTRODUCTION</p>
<p>Effective navigation is a fundamental requirement for domestic robots, allowing them to access specific locations and execute assigned operations [1].Zero-Shot Object Navigation (ZSON) is a critical component of this functionality, which demands that an agent locate and approach a target object from an unseen category through environmental understanding.This navigation capability proves indispensable for performing varied and sophisticated functions in practical home environments.The primary difficulty in ZSON stems from the need to employ broad semantic knowledge to direct movement with optimal efficiency while precisely identifying previously unencountered target objects.</p>
<p>With the recent advancements in visual foundation models, the effectiveness of visual perception has experienced a substantial enhancement, achieving remarkable performance in zero-shot scene understanding.Existing navigation methods</p>
<p>Task Completion</p>
<p>Choose Angle 150: There is an opening to another room.Bedrooms are often located near living areas.</p>
<p>Choose Action ③: The best action is to move forward, as it leads to the open door.</p>
<p>Panoramic Image Input</p>
<p>Getting Observation</p>
<p>Fig. 1: World Model Navigation with VLM.In object navigation, our model first estimates the goal's presence likelihood in each scene of the panoramic image (e.g., a bed typically resides in a living room, which is likely situated at the corridor's terminus), then plans intermediate subtasks and chooses the most appropriate action to execute.can be divided into two categories: network-based navigation [2], [3], [4] and map-based navigation [5], [6], [7], [8], [9].Network-based methods usually use reinforcement learning or imitation learning, which rely on trainable modules that need large amounts of data to train(such as policy learning) [10], [11], [12], resulting in significant computational resources and time for learning.Map-based methods construct maps to preserve semantic information about the scene and generate frontiers or waypoints for path planning.Their path planning heavily relies on accurate maps, which involve complex and time-consuming map construction processes.These methods fail to fully leverage the capabilities of VLMs trained with egocentric RGB images.Additionally, the selection strategies employed are often inflexible, frequently falling short of identifying optimal positions for decisionmaking.To address the limitations inherent in map-based planning, VLMNav [13] introduces an action space generation method that leverages navigable regions in egocentric images, providing a more flexible decision space and fully exploiting the capabilities of VLMs.However, due to the limited field of view of egocentric images, capturing environmental information outside the immediate perspective remains a significant challenge.</p>
<p>What's more, most existing methods, whether network-arXiv:2503.02247v5[cs.CV] 19 Jul 2025</p>
<p>based methods or map-based methods, require actual interaction with the environment to achieve accurate scene understanding.These approaches can not leverage predictive information about future states and the outcomes of potential actions, limiting their ability to perform anticipatory planning and reasoning effectively in uncertain scenarios.VLFM [14] constructs a value map of the likelihood of each region to lead toward the out-of-view target object.It makes use of prediction information to some extent.Still, it uses BLIP-2 [15], which pays more attention to the relevance of image-text pairs and has limited interaction and reasoning capabilities, which makes it difficult to cope with complex planning tasks.When navigating to a specific object, humans can effectively imagine the room layout and anticipate action outcomes based on visual cues and common design patterns (such as walking along the corridor often leads to a new room and going to the bathroom to find a sofa is unwise).The world model is a computational representation of environment dynamics that equips agents with the ability to simulate interactions.By simulating action choices within the virtual environment, agents can explore potential outcomes safely without directly interacting with the environment.This method not only reduces collision with obstacles but also enhances the agent's efficiency in planning and exploring.However, the true challenge lies in creating a versatile world model that can faithfully capture the landscape of an indoor environment.ATD [16] uses a dual-branch self-guided imagination policy based on large language models with a human-like left-right brain architecture.WebDreamer [17] uses LLMs as world models in web environments for modelbased planning of web agents, demonstrating its effectiveness over other baselines.Can VLMs also function as world models?Given that VLMs are trained on vast amounts of first-person human perspective images, we hypothesize that they have acquired sufficient common sense knowledge to simulate the outcomes resulting from human choices and actions.With their extensive pre-trained knowledge spanning indoor layout and general reasoning ability, VLM has great potential to take on this task.</p>
<p>Building on the key insight that VLMs inherently encode comprehensive knowledge about indoor layout and spatial relationships of objects, we propose WMNav as shown in Figure 1, which is a pioneering approach leveraging VLMs in a world model paradigm to enable efficient navigation in complex indoor environments.It receives panoramic scene information and uses VLMs to quantitatively predict future outcomes and finish all the perception, planning, reasoning, and controlling processes without task-specific training, prebuilt maps, or prior knowledge of the surroundings(using detection or segmentation results).</p>
<p>Our contributions can be summarized as follows:</p>
<p>• We introduce a new direction for object goal navigation in a complex, unknown environment using a world model consisting of VLMs and novel modules.• We achieve state-of-the-art results on the Zero-shot Object Navigation task and outperform benchmark methods on HM3D [18] and MP3D [19].</p>
<p>II. RELATED WORK</p>
<p>A. Zero shot Object Goal Navigation</p>
<p>Existing object navigation methods fall into two paradigms: supervised methods and zero-shot methods.Supervised approaches either train visual encoders with RL/IL policies [20], [21], [22], [23] or build semantic maps from training data [24], [25].While effective in known environments, these methods struggle with unseen objects/rooms due to training dependency.Recent zero-shot works address this via open-vocabulary scene understanding.Image-based methods map target objects to visual embeddings [2], [26], [4], while map-based approaches use frontier-based map [6], [5], [27], [28], [8], [9] or waypoint-based map [7] with LLM/VLM reasoning.Here, we focus on the zero-shot ObjectNav task in unknown environments using a novel world model framework.</p>
<p>B. Foundation Model Guided Navigation</p>
<p>Recent advances in embodied navigation highlight the complementary roles of Vision-Language Models (VLMs) and Large Language Models (LLMs), driven by their distinct strengths in visual grounding and common sense reasoning.LLMs enable zero-shot commonsense reasoning through object-room correlation prediction [5], semantic mapping [27], [7], and chain-of-thought planning [28], [29].However, LLMs are unable to process rich visual information and do spatial reasoning.VLMs like CLIP [21], [2], [30] address this by directly aligning visual observations with textual goals through multimodal embeddings, offering seamless integration with exploration policies through unified visual features.Compared to LLM-based methods, VLM-based methods [9], [13] may be more efficient and elegant for target-driven navigation by avoiding language-mediated grounding gaps.This is due to their multimodal nature and better spatial reasoning capabilities [31].Our framework uses pure VLM, and unlike other approaches, we use the VLM to predict the state of the environment caused by actions.</p>
<p>C. World Models</p>
<p>World models originated in classical reinforcement learning with Dyna's simulated experiences [32], evolving into frameworks for predicting state transitions to improve sample efficiency [33], [34].These models enable policy training in diverse domains and emphasize high-fidelity environment dynamics.Recent work explores large language models (LLMs) as abstract world models, prioritizing task The WMNav framework.After acquiring the RGB-D panoramic image and pose information at step t, the PredictVLM first predicts the state of the world, and the state is merged with the curiosity value map s t−1 from the previous step to get the current curiosity value map s t .After that, the updated map projects the scores of each direction back onto the panoramic image, and the direction with the highest score is selected.Secondly, given the selected direction image, the new subtask and the goal flag are determined by PlanVLM and are stored in memory as cost c t , and the memory h t is combined by s t and c t .Finally, the two-stage action proposer annotates the action sequence on the selected image and sends it into ReasonVLM to obtain the final polar coordinate vector action a t for execution.Note that PlanVLM and ReasonVLM are configured by the cost c t−1 .</p>
<p>abstraction over precise simulation.Studies like [35] demonstrate LLMs' ability to leverage commonsense knowledge for planning in simple environments, while [36] applies LLMs to web navigation.However, LLMs struggle with visually grounded decision-making.Therefore, it is more advantageous to integrate VLMs into world models in visual navigation tasks.VLMs combine visual grounding with semantic reasoning, enabling multimodal state understanding and adaptation to unseen environments through joint visualtextual prompts.Our approach takes VLM in a world model framework and involves it in all the processes of navigation, including predicting, planning, reasoning, action, etc.</p>
<p>III. WMNAV APPROACH A. Task Definition</p>
<p>The traditional object goal navigation task requires the agent to explore an unknown indoor environment and navigate to an arbitrary instance i within a given category c (e.g., bed, sofa, toilet).The agent starts from a designated initial position.At each time step t, the agent takes an RGB-D observation O t of the surroundings and its real-time pose P t .Then, the agent determines the action a t to find the goal.The task is deemed successful if the agent stops at a position within a predefined distance threshold d thres from the goal.While most previous works utilize a discrete action space, such as {Stop, Move Forward, Turn Left, Turn Right, Look Up, Look Down}, we adopt polar coordinates (r t , θ t ) to represent the action a t , where θ t denotes the direction of the action and r t indicates the distance traveled by the action.</p>
<p>B. Overview</p>
<p>The framework of WMNav is depicted in Figure 2. A panoramic understanding is essential to make a comprehen-sive perception.To this end, the agent undergoes a series of rotations A = {30 • , 90 • , 150 • , 210 • , 270 • , 330 • } and captures six distinct RGB-D images which are converted into a panoramic image I pan t .In our framework, the world model consists of PredictVLM and the memory constructed by curiosity value map and cost.The world model does not receive any actual reward signals from the environment, which means it is only used to predict and simplify the future state of the environment.PredictVLM quantitatively predicts the likelihood of the goal's presence in each direction and projects the scores from a panoramic image to a top-down map.The map is then merged with the curiosity value map from the previous step and is stored in the memory.After that, the curiosity scores are projected back onto the panoramic image.Then, the direction in the panoramic image with the highest score is selected and sent to the navigation policy module.The navigation policy module has access to the reward information from the environment.For PlanVLM and ReasonVLM in the policy module, the cost (the previous step's subtask and the goal flag) is used to configure their prompts, thus optimizing the action output of the entire policy module without any fine-tuning of the VLMs.</p>
<p>C. World Model 1) VLM-based State Prediction:</p>
<p>The core capability of the world model is to estimate the state of the world, which is not provided by perception, and to predict possible future state changes.We employ a VLM as the predictor in the world model.To guide the VLM to make reasonable predictions about the indoor scene, we design a novel prompting strategy as illustrated in Figure 3 (a).We use a panoramic image as the image prompt.PredictVLM is tasked with Question: The agent has been tasked with navigating to a BED.The agent has sent you the panoramic image describing your surrounding environment.Your job is to predict whether each direction is worth exploring and predict a score to each direction (ranging from 0 to 10).Please follow my step-by-step instructions:</p>
<p>(1)A scene with no feasible path and no target is assigned a score of 0.</p>
<p>(2)A scene where the bed is discovered is assigned a score of 10.</p>
<p>(3)Intermediate cases are assigned scores ranging from 0 to 10. predicting each viewpoint's curiosity value, which represents the likelihood of the target's presence in each direction and scores from 0 to 10.The VLM outputs scores for each direction denoted as Score t (α), (α ∈ A).At each time step t, the panoramic image I pan t is input to PredictVLM, which outputs scores Score t for each direction in the current panoramic view:
Score t = P redictV LM (I pan t )(1)
These scores can then be leveraged to construct the curiosity value map.</p>
<p>2) Curiosity Value Map Construction: The curiosity value map M cv has a size of map_size × map_size × 1, where each pixel value represents the curiosity value of the position in the entire scene, ranging from 0 to 10.For regions that have been visited and found to be devoid of the goal object, the curiosity value is set to 0. For example, an observed bedroom that does not contain the goal television would have a curiosity value of 0. Regions where the goal can be directly discovered have a curiosity value of 10.For areas that have not yet revealed the goal but offer potential pathways to the goal, the curiosity value is set between 0 and 10 based on the VLM's imagination.</p>
<p>The construction and update process is illustrated in Figure 3 (c).Initially, all pixels in M cv 0 are set to 10, indicating it is possible to find the target in all regions as the agent does not have any information about the entire scene.To convert the scores from the ego-centric perspective to the top-down view of the curiosity value map, we perform a projection transformation using depth information D t and pose P t , similar to [13] [37].We project the navigable area with scores Score t from the ego-centric view onto a top-down map M nav t as in Figure 3 (b):
M nav t = P rojection(Score t )(2)
where M nav t has the same size as M cv .Then M cv t (s t in Figure 2) is updated by combining M nav t with the curiosity value map in the previous step M cv t−1 (s t−1 in Figure 2):
M cv t (u, v) = min(M cv t−1 (u, v), M nav t (u, v))(3)
3) Cost: The cost module in the world model is used to provide environmental rewards.In our work, we use the subtask and the goal flag as the cost.For the detailed generation process of subtasks, refer to the Subtask Decomposition section (see Section III-D).The goal flag indicates whether the PlanVLM finds the goal in the selected image and is set to either True or False.The cost is fed into PlanVLM and ReasonVLM as part of their prompts to implicitly optimize the outputs in the navigation policy.PlanVLM receives the subtask from the previous step, and ReasonVLM receives the current subtask and the goal flag to switch between two configuration states.</p>
<p>D. Subtask Decomposition</p>
<p>Planning a route to an unseen goal in a completely unknown environment is a challenging task because it is hard for the model to get dense rewards.Decomposing the final goal and identifying an intermediate subtask at each step is helpful.For instance, when searching for a bed, an efficient approach would be first finding the corridor leading to the bedroom, then reaching the bedroom door, and finally approaching the bed.We adopt a subtask decomposition strategy to obtain more feedback from the environment.After the updated curiosity value map is stored in the memory,</p>
<p>The agent has been tasked with navigating to a {goal}.It has sent you the following elements: (1)<The observed image>: The image taken from its current location.</p>
<p>(2){Explanation}.This explains why you should go in this direction.</p>
<p>Your job is to describe next place to go.To help you plan your best next step, I can give you some human suggestions: (1) If the {goal} appears in the image, directly choose the target as the next step in the plan.</p>
<p>(2) If the {goal} is not found and the previous subtask {Last Subtask} has not completed, continue to complete the previous subtask.it is projected back to the current navigable areas, and the average curiosity value for each direction is calculated to obtain the final curiosity value score Score t .We then select the navigable region corresponding to the direction α with the highest score.After selecting a direction α based on the curiosity value, we perform more specific planning on this image as shown in Figure 4. Specifically, we input the selected image I t (α) and the previous subtask SubT ask t−1 into PlanVLM and it outputs a new subtask SubT ask t and the goal flag GoalF lag t (c t in Figure 2):
SubT ask t , GoalF lag t = P lanV LM (I t (α), SubT ask t−1 )(4)</p>
<p>E. Two-stage Action Proposer</p>
<p>We employ an action proposer [13] to prepare action choices for ReasonVLM.The polar coordinate action space is sampled from the navigable area.For the selected navigable area in the image I t (α), we first sample K vectors at regular angular intervals ∆θ within the navigable area and map them to the agent's coordinate system to generate an initial action sequence A init t = {(r t,i , θ t,i )} K i=1 .Then, actions falling within explored regions are filtered out based on the exploration state map, and the action sequence is further refined by limiting the movement distance and angular intervals.This results in a final candidate action sequence consisting of K ′ actions.The candidate action sequence
A cand t = {(r t,j , θ t,j )} K ′
j=1 in the agent's coordinate system is then mapped back to the image and annotated to obtain I ann t , as shown in Figure 5.In order to make the agent more purposeful when choosing actions and reduce confusion of VLMs in accurately estimating the distance of objects in images, we divide the entire action-decision process into two stages as in Figure 5.The first stage is the exploration stage, where the task is to explore the regions most likely to contain the goal, ultimately discovering and accurately locating its position.The second stage is the goal-approaching stage, where the task is to move as close as possible to the goal location and eventually stop at the goal.</p>
<p>Candidate Actions</p>
<p>Goal detected Execute Fig. 5: Reason the Action.In the exploration stage, the agent uses the action proposer to filter sampled actions.ActionVLM(obtained by configuring ReasonVLM) selects the most appropriate action for execution from the image with a labeled candidate action sequence, continuing until the target is found and the agent shifts to the next stage.The next stage is the goal-approaching stage.The agent uses the Goal Proposer to densely sample actions from the image.The GoalVLM(also obtained by configuring ReasonVLM) then selects the action that best represents the goal location.</p>
<p>selects the most appropriate action a t from the action sequence A cand t in I ann t for execution:
a t = ReasonV LM (SubT ask t , Goal t , I ann t )(5)
The final action satisfies a t = (r t , θ t ).</p>
<p>2) Goal-approaching Stage: Due to the limitations of the existing VLMs' capability, we do not rely on the VLM to estimate the stopping condition directly from the observed image.Instead, we employ a strategy similar to the action proposer to determine the precise location of the goal when the goal appears in the current observation to make the stopping condition more reliable.The length constraint on the polar coordinate vectors is removed, and sampling in the navigable regions is made denser to ensure the presence of vectors that lead to the goal's ground location at the end.This strategy allows for accurate localization of the target.The agent's stopping condition is set as follows:
StopF lag = T rue, if DistanceT oGoal &lt; d thres F alse, else.(6)
where DistanceT oGoal is the Euclidean distance between the current position and the goal position.IV. EXPERIMENTS</p>
<p>A. Datasets and Evaluation Metrics</p>
<p>Datasets The HM3D v0.1 [38] is used in the Habitat 2022 ObjectNav challenge, providing 2000 validation episodes on 20 validation environments with 6 goal object categories.The HM3D v0.2 [38] is the new version of HM3D with higher quality.It improves geometry and semantic labels with typo fixes, painting error corrections, and has 1000 validation episodes.MP3D [19] contains 11 high-fidelity scenes and 2195 episodes for validation, with 21 categories of object goals.</p>
<p>Metrics We adopt Success Rate (SR) and Success Rate Weighted by Inverse Path Length (SPL) as the evaluation metrics.SR represents the percentage of episodes that are completed.SPL quantifies the agent's navigation efficiency by calculating the inverse ratio of the actual path length traversed to the optimal path length weighted by success rate.</p>
<p>B. Implementation Details</p>
<p>We set 40 as the agent's maximal navigation steps.The agent adopts a cylindrical body of radius 0.18m and height 0.88m.We equip the agent with an egocentric RGB-D camera with resolution 640 × 480 and an HFoV of 79 • .The camera is tilted down with a pitch of 14 • , which helps determine navigable areas.d thres is set to 1.0, which means if the agent stops when the distance to the goal is less than 0.1m, the episode is considered successful.We mainly use Gemini VLM for our experiments, given its low cost and high effectiveness.</p>
<p>C. Comparison with SOTA Methods</p>
<p>In this section, we compare our WMNav method with the representative methods for object navigation on the MP3D [19] and HM3D [38] benchmarks.As shown in ).Compared to all methods, including supervised methods, our approach also achieves the optimal SR on MP3D and the best SPL on HM3D, demonstrating the effectiveness of our method.</p>
<p>Most existing zero-shot methods utilize carefully constructed semantic maps to obtain spatial layout information and utilize Foundation Models (LLMs or VLMs) for commonsense reasoning.Both ESC [5] and L3MVN [27] adopt the frontier map exploration strategy and leverage an LLM to select appropriate frontier points.ESC [5] converts the image into text scene information for LLM to reason.But textual information cannot accurately describe the spatial relationships in the scene, and it is difficult for LLM to make good spatial decisions.OpenFMNav [8] uses detectors (Grounding DINO [41] and SAM [42]) and VLMs to process the image to obtain an informative semantic map.Yet, building a fine map with the help of detectors is too complicated and does not fully stimulate the VLM's ability to understand the scene.TopV-Nav [9] directly takes the top-view map as the VLM's input to utilize the complete spatial information.However, since VLM is trained on egocentric image data, it does not take advantage of VLM's powerful egocentric reasoning ability.Similar to the frontier map, our simple and online maintained Curiosity Value Map, without prior information from other detectors, makes full use of the scene semantic understanding and predicting ability of VLM to implicitly encode the spatial layout of the scene and predict the results, thus enhancing the navigation efficiency in the unknown environment.</p>
<p>What's more, methods based on LLM or VLM suffer from hallucination.Therefore, we decompose the task into multiple subtasks and send the previous subtask to VLMs as a cost to reduce misleading hallucinations without finetuning the VLM.Our two-stage action proposer strategy avoids reliance on local policy and learning-based policy while utilizing the powerful spatial reasoning ability of VLM.So, the agent only needs a VLM base to complete all the processes without any policy modules to train.</p>
<p>D. Ablation Study</p>
<p>Effect of Different Modules.To manifest the contribution of each module, we compare three ablation models on HM3D V0.2 as it is more representative.Removing TAP implies using only the Action Proposer without computing the goal location after the goal is detected and relying on a stoppingVLM to directly determine the stopping condition according to the observation.As indicated in Table II, a and b, a and d, e and f respectively show the effectiveness of modules SubTask Decomposition, Curiosity Value Map, and Two-stage Action Proposer for improving navigation performance.</p>
<p>Effect of different VLMs.We further evaluate the abilities of different VLMs in navigation as shown in Table III, including open-source models and proprietary models.For each row of the table, all the VLMs used in the framework are guaranteed to be of the same model.The comparison between the 3B and 7B models of Qwen2.5-VL[43] shows that an increase in the model's scale can effectively enhance the agent's capabilities.Comparison between Qwen2.5-VL and the Gemini series models reveals that current opensource multimodal models still lag behind advanced proprietary models in terms of ZSON.Gemini 1.5 Pro demonstrates superior capability in the task.Note that when using the smaller Gemini 1.5 Flash VLM, our approach still achieves competitive performance compared to other methods on HM3D, which means our framework is practical on its own rather than relying solely on the capabilities of VLM.What's more, With the evolution of open-source and proprietary VLMs, the ability of each module of our model can be significantly enhanced, and as a whole, it has the potential to show better performance.</p>
<p>Effect of different memory strategies.As shown in the Table II, we also explore the influence of different memory strategies.No Memory represents that no map memory is used.The Text-Image Memory strategy first uses the VLM to generate textual descriptions of the observation.It constructs a top-down trajectory map and then inputs it as a prompt to the VLM for planning.Curiosity Value Map is our method, which uses a curiosity value map to function as memory.These three strategies all employ SD but do not employ TAP (corresponding to b, c, and e, respectively).The textimage memory strategy performs even worse than the nomap memory strategy.This is because directly feeding the VLM with a text-image combination can easily induce hallucinations, leading to erroneous memory information.Our method shows improvements in both SR and SPL metrics, as the quantitative construction of the Curiosity Value Map forces the VLM to produce output as rigorously as possible and guarantee the accuracy of each call, ensuring reliable memory.</p>
<p>V. CONCLUSION</p>
<p>We have introduced WMNav, which finds a novel direction for object goal navigation in unknown environments by leveraging VLMs in a world model framework and significantly enhancing ZSON performance.Our method addresses serious inefficiency problems of back-and-forth redundant movement by employing an online Curiosity Value Map to quantitatively predict the likelihood of the target's presence.The subtask decomposition module provides a denser reward for the prompt-based optimization of the policy module.Moreover, the two-stage action proposer strategy leads to more purposeful navigation and efficient exploration.By constructing the world model architecture based on VLMs, concise memory map building, and task breakdown, WMNav indicates a new optimization direction for the ZSON task and opens up new pathways for embodied robots to interact with environments.</p>
<p>1 3 2
13
Found the BedChoose Action ⑦: The bed is in the other room, and position ⑦ is the closest to the bed.</p>
<p>Fig.2: The WMNav framework.After acquiring the RGB-D panoramic image and pose information at step t, the PredictVLM first predicts the state of the world, and the state is merged with the curiosity value map s t−1 from the previous step to get the current curiosity value map s t .After that, the updated map projects the scores of each direction back onto the panoramic image, and the direction with the highest score is selected.Secondly, given the selected direction image, the new subtask and the goal flag are determined by PlanVLM and are stored in memory as cost c t , and the memory h t is combined by s t and c t .Finally, the two-stage action proposer annotates the action sequence on the selected image and sends it into ReasonVLM to obtain the final polar coordinate vector action a t for execution.Note that PlanVLM and ReasonVLM are configured by the cost c t−1 .</p>
<p>FormatFig. 3 :
3
Fig. 3: Predict the Likelihood.(a) The world model predicts the Curiosity Value for each direction in the panoramic image based on the likelihood of the goal's presence.(b) The mutual projection of the navigable area between the ego-centric and top-down view perspectives.(c) Curiosity Value Map construction: The predicted scores from the world model are projected onto the top-down map and then fused with the previous step's Curiosity Value Map.</p>
<p>( 3 )Fig. 4 :
34
Fig.4: Plan the Route.Text prompt is configured by the previous step's subtask, the explanation for selecting the highest-scoring image, and the goal.Using the image with the highest curiosity value as the image prompt, the VLM is invoked to plan the agent's new subtask and detect the goal.</p>
<p>1 )
1
Exploration Stage: Given SubT ask t and I ann t , to satisfy the requirements of the subtask SubT ask t , the VLM Action Proposer VLM Response: 'Action ①' Text prompt = 'TASK: Go to the hallway <Subtask>.Your final task is to NAVIGATE TO THE NEAREST SOFA, and get as close to it as possible.Choose your action from the image promptText prompt = 'The agent has been tasked with navigating to a SOFA.First confirm whether the sofa is in the image prompt.if there is a sofa in the image, then choose the best position.If there is no sofa, then return failure message</p>
<p>TABLE I :
I
[19]-shot object navigation results on HM3D v0.1[38]and MP3D[19]benchmarks.TF refers to trainingfree, and ZS refers to zero-shot.
Model TF ZSHM3D SR(%)↑ SPL(%)↑ SR(%)↑ SPL(%)↑ MP3DHabitat-Web [22] ✗ ✗41.516.031.68.5OVRL [10] ✗ ✗--28.67.4OVRL-V2 [11] ✗ ✗64.728.1--ZSON [2] ✗ ✓25.512.615.34.8PSL [39] ✗ ✓42.419.218.96.4PixNav [12] ✗ ✓37.920.5--SGM [40] ✗ ✓60.230.837.714.7VLFM [14] ✗ ✓52.530.436.417.5CoW [4] ✓ ✓--9.24.9ESC [5] ✓ ✓39.222.328.714.2L3MVN [27] ✓ ✓50.423.1--VoroNav [7] ✓ ✓42.026.0--OpenFMNav [8] ✓ ✓54.924.4--TopV-Nav [9] ✓ ✓45.928.031.916.1WMNav(Ours) ✓ ✓58.131.245.417.2</p>
<p>TABLE II :
II
[38]tion study of different modules and memory strategies on HM3D v0.2[38].SD refers to the subtask decomposition, TAP refers to the Two-stage Action Proposer strategy, No refers to No Memory, Text-Image refers to Text-Image Memory, and CVM refers to Curiosity Value Map.
Memory SD TAP SR(%)↑ SPL(%)↑aNo✗✗65.825.8bNo✓ ✗67.433.1c Text-Image ✓ ✗62.029.6d CVM(Ours) ✗✗69.534.9e CVM(Ours) ✓ ✗70.134.7f CVM(Ours) ✓ ✓72.233.3Table I, our method outperforms all the state-of-the-art zero-shot methods (+3.2% SR and +3.2% SPL on HM3D, +13.5%SR and +1.1% SPL on MP3D</p>
<p>TABLE III :
III
[38]tion study of VLMs on HM3D v0.1[38].
VLM SR(%)↑ SPL(%)↑Qwen2.5-VL-3B29.715.8Qwen2.5-VL-7B46.120.7Gemini 1.5 Flash53.527.0Gemini 2.0 Flash57.930.7Gemini 1.5 Pro58.131.2
ACKNOWLEDGMENTThis work was supported by the National Natural Science Foundation of China under Grant 62373356 and the Joint Funds of the National Natural Science Foundation of China under U24B20162.
Objectnav revisited: On evaluation of embodied agents navigating to objects. D Batra, A Gokaslan, A Kembhavi, O Maksymets, R Mottaghi, M Savva, A Toshev, E Wijmans, arXiv:2006.131712020arXiv preprint</p>
<p>ZSON: Zero-shot object-goal navigation using multimodal goal embeddings. A Majumdar, G Aggarwal, B S Devnani, J Hoffman, D Batra, Advances in Neural Information Processing Systems. A H Oh, A Agarwal, D Belgrave, K Cho, 2022</p>
<p>Zero-shot object searching using large-scale object relationship prior. H Chen, R Xu, S Cheng, P A Vela, D Xu, 2023</p>
<p>Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation. S Y Gadre, M Wortsman, G Ilharco, L Schmidt, S Song, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202323181</p>
<p>Esc: Exploration with soft commonsense constraints for zeroshot object navigation. K Zhou, K Zheng, C Pryor, Y Shen, H Jin, L Getoor, X E Wang, arXiv:2301.131662023arXiv preprint</p>
<p>How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers. J Chen, G Li, S Kumar, B Ghanem, F Yu, 2023</p>
<p>Voronav: Voronoi-based zero-shot object navigation with large language model. P Wu, Y Mu, B Wu, Y Hou, J Ma, S Zhang, C Liu, arXiv:2401.026952024arXiv preprint</p>
<p>Openfmnav: Towards open-set zeroshot object navigation via vision-language foundation models. Y Kuang, H Lin, M Jiang, arXiv:2402.106702024arXiv preprint</p>
<p>Topv-nav: Unlocking the top-view spatial reasoning potential of mllm for zero-shot object navigation. L Zhong, C Gao, Z Ding, Y Liao, S Liu, arXiv:2411.164252024arXiv preprint</p>
<p>Offline visual representation learning for embodied navigation. K Yadav, R Ramrakhya, A Majumdar, V.-P Berges, S Kuhar, D Batra, A Baevski, O Maksymets, Workshop on Reincarnating Reinforcement Learning at ICLR 2023. 2023</p>
<p>Ovrl-v2: A simple state-of-art baseline for imagenav and objectnav. K Yadav, A Majumdar, R Ramrakhya, N Yokoyama, A Baevski, Z Kira, O Maksymets, D Batra, arXiv:2303.077982023arXiv preprint</p>
<p>Bridging zero-shot object navigation and foundation models through pixel-guided navigation skill. W Cai, S Huang, G Cheng, Y Long, P Gao, C Sun, H Dong, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>End-to-end navigation with vision language models: Transforming spatial reasoning into question-answering. D Goetting, H G Singh, A Loquercio, arXiv:2411.057552024arXiv preprint</p>
<p>Vlfm: Visionlanguage frontier maps for zero-shot semantic navigation. N Yokoyama, S Ha, D Batra, J Wang, B Bucher, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, International conference on machine learning. PMLR202319742</p>
<p>Cross from left to right brain: Adaptive text dreamer for vision-and-language navigation. P Zhang, Y Su, P Wu, D An, L Zhang, Z Wang, D Wang, Y Ding, B Zhao, X Li, arXiv:2505.208972025arXiv preprint</p>
<p>Is your llm secretly a world model of the internet? model-based planning for web agents. Y Gu, B Zheng, B Gou, K Zhang, C Chang, S Srivastava, Y Xie, P Qi, H Sun, Y Su, arXiv:2411.065592024arXiv preprint</p>
<p>S K Ramakrishnan, A Gokaslan, E Wijmans, O Maksymets, A Clegg, J Turner, E Undersander, W Galuba, A Westbury, A X Chang, arXiv:2109.08238Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. 2021arXiv preprint</p>
<p>Matterport3D: Learning from RGB-D data in indoor environments. A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, International Conference on 3D Vision (3DV). 2017</p>
<p>Thda: Treasure hunt data augmentation for semantic navigation. O Maksymets, V Cartillier, A Gokaslan, E Wijmans, W Galuba, S Lee, D Batra, 2021 IEEE/CVF International Conference on Computer Vision (ICCV). 2021363</p>
<p>Simple but effective: Clip embeddings for embodied ai. A Khandelwal, L Weihs, R Mottaghi, A Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2022</p>
<p>Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. R Ramrakhya, E Undersander, D Batra, A Das, CVPR. 2022</p>
<p>Learning active camera for multi-object navigation. P Chen, D Ji, K Lin, W Hu, W Huang, T H Li, M Tan, C Gan, Advances in Neural Information Processing Systems. A H Oh, A Agarwal, D Belgrave, K Cho, 2022</p>
<p>FILM: Following instructions in language with modular methods. S Y Min, D S Chaplot, P K Ravikumar, Y Bisk, R Salakhutdinov, International Conference on Learning Representations. 2022</p>
<p>Jarvis: A neuro-symbolic commonsense reasoning framework for conversational embodied agents. K Zheng, K Zhou, J Gu, Y Fan, J Wang, Z Li, X He, X E Wang, arXiv:2208.132662022arXiv preprint</p>
<p>Zero experience required: Plug &amp; play modular transfer learning for semantic visual navigation. Z Al-Halah, S K Ramakrishnan, K Grauman, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20221741</p>
<p>L3mvn: Leveraging large language models for visual target navigation. B Yu, H Kasaei, M Cao, arXiv:2304.055012023arXiv preprint</p>
<p>Navigation with large language models: Semantic guesswork as a heuristic for planning. D Shah, M Equi, B Osinski, F Xia, B Ichter, S Levine, 7th Annual Conference on Robot Learning. 2023</p>
<p>Bridging zero-shot object navigation and foundation models through pixel-guided navigation skill. W Cai, S Huang, G Cheng, Y Long, P Gao, C Sun, H Dong, arXiv:2309.103092023arXiv preprint</p>
<p>Grounded language-image pre-training. L H Li, P Zhang, H Zhang, J Yang, C Li, Y Zhong, L Wang, L Yuan, L Zhang, J.-N Hwang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202210975</p>
<p>Surds: Benchmarking spatial understanding and reasoning in driving scenarios with vision language models. X Guo, R Zhang, Y Duan, Y He, D Nie, W Huang, C Zhang, S Liu, H Zhao, L Chen, arXiv:2411.131122024arXiv preprint</p>
<p>Dyna, an integrated architecture for learning, planning, and reacting. R S Sutton, ACM Sigart Bulletin. 241991</p>
<p>World models. D Ha, J Schmidhuber, abs/1803.10122ArXiv preprint. 2018</p>
<p>Modelbased reinforcement learning: A survey. T M Moerland, J Broekens, A Plaat, C M Jonker, Foundations and Trends® in Machine Learning. 202316</p>
<p>Cognitive map for language models: Optimal planning via verbally representing the world model. D Kim, J Lee, J Park, M Seo, ArXiv preprint. 2406.15275, 2024</p>
<p>Web agents with world models: Learning and leveraging environment dynamics in web navigation. H Chae, N Kim, K T .-I. Ong, M Gwak, G Song, J Kim, S Kim, D Lee, J Yeo, arXiv:2410.132322024arXiv preprint</p>
<p>Moma-kitchen: A 100k+ benchmark for affordancegrounded last-mile navigation in mobile manipulation. P Zhang, X Gao, Y Wu, K Liu, D Wang, Z Wang, B Zhao, Y Ding, X Li, arXiv:2503.110812025arXiv preprint</p>
<p>S K Ramakrishnan, A Gokaslan, E Wijmans, O Maksymets, A Clegg, J M Turner, E Undersander, W Galuba, A Westbury, A X Chang, M Savva, Y Zhao, D Batra, Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI," in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Prioritized semantic learning for zero-shot instance navigation. X Sun, L Liu, H Zhi, R Qiu, J Liang, European Conference on Computer Vision. Springer2024</p>
<p>Imagine before go: Self-supervised generative map for object goal navigation. S Zhang, X Yu, X Song, X Wang, S Jiang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202416425</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. S Liu, Z Zeng, T Ren, F Li, H Zhang, J Yang, C Li, J Yang, H Su, J Zhu, arXiv:2303.054992023arXiv preprint</p>
<p>Segment anything. A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Q Team, arXiv:2407.10671Qwen2 technical report. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
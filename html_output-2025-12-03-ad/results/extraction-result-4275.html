<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4275 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4275</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4275</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-280677755</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.11779v1.pdf" target="_blank">A Multi-Task Evaluation of LLMs'Processing of Academic Text Input</a></p>
                <p><strong>Paper Abstract:</strong> How much large language models (LLMs) can aid scientific discovery, notably in assisting academic peer review, is in heated debate. Between a literature digest and a human-comparable research assistant lies their practical application potential. We organize individual tasks that computer science studies employ in separate terms into a guided and robust workflow to evaluate LLMs'processing of academic text input. We employ four tasks in the assessment: content reproduction/comparison/scoring/reflection, each demanding a specific role of the LLM (oracle/judgmental arbiter/knowledgeable arbiter/collaborator) in assisting scholarly works, and altogether testing LLMs with questions that increasingly require intellectual capabilities towards a solid understanding of scientific texts to yield desirable solutions. We exemplify a rigorous performance evaluation with detailed instructions on the prompts. Adopting first-rate Information Systems articles at three top journals as the input texts and an abundant set of text metrics, we record a compromised performance of the leading LLM - Google's Gemini: its summary and paraphrase of academic text is acceptably reliable; using it to rank texts through pairwise text comparison is faintly scalable; asking it to grade academic texts is prone to poor discrimination; its qualitative reflection on the text is self-consistent yet hardly insightful to inspire meaningful research. This evidence against an endorsement of LLMs'text-processing capabilities is consistent across metric-based internal (linguistic assessment), external (comparing to the ground truth), and human evaluation, and is robust to the variations of the prompt. Overall, we do not recommend an unchecked use of LLMs in constructing peer reviews.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4275.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4275.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ThisStudy_Gemini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Multi-Task Evaluation of LLMs' Processing of Academic Text Input (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This study uses Google's Gemini Pro (1.0/1.5) to process 246 Information Systems articles with a four-task workflow (content reproduction, comparison, scoring, reflection) to probe whether an LLM can reliably summarize, rank, score, and qualitatively reflect on scholarly papers and to surface recurring critique subjects and text-quality patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Google's Gemini Pro (versions 1.0 and 1.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Multi-task evaluation workflow (Tasks 1-4: reproduction, comparison, scoring, reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>One-round, zero-shot prompting pipeline applied to the Introduction and Conclusion sections of each paper; four explicit tasks: (1) Content reproduction — generate keywords and abstract from provided intro+conclusion (oracle role); (2) Content comparison — pairwise preference between two articles to build Copeland rankings (judgmental arbiter); (3) Content scoring — assign a 1–10 quality score to a single article (knowledgeable arbiter); (4) Content reflection — produce up to 3 critiques per article (collaborator role). Prompts were varied across four robustness axes (semantic paraphrase, prompt richness, data abundance, instruction specificity) and each prompt-run repeated five times to form an ensemble. Outputs were evaluated with internal linguistic metrics (lexical density, Shannon entropy, TTR, Flesch–Kincaid), external text-similarity metrics (Jaccard, TF-IDF cosine, BLEU, ROUGE), Copeland ranking analysis for pairwise comparisons, distributional analyses for scoring (skewness, kurtosis), and human arbiter ratings for abstracts and critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>246 papers</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Information Systems (interdisciplinary IS articles from ISR, JMIS, Management Science)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>qualitative critique subjects and recurring textual/pattern signals about article quality (empirical generalizations about common weaknesses), summary/paraphrase output, pairwise preference patterns and scoring distributions (patterns in scoring behavior rather than formal scientific laws)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Examples of recurring qualitative patterns extracted by the LLM: 'lack of empirical evidence', 'limited generalizability', 'limited scope of analysis' (top recurring critique subjects); aggregate findings such as LLM-generated texts using larger but more concentrated vocabularies; measured pairwise preference error rates (22%–41%) indicating inconsistency in comparative judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Metric-based internal evaluation (lexical density, entropy, TTR, FK), external similarity (Jaccard, TF-IDF cosine, BLEU, ROUGE) against ground-truth keywords/abstracts, Copeland ranking deviation analysis for pairwise preferences, statistical analysis of score distributions (skewness/kurtosis), and human evaluation by 87 graduate student arbiters scoring LLM abstracts/critiques on 1–5 scales.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Key quantitative outcomes reported: human reliability score for LLM-generated abstracts (HE_abs) mean ≈ 3.3/5; human insightfulness score for critiques (HE_cri) mean ≈ 3.1/5; LLM-output score mean ŵave = 8.3 (scale 1–10), std ≈ 0.50, min/max 6.0/9.6; pairwise comparison error-rate estimates ϵ best-fit per article type = {RA:0.41, RN:0.40, SI:0.35, JM:0.36, MS:0.22}; external similarity means for abstracts (e.g., EE_cos ≈ 0.603 across set) and low BLEU/ROUGE for keywords; robustness deviations <5% in many prompt variants (39/58 cases).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared LLM outputs to ground-truth (authors' keywords/abstracts), to human arbiter judgments (expert crowd of graduate students), and across prompt variants; no alternative model baselines (other LLMs) were experimented with in this paper's main analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM is acceptably reliable at content reproduction (summarization/paraphrase) but shows diminished performance as tasks require more scientific understanding: pairwise comparisons are inconsistent and not scalable (error rates 22%–41%), scoring exhibits shrinkage and poor discrimination (scores concentrated and skewed upward), and qualitative reflections (critiques) are self-consistent but lack deep insight. Prompt instruction specificity and semantic prompt phrasing matter; richness/abundance matter less. LLM outputs and ground-truth bibliometric metrics appear to capture different quality dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Identified limitations include hallucination in content reproduction, stochasticity/inconsistency in pairwise comparisons, shrinkage/tempered objectivity in scoring, non-understanding leading to bounded insightfulness in critiques, input/output token limits restricting context, and general issues (deception risk, bias).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "A Multi-Task Evaluation of LLMs'Processing of Academic Text Input", 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4275.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4275.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Scientist_Lu2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-scientist for Fully Automated Open-Ended Scientific Discovery (Lu et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work that claims an 'AI-scientist' system can execute end-to-end scientific workflows (idea generation, code, experiments, writing, review) and produce formal manuscripts, indicating an AI-driven route toward automated scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned as a recent development: an AI system capable of executing an end-to-end workflow that generates ideas, builds code, runs experiments, writes manuscripts and conducts review; the paper uses this citation to motivate the argument that LLM-based systems could approach roles in discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>General/computer-science research workflows (claimed cross-domain automation)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Implied generation/extraction of research artifacts and potentially hypotheses/manuscripts rather than explicit, formal scientific laws; the citation is used to support claims about automating discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed in this paper (cited only); used as contextual evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that automated systems are moving toward full research workflows, strengthening the argument that LLMs might become research assistants; the authors nevertheless note skepticism about genuine scientific understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Paper uses Lu et al. as motivation but emphasizes that automated discovery still relies on substantial human supervision and may not equate to true scientific understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "A Multi-Task Evaluation of LLMs'Processing of Academic Text Input", 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4275.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4275.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PathChat_Lu2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PathChat (multimodal AI copilot) reported in Lu et al. (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal AI copilot claimed (in the cited Lu et al.) to produce accurate, pathologist-preferred responses to diverse pathology queries; cited here as an example of LLM-like systems assisting domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PathChat (multimodal AI copilot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Described briefly as a multimodal copilot that handles diverse queries related to pathology, illustrating multimodal LLM applications in domain-specific tasks (education, research, clinical decision-making).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Pathology / biomedical domain (example application)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Not described as extracting formal laws; rather produces domain-appropriate answers and assists decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in this paper (cited as claim).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used as an example that multimodal AI copilots can provide accurate domain responses, strengthening plausibility of AI assistance in scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Noted in context that such systems still raise concerns about overclaiming understanding and require oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "A Multi-Task Evaluation of LLMs'Processing of Academic Text Input", 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4275.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4275.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yuksekgonul2025_Workflow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Ensemble Workflow with Backpropagated Feedback (Yuksekgonul et al., 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited study assembling multiple LLMs into a workflow where specialized instances summarize, rate, and critique and iterative feedback/backpropagation optimizes the final output — an architecture relevant to extracting structured insights from scholarly texts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Optimizing generative AI by backpropagating language model feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Multi-LLM ensemble with feedback/backpropagation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Described as a pipeline of LLMs each playing different roles (summarizer, rater, critic) with iterative feedback (backpropagation-like) to improve outputs; paper references this as an approach that mirrors the study's Tasks 1, 3, and 4 (summarize, rate, critique).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Generative-AI methodology (general)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Workflow intended to improve generation of structured outputs (summaries, critiques) and could be used to surface recurring qualitative patterns or hypotheses from corpora, though extracting formal laws is not explicitly claimed here.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not described in detail in this paper; cited as methodological precedent.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of multi-model workflows that can refine outputs through self-feedback, relevant to constructing pipelines for extracting recurring qualitative patterns from papers.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Paper notes general concerns about overconfidence if ensemble feedback loops are assumed to produce understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "A Multi-Task Evaluation of LLMs'Processing of Academic Text Input", 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4275.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4275.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iten2020_PhysicalConcepts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discovering Physical Concepts with Neural Networks (Iten et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work demonstrating that neural networks can discover or represent physical concepts from data; used in this paper to illustrate AI systems extracting scientific concepts, analogous to extracting qualitative principles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discovering physical concepts with neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>neural networks (concept-discovery networks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced as an example where ML systems extract compact, interpretable physical concepts from data (not necessarily from papers); cited to motivate the idea of AI discovering concepts or theories.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Physics / AI for scientific concept discovery</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Physical concepts / interpretable structures learned from data — mechanistic or conceptual constructs rather than textual summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed in this paper (cited as prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that neural models can yield interpretable scientific concepts from raw data, supporting the broader discussion of AI-assisted discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Authors use this to contrast domain-specific discovery (data-driven concept extraction) with the more difficult challenge of textual understanding and theory extraction from papers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "A Multi-Task Evaluation of LLMs'Processing of Academic Text Input", 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4275.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4275.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Farquhar2024_SemanticEntropy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Detecting Hallucinations in Large Language Models Using Semantic Entropy (Farquhar et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited method (semantic entropy) that uses auxiliary LLM-based evaluation to detect hallucinations and assess semantic fidelity in model outputs, relevant to validating extracted qualitative claims from scholarly text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Detecting hallucinations in large language models using semantic entropy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Semantic entropy-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced as an advanced metric for external evaluation of LLM outputs that employs other LLM(s) to estimate semantic plausibility/factuality (semantic entropy), proposed as a complement to word-based internal/external metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>LLM evaluation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Not directly extracting laws; provides an evaluation tool to assess the trustworthiness of qualitative outputs claimed by LLMs (e.g., extracted principles or critiques).</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Cited as an improved evaluation approach for detecting hallucinations and semantic misalignment in LLM-generated claims.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper suggests semantic-meaning-grounded metrics (like semantic entropy) can outrun word-overlap metrics (BLEU/ROUGE) for evaluating LLMs' extraction of meaningful claims from texts.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Indicates need for better semantic evaluation when judging qualitative claims produced by LLMs from scholarly inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "A Multi-Task Evaluation of LLMs'Processing of Academic Text Input", 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery <em>(Rating: 2)</em></li>
                <li>Discovering physical concepts with neural networks <em>(Rating: 2)</em></li>
                <li>Optimizing generative AI by backpropagating language model feedback <em>(Rating: 2)</em></li>
                <li>Detecting hallucinations in large language models using semantic entropy <em>(Rating: 2)</em></li>
                <li>Functional genomic hypothesis generation and experimentation by a robot scientist <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4275",
    "paper_id": "paper-280677755",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "ThisStudy_Gemini",
            "name_full": "A Multi-Task Evaluation of LLMs' Processing of Academic Text Input (this paper)",
            "brief_description": "This study uses Google's Gemini Pro (1.0/1.5) to process 246 Information Systems articles with a four-task workflow (content reproduction, comparison, scoring, reflection) to probe whether an LLM can reliably summarize, rank, score, and qualitatively reflect on scholarly papers and to surface recurring critique subjects and text-quality patterns.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Google's Gemini Pro (versions 1.0 and 1.5)",
            "model_size": null,
            "method_name": "Multi-task evaluation workflow (Tasks 1-4: reproduction, comparison, scoring, reflection)",
            "method_description": "One-round, zero-shot prompting pipeline applied to the Introduction and Conclusion sections of each paper; four explicit tasks: (1) Content reproduction — generate keywords and abstract from provided intro+conclusion (oracle role); (2) Content comparison — pairwise preference between two articles to build Copeland rankings (judgmental arbiter); (3) Content scoring — assign a 1–10 quality score to a single article (knowledgeable arbiter); (4) Content reflection — produce up to 3 critiques per article (collaborator role). Prompts were varied across four robustness axes (semantic paraphrase, prompt richness, data abundance, instruction specificity) and each prompt-run repeated five times to form an ensemble. Outputs were evaluated with internal linguistic metrics (lexical density, Shannon entropy, TTR, Flesch–Kincaid), external text-similarity metrics (Jaccard, TF-IDF cosine, BLEU, ROUGE), Copeland ranking analysis for pairwise comparisons, distributional analyses for scoring (skewness, kurtosis), and human arbiter ratings for abstracts and critiques.",
            "number_of_papers": "246 papers",
            "domain_or_field": "Information Systems (interdisciplinary IS articles from ISR, JMIS, Management Science)",
            "type_of_laws_extracted": "qualitative critique subjects and recurring textual/pattern signals about article quality (empirical generalizations about common weaknesses), summary/paraphrase output, pairwise preference patterns and scoring distributions (patterns in scoring behavior rather than formal scientific laws)",
            "example_laws_extracted": "Examples of recurring qualitative patterns extracted by the LLM: 'lack of empirical evidence', 'limited generalizability', 'limited scope of analysis' (top recurring critique subjects); aggregate findings such as LLM-generated texts using larger but more concentrated vocabularies; measured pairwise preference error rates (22%–41%) indicating inconsistency in comparative judgments.",
            "evaluation_method": "Metric-based internal evaluation (lexical density, entropy, TTR, FK), external similarity (Jaccard, TF-IDF cosine, BLEU, ROUGE) against ground-truth keywords/abstracts, Copeland ranking deviation analysis for pairwise preferences, statistical analysis of score distributions (skewness/kurtosis), and human evaluation by 87 graduate student arbiters scoring LLM abstracts/critiques on 1–5 scales.",
            "performance_metrics": "Key quantitative outcomes reported: human reliability score for LLM-generated abstracts (HE_abs) mean ≈ 3.3/5; human insightfulness score for critiques (HE_cri) mean ≈ 3.1/5; LLM-output score mean ŵave = 8.3 (scale 1–10), std ≈ 0.50, min/max 6.0/9.6; pairwise comparison error-rate estimates ϵ best-fit per article type = {RA:0.41, RN:0.40, SI:0.35, JM:0.36, MS:0.22}; external similarity means for abstracts (e.g., EE_cos ≈ 0.603 across set) and low BLEU/ROUGE for keywords; robustness deviations &lt;5% in many prompt variants (39/58 cases).",
            "comparison_baseline": "Compared LLM outputs to ground-truth (authors' keywords/abstracts), to human arbiter judgments (expert crowd of graduate students), and across prompt variants; no alternative model baselines (other LLMs) were experimented with in this paper's main analysis.",
            "key_findings": "LLM is acceptably reliable at content reproduction (summarization/paraphrase) but shows diminished performance as tasks require more scientific understanding: pairwise comparisons are inconsistent and not scalable (error rates 22%–41%), scoring exhibits shrinkage and poor discrimination (scores concentrated and skewed upward), and qualitative reflections (critiques) are self-consistent but lack deep insight. Prompt instruction specificity and semantic prompt phrasing matter; richness/abundance matter less. LLM outputs and ground-truth bibliometric metrics appear to capture different quality dimensions.",
            "challenges_limitations": "Identified limitations include hallucination in content reproduction, stochasticity/inconsistency in pairwise comparisons, shrinkage/tempered objectivity in scoring, non-understanding leading to bounded insightfulness in critiques, input/output token limits restricting context, and general issues (deception risk, bias).",
            "uuid": "e4275.0",
            "source_info": {
                "paper_title": "A Multi-Task Evaluation of LLMs'Processing of Academic Text Input",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "AI-Scientist_Lu2024",
            "name_full": "AI-scientist for Fully Automated Open-Ended Scientific Discovery (Lu et al., 2024)",
            "brief_description": "Cited work that claims an 'AI-scientist' system can execute end-to-end scientific workflows (idea generation, code, experiments, writing, review) and produce formal manuscripts, indicating an AI-driven route toward automated scientific discovery.",
            "citation_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": null,
            "method_description": "Mentioned as a recent development: an AI system capable of executing an end-to-end workflow that generates ideas, builds code, runs experiments, writes manuscripts and conducts review; the paper uses this citation to motivate the argument that LLM-based systems could approach roles in discovery.",
            "number_of_papers": null,
            "domain_or_field": "General/computer-science research workflows (claimed cross-domain automation)",
            "type_of_laws_extracted": "Implied generation/extraction of research artifacts and potentially hypotheses/manuscripts rather than explicit, formal scientific laws; the citation is used to support claims about automating discovery.",
            "example_laws_extracted": "",
            "evaluation_method": "Not detailed in this paper (cited only); used as contextual evidence.",
            "performance_metrics": "",
            "comparison_baseline": "",
            "key_findings": "Cited as evidence that automated systems are moving toward full research workflows, strengthening the argument that LLMs might become research assistants; the authors nevertheless note skepticism about genuine scientific understanding.",
            "challenges_limitations": "Paper uses Lu et al. as motivation but emphasizes that automated discovery still relies on substantial human supervision and may not equate to true scientific understanding.",
            "uuid": "e4275.1",
            "source_info": {
                "paper_title": "A Multi-Task Evaluation of LLMs'Processing of Academic Text Input",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "PathChat_Lu2024",
            "name_full": "PathChat (multimodal AI copilot) reported in Lu et al. (2024)",
            "brief_description": "A multimodal AI copilot claimed (in the cited Lu et al.) to produce accurate, pathologist-preferred responses to diverse pathology queries; cited here as an example of LLM-like systems assisting domain experts.",
            "citation_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "mention_or_use": "mention",
            "model_name": "PathChat (multimodal AI copilot)",
            "model_size": null,
            "method_name": null,
            "method_description": "Described briefly as a multimodal copilot that handles diverse queries related to pathology, illustrating multimodal LLM applications in domain-specific tasks (education, research, clinical decision-making).",
            "number_of_papers": null,
            "domain_or_field": "Pathology / biomedical domain (example application)",
            "type_of_laws_extracted": "Not described as extracting formal laws; rather produces domain-appropriate answers and assists decision-making.",
            "example_laws_extracted": "",
            "evaluation_method": "Not specified in this paper (cited as claim).",
            "performance_metrics": "",
            "comparison_baseline": "",
            "key_findings": "Used as an example that multimodal AI copilots can provide accurate domain responses, strengthening plausibility of AI assistance in scientific tasks.",
            "challenges_limitations": "Noted in context that such systems still raise concerns about overclaiming understanding and require oversight.",
            "uuid": "e4275.2",
            "source_info": {
                "paper_title": "A Multi-Task Evaluation of LLMs'Processing of Academic Text Input",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Yuksekgonul2025_Workflow",
            "name_full": "LLM Ensemble Workflow with Backpropagated Feedback (Yuksekgonul et al., 2025)",
            "brief_description": "Cited study assembling multiple LLMs into a workflow where specialized instances summarize, rate, and critique and iterative feedback/backpropagation optimizes the final output — an architecture relevant to extracting structured insights from scholarly texts.",
            "citation_title": "Optimizing generative AI by backpropagating language model feedback",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "Multi-LLM ensemble with feedback/backpropagation",
            "method_description": "Described as a pipeline of LLMs each playing different roles (summarizer, rater, critic) with iterative feedback (backpropagation-like) to improve outputs; paper references this as an approach that mirrors the study's Tasks 1, 3, and 4 (summarize, rate, critique).",
            "number_of_papers": null,
            "domain_or_field": "Generative-AI methodology (general)",
            "type_of_laws_extracted": "Workflow intended to improve generation of structured outputs (summaries, critiques) and could be used to surface recurring qualitative patterns or hypotheses from corpora, though extracting formal laws is not explicitly claimed here.",
            "example_laws_extracted": "",
            "evaluation_method": "Not described in detail in this paper; cited as methodological precedent.",
            "performance_metrics": "",
            "comparison_baseline": "",
            "key_findings": "Cited as an example of multi-model workflows that can refine outputs through self-feedback, relevant to constructing pipelines for extracting recurring qualitative patterns from papers.",
            "challenges_limitations": "Paper notes general concerns about overconfidence if ensemble feedback loops are assumed to produce understanding.",
            "uuid": "e4275.3",
            "source_info": {
                "paper_title": "A Multi-Task Evaluation of LLMs'Processing of Academic Text Input",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Iten2020_PhysicalConcepts",
            "name_full": "Discovering Physical Concepts with Neural Networks (Iten et al., 2020)",
            "brief_description": "Cited work demonstrating that neural networks can discover or represent physical concepts from data; used in this paper to illustrate AI systems extracting scientific concepts, analogous to extracting qualitative principles.",
            "citation_title": "Discovering physical concepts with neural networks",
            "mention_or_use": "mention",
            "model_name": "neural networks (concept-discovery networks)",
            "model_size": null,
            "method_name": null,
            "method_description": "Referenced as an example where ML systems extract compact, interpretable physical concepts from data (not necessarily from papers); cited to motivate the idea of AI discovering concepts or theories.",
            "number_of_papers": null,
            "domain_or_field": "Physics / AI for scientific concept discovery",
            "type_of_laws_extracted": "Physical concepts / interpretable structures learned from data — mechanistic or conceptual constructs rather than textual summaries.",
            "example_laws_extracted": "",
            "evaluation_method": "Not detailed in this paper (cited as prior work).",
            "performance_metrics": "",
            "comparison_baseline": "",
            "key_findings": "Cited as evidence that neural models can yield interpretable scientific concepts from raw data, supporting the broader discussion of AI-assisted discovery.",
            "challenges_limitations": "Authors use this to contrast domain-specific discovery (data-driven concept extraction) with the more difficult challenge of textual understanding and theory extraction from papers.",
            "uuid": "e4275.4",
            "source_info": {
                "paper_title": "A Multi-Task Evaluation of LLMs'Processing of Academic Text Input",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Farquhar2024_SemanticEntropy",
            "name_full": "Detecting Hallucinations in Large Language Models Using Semantic Entropy (Farquhar et al., 2024)",
            "brief_description": "Cited method (semantic entropy) that uses auxiliary LLM-based evaluation to detect hallucinations and assess semantic fidelity in model outputs, relevant to validating extracted qualitative claims from scholarly text.",
            "citation_title": "Detecting hallucinations in large language models using semantic entropy",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "Semantic entropy-based evaluation",
            "method_description": "Referenced as an advanced metric for external evaluation of LLM outputs that employs other LLM(s) to estimate semantic plausibility/factuality (semantic entropy), proposed as a complement to word-based internal/external metrics.",
            "number_of_papers": null,
            "domain_or_field": "LLM evaluation / NLP",
            "type_of_laws_extracted": "Not directly extracting laws; provides an evaluation tool to assess the trustworthiness of qualitative outputs claimed by LLMs (e.g., extracted principles or critiques).",
            "example_laws_extracted": "",
            "evaluation_method": "Cited as an improved evaluation approach for detecting hallucinations and semantic misalignment in LLM-generated claims.",
            "performance_metrics": "",
            "comparison_baseline": "",
            "key_findings": "Paper suggests semantic-meaning-grounded metrics (like semantic entropy) can outrun word-overlap metrics (BLEU/ROUGE) for evaluating LLMs' extraction of meaningful claims from texts.",
            "challenges_limitations": "Indicates need for better semantic evaluation when judging qualitative claims produced by LLMs from scholarly inputs.",
            "uuid": "e4275.5",
            "source_info": {
                "paper_title": "A Multi-Task Evaluation of LLMs'Processing of Academic Text Input",
                "publication_date_yy_mm": "2025-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Discovering physical concepts with neural networks",
            "rating": 2,
            "sanitized_title": "discovering_physical_concepts_with_neural_networks"
        },
        {
            "paper_title": "Optimizing generative AI by backpropagating language model feedback",
            "rating": 2,
            "sanitized_title": "optimizing_generative_ai_by_backpropagating_language_model_feedback"
        },
        {
            "paper_title": "Detecting hallucinations in large language models using semantic entropy",
            "rating": 2,
            "sanitized_title": "detecting_hallucinations_in_large_language_models_using_semantic_entropy"
        },
        {
            "paper_title": "Functional genomic hypothesis generation and experimentation by a robot scientist",
            "rating": 1,
            "sanitized_title": "functional_genomic_hypothesis_generation_and_experimentation_by_a_robot_scientist"
        }
    ],
    "cost": 0.021902249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Multi-Task Evaluation of LLMs' Processing of Academic Text Input
15 Aug 2025</p>
<p>Tianyi Li tianyi.li@cuhk.edu.hk.co-firstauthor. 
Yu Qin yuqin@asu.edu.co-firstauthor. 
Olivia R Liu olivia.liu.sheng@asu.edu </p>
<p>Department of Decisions, Operations and Technology
CUHK</p>
<p>Department of Information Systems
Arizona State University</p>
<p>A Multi-Task Evaluation of LLMs' Processing of Academic Text Input
15 Aug 2025186846E05D4011AE962A25FDD9FF771FarXiv:2508.11779v1[cs.CL]large language modelsprocessing of academic textInformation Systemspeer review
How much large language models (LLMs) can aid scientific discovery, notably in assisting academic peer review, is in heated debate.Between a literature digest and a human-comparable research assistant lies their practical application potential.We organize individual tasks that computer science studies employ in separate terms into a guided and robust workflow to evaluate LLMs' processing of academic text input.We employ four tasks in the assessment: content reproduction/comparison/scoring/reflection, each demanding a specific role of the LLM (oracle/judgmental arbiter/knowledgeable arbiter/collaborator) in assisting scholarly works, and altogether testing LLMs with questions that increasingly require intellectual capabilities towards a solid understanding of scientific texts to yield desirable solutions.We exemplify a rigorous performance evaluation with detailed instructions on the prompts.Adopting firstrate Information Systems articles at three top journals as the input texts and an abundant set of text metrics, we record a compromised performance of the leading LLM -Google's Gemini: its summary and paraphrase of academic text is acceptably reliable; using it to rank texts through pairwise text comparison is faintly scalable; asking it to grade academic texts is prone to poor discrimination; its qualitative reflection on the text is self-consistent yet hardly insightful to inspire meaningful research.This evidence against an endorsement of LLMs' text-processing capabilities is consistent across metric-based internal (linguistic assessment), external (comparing to the ground truth), and human evaluation, and is robust to the variations of the prompt.Overall, we do not recommend an unchecked use of LLMs in constructing peer reviews.</p>
<p>Introduction</p>
<p>Using artificial intelligence (AI) for scientific discovery is an ambitious vision for the future of science.The discussion has drawn much attention in past decades (e.g., Lindsay et al., 1993;King et al., 2004King et al., , 2009;;Waltz and Buchanan, 2009) and is gaining enormous momentum along the recent surge in AI (e.g., Zdeborová, 2017;Extance, 2018;Kitano, 2021;Sourati and Evans, 2023), notably the AI for Science campaign (e.g., Wang et al., 2023)."Self-driving labs" (Crabtree, 2020) transform science labs into automated factories of discovery (Angelopoulos et al., 2024), aiming to help chemical syntheses (Dai et al., 2024) especially drug discovery (Vert, 2023), material design (Jablonka et al., 2023), mathematical proofs (Frieder et al., 2024) and even the development of physical concepts (Iten et al., 2020;He, 2024), despite warnings such as the reproducibility crisis (Kapoor and Narayanan, 2023), the danger of "fast science" (Frith, 2020), the illusions of scientific understanding (Messeri and Crockett, 2024), and the over-reliance on AI (Narayanan and Kapoor , 2025).</p>
<p>Generative AI, in particular large language models (LLMs), joins the landscape in a strong whirl.Advancing conventional computational tools which emphasize machines' capabilities in search, analysis, and optimization that help override "human bottlenecks" (Gil et al., 2014), LLMs proclaim more capabilities especially via advanced text-processing to help address intellectual tasks including creative works (Chen and Chan, 2024).People envision employing LLMs to facilitate scholarly work during problem formulation, hypothesis generation, research design, data collection/analysis, interpretation/theorization, and composition/writing (Agrawal et al., 2024;Arora et al., 2024;Nowogrodzki , 2024), despite practical challenges (e.g., Brainard , 2023), impediments from the LLM explosion (e.g., Abbasi et al., 2024), biases in their output (e.g., Fang et al., 2024), and researchers' mixed feelings on this new technology (e.g., Prillaman, 2024).See Wiley's ExplanAItions study1 on AI's role in research and insights into how researchers are or are not embracing AI.</p>
<p>1.1 The grand debate: How much can LLMs aid scientific discovery?</p>
<p>On the ground level, LLMs' excellent capabilities in summarizing and paraphrasing promise to help researchers digest scientific literature, which is in tremendous growth over the years (Landhuis, 2016;Bornmann et al., 2021).We can use LLMs to filter content, condense texts, extract findings, and communicate the science (Alvarez et al., 2024).This application of LLMs is propelled by the increasing use of academic papers as the training data (Gibney, 2024b) and gets mature along with the development of commercial products such as Elicit (https://elicit.com),Semantic Reader2 , and Elsevier's Scopus AI.These tools nonetheless encounter barriers, such as the lack of accuracy and verifiability, restricted access to full texts, and non-machine-readable formats, bringing inevitable limitations (Callaway, 2023) and calling for tremendous system amelioration.As a result, although LLMs may help increase productivity and confidence in writing (Li et al., 2024a), many people display negative perceptions of LLM-assisted writing (Li et al., 2024b).</p>
<p>A prominent question is to what extent the other envisioned applications of LLMs aiding scientific discovery, which build on their basic function of literature digest, can be validated.</p>
<p>(+) To one extreme, some argue that LLMs have the potential to evolve into a full-fledged research assistant with solid scientific understanding; in this picture, with the continuous advancement of their technical core, LLMs will have a prodigious impact on occupations closely related to scientific discovery which are among those having the highest share of tasks exposed to LLMs (Eloundou et al., 2024, Figure 16 in Supplementary Materials).For example, the recent development of an "AI-scientist for fully automated open-ended scientific discovery" (Lu et al., 2024) which can produce formal manuscripts for computer science (CS) conference submissions from a complete workflow of idea generation, code building, experiment execution, and writing and review, adds a sounding weight to this conjecture; the multimodal AI copilot PathChat (Lu et al., 2024) can produce "accurate and pathologist-preferable responses to diverse queries related to pathology" and considerably help education, research, and clinical decision-making.</p>
<p>(-) Despite the promising landscape, many remain skeptical of this ambition and contend that scientific discovery is different from scientific understanding and the current AI-assisted approach is not and will not easily reach the level of understanding (e.g., see the discussions in Science by Melanie Mitchell) 3 which should "recognize qualitatively characteristic consequences of a theory without performing exact computations and use them in a new context" or effectively "teach human experts" (De Regt and Dieks, 2005;Gao et al., 2024).Therefore, instead of viewing AI tools as an "agent of understanding" (Krenn et al., 2022), we should view them as an artificial agency that embodies sufficient complexity but without the capability of understanding.For example, with the AI scientist in Lu et al. (2024), one would argue that its workflow of science production closely follows human instructions and the produced manuscript is strictly structured by human input; the "automatic" discovery relies on much intervention from the human supervisor.</p>
<p>The practical application potential of LLMs lies somewhere between that of a literature digest and a humancomparable research assistant-the two ends that span the spectrum of LLMs' intellectual capabilities.Studies evaluating LLMs should look into this spectrum and investigate LLMs' capabilities from the ground up to the full-fledged scenario.</p>
<p>1.2 The focal debate: How feasible is LLM-assisted peer review?Specifically, of AI's many potential applications for scientific discovery, AI-assisted peer review (Checco et al., 2021;Van Dinter et al., 2021;Wagner et al., 2022) stands out as having promising conditions to materialize in the near term.Peer review lies at the core of science production (Spier , 2002;Rai , 2016;Sarker et al., 2023;Brown, 2024) yet increasingly suffers from speed and objectivity issues, among others (e.g., Shah, 2022); academia has been testing innovative ways (e.g., Tennant et al., 2017;Kerzendorf et al., 2020) to accelerate this heavy-laden research evaluation process (e.g., Kovanis et al., 2016;Andersen, 2020) and mitigate the bias within it (e.g., Lee et al., 2013;Leibbrandt et al., 2018).</p>
<p>Progress in natural language processing and machine learning in general allows people to imagine employing computational tools to automate the peer review of academic texts (e.g., Kuznetsov et al., 2024) presumably with an enhanced speed and objectivity, and potentially with better structuring (Malički , 2024).Predictive AI tools such as machine classifiers have already been implemented in the peer review workflow, e.g., for plagiarism/misinformation/error detection and statistics/structure/method checks during the initial quality control of submitted manuscripts (e.g., Kousha and Thelwall , 2024).Generative AI and in particular LLMs holds great promise in this application, and researchers are extensively investigating the feasibility of LLMassisted peer review (e.g., Liu and Shah, 2023;Liang et al., 2024;Roberson, 2023;Du et al., 2024) and are developing trial tools (e.g., D' Arcy et al., 2024;Jin et al., 2024;Sun et al., 2024).</p>
<p>The results from initial investigations reside in two camps.</p>
<p>(+) Some studies support the utility of LLMs.For example, in a large-scale empirical study, Liang et al. (2024) reported that a dataset of GPT-4-generated reviews aligned reasonably with human reviews and a large portion of the surveyed user base found LLM-generated reviews useful; however, GPT-4 focused on certain aspects of the feedback and struggled to provide in-depth critiques.</p>
<p>(-) Other studies question the augmented use of the tool.For example, Liu and Shah (2023) reported that GPT-4, outperforming several other LLMs, did well in error identification and submission checklist verification but not so in choosing the better abstract from two candidates; the tested LLM was thus not yet suitable for a complete evaluation of papers or proposals despite a promising use in specific review tasks.</p>
<p>The potential and possible pitfalls of employing LLMs in aiding academic peer review need careful examination to release the power of the tool and avoid bad practice (Shmueli et al., 2023;Liang et al., 2024;Liao and Zhang, 2024;Zou, 2024;Bhargava et al., 2025;Naddaf , 2025).While the literature on leveraging LLMs in text assessment such as in dialogue quality measurement and evaluation (e.g., Huynh et al., 2023;Jia et al., 2024) expands rapidly, studies focusing on academic text input are at an infant stage.Existing works also often consider particular tasks around peer review while lacking an overarching investigation.</p>
<p>Our guided evaluation</p>
<p>In a seminal discussion, Messeri and Crockett (2024) proposed a conceptual framework to help address the ongoing debates.They argue that AI can serve four roles -oracle, surrogate, quant, and arbiter -in scientific assistantship, where the digital tools are primarily used for literature digest, data collection, data analysis, and content evaluation, respectively.The idea of AI serving as a human-like collaborator with solid scientific understanding is rejected, and they imply that the increasing epistemic trust people grant to AI that builds on its presumed objectivity, proclaimed analytical depth, and pre-acknowledged efficiency incites an over-optimism on employing AI in assisting scientific research.</p>
<p>Among the four roles of AI in scientific assistantship, the oracle role and the arbiter role, which ask the AI to directly process academic text input, apply readily to LLMs.In this study, we develop a guided evaluation echoing Messeri and Crockett (2024)'s conceptual framework to assess LLMs' processing of academic text input and investigate its practical potential in scientific discovery particularly in assisting academic peer review.We focus on LLMs' processing of academic text input instead of their processing of academic texts in general, e.g., searching and summarizing the existing literature.</p>
<p>We employ four tasks assembled into a guided workflow to conduct the assessment (see details in Section 2): content reproduction (Task 1), content comparison (Task 2), content scoring (Task 3), and content reflection (Task 4), while existing studies only employ a subset of them to evaluate LLMs (see Appendix A for these studies).We propose that each task demands a specific role of LLMs in assisting scientific discovery and tests different capabilities of the LLM; from Tasks 1 to 4, the difficulty for LLMs to satisfactorily complete the task increases, as a good solution there calls more and more for the proximity to a solid scientific understanding.</p>
<p>The four tasks adhere to the components of a typical academic peer review:</p>
<p>• Content reproduction can occur when editors screen articles to determine whether to send manuscripts out for review or when they create content outlines (e.g., when reviewers produce summaries at the beginning of review reports).</p>
<p>• Content comparison can assist editors, who are subject to limited reviewer pools and small publication selection quotas, in handling a large number of manuscripts, which is typical for conference submissions.</p>
<p>• Content scoring can help evaluate the differential publishability of manuscripts where reviewers give quantitative scores to the reviewed manuscripts for editors' consideration and decision.</p>
<p>• Content reflection can be called for when reviewers provide authors with qualitative and instructive feedback on their manuscripts, which constitutes the essential content in review reports.</p>
<p>These tasks thus help assess LLMs' application potential at different stages of an academic peer review: authors to editors, editors to reviewers, back to editors, and finally back to authors (Shmueli et al., 2023).</p>
<p>Do NOT take LLMs' capabilities for granted</p>
<p>Our experiments across the four tasks examine a spectrum of LLMs' capabilities to investigate their application potential, noting that LLMs' different capabilities should not be taken for granted.</p>
<p>In Eloundou et al. (2024)'s high-impact work on GPTs' influence on labor markets, LLMs' capabilities at summarizing medium-length documents (corresponding to content reproduction) and providing feedback on documents (corresponding to content reflection) are juxtaposed as their basic capabilities.Similarly, in Yuksekgonul et al. (2025), multiple LLMs are assembled in a workflow to optimize the final output through the backpropagation of feedback, with each LLM playing a different function in the system: summarize, rate, or critique; the three functionalities are investigated in our evaluation (Tasks 1, 3, and 4).These two are typical examples in social science (Eloundou et al., 2024) or engineering fields (Yuksekgonul et al., 2025) where one overlooks the difference in tasks for LLMs and stays over-confident in their capabilities.We warn that this blind assumption that LLMs could perform well at different functionalities is unwarranted.</p>
<p>Moreover, evidence suggests that although people found LLM-generated reviews helpful to a certain extent, e.g., at providing timely feedback, for early-stage manuscripts, or when the review has been well structured (Liang et al., 2024;Drori and Te'eni , 2024;Gruda, 2024), limitations of the current leading LLMs with regard to the features (in italic font) evaluated at our four tasks are apparent:</p>
<p>• At content reproduction, Tao et al. (2024) reported that GPT-4 responded with 86.9% accuracy in reproducing content upon queries (acceptable reliability).</p>
<p>• At content comparison, Liu and Shah (2023) reported that the LLM committed errors six out of ten times when comparing two abstracts and selecting the outperforming one (unwarranted scalability).</p>
<p>• At content scoring, Drori and Te'eni (2024) reported that LLM scores were higher than human scores by a substantial positive bias of around 23% (skewed discrimination).</p>
<p>• At content reflection, Liang et al. (2024) reported that despite the ability to generate non-generic feedback, LLM commented on research implications 7.27 times more frequently, yet on research novelty 10.69 times less frequently, than humans did (bounded insightfulness).</p>
<p>Noting the limitations, we organize these tasks into an integrated workflow to measure these four important features of the LLM output.Our standpoint thus contrasts with those studies (e.g., Wang et al., 2023) that advocate for LLMs' intellectual capabilities in particular with respect to human intelligence.Overall, our results provide evidence of unwarranted over confidence in LLMs' capabilities across the four tasks.</p>
<p>Establishing an exemplary analysis</p>
<p>LLMs' text-processing capabilities vary by the LLM and the input text.For the LLM, open-(to certain degrees (Gibney, 2024a)) or closed-source products (Oketch et al., 2025) based on small-(e.g., Zhou et al., 2024) to large-scale architectures are developed for special (e.g., Shen et al., 2024) or general applications, and are getting constantly upgraded under market competition.For the input academic text, articles from diverse scientific disciplines can differ considerably in content ingredients and semantic styles.To allow a comprehensive evaluation of LLMs' processing capabilities, it is imperative to conduct extensive studies with various LLM products and text samples.</p>
<p>A feasible idea for recruiting miscellaneous academic content is to use the articles from interdisciplinary journals or essentially, to focus on an interdisciplinary field.The discipline of Information Systems (IS) provides a suitable option, which is a field known for exhibiting interdisciplinary research across social, physical, and managerial sciences and supporting diverse scientific approaches in studying information technologies (Benbasat and Weber , 1996;Robey, 1996).As such, academic texts in IS cover an extended range of topics, methodologies, and materials and demonstrate various semantic styles, compared to texts from a more concentrated discipline; this variety brings the necessary diversity of the input for testing the LLM.</p>
<p>In this study, we present an exemplary analysis for our evaluation framework, focusing on demonstrating the effectiveness and robustness of its tasks and the computational workflow in processing text input from a single discipline by an LLM of choice.For the academic texts, we collect 246 articles recently published in three top journals (Information Systems Research, Journal of Management Information Systems, and IS Department of Management Sciences), all publishing diverse first-rate articles devoted to the IS field.For the LLM, we use the popular Google's Gemini Pro4 .The focal assessment is on whether it is desirable to recruit the tested LLM in aiding the review of IS articles, which has not been directly investigated by previous studies.Based on the focal evaluation, we show the potential of the guided framework in providing insights that benefit future evaluations applied to academic content in broad disciplines or other LLMs.</p>
<p>Overall, the motivations and outline of this research are summarized in Figure 1.</p>
<p>Research contributions</p>
<p>This study investigates a pivotal topic that draws increasing attention from different research disciplines.With global investment in AI-particularly in large language models (LLMs)-estimated to reach trillions over the past and coming decades (Letzing, 2024), research on LLMs is expected to remain active.Information Systems (IS) scholars bear the responsibility of evaluating their role, performance, and deployment.</p>
<p>Our evaluation workflow follows the design science principles in IS studies (Hevner et al., 2004;Gregor and Hevner , 2013;Rai et al., 2017;Abbasi et al., 2024) and the steps of classical artifacts (Prat et al., 2015) of computational frameworks (e.g., Chang and Woo, 1994;Abbasi and Chen, 2008;Suarez et al., 2024).Supplementing technical viability to the conceptual discussion on AI and the future of research, we contribute a context to help connect IS to broader audiences (Padmanabhan et al., 2022) in surrounding fields with qualitative (e.g., Messeri and Crockett, 2024) or quantitative (e.g., computer sciences) methodologies.</p>
<p>Studying the behavior of LLMs in aiding future research during our human-machine interaction (Jain et al., 2021;Caro et al., 2022) is essential for developing the abilities to control the actions of these artificial agents, reap their benefits, and minimize their harms (Rahwan et al., 2019).This is important for understanding the "Janus effects" of AI tools and promoting their responsible use (Susarla et al., 2023).As such, we answer the call in Yoo et al. (2024) for future IS studies to have "balanced attention to both the positive and negative externalities of digital innovations" and in particular "being aware of and actively engaging with the potential negative consequences of digital innovation", while "[believing] that nuanced, human-centered, responsibly designed technological innovations can play essential roles in addressing challenges."In a fresh debate, Binz et al. (2025) collects scientists' perspectives on LLMs' effect on the future practice of science: some see working with LLMs similar to working with humans, some deny the possibilities of AI scientists replacing humans and caution for misuse, while some envision AI and humans working together with us remaining responsible for determining the research roadmap.Following Bhargava et al. (2025), we contend that the integration of generative AI in scholarly work is a "means to augment, not replace, human contributions."In a future landscape where LLMs keep advancing, our work contributes a viable tool for their assessment; nevertheless, as issues such as the shortage of training data and the fall of the "scaling law" (e.g., Villalobos et al., 2024) start to question the optimism around LLMs' continued upgrades, we emphasize on LLMs' limitations and the imperfect LLM output.Overall, securing the right path for future science asks for extensive interdisciplinary efforts; we try to contribute a responsible discussion from the IS field, where scholars are "positioned to offer guidelines that ensure the ethical and effective use of generative AI across diverse business communities, facilitating interdisciplinary collaboration."(Bhargava et al., 2025) LLMs' imperfect processing of academic texts is nonetheless useful.An LLM acts effectively as an "intelligent and well-rounded layman": this artificial audience possesses the knowledge base of diverse scientific fields.This allows LLMs to produce a fair evaluation of academic texts across disciplines: texts teemed with nomenclature (e.g., in biology), equations (e.g., in maths), or algorithms (e.g., in CS), can be assessed with an indifferent eye.A perfect literature digest may not help distinguish the texts as it outputs excellent summaries; an intermediate-level "knowing-all-by-a-little", which suitably describes early-stage LLMs, is effective for discriminating texts of diverse genres and qualities with its flawed output.Our evaluation thus does not chase state-of-the-art LLMs but embraces the imperfect LLM output, dismissing the worry about the obsolescence or sub-optimality of LLMs, which currently undergo rapid market transitions.</p>
<p>Promisingly, LLMs' processing can be utilized to indicate text quality.This helps design measures for articles/journals/scholars, complementing current diffusion-based metrics (e.g., impact factor, H-Index (Waltman, 2016; Aria and Cuccurullo, 2017)) that are rather discipline-specific (Sunahara et al., 2021), contributing to the amelioration of science evaluation (Allen, 2025;Steyn, 2025) (observing the Leiden Manifesto (Hicks et al., 2015)).</p>
<p>The Multi-task Evaluation</p>
<p>We employ four tasks to study LLMs' processing of academic text input (Figure 2): content reproduction (Task 1), content comparison (Task 2), content scoring (Task 3), and content reflection (Task 4).</p>
<p>Figure 2: The multi-task evaluation of LLMs' processing of academic text input.</p>
<p>At content reproduction, we ask the LLM to reproduce short texts (keywords and abstract) based on the long input text.The LLM plays the role of oracle.The task is intrinsically a language generation question, the least difficult for LLMs, and from the results, we evaluate LLMs' basic capability of paraphrasing and summarizing.We desire reliability from the LLM output: the reproduced texts are reliable to the original text.This task assesses LLMs' basic application potential as literature digests.</p>
<p>At content comparison, we ask the LLM to compare two input texts and output a quality preference.The LLM plays the role of arbiter; we further note that here it is a judgmental arbiter that works based on the input material.The task is a closed-ended question, calling for LLMs' capability of making comparisons.We desire scalability from the LLM output: the pairwise preferences can scale up.Scalable pairwise comparisons can underlie the application potential of LLMs at text ranking (e.g., during manuscript competitions).</p>
<p>At content scoring, we ask the LLM to evaluate a single input text and output a quality score.The LLM plays the role of arbiter and in particular a knowledgeable arbiter, as the text evaluation should rely on the knowledge of relevant scientific literature.The task is then an open-ended question, testing LLMs' capability of making quantitative evaluations.We desire discrimination from the LLM output: the quality scores can effectively discriminate texts.A good content scoring can support LLMs' application in text rating (e.g., of grant proposals).</p>
<p>At content reflection, we ask the LLM to examine the input text and output a list of critiques.In this most difficult task, the LLM eventually plays the role of collaborator during science production.The task poses an intellectual question to the LLM, asking it to analyze the text with sophistication that adheres to scientific understanding.We desire insightfulness from the LLM output: the qualitative reflections on the text bring insights.LLMs' potential as a full-fledged research assistant is not impossible if they succeed in this task.</p>
<p>Across the four tasks, the LLM is posed with questions that increasingly require intellectual capabilities towards a solid scientific understanding to yield desirable solutions.In each task, we assess whether a desired feature (reliability, scalability, discrimination, insightfulness) can be established from the LLM output, using different analytical tools to study the LLM output (Section 3).Notably, Tasks 1 and 4 evaluate LLMs' capabilities in outputting texts, while Tasks 2 and 3 evaluate LLMs' capabilities as judges (Zheng et al., 2023).Desirable performances in these tasks endorse LLMs' application potential at literature digest (upon reliable content reproduction), text ranking (upon scalable content comparison), text rating (upon discriminating content scoring), and finally full research assistantship (upon insightful content reflection), respectively.The explicit delineation of the roles of the LLM, the means of the evaluation, and the objectives of the assessment conforms closely to IS design science principles (Hevner et al., 2004;Peffers et al., 2007).</p>
<p>A major issue in LLM evaluation is the lack of robustness due to prompt diversity and probabilistic nature of the LLM output.The engineering of LLM prompts, which consists of the data part (input data for LLMs to process) and the instruction part (output instructions for LLMs to follow), is a central topic in LLM research that denies simple discussions (e.g., Lin, 2024;Schulhoff et al., 2024).In our experiments, we vary the baseline prompts and conduct robustness checks, considering the following four aspects of the LLM prompts' robustness:</p>
<p>• Semantic robustness of the prompt (R1).We substitute the keywords of the prompt with words of similar meanings; we test if the prompt is robust to semantic variations.</p>
<p>• Richness of the prompt (R2).We add extra input of a different data type to the baseline input; we test if enhancing the richness of the prompt affects the quality of the LLM output.</p>
<p>• Abundance of the data prompt (R3).We increase the length of the data prompt; we test if enhancing the abundance of the data prompt affects the quality of the LLM output.</p>
<p>• Specificity of the instruction prompt (R4).We give more specific instructions compared to the baseline; we test if enhancing the specificity of the instruction prompt affects the quality of the LLM output.</p>
<p>Below, we explain the baseline prompts and their variants (following the rationales of R1-R4).To account for LLMs' probabilistic output, we conduct five runs under each prompt and analyze the result ensemble.</p>
<p>Our experiments are summarized in Table 1.</p>
<p>Task 1: Content Reproduction</p>
<p>Experiment 1-0: Input the introduction and conclusion sections of a scientific article.Ask the LLM to generate a list of keywords for this text.</p>
<p>Baseline Prompt."For a scientific article (in the field of Information Systems) with the following 'Introduction' and 'Conclusion' sections, generate a list of appropriate keywords (please only output necessary keywords and separate the keywords by commas).'Introduction:' {introduction}.'Conclusion:' {conclusion}."</p>
<p>Robustness checks.Building on Experiment 1-0, investigate the following variants of the baseline prompt.</p>
<p>• Experiment 1-1, 1-2: Input only the introduction or the conclusion section.(R3)</p>
<p>• Experiment 1-3: Include the article title in the prompt.(R2)</p>
<p>• Experiment 1-4: Specify the number of keywords to be generated.(R4)</p>
<p>• Experiment 1-5: Demand that the output keywords be ordered by importance.(R4)</p>
<p>Experiment 2-0: Input the introduction and conclusion sections of a scientific article.Ask the LLM to generate an abstract for this text.</p>
<p>Baseline Prompt."For a scientific article (in the field of Information Systems) with the following 'Introduction' and 'Conclusion' sections, generate a one-paragraph abstract that contains no more than 300 words (please only output the main abstract).'Introduction:' {introduction}.'Conclusion:' {conclusion}."</p>
<p>Robustness checks.Building on Experiment 2-0, investigate the following variants of the baseline prompt.</p>
<p>• Experiment 2-1, 2-2: Input only the introduction or the conclusion section.(R3)</p>
<p>• Experiment 2-3: Include the ground-truth keywords in the prompt.(R2)</p>
<p>• Experiment 2-4: Include the LLM-generated keywords in the prompt.(R2)</p>
<p>• Experiment 2-5: Specify the length of the abstract to be generated.Robustness checks.Building on Experiment 3-0, investigate the following variants of the baseline prompt.</p>
<p>• Experiment 3-1/2/3: Change the word "quality" in the prompt to "information density"/"scientific value"/"comprehension difficulty".(R1) Robustness checks.Building on Experiment 4-0, investigate the following variants of the baseline prompt.</p>
<p>• Experiment 4-1/2/3: Change the word "quality" in the prompt to "information density"/"scientific value"/"comprehension difficulty".(R1)</p>
<p>• Experiment 4-4: Include the ground-truth abstract in the prompt.(R2)</p>
<p>• Experiment 4-5: Specify the score interval (set as 0.1).(R4)</p>
<p>Task 4: Content Reflection</p>
<p>Experiment 5-0: Input the introduction and conclusion sections of a scientific article.Ask the LLM to reflect on this text and generate a list of critiques.</p>
<p>Baseline Prompt."For a scientific article (in the field of Information Systems) with the following 'Introduction' and 'Conclusion' sections, output a list of critiques (in at most 3 bullet points) on its content.'Introduction:' {introduction}.'Conclusion:' {conclusion}."</p>
<p>Output analysis</p>
<p>We collect the LLM output to conduct analyses.(A) Objective analyses.For Tasks 1 and 4, we conduct metric-based internal (linguistic assessment) and external (comparing to the ground truth (Task 1) or the text input (Task 4)) evaluation of the LLM-generated keywords, abstracts, and critiques.For Task 2, we construct a ranking over the text ensemble based on LLM's pairwise preferences.For Task 3, we investigate the effectiveness of discrimination of the LLM-output text scores.(B) Subjective analyses.We recruit human arbiters to evaluate LLM-generated abstracts (Task 1) and critiques (Task 4).See details in Section 3. Table 1: Experimental setup.e.g., E1-0: Experiment 1-0.Across the five experiments, there are a panel of robustness checks concerning four aspects of the variation of the prompt for this one-round human-LLM interaction: (1) semantic robustness of the prompt (E3-1/2/3, 4-1/2/3), (2) richness of the prompt (E1-3, 2-3, 2-4, 4-4), (3) abundance of the data prompt (E1-1, 1-2, 2-1, 2-2), and ( 4) specificity of the instruction prompt (E1-4, 1-5, 2-5, 4-5).</p>
<p>3 Analytical Tools</p>
<p>Metrics for text evaluation</p>
<p>For content reproduction (Task 1) and content reflection (Task 4), we employ complementary metrics to evaluate the quality of LLM-generated texts from two angles: internal and external.For internal evaluation, we assess the linguistic quality of the text (Kyle, 2019), employing four metrics: information density (via the Halliday lexical density (Halliday, 1985)), richness (via the Shannon information entropy (Shi and Lei , 2022) or the type-token ratio (TTR) (Richards, 1987)), and readability (via the Flesch-Kincaid (FK) score (Kincaid et al., 1975)).For external evaluation, we compare the LLM-generated keywords/abstracts/critiques to the ground truth (Task 1) or the text input (Task 4), employing four metrics to indicate text similarity: Jaccard index, cosine similarity (upon the Term Frequency-Inverse Document Frequency (TF-IDF) (Lahitani et al., 2016)), bilingual evaluation understudy (BLEU) (Papineni et al., 2002), and recall-oriented understudy for gisting evaluation (ROUGE) (Lin, 2004).The metrics for text evaluation are summarized in Table 2. Cosine similarity
EE cos = n i=1 T F IDForiginal * T F IDFgenerated √ n i=1 T F IDF 2 original √ n i=1 T F IDF 2 generated BLEU EE BLEU = BP * exp( N n=1 w n log p n ) ROUGE EE ROU GE = (1+β 2 )EEROUGE L −recallEEROU GE L −precision EEROUGE L −recall+β 2 EEROUGE L −precision
Table 2: Metrics for internal/external text evaluation.</p>
<p>Internal Evaluation</p>
<p>Information density (via the Halliday lexical density).The information density of the text is measured using the Halliday lexical density, defined as the ratio of the number of lexical items to the number of clauses in the text.Following To et al. (2013), we determine the number of lexical items/clauses based on the presence of finite verbs, i.e., the ratio of all words to verbs.The metric IE H−density is given by
IE H−density = number of lexical items number of clauses . (1)
Richness (via the Shannon information entropy).To evaluate the richness of the text, we compute the Shannon entropy based on the frequency of words in the text.High entropy indicates a rich text employing a large vocabulary.The metric IE entropy is given by
IE entropy = − n i=1 p i log 2 p i , (2)
where n is the number of unique words and p i is the probability of the occurrence of word i.</p>
<p>Richness (via the type-token ratio (TTR)).TTR measures the richness of the text by the ratio of unique words ("types") to the total number of words ("tokens") in the text.A high TTR indicates a rich vocabulary.The metric IE T T R is given by (4)
IE T T R =
We use the toolkit syllapy (https://github.com/mholtzscher/syllapy) to count text syllables.</p>
<p>External Evaluation</p>
<p>Jaccard index.Jaccard index measures the similarity of two sets of elements.Here the elements are unique words in the text.The metric EE Jac is given by (n is the number of unique words)
EE Jac = |n original ∩ n generated | |n original ∪ n generated | ,(5)
Cosine similarity.Cosine similarity measures the angle between two vectors in a multi-dimensional space.</p>
<p>Here text vectors are represented with the TF-IDF, which is proportional to word frequency in the text while offset by word frequency in a background corpus (English dictionary), adjusting for common words.The metric EE cos is given by (n is the number of unique words)
EE cos = n i=1 T F IDF original * T F IDF generated n i=1 T F IDF 2 original n i=1 T F IDF 2 generated ,(6)
Bilingual Evaluation Understudy (BLEU).BLEU evaluates machine-translated texts against reference translations.Here it calculates the precision of thewords in the LLM-generated text that appear in the ground-truth text, applying a penalty to discourage short texts.The metric EE BLEU is given by
EE BLEU = BP * exp( N n=1 w n log p n ),(7)
where p n and w n are the precision and weight of each word; BP = e − max (0,r/c−1) is penalizing short texts, with c/r as the respective lengths of the LLM-generated/ground-truth text.</p>
<p>Recall-Oriented Understudy for Gisting Evaluation (ROUGE).ROUGE evaluates machine translations against references by measuring the match of words, word pairs, and word sequences between the two texts.Here we use ROUGE-L, which calculates the F-measure of the longest common subsequences (LCSs) between the two texts.We construct the metric EE ROU GE , given by ,
EE ROU GE L −recall = LCS(refEE ROU GE = (1 + β 2 )EE ROU GE L −recall EE ROU GE L −precision EE ROU GE L −recall + β 2 EE ROU GE L −precision , (8)
where β is the weight for recall relative to precision (set to 1 in our calculation to give equal importance to precision and recall).</p>
<p>Text ranking from quality preferences</p>
<p>For content comparison (Task 2), we collect the LLM's pairwise preferences to construct a ranking over the text ensemble.Ranking objects based on pairwise comparisons is an important problem (Wang et al., 2013) with various algorithmic solutions (e.g., Jamieson and Nowak , 2011;Negahban et al., 2012).We use the classic Copeland counting (Copeland , 1951;Shah and Wainwright, 2018) to construct the ranking.For two objects i and j, y ij ∈ [−1, 0, 1] is the ground-truth outcome of their comparison: j beating i (y ij = −1), dual (y ij = 0), or i beating j (y ij = 1); the sum of an object's scores from N − 1 comparisons is its Copeland score.For error-free comparisons, the resulting Copeland scores for the compared items should form the perfect sequence N − 1, N − 3, N − 5, ..., −(N − 3), −(N − 1).</p>
<p>Notably, due to the stochasticity of the LLM, it is possible that the outcome of comparing i to j, ŷij , differs from the (opposite) outcome of comparing j to i, − ŷji .We average the two outcomes, ẑij = 9 ŷij + (− ŷji ))/2, and obtain the matrix Z = { ẑij }, which is asymmetric and deviates from the symmetric ground-truth outcome matrix Y = {y ij }.The distance between the measured Copeland scores (computed on Z) and the perfect sequence (computed on Y) can help estimate the error rate of the LLM output (Section 5.2).</p>
<p>Assessing text quality scores</p>
<p>For content scoring (Task 3), we assess the discrimination of the LLM-output text scores ŵi .We indicate the spread of ŵi over the N texts using mean/median/mode, std, and percentiles.We demonstrate the sharpness (Gneiting et al., 2007) of the score distribution using skewness and kurtosis:
skewness = m 3 /m 3/2 2 , kurtosis = m 4 /m 2
2 − 3, where m p is the p-th moment of the score distribution:
m p = 1 N N i=1 ( ŵi − ŵave ) p .(9)
We consider shrinkage bias in the LLM-output scores which narrows the default score range [1, 10] to an internal range [H l , H h ].Under this bias, we discuss the condition for LLM-output scores to distinguish ground-truth score differences (Section 5.3).</p>
<p>Human evaluation of LLM-generated texts</p>
<p>We recruited 87 graduate students (including 1 undergraduate and 1 unreported) from business schools and engineering schools as paid human arbiters to evaluate LLM-generated texts; recruited arbiters' English reading skills are endorsed by sufficient language test scores (Appendix B).At content reproduction (Task 1), arbiters compare the LLM-generated abstract to the ground-truth abstract and submit an integer score HE abs from 1 (least) to 5 (most) to indicate the LLM output's reliability.At content reflection (Task 4), arbiters read the input article sections and the LLM-generated critiques and submit an integer score HE cri from 1 (least) to 5 (most) to indicate the LLM output's insightfulness.</p>
<p>We follow the guidelines of prior studies on asking human arbiters to evaluate AI-generated content (texts or images) (e.g., Nightingale and Farid , 2022;Hitsuwari et al., 2023;Miller et al., 2023).The Institutional Review Board of the University approved the study protocols.The evaluations were performed in accordance with all relevant guidelines and regulations; informed consent was obtained from all participants.The collected data was stored securely and did not include any direct information that could reveal participants' identities or any sensitive information that could link to participants' personal records.</p>
<p>We developed an online interface for this crowdsourced evaluation (Appendix C).Arbiters were encouraged to complete the assignment remotely through the web page.After reading a short instruction, each arbiter was randomly assigned 12 abstract pairs and 8 article-critique pairs, displayed in a sequence; we thus gathered six samples for each HE abs and four samples for each HE cri .The time limit was 3 minutes for abstract comparison and 8 minutes for critique assessment.We recorded arbiters' judgment time and discarded the samples from very fast judgment, i.e., &lt;15 seconds at abstract comparison or &lt;60 seconds at critique assessment.There were multiple breaks between evaluation sessions; each arbiter devoted a maximum of 110 minutes (including a maximum of 100 minutes for the evaluation) to the assignment.In this study, we adopt Google's Gemini, which ranks high in the LLM leaderboard and is reported to perform well in academic writing tasks (Gruda, 2024).Google applies strict training restrictions to ensure that no LLM input is used for model training or fine-tuning; the sole purpose of logging prompt data is to detect potential misuse or policy violations7 .The academic content that we use in this study is thus secure and does not spill beyond the scope of our evaluation, precluding copyright concerns.We use the stable model versions Gemini Pro 1.0 and 1.5 (1.0 discontinued on Apr.2025) which support text input/output and are optimized for natural language tasks.The application parameters of Gemini models include input/output token limit (maximum number of tokens8 allowed in the prompt/response), temperature (degree of randomness in token selection; low temperatures corresponding to more deterministic responses), topK (the next token is to be selected from how many candidates), and topP (selecting tokens from the most to the least probable until the probability sum equals this value).We use the default configuration in our experiments; temperature is set at 0.9 to encourage stochastic output (Table 4).Under each prompt, we conduct five runs and build results over the output ensemble.We employ zero-shot prompts throughout our interaction with the LLM.</p>
<p>Large Language Model</p>
<p>input token limit output token limit temperature topK topP Google's Gemini (Pro 1.0/1.5)30720 2048 0.9 None 1.0</p>
<p>Table 4: Parameters of the LLM (Google's Gemini) used in this study.</p>
<p>Results</p>
<p>Reliability of content reproduction</p>
<p>We report the statistics of word use of ground-truth/LLM-generated keywords and abstracts at Task 1 (Table 5).Different forms of the same word are grouped.We compare the top-30 frequent words.At keywords reproduction, the LLM output employs a clearly larger vocabulary than the ground truth (6308 vs. 3053), yet much of the vocabulary is less frequently used, manifested in the higher Gini index (0.70 vs. 0.54) and fewer unique elements (1 vs. 9) in the top-30 word list.This suggests a more concentrated word use of LLM-generated texts.The word use of ground-truth/LLM-generated abstracts is similar.</p>
<p>Internal evaluation of abstracts.We show internal evaluation results of ground-truth/LLM-generated abstracts for the five article types (RA, RN, SI, JM, MS).For each article, we evaluated the linguistic quality of the two abstracts under the four metrics IE H−density/entropy/T T R/F K (Figure 3 and Table 6).</p>
<p>On the frequency plots (Figure 3) of IE • , both ground-truth and LLM-generated abstracts exhibit approximate Gaussian distributions yet with different means and variances.LLM-generated abstracts have a lower mean and smaller variance under IE H−density , IE entropy , and IE F K , suggesting more significant text homogeneity.LLM-generated abstracts obtain a higher mean score than ground-truth abstracts at IE T T R , revealing a larger vocabulary use in the LLM output.We conduct the paired t-test on the abstract pairs (Table 6a) and find these differences in IE • significant across the article set and also within the five subsets, except for a few cases.We conduct a one-way ANOVA on the five article types; in many cases (Table 6c), results suggest a significant difference across article types for both ground-truth and LLM-generated texts.</p>
<p>External evaluation of keywords/abstracts.We show external evaluation results comparing groundtruth/LLM-generated keywords and abstracts.For each article, the similarity of the two keyword lists/abstracts is evaluated under EE Jac/cos/BLEU/ROU GE (Figure 4).EE BLEU/ROU GE does not apply to the comparison of the keyword lists due to the absence of sentences.</p>
<p>Similar to IE • , the measured EE • displays approximate Gaussian distributions but now with greater deviations.Except for at EE Jac comparing ground-truth/LLM-generated keywords, where the distribution is highly right-skewed and fitting a symmetric normal distribution thus results in a negative fitted mean.For the LLM, the keyword output obtains smaller similarity scores (EE Jac/cos ) than the abstract output (Table 6b), as it is more challenging to reproduce the exact ground-truth keywords than to reproduce a similar abstract.Across five article types, one-way ANOVA results (Table 6c) reveal a significant difference in the Jaccard similarity at both keywords and the abstract, and in the ROUGE score at the abstract.</p>
<p>Human evaluation of abstracts.We show human evaluation results of LLM-generated abstracts.For each article, the reliability of the LLM-generated abstract with respect to the ground truth is indicated by the average score HE abs ∈ [1, 5] from human arbiters.</p>
<p>The arbiters' mean score given to the 12 assigned abstracts skewed upwards (Figure 5a), the lowest mean score is 2.1 and the average is 3.3.Collecting the six (five in a few cases) evaluations from arbiters, abstracts' mean score HE abs is distributed between 2.0 and 4.5, averaging at 3.3 and mostly concentrated within [3, 4] (Figure 5b).These scores suggest an acceptable acknowledgment of LLM-generated abstracts by professional human arbiters.HE abs does not show a significant difference across article types (Table 6c).Summary on reliability.Overall, our results from internal, external, and human evaluations of LLMgenerated keywords and abstracts suggest that LLMs' ability to summarize and paraphrase academic texts is acceptably reliable, although in some cases discouraging.The limitation may arise from its hallucinations (e.g., Ji et al., 2023).That said, the LLM can reasonably play the role of oracle, and its application potential is admissible at the level of literature digest.</p>
<p>Scalability of content comparison</p>
<p>We report the results of pairwise content comparison at Task 2. We compare the texts within each type (RA, RN, SI, JM, MS) for computational feasibility (due to the quadratic pairs); tests suggest that text comparisons across types bring similar results.A key feature of the results is that the LLM-output preference when asked to compare article i to j, ŷij , does not always agree with when asked to compare j to i, ŷji .Thus, for each article pair, we conduct two comparisons with each article in the front of the prompt and average the two outcomes, obtaining ẑij = (ŷ ij + (−ŷ ji ))/2.On the preference matrix Z = {ẑ ij } (Figure 6a), a value of −1 (black) or 1 (white) then indicates that the row article beats or is beaten by the column article twice in the comparison, and a value of 0 (gray) indicates that the row and the column article each wins one comparison.Z is not diagonally symmetric but has the -1 elements mirroring the 1 elements.We analyze this error as follows.Suppose the ground-truth outcome of comparing text i and j is y ij .The LLM-output preference ŷij contains error with probability ϵ:
ŷij = y ij , P = 1 − ϵ, −y ij , P = ϵ. (10)
ẑij averages the outcomes ŷij and ŷji from the two instances of comparison between i and j.Suppose the ground truth is y ij = −1.There are four cases for the outcome at ẑij : (i) ŷij is false and ŷji is true, ẑij = 0;</p>
<p>(ii) ŷij and ŷji are both true, ẑij = −1; (iii) ŷij and ŷji are both false, ẑij = 1; (iv) ŷij is true and ŷji is false, ẑij = 0. Cases (i) and (iv) yield the same outcome; the probabilities for different values of ẑij are
     P (ẑ ij = 0) = 2ϵ(1 − ϵ), P (ẑ ij = y ij [True outcome (TO)]) = (1 − ϵ) 2 , P (ẑ ij = −y ij [Inverse outcome (IO)]) = ϵ 2 . (11)
The Copeland ranking is based on the scores that sum on the matrix Z: S Copeland (i) =</p>
<p>N −1 j ẑij .We analyze the probability of having the true score S Copeland (i).In Appendix D, we prove that for a text of any true ranking in the list, the probability of obtaining its true score S Copeland decreases uniformly as N increases (see Figure 7a for the visualization of this result).This directly means that the content comparison is poorly scalable, in the sense of using it to construct the ranking over a large set of texts.</p>
<p>We use the subset of Z (randomly selecting n texts; averaging over 500 random selections) to compute S Copeland and then its deviation from the perfect sequence: the sum of the absolute distances between elements in the two ordered sequences, denoted as ∆ S .We use ∆ S to quantify the scalability loss (Figure 7b).Results confirm that ∆ S grows as n increases, at a speed inferior to the case of an all-zero Z matrix, where there is the closed form of ∆ S = N (N + 1)/2.</p>
<p>Notably, with this model, the ∆ S plot can help yield an estimate for the error probability ϵ in the LLM output.For a specific value of ϵ, equation ( 11) realizes a set of probabilities for elements in Z to take values of -1, 0, 1, respectively; the resulting Z matrix is used to compute ∆ S at this ϵ level.The error-free case (ϵ = 0) reproduces the perfect sequence (∆ S = 0); as ϵ increases, the elements in Z become more random, and the deviation ∆ S increases.Fitting the ∆ S plot from the measured Z to different ∆ S plots from ϵ-specific Zs then helps reveal the ϵ level in the LLM output.From our experiments, results show that the best-fit ϵ is 0.41/0.40/0.35/0.36/0.22 for RA/RN/SI/JM/MS articles (Figure 7b), i.e., the LLM outputs content preferences with an error rate of 22%-41%, lower than the 60% reported in Liu and Shah (2023).Summary on scalability.Our results suggest that using LLMs to rank texts through pairwise text comparison is feasible yet faintly scalable (i.e., the performance gets worse when more texts are compared) due to the lack of robustness regarding the order of comparison.The limitation may arise from its stochasticity (e.g., Bender et al., 2021).As a result, the LLM does not serve well as a judgmental arbiter, and its application potential in text ranking is much limited.</p>
<p>Discrimination of content scoring</p>
<p>To report the results of content scoring at Task 3, we show the distribution of the LLM-output scores ŵi at different article types (Figure 8).For the whole set of articles, the mean score is 8.3; median, mode, STD, skewness, and kurtosis are 8.4, 8.4, 0.50, -1.08, and 0.57, respectively; minimum/maximum score is 6.0/9.6.We investigate a shrinkage bias in the LLM-output scores, considering that the LLM scores are skewed (presumably upwards, similar to human arbiters' scores (Figures 4 and 12); see evidence in e.g., Drori and Te'eni (2024)).Suppose that the (unknown) ground-truth score w is drawn from the scale [1,10], while the LLM-output score ŵ is drawn from a narrower scale [1 ≤ H l , H h ≤ 10].The score distortion then follows:
(10 − w) : (w − 1) = (H h − ŵ) : ( ŵ − H l ) ⇒ ŵ = H h − H l 9 w + 10 9 H l − 1 9 H h ; w = 9 H h − H l ŵ + H h − 10H l H h − H l . (12)
Suppose the score interval for ŵ is 0.1; the condition for two ground-truth scores to be distinguishable is:
∆ ŵ ≥ 0.1 ⇒ H h − H l 9 ∆w ≥ 0.1 ⇒ ∆w ≥ 0.9 H h − H l . (13)
The LLM-output scores can thus only discriminate a sufficiently large ground-truth score difference ∆w.</p>
<p>When the biased score range (H h − H l ) is narrow, the condition for score discrimination is more stringent: two ground-truth scores must have a very large difference so that their discrimination can manifest in the LLM-output scores.Note that H l and H h are not equal to the min and max of the measured scores ŵ; the ŵ falls within [H l , H h ] but its distances from the two boundaries can be substantial.</p>
<p>To further analyze the condition for score discrimination, we may impose constraints on the distribution of ground-truth scores w.For example, we consider that these articles' quality averages at w ave and that the minimum is w min .Imposing these two metrics is valid as the "ground-truth" scores are eventually subjective and we can constrain their distribution with reasonable empirical considerations.This gives rise to 9
H h − H l ŵave + H h − 10H l H h − H l = w ave ⇒ H h − H l = 9( ŵave − H l ) w ave − 1 , 9 H h − H l ŵmin + H h − 10H l H h − H l = w min ⇒ H h − H l = 9( ŵmin − H l ) w min − 1 . (14)
The above equation set solves</p>
<p>proportional to the LLM score interval 0.1.When the LLM score range shrinks, w ave − w min is always no less than ŵave − ŵmin , and the condition for discriminating ground-truth scores is stricter.</p>
<p>Metrics ŵave , ŵmin can be measured from the LLM-output scores; for our results (Figure 8), ŵave = 8.3, ŵmin = 6.0.Metrics w ave , w min can be determined with empirical considerations: in the current case, as articles in the dataset are peer-reviewed and published in top journals, their quality is endorsed and thus relatively large values for w ave and w min are appropriate.We consider w ave ∈ [8.0, 9.0], w min ∈ [5.0, 7.0] and show the resulting H h , H l , and min(∆w) (Figure 9).</p>
<p>When the desired score spread is large, i.e., the ground-truth w min is small and w ave is large, the shrinkage of the LLM score range is severe: low H h , high H l , narrow H h − H l , and large min(∆w), i.e., the condition for score discrimination is strict.By contrast, when the ground-truth w min approaches w ave , which means that ground-truth scores hardly spread, or when they spread similarly to the LLM-output scores (w ave − w min ∼ ŵave − ŵmin ), the corresponding [H l , H h ] then approaches the ground-truth scale [1, 10], and min(∆w) ∼ 0.1.Summary on discrimination.Our results suggest that adopting LLMs to grade academic texts is feasible but prone to poor discrimination due to the possibility of skewed scores and score range shrinkage.The limitation may arise from its tempered objectivity (e.g., Denison et al., 2024).The LLM does not play the role of knowledgeable arbiter well, and its application potential in text rating is questionable.</p>
<p>Insightfulness of content reflection</p>
<p>We report the results of content reflection at Task 4. The LLM-generated article critiques are itemized.We obtain three pieces of critiques in one run and in total 15 pieces of critiques in the five runs.We split each critique into the subject and the detail: for example, one critique reads "Lack of Theoretical Grounding: While the introduction mentions the relevance of process virtualization theory (PVT), it fails to provide a detailed theoretical grounding for the study.A more robust theoretical framework would strengthen the paper's structure and clarify the research objectives."Here, the subject is "Lack of Theoretical Grounding" and the rest is the detail.</p>
<p>Across the 3690 pieces of critique in total, there are 2376 unique subjects, and 287 of them appear more than once; the most frequent three are "lack of empirical evidence," "limited generalizability," and "limited scope of analysis."We group similar terms (e.g., "lack of empirical evidence," "empirical validation," "empirical data").The grouped top-25 critique subjects are shown in Figure 10 (see Appendix E for the full list).Internal/external evaluation of article critiques.We evaluate the entire critique (subject and detail) on its linguistic quality (internal evaluation) using the IE • scores and its similarity to the input text (external evaluation) using the EE • scores.For internal evaluation, we calculate the linguistic quality of the input text as the background.For external evaluation, we consider (i) comparing the three critiques at one run to the input text and then taking the average EE • of the five runs, and also (ii) merging the 15 critiques in a single text and then comparing it to the input text.</p>
<p>Results suggest that (Figures 10,11), for internal evaluation, the linguistic quality of the input text (blue) is significantly higher than that of the LLM-generated critiques (green), when measured by IE H−density/entropy/F K .This quality discrepancy is larger than that of comparing ground-truth/LLM-generated abstracts (Figure 3), as revealed by the paired t-test (Table 7).This suggests that LLM-generated critiques have a smaller linguistic complexity than LLM-generated abstracts.When measured by IE T T R (left bottom of Figure 11), the LLM-generated text obtains a higher score than the input article text, consistent with Figure 3 that suggests a larger vocabulary use in the LLM output.(a) Paired t-test for the IE• difference between the input text and the LLM-generated critiques (Figure 11) and the EE• difference between the merged critique case and the three-piece critique case (Figure 12 For external evaluation, merging the 15 critiques in a single text (option (ii), red) leads to a higher similarity to the input text than the three-critique case (option (i), orange) at EE Jac/cos/ROU GE , as revealed by the paired t-test (Table 7).This suggests that the content of critiques differs in different runs.Compared to LLM-generated abstracts (Table 6), LLM-generated critiques (Table 7) have a comparable similarity to the article text.This implies that LLM-generated critiques refer to the input text to a considerable extent, approaching content reproduction.For EE BLEU (left bottom of Figure 12), as the critique is substantially shorter than the article text, the score has very small values (equation ( 7)) and does not apply to this situation well, especially at the three-piece critique.Similar to Task 1, results suggest a significant difference across article types at both internal and external evaluation of LLM-generated critiques (Table 7b).</p>
<p>Human evaluation of article critiques.We then ask human arbiters to evaluate the critiques.For each article, the arbiter submits a score HE cri ∈ [1, 5] to indicate the insightfulness of the critiques after reading them altogether with the input text.</p>
<p>Arbiters' mean scores given to the eight assigned critiques fell within [1.5, 4.4] and averaged at 3.1 (Figure 13a).Collecting the four (three in a few cases) evaluations from arbiters, the mean score for the set of critiques at each article, HE cri , is distributed between 1.2 and 4.2, averaging at 3.1 and mostly concentrated within [2.5, 4.0] (Figure 13b).HE cri does not show a significant difference across article types (Table 7b).We compare the distributions of HE abs and HE cri at the mean/median/mode, Cohen's d (effect size), variance (Levene's test), and shape (KS test).Results (Table 8) suggest significantly different score distributions and in particular a lower score (p &lt; 0.01) for LLM-generated critiques than for LLM-generated abstracts, reflecting professional human arbiters' decreased confidence in LLM-generated critiques (see more details on human evaluation results in Appendix F).Summary on insightfulness.Overall, our results from internal, external, and human evaluation of LLMgenerated critiques suggest that the LLM's qualitative reflection on the text is self-consistent but hardly insightful to inspire meaningful research.Moreover, human users give less acknowledgment to the LLM's insightfulness than they do to its output from text summary and paraphrasing.The limitation may arise from the LLM's non-understanding (e.g., Mitchell , 2023).The LLM does not sufficiently qualify as a collaborator in scientific research, and its application potential is far from endorsing a full research assistantship.</p>
<p>Robustness Checks</p>
<p>We report the results of Experiments 1-4 under variants of the baseline prompts.These robustness checks (Table 1) concern four aspects of the input prompt: semantic robustness of the prompt (E3-1/2/3, E4-1/2/3), richness of the prompt (E1-3, E2-3, E2-4, E4-4), abundance of the data prompt (E1-1, E1-2, E2-1, E2-2), and specificity of the instruction prompt (E1-4, E1-5, E2-5, E4-5).For each robustness check, we report the deviation from the main result in the baseline experiment (Experiments •-0, Sections 5.1-5.3),considering the value of a principal measurement in each result: for Experiments 1-• and 2-• (Task 1, content reproduction), consider the mean of IE • or EE • , IE • or EE • (Section 5.1); for Experiments 3-• (Task 2, content comparison), consider the difference between the obtained S Copeland and the perfect sequence, ∆ S (Section 5.2); for Experiments 4-• (Task 3, content scoring), consider the mean of the LLM-output score, ŵave (Section 5.3).We show the percentage deviation of these measurements at corresponding robustness checks; for Experiments 1-• and 2-•, we further show the average deviation across the measurements.These deviations are calculated for each article; at each check, we report the mean and variance of the deviation over an ensemble of 50 articles sampled from the whole set.</p>
<p>Results (Figure 14) show &lt; 5% deviations of these measurements in 39/58 cases, suggesting a strong robustness for our findings.Compared to other experiments, the variance is larger at (1) Experiments 1-•, which is consistent with the more fluctuating outcome during keywords reproduction due to its higher output restriction (e.g., compared to abstract reproduction (Section 5.1)), and at (2) Experiments 3-•, which is consistent with the substantial error rate of the LLM-output content comparison (Section 5.2).For the four aspects of the input prompt, our results suggest that semantic robustness (blue) and the specificity of the instruction prompt (purple) play a more important role in prompt robustness than the prompt's richness (red) or data abundance (yellow).The results' deviations are larger when the prompt is paraphrased or when the prompt instruction's specificity changes; the richness and data abundance influence the prompt to a lesser extent.• /EE cri • /HE cri from internal/external/human evaluation of LLM-generated critiques.We employ these 22 metrics that measure different aspects of LLMs' processing to indicate text quality.We study the correlations between these LLM-reliant quality metrics and the collected ground-truth quality metrics: articles' acceptance time and download count for ISR and MS articles, view count and citation count for JMIS articles (Section 4.1).We calculate Pearson's r (Figure 15) between these 26 quality metrics and Spearman's ρ and Kendall's τ (Appendix G) between their ranks.The results are similar at Spearman's ρ and Kendall's τ (Appendix G).This suggests that the two categories of metrics (LLM-reliant vs. ground-truth) do not indicate text quality along the same dimension.It points to the potential of utilizing LLMs' imperfect yet interdisciplinary processing of academic content to construct (arguably) less-biased metrics for articles, journals, and scholars, which can complement current diffusionbased metrics that are more or less discipline-specific and inevitably biased by human factors.</p>
<p>Concluding Remarks</p>
<p>The output of AI tools shares features that align with human preferences (Messeri and Crockett, 2024).LLMs produce "standard" responses -which we desire in many contexts -relying on their capability of conducting effective information compression (e.g., Sayood , 2017;Wolff , 2019), an important test for AI systems (Mahoney, 1999) largely considered in the fabrication of LLMs (e.g., Touvron et al., 2023).This capability underlies LLMs' basic text-processing function of literature digest and our expectation of employing them in aiding scientific research.Yet reduction and synthesis trades off elaboration and complexity, and we should not take LLMs' information compression for granted.We need to probe into the limitations of LLMs in this regard and the tradeoffs in their employment, investigating the costs associated with the gains in efficiency.</p>
<p>In this study, we developed a guided workflow assembling known analyses and employing well-adopted metrics as well as human arbiters to evaluate the potential of recruiting LLMs to process academic text input and construct peer reviews in future scientific practice.We assessed four features of the LLM output in conducting four tasks: reliability at content reproduction, scalability at content comparison, discrimination at content scoring, and insightfulness at content reflection.Overall, we recorded a compromised performance of the tested LLM in each aspect: the LLM's ability to summarize and paraphrase academic texts is acceptably reliable yet discouraging; its ability to rank texts through pairwise text comparison is faintly scalable; its ability to grade academic texts is prone to poor discrimination; and its qualitative reflection on a given text is self-consistent but hardly insightful in terms of inspiring meaningful research.These phenomena, consistent across metric-based internal evaluation (linguistic assessment), external evaluation (comparing to the ground truth), and human evaluation, are robust to the variation of LLM prompts in semantics, input richness, data abundance, and instruction specificity.Specifically, we found that: (1) LLM-generated texts can employ a larger vocabulary but its word use is more concentrated than that of ground-truth texts; (2) the quality of the LLM output varies across different input article types; (3) when comparing two texts, the error rate of the LLM-output preference is 22% − 41%; (4) LLM-generated critiques have a smaller linguistic complexity than LLM-generated abstracts; (5) the specificity of the instruction influences the prompt more than the prompt's richness or data abundance, and the prompt is most robust to semantic variations; (6) the LLM-generated text balances abundant and frequent word use, and the LLM's abundant word use hinders the reproduction of the ground truth; (7) LLM-reliant and ground-truth text metrics do not indicate input text quality along the same dimension.</p>
<p>Our results suggest that the LLM's performance downgrades along with a task's increasing need for solid scientific understanding to yield desirable solutions.The LLM can reasonably play the role of oracle, but it does not do as well when serving as a judgmental or knowledgeable arbiter, and it insufficiently qualifies as a collaborator.This implies that, in the future of science, for aiding research development, LLMs' application potential can be acceptable at the level of literature digest, but it has diminishing value at text ranking and text rating, and it is far from endorsable as a full research assistant.LLMs can work well when summarizing academic content and helping researchers navigate the literature, but they do not perform as well when selecting content, evaluating content, or suggesting research directions.From these experiments, upon the acceptable reliability, unwarranted scalability, skewed discrimination, and bounded insightfulness of their output, we found it appropriate not to recommend, rather than to recommend, an unchecked use of LLMs in conducting academic peer reviews.</p>
<p>Research implications</p>
<p>In summary, our study brings the following insights to the audience in IS and broad managerial sciences.</p>
<p>(1) We organize four individual tasks that CS studies widely employ to evaluate LLMs' capabilities into a guided workflow to assess LLMs' performance in a focal application: process academic texts and output the generated abstract/keywords/critiques (content output) for the input article or a preference/score (decision output).Through a series of tasks, we look into a spectrum of LLMs' capabilities that increasingly demand a deep understanding of scientific content.This investigation goes beyond viewing LLMs as tools and looking for ameliorating their engineering, as is the case in CS studies, and points to discussing their behavior and our acknowledgment of them at the human-machine interaction, in IS studies.</p>
<p>(2) Our evaluation helps determine LLMs' potential to be recruited in aiding academic peer review.We exemplify an effective workflow of the four tasks with detailed instructions on the prompts.The tasks are connected to different components of the peer review and can be adopted by different stakeholders, e.g., editors selecting submissions (content comparison), reviewers summarizing content (content reproduction) or rating submissions (content scoring), and authors developing manuscripts (content reflection).At each task, we demonstrate the analyses on the LLM output in detail to inform future evaluations.These analyses are modular and fully transparent to ensure access and replicability.</p>
<p>(3) We use a unique set of text materials to assess the LLM and an abundant set of metrics in the metric-based evaluation of its output.Compared to more specialized content, the diverse content of IS studies with articles' different semantic styles provides a comprehensive testbed for assessing LLMs' text processing.We adopt four metrics each in both the internal evaluation (linguistic assessment) and the external evaluation (comparing to the ground truth) of the LLM-generated texts and compare the measurements at different types of articles (e.g., regular/special issue) at three top journals.These considerations underlie the extensibility of our evaluation.</p>
<p>(4) Our exemplary analyses suggest an integrated piece of evidence against an endorsement of LLMs' textprocessing capabilities.The findings are robust to the variation of LLM prompts, and we thus find it appropriate to avoid an overlooked use of LLMs in aiding academic peer reviews.Compared to other evaluation studies, our discussions heavily emphasize LLMs' limitations, aligning with the conservative view on the development of LLMs and the generative AI landscape, as is primarily adopted by social science researchers standing against the over-optimists and opportunists in engineering fields.</p>
<p>Limitations and future directions</p>
<p>Our study is limited and points to future research directions.Above all, our guided workflow is only one viable tool for assessing LLMs' processing of a particular type of text input, and our evaluation is by no means comprehensive.Alternative tasks, metrics, and analyses can be called for in other evaluation paradigms.</p>
<p>(1) Text input/output length limits.One major limitation of our analysis involves the incomplete input of scientific articles into the LLM due to its input length limit.To include more article content besides the introduction and conclusion sections, we can adopt the latest versions of some LLMs that support longer text input or consider multi-round dialogues to feed in the text by segments.The LLMs' output length is also capped; we currently query LLMs for structured output (keywords/abstracts/critiques), and querying LLMs for unstructured output potentially with a greater output size merits serious exploration.</p>
<p>(2) Drawbacks of evaluations.For internal evaluation, metrics that assess the text's linguistic quality based on words can be outrun by metrics based on the text's semantic meaning, e.g., the "semantic entropy" (Farquhar et al., 2024) developed in particular for evaluating the LLM output, which employs alternative LLMs to evaluate the output of a focal LLM.For external evaluation, text similarity metrics like BLEU and ROUGE may correlate weakly with human judgment: it is possible that two distinct texts for the same context, while both valid in human eyes, do not share any words in common and thus have zero similarity under these metrics (Liu et al., 2016).Overall, metric-based evaluations are rigid.We conducted a human evaluation to compensate for this deficiency, while its design is subject to elaborate improvements.Our recruited arbiter team is also less academically established than ordinary reviewers (e.g., faculty members).</p>
<p>(3) Broad empirical tests.Different LLMs and academic texts from diverse disciplines can be adopted to replicate the analysis.Currently, we use those sections of articles where figures/tables/equations are rare; optical character recognition could allow non-text input with multimodal LLMs (Biswas and Talukdar , 2024).We used the articles from an interdisciplinary field (IS) to include content of different topics and writing styles; the LLM's performance may differ across disciplines with more specialized content.For example, LLMs may perform better in evaluating texts from CS conferences whose open reviews may have been utilized in training LLMs.</p>
<p>(4) Enhancing the LLM output.For first-order results, we focused on online, one-round, and zero-shot human-LLM dialogues with non-pretrained LLMs.LLMs' behavior evolves in offline, multi-round dialogues with few-shot prompting (Brown et al., 2020), and the LLM output may get enhanced through chain-ofthought (Wei et al., 2022), fine-tuning (Hu et al., 2021), contextual calibration (Zhao et al., 2021), text purification (Lucas et al., 2023), or model merging (Akiba et al., 2024), etc., besides the continued upgrade of the LLM itself due to fierce market competition.These techniques and advanced LLMs can be studied in future experiments.Further, the four tasks can be assembled into an integrated peer review where LLMs are asked to provide a structured response (Appendix H); this integrated LLM output can be studied closely.</p>
<p>(5) Common pitfalls of LLMs.Studies have revealed that LLM output is subject to common pitfalls, some of which are inevitable (e.g., Xu et al., 2024), around the challenges in factuality (Augenstein et al., 2024).These can become their major obstacles to satisfactorily completing the tasks.As mentioned, these include hallucination (Ji et al., 2023) (primarily at Task 1), stochasticity (Bender et al., 2021) (primarily at Task 2), tempered objectivity (Denison et al., 2024) (primarily at Task 3), non-understanding (Mitchell , 2023) (primarily at Task 4), and deception (Hagendorff , 2024) (potentially at all tasks), and it is noted that a small factual lapse may drastically poison the scientific literature (Yang et al., 2024) and that in general, using LLMs as judges may engender various types of biases (Zheng et al., 2023).Mapping the pitfalls of LLMs to their employment in different tasks during scientific research assistantship is an important direction.</p>
<p>(6) Reflexivity of LLMs.We can investigate the self-reflection of LLMs (Balmer , 2023;Renze and Guven, 2024) in our workflow: one may ask the LLM to (i) suggest tasks for evaluating its processing of academic texts and its potential in constructing peer reviews; (ii) generate prompts based on an original prompt and use the LLM-generated prompts; and (iii) explain the logic of its quality preference/quality score.These inquiries are essential for studying LLMs' capability of understanding.Further, we can utilize other LLMs to evaluate the output of the focal LLM (similar to Farquhar et al. (2024)), parallel to the human evaluation; however, as we showed that the LLM-led reviews are flawed, this self-judging may encounter a system error.</p>
<p>(7) Related topics around AI-assisted peer review.We focused on discussing the benefits of LLMs for input text processing and scientific peer review and didn't consider their abuse in misconducts around science production such as plagiarism (Quidwai et al., 2023), fake data (Taloni et al., 2023), dummy review (Brainard , 2024), or predatory publishing (Kendall and da Silva, 2024) that are severely detrimental to academic integrity.These problems call for serious discussions to prevent the misuse of LLMs in scientific research.</p>
<p>LLMs and generative AI en gros are not cooking a free lunch for our future.For them to be put to responsible use, their values ought to align with ours (Norhashim and Hahn, 2024).If there are sacrifices we must make to embrace the hot air for the sky ride, science and the integrity of it should be among the last items that we throw out of the balloon basket.Surrendering scientific exploration and conversation to automatic agents is hardly a humain decision; in the pursuit of truth, we can take nothing for granted.</p>
<p>Appendix A: Relevant studies on LLM evaluation</p>
<p>Along with the rapidly growing research attention around LLMs, there has been abundant literature on the evaluation of LLMs that employ these four tasks.We summarize a list of key studies (Table A1) that focus on (i) assessing LLMs' text output, which primarily employs content reproduction (Task 1) and content reflection (Task 4), via metric-based evaluation or human evaluation; or (ii) assessing LLMs' capabilities as judges, which primarily employ content comparison (Task 2) and content scoring (Task 3)), via pair-wise (as opposed to list-wise, set-wise, etc. (Zhuang et al., 2024)) comparisons and/or direct or step-wise scoring that possibly lead to content ranking.The list is nonetheless by no means exhaustive, as ongoing research efforts continue to broaden the ground for evaluating the behavior of LLMs and more elaborate AI tools built around them.Table A1: Relevant studies on LLM evaluation that employ similar tasks.</p>
<p>Content</p>
<p>[Materials 9 -12] -Break: Please take a break for 1 minute.[Pause for 1 minute]</p>
<p>Evaluation #2: LLM-generated article critiques</p>
<p>In the following evaluation, please read the input article sections and the LLM-generated critiques on the article and submit a score from 1 (least) to 5 (most) to indicate the LLM output's logicalness.You will read 8 such article-critique pairs.The time limit is 8 minutes for each evaluation.</p>
<p>[Materials 1 -2] -Break: Please take a break for 1 minute.Afterwords:</p>
<p>Thank you very much for your participation.Your input is important to our study.</p>
<p>Your payments will be processed shortly.Please help keep the contents confidential from a third party.</p>
<p>Your input has been successfully recorded.You can now close this webpage.(28)</p>
<p>Such a condition cannot be satisfied when N is positive, which means that dP (R 2 , TO)/dN &lt; 0 uniformly for all ϵ, same as the R 1 case.</p>
<p>Similar derivations establish that dP (R m , TO)/dN &lt; 0 for all ϵ and m, which can be visualized with simulation results (Figure 7a).This means that for a text of any ground-truth ranking, the probability of obtaining its correct score S Copeland decreases uniformly as N increases, i.e., the content comparison is not scalable, in the sense of when using it to construct the text ranking.</p>
<p>The output S Copeland list (Figure 6b) does not follow the perfect sequence N − 1, N − 3, N − 5, ..., 1 − N .The deviation from this perfect sequence can be used to measure the loss in scalability.We use the subset of Z to compute S Copeland and such a deviation, and show this scalability loss as N increases (Figure 7b).</p>
<p>Results show that as the number of texts N increases, P (R m , TO) decreases for all ϵ and m.Note that P (R • , T O) approaches P (R 1 , T O) when ϵ is small, as the terms with the ϵ multipliers vanish; the difference between P (R 1 , T O) and P (R 2 , T O) can be seen, e.g., at ϵ = 0.5.Notes: Standard errors (HC3-robust) are in parentheses.† p &lt; 0.10, * p &lt; 0.05, * * p &lt; 0.01, * * * p &lt; 0.001.</p>
<p>Table A4: OLS Regression: Arbiters' background characteristics on their delivered HE abs and HE cri . 9https://www.ets.org/toefl/institutions/ibt/compare-scores.html 10 https://www.bcu.ac.uk/international/your-application/english-language-and-english-tests/accepted-qualificationsExcept for rare cases, the correlations from the three measures agree with one another, and the cases of significance concur.Overall, LLM-reliant metrics demonstrate rare correlation or anti-correlation with ground-truth metrics, measured by Pearson's r, Spearman's ρ, or Kendall's τ .</p>
<p>Figure 1 :
1
Figure 1: Research motivations and outline.</p>
<p>Input the introduction and conclusion sections of two scientific articles.Ask the LLM to compare the two texts and output a quality preference.Baseline Prompt."For two scientific articles (in the field of Information Systems) with the following 'Introduction' and 'Conclusion' sections, compare their quality and output which one is better.Article A: 'Introduction:' {introduction}.'Conclusion:' {conclusion}.Article B: 'Introduction:' {introduction}.'Conclusion:' {conclusion}."</p>
<p>Input the introduction and conclusion sections of a scientific article.Ask the LLM to evaluate this text and output a quality score in [1, 10].Baseline Prompt."For a scientific article (in the field of Information Systems) with the following 'Introduction' and 'Conclusion' sections, evaluate the quality of this text on a scale of 1 (worst) to 10 (best) and output the score.'Introduction:' {introduction}.'Conclusion:' {conclusion}."</p>
<p>LLMs have seen rapid development in recent years; leading tech corporations compete in launching and upgrading their LLMs that adopt an open-or closed-source design (seeLin (2024) for a comparison of several major open-/closed-source LLMs).Users evaluate LLMs' performance on the chatbot arena leaderboard (https://chat.lmsys.org/leaderboard)supported by leading research institutions.On this crowdsourced leaderboard, participants are presented with text responses from two anonymous LLMs to each query they input; the dialogue proceeds until one discerns that an LLM yields the superior response, at which point the participant casts a vote.As of Aug 2025, over 3,820,000 effective votes have been collected.</p>
<p>Figure 3 :
3
Figure 3: Content reproduction (Task 1) -Internal evaluation of ground-truth (blue)/LLM-generated (green) abstracts for different article types (RA [empty], RN [slash], SI [cross], JM [dot], MS [star]) under metrics IE H−density/entropy/T T R/F K (a/b/c/d; x-axis: scores).Solid lines: best-fit Gaussian distributions.</p>
<p>Figure 4 :
4
Figure 4: Content reproduction (Task 1) -External evaluation results comparing the ground-truth/LLMgenerated keywords (pink) and abstracts (orange) for different article types (RA [empty], RN [slash], SI [cross], JM [dot], MS [star]) under metrics EE Jac/cos (keywords) (a/b) and EE Jac/cos/BLEU/ROU GE (abstracts) (a/b/c/d; x-axis: scores).Solid lines: best-fit Gaussian distributions.</p>
<p>Figure 5 :
5
Figure 5: Content reproduction (Task 1) -Human evaluation results of professional arbiters grading LLMgenerated abstracts.(a) Distribution and statistics of arbiters' mean scores given to the assigned abstracts.(b) Distribution and statistics of average scores HE abs for LLM-generated abstracts.Solid lines: best-fit Gaussian distributions.</p>
<p>Figure 6 :
6
Figure 6: Content comparison (Task 2) -Demonstrating the RA case.(a) Preference matrix Z averaging two pairwise comparisons with either text in the front of the prompt.Entry −1 (black) or 1 (white) indicates the row text beating or beaten by the column text twice in the comparison; 0 (gray) indicates a tie.(b) Copeland scores (summing the row entries of Z).Dotted line: perfect scores.Row indices in (a) correspond to the text indices in (b).We sum the row entries of Z to obtain the Copeland score for the row article; the articles' ranking is shown in Figure 6b.The obtained Copeland scores deviate from the (unknown) ground-truth scores: if there is no error in comparison outcomes, articles' Copeland scores should form the perfect sequence N −1, N −3, N −5, ..., −(N − 3), −(N − 1).The imperfect Copeland scores result from errors in the LLM-output preferences.We analyze this error as follows.Suppose the ground-truth outcome of comparing text i and j is y ij .The LLM-output preference ŷij contains error with probability ϵ:</p>
<p>Figure 7 :
7
Figure 7: Content comparison (Task 2).(a) Probabilities of obtaining true S Copeland from content comparisons (six ϵ levels).As the number of texts N increases, such probabilities uniformly decrease for all ϵ and the text of any ground-truth ranking (see Appendix D for P (R • , TO)).(b) Quantifying the loss of scalability with the deviation of S Copeland (denoted as ∆ S ) from the perfect sequence.Output error ϵ is estimated.</p>
<p>Figure 8 :
8
Figure 8: Content Scoring (Task 3) -Distribution of the LLM-output scores ŵi at different article types and for the whole set of articles (inset: statistics).Solid lines: best-fit Gaussian distributions.</p>
<p>w min ) ŵave − (10 − w ave ) ŵmin w ave − w min ; H l = (w ave − 1) ŵmin − (w min − 1) ŵave w ave − w min ; ⇒ ∆w ≥ 0.1 w ave − w min ŵave − ŵmin ,</p>
<p>Figure 9 :
9
Figure 9: Content Scoring (Task 3) -Theoretical results.(a) LLM-output score range [H l , H h ] as a function of the ground-truth score minimum w min and average w ave .(b) Condition for discriminating ground-truth scores, min(∆w).Measured ŵave = 8.3, ŵmin = 6.0.</p>
<p>Figure 10 :
10
Figure 10: Content reflection (Task 4) -Statistics of critique subjects.Grouped top-25 critique subjects.</p>
<p>Figure 11 :
11
Figure 11: Content reflection (Task 4) -Internal evaluation of the input text (blue)/LLM-generated critiques (green) for different article types (RA [empty], RN [slash], SI [cross], JM [dot], MS [star]) under metrics IE H−density/entropy/T T R/F K (a/b/c/d; x-axis: scores).Solid lines: best-fit Gaussian distributions.</p>
<p>Figure 12 :
12
Figure 12: Content reflection (Task 4) -External evaluation comparing the merged critiques (red) or the three-piece critiques (orange) to the input text for different article types (RA [empty], RN [slash], SI [cross], JM [dot], MS [star]) under metrics EE Jac/cos/BLEU/ROU GE (a/b/c/d; x-axis: scores).Solid lines: best-fit Gaussian distributions.</p>
<p>Figure 13 :
13
Figure 13: Content reflection (Task 4) -Human evaluation results.(a) Distribution and statistics of average scores from arbiters.(b) Distribution and statistics of average scores HE cri for LLM-generated critiques.Solid lines: best-fit Gaussian distributions.</p>
<p>Figure 14 :
14
Figure 14: Robustness checks.Percentage deviations (dots and error bars: means and variances) of the measurements in baseline experiments (Experiments •-0): IE • /EE • for Experiments 1-• (two measurements) and 2-• (eight measurements); ∆ S for Experiments 3-•; ŵave for Experiments 4-•.Consider four aspects of the input prompt: semantic robustness of the prompt (blue), richness of the prompt (red), abundance of the data prompt (yellow), and specificity of the instruction prompt (purple).Dash lines: 5% deviations.</p>
<p>Figure 15 :
15
Figure 15: Correlation (Pearson's r) between LLM-reliant (blue) and ground-truth (orange) text metrics.*/ * * / * * * : p &lt; 0.05/0.01/0.001.</p>
<p>[Pause for 1 minute] [Materials 3 -4] -Break: Please take a break for 1 minute.[Pause for 1 minute] [Materials 5 -6] -Break: Please take a break for 1 minute.[Pause for 1 minute] [Materials 7 -8]</p>
<p>⇒N</p>
<p>and, for example,P (R N −1 , TO) = P (R 2 , TO) = (1 − ϵ) 2(N −1) + (1 − ϵ) 2(N −3) C 1 1 C 1 N −2 ϵ 2×2 , P (R N , TO) = P (R 1 , TO) = (1 − ϵ) 2(N −1) .(25)To investigate the scalability of this text ranking through content comparison, we analyze the change of the probability of obtaining the correct score S Copeland (R • ), P (R • , TO), as N goes up.For the #1 text R 1 , dP (R 1 , TO) of ϵ, the probability of obtaining the correct score for text R 1 always decreases as N goes up.For the #2 text R 2 , dP (R 2 , TO) dN = d(1 − ϵ) 2(N −1) + (1 − ϵ) 2(N −3) C 1 1 C 1 N −2 ϵ 2×2 dN = (1 − ϵ) 2(N −3) {ϵ 4 + 2ln(1 − ϵ)[(1 − ϵ) 4 + (N − 2)ϵ 4 ]}.(27) Then dP (R 2 , TO) dN &gt; 0 ⇒ϵ 4 + 2ln(1 − ϵ)[(1 − ϵ) 4 + (N − 2)ϵ 4 ] &gt; 0</p>
<p>Readability (via the Flesch-Kincaid (FK) score).The FK score indicates the level of difficulty in text understanding, widely used in educational settings, etc.The score is based on the number of syllables, words, and sentences in the text, with a high score denoting great readability.The metric IE F K is given by
number of unique words (types) total number of words (tokens).(3)IE F K = 206.835 − 1.015  *  (number of words number of sentences) − 84.6  *  (number of syllables number of words).</p>
<p>erence text, generated text)
words∈ref erence text Count(words),EE ROU GE L −precision =LCS(ref erence text, generated text) words∈generated text Count(words)</p>
<p>Table 3 :
3
We collect articles published in Information Systems Research (ISR) from Dec. 2022 to Mar. 2024, articles published in Journal of Management Information Systems (JMIS) from Apr. 2022 to Jun. 2024, and articles published in the IS department of Management Sciences (MS) in 2022 and 2023.The dataset consists of 246 articles, including 84 regular articles (RA), 19 research notes (RN), and 13 special issue articles (SI) from ISR, 81 articles (JM) from JMIS, and 49 articles (MS) from MS (Table3).We compile the keywords, abstract, main text, and metadata (for ISR/MS, online publication date, received/accepted date, and download count 5 ; for JMIS, online publication date, view count, and CrossRef citation count 6 ) of each article.An article's acceptance time is from the received date to the accepted date; the download, view, and citation counts are normalized by the elapsed time (from the online publication date to the current date).
4 Materials4.1 Academic TextsArticle TypeRARNSIJMMSNo. Articles8419138149Ave. No. Keywords5.96.25.47.35.0Ave. Len. Abstract (words)268274247221247Ave. Len. Main Text (words) 12486 9885 11911 11776 11284Ave. Acceptance Time (days) 835.5 824.7 628.6-754Ave. Norm. Download (times)3.43.01.1-1.38Ave. Norm. View (times)---1351.7-Ave. Norm. Citation (times)---0.9-
Information on article data.No.: number of.Ave.: average.Len.: length of.Norm.: normalized.</p>
<p>Table 5 :
5
Content reproduction (Task 1) -Statistics of word use in ground-truth/LLM-generated (LLM-gen.)keywords and abstracts.No.: Number.</p>
<ul>
<li>: words shared in the four lists.</li>
</ul>
<p>Table 6 :
6(a) Paired t-test between the ground-truth and LLM-generated abstracts (internal evaluation)CountIE H−densityIE entropyTypeground-truthLLM-generatedground-truthLLM-generatedMeanStdMeanStdMeanStdMeanStdRA847.374 1.368 6.616<strong><em> 0.898 6.7160.2536.272</em></strong>0.422RN197.592 1.3767.1340.753 6.7430.3426.295<strong><em>0.142SI136.858 0.9057.0681.128 6.6260.3336.359</em>*0.109JM817.368 1.415 6.783</strong><em> 0.898 6.5990.2236.310</em><strong>0.134MS497.239 1.6857.4520.927 6.6690.3196.6090.153All2467.334 1.430 6.902</strong><em> 0.953 6.6650.2736.358</em><strong>0.299CountIE T T RIE F KTypeground-truthLLM-generatedground-truthLLM-generatedMeanStdMeanStdMeanStdMeanStdRA840.608 0.066 0.673</strong><em> 0.036 16.709 11.5249.980</em><strong>9.541RN190.604 0.054 0.663</strong><em> 0.045 17.445 10.091 10.288</em><strong> 9.968SI130.617 0.053 0.681</strong><em> 0.036 21.412 9.27415.353</em><em>5.711JM810.646 0.059 0.689</em><strong> 0.044 14.733 12.3065.567</strong><em>9.541MS490.629 0.064 0.699</em><strong> 0.048 24.855 11.7698.035</strong><em> 10.459All2460.625 0.064 0.684</em><strong> 0.045 17.987 12.1418.447</strong><em> 10.512(b) Comparing ground-truth/LLM-generated keywords and abstracts (external evaluation)CountEE JacEE cosTypekeywordsabstractkeywordsabstractMeanStdMeanStdMeanStdMeanStdRA840.089 0.054 0.197 0.030 0.357 0.134 0.605 0.082RN190.099 0.053 0.216 0.033 0.400 0.134 0.634 0.060SI130.094 0.051 0.205 0.026 0.364 0.125 0.590 0.061JM810.019 0.029 0.220 0.036 0.381 0.127 0.600 0.076MS490.011 0.021 0.221 0.038 0.332 0.121 0.595 0.071All2460.052 0.056 0.211 0.035 0.372 0.127 0.603 0.075CountEE BLEUEE ROU GETypekeywordsabstractkeywordsabstractMeanStdMeanStdMeanStdMeanStdRA84--0.043 0.025--0.296 0.042RN19--0.054 0.039--0.318 0.040SI13--0.046 0.023--0.309 0.039JM81--0.060 0.040--0.324 0.048MS49--0.051 0.030--0.329 0.049All246--0.051 0.033--0.314 0.047(c) One-Way ANOVA results on different article types (internal/external/human evaluation)Measure (IE • )ground-truth abstractLLM-generated abstractsum sqFp-valuesum sqFp-valueIE H−density4.8790.5930.66824.2277.368 &lt;0.001</em><strong>IE entropy0.7082.4260.048<em>3.96813.347 &lt;0.001</em></strong>IE T T R0.0694.4500.001<strong><em>0.0263.2850.012</em>IE F K3464.762 6.393 &lt;0.001</strong><em> 1561.978 3.6890.006</em><em>Measure (EE • )keywordsabstractsum sqFp-valuesum sqFp-valueEE Jac0.35252.01 &lt;0.001</em><strong>0.0306.469 &lt;0.001</strong><em>EE cos0.2063.2910.012</em>0.0241.0690.372EE BLEU---0.0122.9020.023<em>EE ROU GE---0.0475.776 &lt;0.001</em>*<em>Measure (HE abs )abstractsum sqFp-valueHE abs1.9202.5800.054
Content reproduction (Task 1) -Statistics and inference analyses.</em> / * * / * * * : p &lt; 0.05/0.01/0.001.</p>
<p>Table 7 :
7
Content reflection (Task 4) -Inference analyses.
)
* / * * / * * * : p &lt; 0.05/0.01/0.001.</p>
<p>Table 8 :
8
Comparison of human evaluation results at Tasks 1 and 4.
StatisticHE abs HE cri DifferenceMean3.343.080.26<strong>Median3.333.250.08Mode3.333.50-0.17Cohen's d (Effect Size)--0.47Variance (Levene's Test)--</strong>Distribution Shape (KS Test)--**
* * : p &lt; 0.01.</p>
<p>Table A4
A4
shows the results.Overall, arbiters' background characteristics do not demonstrate statistically significant associations with their scoring.
HE absHE criVariableCoefficient (Std. Error) Coefficient(Std. Error)Intercept3.278  *  *  <em>(0.545)4.424  *  *  </em>(0.952)Area of Study (Reference: IS &amp; CS)Economics−0.026(0.256)−0.037(0.262)Finance &amp; Accounting−0.104(0.170)−0.216(0.182)Management &amp; Marketing−0.045(0.154)0.141(0.170)Operations Research &amp; SCM−0.127(0.175)−0.172(0.323)Other Area−0.151(0.251)−0.345(0.227)Program Seniority (Status)−0.036(0.049)−0.036(0.050)Log(Time Spent)0.018(0.097)−0.133(0.162)Familiarity with LLMs0.026(0.058)0.050(0.074)English Proficiency (TOEFL equiv.)0.002(0.003)−0.006  *(0.003)R-squared0.0350.129Adjusted R-squared-0.0780.027F-statistic0.3501.423Prob (F-statistic)0.9550.193Observations8787</p>
<p>Table A5 :
A5
Correlation between LLM-reliant and ground-truth text metrics (for ISR and MS articles).Spearman's ρ Kendall's τ Pearson's r Spearman's ρ Kendall's τ
0.12640.12810.08620.14480.28220.1884EE key cos IE abs H−density0.0702 -0.1694<em>0.1152 -0.1837</em>0.0802 -0.1193<em>0.1751</em> -0.05380.2780<em> -0.04710.1934</em> -0.0307IE abs entropy-0.0498-0.0841-0.0573-0.1930<em>-0.1542</em>-0.1064<em>IE abs T T R IE abs F K EE abs Jac EE abs cos EE abs BLEU EE abs ROU GE−L-0.0932 0.1872</em> 0.1271 0.1383 0.1426 0.1503-0.0877 0.1792<em> 0.1386 0.1422 0.1162 0.1478-0.0629 0.1240</em> 0.0916 0.0966 0.0814 0.09380.0961 0.0580 -0.1476 -0.0551 -0.0663 -0.1829-0.1089 0.0149 -0.0178 0.1636 -0.0195 -0.0552<em>-0.0748 0.0128 -0.0159 0.1104 -0.0103 -0.0379</em>S Copeland0.10630.056600.03610.06190.08280.0542ŵ-0.0296-0.0444-0.02740.2357<strong>0.1731</strong>0.1243<strong>IE cri H−density IE cri entropy IE cri T T R IE cri F K EE cri Jac EE cri cos EE cri BLEU EE cri ROU GE−L HE abs-0.0295 -0.1347 -0.0773 0.1217 -0.0929 0.0380 -0.2029</strong> -0.0761 0.1357-0.0429 -0.0836 -0.0826 0.1287 -0.1339 0.0154 -0.1624<strong> -0.1172 0.1059-0.0279 -0.0591 -0.0552 0.0870 -0.080 0.0120 -0.1033</strong> -0.0747 0.0747-0.1292 -0.1764<em> 0.0542 0.0848 -0.0570 0.0060 -0.0419 -0.0697 -0.0669-0.1215 -0.2519</em> -0.1173 0.1473 -0.0220 0.0242 -0.1604 -0.0408 0.0183-0.0823 -0.1728<em> -0.0780 0.0992 -0.0133 0.0169 -0.1073 -0.0231 0.0154HE cri-0.04420.00880.0069-0.0764-0.1071-0.0732view countcitation countPearson's r EE key 0.1071 Jac EE key 0.0213 cos IE abs H−density -0.01150.0949 0.0480 -0.16370.0677 0.0408 -0.10730.0674 0.1836 -0.0777-0.0127 0.2175 -0.1483-0.0089 0.1684 -0.1073IE abs entropy-0.1922-0.1634-0.1200-0.0601-0.1852-0.1494IE abs T T R IE abs F K EE abs Jac EE abs cos EE abs BLEU EE abs ROU GE−L-0.1502 0.0049 0.0788 0.0733 0.0290 0.0448-0.1244 -0.0645 0.0963 0.0782 0.0513 0.0279-0.0896 -0.0472 0.0756 0.0548 0.0358 0.0364-0.0857 0.0333 -0.1969 0.0869 -0.0724 -0.2031-0.2141 0.0770 -0.1584 0.1970 -0.0464 -0.1065-0.1717 0.0553 -0.1246 0.1477 -0.0363 -0.0792S Copeland-0.1095-0.0706-0.0456-0.1318-0.1531-0.1234ŵ0.00780.01830.0151-0.1412-0.1914-0.1587IE cri H−density IE cri entropy IE cri T T R IE cri F K EE cri Jac EE cri cos EE cri BLEU EE cri ROU GE−L HE abs-0.1498 -0.1516 -0.2559</em> 0.2997<strong> -0.1650 -0.0712 0.0729 -0.1885 -0.2979<em>-0.2141 -0.1340 0.0096</em> 0.1557</strong> -0.1916 -0.0315 0.0192 -0.2319 -0.2477<strong>-0.1485 -0.0965 0.0054* 0.1073</strong> -0.1149 -0.0250 0.0092 -0.1491 -0.1793<em>-0.0743 0.0011 -0.1199 0.1203 -0.2206</em> -0.0594 -0.0299 -0.2920<strong> -0.1733-0.1207 -0.0319 0.0257 0.1752 -0.1857* 0.0402 -0.0856 -0.2984</strong> -0.2544-0.0946 -0.0289 0.0165 0.1378 -0.1411<em> 0.0355 -0.0652 -0.2319</em> -0.1973HE cri-0.0113-0.0467-0.0372-0.0921-0.1432-0.1161</p>
<p>Table A6 :
A6
Correlation between LLM-reliant and ground-truth text metrics (for JMIS articles).</p>
<p>https://www.wiley.com/en-cn/ai-study
https://www.semanticscholar.org/product/semantic-reader
https://ai.google.dev/gemini-api/docs/models/gemini
https://pubsonline.informs.org/
https://www.tandfonline.com/journals/mmis20
https://cloud.google.com/vertex-ai/generative-ai/docs/data-governance
One token is approximately four English characters; 100 tokens correspond to 60-80 English words.
Appendix B: Information on human arbitersAppendix C: Online interface for human evaluationForewords and Instructions:Welcome to the evaluation of LLM (large language model)-generated texts.Please complete the assignment through this web page.Your input will be utilized for studying LLMs' performance in processing texts.With sufficient domain knowledge in management studies, you will serve as an expert arbiter and grade the LLM-generated outputs (abstracts/critiques) for an input text which is an article published in a top journal in the field of information systems in management sciences (Information Systems Research/ISR or Journal of Management Information Systems/JMIS; both known for exhibiting interdisciplinary research across social, physical, and managerial sciences and supporting diverse scientific approaches in studying information technologies).The LLM used for generating the output is Google's Gemini Pro 1.0.You will conduct two types of evaluations.In each instance of evaluation, please submit an integer from 1 (least) to 5 (most) to indicate the LLM output's quality.Non-integer or out-of-range scores will not pass.In Evaluation #1, please compare an LLM-generated abstract to the ground-truth article abstract and submit a score to indicate the LLM output's reliability.You will read 12 such abstract pairs.The time limit is 3 minutes for each evaluation.In Evaluation #2, please read the input article sections and the LLM-generated critiques on the article and submit a score to indicate the LLM output's logicalness.You will read 8 such article-critique pairs.The time limit is 8 minutes for each evaluation.There will be multiple breaks between evaluation sessions.The elapsed time will be displayed on the web page.You will devote 120 minutes at maximum to the assignment.Please make careful evaluations.Thanks in advance for your participation.[Pause for 2 minutes; then display "Click to move on to the next page"] Please indicate your familiarity with LLM-generated content with a score from 1 (least) to 5 (most):Your input will not be publicized or used for commercial purposes.[Pause for 1 minute; then display "Click to start the evaluation"]Evaluation #1: LLM-generated article abstractsIn the following evaluation, please compare an LLM-generated abstract to the ground-truth article abstract and submit a score from 1 (least) to 5 (most) to indicate the LLM output's reliability.You will read 12 such abstract pairs.The time limit is 3 minutes for each evaluation.[Materials 1 -4] -Break: Please take a break for 1 minute.[Pause for 1 minute][Materials 5 -8] -Break: Please take a break for 1 minute.[Pause for 1 minute]Appendix D: Probabilities of correct Copeland scoresSuppose the ground-truth outcome of comparing text i and text j is y ij .The LLM-output preference ŷij contains error with probability ϵ:Averaging the outcomes ŷij and ŷji from the two instances of comparison between i and j, we obtainwhich constitutes the matrix Z = {ẑ ij }.Suppose the ground truth is y ij = −1.There are four cases for the outcome at ẑij : (i) when ŷij is false and ŷji is true, ẑij = 0; (ii) when ŷij is true and ŷji is true, ẑij = −1;(iii) when ŷij is false and ŷji is false, ẑij = 1; (iv) when ŷij is true and ŷji is false, ẑij = 0. Cases (i) and (iv) yield the same outcome; the probabilities for different values of ẑij areThe Copeland ranking is based on the scores that sum on the matrix Z:where N is the number of texts in the ensemble.We analyze the probability of having the correct score S Copeland (i).First consider the text having the ground-truth ranking #1.This text R 1 wins all comparisons: ŷR1j = 1 for any j ̸ = R 1 , and S Copeland (R 1 ) = N − 1.To realize this score S Copeland (R 1 ), all elements in the sum need to be true, which has a probabilityNext consider the text having the ground-truth ranking #2.This text R 2 wins all comparisons except the comparison with text R 1 : ŷR2R1 = −1, and ŷR2j = 1 for any j ̸ = R 1 , j ̸ = R 2 , and S Copeland (R 1 ) = N − 3.To realize this score S Copeland (R 2 ), either (i) all elements in the sum are true, or (ii) ẑR2R1 and another ẑR2j for an arbitrary j are false, and the rest N − 3 elements are true.This has a probabilitySimilarly, for S Copeland (R 3 ) to be true, there are three possible cases, and their combined probability isFor S Copeland (R m ) to be true (text ranked #m), there are m possible cases, and the combined probability isThis ends halfway at the rank: N − m ≥ m − 1 ⇒ m ≤ (N + 1)/2.For the second half of the rank, m &gt; (N + 1)/2, the probabilities mirror the first half, with the substitution m − 1 → N − m.ThenAppendix E: Grouped top-25 LLM-generated critique subjectsAppendix F: Details of human evaluation resultsWe check whether human arbiters' background characteristics influence their scoring.We employ ordinary least squares (OLS) regression, incorporating HC3-robust standard errors to ensure valid inference under heteroskedasticity, i.e., non-constant error variance across covariates.We consider four independent variables: arbiters' academic status, area of study, familiarity with LLMs, and English language proficiency.We check if these characteristics can predict arbiters' delivered HE abs and HE cri .Separate OLS models are estimated for HE abs and HE cri .We further control the average time of the arbiters' evaluation.The background characteristics are pre-processed as follows:• Academic status.We consider the number of years (0-5) each arbiter has been enrolled in their Ph.D. program.We drop the one undergraduate and the one not-reported.• Area of study.• Familiarity with LLMs.No pre-processing.• English language proficiency.We use the TOEFL score as the indicator for English language proficiency.IELTS scores are converted to their TOEFL score equivalents using the official ETS concordance Appendix G: LLM-reliant/ground-truth text metrics correlationsWe calculate Pearson's r (Figure15in the main text) between LLM-reliant and ground-truth text quality metrics and Spearman's ρ and Kendall's τ between their ranks (TablesA5, A6).LLM-reliant text quality metrics: at Task 1, the IE We can aggregate the four tasks in a holistic workflow and construct an integrated prompt for the peer review of academic manuscripts (FigureA1).The prompt consists of four parts: role assumption (instruction prompt specifying the LLM role of a professional reviewer), text input (data prompt), task specification (instruction prompt specifying peer review deliverables), and response collection (instruction prompt soliciting the peer review output).The four tasks (in red) constitute the body of the instruction prompt; elements of the investigated robustness checks (in blue) are embodied in text input and task specification.(2) LLM-generated text summary indicate a good content reproduction?(3) LLM-output text grade indicate a good content scoring?(4) LLM-generated text critique indicate a good content reflection?
CyberGate: A design framework and system for text analysis of computermediated communication. A Abbasi, H Chen, MIS Quarterly. 2008</p>
<p>. A Abbasi, J Parsons, G Pant, O R L Sheng, S Sarker, Pathways for Design Research on Artificial Intelligence. 2024Information Systems Research</p>
<p>Artificial intelligence and scientific discovery: A model of prioritized search. A Agrawal, J Mchale, A Oettl, Research Policy. 5351049892024</p>
<p>T Akiba, M Shing, Y Tang, Q Sun, D Ha, arXiv:2403.13187Evolutionary optimization of model merging recipes. 2024</p>
<p>Measure the deeds that make academic life fulfilling. K Allen, Nature. 6388612025</p>
<p>Science communication with generative AI. A Alvarez, A Caliskan, M J Crockett, S S Ho, L Messeri, J West, Nature Human Behaviour. 2024</p>
<p>Easing the burden of peer review. M Andersen, Nature Astronomy. 472020</p>
<p>Transforming science labs into automated factories of discovery. A Angelopoulos, J F Cahoon, R Altrovitz, Science Robotics. 9592024</p>
<p>bibliometrix: An R-tool for comprehensive science mapping analysis. M Aria, C Cuccurullo, Journal of Informetrics. 1142017</p>
<p>N Arora, I Chakraborty, Y Nishimura, AI-Human Hybrids for Marketing Research: Leveraging LLMs as Collaborators. 202400222429241276529</p>
<p>Factuality challenges in the era of large language models and opportunities for fact-checking. I Augenstein, T Baldwin, M Cha, T Chakraborty, G L Ciampaglia, D Corney, . . Zagni, G , Nature Machine Intelligence. 2024</p>
<p>A sociological conversation with ChatGPT about AI ethics, affect and reflexivity. A Balmer, Sociology. 5752023</p>
<p>Research commentary: Rethinking "diversity" in information systems research. I Benbasat, R Weber, Information Systems Research. 741996</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and Transparency2021, March</p>
<p>Exploring generative AI's impact on research: Perspectives from senior scholars in management information systems. H K Bhargava, S Brown, A Ghose, A Gupta, D Leidner, D J Wu, ACM Transactions on Management Information Systems. 1622025</p>
<p>How should the advancement of large language models affect the practice of science. M Binz, S Alaniz, A Roskies, B Aczel, C T Bergstrom, C Allen, . . Schulz, E , Proceedings of the National Academy of Sciences. 1225e24012271212025</p>
<p>Robustness of Structured Data Extraction from In-Plane Rotated Documents Using Multi-Modal Large Language Models (LLM). A Biswas, W Talukdar, Journal of Artificial Intelligence Research. 2024</p>
<p>Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. L Bornmann, R Haunschild, R Mutz, Humanities and Social Sciences Communications. 812021</p>
<p>Can AI help scientists surf a paper flood?. J Brainard, Science. 38266732023</p>
<p>Duplicated phrases in peer review draw scrutiny. J Brainard, Science. 671411502024</p>
<p>S Brown, Community Building through Virtuous Reviewing. 202448</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, . . Amodei, D , Advances in Neural Information Processing Systems. 202033</p>
<p>AI writes summaries of preprints in bioRxiv trial. E Callaway, Nature. 2023</p>
<p>Management Science Special Issue on the Human-Algorithm Connection. F Caro, J E Colliard, E Katok, A Ockenfels, N Stier-Moses, C Tucker, D J Wu, Management Science. 6812022</p>
<p>A speech-act-based negotiation protocol: design, implementation, and test use. M K Chang, C C Woo, ACM Transactions on Information Systems (TOIS). 1241994</p>
<p>AI-assisted peer review. A Checco, L Bracciale, P Loreti, S Pinfield, G Bianchi, Humanities and Social Sciences Communications. 812021</p>
<p>Large language model in creative work: The role of collaboration modality and user expertise. Z Chen, J Chan, Management Science. 2024</p>
<p>A reasonable social welfare function. A H Copeland, University of Michigan Seminar on Applications of Mathematics to the Social Sciences. 1951</p>
<p>Self-driving laboratories coming of age. G Crabtree, Joule. 4122020</p>
<p>Autonomous mobile robots for exploratory synthetic chemistry. T Dai, S Vijayakrishnan, F T Szczypiński, J F Ayme, E Simaei, T Fellowes, . . Cooper, A I , Nature. 2024</p>
<p>M D'arcy, T Hope, L Birnbaum, D Downey, arXiv:2401.04259Marg: Multi-agent review generation for scientific papers. 2024</p>
<p>C E Denison, M S Macdiarmid, F Barez, D K Duvenaud, S Kravec, S Marks, . . Hubinger, E , arXiv:2406.10162Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models. 2024</p>
<p>A contextual approach to scientific understanding. H W De Regt, D Dieks, Synthese. 1442005</p>
<p>Human-in-the-Loop AI Reviewing: Feasibility, Opportunities, and Risks. I Drori, D Te'eni, Journal of the Association for Information Systems. 2512024</p>
<p>J Du, Y Wang, W Zhao, Z Deng, S Liu, R Lou, . . Yin, W , arXiv:2406.16253Llms assist nlp researchers: Critique paper (meta-) reviewing. 2024</p>
<p>T Eloundou, S Manning, P Mishkin, D Rock, GPTs are GPTs: Labor market impact potential of LLMs. 2024384</p>
<p>How AI technology can tame the scientific literature. A Extance, Nature. 56177222018</p>
<p>Bias of AI-generated content: an examination of news produced by large language models. X Fang, S Che, M Mao, H Zhang, M Zhao, X Zhao, Scientific Reports. 14152242024</p>
<p>Detecting hallucinations in large language models using semantic entropy. S Farquhar, J Kossen, L Kuhn, Y Gal, Nature. 63080172024</p>
<p>Mathematical capabilities of chatgpt. S Frieder, L Pinchetti, R R Griffiths, T Salvatori, T Lukasiewicz, P Petersen, J Berner, Advances in Neural Information Processing Systems. 202436</p>
<p>Fast lane to slow science. U Frith, Trends in Cognitive Sciences. 2412020</p>
<p>Y Gao, D Lee, G Burtch, S Fazelpour, arXiv:2410.19599Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina. 2024</p>
<p>Not all "open source" AI models are actually open: here's a ranking. E Gibney, Nature. 2024</p>
<p>Has your paper been used to train an AI model? Almost certainly. E Gibney, Nature. 6322024</p>
<p>Amplify scientific discovery with artificial intelligence. Y Gil, M Greaves, J Hendler, H Hirsh, Science. 34662062014</p>
<p>Probabilistic forecasts, calibration and sharpness. T Gneiting, F Balabdaoui, A E Raftery, Journal of the Royal Statistical Society Series B: Statistical Methodology. 6922007</p>
<p>Positioning and presenting design science research for maximum impact. S Gregor, A R Hevner, MIS Quarterly. 2013</p>
<p>Three ways ChatGPT helps me in my academic writing. D Gruda, Nature. 2024</p>
<p>Deception abilities emerged in large language models. T Hagendorff, Proceedings of the National Academy of Sciences. 12124e23179671212024</p>
<p>Spoken and written language. M A K Halliday, 1985Victoria Deakin University Press</p>
<p>AI-driven research in pure mathematics and theoretical physics. Y H He, Nature Reviews Physics. 2024</p>
<p>Design science in information systems research. A R Hevner, S T March, J Park, S Ram, MIS Quarterly. 2004</p>
<p>D Hicks, P Wouters, L Waltman, S De Rijcke, I Rafols, Bibliometrics: the Leiden Manifesto for research metrics. 2015520</p>
<p>Does human-AI collaboration lead to more creative art? Aesthetic evaluation of human-made and AI-generated haiku poetry. J Hitsuwari, Y Ueda, W Yun, M Nomura, Computers in Human Behavior. 1391075022023</p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, . . Chen, W , arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021</p>
<p>J Huynh, C Jiao, P Gupta, S Mehri, P Bajaj, V Chaudhary, M Eskenazi, arXiv:2301.12004Understanding the effectiveness of very large language models on dialog evaluation. 2023</p>
<p>Discovering physical concepts with neural networks. R Iten, T Metger, H Wilming, L Del Rio, R Renner, Physical Review Letters. 1241105082020</p>
<p>14 examples of how LLMs can transform materials science and chemistry: a reflection on a large language model hackathon. K M Jablonka, Q Ai, A Al-Feghali, S Badhwar, J D Bocarsly, A M Bran, . . Blaiszik, B , Digital Discovery. 252023</p>
<p>Editorial for the special section on humans, algorithms, and augmented intelligence: The future of work, organizations, and society. H Jain, B Padmanabhan, P A Pavlou, T S Raghu, Information Systems Research. 3232021</p>
<p>Active ranking using pairwise comparisons. K G Jamieson, R Nowak, Advances in NIPS. 201124</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, . . Fung, P , ACM Computing Surveys. 55122023</p>
<p>J Jia, A Komma, T Leffel, X Peng, A Nagesh, T Soliman, . . Kumar, A , Leveraging LLMs for dialogue quality measurement. 2024</p>
<p>Y Jin, Q Zhao, Y Wang, H Chen, K Zhu, Y Xiao, J Wang, arXiv:2406.12708AgentReview: Exploring Peer Review Dynamics with LLM Agents. 2024</p>
<p>Leakage and the reproducibility crisis in machine-learning-based science. S Kapoor, A Narayanan, Patterns. 942023</p>
<p>Risks of abuse of large language models, like ChatGPT, in scientific publishing: Authorship, predatory publishing, and paper mills. G Kendall, J A T Da Silva, Learned Publishing. 3712024</p>
<p>Distributed peer review enhanced with natural language processing and machine learning. W E Kerzendorf, F Patat, D Bordelon, G Van De Ven, T A Pritchard, Nature Astronomy. 472020</p>
<p>Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. J P Kincaid, R P FishburneJr, R L Rogers, B S Chissom, 1975</p>
<p>Functional genomic hypothesis generation and experimentation by a robot scientist. R D King, K E Whelan, F M Jones, P G Reiser, C H Bryant, S H Muggleton, . . Oliver, S G , Nature. 42769712004</p>
<p>The automation of science. R D King, J Rowland, S G Oliver, M Young, W Aubrey, E Byrne, . . Clare, A , Science. 32459232009</p>
<p>Nobel Turing Challenge: creating the engine for scientific discovery. H Kitano, NPJ Systems Biology and Applications. 71292021</p>
<p>Artificial intelligence to support publishing and peer review: A summary and review. K Kousha, M Thelwall, 2024Learned Publishing37</p>
<p>The global burden of journal peer review in the biomedical literature: Strong imbalance in the collective enterprise. M Kovanis, R Porcher, P Ravaud, L Trinquart, PloS One. 1111e01663872016</p>
<p>On scientific understanding with artificial intelligence. M Krenn, R Pollice, S Y Guo, M Aldeghi, A Cervera-Lierta, P Friederich, . . Aspuru-Guzik, A , Nature Reviews Physics. 4122022</p>
<p>I Kuznetsov, O M Afzal, K Dercksen, N Dycke, A Goldberg, T Hope, . . Gurevych, I , arXiv:2405.06563What Can Natural Language Processing Do for Peer Review. 2024</p>
<p>Measuring lexical richness, The Routledge Handbook of Vocabulary Studies. K Kyle, 2019</p>
<p>Scientific literature: Information overload. E Landhuis, Nature. 53576122016</p>
<p>A R Lahitani, A E Permanasari, N A Setiawan, Cosine similarity to determine similarity measure: Study case in online essay assessment, International Conference on Cyber and IT Service Management. 2016</p>
<p>Bias in peer review. C J Lee, C R Sugimoto, G Zhang, B Cronin, Journal of the American Society for Information Science and Technology. 6412013</p>
<p>Gender quotas, competitions, and peer review: Experimental evidence on the backlash against women. A Leibbrandt, L C Wang, C Foo, Management Science. 6482018</p>
<p>To Fully Appreciate AI Expectations, Look to the Trillions Being Invested. J Letzing, 2024World Economic Forum</p>
<p>The value, benefits, and concerns of generative ai-powered assistance in writing. Z Li, C Liang, J Peng, M Yin, 2024 CHI Conference on Human Factors in Computing Systems. 2024. May</p>
<p>Z Li, C Liang, J Peng, M Yin, arXiv:2410.04545How Does the Disclosure of AI Assistance Affect the Perceptions of Writing. 2024</p>
<p>W Liang, Z Izzo, Y Zhang, H Lepp, H Cao, X Zhao, . . Zou, J Y , arXiv:2403.07183Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews. 2024</p>
<p>W Liang, Y Zhang, H Cao, B Wang, D Ding, X Yang, . . Zou, J , arXiv:2310.01783Can large language models provide useful feedback on research papers? A large-scale empirical analysis. 2023</p>
<p>Generative AI makes for better scientific writing -but beware the pitfalls. Z Liao, C Zhang, Nature. 5056312024</p>
<p>Rouge: A package for automatic evaluation of summaries. C Y Lin, Text Summarization Branches Out. 2004</p>
<p>How to write effective prompts for large language models. Z Lin, Nature Human Behaviour. 2024</p>
<p>DENDRAL: a case study of the first expert system for scientific hypothesis formation. R K Lindsay, B G Buchanan, E A Feigenbaum, J Lederberg, Artificial Intelligence. 6121993</p>
<p>How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. C W Liu, R Lowe, I V Serban, M Noseworthy, L Charlin, J Pineau, arXiv:1603.080232016</p>
<p>R Liu, N B Shah, arXiv:2306.00622Reviewergpt? an exploratory study on using large language models for paper reviewing. 2023</p>
<p>C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.06292The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. 2024</p>
<p>A Multimodal Generative AI Copilot for Human Pathology. M Y Lu, B Chen, D F Williamson, R J Chen, M Zhao, A K Chow, . . Mahmood, F , Nature. 2024</p>
<p>J Lucas, A Uchendu, M Yamashita, J Lee, S Rohatgi, D Lee, arXiv:2310.15515Fighting fire with fire: The dual role of llms in crafting and detecting elusive disinformation. 2023</p>
<p>Text compression as a test for artificial intelligence. M V Mahoney, AAAI/IAAI, 970. Malički, M. 1999. 2024631Structure peer review to make it more robust</p>
<p>Artificial intelligence and illusions of understanding in scientific research. L Messeri, M J Crockett, Nature. 62780022024</p>
<p>AI hyperrealism: Why AI faces are perceived as more real than human ones. E J Miller, B A Steward, Z Witkower, C A Sutherland, E G Krumhuber, A Dawel, Psychological Science. 34122023</p>
<p>AI's challenge of understanding the world. M Mitchell, Science. 38266712023. 2025Nature</p>
<p>Why an overreliance on AI-driven modelling is bad for science. A Narayanan, S Kapoor, Nature. 64080582025</p>
<p>Iterative ranking from pair-wise comparisons. S Negahban, S Oh, D Shah, Advances in Neural Information Processing Systems. 201225</p>
<p>AI-synthesized faces are indistinguishable from real faces and more trustworthy. S J Nightingale, H Farid, Proceedings of the National Academy of Sciences. 1198e21204811192022</p>
<p>Measuring human-ai value alignment in large language models. H Norhashim, J Hahn, Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. the AAAI/ACM Conference on AI, Ethics, and Society2024, October7</p>
<p>ChatGPT for science: how to talk to your data. J Nowogrodzki, Nature. 6312024</p>
<p>Bridging the LLM accessibility divide? performance, fairness, and cost of closed versus open llms for automated essay scoring. K Oketch, J P Lalor, Y Yang, A Abbasi, arXiv:2503.118272025</p>
<p>B Padmanabhan, X Fang, N Sahoo, A Burton-Jones, Machine Learning in Information Systems Research. 202246</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W J Zhu, Proceedings of the 40th Annual Meeting of ACL. the 40th Annual Meeting of ACL2002</p>
<p>A design science research methodology for information systems research. K Peffers, T Tuunanen, M A Rothenberger, S Chatterjee, Journal of Management Information Systems. 2432007</p>
<p>A taxonomy of evaluation methods for information systems artifacts. N Prat, I Comyn-Wattiau, J Akoka, Journal of Management Information Systems. 3232015</p>
<p>Is ChatGPT making scientists hyper-productive? The highs and lows of using AI. M Prillaman, Nature. 2024</p>
<p>Beyond black box ai-generated plagiarism detection: From sentence to document level. M A Quidwai, C Li, P Dube, arXiv:2306.081222023</p>
<p>I Rahwan, M Cebrian, N Obradovich, J Bongard, J F Bonnefon, C Breazeal, . . Wellman, M , Machine behaviour. 2019568</p>
<p>Editor's comments: writing a virtuous review. A Rai, MIS Quarterly. 4032016iii-x</p>
<p>Editor's comments: Diversity of design science research. A Rai, A Burton-Jones, H Chen, A Gupta, A R Hevner, W Ketter, . . Yoo, Y , MIS Quarterly. 4112017iii-xviii</p>
<p>M Renze, E Guven, arXiv:2405.06682Self-Reflection in LLM Agents: Effects on Problem-Solving Performance. 2024</p>
<p>Type/token ratios: What do they really tell us. B Richards, Journal of Child Language. 1421987</p>
<p>Research commentary: diversity in information systems research: threat, promise, and responsibility. D Robey, Information Systems Research. 741996</p>
<p>Z Robertson, arXiv:2307.05492Gpt4 is slightly helpful for peer-review assistance: A pilot study. 2023</p>
<p>Some thoughts on reviewing for Information Systems Research and other leading information systems journals. S Sarker, E A Whitley, K Y Goh, Y Hong, M Mähring, P Sanyal, . . Zhao, H , Information Systems Research. 3442023</p>
<p>Introduction to Data Compression. K Sayood, 2017Morgan Kaufmann</p>
<p>S Schulhoff, M Ilie, N Balepur, K Kahadze, A Liu, C Si, . . Resnik, P , arXiv:2406.06608The Prompt Report: A Systematic Survey of Prompting Techniques. 2024</p>
<p>Challenges, experiments, and computational solutions in peer review. N B Shah, Communications of the ACM. 6562022</p>
<p>Simple, robust and optimal ranking from pairwise comparisons. N B Shah, M J Wainwright, Journal of Machine Learning Research. 181992018</p>
<p>J Shen, N Tenenholtz, J B Hall, D Alvarez-Melis, N Fusi, arXiv:2402.05140Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains. 2024</p>
<p>Lexical richness and text length: An entropy-based perspective. Y Shi, L Lei, Journal of Quantitative Linguistics. 2912022</p>
<p>How can IJDS authors, reviewers, and editors use (and misuse) generative AI?. G Shmueli, B Maria Colosimo, D Martens, R Padman, M Saar-Tsechansky, R Liu Sheng, O , . . Tsui, K L , INFORMS Journal on Data Science. 212023</p>
<p>Accelerating science with human-aware artificial intelligence. J Sourati, J A Evans, Nature Human Behaviour. 7102023</p>
<p>The history of the peer-review process. R Spier, TRENDS in Biotechnology. 2082002</p>
<p>Let's find ways to measure novelty in science. B Steyn, Nature. 5432025</p>
<p>Integrated decision support for disaster risk management: Aiding preparedness and response decisions in wildfire management. D Suarez, C Gomez, A L Medaglia, R Akhavan-Tabatabaei, S Grajales, Information Systems Research. 3522024</p>
<p>ReviewFlow: Intelligent Scaffolding to Support Academic Peer Reviewing. L Sun, A Chan, Y S Chang, S P Dow, International Conference on Intelligent User Interfaces. 2024, March</p>
<p>Association between productivity and journal impact across disciplines and career age. A S Sunahara, M Perc, H V Ribeiro, Physical Review Research. 33331582021</p>
<p>The Janus Effect of Generative AI: Charting the Path for Responsible Conduct of Scholarly Activities in Information Systems. A Susarla, R Gopal, J B Thatcher, S Sarker, Information Systems Research. 3422023</p>
<p>Large language model advanced data analysis abuse to create a fake data set in medical research. A Taloni, V Scorcia, G Giannaccare, JAMA Ophthalmology. 141122023</p>
<p>GPT-4 performance on querying scientific publications: reproducibility, accuracy, and impact of an instruction sheet. K Tao, Z A Osman, P L Tzou, S Y Rhee, V Ahluwalia, R W Shafer, BMC Medical Research Methodology. 2411392024</p>
<p>J P Tennant, J M Dugan, D Graziotin, D C Jacques, F Waldner, D Mietchen, . . Colomb, J , A multi-disciplinary perspective on emergent and future innovations in peer review. 2017. F1000Research</p>
<p>Lexical density and readability: A case study of English textbooks. V To, S Fan, D Thomas, Internet Journal of Language. 37372013Culture and Society</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, . . Lample, G , arXiv:2302.13971Llama: Open and efficient foundation language models. 2023</p>
<p>Automation of systematic literature reviews: A systematic literature review, Information and Software Technology. R Van Dinter, B Tekinerdogan, C Catal, 2021136106589</p>
<p>How will generative AI disrupt data science in drug discovery?. J P Vert, Nature Biotechnology. 4162023</p>
<p>Position: Will we run out of data? Limits of LLM scaling based on human-generated data. P Villalobos, A Ho, J Sevilla, T Besiroglu, L Heim, M Hobbhahn, Forty-first International Conference on Machine Learning. 2024, July</p>
<p>Artificial intelligence and the conduct of literature reviews. G Wagner, R Lukyanenko, G Paré, Journal of Information Technology. 3722022</p>
<p>A review of the literature on citation impact indicators. L Waltman, Journal of Informetrics. 1022016</p>
<p>D Waltz, B G Buchanan, Automating science. 2009324</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, T Fu, Y Du, W Gao, K Huang, Z Liu, . . Zitnik, M , Nature. 62079722023</p>
<p>Unraveling generative ai from a human intelligence perspective: A battery of experiments. W Wang, S Pei, T Sun, 20234543351</p>
<p>Efficient ranking from pairwise comparisons. F Wauthier, M Jordan, N Jojic, International Conference on Machine Learning. PMLR2013. May</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, . . Zhou, D , NIPS. 352022</p>
<p>Information compression as a unifying principle in human learning, perception, and cognition. J G Wolff, Complexity. 2019118797462019</p>
<p>Z Xu, S Jain, M Kankanhalli, arXiv:2401.11817Hallucination is inevitable: An innate limitation of large language models. 2024</p>
<p>Poisoning medical knowledge using large language models. J Yang, H Xu, S Mirzoyan, T Chen, Z Liu, Z Liu, . . Wang, S , Nature Machine Intelligence. 2024</p>
<p>The next frontiers of digital innovation research. Y Yoo, O Henfridsson, J Kallinikos, R Gregory, G Burtch, S Chatterjee, S Sarker, Information Systems Research. 3542024</p>
<p>Optimizing generative AI by backpropagating language model feedback. M Yuksekgonul, F Bianchi, J Boen, S Liu, P Lu, Z Huang, . . Zou, J , Nature. 63980552025</p>
<p>New tool in the box. L Zdeborová, Nature Physics. 1352017</p>
<p>Calibrate before use: Improving few-shot performance of language models. Z Zhao, E Wallace, S Feng, D Klein, S Singh, ICML. PMLR2021, July</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, . . Stoica, I , Advances in NIPS. 362023</p>
<p>B Zhou, Y Hu, X Weng, J Jia, J Luo, X Liu, . . Huang, L , arXiv:2402.14289TinyLLaVA: A Framework of Small-scale Large Multimodal Models. 2024</p>
<p>A setwise approach for effective and highly efficient zero-shot ranking with large language models. S Zhuang, H Zhuang, B Koopman, G Zuccon, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024, July</p>
<p>Automated Focused Feedback Generation for Scientific Writing Assistance. J Zou, E References Chamoun, M Schlichtkrull, A Vlachos, Findings of the Association for Computational Linguistics ACL 2024. 2024. 2024, August635ChatGPT is transforming peer review -how can we use it responsibly?</p>
<p>Exploring the use of large language models for reference-free text quality evaluation: An empirical study. Y Chen, R Wang, H Jiang, S Shi, R Xu, arXiv:2304.007232023</p>
<p>Human-in-the-Loop AI Reviewing: Feasibility, Opportunities, and Risks. I Drori, D Te'eni, Journal of the Association for Information Systems. 2512024</p>
<p>J Du, Y Wang, W Zhao, Z Deng, S Liu, R Lou, . . Yin, W , arXiv:2406.16253Llms assist nlp researchers: Critique paper (meta-) reviewing. 2024</p>
<p>Z Gao, K Brantley, T Joachims, arXiv:2402.10886Reviewer2: Optimizing review generation through prompt generation. 2024</p>
<p>X Gao, J Ruan, J Gao, T Liu, Y Fu, arXiv:2503.08506Reviewagents: Bridging the gap between human and ai-generated paper reviews. 2025</p>
<p>Y Jin, Q Zhao, Y Wang, H Chen, K Zhu, Y Xiao, J Wang, arXiv:2406.12708AgentReview: Exploring Peer Review Dynamics with LLM Agents. 2024</p>
<p>Y Lee, W Cho, J Kim, arXiv:2403.18771Checkeval: A reliable llm-as-a-judge framework for evaluating text generation using checklists. 2024</p>
<p>M Li, H Li, C Tan, arXiv:2504.07174HypoEval: Hypothesis-Guided Evaluation for Natural Language Generation. 2025</p>
<p>W Liang, Y Zhang, H Cao, B Wang, D Ding, X Yang, . . Zou, J , arXiv:2310.01783Can large language models provide useful feedback on research papers? A large-scale empirical analysis. 2023</p>
<p>Y Liu, Z Guo, T Liang, E Shareghi, I Vulić, N Collier, arXiv:2410.02205Aligning with logic: Measuring, evaluating and improving logical consistency in large language models. 2024</p>
<p>Y Liu, H Zhou, Z Guo, E Shareghi, I Vulić, A Korhonen, N Collier, arXiv:2403.16950Aligning with human judgement: The role of pairwise preference in large language model evaluators. 2024</p>
<p>R Liu, N B Shah, arXiv:2306.00622Reviewergpt? an exploratory study on using large language models for paper reviewing. 2023</p>
<p>A Liusie, P Manakul, M J Gales, arXiv:2307.07889LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models. 2023</p>
<p>Z Qin, R Jagerman, K Hui, H Zhuang, J Wu, L Yan, . . Bendersky, M , arXiv:2306.17563Large language models are effective text rankers with pairwise ranking prompting. 2023</p>
<p>C Shen, L Cheng, X P Nguyen, Y You, L Bing, arXiv:2305.13091Large language models are not yet humanlevel evaluators for abstractive summarization. 2023</p>
<p>H Shin, J Tang, Y Lee, N Kim, H Lim, J Y Cho, . . Kim, J , arXiv:2502.17086Automatically Evaluating the Paper Reviewing Capability of Large Language Models. 2025</p>
<p>GPT-4 performance on querying scientific publications: reproducibility, accuracy, and impact of an instruction sheet. K Tao, Z A Osman, P L Tzou, S Y Rhee, V Ahluwalia, R W Shafer, BMC Medical Research Methodology. 2411392024</p>
<p>ReviewRobot: Explainable Paper Review Generation based on Knowledge Synthesis. Q Wang, Q Zeng, L Huang, K Knight, H Ji, N F Rajani, Proceedings of the 13th International Conference on Natural Language Generation. the 13th International Conference on Natural Language Generation2020, December</p>
<p>Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and Analysis. J Yu, Z Ding, J Tan, K Luo, Z Weng, C Gong, . . Li, X , Findings of the Association for Computational Linguistics: EMNLP 2024. 2024, November</p>
<p>Can we automate scientific reviewing. W Yuan, P Liu, G Neubig, Journal of Artificial Intelligence Research. 752022</p>
<p>Scientific opinion summarization: Paper meta-review generation dataset, methods, and evaluation. Q Zeng, M Sidhu, A Blume, H P Chan, L Wang, H Ji, AI4Research 2024 Proceedings. Springer Nature202420</p>
<p>Y Zeng, O Tendolkar, R Baartmans, Q Wu, L Chen, H Wang, arXiv:2406.00231LLM-RankFusion: Mitigating Intrinsic Inconsistency in LLM-based Ranking. 2024</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, . . Stoica, I , Advances in Neural Information Processing Systems. 202336</p>
<p>Is LLM a reliable reviewer? A comprehensive evaluation of LLM on automatic paper reviewing tasks. R Zhou, L Chen, K Yu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 20242024, May</p>
<p>A setwise approach for effective and highly efficient zero-shot ranking with large language models. S Zhuang, H Zhuang, B Koopman, G Zuccon, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024, July</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8729 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8729</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8729</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-280011050</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.21285v3.pdf" target="_blank">Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning</a></p>
                <p><strong>Paper Abstract:</strong> While slow-thinking large language models (LLMs) exhibit reflection-like reasoning, commonly referred to as the"aha moment:, their ability to generate informative critiques and refine prior solutions remains limited. In this paper, we introduce Double-Checker, a principled framework designed to enhance the reasoning capabilities of slow-thinking LLMs by fostering explicit self-critique and iterative refinement of their previous solutions. By fine-tuning on our curated 1,730 self-critical instances, Double-Checker empowers long-CoT LLMs to iteratively critique and refine their outputs during inference until they evaluate their solutions as correct under self-generated critiques. We validate the efficacy of Double-Checker across a comprehensive suite of reasoning benchmarks, demonstrating that iterative self-critique significantly enhances the reasoning capabilities of long-CoT LLMs. Notably, our Double-Checker increases the pass@1 performance on challenging AIME benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These results highlight a promising direction for developing more trustworthy and effective LLMs capable of structured self-critique. Our codes and data are available at https://github.com/XinXU-USTC/DoubleChecker</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8729.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8729.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Double-Checker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Double-Checker: Self-Critical Fine-Tuning for Iterative Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training and inference framework that fine-tunes long-CoT LLMs on a curated set of self-critical critique-refine examples (1,730 instances) so that a single model can generate structured critiques and iteratively refine its prior solutions at inference until the model's own critique judges the answer correct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-Distill-Qwen (variants: DS-7B, DS-32B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Distilled long-chain-of-thought LLM variants derived from DeepSeek-R1, evaluated at two scales: ~7B and ~32B parameters; models were fine-tuned (full-parameter SFT) with max sequence length 16,384 tokens using DeepSpeed/FlashAttention2.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Double-Checker (self-critical fine-tuning / iterative self-critique & refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train a single long-CoT LLM on a mixed dataset of direct inference examples and curated critique-refine examples. At inference: Round 0 generates a long thought T0 and summary S0; in each subsequent round n the model receives Q + S_{n-1}, generates a structured critique Cn (analysis, actionable improvement suggestions, correctness judgment) and a refined thought Tn and summary Sn; iteration continues until Cn signals 'correct' or a max iteration N is reached. Summaries (S_n) are used to keep context short.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical reasoning benchmarks (AIME24, AIME25) and other reasoning tasks (MATH500, OlympiadBench, GPQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Difficult mathematical contest problems (AIME24/AIME25: ~30 examples each) and other math/multidisciplinary reasoning benchmarks used to evaluate chain-of-thought reasoning and self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>DS-7B: AIME24 = 66.4% (pass@1, N up to 3); DS-32B: AIME24 = 79.8% (pass@1, N=1); DS-32B: AIME25 = 68.6% (pass@1). Overall average gains reported (across benchmarks): DS-7B average +4.1% over distilled baseline; DS-32B average +6.6% over distilled baseline. Specific examples: DeepSeek-R1-Distill-7B baseline 56.7% → Double-Checker 66.4% on AIME24; DeepSeek-R1-Distill-32B baseline 72.1% → Double-Checker 79.8% on AIME24.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Direct-inference long-CoT (no iterative self-critique): DS-7B baseline AIME24 = 56.7% (pass@1); DS-32B baseline AIME24 = 72.1% (pass@1). 'Naive SFT' (same questions but without critique data) DS-7B AIME24 = 57.1% (pass@1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Specialized SFT (fine-tuning) on a mixed dataset that includes (Q, S0, C1, T1, S1) critique-refine tuples; critique generation during inference is produced by the same model using structured prompts; iterative loop implemented purely via prompting/conditioning on previous summary (no separate critique model or external tool).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: consistent improvements across benchmarks and scales when using iterative self-critique compared to the same model w/o critique training (examples: DS-7B AIME24 improved from 56.7% to 66.4%; DS-32B AIME24 improved from 72.1% to 79.8%; ablation shows mixing direct-inference data and critique examples is necessary). Per-round ablation (Fig.3): for 7B, N=0→N=1 increases AIME24 from 57.3% to 64.6%, N=2→66.5% (further gains at small scale); 32B saturates after N=1 (77.7%→79.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: (1) without explicit critique training, long-CoT models often fail to produce informative critiques (the 'aha moment' does not imply effective self-critique); (2) some examples cannot be verified by the model's critique, so a max-iteration cap N is required to avoid infinite loops; (3) diminishing returns and saturation at larger scale (32B saturates after one iteration); (4) computational trade-off: cumulative tokens increase with rounds (though per-round token cost declines); (5) efficacy depends on the curated critique dataset size (only 1,730 instances used) and quality—removing direct-inference data or critique data degrades performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms naive SFT (same questions without critique training) and shows larger gains than direct application of existing test-time multi-round methods (ThinkTwice, Wait) when adapted to the same models. The paper also contrasts Double-Checker with split-architecture critique models (which add overhead), with Double-Checker achieving iterative refinement within a single model.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Removing direct-inference data (exclude D_direct) drops average accuracy to 54.2%, showing the need to mix direct and critique data. Removing Qwen3-generated critique examples yields a small drop (61.5%→61.1% avg). Adding limited extra round-2 training examples (<400) produced no significant gain. These ablations indicate the importance of (i) retaining direct inference examples and (ii) curated critique examples for improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8729.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8729.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Critique Probe (DeepSeek-R1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probe-induced Self-Critique Evaluation of DeepSeek-R1-Distill Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probing experiment that prompts pre-existing long-CoT models (DeepSeek-R1-Distill-7B and -32B) to produce critiques of their previous solution to test whether 'aha moment' behavior implies usable self-critique.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Qwen-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Distilled long-CoT LLMs (7B and 32B) from DeepSeek-R1 used as initial models before Double-Checker fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Probe-induced self-critique (prompted critique)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use a probe prompt adapted from prior work to ask the model to critique each reasoning step of its previous solution and decide if it is correct; then (optionally) generate a refined solution based on that critique.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AIME24 probe experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate whether models, when prompted, generate informative critiques and whether those critiques lead to improved final answers on hard math problems (AIME24).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>When probed, informative critiques were produced in 0% of cases for the 7B model and 8.5% for the 32B model; refinement after probe produced small accuracy increases: 7B improved by 1.6% (57.1% → 58.7%), 32B improved by 0.8% (72.1% → 72.9%) on AIME24.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline (no probe-induced refinement) AIME24: 7B = 57.1% (or 56.7% depending on rounding), 32B = 72.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering: a probe prompt asking the model to critique prior reasoning and judge correctness; no fine-tuning on critique examples.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative probe results show that prompting alone rarely elicits informative critiques (0% / 8.5%) and yields only marginal performance improvements (≤1.6%), supporting the paper's claim that 'aha moment' reflection-like behaviors do not guarantee useful self-critique.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The probe often elicits uninformative critiques leading to no change in the final answer (7B produced informative critiques 0% of the time). Prompting-only approaches (without critique fine-tuning) do not reliably enable iterative improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to Double-Checker (which fine-tunes on critique data), probe-induced prompting is far less effective; the paper cites prior work that prompt-only critique often produces uninformative feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8729.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8729.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative refinement with self-feedback (adapted baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior iterative refinement method where the model is prompted to search for errors in its solution and produce corrections; adapted as a baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-Distill-Qwen-7B (adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B distilled long-CoT model; adapted baseline prompted similarly to the Self-Refine method described in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine (prompted iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>After initial generation, the model is prompted (e.g., 'There is an error in the solution above. To find the error, go through the semantically complete solution and check if everything looks good.') and asked to produce corrections; used as a single-model baseline without SFT on critique examples.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AIME24 (and other math benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Iterative self-feedback evaluation applied to hard math problems to see if prompting alone can elicit corrective refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>DeepSeek-R1-Distill-7B + self-refine: AIME24 = 55.0% (pass@1) as reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>DeepSeek-R1-Distill-7B baseline (no self-refine) AIME24 = 56.7% (pass@1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering applied at test time; no additional critique fine-tuning used.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>In this adaptation, Self-Refine did not improve performance—on the 7B model it slightly decreased AIME24 from 56.7% to 55.0%—evidence that prompting-only self-refinement can be ineffective or harmful without targeted fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Adapted Self-Refine baseline performed worse than the baseline on AIME24 for 7B, demonstrating that naive prompting for self-correction can fail; paper highlights that specialized fine-tuning on critique examples is required.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Underperformed compared to Double-Checker (fine-tuned critique-refine) and even sometimes under baseline; contrasts with Double-Checker which achieves large gains after SFT on critique data.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8729.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8729.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ThinkTwice (adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ThinkTwice: Multi-round test-time reflection (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An existing method that performs multi-round test-time 'reflect-and-refine' thinking; paper adapts ThinkTwice to their DeepSeek models as a baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-Distill-Qwen (7B and 32B adaptations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Adaptations of the distilled DeepSeek long-CoT models instructed to perform multi-round test-time thinking according to ThinkTwice-like prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>ThinkTwice (multi-round test-time reflect-and-refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Test-time multi-round reflection where the model is prompted to reconsider or rerun reasoning steps multiple times without specialized fine-tuning on critique data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AIME24 and other math benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate multi-round test-time reflection without fine-tuning to see performance gains relative to Double-Checker and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Adapted results: DeepSeek-R1-Distill-7B + ThinkTwice: AIME24 = 58.7% (pass@1); DeepSeek-R1-Distill-32B + ThinkTwice: AIME24 = 72.9% (pass@1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Corresponding baselines: DS-7B = 56.7% (AIME24), DS-32B = 72.1% (AIME24).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Test-time prompt engineering (multi-round), not specialized SFT on critique examples.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Small improvements observed for 7B (56.7→58.7), minimal improvement for 32B (72.1→72.9); however, Double-Checker's SFT approach yields substantially larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Gains are modest; test-time-only multi-round methods without critique SFT are less effective than training the model on critique-refine examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed by Double-Checker (which uses critic-targeted SFT). Compared to naive SFT, ThinkTwice gave slightly better or comparable performance but not the large gains seen with critique fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8729.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8729.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wait (test-time scaling)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wait: Simple test-time scaling via appending 'wait' before the end of thinking (adapted baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple test-time trick whereby the token 'wait' is appended before the end of the model's thinking to encourage additional internal deliberation; included as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-Distill-Qwen (7B and 32B adaptations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Distilled long-CoT LLMs adapted with the 'wait' prompting trick during decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Wait (test-time scaling / delay-before-answer)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>A short prompt modification appended at test time (e.g., adding 'wait') to induce longer or more careful deliberation during generation without additional training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AIME24 and related benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluates whether simple test-time prompting tricks can rival structured critique-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Adapted results: DeepSeek-R1-Distill-7B + Wait: AIME24 = 57.5% (pass@1); DeepSeek-R1-Distill-32B + Wait: AIME24 = 72.8% (pass@1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baselines: DS-7B = 56.7% (AIME24), DS-32B = 72.1% (AIME24).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering (test-time token insertion) to encourage 'more thinking' without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Very small improvements or near parity with baseline, indicating that simple test-time tricks alone are insufficient to match gains from structured critique SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Marginal gains only; cannot substitute for specialized training on critique-refine examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Less effective than Double-Checker and comparable to or slightly better than distilled baseline; far less effective than SFT-based self-critique.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Think twice: Enhancing llm reasoning by scaling multi-round test-time thinking <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Critic: Large language models can self-correct with tool-interactive critiquing <em>(Rating: 1)</em></li>
                <li>Dancing with critiques: Enhancing llm reasoning with stepwise natural language self-critique <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8729",
    "paper_id": "paper-280011050",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Double-Checker",
            "name_full": "Double-Checker: Self-Critical Fine-Tuning for Iterative Refinement",
            "brief_description": "A training and inference framework that fine-tunes long-CoT LLMs on a curated set of self-critical critique-refine examples (1,730 instances) so that a single model can generate structured critiques and iteratively refine its prior solutions at inference until the model's own critique judges the answer correct.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-Distill-Qwen (variants: DS-7B, DS-32B)",
            "model_description": "Distilled long-chain-of-thought LLM variants derived from DeepSeek-R1, evaluated at two scales: ~7B and ~32B parameters; models were fine-tuned (full-parameter SFT) with max sequence length 16,384 tokens using DeepSpeed/FlashAttention2.",
            "reflection_method_name": "Double-Checker (self-critical fine-tuning / iterative self-critique & refine)",
            "reflection_method_description": "Train a single long-CoT LLM on a mixed dataset of direct inference examples and curated critique-refine examples. At inference: Round 0 generates a long thought T0 and summary S0; in each subsequent round n the model receives Q + S_{n-1}, generates a structured critique Cn (analysis, actionable improvement suggestions, correctness judgment) and a refined thought Tn and summary Sn; iteration continues until Cn signals 'correct' or a max iteration N is reached. Summaries (S_n) are used to keep context short.",
            "task_name": "Mathematical reasoning benchmarks (AIME24, AIME25) and other reasoning tasks (MATH500, OlympiadBench, GPQA)",
            "task_description": "Difficult mathematical contest problems (AIME24/AIME25: ~30 examples each) and other math/multidisciplinary reasoning benchmarks used to evaluate chain-of-thought reasoning and self-refinement.",
            "performance_with_reflection": "DS-7B: AIME24 = 66.4% (pass@1, N up to 3); DS-32B: AIME24 = 79.8% (pass@1, N=1); DS-32B: AIME25 = 68.6% (pass@1). Overall average gains reported (across benchmarks): DS-7B average +4.1% over distilled baseline; DS-32B average +6.6% over distilled baseline. Specific examples: DeepSeek-R1-Distill-7B baseline 56.7% → Double-Checker 66.4% on AIME24; DeepSeek-R1-Distill-32B baseline 72.1% → Double-Checker 79.8% on AIME24.",
            "performance_without_reflection": "Direct-inference long-CoT (no iterative self-critique): DS-7B baseline AIME24 = 56.7% (pass@1); DS-32B baseline AIME24 = 72.1% (pass@1). 'Naive SFT' (same questions but without critique data) DS-7B AIME24 = 57.1% (pass@1).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Specialized SFT (fine-tuning) on a mixed dataset that includes (Q, S0, C1, T1, S1) critique-refine tuples; critique generation during inference is produced by the same model using structured prompts; iterative loop implemented purely via prompting/conditioning on previous summary (no separate critique model or external tool).",
            "number_of_iterations": 3,
            "evidence_for_improvement": "Quantitative: consistent improvements across benchmarks and scales when using iterative self-critique compared to the same model w/o critique training (examples: DS-7B AIME24 improved from 56.7% to 66.4%; DS-32B AIME24 improved from 72.1% to 79.8%; ablation shows mixing direct-inference data and critique examples is necessary). Per-round ablation (Fig.3): for 7B, N=0→N=1 increases AIME24 from 57.3% to 64.6%, N=2→66.5% (further gains at small scale); 32B saturates after N=1 (77.7%→79.8%).",
            "limitations_or_failure_cases": "Reported limitations include: (1) without explicit critique training, long-CoT models often fail to produce informative critiques (the 'aha moment' does not imply effective self-critique); (2) some examples cannot be verified by the model's critique, so a max-iteration cap N is required to avoid infinite loops; (3) diminishing returns and saturation at larger scale (32B saturates after one iteration); (4) computational trade-off: cumulative tokens increase with rounds (though per-round token cost declines); (5) efficacy depends on the curated critique dataset size (only 1,730 instances used) and quality—removing direct-inference data or critique data degrades performance.",
            "comparison_to_other_methods": "Outperforms naive SFT (same questions without critique training) and shows larger gains than direct application of existing test-time multi-round methods (ThinkTwice, Wait) when adapted to the same models. The paper also contrasts Double-Checker with split-architecture critique models (which add overhead), with Double-Checker achieving iterative refinement within a single model.",
            "ablation_study_results": "Removing direct-inference data (exclude D_direct) drops average accuracy to 54.2%, showing the need to mix direct and critique data. Removing Qwen3-generated critique examples yields a small drop (61.5%→61.1% avg). Adding limited extra round-2 training examples (&lt;400) produced no significant gain. These ablations indicate the importance of (i) retaining direct inference examples and (ii) curated critique examples for improvement.",
            "uuid": "e8729.0",
            "source_info": {
                "paper_title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Self-Critique Probe (DeepSeek-R1)",
            "name_full": "Probe-induced Self-Critique Evaluation of DeepSeek-R1-Distill Models",
            "brief_description": "A probing experiment that prompts pre-existing long-CoT models (DeepSeek-R1-Distill-7B and -32B) to produce critiques of their previous solution to test whether 'aha moment' behavior implies usable self-critique.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Qwen-32B",
            "model_description": "Distilled long-CoT LLMs (7B and 32B) from DeepSeek-R1 used as initial models before Double-Checker fine-tuning.",
            "reflection_method_name": "Probe-induced self-critique (prompted critique)",
            "reflection_method_description": "Use a probe prompt adapted from prior work to ask the model to critique each reasoning step of its previous solution and decide if it is correct; then (optionally) generate a refined solution based on that critique.",
            "task_name": "AIME24 probe experiments",
            "task_description": "Evaluate whether models, when prompted, generate informative critiques and whether those critiques lead to improved final answers on hard math problems (AIME24).",
            "performance_with_reflection": "When probed, informative critiques were produced in 0% of cases for the 7B model and 8.5% for the 32B model; refinement after probe produced small accuracy increases: 7B improved by 1.6% (57.1% → 58.7%), 32B improved by 0.8% (72.1% → 72.9%) on AIME24.",
            "performance_without_reflection": "Baseline (no probe-induced refinement) AIME24: 7B = 57.1% (or 56.7% depending on rounding), 32B = 72.1%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering: a probe prompt asking the model to critique prior reasoning and judge correctness; no fine-tuning on critique examples.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative probe results show that prompting alone rarely elicits informative critiques (0% / 8.5%) and yields only marginal performance improvements (≤1.6%), supporting the paper's claim that 'aha moment' reflection-like behaviors do not guarantee useful self-critique.",
            "limitations_or_failure_cases": "The probe often elicits uninformative critiques leading to no change in the final answer (7B produced informative critiques 0% of the time). Prompting-only approaches (without critique fine-tuning) do not reliably enable iterative improvement.",
            "comparison_to_other_methods": "Compared to Double-Checker (which fine-tunes on critique data), probe-induced prompting is far less effective; the paper cites prior work that prompt-only critique often produces uninformative feedback.",
            "ablation_study_results": null,
            "uuid": "e8729.1",
            "source_info": {
                "paper_title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Self-Refine (baseline)",
            "name_full": "Self-Refine: Iterative refinement with self-feedback (adapted baseline)",
            "brief_description": "A prior iterative refinement method where the model is prompted to search for errors in its solution and produce corrections; adapted as a baseline in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-Distill-Qwen-7B (adaptation)",
            "model_description": "7B distilled long-CoT model; adapted baseline prompted similarly to the Self-Refine method described in prior work.",
            "reflection_method_name": "Self-Refine (prompted iterative self-feedback)",
            "reflection_method_description": "After initial generation, the model is prompted (e.g., 'There is an error in the solution above. To find the error, go through the semantically complete solution and check if everything looks good.') and asked to produce corrections; used as a single-model baseline without SFT on critique examples.",
            "task_name": "AIME24 (and other math benchmarks)",
            "task_description": "Iterative self-feedback evaluation applied to hard math problems to see if prompting alone can elicit corrective refinements.",
            "performance_with_reflection": "DeepSeek-R1-Distill-7B + self-refine: AIME24 = 55.0% (pass@1) as reported in Table 1.",
            "performance_without_reflection": "DeepSeek-R1-Distill-7B baseline (no self-refine) AIME24 = 56.7% (pass@1).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering applied at test time; no additional critique fine-tuning used.",
            "number_of_iterations": null,
            "evidence_for_improvement": "In this adaptation, Self-Refine did not improve performance—on the 7B model it slightly decreased AIME24 from 56.7% to 55.0%—evidence that prompting-only self-refinement can be ineffective or harmful without targeted fine-tuning.",
            "limitations_or_failure_cases": "Adapted Self-Refine baseline performed worse than the baseline on AIME24 for 7B, demonstrating that naive prompting for self-correction can fail; paper highlights that specialized fine-tuning on critique examples is required.",
            "comparison_to_other_methods": "Underperformed compared to Double-Checker (fine-tuned critique-refine) and even sometimes under baseline; contrasts with Double-Checker which achieves large gains after SFT on critique data.",
            "ablation_study_results": null,
            "uuid": "e8729.2",
            "source_info": {
                "paper_title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ThinkTwice (adaptation)",
            "name_full": "ThinkTwice: Multi-round test-time reflection (adapted)",
            "brief_description": "An existing method that performs multi-round test-time 'reflect-and-refine' thinking; paper adapts ThinkTwice to their DeepSeek models as a baseline for comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-Distill-Qwen (7B and 32B adaptations)",
            "model_description": "Adaptations of the distilled DeepSeek long-CoT models instructed to perform multi-round test-time thinking according to ThinkTwice-like prompts.",
            "reflection_method_name": "ThinkTwice (multi-round test-time reflect-and-refine)",
            "reflection_method_description": "Test-time multi-round reflection where the model is prompted to reconsider or rerun reasoning steps multiple times without specialized fine-tuning on critique data.",
            "task_name": "AIME24 and other math benchmarks",
            "task_description": "Evaluate multi-round test-time reflection without fine-tuning to see performance gains relative to Double-Checker and baselines.",
            "performance_with_reflection": "Adapted results: DeepSeek-R1-Distill-7B + ThinkTwice: AIME24 = 58.7% (pass@1); DeepSeek-R1-Distill-32B + ThinkTwice: AIME24 = 72.9% (pass@1).",
            "performance_without_reflection": "Corresponding baselines: DS-7B = 56.7% (AIME24), DS-32B = 72.1% (AIME24).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Test-time prompt engineering (multi-round), not specialized SFT on critique examples.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Small improvements observed for 7B (56.7→58.7), minimal improvement for 32B (72.1→72.9); however, Double-Checker's SFT approach yields substantially larger gains.",
            "limitations_or_failure_cases": "Gains are modest; test-time-only multi-round methods without critique SFT are less effective than training the model on critique-refine examples.",
            "comparison_to_other_methods": "Outperformed by Double-Checker (which uses critic-targeted SFT). Compared to naive SFT, ThinkTwice gave slightly better or comparable performance but not the large gains seen with critique fine-tuning.",
            "ablation_study_results": null,
            "uuid": "e8729.3",
            "source_info": {
                "paper_title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Wait (test-time scaling)",
            "name_full": "Wait: Simple test-time scaling via appending 'wait' before the end of thinking (adapted baseline)",
            "brief_description": "A simple test-time trick whereby the token 'wait' is appended before the end of the model's thinking to encourage additional internal deliberation; included as a baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-Distill-Qwen (7B and 32B adaptations)",
            "model_description": "Distilled long-CoT LLMs adapted with the 'wait' prompting trick during decoding.",
            "reflection_method_name": "Wait (test-time scaling / delay-before-answer)",
            "reflection_method_description": "A short prompt modification appended at test time (e.g., adding 'wait') to induce longer or more careful deliberation during generation without additional training.",
            "task_name": "AIME24 and related benchmarks",
            "task_description": "Evaluates whether simple test-time prompting tricks can rival structured critique-based methods.",
            "performance_with_reflection": "Adapted results: DeepSeek-R1-Distill-7B + Wait: AIME24 = 57.5% (pass@1); DeepSeek-R1-Distill-32B + Wait: AIME24 = 72.8% (pass@1).",
            "performance_without_reflection": "Baselines: DS-7B = 56.7% (AIME24), DS-32B = 72.1% (AIME24).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering (test-time token insertion) to encourage 'more thinking' without fine-tuning.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Very small improvements or near parity with baseline, indicating that simple test-time tricks alone are insufficient to match gains from structured critique SFT.",
            "limitations_or_failure_cases": "Marginal gains only; cannot substitute for specialized training on critique-refine examples.",
            "comparison_to_other_methods": "Less effective than Double-Checker and comparable to or slightly better than distilled baseline; far less effective than SFT-based self-critique.",
            "ablation_study_results": null,
            "uuid": "e8729.4",
            "source_info": {
                "paper_title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Think twice: Enhancing llm reasoning by scaling multi-round test-time thinking",
            "rating": 2,
            "sanitized_title": "think_twice_enhancing_llm_reasoning_by_scaling_multiround_testtime_thinking"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "rating": 1,
            "sanitized_title": "critic_large_language_models_can_selfcorrect_with_toolinteractive_critiquing"
        },
        {
            "paper_title": "Dancing with critiques: Enhancing llm reasoning with stepwise natural language self-critique",
            "rating": 1,
            "sanitized_title": "dancing_with_critiques_enhancing_llm_reasoning_with_stepwise_natural_language_selfcritique"
        }
    ],
    "cost": 0.016554,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning
2 Oct 2025</p>
<p>Xin Xu 
The Hong Kong University of Science and Technology</p>
<p>Tianhao Chen 
The Hong Kong University of Science and Technology</p>
<p>Fan Zhang 
The Hong Kong University of Science and Technology</p>
<p>Wanlong Liu 
University of Electronic Science
Technology of China</p>
<p>Pengxiang Li 
Dalian University of Technology</p>
<p>Ajay Kumar Jaiswal 
University of Texas at Austin</p>
<p>Yuchen Yan 
Zhejiang University</p>
<p>Jishan Hu 
The Hong Kong University of Science and Technology</p>
<p>Yang Wang 
The Hong Kong University of Science and Technology</p>
<p>Hao Chen 
The Hong Kong University of Science and Technology</p>
<p>Shiwei Liu 
University of Oxford</p>
<p>Shizhe Diao 
NVIDIA</p>
<p>Can Yang 
The Hong Kong University of Science and Technology</p>
<p>Lu Yin l.yin@surrey.ac.uk 
University of Surrey</p>
<p>Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning
2 Oct 20256E2F33FA762F67C53825BCB6B9550513arXiv:2506.21285v3[cs.CL]
While slow-thinking large language models (LLMs) exhibit reflection-like reasoning, commonly referred to as the "aha moment", their ability to generate informative critiques and refine prior solutions remains limited.In this paper, we introduce Double-Checker, a principled framework designed to enhance the reasoning capabilities of slow-thinking LLMs by fostering explicit self-critique and iterative refinement of their previous solutions.By fine-tuning on our curated 1,730 self-critical instances, Double-Checker empowers long-CoT LLMs to iteratively critique and refine their outputs during inference until they evaluate their solutions as correct under self-generated critiques.We validate the efficacy of Double-Checker across a comprehensive suite of reasoning benchmarks, demonstrating that iterative self-critique significantly enhances the reasoning capabilities of long-CoT LLMs.Notably, our Double-Checker increases the pass@1 performance on challenging AIME benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs.These results highlight a promising direction for developing more trustworthy and effective LLMs capable of structured self-critique.Our codes and data are available at https://github.com/XinXU-USTC/DoubleChecker.</p>
<p>Introduction</p>
<p>Reasoning-the capacity to solve complex tasks by logically connecting facts and drawing conclusions-represents a critical milestone in the quest for Artificial General Intelligence [1,2,3,4].Following the advent of large language models (LLMs), extensive research has sought to further enhance their reasoning ability, spanning more effective pretraining [5,6], supervised finetuning [7,8,9,10,11], rigorous evaluation [4,12,13], and, more recently, reinforcement learning (RL) [14,15,16].In particular, [15] shows that RL with verifiable rewards can push LLMs toward generating long chains of thought (long-CoT) [17] and exhibiting reflective-like reasoning behavior, often termed as the "aha moment" [18].Despite these gains, Recent works [19,20] suggest that revisiting and refining previous solutions might unlock further improvements, motivating us to integrate the "aha moment" into a systematic "reflect-and-refine" loop.</p>
<p>The key concept lies in the "reflect-and-refine" is critique: the model's explicit evaluation of whether a solution is correct and, if needed, how it can be improved [21,22,23].Critique underpins the principle of selectively refining only those solutions that need fixing, thereby preserving originally correct Question: Among the 900 residents of Aimeville, there are 195 who own a diamond ring, 367 who own a set of golf clubs, and 562 who own a garden spade.In addition, each of the 900 residents owns a bag of candy hearts.There are 437 residents who own exactly two of these things, and 234 residents who own exactly three of these things.Find the number of residents of Aimeville who own all four of these things.</p>
<p>Answer: 73</p>
<p>DeepSeek-Distill-Qwen-7B  answers [24,25].Numerous studies have demonstrated that critique can subsequently be utilized to enhance the quality of generated outputs [22,23,26].For example, [22,27] train specialized critiqueoriented LLMs capable of providing feedback to generator LLMs.However, employing a separate model exclusively for critique introduces additional overhead [22].Alternatively, [23] proposes integrating critique as a training objective.Nevertheless, the resulting LLMs are unable to leverage self-critique effectively during inference.Furthermore, [19] reports only marginal improvements for Long-CoT LLMs, even after fine-tuning on 100K self-critique examples.These raise an open question: Do Long-CoT LLMs, which demonstrate reflection-like reasoning, possess the capacity to leverage self-critique to enhance performance?If not, how can we equip them with this ability?</p>
<p>In this paper, we investigate the integration of reflection-like reasoning with self-critique to enhance the reasoning abilities of slow-thinking LLMs.Specifically, we start by examining whether long-CoT LLMs can leverage self-critique to iteratively refine their prior solutions during inference in a probeinduced manner.Our findings reveal that the occurrence of an "aha moment" does not necessarily indicate the presence of a self-critique mechanism (see Sec. 3.1).For instance, as illustrated in Fig. 1, DeepSeek-Distill-Qwen-7B fails to generate informative critiques of its prior solution, ultimately arriving at the same incorrect answer.To address this, we introduce Double-Checker, a novel framework designed to empower LLMs to critique and refine their prior solutions iteratively and adaptively.Through a specialized training process that combines direct inference instances with curated critique-refine data (1,730 instances in total), our Double-Checker equips long-CoT LLMs with an effective self-critique capability.This enables iterative improvements in performance during inference via self-critique.An example of this process is shown in Fig. 1, where Double-Checker successfully resolves a complex math problem using the "reflect-and-refine" approach.</p>
<p>Our main contributions can be summarized as follows: ❶ We investigate the self-critical behavior of long-CoT LLMs via a probing and find that they are unable to generate informative critiques to improve their prior solutions.❷ We propose Double-Checker, a novel framework that pairs direct inference data with a carefully curated critique-refine dataset (1,730 in total), enabling LLMs to iteratively correct flawed reasoning during inference.❸ Experiments on a wide range of reasoning benchmarks demonstrate that even with a modest amount of critique data, Double-Checker unlocks substantial improvements in accuracy.Notably, our method raises pass@1 performance on challenging AIME benchmarks from 4.4% to 18.2%, underscoring the impact of explicit self-critique.</p>
<p>Related Work</p>
<p>Long Chain-of-Thought and Slow Thinking.The rise of LLMs has driven extensive research to enhance their reasoning capabilities through various strategies.Early efforts include advancements in pretraining methodologies [5,6], supervised fine-tuning [7,8,9,10,11], and rigorous evaluation techniques [4,12,13].More recently, RL has emerged as a key paradigm for improving reasoning in LLMs.For instance, [15] demonstrates that RL with verifiable rewards enables models to generate long chains of thought (long-CoT) [17], fostering more structured, multi-step problem-solving skills.This approach has been shown to promote reflective reasoning behaviors, termed as the "aha moments" [18].These advancements mark significant progress in LLM reasoning [28].However, our work reveals a critical limitation: while strong reflection-like reasoning allows LLMs to recognize errors or inconsistencies, it does not inherently ensure robust self-improvement [29,30].Additionally, existing self-improvement methods [29,30] often depend on external tools or explicit feedback mechanisms, making it challenging to guide a single LLM through multiple, reliable rounds of refinement.Addressing these challenges is crucial to unlocking the full potential of self-improvement in LLMs.</p>
<p>Critique LLMs and Integrated Self-Improvement.A parallel line of research employs critique models or reward estimators to score and refine outputs from a "generator" model, especially in mathematical domains [31,32,33,34].While effective in principle, this split-architecture strategy requires substantial overhead (running two separate LLMs) or produces numeric feedback that lacks actionable corrections [35].Other efforts have tried to incorporate critique into a single model's training objective [23] or train on large multi-round self-critique data [19], but with limited gains in iterative refinement.Against this backdrop, our work introduces Double-Checker, which merges critique and generation into a unified "reflect-and-refine" loop within one long-CoT LLM.By carefully curating critique-oriented examples and integrating them with direct-inference data, we equip long-CoT LLMs with the capability to generate meaningful critiques and adaptively refine their prior solutions based on self-generated critiques, ultimately enabling robust self-improvement.</p>
<p>3 Method</p>
<p>Aha Moment Does Not Equate to Effective Self-Critique</p>
<p>Previous studies have observed that fast-thinking LLMs often generate uninformative critiques, limiting their capacity for self-improvement [22,36].In contrast, slow-thinking LLMs are believed to exhibit self-reflection behaviors, identifying and potentially correcting errors in their reasoning steps [15].This raises the intriguing question: Can long-CoT LLMs with strong reflection-like reasoning abilities perform effective self-critique?To investigate this, we conduct experiments on AIME24 using DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-32B, employing a probe to induce self-critique behavior (see Appendix A.1 for detailed settings).We have the following results: ❶ DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-32B follow the probe prompt and produce informative critiques in only 0% and 8.5% of cases, respectively.❷ The performance on AIME24 improves slightly after refinement with self-critique (1.6% for 7B: 57.1% → 58.7% and 0.8% for 32B: 72.1% → 72.9%).These findings suggest that the aha moment does not inherently translate into effective self-critique.While these models demonstrate strong capabilities in reflection-type reasoning, their capacity to autonomously evolve through effective self-critique remains limited.</p>
<p>Double-Checker Framework</p>
<p>Despite exhibiting the "aha moment," long-CoT LLMs demonstrate limited ability to generate actionable critiques and effectively apply them for iterative self-refinement.We hypothesize that this limitation arises because current long-CoT LLMs are primarily trained for direct inference.As a result, these models do not naturally transition toward interactive refinement through self-critique, even when prompted with carefully designed probes (see Sec. 3.1).To address this gap, we propose Double-Checker, a novel framework designed to enable long-CoT LLMs to critique their prior solutions and iteratively refine their reasoning.An overview of Double-Checker is depicted in Fig. 2.This section presents the detailed training and inference process of Double-Checker.</p>
<p>Training Process</p>
<p>As shown in Fig. Critique with Answer Correctness.Given a question Q and its preceding summary S 0 , we employ a proficient LLM C to generate detailed critiques.The critique explicitly signals the correctness of the initial answer A 0 (correct/incorrect).To optimize critique quality, we employ distinct prompts tailored to correct and incorrect A 0 (see App. A. 3).This answer correctness signal is indispensable for effective critique generation.Formally, critique generation follows: C : {Instruction incorporating Answer Correctness Signal} ⊕ Q ⊕ S 0 → C 1 where C 1 adheres to the structured format defined in App.A.2.</p>
<p>Refinement.</p>
<p>When the initial answer A 0 is correct, we collect the corresponding critique C 1 and store the triplet (Q, S 0 , C 1 ) into our training set D critique .For incorrect A 0 , we refine the solution using a Refinement long-CoT LLM R, which takes the question Q, prior summary S 0 , and critique C 1 as input: R :
Q ⊕ S 0 ⊕ C 1 → T 1 ⊕ S 1 ,
where T 1 and S 1 represent the refined reasoning and summary, respectively.The refined answer A 1 is extracted from S 1 and compared to the ground truth
GT . If A 1 matches GT , (Q, S 0 , C 1 , T 1 , S 1 ) is added to D critique ; otherwise, it is discarded.</p>
<p>Distillation. After the data curation stage, training examples are categorized into two formats:</p>
<p>(Q, S 0 , C 1 ) for correct answers A 0 and (Q, S 0 , C 1 , T 1 , S 1 ) for incorrect answers.For simplicity, instances with (Q, S 0 , C 1 ) are padded to (Q, S 0 , C 1 , T ′ 1 , S ′ 1 ) using a predefined template (see App. A.4).To maintain the ability of direct inference of the original long-CoT LLM M, we will mix D critique with a direct inference training set D direc = {(Q, T 0 , S 0 )}.Finally, our training set will be D train = D direc ∪ D critique .The learning objective is
min θ − 1 |Dtrain| D direct log PM θ (T0 ⊕ S0 | Q) + D critique log PM θ (C1 ⊕ T1 ⊕ S1 | Q ⊕ S0) ,
where θ is the parameters of long-CoT LLM M and |D train | denotes the number of training examples.</p>
<p>Inference Pipeline</p>
<p>We will first introduce a paradigm shift from direct inference (Fig. 2 (a)) to iterative refinement via self-critique (Fig. • Round 0 (Direct Inference).Given a question Q, the model M generates a detailed reasoning chain T 0 and a final summary S 0 , i.e., M :
Q → T 0 ⊕ S 0 .
where ⊕ denotes the string concatenation.This baseline (long-CoT) forms the initial solution.• Round 1 (Self-Critique + Refinement).We now feed both Q and the prior summary S 0 to M. The model produces a critique C 1 of S 0 and then refines the solution into a new thought T 1 , finally yielding a new summary S 1 .Formally,
M : Q ⊕ S 0 → C 1 ⊕ T 1 ⊕ S 1 .
• Round n (Repeated Refinement).For subsequent rounds (1 ≤ n ≤ N ), the model receives Q ⊕ S n−1 , generates C n to critique the previous summary, and refines the solution into T n and S n .Symbolically,
M : Q ⊕ S n−1 → C n ⊕ T n ⊕ S n .
We continue until the critique C n deems the answer correct or a maximum iteration limit N is reached.</p>
<p>Context Window.The thought T i is typically lengthy, while the corresponding summary S i usually encapsulates all the essential information of T i , serving as a concise version of T i .Discarding T i and retaining only S i for each refinement round will ensure that the entire refinement process remains within the context window of Long-CoT LLMs.</p>
<p>Critique Space.The critique evaluates the prior summary, assessing whether the answer is correct and proposing actionable suggestions to enhance the solution when needed.Following [22], our critique consists of three components: 1) an analysis of the summary, 2) actionable improvement suggestions, 3) an answer correctness judgment (correct/incorrect).This judgment enables early termination of the iterative refinement process when the solution is deemed correct (see Sec. 3.2).An example of the critique structure is provided in Appendix A.2.</p>
<p>As illustrated in Fig. 2 (d), our Double-Checker adopts an iterative refinement pipeline that alternates between: (1) appending the previous summary (S n−1 ) to the input question (Q), and (2) generating an informative critique (C n ) followed by refining the prior solution (T n , S n ).The process terminates when the critique C n predicts the correctness of the answer extracted from S n−1 .The complete inference procedure is formally presented in Algorithm 1.To ensure termination, we define a maximum iteration limit N .Although theoretically the process could iterate infinitely until all test examples achieve critique-verified correctness, we impose a finite N in practice to prevent computational divergence due to potential inability to predict correctness for certain cases.</p>
<p>4 Experiments and Results</p>
<p>Training Setup</p>
<p>Training Data Construction.To construct the original training set D orig , we compile a pool of candidate problems from existing mathematical reasoning datasets: S1.1 [11], DeepMath-103K [37], OpenRS [38], and ORZ-Math-Hard [39].We filter these candidates using two key criteria: 1) Answer Verifiability: Ensuring ground-truth labels are verifiable via rule-based validation, 2) Difficulty: Selecting problems with appropriate complexity.We get a collection of around 8K high-quality questions, calibrated for both difficulty and correctness.For initial generation, critique annotation, and refinement, we utilize Qwen3-235B-A22B Training Details.We train the Distilled long CoT variants of DeepSeek-R1 (7B and 32B parameters) on our curated training set D train using full-parameter fine-tuning.The training process employs DeepSpeed ZeRO optimization [41] for efficient memory utilization and FlashAttention2 [42] for accelerated training.Following the implementation in [10], we set the maximum sequence length to 16,384 tokens and adopt a learning rate of 5 × 10 −6 .Implementation details are in App.B.2.</p>
<p>Evaluation Setup</p>
<p>Evaluation Setting.We evaluate on AIME24, AIME25, MATH500 [43], and OlympiadBench [44] for mathematical reasoning, and GPQA [12] for multidisciplinary problems.Following [10], we adopt an unbiased pass@1 metric for AIME24 and AIME25, generating 16 samples with a decoding temperature of 0.6.For the remaining benchmarks, we generate 4 samples per problem to report pass@1.We use vLLM [45] to accelerate inference and set the maximum sequence length to be 32,768 tokens.We set N in Algorithm 1 to 3 for Double-Checker-DS-7B and 1 for Double-Checker-DS-32B.A brief introduction to different benchmarks and detailed evaluation setting can be found in App.B.3.</p>
<p>Baselines.We compare Double-Checker against a comprehensive set of baselines, categorized as follows: 1) DeepSeek-R1-Distill-Qwen Series (7B, 32B): Strong long-CoT LLMs distilled from DeepSeek-R1 using 800K examples.2) S1.1 (7B, 32B) [11]: Two CoT LLMs distilled from DeepSeek-R1 using 1K high-quality from multiple sources.3) LIMO-32B [10]: A powerful LLM trained on 837 carefully curated examples.4) InftyThink (7B, 32B) [46]: Models trained on 333K examples adapted from OpenR1-Math, with results from the original paper using multiround interactive inference.5) Light-R1 (7B, 32B) [28]: Two-stage SFT (79K data) + RL-trained models.6) Naive-SFT Baseline (7B, 32B): DeepSeek-R1-Distill-Qwen trained on questions of our D train using standard SFT (without critique learning), which can isolate the contribution of training data.7) We also include OpenAI-o1 series [14] and DeepSeek-R1 for reference.8) We also adapt ThinkTwice [19] with DeepSeek-R1-Distill-Qwen Series to see the effect of "reflect-andrefine" without any finetuning.9) Self-Refine: We adopt a similar approach as (author?) [29].10) We also introduce a sequential test-time scaling approach by appending "wait" before the end of thinking [11].For 4) and 7), we report the results from other papers directly (see App. C.1), and run the remaining baselines by our own.</p>
<p>Main Results</p>
<p>Table 1 summarizes the primary evaluation results on multiple challenging reasoning benchmarks.</p>
<p>From Table 1, we have several key observations:</p>
<p>Double-Checker consistently enhances the performance of original long-CoT LLMs across all reasoning benchmarks and model scales.Our model, Double-Checker-DS-7B, outperforms DeepSeek-Distill-Qwen-7B by an average of 4.1%, while Double-Checker-DS-32B exceeds DeepSeek-Distill-32B by 6.6%.Notably, Double-Checker-DS-32B achieves a significant improvement of 18.2% in pass@1 on the AIME25 benchmark, and Double-Checker-DS-7B boosts performance on AIME24 by 9.7%, demonstrating the effectiveness of Double-Checker on complex reasoning tasks.</p>
<p>Self-critique is a crucial factor in driving performance improvements across benchmarks.</p>
<p>Compared to the "naive SFT" baseline, which utilizes the same training problems but excludes explicit critical data, our Double-Checker consistently shows superior performance across all benchmarks.On average, Double-Checker outperforms the "naive SFT" baseline by 5.7% for the 7B model and 3.5% for the 32B model.These results underscore the pivotal role of self-critique in enhancing the reasoning of long-CoT LLMs.disciplines is derived from S1.1 dataset, and our training problems constitute a proper subset of S1.1.However, our Double-Checker even shows significant improvements over S1.1, achieving a 21.1% performance gain for the 7B model and a 10.9% gain for the 32B model on GPQA.A similar trend is also observed in LiveCodeBench and MMLU-Pro (Business and Law), further validating the generalizability of Double-Checker, given that it was not trained on any related examples (see App. D.1).Additionally, Double-Checker, which relies solely on SFT, performs comparably to models trained with large-scale RL, such as LightR1.These results align with previous findings that smaller models tend to benefit more from SFT than RL [47,40].5 Analysis</p>
<p>Double</p>
<p>The Effect of Self-Critique</p>
<p>To verify the effectiveness of self-critique, we evaluate different model configurations on AIME24.Figure 3 displays the results for both 7B and 32B models under the following settings:</p>
<p>• DS-Distill-Qwen: A distilled long-CoT baseline.</p>
<p>• Naive SFT: Fine-tuned with the same problems as our training set without explicit critical data.</p>
<p>• N=0,1,2,3: Our Double-Checker approach with N = 0, 1, 2, 3 rounds of inference-stage selfcritique and refinement.</p>
<p>We notice that at the 7B and 32B scales, self-critique leads to immediate accuracy gains over the distilled baseline.In the 7B setting, moving from N=0 (no refinement) to N=1 increases performance from 57.3% to 64.6% on AIME24, with further rounds (N=2,3) pushing AIME24 up to 66.5%; by contrast, Naive SFT yields only modest improvements over DS-Distill-Qwen.In the 32B setting, Double-Checker starts with a higher performance even at N=0 (77.7% on AIME24 vs. 72.1%)and achieves 79.8% on AIME24 by N=1, after which performance saturates (79.6%-79.8% on AIME24).This suggests that the 32B model acquires self-critique more rapidly than the 7B model.Additionally, incorporating self-critical data during SFT proves beneficial for enhancing direct inference ability as well (N = 0 vs. "naive SFT").</p>
<p>These results confirm the strong positive impact of self-critique.Even a single refinement round (N=1) consistently brings notable accuracy gains over baselines, and multiple rounds can yield further improvements-particularly at smaller scales (7B).In contrast, [23] shows even decreased performance of N = 1 over N = 0 (direct inference).By explicitly learning to critique and update its reasoning, Double-Checker effectively bridges the gap between "long chain-of-thought generation" and "iterative self-improvement", resulting in strong reasoning power.</p>
<p>Ablation Study of Training Data</p>
<p>To isolate the contributions of different training data components, we ablate whether (i) we include the original direct-inference data (D Direct ), (ii) we use a naive SFT approach without critique data, and (iii) we exclude Qwen3-annotated examples in our curated dataset, (iv) we further collect training examples that are correctly solved after two rounds of self-critique.Table 2 reports the results on multiple math and reasoning benchmarks.</p>
<p>Excluding Direct Inference Data (exclude D Direct ).Removing the original direct-inference examples (Round 0 data) degrades average accuracy to 54.2%.This decline underscores the importance of retaining a portion of direct inference data in the training mix to preserve the model's overall reasoning capability.We believe this occurs because training exclusively on D critique causes the model to lose its ability to perform direct reasoning at N = 0.</p>
<p>Effect of removal of Qwen3 Data (w.o.Qwen3).Removing Qwen3-generated critical examples reduces performance from 61.5% to 61.1% on average.Although not as large a drop as excluding direct data, this shows that Qwen3 training instances further enrich the critique set and boost final performance.We believe that scaling up the critical data could yield further performance gains.</p>
<p>Additional Round-2 critique training data (+ data (N = 2).Adding additional examples that are correctly solved after two rounds of critique during training yields no further improvements.We hypothesize that this is due to the limited sample size (&lt; 400) added, which may be insufficient to influence the training process significantly.</p>
<p>Overall, these results confirm that our curated critique dataset, especially when combined with the original direct-inference examples and auxiliary data from Qwen3, plays a pivotal role in achieving the best performance.In other words, the model benefits from both (i) Conventional long CoT data for maintaining its direct inference ability (N = 0) and (ii) explicit self-critique examples for learning to iteratively refine its solutions.(iii) potentially more diverse critical data.Blue solid line: the per-round average token count, orange dashed line: the cumulative token count over all rounds; green dash-dotted line: the average token consumption for "naive SFT" baseline without iterative refinement.</p>
<p>Analysis of Response Length</p>
<p>The token usage for different rounds of Double-Checker-DS-7B for AIME24 and GPQA is shown in Fig. 4. We run up to three refinement rounds (n = 0, 1, 2, 3) with Double-Checker.We then record both average and cumulative generated tokens used for the full conversation, including thoughts, critiques, and summaries.We also provide the inference time analysis in App.D.2.</p>
<p>On AIME24, the average tokens per round drops sharply: from 13.5k at n = 0 to 4.6k at n = 1, and continues decreasing across subsequent rounds (3.6k at n = 2, 3.1k at n = 3).A similar pattern holds for GPQA, where the initial 10.9k tokens at n = 0 is reduced to roughly 2-3k in the later rounds.Meanwhile, cumulative tokens grows steadily with each additional round: for instance, it rises from 13.5k to 24.9k on AIME24 by n = 3, and from 10.9k to 17.2k on GPQA.Nevertheless, each new round adds far fewer tokens than the first round.Notably, on GPQA, the total tokens spent in our Double-Checker at N = 2 is even smaller than "naive SFT".</p>
<p>Interestingly, the tokens spent on the direct inference of our Double-Checker is fewer than those of "naive SFT" baseline, indicating that incorporating critical data for SFT could also reduce the token consumption at N = 0.While allowing more rounds naturally increases the total token count, each round's contribution is substantially smaller than that of the direct long-CoT baseline.Hence, there is a clear trade-off between improved accuracy (via multiple critique/refine steps) and total token usage.</p>
<p>In practice, we find that even a single or two rounds of refinement often suffice to boost correctness without incurring a prohibitive increase in cumulative tokens.</p>
<p>Conclusion</p>
<p>We have presented Double-Checker, a framework that explicitly enforces LLMs to critique and refine their previous solutions for self-improvement.Our approach integrates generating reflection-like reasoning and actively correcting potential errors through iterative self-critique.Experimental results on multiple mathematical and multidisciplinary benchmarks demonstrate that Double-Checker consistently improves accuracy over comparable baselines, often by large margins.Furthermore, we emphasize the pivotal role of self-critique during inference in enhancing the reasoning capabilities of long-CoT LLMs.Double-Checker demonstrates the value of equipping LLMs with a structured critique space and training them to reflect and refine their outputs, facilitating more reliable self-improvement for complex reasoning tasks.OpenRS &amp; ORZ-Math-Hard The OpenRS dataset contains 7,000 mathematical reasoning problems, and ORZ-Math-Hard comprises 13,000 challenging math problems.Neither dataset provides DeepSeek-R1-annotated reasoning trajectories.To filter simple questions, we generate four responses per question using DeepSeek-R1-Distill-Qwen-7B with temperature 0.6 and retain only those questions where at least two responses are incorrect.For the remaining questions, we generate four responses using DeepSeek-R1-Distill-Qwen-32B with temperature 0.6 and retain only those with at least two incorrect responses.This yields 2.3K difficult problems from OpenRS and 4.4K from ORZ-Math-Hard.</p>
<p>To balance the distribution of correct and incorrect initial summaries (S 0 ), we also exclude correctly solved questions from DeepMath-103K, OpenRS, and ORZ-Math-Hard based on their initial generation by DeepSeek-R1.</p>
<p>B.2 Training Details</p>
<p>We train the DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Qwen-32B our curated training set D train using full-parameter fine-tuning.The 7B model is trained on either 8×H800 GPUs or 8×H20 GPUs, while the 32B model is trained on either 8×H800 GPUs or 32×H20 GPUs.The training process employs DeepSpeed ZeRO optimization [41] (stage 2 for 7B, and stage 3 for 32B) for efficient memory utilization and FlashAttention2 [42] for accelerated training.Following [10], we set the maximum sequence length to 16,384 tokens and use a batch size of 32, with a learning rate of 5 × 10 −6 .Training times are approximately 0.7 hours per epoch for the 7B model on 8×H20 GPUs and 1.1 hours per epoch for the 32B model on 8×H800 GPUs.</p>
<p>B.3 Evaluation Details</p>
<p>We evaluate our models on six benchmarks covering diverse reasoning tasks.For mathematical reasoning, we use:</p>
<p>• AIME24/AIME25: Extremely difficult math competition problems, each dataset contains only 30 examples.• MATH500 [48]: A high school level math problems, the subset of 500 problems is from the original test set of MATH [43].• OlympiadBench [44]: A benchmark of Olympiad-level problems requiring advanced problem-solving skills.We only include the math subset for our evaluation.</p>
<p>For multidisciplinary reasoning, we use GPQA [12], which covers questions spanning biology, physics, and other domains.</p>
<p>Following [10], we adopt an unbiased pass@1 metric for datasets with limited test examples (AIME24 and AIME25), generating 16 samples with decoding temperature 0.6.For other benchmarks, we generate 4 samples per problem to report pass@1.For baseline models, we use the top-p parameter suggested in their original papers, while for our models, we fix top-p = 1.0.Inference is accelerated using vLLM [45], with a maximum sequence length of 32,768 tokens.</p>
<p>C Results Clarification C.1 Results Source</p>
<p>As described in Section 4.2, most of the results for open-source LLMs presented in Table 1 are reproduced through our own experiments.However, for LLMs that require API access (e.g., OpenAI-o1-1217), we have cited the results from their official technical reports or other sources due to budget constraints.Similarly, the results for InftyThink [46] are sourced from their paper, as their code and models are not publicly accessible.</p>
<p>The sources of these results are detailed as follows:</p>
<p>• Results of InftyThink are from their paper [46].</p>
<p>• The results of OpenAI-o1-Preview on AIME24, MATH500 [43], AMC23, GPQA [12], and OlympiadBench-Math [44] are from LIMO [10].• The results of OpenAI-o1-mini, OpenAI-o1-1217, DeepSeek-R1 on AIME24, MATH500, and GPQA are from DeepSeek-R1 report [15].• The result of OpenAI-o1-mini on UGMathBench is from UGMathBench [4].</p>
<p>• The results of OpenAI-o1-Preview, OpenAI-o1-mini, and DeepSeek-R1 on AIME25 are from [49].</p>
<p>For the Self-Refine baseline, we follow a similar setting to the GSM8K dataset in (author?) [29], where the model is prompted with "There is an error in the solution above.To find the error, go through the semantically complete solution and check if everything looks good."after the initial generation.</p>
<p>D More Results</p>
<p>D.1 About Generalizability</p>
<p>To evaluate the generalizability of our Double-Checker, we report additional results for the 7B model on LiveCodeBench [50] and the Business and Law subsets from MMLU-Pro [51].As shown in Table 3, Double-Checker achieves performance improvements on LiveCodeBench, Business, and Law, despite not being trained on any coding-specific or social science examples.We believe that applying Double-Checker to training data from a broader and more diverse range of domains would further enhance its generalizability to previously unseen tasks.In particular, the data curation pipeline and training paradigm of Double-Checker are directly transferable to other domains.
Q + S 0 Q + S 1 Q + S 2</p>
<p>D.2 Additional Computational Costs Analysis</p>
<p>We provide additional details on computational costs in this appendix.Table 4 summarizes the total number of input tokens per round, including both the problem statement Q and the previous summary S N −1 .Our design discards intermediate reasoning content, retaining only summaries for subsequent rounds, which minimizes input size relative to output size.For N = 0, the input consists solely of Q, while for N ≥ 1, the input comprises Q + S N −1 .As shown, input tokens are negligible compared to output tokens.</p>
<p>Additionally, Table 5 reports the average total inference time for Double-Checker on AIME24, using a single H20 GPU.The inference time per response for the "Naive SFT baseline" is 9.42 seconds, whereas Double-Checker achieves cumulative averages between 7.78 seconds (N = 0) and 9.47 seconds (N = 1).These results demonstrate that the added rounds do not significantly increase computational cost.</p>
<p>Figure 1 :
1
Figure 1: Double-Checker correctly solves a math problem in AIME24 leveraging self-critique, while DeepSeek-Qwen-7B still gets the same wrong answer under a self-critical probe.</p>
<p>Figure 2 :
2
Figure 2: The overview of Double-Checker.(a) Direct inference pipeline of long-CoT LLMs: generating a long thought (T 0 ) followed by a summary (S 0 ) that concludes the answer (A 0 ) for the question (Q).(b) The inference pipeline of iterative refinement with self-critique.(c) Training stage of our Double-Checker.(d) Adaptive inference with self-critique of our Double-Checker.</p>
<p>[40]  and DeepSeek-R1[15], i.e., T = C = R, but with different instructions.We also incorporate a subset of S1.1 training instances as our D direct , resulting in a total training set D train = D direc ∪ D critique of 1,730 training instances.The details of our data sources and filtering process are given in App.B.1.</p>
<p>Figure 3 :
3
Figure 3: Accuracy comparisons on AIME24 for two model sizes (7B and 32B).We compare: (1) DS-Distill-Qwen (a distilled baseline), (2) Naive SFT (fine-tuning without explicit critique), and (3) Double-Checker with varying rounds of self-critique (N = 0, 1, 2, 3).</p>
<p>Figure 4 :
4
Figure4: Token usage for AIME24 (left) and GPQA (right).Blue solid line: the per-round average token count, orange dashed line: the cumulative token count over all rounds; green dash-dotted line: the average token consumption for "naive SFT" baseline without iterative refinement.</p>
<p>Figure 9 :
9
Figure 9: The Padding Template of Training Examples, where ANSWER is A 0 .</p>
<p>Long-CoT LLM M, number of iterations N 1: Generate initial output T0 ⊕ S0 ∼ PM(•|Q) ▷Direct Inference 2: for n ← 1 to N do 3: Critique previous summary and refine Cn ⊕ Tn ⊕ Sn ∼ PM(•|Q ⊕ Sn−1) ▷Self-Critique &amp; Refine 4:if Cn indicates that An−1 (the answer of Sn−1) is correct then
▷Stopping Criteria5:return Sn−16:end if7: end for8: return SN
2 (b)).Concretely: Algorithm 1 Double-Checker Inference PipelineRequire: Question Q,</p>
<p>Table 1 :
1
Main results (in %) on various benchmarks.The best results within each group are in bold.<em> indicates that the results of the corresponding LLM are sourced from their technical reports or other references due to the cost of using APIs or the unavailability of the LLM.Please refer to Appendix C.1 for corresponding references.The remaining results are from our own runs.
ModelAIME24 AIME25 MATH500 Olympiad GPQA AVGOpenAI-o1-Preview</em>44.637.985.552.173.3-OpenAI-o1-mini<em>63.653.890.0-60.0-OpenAI-o1-1217</em>79.2-96.4-75.7-DeepSeek-R1<em>79.870.097.3-71.5-7B ModelsS1.1-7B17.519.680.742.841.340.4InftyThink-7B</em>40.0-91.7-51.9-LightR1-7B-DS57.145.490.359.023.155.0DeepSeek-R1-Distill-7B (initial model)56.743.792.259.035.457.4DeepSeek-R1-Distill-7B + self-refine55.037.591.658.436.855.9DeepSeek-R1-Distill-7B + ThinkTwice58.742.992.358.735.557.6DeepSeek-R1-Distill-7B + Wait57.544.393.158.836.658.1Double-Checker-DS-7B naive SFT57.143.391.458.628.755.8Double-Checker-DS-7B66.448.192.760.040.461.532B ModelsLIMO-32B57.150.893.066.164.566.3S1.1-32B56.747.592.958.866.864.5InftyThink-32B*62.5-96.0-65.6-QwQ-32B-Preview44.234.289.763.658.858.1LightR1-32B-DS76.665.495.267.568.774.7DeepSeek-R1-Distill-32B (initial model)72.150.493.563.064.968.8DeepSeek-R1-Distill-32B + ThinkTwice72.954.893.563.364.469.8DeepSeek-R1-Distill-32B + wait72.853.494.263.865.770.0Double-Checker-DS-32B naive SFT74.660.493.467.263.871.9Double-Checker-DS-32B79.868.694.368.266.075.4AIME24 Accuracy (%)58 60 62 64 66 6856.7 57.1 57.37B64.662.766.5AIME24 Accuracy (%)72 74 76 78 8072.174.677.732B79.8 79.6 79.856AIME24AIME24DS-Distill-QwenNaive SFTN=0N=1N=2N=3
-Checker demonstrates strong generalizability.Although the training data primarily consists of math reasoning problems, Double-Checker achieves remarkable performance on GPQA, a multidisciplinary QA benchmark.Notably, the only source of reasoning problems from other</p>
<p>Table 2 :
2
Ablation study (in %) on Training Data.
ModelAIME24 AIME25 MATH500 Olympiad GPQA AVGDeepSeek-R1-Distill-7B56.743.792.259.035.457.4Double-Checker-DS-7B naive SFT57.143.391.458.628.755.8Double-Checker-DS-7B exclude D Direct57.544.288.957.323.354.2Double-Checker-DS-7B w.o. Qwen362.045.691.659.739.959.8Double-Checker-DS-7B66.448.192.760.040.461.5Double-Checker-DS-7B + data (N=2)66.746.793.258.640.061.1</p>
<p>Prompt for Generating Critiques of Correct AnswersYou are tasked with analyzing your last solution to a problem and providing constructive feedback based on previous solutions.Do NOT provide direct solutions.You have already know your last solution to the problem is correct.Important: Do NOT mention something like "you have already know your last solution is correct" in your feedback.
Structure your response using the following format (without <format> tags):<format>Analysis:{Analysis}Improvement suggestions:{Suggestions}Overall judgment:{Correct}</format>Figure 8: The Prompt for Generating Critiques of Correct Answers.Padding TemplateT ′ 1 :<think>From my last analysis, I have already got the right answer.</think>S ′ 1 :<summary>My previous solution is correct. Therefore, the answer is AN SW ER .</summary></p>
<p>Table 3 :
3
Performance comparison across different benchmarks.
ModelGPQA LiveCodeBench Business LawDeepSeek-R1-Distill-7B (Our initial model)35.438.461.114.3DeepSeek-R1-Distill-7B naive SFT28.738.561.714.9Double-Checker-7B40.439.464.317.2</p>
<p>Table 4 :
4
Total number of input tokens (including problem and previous summary) per round.
N=0N=1N=2N=3avg. input tokens 450970740700input structureQ</p>
<p>Table 5 :
5
The Average Inference time for Double-Checker on AIME24 using one H20 GPU.
N=0 N=1 N=2N=3avg time for this round 7.78 1.69 1.490.95avg cumulative time7.78 9.47 10.96 11.92
AcknowledgmentsThis work was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project Reference Number: AoE/E-601/24-N).A Detailed Settings of Double-CheckerA.1 Setting of Meta-ExperimentsFor the experiment in Sec.3.1, we use the following probe to induce self-critique ability, which is adapted from[23].The evaluation setting on AIME24 aligns with our main experiments (see App. B.3).To evaluate whether the long CoT LLMs can generate informative critiques using the probe, we examine whether their generated content attempts to assess the correctness of their prior solution.If they do not engage in this answer judgment, we consider the critique to be informative.The Critique ProbePlease critique each reasoning step of your previous solution to this problem and explain whether your solution is correct or not.A.2 The Critique SpaceOur critique consists of three components: 1) an analysis of the summary, 2) actionable improvement suggestions, 3) an answer correctness judgment (correct/incorrect).We showcase one such example in Fig.6.A.3 Critique GenerationTo ensure the quality of critiques, we employ different prompts for correct and incorrect A 0 (see Sec. 3.2.1).The prompts are given in Fig.7 and 8.A.4 Distillation DetailsAfter the data curation stage, training examples are categorized into two formats: (Q, S 0 , C 1 ) for correct answers A 0 and (Q, S 0 , C 1 , T 1 , S 1 ) for incorrect answers.For simplicity, instances with (Q, S 0 , C 1 ) are padded to (Q, S 0 , C 1 , T ′ 1 , S ′ 1 ) using a predefined template (see Fig.9).Here, T ′ 1 and S ′ 1 are negligible compared to the original reasoning T 0 and summary S 0 .During inference (as described in Alg.1), if a critique C n predicts correctness, T n and S n can be disregarded since the extracted answer A n from S n will match S n−1 .B Detailed Experimental Setup B.1 Training DataThis appendix outlines the filtering criteria applied to our candidate question pool, focusing on two primary principles: difficulty and answer verifiability.We detail the filtering pipeline for each dataset source as follows:S1.1 Dataset The dataset comprises 1,000 high-quality questions together with their ground-truth answers sourced from multiple domains, with the reasoning trajectory annotated by DeepSeek-R1.However, some questions with open-ended answers can not be reliably evaluated using a rule-basedThe Design of Critque Space <critique> Analysis:The initial approach incorrectly assumes that "exactly two" and "exactly three" categories can be directly summed with those numbers to derive the count of residents owning all four items.This overlooks the distinction between "exactly two" (and "exactly three") and the inclusion of the universal set (candy hearts).The problem requires applying inclusion-exclusion principles to account for overlaps among the four items, not just summing those who own exactly two or three.The solution conflates the total population with direct arithmetic of exclusive categories, leading to an invalid conclusion.Improvement suggestions:1. Re-examine the definitions of "exactly two" and "exactly three" in the context of a universal set (candy hearts), recognizing that these groups inherently include ownership of the universal item.2. Use inclusion-exclusion formulas to model overlaps between the four categories systematically.3. Differentiate between residents owning all four items and those owning subsets (e.g., two specific items plus candy hearts).4. Set up equations based on known totals (e.g., total residents, counts for each item) to solve for unknown overlaps.evaluation framework.After the removal of these unverifiable instances, we retain 861 questions with both validated answers and corresponding DeepSeek-R1 annotated reasoning trajectories.We will also use this subset as our D direc .Overall judgment:DeepMath-103K It contains 103K math problems, each annotated with two reasoning paths generated by DeepSeek-R1.We retain only 0.6K questions where the DeepSeek-R1-annotated reasoning paths are judged to be incorrect compared to ground truth answers by a rule-based evaluation method.We bypass the initial generation stage of Double-Checker and instead initialize the reasoning process directly from the DeepSeek-R1-annotated reasoning paths.
. A Edward, Julian Feigenbaum, Feldman, Computers and thought. 1963</p>
<p>Meredith Ringel, Morris , Jascha Sohl-Dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Farabet, Shane Legg, arXiv:2311.02462Levels of agi for operationalizing progress on the path to agi. 2023arXiv preprint</p>
<p>Benchmarking multi-discipline cognitive reasoning for superintelligent ai. Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Advances in Neural Information Processing Systems. 202437</p>
<p>Ugmathbench: A diverse and dynamic benchmark for undergraduate-level mathematical reasoning with large language models. Xin Xu, Jiaxin Zhang, Tianhao Chen, Zitong Chao, Jishan Hu, Can Yang, arXiv:2501.137662025arXiv preprint</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Li, Daya Wu, Guo, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, arXiv:2409.12122Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. 2024arXiv preprint</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, abs/2309.12284ArXiv preprint. 2023</p>
<p>Can llms solve longer math word problems better? ArXiv preprint. Xin Xu, Tong Xiao, Zitong Chao, Zhenya Huang, Can Yang, Yang Wang, abs/2405.148042024</p>
<p>Dart-math: Difficultyaware rejection tuning for mathematical problem-solving. Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, Junxian He, abs/2407.136902024ArXiv preprint</p>
<p>Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, Pengfei Liu, Limo, arXiv:2502.03387Less is more for reasoning. 2025arXiv preprint</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, First Conference on Language Modeling. 2024</p>
<p>Humanity's last exam. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, abs/2501.142492025ArXiv preprint</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Kimi k1. 5: Scaling reinforcement learning with llms. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, arXiv:2501.125992025arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. S Sanmi Koyejo, A Mohamed, Danielle Agarwal, K Belgrave, A Cho, Oh, NeurIPS; New Orleans, LA, USA2022. 2022. November 28 -December 9, 2022, 2022</p>
<p>Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D Goodman, arXiv:2503.013072025arXiv preprint</p>
<p>Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yunjie Ji, Yiping Peng, Han Zhao, Xiangang Li, arXiv:2503.19855Think twice: Enhancing llm reasoning by scaling multi-round test-time thinking. 2025arXiv preprint</p>
<p>Beyond the last answer: Your reasoning trace uncovers more than you think. Hasan Abed, Al Kader Hammoud, Hani Itani, Bernard Ghanem, arXiv:2504.207082025arXiv preprint</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382023arXiv preprint</p>
<p>Teaching language models to critique via reinforcement learning. Zhihui Xie, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong, arXiv:2502.034922025arXiv preprint</p>
<p>Critique fine-tuning: Learning to critique is more effective than learning to imitate. Yubo Wang, Xiang Yue, Wenhu Chen, arXiv:2501.177032025arXiv preprint</p>
<p>Verify-and-edit: A knowledge-enhanced chain-of-thought framework. Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, Lidong Bing, arXiv:2305.032682023arXiv preprint</p>
<p>Can we verify step by step for incorrect answer detection?. Xin Xu, Shizhe Diao, Can Yang, Yang Wang, arXiv:2402.105282024arXiv preprint</p>
<p>Dancing with critiques: Enhancing llm reasoning with stepwise natural language self-critique. Yansi Li, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Qiuzhi Liu, Rui Wang, Zhuosheng Zhang, Zhaopeng Tu, Haitao Mi, arXiv:2503.173632025arXiv preprint</p>
<p>Deepcritic: Deliberate critique with large language models. Wenkai Yang, Jingwen Chen, Yankai Lin, Ji-Rong Wen, arXiv:2505.006622025arXiv preprint</p>
<p>Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, arXiv:2503.10460Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. 2025arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Solving math word problems with process-and outcome-based feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, arXiv:2211.142752022arXiv preprint</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, arXiv:2409.12122Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. 2024arXiv preprint</p>
<p>Math-shepherd: Verify and reinforce llms step-by-step without human annotations. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, Zhifang Sui, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, Hao Peng, arXiv:2412.01981Free process rewards without process labels. 2024arXiv preprint</p>
<p>Jonathan Daniel Chang, and Prithviraj Ammanabrolu. Critique-out-loud reward models. Zachary Ankner, Mansheej Paul, Brandon Cui, Pluralistic Alignment Workshop at NeurIPS. 2024</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, arXiv:2310.017982023arXiv preprint</p>
<p>Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, arXiv:2504.11456Deepmath-103k: A large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. 2025arXiv preprint</p>
<p>Reinforcement learning for reasoning in small llms: What works and what doesn. Quy-Anh Dang, Chris Ngo, arXiv:2503.162192025t. arXiv preprint</p>
<p>Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, Heung-Yeung Shum, arXiv:2503.242902025arXiv preprint</p>
<p>Zero: Memory optimizations toward training trillion parameter models. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He, SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE2020</p>
<p>Flashattention-2: Faster attention with better parallelism and work partitioning. Tri Dao, arXiv:2307.086912023arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, arXiv:2402.140082024arXiv preprint</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the 29th Symposium on Operating Systems Principles. the 29th Symposium on Operating Systems Principles2023</p>
<p>Inftythink: Breaking the length limits of long-context reasoning in large language models. Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Mengdi Zhang, Jian Shao, Yueting Zhuang, 2503.066922025</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Deepseek-Ai , 2025</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, The Twelfth International Conference on Learning Representations. Bowen Baker, Teddy LeeJan. 2023</p>
<p>Aime-preview: A rigorous and immediate evaluation framework for advanced mathematical reasoning. Yixin Ye, Yang Xiao, Tiantian Mi, Pengfei Liu, 2025GitHub repository</p>
<p>Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, arXiv:2403.079742024arXiv preprint</p>
<p>MMLU-pro: A more robust and challenging multi-task language understanding benchmark. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
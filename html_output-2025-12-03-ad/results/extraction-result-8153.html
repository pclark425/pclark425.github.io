<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8153 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8153</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8153</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-278996512</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.23701v1.pdf" target="_blank">Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation</a></p>
                <p><strong>Paper Abstract:</strong> Final-answer-based metrics are commonly used for evaluating large language models (LLMs) on math word problems, often taken as proxies for reasoning ability. However, such metrics conflate two distinct sub-skills: abstract formulation (capturing mathematical relationships using expressions) and arithmetic computation (executing the calculations). Through a disentangled evaluation on GSM8K and SVAMP, we find that the final-answer accuracy of Llama-3 and Qwen2.5 (1B-32B) without CoT is overwhelmingly bottlenecked by the arithmetic computation step and not by the abstract formulation step. Contrary to the common belief, we show that CoT primarily aids in computation, with limited impact on abstract formulation. Mechanistically, we show that these two skills are composed conjunctively even in a single forward pass without any reasoning steps via an abstract-then-compute mechanism: models first capture problem abstractions, then handle computation. Causal patching confirms these abstractions are present, transferable, composable, and precede computation. These behavioural and mechanistic findings highlight the need for disentangled evaluation to accurately assess LLM reasoning and to guide future improvements.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8153.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8153.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3 (8B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned transformer language model from the Llama-3 family evaluated in this paper; subjected to disentangled evaluation and mechanistic probing showing an abstract-then-compute internal process for arithmetic on math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3 8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned Llama-3 transformer model, 8 billion parameters; evaluated zero-shot (no-CoT and CoT) on GSM8K and SVAMP and used for mechanistic analyses (logit attribution and activation patching) in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step grade-school math word problems (GSM8K, 2-8 steps) and single-step problems (SVAMP); also 1-2 step synthetic templates and a two-operator dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Abstract-then-compute: mid-layer residual stream encodes an abstract operator representation (operator tokens/logits) at ~layers 13-14; operands are moved to the last-position residual around layers 15-16; MLP-dominated computation/execution stage peaks around layer 18; abstractions are represented as transferable symbolic operator representations in mid layers.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Logit attribution (logit lens over attention/MLP/residual subpoints) to track operator/operand/answer logits across layers; activation (causal) patching and cross-prompt patching to causally test and transfer representations between clean/corrupted runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Disentangled evaluation: Llama-3 (8B) Symbolic Abstraction accuracy reported ≈45.7% (no-CoT, GSM8K, Table 6), Original final-answer accuracy lower and bottlenecked by computation; CoT yields large computation gains (paper reports computation gains up to +62.8% in aggregate), but limited abstraction gains (e.g., +6.7% symbolic, +17.6% numerical reported as examples). On two-operator dataset Llama-3 8B achieved only 16.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Primary failure mode is arithmetic computation errors (execution) rather than abstraction; multi-step numeric evaluation is the main bottleneck; symbolic abstraction is imperfect (far from 100%); poor performance on two-operator multi-step cases; analysis restricted to single-token prediction points and models ≤12B.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Logit attribution shows operator token logits rising and diverging for addition vs subtraction at layers ~13-14; attention outputs move inferred operator towards the last token position around L13-14; operand-related logits increase around L15-16; MLP patching at L18 has the largest causal effect on final answer; activation patching of clean abstraction-layer states into corrupted runs restores/changes final answers, and cross-prompt patching of symbolic abstractions flips predictions, demonstrating transferability and causal role of mid-layer abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Abstraction representations are not perfect—symbolic abstraction accuracy remains well below 100%; interventions and analyses are limited to single-token last-position behaviors, so multi-token chain behaviors are not fully addressed; findings reported up to 8B (and generally up to 12B) and may not directly generalize to much larger models; arithmetic execution remains a dominant failure case even when abstraction is present.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8153.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8153.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5 family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-2.5 (3B, 7B, 14B, 32B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of instruction-tuned transformer models (Qwen-2.5) analyzed across sizes for disentangled abstraction vs computation performance and, for some sizes, mechanistic layerwise behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5 (3B, 7B, 14B, 32B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned Qwen-2.5 transformer models evaluated at multiple sizes (3B, 7B, 14B, 32B). The paper reports both behavioural (disentangled evaluation) and mechanistic analyses (for 7B and 14B) showing the same abstract-then-compute pattern shifted to deeper layers with scale.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>GSM8K multi-step problems, SVAMP single-step problems, and 1-2 step synthetic templates; symbolic/numerical abstraction variants and arithmetic computation variants were used.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Same abstract-then-compute structure observed; abstraction emerges in deeper layers as model size increases (e.g., abstraction at ~layers 18-20 for Qwen-2.5 7B, computation starting ~22-23; for Qwen-2.5 14B abstraction at ~29-32, computation starting ~36); mid-layer residuals encode operator abstractions transferable across forms.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Logit attribution (logit lens) and activation patching / cross-prompt patching to identify abstraction and computation layers and to test causal transfer of symbolic abstractions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Behavioural disentangled results: Qwen models often achieve higher symbolic abstraction accuracy than final-answer Original accuracy; example numbers: Qwen-32B Symbolic Abstraction reported ≈76.8% (paper text); Table 6 shows Qwen-7B Original accuracy (no-CoT) ≈61.5% and with CoT ≈74.7%. CoT strongly improves computation performance (paper reports computation gains up to +62.8% aggregate), while improving abstraction modestly.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Arithmetic execution remains the bottleneck for final answers; abstraction performance improves with scale but is not perfect; single-token/single-pass analyses may miss multi-token reasoning behaviors; two-operator problems remain challenging for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Layerwise logit attribution and activation patching show consistent abstraction onset and computation onset at deeper layers as model size increases (explicit layer ranges given above); cross-patch experiments on Qwen models reproduce abstraction transfer and flipping of predictions across symbolic/numeric variants.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Although abstraction layers shift deeper with scale, symbolic abstraction accuracy is not perfect; larger models (e.g., Qwen-32B) show better abstraction but still can fail at computation; mechanistic analyses limited to select sizes (7B, 14B) in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8153.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8153.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Disentangled Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Disentangled Evaluation Framework (Abstract formulation vs Arithmetic computation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation design that decomposes math word problem solving into abstract formulation (mapping Q→E) and arithmetic computation (E→A) and tests each independently via Symbolic Abstraction, Numerical Abstraction, and Arithmetic Computation subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to Llama-3 (1B/3B/8B) and Qwen-2.5 (3B/7B/14B/32B) in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model; an evaluation methodology applied to instruction-tuned transformer LLMs; used symbolic variants (variables x,y...), numerical expression variants, and arithmetic execution variants to isolate sub-skills.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Symbolic abstraction (derive expression with symbolic variables), numerical abstraction (derive numeric expression using given numbers but not calculate), and arithmetic computation (evaluate given expressions); tasks instantiated on GSM8K and SVAMP.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Framework reveals that abstraction (formulation) and computation (execution) are conjunctive components of final-answer prediction; representation-level claims: abstractions are encoded mid-layer residuals and are transferable across surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Behavioral evaluation via designed subtasks, plus mechanistic probes (logit attribution, activation patching, cross-prompt patching) to link behavioural differences to internal representations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Across models, abstraction subtasks (symbolic/numerical) yield higher accuracy than arithmetic computation subtasks in multi-step GSM8K without CoT; reported example numbers: Llama-3 8B symbolic 45.7% (no-CoT) vs lower Original final-answer; CoT yields much larger computation gains (e.g., +62.8%) than abstraction gains (+6.7% symbolic, +17.6% numerical) in aggregate analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Final-answer accuracy conflates abstraction and computation errors; standard evaluations can misattribute arithmetic execution failures as reasoning deficits; symbolic abstraction evaluation sensitive but not fully immune to surface perturbations (symbol order/choice causes mild drops).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Behavioral separations showing higher abstraction accuracy than computation accuracy; CoT effect decomposed to show most improvement in computation; validation via mechanistic patching indicates internal mid-layer abstractions causally contribute to final outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Symbolic abstraction is robust but not perfect (ablation shows small drops with reversed/random symbol choices); evaluation limited to English GSM8K/SVAMP and single-pass generation; some single-step SVAMP arithmetic tasks are easier and show different patterns (e.g., small models can compute single-step expressions well).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8153.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8153.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logit Attribution (Logit Lens)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logit Attribution / Logit Lens</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A layerwise diagnostic that projects hidden states to vocabulary logits (unembedding) at specific sub-layer points to track where token-level information (operators/operands/answers) arises across layers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to Llama-3 8B and Qwen-2.5 (7B, 14B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diagnostic method (not a model) applied at attention/MLP/residual subpoints for the last token position; computes ⟨W_U[t], LN(h)⟩ to attribute logits.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to track operator detection (e.g., '+' vs '-') and operand/answer token logits for single- and two-operator problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Revealed that operator representations appear earlier in middle layers (operator logits diverge at L13-14 in Llama-3 8B) and that operand/answer logits rise subsequently; shows attention moves operator/operand info to the last position.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Direct projection of hidden states to vocab logits at multiple points per layer (attention output, MLP output, residual mid, residual final).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Provides layerwise likelihood/logit trajectories rather than scalar accuracy; used to identify abstraction onset (layer ranges) and to correlate with behavioural outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Limited to single-token attribution at monitored positions; may not capture multi-step token generation dynamics; dependent on quality of unembedding projection.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Observed consistent logit patterns across templates and models: operator logits rise around identified abstraction layers, operand logits rise in subsequent layers, and answer logits peak after MLP-dominated computation layers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Logit attribution captures correlational patterns; causal claims require activation patching; single-token focus and layer-point choices can miss distributed dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8153.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8153.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Activation Patching / Cross-Prompt Patching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Activation Patching (causal patching) and Cross-Prompt Patching</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Causal intervention technique that replaces single-layer activations from a 'clean' run into a 'corrupted' run to measure how much that state restores the clean prediction; cross-prompt patching transfers symbolic abstraction states between different surface forms to test compositionality and transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to Llama-3 8B and Qwen-2.5 (7B, 14B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Interventional method applied at attention/MLP/final outputs across layers at the last position, computing normalized patching effect scores (0-1) based on logit differences between clean and corrupted runs.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to causally probe operator abstraction (changing logic) and computation (changing operands) across single- and two-operator problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Identifies layers causally responsible for abstraction (L13-14 Llama-3 8B) vs operand integration (L15-16) vs computation/execution (L18, MLP peak); cross-patch shows symbolic abstraction states are transferable and can be composed with corrupted operands to produce target arithmetic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Activation patching (Algorithm 1 in paper) and cross-prompt patching (patch clean symbolic-layer states into numerical corrupted runs), tracking per-layer log-probabilities and normalized recovery scores.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Yields per-layer patching effect magnitudes (normalized 0-1) and layerwise log-prob trajectories showing when target answer probability rises/drops; used to demonstrate causal roles of specific layers rather than reporting end-task accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Interventions are single-state replacements and so may under-represent distributed contributions; patching limited to single-token last-position behavior and may not capture multi-token rollouts; compute budget limited analyses to models ≤12B.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Patching abstraction-layer activations from additions into corrupted subtraction runs raises the target (clean) answer log-prob starting at abstraction onset layers and can flip final answers; MLP-layer patching (e.g., L18) yields strongest recovery for numeric computation, confirming layered functional separation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Patching cannot fully restore behaviour if multiple distributed components contribute; cross-prompt patching results show drops in overall log-prob because symbolic runs predict 'x' as first token (so absolute log-probabilities fall), complicating direct interpretation; results restricted to single-token causal tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>interpreting gpt: the logit lens <em>(Rating: 2)</em></li>
                <li>Patchscopes: A unifying framework for inspecting hidden representations of language models <em>(Rating: 2)</em></li>
                <li>Locating and editing factual associations in GPT <em>(Rating: 2)</em></li>
                <li>A careful examination of large language model performance on grade school arithmetic <em>(Rating: 2)</em></li>
                <li>Towards best practices of activation patching in language models: Metrics and methods <em>(Rating: 2)</em></li>
                <li>Interpreting and improving large language models in arithmetic calculation <em>(Rating: 1)</em></li>
                <li>Arithmetic without algorithms: Language models solve math with a bag of heuristics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8153",
    "paper_id": "paper-278996512",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Llama-3-8B",
            "name_full": "Llama-3 (8B)",
            "brief_description": "Instruction-tuned transformer language model from the Llama-3 family evaluated in this paper; subjected to disentangled evaluation and mechanistic probing showing an abstract-then-compute internal process for arithmetic on math word problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3 8B",
            "model_description": "Instruction-tuned Llama-3 transformer model, 8 billion parameters; evaluated zero-shot (no-CoT and CoT) on GSM8K and SVAMP and used for mechanistic analyses (logit attribution and activation patching) in this paper.",
            "arithmetic_task_type": "Multi-step grade-school math word problems (GSM8K, 2-8 steps) and single-step problems (SVAMP); also 1-2 step synthetic templates and a two-operator dataset.",
            "mechanism_or_representation": "Abstract-then-compute: mid-layer residual stream encodes an abstract operator representation (operator tokens/logits) at ~layers 13-14; operands are moved to the last-position residual around layers 15-16; MLP-dominated computation/execution stage peaks around layer 18; abstractions are represented as transferable symbolic operator representations in mid layers.",
            "probing_or_intervention_method": "Logit attribution (logit lens over attention/MLP/residual subpoints) to track operator/operand/answer logits across layers; activation (causal) patching and cross-prompt patching to causally test and transfer representations between clean/corrupted runs.",
            "performance_metrics": "Disentangled evaluation: Llama-3 (8B) Symbolic Abstraction accuracy reported ≈45.7% (no-CoT, GSM8K, Table 6), Original final-answer accuracy lower and bottlenecked by computation; CoT yields large computation gains (paper reports computation gains up to +62.8% in aggregate), but limited abstraction gains (e.g., +6.7% symbolic, +17.6% numerical reported as examples). On two-operator dataset Llama-3 8B achieved only 16.5%.",
            "error_types_or_failure_modes": "Primary failure mode is arithmetic computation errors (execution) rather than abstraction; multi-step numeric evaluation is the main bottleneck; symbolic abstraction is imperfect (far from 100%); poor performance on two-operator multi-step cases; analysis restricted to single-token prediction points and models ≤12B.",
            "evidence_for_mechanism": "Logit attribution shows operator token logits rising and diverging for addition vs subtraction at layers ~13-14; attention outputs move inferred operator towards the last token position around L13-14; operand-related logits increase around L15-16; MLP patching at L18 has the largest causal effect on final answer; activation patching of clean abstraction-layer states into corrupted runs restores/changes final answers, and cross-prompt patching of symbolic abstractions flips predictions, demonstrating transferability and causal role of mid-layer abstraction.",
            "counterexamples_or_challenges": "Abstraction representations are not perfect—symbolic abstraction accuracy remains well below 100%; interventions and analyses are limited to single-token last-position behaviors, so multi-token chain behaviors are not fully addressed; findings reported up to 8B (and generally up to 12B) and may not directly generalize to much larger models; arithmetic execution remains a dominant failure case even when abstraction is present.",
            "uuid": "e8153.0",
            "source_info": {
                "paper_title": "Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen-2.5 family",
            "name_full": "Qwen-2.5 (3B, 7B, 14B, 32B)",
            "brief_description": "A family of instruction-tuned transformer models (Qwen-2.5) analyzed across sizes for disentangled abstraction vs computation performance and, for some sizes, mechanistic layerwise behavior.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5 (3B, 7B, 14B, 32B)",
            "model_description": "Instruction-tuned Qwen-2.5 transformer models evaluated at multiple sizes (3B, 7B, 14B, 32B). The paper reports both behavioural (disentangled evaluation) and mechanistic analyses (for 7B and 14B) showing the same abstract-then-compute pattern shifted to deeper layers with scale.",
            "arithmetic_task_type": "GSM8K multi-step problems, SVAMP single-step problems, and 1-2 step synthetic templates; symbolic/numerical abstraction variants and arithmetic computation variants were used.",
            "mechanism_or_representation": "Same abstract-then-compute structure observed; abstraction emerges in deeper layers as model size increases (e.g., abstraction at ~layers 18-20 for Qwen-2.5 7B, computation starting ~22-23; for Qwen-2.5 14B abstraction at ~29-32, computation starting ~36); mid-layer residuals encode operator abstractions transferable across forms.",
            "probing_or_intervention_method": "Logit attribution (logit lens) and activation patching / cross-prompt patching to identify abstraction and computation layers and to test causal transfer of symbolic abstractions.",
            "performance_metrics": "Behavioural disentangled results: Qwen models often achieve higher symbolic abstraction accuracy than final-answer Original accuracy; example numbers: Qwen-32B Symbolic Abstraction reported ≈76.8% (paper text); Table 6 shows Qwen-7B Original accuracy (no-CoT) ≈61.5% and with CoT ≈74.7%. CoT strongly improves computation performance (paper reports computation gains up to +62.8% aggregate), while improving abstraction modestly.",
            "error_types_or_failure_modes": "Arithmetic execution remains the bottleneck for final answers; abstraction performance improves with scale but is not perfect; single-token/single-pass analyses may miss multi-token reasoning behaviors; two-operator problems remain challenging for some models.",
            "evidence_for_mechanism": "Layerwise logit attribution and activation patching show consistent abstraction onset and computation onset at deeper layers as model size increases (explicit layer ranges given above); cross-patch experiments on Qwen models reproduce abstraction transfer and flipping of predictions across symbolic/numeric variants.",
            "counterexamples_or_challenges": "Although abstraction layers shift deeper with scale, symbolic abstraction accuracy is not perfect; larger models (e.g., Qwen-32B) show better abstraction but still can fail at computation; mechanistic analyses limited to select sizes (7B, 14B) in this paper.",
            "uuid": "e8153.1",
            "source_info": {
                "paper_title": "Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Disentangled Evaluation",
            "name_full": "Disentangled Evaluation Framework (Abstract formulation vs Arithmetic computation)",
            "brief_description": "An evaluation design that decomposes math word problem solving into abstract formulation (mapping Q→E) and arithmetic computation (E→A) and tests each independently via Symbolic Abstraction, Numerical Abstraction, and Arithmetic Computation subtasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to Llama-3 (1B/3B/8B) and Qwen-2.5 (3B/7B/14B/32B) in this paper",
            "model_description": "Not a model; an evaluation methodology applied to instruction-tuned transformer LLMs; used symbolic variants (variables x,y...), numerical expression variants, and arithmetic execution variants to isolate sub-skills.",
            "arithmetic_task_type": "Symbolic abstraction (derive expression with symbolic variables), numerical abstraction (derive numeric expression using given numbers but not calculate), and arithmetic computation (evaluate given expressions); tasks instantiated on GSM8K and SVAMP.",
            "mechanism_or_representation": "Framework reveals that abstraction (formulation) and computation (execution) are conjunctive components of final-answer prediction; representation-level claims: abstractions are encoded mid-layer residuals and are transferable across surface forms.",
            "probing_or_intervention_method": "Behavioral evaluation via designed subtasks, plus mechanistic probes (logit attribution, activation patching, cross-prompt patching) to link behavioural differences to internal representations.",
            "performance_metrics": "Across models, abstraction subtasks (symbolic/numerical) yield higher accuracy than arithmetic computation subtasks in multi-step GSM8K without CoT; reported example numbers: Llama-3 8B symbolic 45.7% (no-CoT) vs lower Original final-answer; CoT yields much larger computation gains (e.g., +62.8%) than abstraction gains (+6.7% symbolic, +17.6% numerical) in aggregate analyses.",
            "error_types_or_failure_modes": "Final-answer accuracy conflates abstraction and computation errors; standard evaluations can misattribute arithmetic execution failures as reasoning deficits; symbolic abstraction evaluation sensitive but not fully immune to surface perturbations (symbol order/choice causes mild drops).",
            "evidence_for_mechanism": "Behavioral separations showing higher abstraction accuracy than computation accuracy; CoT effect decomposed to show most improvement in computation; validation via mechanistic patching indicates internal mid-layer abstractions causally contribute to final outputs.",
            "counterexamples_or_challenges": "Symbolic abstraction is robust but not perfect (ablation shows small drops with reversed/random symbol choices); evaluation limited to English GSM8K/SVAMP and single-pass generation; some single-step SVAMP arithmetic tasks are easier and show different patterns (e.g., small models can compute single-step expressions well).",
            "uuid": "e8153.2",
            "source_info": {
                "paper_title": "Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Logit Attribution (Logit Lens)",
            "name_full": "Logit Attribution / Logit Lens",
            "brief_description": "A layerwise diagnostic that projects hidden states to vocabulary logits (unembedding) at specific sub-layer points to track where token-level information (operators/operands/answers) arises across layers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to Llama-3 8B and Qwen-2.5 (7B, 14B)",
            "model_description": "Diagnostic method (not a model) applied at attention/MLP/residual subpoints for the last token position; computes ⟨W_U[t], LN(h)⟩ to attribute logits.",
            "arithmetic_task_type": "Used to track operator detection (e.g., '+' vs '-') and operand/answer token logits for single- and two-operator problems.",
            "mechanism_or_representation": "Revealed that operator representations appear earlier in middle layers (operator logits diverge at L13-14 in Llama-3 8B) and that operand/answer logits rise subsequently; shows attention moves operator/operand info to the last position.",
            "probing_or_intervention_method": "Direct projection of hidden states to vocab logits at multiple points per layer (attention output, MLP output, residual mid, residual final).",
            "performance_metrics": "Provides layerwise likelihood/logit trajectories rather than scalar accuracy; used to identify abstraction onset (layer ranges) and to correlate with behavioural outcomes.",
            "error_types_or_failure_modes": "Limited to single-token attribution at monitored positions; may not capture multi-step token generation dynamics; dependent on quality of unembedding projection.",
            "evidence_for_mechanism": "Observed consistent logit patterns across templates and models: operator logits rise around identified abstraction layers, operand logits rise in subsequent layers, and answer logits peak after MLP-dominated computation layers.",
            "counterexamples_or_challenges": "Logit attribution captures correlational patterns; causal claims require activation patching; single-token focus and layer-point choices can miss distributed dynamics.",
            "uuid": "e8153.3",
            "source_info": {
                "paper_title": "Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Activation Patching / Cross-Prompt Patching",
            "name_full": "Activation Patching (causal patching) and Cross-Prompt Patching",
            "brief_description": "Causal intervention technique that replaces single-layer activations from a 'clean' run into a 'corrupted' run to measure how much that state restores the clean prediction; cross-prompt patching transfers symbolic abstraction states between different surface forms to test compositionality and transfer.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to Llama-3 8B and Qwen-2.5 (7B, 14B)",
            "model_description": "Interventional method applied at attention/MLP/final outputs across layers at the last position, computing normalized patching effect scores (0-1) based on logit differences between clean and corrupted runs.",
            "arithmetic_task_type": "Used to causally probe operator abstraction (changing logic) and computation (changing operands) across single- and two-operator problems.",
            "mechanism_or_representation": "Identifies layers causally responsible for abstraction (L13-14 Llama-3 8B) vs operand integration (L15-16) vs computation/execution (L18, MLP peak); cross-patch shows symbolic abstraction states are transferable and can be composed with corrupted operands to produce target arithmetic outputs.",
            "probing_or_intervention_method": "Activation patching (Algorithm 1 in paper) and cross-prompt patching (patch clean symbolic-layer states into numerical corrupted runs), tracking per-layer log-probabilities and normalized recovery scores.",
            "performance_metrics": "Yields per-layer patching effect magnitudes (normalized 0-1) and layerwise log-prob trajectories showing when target answer probability rises/drops; used to demonstrate causal roles of specific layers rather than reporting end-task accuracy.",
            "error_types_or_failure_modes": "Interventions are single-state replacements and so may under-represent distributed contributions; patching limited to single-token last-position behavior and may not capture multi-token rollouts; compute budget limited analyses to models ≤12B.",
            "evidence_for_mechanism": "Patching abstraction-layer activations from additions into corrupted subtraction runs raises the target (clean) answer log-prob starting at abstraction onset layers and can flip final answers; MLP-layer patching (e.g., L18) yields strongest recovery for numeric computation, confirming layered functional separation.",
            "counterexamples_or_challenges": "Patching cannot fully restore behaviour if multiple distributed components contribute; cross-prompt patching results show drops in overall log-prob because symbolic runs predict 'x' as first token (so absolute log-probabilities fall), complicating direct interpretation; results restricted to single-token causal tests.",
            "uuid": "e8153.4",
            "source_info": {
                "paper_title": "Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "interpreting gpt: the logit lens",
            "rating": 2,
            "sanitized_title": "interpreting_gpt_the_logit_lens"
        },
        {
            "paper_title": "Patchscopes: A unifying framework for inspecting hidden representations of language models",
            "rating": 2,
            "sanitized_title": "patchscopes_a_unifying_framework_for_inspecting_hidden_representations_of_language_models"
        },
        {
            "paper_title": "Locating and editing factual associations in GPT",
            "rating": 2,
            "sanitized_title": "locating_and_editing_factual_associations_in_gpt"
        },
        {
            "paper_title": "A careful examination of large language model performance on grade school arithmetic",
            "rating": 2,
            "sanitized_title": "a_careful_examination_of_large_language_model_performance_on_grade_school_arithmetic"
        },
        {
            "paper_title": "Towards best practices of activation patching in language models: Metrics and methods",
            "rating": 2,
            "sanitized_title": "towards_best_practices_of_activation_patching_in_language_models_metrics_and_methods"
        },
        {
            "paper_title": "Interpreting and improving large language models in arithmetic calculation",
            "rating": 1,
            "sanitized_title": "interpreting_and_improving_large_language_models_in_arithmetic_calculation"
        },
        {
            "paper_title": "Arithmetic without algorithms: Language models solve math with a bag of heuristics",
            "rating": 1,
            "sanitized_title": "arithmetic_without_algorithms_language_models_solve_math_with_a_bag_of_heuristics"
        }
    ],
    "cost": 0.016344499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation
29 May 2025</p>
<p>Ziling Cheng ziling.cheng@mail.mcgill.ca 
Mila -Quebec AI Institute</p>
<p>McGill University</p>
<p>Meng Cao meng.cao@mail.mcgill.ca 
Mila -Quebec AI Institute</p>
<p>McGill University</p>
<p>Leila Pishdad leila.pishdad@borealisai.com 
BorealisAI</p>
<p>Yanshuai Cao yanshuai.cao@borealisai.com 
BorealisAI</p>
<p>Jackie Chi 
Kit Cheung cheungja@mila.quebec 
Mila -Quebec AI Institute</p>
<p>McGill University</p>
<p>CIFAR AI Chair
Canada</p>
<p>Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation
29 May 20258ACD7058C6D3CC2A8C93DD6A3F14539EarXiv:2505.23701v1[cs.CL]
Final-answer-based metrics are commonly used for evaluating large language models (LLMs) on math word problems, often taken as proxies for reasoning ability.However, such metrics conflate two distinct sub-skills: abstract formulation (capturing mathematical relationships using expressions) and arithmetic computation (executing the calculations).Through a disentangled evaluation on GSM8K and SVAMP, we find that the final-answer accuracy of Llama-3 and Qwen2.5 (1B-32B) without CoT is overwhelmingly bottlenecked by the arithmetic computation step and not by the abstract formulation step.Contrary to the common belief, we show that CoT primarily aids in computation, with limited impact on abstract formulation.Mechanistically, we show that these two skills are composed conjunctively even in a single forward pass without any reasoning steps via an abstractthen-compute mechanism: models first capture problem abstractions, then handle computation.Causal patching confirms these abstractions are present, transferable, composable, and precede computation.These behavioural and mechanistic findings highlight the need for disentangled evaluation to accurately assess LLM reasoning and to guide future improvements. 1</p>
<p>Introduction</p>
<p>Large language models (LLMs) have demonstrated impressive progress on various math problem datasets (Cobbe et al., 2021;Hendrycks et al., 2021b;Patel et al., 2021), often leveraging Chainof-Thought (CoT) prompting (Wei et al., 2022).Despite the availability of step-by-step reasoning chains, standard evaluation predominantly relies on final-answer accuracy (comparing the model's final numerical output against a gold answer), which † Work done during a Mitacs internship at Borealis AI. 1 Code and data will be made publicly available upon acceptance.</p>
<p>reduces model performance to a single metric (Liu et al., 2024;Opedal et al., 2024).This reduction limits the possible insights when diagnosing LLMs' reasoning abilities, especially in zero-shot scenarios without CoT.When an LLM fails to produce the correct answer, is it due to "reasoning deficits", or could it be a calculation error?</p>
<p>To investigate this, we propose a disentangled evaluation framework that separately measures two core skills of mathematical problem-solving (See Figure 1): (1) abstract formulation (hereafter, abstraction) -the ability to identify relevant quantities and translate the natural language problem into its underlying mathematical relationships (e.g., 36 + 47 or x + y in Figure 1); and (2) arithmetic computation (hereafter, computation) -the capacity to calculate the final answer from that expression (e.g., evaluate 36 + 47 to 83).</p>
<p>Using this disentangled evaluation on GSM8K (Cobbe et al., 2021) and SVAMP (Patel et al., 2021) with Llama-3 and Qwen-2.5 models (1B-32B), we find that even without CoT: (i) models surprisingly perform better at abstraction than computation, despite the former's perceived conceptual complexity.(ii) if deriving the final answer in math word problems depends on these two skills conjunctively, final-answer accuracy alone may give a misleading picture of models' reasoning abilities in math word problems.Moreover, we show that CoT primarily improves computation, with limited gains in abstraction, further demonstrating the value of disentangled evaluation.</p>
<p>While these behavioural findings suggest that models can formulate abstractions without explicit CoT when separately prompted, it remains unclear whether abstraction and computation are composed conjunctively when deriving the final answer during single-pass inference.To explore this, we move beyond outcome-based evaluation, and conduct mechanistic interpretability analyses.Using logit attribution and activation patching, we identify a consistent and sequential abstract-thencompute mechanism (see Figure 1a).Moreover, cross-prompt patching provides evidence that models do form abstractions internally independent of the surface form (numerical or symbolic, see Figure 1b): when these symbolic abstractions (e.g.</p>
<p>x − y) are transferred into a different problem, they are utilized and composed with the subsequent computation stages, altering the final answer.</p>
<p>Contributions: (i) Through disentangled evaluation, we show that without CoT, models exhibit stronger reasoning ability than final-answer accuracy suggests, and that CoT primarily aids calculation.(ii) Using mechanistic interpretability, we uncover an abstract-then-compute mechanism in a single-pass generation, where abstractions are transferrable across problem variants.Collectively, our findings suggest an alternative narrative: poor final-answer accuracy without CoT (Wei et al., 2022;Sprague et al., 2025), or performance declines on problem variants (Zhang et al., 2024a;Shi et al., 2023;Mirzadeh et al., 2025), can stem from arithmetic errors rather than reasoning deficits.</p>
<p>Related Work</p>
<p>Mathematical Reasoning Evaluation Existing math problem-solving benchmarks spans elementary word problems (Cobbe et al., 2021;Patel et al., 2021;Amini et al., 2019;Miao et al., 2020;Ling et al., 2017;Koncel-Kedziorski et al., 2016;Shi et al., 2015) to higher levels (Hendrycks et al., 2021b,a;Zhong et al., 2024;Zhang et al., 2023;He et al., 2024).Early datasets paired expressions with answers, but evaluation largely focused on final-answer-based metrics (Patel et al., 2021;Shi et al., 2015).With the rise of LLMs and CoT prompting (Wei et al., 2022), rationale-based formats became common (Hendrycks et al., 2021b;Cobbe et al., 2021), yet standard evaluations still predominantly use final-answer metrics, and occasionally code execution from rationales (Mishra et al., 2022;Gao et al., 2023).In contrast, we move beyond this final-answer-centric paradigm, by decomposing problem-solving into abstract formulation and arithmetic computation, inspired by the cognitive theories (Opedal et al., 2024).</p>
<p>Memorization vs. Generalization Variants of math word problems with perturbations were introduced to test generalization beyond memorization (Zhang et al., 2024a;Ye et al., 2025;Gao et al., 2023;Shi et al., 2023;Li et al., 2024;Mirzadeh et al., 2025).While performance drops are often interpreted as reasoning failures, our results suggest they may instead stem mainly from arithmetic errors, pointing to a different improvement strategy.</p>
<p>Mechanistic Interpretability Mechanistic interpretability methods, such as logit attribution (nostalgebraist, 2020;Belrose et al., 2023) and causal patching (Goldowsky-Dill et al., 2023;Wang et al., 2023;Meng et al., 2022;Zhang and Nanda, 2023;Merullo et al., 2024;Cheng et al., 2025), have been used to trace model computations.Prior work on math reasoning largely focuses on the mechanisms behind arithmetic computations (Nikankin et al., 2025;Zhang et al., 2024b), while recent work on word problems use probing classifiers to track explicit variable reasoning (Ye et al., 2025).In contrast, we examine implicit reasoning within a single forward pass to uncover abstraction beyond computation.</p>
<p>Dataset and Experimental Design</p>
<p>Task and Dataset We study math word problems using GSM-8K (Cobbe et al., 2021) and SVAMP (Patel et al., 2021).GSM-8K spans 2-8 steps without distractors (See Figure 2 for statistics), while SVAMP involves single-step reasoning with distractor variants.</p>
<p>Disentangling CoT Gains</p>
<p>We now apply disentangled evaluation with CoT to disentangle CoT gains (Table 2).We show that CoT yields the largest gains in computation (e.g., +62.8%), confirming its effectiveness in multi-step arithmetic.In contrast, abstraction shows limited improvement (e.g., +6.7% for Symbolic abstraction and +17.6% for Numerical abstraction), even with extended generation budgets (512 tokens), suggesting CoT is less helpful for abstraction.Gains in the Original setting (e.g., +62.8%) likely reflect a mix of benefits from both components and possible data leakage.See additional results in Appendix A.4.</p>
<p>Summary: These findings challenge the view that poor final-answer accuracy in math reasoning benchmarks always implies 'poor reasoning'.Instead, our disentangled design reveals that many models do possess a level of abstract formulation capabilities, which are often obscured in standard evaluations due to their limited arithmetic competence.Crucially, while abstraction variants indicate far higher performance than the Original setting, models are still not perfect -performance in Symbolic Abstraction remains far from 100% (45.7% for Llama-8B, 76.8% for Qwen-32B), but the gap is significantly narrower than previously assumed, calling for more precise definition and evaluation of reasoning.</p>
<p>Inside the Model: Probing Abstraction and Computation</p>
<p>To investigate whether abstraction and computation are composed conjunctively when producing a final numerical answer in a single forward pass, we move beyond outcome-based evaluation and apply mechanistic interpretability.We hypothesize an abstract-then-compute process: first inferring the abstraction (e.g., '+' from "buys"), then performing the computation (e.g., 5 + 3).Section 5.1 identifies key layers for each stage; Section 5.2 validate these layers and tests abstraction transferability across forms (symbolic/concrete) and logic.</p>
<p>Uncovering the Abstract-Then-Compute</p>
<p>Mechanism in One Forward Pass</p>
<p>Methods</p>
<p>We use logit attribution (nostalgebraist, 2020;Belrose et al., 2023) and activation patching (Ghandeharioun et al., 2024;Zhang and Nanda, 2023;Meng et al., 2022) to probe whether abstraction and computation occur during single-step generation.As summarized in Figure 4, we seek evidence of abstraction and computation.</p>
<p>Logit Attribution We use logit attribution to examine specific information (e.g., operator or answer tokens) at each layer (See Figure 4a for illustration).Specifically, we compute direct logit attribution (nostalgebraist, 2020; Belrose et al., 2023) of a target token t by projecting hidden states at various points in each layer onto the vocabulary space: logit(t) = ⟨W U [t], LN(h)⟩, where h is the hidden state, LN is LayerNorm, and W U [t] is the unembedding vector.We probe four points within each layer at the last token position: the attention output, MLP output, and the residual stream immediately after merging the attention output (resid mid) and after merging the MLP output (resid final).As summarized in Figure 4a, we track abstraction via the logits of operator tokens (e.g., "+", "add", "addition") and computation via the logits of operand and answer tokens across layers.</p>
<p>Activation Patching To identify components causally responsible for abstraction and computation, we apply activation patching (Algorithm 1, see Figure 4b for visualization)) (Ghandeharioun et al., 2024;Zhang and Nanda, 2023;Meng et al., 2022).pass with the corresponding hidden state from the clean run and measures how much this single hidden state injected in corrupted forward pass can restore the prediction of the clean answer.This patching effect per state per layer is a normalized score from 0 (no recovery) to 1 (full recovery to clean performance), with higher indicating more contribution.We patch attention, MLP and final layer outputs across layers at the last position.Formally, we quantify causal impact using the logit difference between clean a (i) cl and corrupted answers a
(i) cl , X (i) cor ) ∈ Ω do 4: logit o , A cl ← M(X (i) cl , A cl ) #(i) cor in Eq. 2. LD * (i) = logit * (a (i) cl ) − logit * (a (i) cor )
(1)
e (i) s = LD p (i) − LD c (i) LD o (i) − LD c (i)(2)
To probe abstraction (Figure 4b), we construct minimally different clean/corrupted pairs that vary in their underlying logic (e.g., "buys" for addition vs. "eats" for subtraction) but have the same numbers (e.g., "5,3").In Figure 4b, the clean input implies 5 + 3 = 8, while the corrupted input implies 5 − 3 = 2.We patch individual clean states to the corrupted run to identify critical layers for restoring the addition logic and recovering the clean answer '8'.For computation (Figure 4b), we use pairs with the same logic (e.g., addition), but different numbers (e.g., "5,3" vs. "11,7").Here, we seek to identify layers whose states when patched individually from the clean run to the corrupted run, are critical to perform the clean-run-specific computation with clean operands 5, 3 and output "8".</p>
<p>Abstract-then-Compute Hypothesis</p>
<p>As shown in Figure 5, we observe distinct stages for abstraction and computation, supporting the abstract-then-compute hypothesis.Logit attribution reveals that around L13-14, attention begins moving the inferred operator (e.g., '+', plus', add') to the last position (Figure 5i, iv).This coincides with a divergence in logit differences between target operators (+' vs. '-') in addition and subtraction problems (Figure 5v), suggesting that while earlier layers encode generic operator features, problemspecific abstraction emerges here.Subsequently, around L15-16, Figure 5i,iv shows operands transfer to the last position; Following abstraction, the computation phase appears to begin at L18, primarily through MLPs layers (Figure 5ii, iv).Activation patching confirms the distinct stages: abstraction starts at around L13, with rising attention and layer patching effects (Figure 5iii</p>
<p>Validation and Abstraction Transfer with Cross-Prompt Patching</p>
<p>We now validate the causal role of the critical layers for abstraction (L13,14) and computation (L15,16 for operands, L18 for execution).We also investigate if the abstraction representations formed at around L13,14 are transferable across problem forms (symbolic/concrete) and templates, and can be composed with subsequent computation stage.</p>
<p>Method Cross-prompt patching also uses Algorithm 1, but instead of computing patching effects, we track the log-probability of specific tokens across layers in each patched run.This acts as a form of "knock-out" intervention: we overwrite a single layer's activations in the corrupted run with clean activations that are hypothesized to contain specific information (e.g., abstraction, operands, computation), and observe whether this information is reflected in the output level.</p>
<p>To validate the critical layers for each stage, we To investigate if the abstraction representations can be transferred across problem forms (symbolic/numerical) and templates, and if they can be composed with subsequent computation stage, we cross-patch for symbolic abstraction (See Figure 4c) -patching symbolic clean states to numerical corrupted run.Here, clean inputs are symbolic (no concrete numbers), and corrupted inputs are numerical problems with a different underlying logic.This ensures that only abstraction (no operands or computation) is transferred from the clean run, unlike numerical abstraction cross-patching.In the example in Figure 4c, clean run predicts x − y, while the corrupted run corresponds to 5 + 3 = 8.By patching clean states from the symbolic problem to the numerical corrupted run, we examine (i) if symbolic abstractions are also formed at around L13-14, despite predicting 'x' as the first token, and (ii) if this abstraction (x − y), when transferred into numerical corrupted run, can be used and composed with corrupted operands (5, 3) to compute 5 − 3 = 2.To assess this, we track the per-layer log-probabilities of the target answer (clean logic + corrupted operands, 2 = 5 − 3) and corrupted answer (8 = 5 + 3), and omit the clean answer 'x'.If symbolic abstraction transfer occurs, we expect an increase in the target answer log-prob, and a corresponding decrease in the corrupted answer starting around L13.Note that since the symbolic clean states across layers are predicting 'x', we expect both answer log-probs to drop.</p>
<p>Results Figure 6a shows results for numerical abstraction cross-patching results corresponding to the example illustrated in Figure 4.As expected, the target answer log-probability ('18') begins rising at L13 (abstraction onset), peaks at L14 (abstraction formed), and drops when clean operands are introduced (L15).The clean answer ('8') logprobability keeps rising from L13 (abstraction) and continue at 15 (operand integration), stabilizing by L18 (computation).The corrupted answer ('4') log-probability drops after L13.These trends hold across underlying logic (Figure 6b-d Together, these results provide strong support for the abstract-then-compute hypothesis with critical layers for abstraction (L13,14) and computation (L15,16 for operands and L18 for computation), and further demonstrate that: (i) abstraction can be transferred and composed with subsequent computation across surface forms (symbolic/concrete) and templates, and (ii) even at the last position in symbolic problems, when predicting the first output token 'x', middle layers already encode abstraction (e.g., the correct operator), indicating that nexttoken prediction reflects not just immediate token prediction, but also anticipates future outputs.</p>
<p>Conclusion</p>
<p>Disentangled evaluation reveals that, without CoT, models perform better at abstraction than computation, with the latter bottlenecking final-answer accuracy -challenging the view that poor performance always imply reasoning failure.Mechanistic interpretability uncovers an abstract-then-compute mechanism with transferable abstractions.We argue for disentangled evaluation to more precisely assess model abilities and inform architectural design.</p>
<p>Our study has several limitations.First, we focus solely on English-language datasets; whether the abstract-then-compute mechanism generalizes to other languages remains an open question.Second, our evaluation decomposes mathematical problemsolving into only two stages: abstract formulation and arithmetic computation.Finer-grained breakdowns (e.g., Opedal et al. ( 2024)) may offer deeper insight.Third, our interpretability analysis is limited to single-step generation, as common techniques (e.g., activation patching, logit attribution) target single-token behavior.Extending these to multi-step reasoning is an ongoing challenge, with recent work like SelfIE (Chen et al., 2024) provides initial steps.Fourth, while we focus on critical layers involved in abstraction and computation, we leave detailed analysis of components to future work.Finally, due to compute constraints, we analyze models up to 12B parameters.Extending to larger models is left for future studies.</p>
<p>A Disentangled Evaluation Details and Additional Results</p>
<p>A.1 Symbolic Variant Creation For GSM8K and SVAMP</p>
<p>All our evaluations are conducted on the GSM8K test set and the full SVAMP dataset.To support our evaluation of abstract formulation and arithmetic computation in Section 4, we construct symbolic question and expression answer variants for both SVAMP and GSM-8K.Examples are shown in Table 4.</p>
<p>For SVAMP, which already includes both the expression (e.g., 20 × 10) and the final numerical answer (e.g., 200), we create symbolic abstraction variants by replacing all numeric values with symbolic variables (e.g., x, y) in both the question and the corresponding expression.This preserves the structure and semantics of the original problem while abstracting away from the concrete numbers.For arithmetic computation variant, we use the paired expression.</p>
<p>For GSM-8K, which lacks such annotations, we generate both the symbolic abstraction variant and the numerical expressions using a twostage generate-then-validate pipeline (Figure 7).In the generation stage, we use GPT-4o-mini (Ope-nAI, 2024) to produce triplets from original question-solution pairs.Each triplet consists of: (1) a symbolic version of the question, where relevant numbers are replaced with variables while maintaining the semantic content; (2) a symbolic expression that represents the solution in closed-form using those variables; and (3) a substitution rule that maps each variable to its original numeric value.In the validation stage, we verify the correctness of each generated sample.We apply the substitution rule to the symbolic expression, obtaining a numerical expression, then using sympy to evaluate the expression, and compare the resulting numeric answer to the gold answer from GSM-8K.Triplets that fail this check are manually reviewed and corrected.</p>
<p>A.2 Evaluation Details</p>
<p>In this section, we detail the evaluation of the four settings.First, we show the instructions used in each settings in Table 5 with and without CoT.The prompt used in each setting is then a concatenation of the instruction and the question.</p>
<p>For the Original and Arithmetic Computation settings, where the expected output is a final inte-   ger answer, we extract the answer following the token ####, remove any accompanying units, and normalize formatting (e.g., removing commas, dollar signs, percentage symbols, and units like 'g') before comparing it with the gold answer.</p>
<p>For the Numerical Abstraction setting, where answers are expected to be numerical expressions, we first convert LaTeX-style expressions to Python syntax (when written in Markdown form), then evaluate them using sympy to check equivalence with the gold expression.</p>
<p>In the Symbolic Abstraction setting, where outputs are symbolic expressions, we use gpt-4o-mini as an automated evaluator.The prompting to gpt-4o-mini is shown in Box 1, and responses are generated with temperature set to 0. To validate this method, we annotated a held-out set of 120 samples manually for correctness, and compared our annotations with the gpt-4o-mini evaluator's decisions.We find that gpt-4o-mini achieves 94% agreement with our judgment in identifying symbolic expression equivalence.Example comparisons are shown in Table 3.</p>
<p>A.3 Additional Result of Disentangled Evaluation Without CoT</p>
<p>We report zero-shot, no-CoT performance on SVAMP in Figure 9. Compared to GSM8K, SVAMP is a significantly simpler benchmark consisting of math word problems that require only a single reasoning step -namely, a single arithmetic operation.As with GSM8K, models perform better on the abstraction variants than in the original setting, though the performance gap is smaller due to the task's simplicity.Interestingly, we observe a notable difference from GSM8K: across all model sizes, even small models such as LLAMA 1B and 3B perform well on the Arithmetic Computation variant, often outperforming both the abstraction variants and the original setting.This suggests that computing onestep expressions (e.g., 5 − 3) is less challenging Weng earns $x an hour for babysitting.Yesterday, she just did y minutes of babysitting.How much did she earn?</p>
<p>x * (y/60)</p>
<p>than deriving an abstract formulation with only one step.However, in tasks involving multiple steps, abstraction becomes comparatively easier than executing the full computation correctly, as shown in the case of GSM8K.This highlights how model capabilities depend not just on the skill type but also on the complexity of the required operation.</p>
<p>Weng earns $x an hour for babysitting.Yesterday, she just did y minutes of babysitting.How much did she earn?</p>
<p>x * (y/60)</p>
<p>Original Symbols</p>
<p>Weng earns $y an hour for babysitting.Yesterday, she just did x minutes of babysitting.How much did she earn?</p>
<p>A.4 Additional Resuls of Disentangled Evaluation With CoT</p>
<p>We present the full results on GSM8K for Llama family and Qwen family in Figure 10, and full results on SVAMP for Llama family and Qwen family in Figure 11.</p>
<p>A.5 Ablation on Symbolic Abstraction Variant</p>
<p>To assess the reliability and external validity of the symbolic abstraction evaluation, we perform ablations over symbol order and symbol choice.As illustrated in Figure 8, we compare three settings:</p>
<p>• Original Symbols: Variables are consistently represented using a fixed set of letters in order-x, y, z, u, v, w, p, q, r, s, t-e.g., x × (y/60).</p>
<p>• Reversed Symbols: The same set of symbols is used, but the order is reversed (e.g., y×(x/60)), preserving the semantic and structural content of the problem while changing the superficial presentation.</p>
<p>• Random Symbols: Each original symbol is replaced with a randomly sampled letter from the alphabet, unique to each dataset.This preserves the structure of the expression while removing any consistent identity cues.The mappings are as follows: {'a': 'h', 'd': 'i', 'm': 's', 'n': 'r', 'p': 'e',  'q': 'l', 'r': 'c', 's': 'v', 't': 'j', 'u': 'm', 'v': 't', 'w': 'o', 'x': 'u', 'y': 'p', 'z': 'b', 'Z': 'f'}</p>
<p>In Table 6, we observe mild performance degradation with symbol perturbations on both models, (e.g., three-point drop with Reversed and another two points with Random), but models retain strong accuracy compared to the Original setting.This suggests that Symbolic Abstraction is relatively robust to surface-level symbol changes.</p>
<p>B Intepretability Results</p>
<p>B.1 Interpretability Data Construction</p>
<p>To construct a dataset suitable for mechanistic interpretability, we focus on simpler math word problems that require only one or two reasoning steps with one or two basic arithmetic operations (addition, subtraction, multiplication, or division).We deliberately avoid more complex multi-step problems, as model performance on such tasks tends to be poor, potentially confounding interpretability analyses.</p>
<p>For each pair of arithmetic operations-(x + y, x − y) and (x × y, x ÷ y) and (x + y + z, x + y − z, x − y + z, x − y − z) -we use a proprietary model to generate 150 template pairs, totaling 1200 templates.These templates are minimally different in semantics but vary across a broad range of topics, verb choices, names, and syntactic structures.Examples are presented in Table 7.For instance, a representative pair might include: Each template is instantiated by replacing the [name] placeholder with a randomly selected name from a curated list of 30 English first names, shown below:</p>
<p>James, Emma, William, Olivia, Benjamin, Charlotte, Henry, Amelia,</p>
<p>Figure 1 :
1
Figure 1: Left (Disentangled evaluation framework): Final-answer accuracy obscures reasoning ability due to conflating abstract formulation and arithmetic computation.Right (Abstract-then-Compute Mechanism in Llama-3 8B): (a) Residual stream at the last token position shows that models first capture problem abstraction (L13-14), followed by computation (L18).(b) Same as (a), but one critical layer output is patched with a different symbolic abstraction (e.g., x − y), causally changing the computation from 5 + 3 = 8 to 5 − 3 = 2.</p>
<p>Figure 2 :
2
Figure 2: Distribution of problem characteristics by number of reasoning steps (GSM8K) and presence of distractors (SVAMP).</p>
<p>Figure 3 :
3
Figure 3: Model zero-shot without CoT performance on GSM8K.(i) Models exhibit much better abstraction performance (Symbolic and Numerical) than in actually computing the expressions (Arithmetic Computation).(ii) Final-answer accuracy in the Original setting may provide a misleading picture of models' reasoning ability, possibly due to arithmetic limitations.</p>
<p>Figure 4 :
4
Figure 4: Overview of interpretability methods probing the abstract-then-compute mechanism in simple math problems, focusing on hidden states at the last token position across layers.</p>
<p>); The rise of attention and layer patching effects in L15,16 in Figure 5vi aligns with our previous observation that operands are being moved to the last position.Finally, the peak patching effect of MLP at L18 high-light their crucial role in calculating the answer.These combined results support our hypothesis that the model follows an abstract-then-compute mechanism within a single forward pass.Additional logit attribution and activation patching results for other models and two-operator problems are in Appendix B.2.</p>
<p>Figure 5 :
5
Figure 5: Visualizations of internal computations at last token position in Llama-3 8B for addition math word problems: (i,ii, iv, v) for logit attribution results where a, b are operands and c is the result; (iii, vi) for activation patching results.We label the starting layer of abstraction, operand moving and computation in pink, blue and green, respectively.</p>
<p>Figure 6: Cross-patching results for Llama-3 8B with corresponding clean and corrupted run.a, b indicate concrete numerical problems, while x, y indicate symbolic problems.Top (Numerical Abstraction): Patching concrete problems with different abstractions shows target log-prob rising at 13 (abstraction onset), peaking at 14 (abstraction formed), then falling as clean operands are introduced.Meanwhile, the clean answer's log-prob rises from 13 (abstraction) and 15 (operand integration), stabilizing at layer 18 (computation).Bottom (Symbolic Abstraction): Patching symbolic problems into concrete addition shows target log-probability rising at layer 13, peaking at 15 (where predictions flip), then declining.</p>
<p>), confirming the roles of these critical layers as identified earlier.In symbolic abstraction cross-patching (Figure6e-h), we observe consistent behaviour: from L13 onward, the target answer probability increases while the clean answer decreases, eventually flipping.This indicates that (i) abstractions injected via patching are composed with corrupted operands to produce valid outputs, and (ii) abstraction representations at L13-14 are invariant to surface form and problem template.Concretely, comparing Figure6eand Figure6f, where minimally different templates are used in (e) and random templates in (f), we observe near-identical effects in both cases -suggesting abstraction transfer is templateinvariant.Furthermore, (g) and (h) show that injecting symbolic multiplication and division abstrac-tions into concrete addition problems still flips the model's prediction-demonstrating the generality of abstraction transfer.Cross-patching results for other models, and two-operator problems are in Appendix B.3.</p>
<p>Question:Figure 7 :
7
Figure7: Generate-then-validate pipeline: We use API calls to obtain abstract question-answer-substitution triplets from the concrete question-solution pair from GSM-8K, then validate them against gold answer using sympy.Triplets that fail this check are manully reviewer and corrected.</p>
<p>(x * z+z+(z/y)+9)/3 x = 3, y = 1 2 , z = 21 SVAMP Each pack of DVDs costs x dollars.If there is a discount of y dollars on each pack x − y x = 76, y = 25 SVAMP An industrial machine worked for x minutes.It can make y shirts a minute.x • y x = 4, y = 5 SVAMP Paco had x salty cookies and y sweet cookies.He ate z sweet cookies and u salty cookies.How many salty cookies did Paco have left?x − u x = 26, y = 17, z = 14, u = 9</p>
<p>Figure 8 :
8
Figure 8: Experiment configurations for the ablation study on symbol choices and symbol order.</p>
<p>Figure 9 :
9
Figure 9: Model zero-shot without CoT performance on SVAMP.</p>
<p>Figure 10 :
10
Figure 10: Model zero-shot with and without CoT performance on GSM8K.A.C.: Arithmetic Computation; N.A.: Numerical Abstraction; O.: Original; S.A.: Symbolic Abstraction.</p>
<p>which assess abstraction using symbolic variables; Numerical Abstraction, evaluating ab- straction with concrete numbers but without com- putation; and Arithmetic Computation, which di- rectly tests execution of fully specified expressions</p>
<p>Framework Suppose a task T can be decomposed into a set of sub-skills {s 1 , s 2 , ..., s n }, such that solving T requires executing these skills conjunctively (i.e., T = s 1 ∩ s 2 ∩ • • • ∩ s n ).Disentangled evaluation aims to assess each sub-skill s i independently via a corresponding subtask t i , designed to isolate and test that specific skill.Let Eval(T ) denote the evaluation metric on the full task, and Eval(t i ) the metric for subtask t i .fromQ.See Table1and Appendix A.2 for details.
SettingSkills TestedQuestion Form and ExampleAnswer Form and ExampleOriginalAbstractionNumerical: Weng earns $12 for every hour she works.Number: 10+ComputationIf she worked for 50 minutes, how much did she earn?Arithmetic Computation Numerical Abstraction Symbolic AbstractionComputation Abstraction AbstractionNumerical: What is the value of 12 × 50 60 ? or 12 × 50 60 =? 4 Disentangled Evaluation Number: 10 Numerical: Weng earns $12 for every hour she works. Expression: 12 × 50 60 If she worked for 50 minutes, how much did she earn? We first introduce the disentangled evaluation framework, then present results without CoT in Sec. 4.1, followed by an analysis of CoT's impact Symbolic: Weng earns $x for every hour she works. If she worked for y minutes, how much did she earn? Expression: x × y 60in Sec. 4.2.See Appendix A.1 and Table 4 for details and exam-ples. For interpretability, we generate 3,600 simple1-2 step 2 word problems involving basic operations(+, −, ×, ÷) from 1,200 diverse LLM-generatedtemplates, covering varied scenarios, verb choices,entities, names and sentence structures. See Ap-pendix B.1 and Table 7 for details and examples.4.1 Understanding Model Failures:Models We evaluate instruction-tuned Llama-3Reasoning or Arithmetic Error?(1B, 3B, 8B) (Grattafiori et al., 2024) and Qwen 2.5 (Yang et al., 2024) (3B, 7B, 14B, 32B) mod-els. Mechanistic interpretability analyses focus on Llama-3 8B, Qwen 2.5 7B, and Qwen 2.5 14B.We first apply disentangled evaluation zero-shot without CoT across multiple model sizes of Llama-3 and Qwen2.5 families. As shown in Figure 3, the error rates are consistently lower for abstract formu-Evaluation All experiments use greedy decodinglation (both Numerical and Symbolic Abstraction)and FP16 precision on RTX 8000/A100L GPUs.compared to arithmetic computation. This suggestsNumeric answers are evaluated via normalized Ex-that if final-answer accuracy in the Original set-act Match. Symbolic expressions are evaluated us-ting depends on both competencies conjunctively,ing gpt-4o-mini (94% agreement with humans onpoor performance observed in the Original settingcould stem from arithmetic computation failures,rather than reasoning deficits. Consequently, thisindicates that final-answer accuracy alone from
To evaluate abstract formulation, we create symbolic variants: SVAMP expressions are templated into variable-based forms; GSM-8K symbolic versions from the test set are generated using gpt-4o-mini (OpenAI, 2024) via a two-stage generate-then-validate to ensure the correctness.Measuring Eval(t 1 ), . . ., Eval(t n ) enables finergrained attribution of performance, identifying failure of specific skills.In math word problems, let (Q, E, A) be the question, expression and answer triplets, we decompose mathematical problemsolving into abstract formulation (translating Q to mathematical relationships E) and arithmetic computation (executing the calculation from E to produce A).Besides the standard Original setting (requiring both abstraction and computation), we design three targeted subtasks: Symbolic Abstraction,</p>
<p>Table 1 :
1
Disentangled evaluation in math word problems with tested skills, varying by question and answer forms.Instructions in Appendix Table5.</p>
<p>Set Ω of clean and corrupted sample pairs (X cl , X cor ), model M with hidden states S. 2: Output: Patching effects for S: E S .</p>
<p>To quantify the contribution of each component across layers, this method replaces a single intermediate hidden state in the corrupted forward Algorithm 1 Activation Patching 1: Input: 3: for (X</p>
<p>Table 3 :
3
Comparison of gold answers, model generations, our annotated correctness, and GPT-4o-mini evaluation on a held-out set of 120 samples.GSM8KAdrian's age is x times the age of Harriet, and Harriet is y the age of Zack.Calculate the average age of the three in three years if Harriet is z years old now.
Gold AnswerModel GenerationOur Eval GPT-4o-mini Evalu  *  (x + y + z)xu + yu + zuTrueTruex + x  *  (1/y)x + (x/y)TrueTrue0.5(x + yz)z  *  (y + 1)  *  x/2FalseFalse(y + z)/xxz − y = xyFalseFalsexz  *  ((1 − y)/100) (x  *  (1 − y/100)  *  z)FalseTrue(12/x)  *  yy  *  12FalseTrueDatasetSymbolic QuestionAnswerSubstitutionGSM8KI have x liters of orange drink that are y% water and I(y • (x − v) + u • z)/100 x = 10, y =wish to add it to z liters of pineapple drink that is u% water. But as I pour it, I spill v liters of the orange drink. How much water is in the remaining w liters?2 3 , z = 15, u = 3 5 , v = 1, w = 24GSM8KJerry has a flock of chickens. The red chickens produce x(z − u • y)/(x + y)x = 3, y = 5, z =eggs a day, and the white chickens produce y eggs a day.42, u = 2Every day Jerry collects z eggs. If he has u more whitechickens than red chickens, how many red chickens doeshe have?</p>
<p>Table 4 :
4
Constructed symbolic examples from GSM8K and SVAMP datasets.</p>
<p>Table 5 :
5
Prompting Strategies, Problem Variants and Instructions
SettingStrategyInstructionQuestionAnswerOriginalNo CoTPlease answer the question directly WITHOUTWeng earns $12 an hour for10showing the reasoning process, you MUSTbabysitting. Yesterday, she justwrite the answer as an integer after '####',did 50 minutes of babysitting.without including the equation or units.How much did she earn?OriginalCoTLet's think step by step, you MUST writeWeng earns $12 an hour for10the answer as an integer after '####' withoutbabysitting. Yesterday, she justincluding the units. Write the answer at thedid 50 minutes of babysitting.end.How much did she earn?Arithmetic ComputationNo CoTPlease answer the question directly WITHOUTWhat is the value of 12 * (50/60)?10showing the reasoning process, you MUSTwrite the answer as an integer after '####'Arithmetic ComputationCoTLet's think step by step, you MUST write theWhat is the value of 12 * (50/60)?answer as an integer after '####' . Write theanswer at the end.Numerical AbstractionNo CoTPlease answer the question directly withoutWeng earns $12 an hour for12  *  (50/60)showing the reasoning process, you MUSTbabysitting. Yesterday, she justwrite the expression with appropriate rounddid 50 minutes of babysitting.brackets after '####', without including theHow much did she earn?units, and you DO NOT need to simplify theexpression.Numerical AbstractionCoTLet's think step by step, at the end, youWeng earns $12 an hour for12  *  (50/60)MUST write the expression with appropriatebabysitting. Yesterday, she justparenthesis after '####', without including thedid 50 minutes of babysitting.units, but you DO NOT need to simplify theHow much did she earn?expression.Symbolic AbstractionNo CoTPlease answer the question directly WITHOUTWeng earns $x an hour for babysit-x  *  (y/60)showing the reasoning process, you MUSTting. Yesterday, she just did y min-write the expression with appropriate roundutes of babysitting. How much didbrackets after '####' without including theshe earn?units, and you DO NOT need to simplify theexpression.Symbolic AbstractionCoTLet's think step by step, at the end, youMUST write the expression with appropriateround brackets after '####' without includingthe units, but you DO NOT need to simplifythe expression.</p>
<p>Table 6 :
6
Results of ablation study on symbol choices and symbol order, with and without CoT under zeroshot setting on GSM8K.
SettingLlama 8BQwen 7BNo CoT CoT No CoT CoToriginal45.756.761.574.7reverse42.851.861.974.8random41.053.158.071.9</p>
<p>•</p>
<p>[name] has {x} apples.They get {y} more apples.How many apples does [name] have now?(corresponding to x + y) • [name] has {x} apples.They give away {y} apples.How many apples does [name] have now?(corresponding to x − y)</p>
<p>We focus on 1-2 step problems, as models often fail simple word problems involving multi-step computations in a single forward pass. 120 samples, prompt and details in Appendix A.2) and with sympy for numeric expressions. We report standard accuracy. CoT generations are capped at 512 tokens. See Appendix A.2 for details.
AcknowledgmentsThis work is supported by the Mitacs Accelerate Program (Project ID: IT27067) and partially enabled by Mila's computing resources (mila.quebec).We thank Zichao Li, Cesare Spinoso-Di Piano, and Xiyuan Zou for helpful discussions.Jackie Chi Kit Cheung is supported by the Canada CIFAR AI Chair program.We acknowledge the material support of NVIDIA for providing computational resources.(×, ÷) (×) The glacier recedes x inches daily.How much will it shrink after y days?(÷) The glacier retreated x inches over y days.What was the average daily recession?(×, ÷) (×) Each server rack uses x kilowatts.What's the total power for y racks?(÷) The data center used x kilowatts across y racks.What was the average per rack?(×, ÷) (×) The spaceship's shield blocks x radiation units hourly.How much radiation can it block in y hours?(÷) The shield blocked x units over y hours.What was its average protection rate?Two operations  Alexander, Ava, Samuel, Sophia, Jacob, Mia, Daniel, Lily, Michael, Grace, Ethan, Ella, Jack, Chloe, Lucas, Harper, Thomas, Zoe, Matthew, Nora, Nathan, Isla.The numerical placeholders {x} and {y} are populated with integers ≤ 50, to avoid detokenization issues during model processing.B.2 Logit Attribution and Activation Patching Additional ResultsOther Models We observe a similar abstractthen-compute mechanism in other models, including Qwen 2.5 7B and Qwen 2.5 14B.In Qwen 2.5 7B, the abstraction stage occurs around layers 18-20, with the computation stage beginning around layers 22-23.In Qwen 2.5 14B, abstraction takes place around layers 29-32, followed by computation starting at layer 36.For additional interpretability results using logit lens and activation patching:• See Figure12, Figure13, and Figure14for Llama-3 8B on subtraction, multiplication, and division.• See Figure15Two-Operator Dataset For two operator dataset, we only report results for Qwen 2.5 7B and Qwen 2.5 14B, because Llama-3 8B only achieve 16.5% accuracy on this dataset.See Figure27and Figure28for logit attribution results for Qwen 2.5 7B and Qwen 2.5 14B, respectively.Box 1: Symbolic Evaluation PromptDetermine whether the following two mathematical expressions are equivalent.The expressions may be written in simplified or unsimplified symbolic form (e.g., 1/2x + 3), natural language (e.g., "Susan made 1/2x + 3 buttons") or in LaTeX notation.Consider expressions equivalent if they represent the same mathematical value, even if written differently (e.g., different notation, simplification, or variable order when valid).Respond only with: True or False.Example:B.3 Cross-Prompt Patching Additional ResultsOther Models See Figure23, Figure24, and Figure25for symbolic abstraction cross-prompt patching results (for single operators: +, −, ×, ÷) on Llama3 8B, Qwen 2.5 7B, and Qwen 2.5 14B, respectively.The results are consistent across models: the likelihood of the target answer peaks at the abstraction stage, while the likelihood of the corrupted answer drops significantly starting from the same stage.See Figure26for numerical abstraction crossprompt patching results on Llama3 8B, Qwen 2.5 7B, and Qwen 2.5 14B.We observe consistent trends across all models: the probability of the target answer begins to rise at the onset of the abstraction stage and peaks by its end.Meanwhile, the clean answer probability increases steadily throughout the abstraction stage, reaching a log-probability of 0 at the start of the computation stage.Two-Operator Dataset For the two-operator dataset, we report results only for Qwen 2.5 7B and Qwen 2.5 14B, as Llama-3 8B performs poorly on this setting, achieving only 16.5% accuracy.See Figure29for symbolic abstraction crossprompt patching results on Qwen 2.5 7B and Qwen 2.5 14B.
MathQA: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, 10.18653/v1/N19-1245Proceedings of the 2019 Conference of the North American Chapter. the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American ChapterMinneapolis, MinnesotaAssociation for Computational Linguistics20191Long and Short Papers</p>
<p>Eliciting latent predictions from transformers with the tuned lens. Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev Mckinney, Stella Biderman, Jacob Steinhardt, arXiv:2303.081122023arXiv preprint</p>
<p>SelfIE: Self-interpretation of large language model embeddings. Haozhe Chen, Carl Vondrick, Chengzhi Mao, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine LearningPMLR2024235of Proceedings of Machine Learning Research</p>
<p>Stochastic chameleons: Irrelevant context hallucinations reveal class-based (mis)generalization in llms. Ziling Cheng, Meng Cao, Marc-Antoine Rondeau, Jackie Chi, Kit Cheung, arXiv:2505.226302025Preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, arXiv:2110.14168Reiichiro Nakano, and 1 others. 2021. Training verifiers to solve math word problems. arXiv preprint</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, Mor Geva, arXiv:2401.06102Patchscopes: A unifying framework for inspecting hidden representations of language models. 2024arXiv preprint</p>
<p>Nicholas Goldowsky-Dill, Chris Macleod, Lucas Sato, Aryaman Arora, arXiv:2304.05969Localizing model behavior with path patching. 2023arXiv preprint</p>
<p>Alan Schelten, Alex Vaughan, and 1 others. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>OlympiadBench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun, 10.18653/v1/2024.acl-long.211Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, International Conference on Learning Representations. 2021a</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021bNeurIPS</p>
<p>MAWPS: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, 10.18653/v1/N16-1136Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational Linguistics2016</p>
<p>Gsm-plus: A comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, Wei Bi, arXiv:2402.192552024arXiv preprint</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>ECBD: Evidence-centered benchmark design for NLP. Yu Lu Liu, Su Lin Blodgett, Jackie Cheung, Q Vera, Alexandra Liao, Ziang Olteanu, Xiao, 10.18653/v1/2024.acl-long.861Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Locating and editing factual associations in GPT. Kevin Meng, David Bau, Alex J Andonian, Yonatan Belinkov, Advances in Neural Information Processing Systems. 2022</p>
<p>Language models implement simple Word2Vec-style vector arithmetic. Jack Merullo, Carsten Eickhoff, Ellie Pavlick, 10.18653/v1/2024.naacl-long.281Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics20241</p>
<p>A diverse corpus for evaluating and developing english math word problem solvers. Chao-Chun Shen-Yun Miao, Keh-Yih Liang, Su, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>GSM-symbolic: Understanding the limitations of mathematical reasoning in large language models. Keivan Seyed Iman Mirzadeh, Hooman Alizadeh, Oncel Shahrokhi, Samy Tuzel, Mehrdad Bengio, Farajtabar, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>LILA: A unified benchmark for mathematical reasoning. Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin Kalyan, 10.18653/v1/2022.emnlp-main.392Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Abulhair Saparov, and Mrinmaya Sachan. 2024. Do language models exhibit the same cognitive biases in problem solving as human learners. Yaniv Nikankin, Anja Reusch, Aaron Mueller, Yonatan Belinkov, Andreas Opedal, Alessandro Stolfo, Haruki Shirakami, Ying Jiao, Ryan Cotterell, Bernhard Schölkopf, arXiv:2401.18070The Thirteenth International Conference on Learning Representations. nostalgebraist. 2020. interpreting gpt: the logit lens. 2025arXiv preprintArithmetic without algorithms: Language models solve math with a bag of heuristics</p>
<p>Gpt-4o mini: Advancing cost-efficient intelligence. 2024OpenAI</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, Denny Zhou, International Conference on Machine Learning. PMLR2023</p>
<p>Automatically solving number word problems by semantic parsing and reasoning. Shuming Shi, Yuehui Wang, Chin-Yew Lin, Xiaojiang Liu, Yong Rui, Proceedings of the 2015 conference on empirical methods in natural language processing. the 2015 conference on empirical methods in natural language processing2015</p>
<p>To cot or not to cot? chain-ofthought helps mainly on math and symbolic reasoning. Zayne Rea Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. Kevin Ro, Wang , Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>. An Qwen, Baosong Yang, Beichen Yang, Binyuan Zhang, Bo Hui, Bowen Zheng, Chengyuan Yu, Dayiheng Li, Fei Liu, Guanting Huang, Haoran Dong, Huan Wei, Jian Lin, Jianhong Yang, Tu, Jianwei</p>
<p>Jianxin Zhang, Jiaxin Yang, Jingren Yang, Zhou, ArXiv, abs/2412.15115Qwen2.5 technical report. Junyang Lin, and 25 others. 2024</p>
<p>Physics of language models: Part 2.1, grade-school math and the hidden reasoning process. Tian Ye, Zicheng Xu, Yuanzhi Li, Zeyuan Allen-Zhu, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Fred Zhang, Neel Nanda, arXiv:2309.16042Towards best practices of activation patching in language models: Metrics and methods. 2023arXiv preprint</p>
<p>A careful examination of large language model performance on grade school arithmetic. Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, William Song, Tiffany Zhao, Pranav Vishnu Raja, Charlotte Zhuang, Dylan Z Slack, Qin Lyu, Sean M Hendryx, Russell Kaplan, Michele Lunati, Summer Yue, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024a</p>
<p>Interpreting and improving large language models in arithmetic calculation. Wei Zhang, Chaoqun Wan, Yonggang Zhang, Yiu-Ming Cheung, Xinmei Tian, Xu Shen, Jieping Ye, arXiv:2409.016592024barXiv preprint</p>
<p>Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, Xipeng Qiu, arXiv:2305.12474Evaluating the performance of large language models on gaokao benchmark. 2023arXiv preprint</p>
<p>AGIEval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, 10.18653/v1/2024.findings-naacl.149Findings of the Association for Computational Linguistics: NAACL 2024. Mexico City, Mexico2024Association for Computational Linguistics</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1053 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1053</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1053</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-265506816</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.18044v3.pdf" target="_blank">Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges</a></p>
                <p><strong>Paper Abstract:</strong> Transfer learning is a conceptually-enticing paradigm in pursuit of truly intelligent embodied agents. The core concept -- reusing prior knowledge to learn in and from novel situations -- is successfully leveraged by humans to handle novel situations. In recent years, transfer learning has received renewed interest from the community from different perspectives, including imitation learning, domain adaptation, and transfer of experience from simulation to the real world, among others. In this paper, we unify the concept of transfer learning in robotics and provide the first taxonomy of its kind considering the key concepts of robot, task, and environment. Through a review of the promises and challenges in the field, we identify the need of transferring at different abstraction levels, the need of quantifying the transfer gap and the quality of transfer, as well as the dangers of negative transfer. Via this position paper, we hope to channel the effort of the community towards the most significant roadblocks to realize the full potential of transfer learning in robotics.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1053.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1053.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>sim-to-real</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulation-to-Reality Transfer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Class of methods that train policies or models in simulated environments and transfer them to physical robots; paper reviews domain randomization, privileged learning, few-shot adaptation and the reality gap as central challenges and solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>simulated policy (transfer target: physical robot)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policies/models trained in simulation using reinforcement learning and/or supervised/backpropagation (including privileged teacher-student setups), then transferred to real robots often via domain randomization and few-shot fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (policy) transferred to physical robot</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>simulator → real-world domains</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated environments parameterized to approximate real-world physics (contacts, dynamics, sensors) and target real-world environments which exhibit higher-fidelity dynamics, unmodelled effects, and sensor noise; complexity ranges from low-fidelity object motion to contact-rich deformable interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Qualitative: physics/dynamics fidelity (presence of contact-rich interactions, deformables, frictional contacts), degrees of freedom of robot, task dynamical complexity (e.g., high-velocity/flinging vs. slow placement). No single numeric measure reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies; often high for contact-rich and deformable-object tasks</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Domain randomization degree (number and range of randomized image/visual parameters, physical parameters like mass/friction, controller parameters); few-shot settings measured as number of real-world fine-tuning episodes in cited works (not quantified in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>often high when domain randomization is used; can be medium when limited sim variations are applied</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Real-world task performance (success rate), jumpstart (initial benefit), asymptotic performance, time-to-threshold; in practice measured as task success and robustness after transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Discussed qualitatively: increasing environment variation (via domain randomization) aims to cover the spectrum of real-world parameters and improve transfer robustness, but one-shot transfer is 'seldom successful' — additional real-world fine-tuning (few-shot RL/backprop) is typically required. Trade-off: wide variation can improve coverage but increases the need for either larger capacity models or more adaptation in the target domain; privileged learning can reduce required real samples by training a proprioceptive-only student from a privileged teacher.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>domain randomization, privileged learning (teacher-student), reinforcement learning, few-shot fine-tuning, sim-to-sim experiments</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Paper reports that generalization from sim→real is possible but rarely one-shot; domain randomization improves robustness but usually needs additional (few-shot) adaptation in the real domain. Privileged learning showed strong demonstrations in quadrupedal locomotion by cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Often low for real-world adaptation; examples cited where substantial real data or many simulated episodes are used (no unified numeric sample counts in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sim-to-real is a primary instance of environment transfer; domain randomization is common but not sufficient for one-shot transfer; privileged information and few-shot/adaptive methods substantially ease real-world transfer; defining and quantifying the reality gap is necessary to predict transfer performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1053.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1053.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-1: Robotics transformer for real-world control at scale</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language transformer model that maps sequences of images and natural language instructions to discrete robot action tokens; trained on a large real-world dataset and used as an example of robotics transformers in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RT-1: Robotics transformer for real-world control at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RT-1 policy (robotics transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer-based supervised model that tokenizes image patches and language instructions and outputs discrete action tokens (11 discrete variables) to control robot arm, gripper, and base; trained on demonstrations collected in the real world.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>embodied model controlling physical robots (real-world deployment / dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>real-world manipulation environment (dataset of ≈700 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A large set of relatively simple real-world manipulation tasks (≈700 tasks) with diverse object appearances and instructions gathered across many episodes; complexity limited to tasks present in the dataset and to the discrete action parameterization used.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of tasks (~700), dataset size (~130,000 episodes), action dimensionality (11 discrete variables), closed-loop control frequency (3 Hz); task difficulty characterized qualitatively (mostly simple manipulation tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (many tasks but individually relatively simple and limited low-level control modalities)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Dataset diversity: number of distinct tasks (≈700), number of episodes (~130k), visual diversity across scenes and objects; variation constrained to seen concepts in dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high within the training distribution (many tasks / episodes) but limited out-of-distribution variation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task completion and generalization to combinations of seen concepts; ability to match demonstrator performance (success rate / qualitative comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Trained on ≈130,000 episodes over ≈700 tasks (dataset size reported); review states RT-1 'cannot outperform the task demonstrator' and generalization is limited to seen concept combinations (no numeric success rates provided in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper highlights a trade-off: RT-1 fuses high-level semantic reasoning with low-level action mapping into a single monolithic model which eases some forms of transfer (semantic-level) but comes at the cost of reduced low-level performance and limited capability for compliant/dexterous tasks; i.e., scaling up variation and semantic knowledge helps certain generalization but does not solve low-level control complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>supervised imitation learning from a large real-world dataset (tokenized vision + language → discrete actions)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Generalizes to combinations of seen concepts in the dataset, but fails on tasks requiring compliant motions or complex dexterous manipulation; generalization beyond seen concepts is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Relies on large-scale real-world data (≈130,000 episodes) — not sample-efficient in the sense of few-shot in the real world.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large vision-language-action transformers can encode semantic/generalization capabilities from large datasets but are limited by dataset coverage for low-level control tasks; increasing scale helps emergent semantic skills but does not negate the need for low-level controllers or more specialized models for dexterous tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1053.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1053.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2: Vision-language-action models transfer web knowledge to robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger vision-language-action transformer that co-finetunes web-scale vision-language data with robotic datasets, achieving emergent capabilities in the largest models but with practical deployment limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RT-2: Vision-languageaction models transfer web knowledge to robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RT-2 policy (large vision-language-action transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A vision-language-action transformer co-finetuned on web-scale data and robotic action data to transfer abstract visual and semantic knowledge to robot control; produces robot actions but remains constrained by robotic dataset coverage for fine-grained motions.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>embodied model intended for controlling physical robots (research prototype)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>combined web-scale visual/text contexts + robotic action datasets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Training regime spans web images/text and robot demonstrations; environments implicit in robotic datasets and physics are those represented in the robotic data — diversity is large but low-level dynamics fidelity depends on dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Model capacity (55 billion parameters for largest instantiation), multimodal training data diversity (web + robot), and the complexity of robotic tasks present in the robotic dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (large model capacity and multimodal training), but real-world low-level task complexity remains bounded by dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Training data variation: web-scale visual-language variation + variety of robotic tasks; model emergent capabilities correlate with model size.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>very high in training data; practical variation in deployed tasks limited by robotic dataset coverage</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Qualitative emergent capabilities, ability to leverage web-derived abstract relations to solve novel task variants; inference latency (noted as ~1 Hz for the largest model).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Largest RT-2 reported at ~55 billion parameters; inference speed ~1 Hz for largest model; no numeric task success rates provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper notes that scale (model capacity + data variation) yields emergent semantic abilities, but scaling also incurs deployment costs (latency, infrastructure) and does not guarantee improved low-level control for dexterous/compliant tasks; thus higher variation/scale helps semantic generalization but is not a substitute for robust low-level controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>co-finetuning of pre-trained vision-language backbones on mixed web-scale and robot datasets (supervised), large-scale multimodal pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Emergent semantic generalization observed in the largest models enabling some novel task completions using web-derived high-level concepts; still limited in producing unseen low-level motions and impractical to deploy onboard robots due to size/latency.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Low (requires massive pretraining and data), not sample-efficient for on-robot learning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Scaling multimodal models (web + robot) yields higher-level emergent capabilities that aid semantic transfer, but such models are large, slow, and still limited on low-level, contact-rich, or dexterous tasks; decomposing capability stacks may yield better low-level performance and deployability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1053.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1053.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>privileged-learning-quadruped</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Privileged Learning (teacher-student) for Quadrupedal Locomotion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Privileged learning uses simulation-only privileged information to train a high-performance teacher policy which then supervises a proprioceptive-only student policy; cited as effective in quadrupedal locomotion sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning quadrupedal locomotion over challenging terrain.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>quadruped locomotion policies (teacher & student)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Teacher policy trained in simulation with access to privileged simulation signals; a student policy (proprioceptive-only) is trained to imitate the teacher to enable robust sim-to-real transfer, typically using reinforcement learning techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated teacher policy → physical quadruped robot (student policy)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>challenging terrain locomotion (simulation → real-world)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Terrain with varying slopes, roughness, and disturbances that generate complex contact dynamics and require robust dynamically-stable locomotion behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Terrain complexity (roughness, slopes), contact dynamics richness, control DoFs of the quadruped; complexity characterized qualitatively as 'challenging terrain'.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Range of simulated privileged conditions and randomized terrain parameters; degree of variation controlled in simulation during teacher training (not numerically specified in review).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium–high (simulation covers diverse terrains and disturbances)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Real-world locomotion success (stable traversal), robustness to disturbances; in cited works measured by successful gait execution on real robots.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Privileged learning reduces the need to model extensive variation in the student by letting the teacher exploit rich simulated signals; thus it trades off simulation-side complexity/privileged sensing for reduced real-world sample requirements and improved transfer robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>privileged information teacher-student training in simulation followed by student deployment; reinforcement learning for teacher and behavior cloning / distillation for student</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Cited as successfully demonstrated in quadrupedal locomotion by multiple groups (privileged teacher enables robust student policies that transfer to real robots), improving real-world transfer compared to naive sim-to-real methods.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improves apparent sample efficiency on the real robot because the student requires fewer real interactions; exact counts not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Privileged learning is an effective sim-to-real strategy for high-complexity locomotion tasks: simulation-side privileged signals allow learning strong controllers that can be distilled into proprioceptive-only policies that transfer more robustly to real hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1053.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1053.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>inverse-dynamics-negative-transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Negative Transfer of Inverse Dynamics Between Quadrotors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited empirical finding that transferring an inverse-dynamics model learned on one quadrotor to a significantly different quadrotor can degrade control performance relative to a baseline controller that ignores the transferred model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Experience selection using dynamics similarity for efficient multi-source transfer learning between robots.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>quadrotor inverse-dynamics model (transferred)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Inverse-dynamics model trained on a source quadrotor and transferred to a target quadrotor; used as a low-level controller or model within a controller, shown to sometimes cause negative transfer when physical properties differ significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>physical quadrotor robots (source → target)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>aerial flight dynamics environments (source and target quadrotors)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Flight in air with different vehicle masses, inertias, and actuator characteristics producing differences in dynamics; complexity arises from continuous-time flight dynamics and actuator-response differences.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Difference in physical properties (mass, inertia, actuator dynamics), number of controlled axes, continuous-time system dynamics — characterized qualitatively in review.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium–high (aerial dynamics sensitive to physical parameter variations)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Magnitude of distributional shift in dynamics between source and target (physical parameter mismatch); not numerically specified in review.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>can be high if target differs markedly from source</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Control performance relative to baseline controller (tracking/error metrics); in cited studies the transferred inverse-dynamics worsened performance versus baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper highlights that low-level transfer (e.g., inverse dynamics) is brittle: small-to-moderate variation in dynamics can cause negative transfer, implying a trade-off where low-level models transfer only across closely related robots while higher-level abstractions transfer more broadly.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>transfer of learned inverse-dynamics model from source robot to target robot; selection or rejection of experience based on dynamics similarity (cited works address experience selection).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Cited empirical result: transferring inverse dynamics from a substantially different source can degrade target performance below a baseline that ignores the model (i.e., negative transfer occurs), indicating sensitivity of low-level transfer to variation in robot dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Negative transfer is a real hazard at low control levels; low-level models (inverse dynamics) are only safe to transfer across closely-related robots; assessing dynamics similarity is important to avoid performance degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1053.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1053.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARMAR-III-ALMA-example</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ARMAR-III and ALMA cross-embodiment / task / environment transfer example</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Illustrative example used in the paper showing transfer possibilities across two different robots (ARMAR-III humanoid and ALMA quadruped with manipulator), multiple tasks (cloth flinging / box tossing / handover), and environments (simulator and real instances) to highlight environment complexity and variation considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ARMAR-III: An integrated humanoid platform for sensory-motor control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ARMAR-III humanoid & ALMA quadruped-with-arm</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two different embodied robots with different morphologies and capabilities; used in conceptual examples where bimanual strategies and dynamic actions are transferred across embodiments and environments (discussed qualitatively rather than experimentally in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>physical robots (humanoid dual-arm; four-legged robot with manipulator); also simulated counterparts in illustrations</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>simulator (D1) and real-world instances (D2, D3) performing tasks like cloth flinging and box tossing</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulator environment used to train policies and multiple real-world environment instances differing by object instances (cloth, box) and physical conditions; complexity arises from high-velocity dynamic actions required for flinging/tossing and contact/dynamics differences between earth and space (gravity differences mentioned in conceptual example).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task dynamical complexity (e.g., high-velocity flinging), number/type of interacting objects (cloth = deformable, box = rigid), embodiment constraints (balance for humanoid), and physical domain differences (gravity). Measurements are qualitative in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high for dynamic deformable-object manipulation (cloth flinging); medium for rigid-object tossing/placement</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of environment instances (simulator D1, real D2, real D3), object instance variability (different cloths/boxes), and changes in physical parameters (e.g., gravity for space example) — described qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (multiple real instances) to high when domain physics differ substantially (e.g., space vs earth)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Conceptual: transfer success (ability to reuse bimanual strategy across tasks/embodiments/environments), robustness to differing physics; no empirical numeric metrics reported in the review for these illustrative cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Used to illustrate multiple relationships: (1) similar tasks/environments ease transfer even across different robots; (2) large differences in robots/tasks/environments can produce negative transfer; (3) high abstraction-level transfer (task/semantic) can bridge larger variations but requires stronger capabilities (planners, skills).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Illustrative examples mention cross-embodiment transfer, motion retargeting, and the potential use of constrained optimization or reinforcement learning to resolve embodiment-specific tasks (no single implemented training strategy in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Conceptual claim: transfer is feasible when commonalities exist (e.g., bimanual structure) but can fail or be detrimental when embodiments, tasks, and environments differ greatly; transfer at higher abstraction levels (task/semantic) can generalize more but requires richer capability stacks.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Illustrates that environment complexity (deformable vs rigid objects, gravity differences) and environment variation (multiple real instances) interact with embodiment and task similarity; higher abstraction transfers are more tolerant to environment variation but demand more complex robot capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim2real in robotics and automation: Applications and challenges <em>(Rating: 2)</em></li>
                <li>Learning quadrupedal locomotion over challenging terrain <em>(Rating: 2)</em></li>
                <li>Experience selection using dynamics similarity for efficient multi-source transfer learning between robots <em>(Rating: 2)</em></li>
                <li>RT-1: Robotics transformer for real-world control at scale. <em>(Rating: 2)</em></li>
                <li>RT-2: Vision-languageaction models transfer web knowledge to robotic control. <em>(Rating: 2)</em></li>
                <li>Domain adaptation with structural correspondence learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1053",
    "paper_id": "paper-265506816",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "sim-to-real",
            "name_full": "Simulation-to-Reality Transfer",
            "brief_description": "Class of methods that train policies or models in simulated environments and transfer them to physical robots; paper reviews domain randomization, privileged learning, few-shot adaptation and the reality gap as central challenges and solutions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "simulated policy (transfer target: physical robot)",
            "agent_description": "Policies/models trained in simulation using reinforcement learning and/or supervised/backpropagation (including privileged teacher-student setups), then transferred to real robots often via domain randomization and few-shot fine-tuning.",
            "agent_type": "simulated agent (policy) transferred to physical robot",
            "environment_name": "simulator → real-world domains",
            "environment_description": "Simulated environments parameterized to approximate real-world physics (contacts, dynamics, sensors) and target real-world environments which exhibit higher-fidelity dynamics, unmodelled effects, and sensor noise; complexity ranges from low-fidelity object motion to contact-rich deformable interactions.",
            "complexity_measure": "Qualitative: physics/dynamics fidelity (presence of contact-rich interactions, deformables, frictional contacts), degrees of freedom of robot, task dynamical complexity (e.g., high-velocity/flinging vs. slow placement). No single numeric measure reported in this review.",
            "complexity_level": "varies; often high for contact-rich and deformable-object tasks",
            "variation_measure": "Domain randomization degree (number and range of randomized image/visual parameters, physical parameters like mass/friction, controller parameters); few-shot settings measured as number of real-world fine-tuning episodes in cited works (not quantified in this review).",
            "variation_level": "often high when domain randomization is used; can be medium when limited sim variations are applied",
            "performance_metric": "Real-world task performance (success rate), jumpstart (initial benefit), asymptotic performance, time-to-threshold; in practice measured as task success and robustness after transfer.",
            "performance_value": null,
            "complexity_variation_relationship": "Discussed qualitatively: increasing environment variation (via domain randomization) aims to cover the spectrum of real-world parameters and improve transfer robustness, but one-shot transfer is 'seldom successful' — additional real-world fine-tuning (few-shot RL/backprop) is typically required. Trade-off: wide variation can improve coverage but increases the need for either larger capacity models or more adaptation in the target domain; privileged learning can reduce required real samples by training a proprioceptive-only student from a privileged teacher.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "domain randomization, privileged learning (teacher-student), reinforcement learning, few-shot fine-tuning, sim-to-sim experiments",
            "generalization_tested": true,
            "generalization_results": "Paper reports that generalization from sim→real is possible but rarely one-shot; domain randomization improves robustness but usually needs additional (few-shot) adaptation in the real domain. Privileged learning showed strong demonstrations in quadrupedal locomotion by cited works.",
            "sample_efficiency": "Often low for real-world adaptation; examples cited where substantial real data or many simulated episodes are used (no unified numeric sample counts in this review).",
            "key_findings": "Sim-to-real is a primary instance of environment transfer; domain randomization is common but not sufficient for one-shot transfer; privileged information and few-shot/adaptive methods substantially ease real-world transfer; defining and quantifying the reality gap is necessary to predict transfer performance.",
            "uuid": "e1053.0",
            "source_info": {
                "paper_title": "Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RT-1",
            "name_full": "RT-1: Robotics transformer for real-world control at scale",
            "brief_description": "A vision-language transformer model that maps sequences of images and natural language instructions to discrete robot action tokens; trained on a large real-world dataset and used as an example of robotics transformers in the review.",
            "citation_title": "RT-1: Robotics transformer for real-world control at scale.",
            "mention_or_use": "mention",
            "agent_name": "RT-1 policy (robotics transformer)",
            "agent_description": "Transformer-based supervised model that tokenizes image patches and language instructions and outputs discrete action tokens (11 discrete variables) to control robot arm, gripper, and base; trained on demonstrations collected in the real world.",
            "agent_type": "embodied model controlling physical robots (real-world deployment / dataset)",
            "environment_name": "real-world manipulation environment (dataset of ≈700 tasks)",
            "environment_description": "A large set of relatively simple real-world manipulation tasks (≈700 tasks) with diverse object appearances and instructions gathered across many episodes; complexity limited to tasks present in the dataset and to the discrete action parameterization used.",
            "complexity_measure": "Number of tasks (~700), dataset size (~130,000 episodes), action dimensionality (11 discrete variables), closed-loop control frequency (3 Hz); task difficulty characterized qualitatively (mostly simple manipulation tasks).",
            "complexity_level": "medium (many tasks but individually relatively simple and limited low-level control modalities)",
            "variation_measure": "Dataset diversity: number of distinct tasks (≈700), number of episodes (~130k), visual diversity across scenes and objects; variation constrained to seen concepts in dataset.",
            "variation_level": "high within the training distribution (many tasks / episodes) but limited out-of-distribution variation",
            "performance_metric": "Task completion and generalization to combinations of seen concepts; ability to match demonstrator performance (success rate / qualitative comparison).",
            "performance_value": "Trained on ≈130,000 episodes over ≈700 tasks (dataset size reported); review states RT-1 'cannot outperform the task demonstrator' and generalization is limited to seen concept combinations (no numeric success rates provided in this review).",
            "complexity_variation_relationship": "Paper highlights a trade-off: RT-1 fuses high-level semantic reasoning with low-level action mapping into a single monolithic model which eases some forms of transfer (semantic-level) but comes at the cost of reduced low-level performance and limited capability for compliant/dexterous tasks; i.e., scaling up variation and semantic knowledge helps certain generalization but does not solve low-level control complexity.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "supervised imitation learning from a large real-world dataset (tokenized vision + language → discrete actions)",
            "generalization_tested": true,
            "generalization_results": "Generalizes to combinations of seen concepts in the dataset, but fails on tasks requiring compliant motions or complex dexterous manipulation; generalization beyond seen concepts is limited.",
            "sample_efficiency": "Relies on large-scale real-world data (≈130,000 episodes) — not sample-efficient in the sense of few-shot in the real world.",
            "key_findings": "Large vision-language-action transformers can encode semantic/generalization capabilities from large datasets but are limited by dataset coverage for low-level control tasks; increasing scale helps emergent semantic skills but does not negate the need for low-level controllers or more specialized models for dexterous tasks.",
            "uuid": "e1053.1",
            "source_info": {
                "paper_title": "Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2: Vision-language-action models transfer web knowledge to robotic control",
            "brief_description": "A larger vision-language-action transformer that co-finetunes web-scale vision-language data with robotic datasets, achieving emergent capabilities in the largest models but with practical deployment limitations.",
            "citation_title": "RT-2: Vision-languageaction models transfer web knowledge to robotic control.",
            "mention_or_use": "mention",
            "agent_name": "RT-2 policy (large vision-language-action transformer)",
            "agent_description": "A vision-language-action transformer co-finetuned on web-scale data and robotic action data to transfer abstract visual and semantic knowledge to robot control; produces robot actions but remains constrained by robotic dataset coverage for fine-grained motions.",
            "agent_type": "embodied model intended for controlling physical robots (research prototype)",
            "environment_name": "combined web-scale visual/text contexts + robotic action datasets",
            "environment_description": "Training regime spans web images/text and robot demonstrations; environments implicit in robotic datasets and physics are those represented in the robotic data — diversity is large but low-level dynamics fidelity depends on dataset.",
            "complexity_measure": "Model capacity (55 billion parameters for largest instantiation), multimodal training data diversity (web + robot), and the complexity of robotic tasks present in the robotic dataset.",
            "complexity_level": "high (large model capacity and multimodal training), but real-world low-level task complexity remains bounded by dataset.",
            "variation_measure": "Training data variation: web-scale visual-language variation + variety of robotic tasks; model emergent capabilities correlate with model size.",
            "variation_level": "very high in training data; practical variation in deployed tasks limited by robotic dataset coverage",
            "performance_metric": "Qualitative emergent capabilities, ability to leverage web-derived abstract relations to solve novel task variants; inference latency (noted as ~1 Hz for the largest model).",
            "performance_value": "Largest RT-2 reported at ~55 billion parameters; inference speed ~1 Hz for largest model; no numeric task success rates provided in the review.",
            "complexity_variation_relationship": "Paper notes that scale (model capacity + data variation) yields emergent semantic abilities, but scaling also incurs deployment costs (latency, infrastructure) and does not guarantee improved low-level control for dexterous/compliant tasks; thus higher variation/scale helps semantic generalization but is not a substitute for robust low-level controllers.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "co-finetuning of pre-trained vision-language backbones on mixed web-scale and robot datasets (supervised), large-scale multimodal pretraining",
            "generalization_tested": true,
            "generalization_results": "Emergent semantic generalization observed in the largest models enabling some novel task completions using web-derived high-level concepts; still limited in producing unseen low-level motions and impractical to deploy onboard robots due to size/latency.",
            "sample_efficiency": "Low (requires massive pretraining and data), not sample-efficient for on-robot learning.",
            "key_findings": "Scaling multimodal models (web + robot) yields higher-level emergent capabilities that aid semantic transfer, but such models are large, slow, and still limited on low-level, contact-rich, or dexterous tasks; decomposing capability stacks may yield better low-level performance and deployability.",
            "uuid": "e1053.2",
            "source_info": {
                "paper_title": "Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "privileged-learning-quadruped",
            "name_full": "Privileged Learning (teacher-student) for Quadrupedal Locomotion",
            "brief_description": "Privileged learning uses simulation-only privileged information to train a high-performance teacher policy which then supervises a proprioceptive-only student policy; cited as effective in quadrupedal locomotion sim-to-real transfer.",
            "citation_title": "Learning quadrupedal locomotion over challenging terrain.",
            "mention_or_use": "mention",
            "agent_name": "quadruped locomotion policies (teacher & student)",
            "agent_description": "Teacher policy trained in simulation with access to privileged simulation signals; a student policy (proprioceptive-only) is trained to imitate the teacher to enable robust sim-to-real transfer, typically using reinforcement learning techniques.",
            "agent_type": "simulated teacher policy → physical quadruped robot (student policy)",
            "environment_name": "challenging terrain locomotion (simulation → real-world)",
            "environment_description": "Terrain with varying slopes, roughness, and disturbances that generate complex contact dynamics and require robust dynamically-stable locomotion behaviors.",
            "complexity_measure": "Terrain complexity (roughness, slopes), contact dynamics richness, control DoFs of the quadruped; complexity characterized qualitatively as 'challenging terrain'.",
            "complexity_level": "high",
            "variation_measure": "Range of simulated privileged conditions and randomized terrain parameters; degree of variation controlled in simulation during teacher training (not numerically specified in review).",
            "variation_level": "medium–high (simulation covers diverse terrains and disturbances)",
            "performance_metric": "Real-world locomotion success (stable traversal), robustness to disturbances; in cited works measured by successful gait execution on real robots.",
            "performance_value": null,
            "complexity_variation_relationship": "Privileged learning reduces the need to model extensive variation in the student by letting the teacher exploit rich simulated signals; thus it trades off simulation-side complexity/privileged sensing for reduced real-world sample requirements and improved transfer robustness.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "privileged information teacher-student training in simulation followed by student deployment; reinforcement learning for teacher and behavior cloning / distillation for student",
            "generalization_tested": true,
            "generalization_results": "Cited as successfully demonstrated in quadrupedal locomotion by multiple groups (privileged teacher enables robust student policies that transfer to real robots), improving real-world transfer compared to naive sim-to-real methods.",
            "sample_efficiency": "Improves apparent sample efficiency on the real robot because the student requires fewer real interactions; exact counts not provided in this review.",
            "key_findings": "Privileged learning is an effective sim-to-real strategy for high-complexity locomotion tasks: simulation-side privileged signals allow learning strong controllers that can be distilled into proprioceptive-only policies that transfer more robustly to real hardware.",
            "uuid": "e1053.3",
            "source_info": {
                "paper_title": "Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "inverse-dynamics-negative-transfer",
            "name_full": "Negative Transfer of Inverse Dynamics Between Quadrotors",
            "brief_description": "Cited empirical finding that transferring an inverse-dynamics model learned on one quadrotor to a significantly different quadrotor can degrade control performance relative to a baseline controller that ignores the transferred model.",
            "citation_title": "Experience selection using dynamics similarity for efficient multi-source transfer learning between robots.",
            "mention_or_use": "mention",
            "agent_name": "quadrotor inverse-dynamics model (transferred)",
            "agent_description": "Inverse-dynamics model trained on a source quadrotor and transferred to a target quadrotor; used as a low-level controller or model within a controller, shown to sometimes cause negative transfer when physical properties differ significantly.",
            "agent_type": "physical quadrotor robots (source → target)",
            "environment_name": "aerial flight dynamics environments (source and target quadrotors)",
            "environment_description": "Flight in air with different vehicle masses, inertias, and actuator characteristics producing differences in dynamics; complexity arises from continuous-time flight dynamics and actuator-response differences.",
            "complexity_measure": "Difference in physical properties (mass, inertia, actuator dynamics), number of controlled axes, continuous-time system dynamics — characterized qualitatively in review.",
            "complexity_level": "medium–high (aerial dynamics sensitive to physical parameter variations)",
            "variation_measure": "Magnitude of distributional shift in dynamics between source and target (physical parameter mismatch); not numerically specified in review.",
            "variation_level": "can be high if target differs markedly from source",
            "performance_metric": "Control performance relative to baseline controller (tracking/error metrics); in cited studies the transferred inverse-dynamics worsened performance versus baseline.",
            "performance_value": null,
            "complexity_variation_relationship": "Paper highlights that low-level transfer (e.g., inverse dynamics) is brittle: small-to-moderate variation in dynamics can cause negative transfer, implying a trade-off where low-level models transfer only across closely related robots while higher-level abstractions transfer more broadly.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "transfer of learned inverse-dynamics model from source robot to target robot; selection or rejection of experience based on dynamics similarity (cited works address experience selection).",
            "generalization_tested": true,
            "generalization_results": "Cited empirical result: transferring inverse dynamics from a substantially different source can degrade target performance below a baseline that ignores the model (i.e., negative transfer occurs), indicating sensitivity of low-level transfer to variation in robot dynamics.",
            "sample_efficiency": null,
            "key_findings": "Negative transfer is a real hazard at low control levels; low-level models (inverse dynamics) are only safe to transfer across closely-related robots; assessing dynamics similarity is important to avoid performance degradation.",
            "uuid": "e1053.4",
            "source_info": {
                "paper_title": "Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ARMAR-III-ALMA-example",
            "name_full": "ARMAR-III and ALMA cross-embodiment / task / environment transfer example",
            "brief_description": "Illustrative example used in the paper showing transfer possibilities across two different robots (ARMAR-III humanoid and ALMA quadruped with manipulator), multiple tasks (cloth flinging / box tossing / handover), and environments (simulator and real instances) to highlight environment complexity and variation considerations.",
            "citation_title": "ARMAR-III: An integrated humanoid platform for sensory-motor control.",
            "mention_or_use": "mention",
            "agent_name": "ARMAR-III humanoid & ALMA quadruped-with-arm",
            "agent_description": "Two different embodied robots with different morphologies and capabilities; used in conceptual examples where bimanual strategies and dynamic actions are transferred across embodiments and environments (discussed qualitatively rather than experimentally in this review).",
            "agent_type": "physical robots (humanoid dual-arm; four-legged robot with manipulator); also simulated counterparts in illustrations",
            "environment_name": "simulator (D1) and real-world instances (D2, D3) performing tasks like cloth flinging and box tossing",
            "environment_description": "Simulator environment used to train policies and multiple real-world environment instances differing by object instances (cloth, box) and physical conditions; complexity arises from high-velocity dynamic actions required for flinging/tossing and contact/dynamics differences between earth and space (gravity differences mentioned in conceptual example).",
            "complexity_measure": "Task dynamical complexity (e.g., high-velocity flinging), number/type of interacting objects (cloth = deformable, box = rigid), embodiment constraints (balance for humanoid), and physical domain differences (gravity). Measurements are qualitative in the review.",
            "complexity_level": "high for dynamic deformable-object manipulation (cloth flinging); medium for rigid-object tossing/placement",
            "variation_measure": "Number of environment instances (simulator D1, real D2, real D3), object instance variability (different cloths/boxes), and changes in physical parameters (e.g., gravity for space example) — described qualitatively.",
            "variation_level": "medium (multiple real instances) to high when domain physics differ substantially (e.g., space vs earth)",
            "performance_metric": "Conceptual: transfer success (ability to reuse bimanual strategy across tasks/embodiments/environments), robustness to differing physics; no empirical numeric metrics reported in the review for these illustrative cases.",
            "performance_value": null,
            "complexity_variation_relationship": "Used to illustrate multiple relationships: (1) similar tasks/environments ease transfer even across different robots; (2) large differences in robots/tasks/environments can produce negative transfer; (3) high abstraction-level transfer (task/semantic) can bridge larger variations but requires stronger capabilities (planners, skills).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Illustrative examples mention cross-embodiment transfer, motion retargeting, and the potential use of constrained optimization or reinforcement learning to resolve embodiment-specific tasks (no single implemented training strategy in this review).",
            "generalization_tested": true,
            "generalization_results": "Conceptual claim: transfer is feasible when commonalities exist (e.g., bimanual structure) but can fail or be detrimental when embodiments, tasks, and environments differ greatly; transfer at higher abstraction levels (task/semantic) can generalize more but requires richer capability stacks.",
            "sample_efficiency": null,
            "key_findings": "Illustrates that environment complexity (deformable vs rigid objects, gravity differences) and environment variation (multiple real instances) interact with embodiment and task similarity; higher abstraction transfers are more tolerant to environment variation but demand more complex robot capabilities.",
            "uuid": "e1053.5",
            "source_info": {
                "paper_title": "Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim2real in robotics and automation: Applications and challenges",
            "rating": 2,
            "sanitized_title": "sim2real_in_robotics_and_automation_applications_and_challenges"
        },
        {
            "paper_title": "Learning quadrupedal locomotion over challenging terrain",
            "rating": 2,
            "sanitized_title": "learning_quadrupedal_locomotion_over_challenging_terrain"
        },
        {
            "paper_title": "Experience selection using dynamics similarity for efficient multi-source transfer learning between robots",
            "rating": 2,
            "sanitized_title": "experience_selection_using_dynamics_similarity_for_efficient_multisource_transfer_learning_between_robots"
        },
        {
            "paper_title": "RT-1: Robotics transformer for real-world control at scale.",
            "rating": 2,
            "sanitized_title": "rt1_robotics_transformer_for_realworld_control_at_scale"
        },
        {
            "paper_title": "RT-2: Vision-languageaction models transfer web knowledge to robotic control.",
            "rating": 2,
            "sanitized_title": "rt2_visionlanguageaction_models_transfer_web_knowledge_to_robotic_control"
        },
        {
            "paper_title": "Domain adaptation with structural correspondence learning",
            "rating": 1,
            "sanitized_title": "domain_adaptation_with_structural_correspondence_learning"
        }
    ],
    "cost": 0.023521,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges
2 May 2024</p>
<p>C Welle mwelle@kth.se 
KTH Royal Institute of Technology
StockholmSweden</p>
<p>Andrej Gams 
Jožef Stefan Institute
LjubljanaSlovenia</p>
<p>Kunpeng Yao 
Learning Algorithms and Systems Laboratory
Ecole Polytechnique F éd érale de Lausanne
LausanneSwitzerland</p>
<p>Bernardo Fichera 
Learning Algorithms and Systems Laboratory
Ecole Polytechnique F éd érale de Lausanne
LausanneSwitzerland</p>
<p>Aude Billard 
Learning Algorithms and Systems Laboratory
Ecole Polytechnique F éd érale de Lausanne
LausanneSwitzerland</p>
<p>Ale Š Ude 
Jožef Stefan Institute
LjubljanaSlovenia</p>
<p>Faculty of Electrical Engineering
University of Ljubljana
Slovenia</p>
<p>Tamim Asfour 
Institute for Anthropomatics and Robotics
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Danica Kragic 
KTH Royal Institute of Technology
StockholmSweden</p>
<p>KTHMichael C Welle </p>
<p>No émie Jaquier
Institute for Anthropomatics and Robotics
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Royal Institute of Technology
StockholmSweden</p>
<p>Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges
2 May 202491A01BC18A072DC84DEBBD909FC4859310.1177/ToBeAssignedarXiv:2311.18044v3[cs.RO]Transfer learningimitation learningdomain adaptationsim-to-realtask transferembodiment transfer domain adaptationdomain-adaptationmeta-learningmeta learningsim2realsim to realsim-to-realknowledge transfertransfer of knowledgeskill transferskills transfertransfer of skillstask transfertasks transfertransfer of tasksmotion retargetingmotion-retargetingembodiment transfertransfer embodimentsmorphology transfertransfer morphologykinematic transfertransfer kinematicrobot transferrobot to robotimitation learninglearning by imitationimitation-learninglearning from demonstrationslearning from demonstrationlearning by demonstrationslearning by demonstrationbehavioral cloning
Transfer learning is a conceptually-enticing paradigm in pursuit of truly intelligent embodied agents.The core concept -reusing prior knowledge to learn in and from novel situations -is successfully leveraged by humans to handle novel situations.In recent years, transfer learning has received renewed interest from the community from different perspectives, including imitation learning, domain adaptation, and transfer of experience from simulation to the real world, among others.In this paper, we unify the concept of transfer learning in robotics and provide the first taxonomy of its kind considering the key concepts of robot, task, and environment.Through a review of the promises and challenges in the field, we identify the need of transferring at different abstraction levels, the need of quantifying the transfer gap and the quality of transfer, as well as the dangers of negative transfer.Via this position paper, we hope to channel the effort of the community towards the most significant roadblocks to realize the full potential of transfer learning in robotics.</p>
<p>The Rise of Transfer Learning in Robotics</p>
<p>Transferring prior knowledge to novel unknown tasks is one of the abilities that led humans to become the most innovative species on the planet (Reader et al. 2016).In particular, humans' capability to transfer cognitive (Perkins and Salomon 1992;Barnett and Ceci 2002) and motor skills (Schmidt and Young 1987) from one context to another makes the acquisition of new skills and the resolution of problems possible to a large extent.For instance, the difficulty of learning a new language is significantly influenced by factors such as language distance, native language proficiency, and language attitude (Walqui 2000) as humans can transfer their prior experience, e.g., grammatical constructions or words, from their native language into the new language.In addition, transfer is a key concept in education as the context of learning, e.g., the classroom, significantly differs from the context, e.g., the workplace, where the learned concepts should ultimately be applied (Perkins and Salomon 1992).</p>
<p>To evolve seamlessly in the real world, robots must feature outstanding cognitive abilities allowing them to perceive their environment, act and react to achieve various goals, and learn continuously from observation and experience, while coping with changes and uncertainty in the world.The transfer learning paradigm for robotics is a promising avenue to avoid learning from scratch by reusing previouslyacquired experience in new situations, similar to humans.The core idea of transfer learning in robotics, illustrated in Figure 1, is simple: The experience of a robot performing one task in an environment is leveraged to improve the learning process of a (related) task in a different context, i.e., in a different environment or executed by a different robot.</p>
<p>To identify when transfer learning is needed and/or warranted, one need to identify the similarities and differences between the two situations.Following the concepts proposed in imitation learning (Dautenhahn and Nehaniv 2002), we distinguish between three aspects that can be deemed similar or different: The tasks, the environments, and the bodies (a.k.a. the robots in our case).</p>
<p>In the conceptual example of Figure 1, the experience of the two fixed-based manipulators placing a box on a conveyer belt (left) can be transferred to a humanoid robot executing the same task (middle-top).Transfer learning is made possible and easier as the two situations have many aspects in common:</p>
<ol>
<li>Tasks: Both tasks are similar as both robots are engaged in moving an object with two arms.The It is important to note that successful transfer requires commonalities between the source and target robots, tasks, and environments.</li>
</ol>
<p>For instance, a humanoid robot learning to kick a ball will most likely not benefit from the experience of a dual-arm manipulator systems manipulating a box and vice-versa.</p>
<p>notion of similarity with respect to the object may be relative to the robot's own size and payload.For instance, a pair of industrial arms may be able to support objects up to 20 kg whereas a humanoid robot may carry only a quarter of this.Yet, the task and strategy to solve it, namely ensuring coordinated movement of both arms and the trajectory to place an object on a conveyor's belt, remains the same.2. Environments: The two environments are similar, as in both cases, the object is to be placed on a conveyor belt.We can also safely assume that, in both cases, the object is subjected to the same external forces (gravity, friction).3. Robots: In this case, the two robots differ.Yet, their differences can be broken into two distinct parts.Both robots are endowed with two arms of similar structure.Solely the second robot is on legs and hence faces the additional challenge of controlling its balance when placing the object on the conveyor belt.This aspect may be resolved without actual learning, for instance, through constrained-based optimization, controlling for additional constraints at the center of mass (Bouyarmane et al. 2018).Alternatively, reinforcement learning or adaptive control may be used to fine tune gains (Khadivar et al. 2023).</p>
<p>Hence, in this case, transfer learning is mostly conducted across the different robots, while taking advantage of their similar bimanual structure.In other cases, when the two robots and/or environments differ importantly, experience may be transferred across tasks.For instance, the bimanual manipulation strategy used when placing a box on a conveyer belt (Figure 1, middle-top) may be transferred to a handover task (middle).Such transfer may also be achieved across different tasks and different embodiments: The bimanual strategy of the two fixed-based manipulators (left) may directly be transferred to a different robot executing a related task, e.g., to the humanoid robot handing over the box (middle).Experience may also be transferred across environments.For example, the bimanual manipulation strategies used by the humanoid robot to place a box on the conveyer belt or to handover the box (Figure 1, middle-top and middle) may be reused by a humanoid robot manipulating a box in space (right).In this example, the physical rules of the two environments strongly differ due to the influence of gravity on Earth.Finally, in some cases, transferring knowledge may not be beneficial or may even impede the robot performance.For instance, we consider transferring experience from the fixed-based bimanual setup placing a box in a conveyer belt to a humanoid robot kicking a ball.In this case, employing transfer learning might not be beneficial.Instead, the large differences between the robots, tasks, and environments might even cause transfer learning to impede the methods employed to fulfill the ball-kicking task.It is important to highlight that, even if some particular actions can be transferred, conflicting goals may lead to negative transfer.This clearly showcases the crucial importance of identifying the commonalities and differences between two situations when applying transfer learning in robotics.Moreover, it highlights the importance of transfer learning metrics that measure not only the transfer quality, but also the transfer gap between robots, tasks, and environments.</p>
<p>While the terms falling under the umbrella of transfer learning in robotics are not exactly agreed upon, the robotics community has intensified its effort to transfer various forms of knowledge across different contexts.Figure 2 (top) shows the recent rise in the proportion of published papers at the two biggest robotics conferences -IROS and ICRAinvoking keywords * that we consider as falling under the umbrella of transfer learning.This recent interest shows that the community strives for embodied transfer learning, which may be a necessary a-priori for truly intelligent systems (Kremelberg 2019).It is interesting that different keywords related to transfer learning in robotics display different growth rates (see Figure 2, bottom).For instance, terms related to imitation learning, behavioral cloning, and learning from demonstrations display an early rise and a high popularity nowadays, highlighting the effort of the robotics community to tackle transfer between embodiments (humanto-robot or robot-to-robot) from early on (Dautenhahn and Nehaniv 2002).In contrast, terms related to transfer across environments (i.e., sim-to-real and domain adaptation) only recently gained interest, while terms related to transfer across tasks remain less documented, suggesting that transfer learning across environments and tasks is still in its infancy.</p>
<p>In this position paper, we contend that transfer learning in robotics has the potential to revolutionize the robot learning paradigm by enabling robots to leverage past experience in novel contexts.However, key challenges remain to be addressed to fulfill this potential.In particular, the fundamental question of identifying the similarities and differences across tasks, environments, and robots in an automatic manner remains.This position paper reviews the successes of the field and identifies relevant research questions and promising directions paving the way forward.Starting from the definition of transfer learning in the machine learning field, we propose a unified definition of transfer learning in robotics and subsequently build a novel taxonomy of transfer learning in robotics based on the key concepts of robot, task, and environment (see Section 2).Then, we recount successful applications of transfer learning in robotics and show how they align with the proposed taxonomy (Section 3).In Section 4, we outline challenges, as well as promising research directions to tackle them, including abstraction levels and universal representations for transfer learning in robotics, interpretability, benchmarks and simulations, transfer learning metrics, and dangers of negative transfer.Last but not least, we call for actions to address the most immediate roadblocks in Section 5. Overall, the contributions of this paper are twofold: (1) We provide a unified view of transfer learning in robotics by comprehensibly defining the notion of transfer learning in robotics and by introducing the first taxonomy of transfer learning in robotics; (2) We provide a review of promises and challenges in the field of transfer learning in robotics, identifying the most significant roadblocks on the way to unraveling the full potential of transfer learning in robotics.</p>
<p>Transfer Learning Taxonomies: From Machine Learning to Robotics</p>
<p>While the machine learning community has devoted substantial efforts to defining and systematizing transfer learning and categorizing its different instances, transfer learning in robotics is found under various terminologies.This section aims at providing a unified view of transfer learning in robotics.To do so, we take inspiration from machine learning taxonomies and define a taxonomy of transfer learning settings that occur in robotics.</p>
<p>Taxonomy of Transfer Learning in Machine Learning</p>
<p>In this section, we introduce the definitions that are commonly adopted in the transfer learning community, see e.g., (Pan and Yang 2010;Zhuang et al. 2020;Yang et al. 2020).Transfer learning builds on the two fundamental concepts of domain and task.A domain D = {X , p(X)} consists of a feature space X and a marginal distribution p(X), with X denoting an instance set of the feature space such that X = {x i } N i=1 , x i ∈ X .Given a specific domain D, a task T = {Y, f } consists of a label space Y and a predictive function f : X → Y.The predictive function is used to predict new labels y ∈ Y associated with a new instance x ∈ X .It is typically learned from a training dataset {x i , y i } M i=1 , with x i ∈ X , y i ∈ Y.While standard machine learning approaches assume that training and test datasets share common domains and tasks, in the case of transfer learning they may instead belong to different spaces, referred to as source and target spaces.Therefore, it also has the potential to tackle open-set problems (Geng et al. 2021).An example from computer vision is shown in Figure 3.</p>
<p>Transfer learning approaches are commonly categorized into inductive, transductive, and unsupervised transfer learning (Pan and Yang 2010;Zhuang et al. 2020;Yang et al. 2020).This categorization focuses on the availability of labels independently of the relationships between source and target spaces.Namely, in the inductive setting, labels are available in both source and target spaces, while they are only available in the source space in the transductive setting, and are not available in any space in the unsupervised setting.For a more encompassing categorization that generalizes to robotics, we instead propose to focus on the fundamental concepts of domain and task and on their relationship in the source and target spaces.We observe that Definition 2.1 implies the following hierarchical taxonomy of transfer learning settings illustrated in Figure 4.Note that source space may correspond to a union of multiple sub-source tasks and/or domains.</p>
<ol>
<li>Task transfer learning: {D, T S } → {D, T T }.In this setting, the target task differs from the source task, T S ̸ = T T .This can indicate a difference in the label space, the predictive function or both.Notice that we refer to task transfer learning whenever the source and target domains are identical while the source and target tasks differ.The extent and conditions of the task differences were further categorized according to various transfer learning taxonomies, see, e.g., (Pan and Yang 2010;Zhuang et al. 2020;Yang et al. 2020).</li>
</ol>
<p>Task transfer learning approaches include learning strategies involving Gaussian process (GP) prior sharing across different tasks (Lawrence and Platt 2004;Bonilla et al. 2007).Other strategies focus on sharing the parameters of the model itself rather than the hyperparameters.One important category of algorithms in this domain takes advantage of a modified version of support vector machine (SVM) to transfer knowledge between source and target spaces (Evgeniou and Pontil 2004;Li et al. 2012).</p>
<p>In this modified SVM, the model's parameters consist of a part shared across the source and target spaces, while the other part is space-specific.The uniqueness of the solution (learning efficiency) and model interpretation make these convex optimization algorithms an interesting solution for robotics.Multilinear relationship networks (Long et al. 2017) leverages labeled data from related source domains by adopting a Bayesian framework for the task-specific portion of the network.2. Domain transfer learning: {D S , T } → {D T , T }.</p>
<p>This form of transfer learning occurs when the source and target tasks are identical, T S = T T , but the source and target domains differ.The condition D S ̸ = D T can indicate a difference in the feature space, in the marginal distribution, or in both.It is generally assumed that the domains are related to a certain extend.</p>
<p>The difference in marginal distribution is often tackled by learning a mapping between overlapping instances (also known as "support") between the source and target domains D S and D T .Such approaches primarily rely on instance weighting strategies, such as assigning weights to instances or labeled data in D S for reuse in D S .For instance, kernel mean matching (Huang et al. 2006) matches source and target domain instance means within a reproducing kernel Hilbert space.In the case of different feature spaces (X S ̸ = X T ), existing approaches aim at reducing domain differences while preserving the properties or structures within the same domain.For instance, structural correspondence learning (Blitzer et al. 2006) utilizes pivot features to establish pseudo tasks connected to the target task and applies multi-task learning techniques to model relationships between pivot features and other features.Spectral feature alignment (Pan et al. 2010) models inter-dependencies between pivot features and other features using a bipartite graph and identifies novel common features through spectral clustering methods applied to the graph.3. Dual-mode transfer learning: {D S , T S } → {D T , T T }.In this scenario, both the source and target domains and tasks differ.This is the most challenging setting in transfer learning as every additional difference between the source and target label space, predictive function, feature space, and marginal distribution increases the complexity of the problem.</p>
<p>Approaches in the dual-mode are currently mostly related to unsupervised transfer learning scenarios.This remains an underexplored area due to the difficulty of capturing the similarities -or the transferable information (instance, feature, parameter, etc.) -between the source and target spaces.</p>
<p>Figure 3 illustrates the aforementioned transfer learning setting using the transfer dataset Office 31 (Saenko et al. 2010), a classical benchmark for transfer learning.In this case, the domains correspond to the source used to obtain the images (i.e., Amazon and a webcam in Figure 3) and the tasks correspond to the labels of the objects represented in the images.To extend these concepts to robotics, we must consider the robot as an additional mode.In the next section, we discuss its implications for transfer learning in robotics.</p>
<p>Taxonomy of Transfer Learning in Robotics</p>
<p>Transfer learning in robotics builds on the three fundamental concepts of robot, environment, and task.A robot R is defined as an embodiment that can act in and thus influence its environment.It encompasses a body with defined morphology, kinematics, dynamics, and sensor modalities.In robotics, the domain D is generally considered equivalent to the environment, which is defined as the virtual or physical world in which the robot lives and interacts.The robot accesses the state of the environment via sensory observations, e.g., images, contact forces, auditory and olfactory signals.Informally, the task T refers to what the robot is required to do in the environment.More formally, a task is a discrete or continuous (sub)goal that can be achieved by the robot through (inter)actions within the environment.In general, the goal of the robot is to perform a given task in the environment.The goal of transfer learning in robotics is to leverage prior knowledge from a source space, composed by a robot, a task, and an environment, to improve the performance in a target space, where one or more mode differs from the source space.Formally, we define transfer learning in robotics as an analogy of the machine learning definition 2.1 as follows.</p>
<p>Definition 2.2.Transfer Learning in Robotics.Let S = {R S , D S , T S } a source space and T = {R T , D T , T T } a target space.The objective of transfer learning in robotics is to improve the performance of the robot R T executing the task T T in the environment D T by taking advantage of knowledge from the source robot † R S , environment D S , and task T S , where at least one element of the target space T is different from its counterpart in the source space S.</p>
<p>It is important to emphasize that, unlike transfer learning in machine learning which only involves disembodied agents, the agent's embodiment -in other words, the robot -is key for transfer learning in robotics.This introduces additional challenges: The presence of a robot not only adds an additional mode to the transfer learning problem and thus to the hierarchical categorization, but also brings numerous robotics-specific issues.Transfer learning methods for robotics must cope with the fact that robots are embodied agents that act and interact in the real world.</p>
<p>Inspired by the hierarchical taxonomy defined for transfer learning in machine learning community, we propose a hierarchical taxonomy for transfer learning in robotics based on the relationship between the source and target robots, environments, and tasks, as shown in Figure 5. Specific illustrative examples of its categories are depicted in Figure 6.Our taxonomy considers the following settings:</p>
<ol>
<li>
<p>Robot transfer learning: {R S , D, T } → {R T , D, T }.The goal of this setting is to endow a target robot with the ability to perform a given task known by other source robot(s) in the same environment.Note that the source and target robots may have (very) different morphologies, kinematics, and sensor modalities, leading to different capabilities.For example, Figure 6 illustrates a transfer between the humanoid dual-arm robot ARMAR-III (Asfour et al. 2006) and ALMA (Bellicoso et al. 2019), a fourlegged robot equipped with a robotic arm.Moreover, the transfer can happen at different actions levels, e.g., at the level of joint or task-space controllers, or at the planning level, and at different perceptual levels, e.g., across different sensors (Tatiya et al. 2020) or sensory modalities (Lee et al. 2020b).Instances of robot transfer learning are (i) imitation learning (Schaal 1999), where a teacher human or robot provides demonstrations of a task to a student robot that learns to reproduce the given task in the same environment, (ii) (goal-directed) motion retargeting (Dariush et al. 2008;Yin et al. 2023), whose goal is to learn a mapping between different kinematic structures, and (iii) perceptual transfer, where the robots are equipped with different sensory modalities (Silva et al. 2020) such as touch, vision, sound, or olfaction.2. Environment transfer learning: {R, D S , T } → {R, D T , T }.This setting aims at transferring the  ability of a robot to perform a given task in a source environment to a different target environment.Its main challenge is to overcome the mismatch between source and target environments in terms of data and environment parameters such as, e.g., underlying dynamics or transition models.For instance, models learned for a specific task performed on earth typically needs to be adapted to perform the same task in an underwater environment or in space.This requires identifying which physical parameters differ between D S (the earth) and D T (the underwater environment, or space).Typical instances of environment transfer learning are (i) domain adaptation (Bousmalis et al. 2018;Wang and Johnson 2021), and (ii) sim-toreal transfer (Muratore et al. 2022).The latter is a particular case of the former in which the experience is explicitly transferred from a simulation environment -in which training data are inexpensive and models are fast to train -onto the real world.Sim-toreal transfer is showcased by the first and second rows in Figure 6.The second and third rows indicate transfer between two real-world environments, where the objects composing the physical environment (the cloth or the box) differ.</p>
</li>
<li>
<p>Task transfer learning: {R, D, T S } → {R, D, T T }.</p>
</li>
</ol>
<p>This setting aims at leveraging the ability of a robot to perform a given task to learn how to execute a different task in the same environment.The underlying assumption is that the source and target tasks areto some extent -similar, so that experience can be reused between source and target tasks.For instance, the box tossing and cloth flinging tasks of Figure 6 share similar dynamics characteristics: In both cases, the robot must generate high-velocity dynamic actions to successfully execute the task.Therefore, we may expect that experience about box tossing may be reused by the robot when learning to fling a cloth.Challenges of task transfer learning include inferring which part of the source task experience should be transferred and at which level (joint or task space, planning, etc).Notice that generalizing a given task to an unseen context is a special case of task transfer learning (Mandlekar et al. 2020;Li and Figueroa 2023).In this case, the model is made compatible with different instances of the same task.Curriculum learning (Narvekar et al. 2020;Shukla et al. 2022) is also a special case of task transfer learning, where a sequence of intermediary tasks of gradually-increasing difficulty is used to learn a complex target task.Finally, task transfer is also particularly considered in the areas of lifelong learning, where task transfer is considered based on a never-ending stream of data, and compositional learning, which focuses on transfer across compositionally-related tasks (Mendez andEaton 2023, 2021).4. Dual-mode transfer learning: {R S , D S , T } → {R T , D T , T }, {R S , D, T S } → {R T , D, T T }, or {R, D S , T S } → {R, D T , T T }.This setting is concerned by transferring knowledge between two spaces which differ across two modes.It assumes that the similarities between source and target spaces can still be leveraged when they share a single common mode.For instance, in Figure 6, it is reasonable to assume that experience acquired in simulation to toss a box may be reused to fling a cloth with the same robot in the real world.Dual-mode transfer learning remains largely unexplored in robotics due to the additional level of complexity compared to the single-mode transfer setting listed above, which has not yet been fully resolved.5. Triple-mode transfer learning: {R S , D S , T S } → {R T , D T , T T }.This setting assumes that all three modes of the source and target spaces differ.It is inspired by the human ability to successfully acquire knowledge by observing others executing similar tasks in different environments.For instance, one may observe a chef cooking a pie in a restaurant kitchen and reuse some of her techniques to cook a cake in her own non-professional kitchen.Reusing knowledge from a source space in a target space with different robots, environments, and tasks would endow robots with human-like generalization abilities.This setting is the most challenging, and requires bridging the gaps between high-level semantic information -indicating the degree of similarity between spaces -and lowlevel actions.It is the ultimate goal of transfer learning in robotics, as indicated by the red arrow in Figure 6.</p>
<p>Notice that, depending on the relationship between the source and target spaces, our Definition 2.2 intrinsically refers to related fields, some of which received significant attention over the years.In Figure 2 (bottom), we notably observe that imitation learning is the most mentioned transfer learning field followed by sim-to-real and domain adaptation.In this sense, we view transfer learning in robotics as an umbrella term that encompasses "imitation learning", "learning from demonstrations", "sim-to-real", "domain adaption", "meta-learning", "knowledge transfer", "skill transfer", "motion retargeting", "embodiment transfer", "morphology transfer", and "kinematic transfer", among others.</p>
<p>Successes of Transfer Learning in Robotics</p>
<p>Change of environment or domain as in {R, D S , T } → {R, D T , T }, change of task as in {R, D, T S } → {R, D, T T } and change of the robot as in {R S , D, T } → {R T , D, T } have all been addressed with varying success in transfer learning in robotics.The body of literature on the topic is extremely vast, making a comprehensive overview beyond the scope of this paper.On the other hand, research activities that by definition fit into the scope of transfer learning have been addressed before the term took root in robotics.An example of such is imitation learning (Schaal 1999), where task execution knowledge is transferred from the human to the robot, or generalization, where task execution knowledge is transferred to (at least) a variation of the task (Ude et al. 2010).In the following, we provide examples of transfer learning in robotics, also in the light of such abovementioned applications.</p>
<p>Environment Transfer</p>
<p>Change of environment conditions (Kramberger et al. 2016) or the complexity of the environment where the task is being executed (Vosylius and Johns 2023) provide examples of generalization to a declaratively new environment.However, the environment (domain), can be different in other aspects that go beyond just the setting -e.g.contact conditions or other physical conditions might not be the same (Muratore et al. 2022).One example of such is transfer from the simulation-to-reality or sim-to-real.Potentially unjustly, but transfer learning in robotics is often associated exactly with sim-to-real, where typically experience is obtained in one domain -the simulation -, and exploited to accelerate learning in the transferred domain -the real world.Several reviews cover sim-to-real transfer learning in robotics, i. e., (Muratore et al. 2022;Zhao et al. 2020), affirming the notion of a huge body of work in this field.The gist of sim-to-real lies in the notion that collecting the data for modern (deep) learning and other AI algorithms in the real world is too expensive in terms of time and resources to scale up (Muratore et al. 2022).Therefore, the data is collected in simulation, despite the difference between the real and simulated domains.This difference, referred to as the "reality gap" (Collins et al. 2019a), needs to be overcome for real world execution, which is done using transfer learning.Since collecting data in the real world is so time-consuming and expensive, researchers might change the domain to a different simulation, ending up with simto-sim methodologies.These are applied to demonstrate the behavior of transfer learning methodologies.</p>
<p>Different practices have been proposed for sim-to-real transfer learning, starting with realistic modelling (Muratore et al. 2022).No matter how accurate, modelling will never be fully cover all the aspects of the real world (Muratore et al. 2022), thus other approaches have emerged.Domain randomization, such as randomization of image backgrounds, of physical parameters of objects and robot actions, or of controller parameters (Höfer et al. 2021), is a common approach.By randomizing over, for example, physical parameters, the approach tries to cover the entire spectrum of these parameters in the hope that this includes the parameters that describe the real world.Even so, oneshot transfer learning is seldom successful (Zhao et al. 2020), and additional learning is required, for example using reinforcement learning (Ada et al. 2022), backpropagation (Chen et al. 2018) or both (Lončarević et al. 2022).If there are significantly fewer learning iterations in the target domain, the process is called few-shot transfer learning (Ghadirzadeh et al. 2021).Few-shot transfer learning has notably been applied to sim-to-real transfer in robotics (Bharadhwaj et al. 2019;Shukla et al. 2023).Given that more information can be available in the simulation, the notion of privileged learning was introduced, where the privileged information is used to train a high performance policy, which in turn trains a proprioceptive-only student policy (Lee et al. 2020a).The idea was very successfully demonstrated in quadrupedal locomotion by more than one group (Lee et al. 2020a;Kumar et al. 2022), and is general enough to be applied for very different tasks, such as excavator walking (Egli and Hutter 2022) and even robotized handling of textiles (Longhini et al. 2022).</p>
<p>Task Transfer</p>
<p>Transferring of robot walking from one domain to the other can be considered more than just domain transfer, as walking itself can be different for different environments.For instance, a pacing gait learned to walk on smooth ground might not be stable enough for walking on mountainous terrains.Thus, walking on mountainous terrains can be seen as a novel task, which may benefit from transferring previously-learned gaits adapted to other terrains.Moreover, walking is not an isolated instance: If the robot can learn to throw accurately at one target, a modulation of the throwing task to aim at a different target can in fact be considered at the least a different instance of the same task, if not a different task overall.</p>
<p>Such transfers from one (or several) task instances to a new one have been utilized in robotics before, and were often referred to as generalization.In this sense, for example, fast learning from a small set of demonstrations was applied with nonlinear autonomous dynamical system (DS), which have the ability to generalize motions to unseen contexts (Khansari-Zadeh and Billard 2011).Similarly, a set of dynamical systems in the form of dynamic movement primitives was used to generalize to transfer knowledge from known situations to unknown in positions (Ude et al. 2010) and in torques (Deniša et al. 2016), probabilistic movement primitives (ProMPs) encode complete families of motions (Paraschos et al. 2013), TP-GMMs adapt to changes of predefined local frames (Calinon 2018), and Mixture Density Networks adapt a learned motion primitive to new targets specified in a different space (Zhou et al.  2020)  ‡ .Generalization was even termed inter-task transfer learning (Fernández et al. 2010).</p>
<p>Task transfer has also been tackled via meta-learning.In the meta-learning setting, a model is trained on a variety of tasks so that new tasks are solved by using none (zero-shot) or only a limited amount (few-shot) of additional training data (Finn et al. 2017;Nichol et al. 2018).For instance, MAML (Finn et al. 2017) was shown to generalize to new goal velocities and directions in the half cheetah and ant locomotion tasks of the Gymnasium benchmark (Towers et al. 2023) faster than conventional approaches.Other metalearning approaches tackle the transfer problem from a different perspective by learning loss functions (Bechtle et al. 2021(Bechtle et al. , 2022)).The meta-learned loss functions generalize to different tasks, thus alleviating the need of designing taskspecific losses.</p>
<p>Thus, in a broad sense of Definition 2.2, such approaches already propose solutions for {R, D, T S } → {R, D, T T }, although they were not called transfer learning.Complete skill models were learned from a set of executions also with DNNs (Lončarević et al. 2022).The adaptation of the skill model for a new environment is commonly referred to as transfer learning.</p>
<p>Robot Transfer</p>
<p>Above mentioned approaches use knowledge from several instances of a task.However, learning of even one instance of a task could pose a challenge.Imitation learning, where human skill knowledge was transferred to a robot, has been thoroughly researched as the means for learning of task models and their execution on a robot (Billard et al. 2008;Ravichandar et al. 2020).Imitation learning (IL), also known as programming by demonstration (PbD), is in a strict sense an example where the task and the environment remain the same, but the agent is different, {R S , D, T } → {R T , D, T }, since one of the agents is in fact a person.Note that, in some cases, the environment can also change.In PbD one often transfers the demonstrated motion (Ijspeert et al. 2013).However, if only the motion is repeated, the task knowledge might be overlooked and the task correspondence (Heyes 2001) might not get preserved at all.This may be alleviated by transferring other crucial characteristics of the task, so-called task constraints, such as force patterns (Rozo et al. 2016) and posturedependent task requirements (Jaquier et al. 2020), or by retargeting the demonstrated motion (Aberman et al. 2020), e.g., by leveraging optimization methods (Rakita et al. 2017), learning approaches, or Riemannian geometry (Klein et al. 2022).Task descriptions in the form of reward ‡ Note that demonstrations and reproductions performed with the same robot avoid the need for robot transfer.This includes demonstrations acquired via kinesthetic teaching or teleoperation.</p>
<p>Prepared using sagej.clsfunctions learned from demonstrations are also promising for transferring tasks across different robots.For instance, crossembodiment inverse reinforcement learning (XIRL) (Zakka et al. 2022) learns a notion of task progress from demonstrations, which is then used as a reward for robots with different embodiments that successfully learn to reproduce the task.</p>
<p>Challenges and Promising Research Directions</p>
<p>The aforementioned examples highlight that knowledge can be transferred across several robots, tasks, and environments, thus highlighting the potential of transfer learning for robotics.However, several key questions falling under the areas of identifying the similarities and differences across tasks, environments, and robot to single out what should be transferred and when remain to be answered to realize the full potential of transfer learning in robotics.In this section, we describe the key challenges that currently constitute roadblocks on the way to the future of transfer learning in robotics.</p>
<p>Abstraction Levels in Robotics</p>
<p>Humans and some animals, such as great apes, acquire cognitive skills via the concept of social learning (Whiten and Ham 1992), whose main component is to copy (transfer) behavior from one individual to another.Social learning takes place at different levels depending on the goal and context.In biology, the lowest level of transfer corresponds to mimicry, where an individual mimics the actions of another individual superficially, i.e., without any underlying understanding of the goal (Genschow et al. 2017).Instead, with a number of methodological differences, imitation refers to an individual, i.e., the learner, copying the actions of another individual, i.e., the teacher, with the aim of achieving the same goal.As opposed to mimicry, imitation implies an explicit understanding of the goal.At the next level, emulation refers to the case where the learner aims at achieving the same goal as the teacher without copying their motor actions (Whiten et al. 2004).Combining imitation, emulation, and some other techniques such as object movement reenactment, the agent ultimately develops an understanding of the world without having to understand the theoretical concept of causality.These cognitive skill levels can also be roughly identified in robotics, where they intrinsically correspond to different abstraction levels (see Figure 7).At the lowest learning level, a robot simply mimics the motion of a teacher without an explicit understanding of the underlying goal.If the teacher and the learner have similar embodiments, the task can simply be abstracted as a joint-level (positions, velocity, or acceleration) trajectory.However, in the case of different embodiments, transferring joint trajectories will result in very different end-effector trajectories.The task's abstraction level can be increased by specifying, for example, end-effector trajectories and leveraging Cartesian trajectory controllers.To deal with changes in the environment, both the learning and abstraction levels need to be raised.For instance, transferring end-effector trajectories fails if obstacles are present in the environment.In this case, the task needs to be imitated instead of mimicked, i.e., the goal must be explicitly identified by the robot.The task can therefore be abstracted using, e.g., movement primitives (Ijspeert et al. 2002), thus allowing the specification of the key components of the imitated trajectory, e.g., the goal position, while leveraging robot skills such as collision avoidance, localization, and object detection to reproduce the task in different environments.In some cases, the physical capabilities of the teacher and the learner are very different, so the learner cannot achieve a demonstrated task by imitating the teacher.Instead, the learner must infer the goal from the demonstrated task and develop a strategy to achieve the same goal (Schaal 1999).In other words, the transfer should be conducted at the higher abstraction level corresponding to achieving the goal specifications without imitating the teacher-specific actions.This corresponds to the emulation learning level.Finally, on an even higher abstraction level, the teacher should ideally give only highlevel verbal instruction to the robot such as "open the drawer", or "clean the room".This requires the robot to have a skill set resembling that of agents with higher cognitive functions.Such capabilities have the potential to facilitate transfer in more challenging settings, such as dual-and triple-mode transfer.</p>
<p>To elaborate on the different levels of "abstraction", consider the task of transferring a grasp performed by a source hand (robot or human) to a target robotic hand.This transfer can be performed on three levels: (1) The joint angle level (Bouzit 1996;Kyriakopoulos et al. 1997) involves directly replicating joint angles with minor adjustments if the hands exhibit similar kinematic structures and degrees of freedom (DoFs); (ii) The contact level (Peer et al. 2008;Maeda et al. 2016) is applicable when both hands have an equal number of fingers but differ in their kinematic properties (e.g., DoFs, finger lengths).In this scenario, the target hand strives to grasp the object by replicating the contact positions of the source hand; and (iii) The outcome level (Mahler et al. 2019) consists of learning new grasps by optimizing the grasp success with different target hands.</p>
<p>It is important to notice that the abstraction level has a direct influence on the capabilities that are required for successful transfer across robots, environments, and tasks.In particular, transfer at a given abstraction level requires abilities ranging from the bottom of the robot capability stack onto the abilities of the current level (see Figure 7-right).For instance, transfer at the level of verbal instructions demands robots not only to have an abstract understanding of the word, but also to be endowed with task and motion planners, a set of robot skills, and lowlevel controllers to successfully execute the target task on the target robot in the target environment.In this context, recent advances in foundational models are a promising research direction to endow robots with emulated high-level cognitive capabilities (Bommasani et al. 2021;Ahn et al. 2022;Driess et al. 2023).Such foundation models generate semantic plans required to execute a target task based on language and on continuous information collected by the robot (e.g., images, state vectors).Driess et al. (Driess et al. 2023) proposed to address the correspondence problem between tasks at the highest level, i.e., from a semantic perspective, by combining a large language model with perceptual inputs A higher level of abstraction eases the transfer to different agents, environments, and tasks, but requires more and more complex robot's capabilities.Namely, transfer at a given abstraction level requires the robot to be endowed with abilities ranging from the bottom to the corresponding level of the capability stack.</p>
<p>in an embodied multimodal model.Transfer between robots, tasks, and environments is then achieved via a large amount of training data and by training the models on several robots, tasks, and environments simultaneously.In other words, the transfer comes -to some extent -"for free" thanks to the large scale of foundation models.Importantly, such transfer happens only at the highest level, i.e., at the level of semantic planning, while low-level policies and planners are assumed to be given.In other words, transfer is not tackled at lower levels.As a consequence, the difficulty of transfer, as well as the resulting performance, is highly dependent on the capability stack that is made available a priori for each robot.Moreover, training a (still limited) low-level capability stack from scratch, as done, e.g., in (Ahn et al. 2022), requires months of data collection and is not scalable in the long run.</p>
<p>In this sense, we contend that investigating transfer learning methods across the entire robot capability stack is of utmost importance.In particular, we believe that bridging the gap between high-level semantic task transfer (Driess et al. 2023) and low-level execution of various tasks with different robots in the real world is a crucial challenge for transfer learning in robotics.These require grounding the aforementioned transferable high-level representations into the real world via robot sensorimotor experience.Such grounded understanding of the world may enable imitation and emulation learning to be intrinsically linked to the robot's physical capabilities, thus facilitating the inference of what can be transferred, at which level, and in which situation.Previous works aiming at grounding language in robot sensorimotor behaviors (see, e.g., (Krueger et al. 2011;Cangelosi 2010) may serve as a starting point to tackle this problem.An important challenge is to design grounded representations that allow the expansion of the robot capability stacks at all levels based on similarities between tasks, environments, and robots, thus avoiding cumbersome training of medium-and low-level abilities in novel settings.In addition, designing shared grounded representations as proposed in (Krueger et al. 2011;Montesano et al. 2008) is crucial for transfer across different abstraction levels.</p>
<p>Robotics Transformers</p>
<p>As previously mentioned, the use of large pre-trained foundational models (Bommasani et al. 2021) to learn to transfer is enticing.Several large transformer-based models have been adapted for use in robotics, resulting into socalled Robotic Transformers.These models take images and natural language instructions as input and aim to output direct robot actions in the form of Cartesian trajectories.Robotics Transformers were popularized by RT-1 (Brohan et al. 2023b), in which both the input sequence of images and the natural language instructions were tokenized, i.e., broken down into individual units -words or subwordsfor language and patches for images -called tokens.RT-1 essentially consists of a combination of existing architectures.Namely, the natural Language instructions are first embedded using the universal sentence encoder (Cer et al. 2018) and passed into a FiLM layer (Perez et al. 2018), which then constitutes the first layer of EfficientNet-B3 (Tan and Le 2019), thus allowing the fusion of images and language instructions into tokens.To achieve a closed loop action generation at 3Hz, the number of tokens is reduced with the TokenLearner (Ryoo et al. 2021).The obtained sequence of tokens, corresponding to the sequence of images, is then finally fed into the transformer architecture (Vaswani et al. 2017), which outputs the action consisting 11 discrete variables of 256 bins (7 variables for the arm and gripper movement, 3 variables for moving the base, and 1 variable that switches between controlling the arm, the base, or terminating the episode).The model is trained with a large dataset of approximately 130000 episodes performing over 700 tasks collected in the real world.Despite the incorporation of semantic reasoning, as well as the considerable amount of training data and model parameters, RT-1 generalization is limited to the combination of seen concepts.Moreover, it is limited to simple robotic tasks, cannot, e.g., generate compliant motions or solve complex and dexterous manipulation tasks, and cannot outperform the task demonstrator.</p>
<p>The subsequent RT-2 (Brohan et al. 2023a) is a vision-language-action model based on vision-language models (Chen et al. 2023b;Driess et al. 2023) trained on web-scale data and tuned with robotic actions.The largest RT-2 consists of 55 billions parameters.The increased performance of RT-2 compared to RT-1 and other adjusted baseline models (such as VC-1 (Majumdar et al. 2023), R3M (Nair et al. 2022), MOO (Stone et al. 2023)) is attributed to the vision-language backbone combining cofinetuning the pre-trained model jointly on robotics and web data, so that the model considers more abstract visual concepts as well as robot actions.Interestingly, the largest RT-2 model displays encouraging emergent capabilities, Prepared using sagej.clswhere the model is able to use the high-level concepts acquired from the web-scale data such as relative relations between objects to complete tasks that were not present in that form in the robotic dataset.However, these emerging capabilities only emerged in the largest models, which necessitate a complex cloud infrastructure to be deployed.Therefore, they are currently unsuitable for deployment on robotics platforms and self-sufficient autonomous systems.Moreover, the model is not able to produce motions that are not covered by the large robotics dataset.Furthermore, the size of the model (55 billions parameters) can slow the model inference down to 1Hz.</p>
<p>Overall, robotics transformers incorporated high-level semantic reasoning capabilities directly into the robotic actions.This is equivalent to fusing the capability stack of Figure 7 into a single monolith model.This approach comes at the price of reduced low-level performance and limited capabilities compared to traditional methods that output continuous actions, or direct force control for compliant tasks, among others.In addition, fusing the capability stack also results in big and cumbersome models, which are difficult to deploy.In this sense, decomposing the capability stack may result in scalable models resulting in higher performances in robotics tasks.</p>
<p>Importantly, robotics transformers still offer potential beyond their usage as an end-to-end controller.Such models can potentially be used in a similar fashion as large pretrained models have been used in machine learning to obtain compact or universal representations.In this context, it is worth highlighting RT-X (Padalkar et al. 2023) the latest iteration of the robotic transformers, which aggregated 60 robotic datasets with 22 different manipulator embodiments and made this data suitable for the robotic transformer architecture.Such open-source tools and datasets are crucial to bootstrap research in transfer learning for robotics.</p>
<p>Universal Representations</p>
<p>Pretrained representations are widely popular both in machine learning and in robotics (Pari et al. 2022).For instance, ImageNet (Deng et al. 2009) was often used to acquire low-dimensional image representation for picking via suction and parallel gripper (Yen-Chen et al. 2020), contact-rich high-dimensional dexterous manipulation tasks (Shah and Kumar 2021), and household tasks such as scooping involving tools (Liu et al. 2018).The main advantage of such representations is their flexibility in being leveraged for many different downstream tasks with little adaptation.Although these representations are promising, they still require fine-tuning to be transferred to different settings.In this sense, robotics would benefit from truly universal representations that would be intrinsically transferable between robots, environments, and complex tasks without additional training.</p>
<p>In this context, adapting methods such as universal domain adaptation (You et al. 2019) to robotics stands as a particularly promising research direction.Universal domain adaption removes many assumptions regarding the relationship between source and target label dataset.After extracting features from both domains, the proposed universal adaptation network (UAN) employs (1) an adversarial discriminator to match the source and target feature distributions falling under common labels, (2) a nonadversarial discriminator to obtain the domain similarity, i.e., quantify the similarity of an input with the source domain, and (3) a label classifier predicting the probability of the input over to the source classes.Given the domain similarity and the label predicted by the classifier, UAN predicts either a known source label or an unknown class label, thus enabling its use in settings where source and target labels are different.Extending the universal domain adaptation framework beyond classification tasks would be a first promising step towards universal representations for robotics.Such representations may then be directly leveraged for planning and control.</p>
<p>Alternatively, universal representations may be constructed by tasking models with so-called pretext tasks, i.e., tasks designed solely to acquire representations that are then used in a plethora of downstream tasks.In unsupervised visual representation learning, the pretext task of instance discrimination (Wu et al. 2018) inspired many representation models based on contrastive learning (Chen et al. 2020;He et al. 2020;Caron et al. 2020).Importantly, the pretext task does not require any labels.In other words, the unsupervised setting removes any assumptions on source and target labels, similarly as in universal domain adaptation.Training models unsupervisedly and jointly on source and target data may be a promising direction to obtain universal representations for transfer learning in robotics.</p>
<p>Moreover, the advent of large language models and visual language models brought a new breed of representation models that have been rapidly applied in all areas of robotics, e.g., in planning (Shah et al. 2023;Huang et al. 2022), manipulation (Jiang et al. 2022;Ren et al. 2023;Khandelwal et al. 2022), and navigation (Lin et al. 2022;Parisi et al. 2022;Gadre et al. 2022).Such models are excellent candidates to harvest novel universal representations for robotics.Some large visual language models jointly account for different modalities by encoding them in a shared latent space.For instance, CLIP (Radford et al. 2021) is pretrained on a large dataset of images and associated textual descriptions.The model maps both modalities into a shared latent space using a contrastive loss function.The idea of combining representations from different modalities into a shared latent space was also explored in robotics.For instance, Tatiya et al. (2020Tatiya et al. ( , 2023) learned a common latent space from haptic feature space of multiple robots.Knowledge from source robots was then transferred through the latent space to facilitate object recognition by a target robot.Lee et al. (2020b) considered specific encoders for RGB, depth, force-torque, and proprioception modalities, which were aggregated into a multimodal representation with a multimodal fusion model.This shared representation was shown to improve the sample efficiency of the manipulation policy for a peg insertion task.The case of missing modalities during inference time was considered in (Silva et al. 2020).A perceptual model of the world was trained by assuming that some modalities may not be available at all times.The approach can therefore compensate for missing or corrupted modalities during execution.Such joint representations are crucial to design universal representations for transfer.</p>
<p>To be successfully leveraged in various robotic scenarios, universal representations should be expressive, while remaining simple enough to facilitate downstream applications.This is usually achieved via a dimensionality reduction process by extracting low-dimensional latent representations from data.While this latent space was usually assumed to be Euclidean, i.e., flat, recent works have shown the superiority of curved spaces -manifolds like hypersphere, hyperbolic spaces, symmetric spaces, and product of thereof -to learn representations of data exhibiting hierarchical or cyclic structures (Nickel and Kiela 2017;Gu et al. 2019;López et al. 2021).For instance, the compositionality of visual scenes can be preserved via hyperbolic latent representations, thus improving downstream performance in point cloud analysis (Montanaro et al. 2022) and unsupervised visual representation learning (Ge et al. 2023).This suggests that rethinking inductive bias in the form of the geometry of universal representations may also be relevant for robotics applications and for transfer learning in robotics.For example, data associated with robotics taxonomies are better represented in hyperbolic spaces (Jaquier et al. 2024) and manipulation tasks encoded as graphs in the context of visual action planning (Lippi et al. 2023) may benefit from non-Euclidean representations.</p>
<p>Interpretability</p>
<p>Interpretability and explainability of learning-based approaches are key to safely deploy robots into the real world.In particular, black-box approaches lacking humanlevel interpretability can severely hinder natural and safe interactions with robots.In this context, transferable universal representations should also be interpretable and explainable.To do so, approaches in the field of visual action planning (Lippi et al. 2023;Wang et al. 2019) proposed to decode the underlying representations into a humanreadable format, i.e., images.Alternatively, representations can be readily encoded into a human-readable format that is additionally interpretable by many other methods or software architectures.For instance, the universal scene description (USD) (Studio 2023) was designed to interchange 3D graphics information.This format was recently enhanced by Nvidia to facilitate large, complex digital twinsreflections of the real world that can be coupled to physical robots and synchronized in real time (Nvidia 2023).USD is made from sets of data structures and APIs, which are then used to represent and modify virtual environments on supported frameworks such as Omniverse (Mittal et al. 2023), Maya (Autodesk, INC. 2019), andHoudini (SideFX 2022).Such a framework has significant potential to be used for robotics transferability.For instance, it could be leveraged to build joint representations of the world shared across multiple robots, to share knowledge, and even to infer digital twins from sensory readings.</p>
<p>Benchmarking and Simulation</p>
<p>Benchmarks and relevant metrics are key to evaluate and compare methods, thus having the potential to boost the development of innovative novel approaches.For instance, the rapid improvement of deep-learning models benefited from easily-accessible benchmarks that are widely accepted by the community (Krizhevsky 2009;Deng et al. 2009;Lin et al. 2014;Cordts et al. 2016).The robotics community also benefited from impressive strides towards unified benchmarks with efforts such as the YCB- (Calli et al. 2015) and KIT- (Kasper et al. 2012) object dataset, and with regularly-organized benchmark competitions such as RoboCup (Kitano et al. 1997), ANA Avatar Xprice § , and DARPA challenges ¶ .However, they all face robotics unique challenges.First, as robots are real systems evolving in the real world, the deployment of any method can be highly time-consuming.Second, as previously mentioned, transferring methods to robots with different embodiments is non-trivial, which intrinsically hinders benchmarking across different research groups.Last but least, handcrafted, highly tuned solutions usually outperform more general methods to solve any specific or standardized task as defined in classical benchmarks.This is especially notable for robotic manipulation where accepted benchmarks remain scarce.</p>
<p>The Robothon 2023 task board challenge (So et al. 2022) is an example of recent robotics manipulation benchmark.This board is an assembly of various relevant robotics tasks -including inserting a key into a keyhole and turning it, plugging/unplugging an ethernet connector, and pushing switches, among others -allowing the evaluation of different approaches.As required for a benchmark, the task board is standardized and its specifications are given.However, in such settings, handcrafted, or even prerecorded, motions can lead to surprisingly high scores.Randomly orienting the board before every trial was later included to discourage such solutions.Within machine learning benchmarks, handcrafted solutions are prevented by dividing the available data into training and test sets, which consists of different samples drawn from a single distribution.Analogously, the Robothon 2023 challenge would require a large number of task boards consisting of the same high-level tasks but differing in their geometricspecific realization.A promising avenue to overcome the impracticability of producing numerous physical task boards would be to leverage simulators.</p>
<p>Modern robotic simulators, such as Mujoco (Todorov et al. 2012), Bullet (Coumans andBai 2016-2021), and PhysX || have shown impressive improvements in various areas, including in robotics assembly (Narang et al. 2022).Such simulators have the potential to generate various parametrizations of simulated boards, and thus to create training and test sets similar to machine learning benchmarks.In particular, such sets would be of high relevance for transferability in robotics, as they have the potential to evaluate transferability across (1) robots, (2) environments, i.e., different parametrizations, and (3) tasks performed on the same board.The ultimate challenge is to overcome the sim-to-real gap when deploying the developed methods on a real, previously unknown task board using a new robot during live competition.Such benchmarks would boost research in transferability in robotics, as well as § https://www.xprize.org/prizes/avatar¶ https://www.darpa.mil/program/darpa-robotics-challenge ∥ https://developer.nvidia.com/physx-sdkPrepared using sagej.clsprovide valuable information on the difficulty and challenges of each transfer setting.It is worth highlighting that a wide range of works and methods have been developed in the field of machine learning in recent years and subsequently compiled into transfer-learning-libraries (Jiang et al. 2020).Such a consortium of methods offers a huge potential to be used in robotics contexts.Importantly, relevant metrics must be defined to compare different approaches in different transfer settings.</p>
<p>Metrics for Transfer Learning in Robotics</p>
<p>Two types of metrics are relevant for transfer learning in robotics, namely (i) metrics measuring the quality of the transfer, and (ii) metrics measuring the transfer gap.Metrics measuring the transfer quality aim at quantifying algorithmic performance and allows the comparison of the performances of different algorithms on the target space after transfer.In particular, they can also determine when transfer learning is useful.Metrics measuring the transfer gap measure the the discrepancy between the source and target spaces.Essentially, they provide a notion of how different the source and target robots, environments, and tasks are.</p>
<p>Transfer quality metrics are crucial to evaluate and compare the performance of transfer learning algorithms.As such, they are key to the development of novel transfer learning methods.Transfer quality in machine learning is commonly measured by comparing the performance achieved on the target task with or without transfer learning.In this case, the quality metric includes not only the final performance (asymptotic performance), but also the initial benefit of the transfer (jumpstart performance), the time to reach a predefined performance threshold (time to threshold), as well as the sensitivity to different hyperparameter settings (Taylor and Stone 2009;Taylor et al. 2007).In addition to measuring the transfer quality, further analysis of the transfer process can be conducted by comparing the number of required sub-source tasks, the number of demonstrations (Barreto et al. 2018;Zhu et al. 2020b), or the required quality (e.g., suboptimal, expert, oracle) of the source space (Zhu et al. 2020a).Recently, Chen et al. (2023a) highlighted the importance of robust unsupervised evaluation metrics for domain adaptation.Such metrics should be independent of the training method, consistent across hyperparameters and models, and robust to adversarial attacks.Most of the aforementioned transfer quality metrics can and are in fact already used for transfer learning in robotics (Zhu et al. 2023).</p>
<p>Transfer gap metrics provide a measure of the discrepency between the source and target spaces.Note that such metrics may also be used to measure performance in certain circumstances.Robotics adds an additional challenge to the problem of defining suitable transfer gap metrics for transfer learning: Indeed, transfer learning in robotics can be seen as a three-part transfer problem consisting of transfer across robots, tasks, and environments.Several metrics have been defined and directly optimized to solve each of these sub-problems.In this context, domain adaptation received considerable attention from the machine learning community in recent years.When the distribution of the source and target domains can be reliably estimated, simple divergences, e.g., the Kullback-Leibler (KL) divergence (Kullback and Leibler 1951) the Maximum Mean Discrepancy (MMD) (Gretton et al. 2012) for labeled data, or the H∆H divergence (Ben-David et al. 2010) MDD (Zhang et al. 2019), and SND (Saito et al. 2021) for unlabeled data, provide a quantitative estimate of the domain transfer gap.In robotics, a large body of work focuses on the sim-to-real gap -or in other words, the reality gap -as a specific domain gap G D .The simto-real gap is often measured as the capacity of a realistic simulator to emulate the real world.Collins et al. (Collins et al. 2019b) quantified the reality gap by comparing simulated robot trajectories, e.g. using Pybullet (Coumans and Bai 2016) or Mujoco (Todorov et al. 2012), with realworld trajectories captured by a motion capturing system.Importantly, the simulators accurately model kinematics, but generally struggle with dynamics of robots interacting with objects.Zhang et al. (Zhang et al. 2020) specifically focused on the sim-to-real gap in robotics and predicted the transfer performance of reinforcement learning policies using a probabilistic dynamics model.Limited attention was devoted to designing transfer gap metrics for transfer learning across tasks or robots.In particular, the existing literature related to skill transfer learning in human-robot cooperation (Liu et al. 2020) does not agree on a specific skill transfer metric to measure the task transfer gap G T .Although various metrics have also been proposed in the context of motion retargeting, see, e.g., (Gielniak et al. 2013;Penco et al. 2018), quantifying the quality of retargeted motions, as well as robot transfer gap G R , generally remain open questions.We contend that a suitable transfer gap metric G for robotics should consider all three settings of transfer.For instance, this metric may be defined as a simple combination of individual metrics for robot, environment, and task transfer, e.g.,
G = λ 1 G R + λ 2 G D + λ 3 G T ,
where λ 1 , λ 2 , λ 3 are weights adjusting the individual metric influence.Such transfer metric has the potential to bootstrap the development of different transfer learning methods for robotics, as it gives insights about the discrepancy of each mode (i.e., robot, environment, and tasks) from source to target space.</p>
<p>Negative Transfer</p>
<p>Importantly, transfer learning is not necessarily beneficial in all settings.Transfer learning algorithms build on systematic similarities between source and target spaces.However, if non-existing similarities are selected by the algorithm, the transfer can have a negative impact on the performance in the target space (Wang 2021).This phenomena is denoted negative transfer (Rosenstein et al. 2005).Negative transfer has notably been studied within the field of meta-learning (Thrun and Pratt 1998), in which a rapid adaptation to the novel task is assumed to be key for the success of the corresponding models.Preliminary work (Deleu and Bengio 2018) showed that adaptation using meta-learning algorithms, such model-agnostic metalearning (MAML) (Finn et al. 2017), can significantly reduce the performance on meta-training tasks.</p>
<p>In robotics, negative transfer may occur at the different levels of the robot capability stack.For instance, at the low control level, transferring an inverse dynamic model learned for a source quadrotor to a target quadrotor with significantly different physical properties has been shown to lead to worse performances than using a baseline controller that disregards the inverse dynamics (Sorocky et al. 2020(Sorocky et al. , 2021)).Interestingly, the lower levels of the capability stack may be more susceptible to negative transfer as low-level information, e.g., inverse dynamic models, may only be transferred across closely-related source and target spaces.In contrast, experience at the higher levels is more general and may be transferred across a larger range of source and target spaces.</p>
<p>We believe that negative transfer remains an underinvestigated direction in robotics.In particular, negative transfer may be particularly harmful for robotics.First, negative transfer may lead to potentially-damaging behaviors of the target robot, while safety is a crucial aspect when deploying robots in the real world.Second, negative transfer may lead to transfer learning requiring longer training time than directly learning the desired behavior in the target space, while low training time is crucial for real robots acting in the real world.Therefore, the effects and causes of negative learning remain to be thoroughly studied, as they may be key to develop successful and reliable transfer learning algorithms tailored to robotics.</p>
<p>Conclusion: The Future of Transfer Learning in Robotics</p>
<p>The rise of transfer learning implies its potential to enable robots to leverage available knowledge to learn and master novel situations efficiently.In this paper, we aimed at unifying the concept of transfer learning in robotics via a novel taxonomy acting as a bedrock for future developments in the field.Building on the successes of transfer learning in robotics, we outlined relevant challenges that have to be solved to realize its full potential.It is important to highlight that these challenges intrinsically relate to determining what can and cannot be transferred.As illustrated in this paper, transfer learning in machine learning largely relies on identifying whether the distribution of data across two domains remain similar.Identifying similarities and differences across robotic tasks amounts to more than comparing distributions.We have emphasized the need to delineate similarities across tasks, environments, and robots in an effort to ease identification of commonalities and differences in each case.Automatically identify similarities across two situations in robotics relies importantly on spelling out how much prior knowledge on the physics of the robot and environment is provided.With advances in the use of foundational models, the amount of prior knowledge readily available may ease this transfer.</p>
<p>We hope that this position paper paves the way towards successful transfer learning between robots, tasks, and environments, as well as their compositions.Reusing knowledge holds the promise of closing the performance gap between humans and robots in overcoming novel challenges and acquiring new skills and concepts.</p>
<p>Figure 1 .
1
Figure 1.Concept of transfer learning in robotics.The experience of a robot performing a specific task in a specific environment is leveraged to improve the learning of a related task by another robot in a related context.Transfer can occur across embodiments (yellow arrows), across tasks (purple arrows), and/or across environments (blue arrows).It is important to note that successful transfer requires commonalities between the source and target robots, tasks, and environments.For instance, a humanoid robot learning to kick a ball will most likely not benefit from the experience of a dual-arm manipulator systems manipulating a box and vice-versa.</p>
<p>Figure 2 .
2
Figure 2. Percentage of papers including words falling under transfer learning in robotics umbrella over the years for the two biggest robotics conferences.The results are based on a systematic search through the content of 44067 papers, excluding their references.Top: Percentage per conference.Bottom: Percentage per keywords.Keywords related to transfer across embodiments, tasks, and environments are depicted in variations of yellow, purple, and blue.Knowledge transfer is classified independently and thus is depicted in black.</p>
<p>Figure 3 .
3
Figure 3. Example of transfer learning on the Office 31 dataset (Saenko et al. 2010).Transfer can occur (1) between the two label sets (tasks T1 and T2), (2) between the two sources used to obtain the images (domains D1 and D2), and (3) between both tasks and domains.The different transfer learning instances are illustrated with black arrows.</p>
<p>Figure 4 .
4
Figure 4. Hierarchical taxonomy of transfer learning in the context of machine learning.Domain transfer occurs when the source and target domain differ and is characterized by a difference in the feature space and/or in the marginal distribution.Task transfer occurs when the source and target task differ and is characterized by a difference in the label space and/or in the predictive function.Dual-mode transfer occurs when both the source and target domain and task differ.</p>
<p>Figure 5 .
5
Figure 5. Hierarchical taxonomy of transfer learning in the context of robotics.Robot, environment, and task transfer occur when the source and target robot, environment, and task differ, respectively.Dual-mode and triple-mode transfer occur when two of these modalities and the three of them differ, respectively.</p>
<p>Figure 6 .
6
Figure 6.Illustration of the categories of the hierarchical taxonomy for transfer learning in robotics.The humanoid dual-arm robot ARMAR-III (R1) and the four-legged robot ALMA equipped with a manipulator (R2) execute a cloth flinging task (T1) and a box tossing task (T2) in three different environments, namely a simulator (D1), and in the real world with different object instances (D2 and D3).Transfer learning can occur (1) between the two robots, (2) between two environments, (3) between the two tasks, and (4-5) between two or three instances thereof.The different transfer learning instances are illustrated with black arrows.Triple-mode transfer learning, which reuses knowledge from a source space to a target space with different robots, environments, and tasks, is depicted by a red arrow.</p>
<p>Figure 7 .
7
Figure7.Cognitive skill levels in biology, corresponding abstraction levels in robotics, and associated robotic capability stack.A higher level of abstraction eases the transfer to different agents, environments, and tasks, but requires more and more complex robot's capabilities.Namely, transfer at a given abstraction level requires the robot to be endowed with abilities ranging from the bottom to the corresponding level of the capability stack.</p>
<p>Prepared using sagej.cls
† Note that the source R S may be a human instead of a robot. This typically occurs, e.g., in imitation learning.
AcknowledgementsThe research leading to these results has received funding from the European Union's Horizon Europe Framework Programme under grant agreement No 101070596 (euROBIN).
Skeleton-aware networks for deep motion retargeting. K Aberman, P U Li, D Lischinski, O Sorkine-Hornung, D Cohen-Or, Chen B , 10.1145/3386569.3392462ACM Trans. on Graphics. 3942020</p>
<p>Generalization in transfer learning: robust control of robot locomotion. S E Ada, E Ugur, H L Akin, 10.1017/S0263574722000625Robotica. 40112022</p>
<p>Do as I can, not as I say: Grounding language in robotic affordances. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, A Herzog, D Ho, J Hsu, J Ibarz, B Ichter, A Irpan, E Jang, R J Ruano, K Jeffrey, S Jesmonth, N Joshi, R Julian, D Kalashnikov, Y Kuang, K H Lee, S Levine, Y Lu, L Luu, C Parada, P Pastor, J Quiambao, K Rao, J Rettinghouse, D Reyes, P Sermanet, N Sievers, C Tan, A Toshev, V Vanhoucke, F Xia, T Xiao, P Xu, S Xu, Yan M Zeng, A , 2204.016912022arXiv preprint</p>
<p>ARMAR-III: An integrated humanoid platform for sensory-motor control. T Asfour, K Regenstein, P Azad, J Schröder, N Vahrenkamp, R Dillmann, 10.1109/ICHR.2006.321380IEEE/RAS Intl. Conf. on Humanoid Robots (Humanoids). 2006</p>
<p>. Inc Autodesk, 2019</p>
<p>When and where do we apply what we learn? A taxonomy for far transfer. S M Barnett, S J Ceci, 10.1037/0033-2909.128.4.612Psychological Bulletin. 12842002</p>
<p>Transfer in deep reinforcement learning using successor features and generalised policy improvement. A Barreto, D Borsa, J Quan, T Schaul, D Silver, M Hessel, D Mankowitz, A Zidek, R Munos, International Conference on Machine Learning. PMLR2018</p>
<p>Meta learning via learned loss. S Bechtle, A Molchanov, Y Chebotar, E Grefenstette, L Righetti, G Sukhatme, F Meier, 10.1109/ICPR48806.2021.9412010Intl Conf. on Pattern Recognition (ICPR). 2021</p>
<p>S Bechtle, L Righetti, F Meier, 2204.02210Meta learning via learned loss. 2022arXiv preprint</p>
<p>ALMA -articulated locomotion and manipulation for a torque-controllable robot. C D Bellicoso, K Krämer, M Stäuble, D Sako, F Jenelten, M Bjelonic, M Hutter, 10.1109/ICRA.2019.8794273IEEE Intl. Conf. on Robotics and Automation (ICRA). 2019</p>
<p>A theory of learning from different domains. S Ben-David, J Blitzer, K Crammer, A Kulesza, F Pereira, J W Vaughan, 10.1007/s10994-009-5152-4Machine learning. 792010</p>
<p>A dataefficient framework for training and sim-to-real transfer of navigation policies. H Bharadhwaj, Z Wang, Y Bengio, L Paull, 2019 International Conference on Robotics and Automation (ICRA. IEEE2019</p>
<p>Robot Programming by Demonstration. A Billard, S Calinon, R Dillmann, S Schaal, 10.1007/978-3-540-30301-5602008SpringerBerlin, Heidelberg; Berlin Heidelberg</p>
<p>Domain adaptation with structural correspondence learning. J Blitzer, R Mcdonald, F Pereira, Proc. of the conference on empirical methods in natural language processing. of the conference on empirical methods in natural language processing2006</p>
<p>R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, E Brynjolfsson, S Buch, D Card, R Castellon, N S Chatterji, A S Chen, K A Creel, J Davis, D Demszky, C Donahue, M Doumbouya, E Durmus, S Ermon, J Etchemendy, K Ethayarajh, L Fei-Fei, C Finn, T Gale, L E Gillespie, K Goel, N D Goodman, S Grossman, N Guha, T Hashimoto, P Henderson, J Hewitt, D E Ho, J Hong, K Hsu, J Huang, T F Icard, S Jain, D Jurafsky, P Kalluri, S Karamcheti, G Keeling, F Khani, O Khattab, P W Koh, M S Krass, R Krishna, R Kuditipudi, A Kumar, F Ladhak, M Lee, T Lee, J Leskovec, I Levent, X L Li, X Li, T Ma, A Malik, C D Manning, S P Mirchandani, E Mitchell, Z Munyikwa, S Nair, A Narayan, D Narayanan, B Newman, A Nie, J C Niebles, H Nilforoshan, J F Nyarko, G Ogut, L Orr, I Papadimitriou, J S Park, C Piech, E Portelance, C Potts, A Raghunathan, R Reich, H Ren, F Rong, Y H Roohani, C Ruiz, J Ryan, R 'e, C Sadigh, D Sagawa, S Santhanam, K Shih, A Srinivasan, K P Tamkin, A Taori, R Thomas, A W Tramèr, F Wang, R E Wang, W Wu, B Wu, J Wu, Y Xie, S M Yasunaga, M You, J Zaharia, M A Zhang, M Zhang, T Zhang, X Zhang, Y Zheng, L Zhou, K Liang, P , 2108.07258On the opportunities and risks of foundation models. 2021arXiv preprint</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. E V Bonilla, K Chai, C ; Williams, K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, S Levine, V Vanhoucke, 10.1109/ICRA.2018.8460875Neural Information Processing Systems (NeurIPS), volume. 2007. 201820IEEE Intl. Conf. on Robotics and Automation (ICRA)</p>
<p>Quadratic programming for multirobot and task-space force control. K Bouyarmane, K Chappellet, J Vaillant, A Kheddar, IEEE Transactions on Robotics. 3512018</p>
<p>Design, implementation and testing of a data glove with force feedback for virtual and real objects telemanipulation. M Bouzit, 1996Laboratoire de Robotique de Paris, University of Pierre Et Marie CuriePhD Thesis</p>
<p>RT-2: Vision-languageaction models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, P Florence, C Fu, M G Arenas, K Gopalakrishnan, K Han, K Hausman, A Herzog, J Hsu, B Ichter, A Irpan, N Joshi, R Julian, D Kalashnikov, Y Kuang, I Leal, L Lee, Twe Lee, S Levine, Y Lu, H Michalewski, I Mordatch, K Pertsch, K Rao, K Reymann, M Ryoo, G Salazar, P Sanketi, P Sermanet, J Singh, A Singh, R Soricut, H Tran, V Vanhoucke, Q Vuong, A Wahid, S Welker, P Wohlhart, J Wu, F Xia, T Xiao, P Xu, S Xu, Yu T Zitkovich, B , 2307.158182023aarXiv preprint</p>
<p>RT-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, J Ibarz, B Ichter, A Irpan, T Jackson, S Jesmonth, N Joshi, R Julian, D Kalashnikov, Y Kuang, I Leal, K H Lee, S Levine, Y Lu, U Malla, D Manjunath, I Mordatch, O Nachum, C Parada, J Peralta, E Perez, K Pertsch, J Quiambao, K Rao, M Ryoo, G Salazar, P Sanketi, K Sayed, J Singh, S Sontakke, A Stone, C Tan, H Tran, V Vanhoucke, S Vega, Q Vuong, F Xia, T Xiao, P Xu, S Xu, Yu T Zitkovich, B , Robotics: Science and Systems (R:SS). 2023b</p>
<p>Robot Learning with Task-Parameterized Generative Models. S Calinon, 10.1007/978-3-319-60916-472018Springer International PublishingCham</p>
<p>The ycb object and model set: Towards common benchmarks for manipulation research. B Calli, A Singh, A Walsman, S Srinivasa, P Abbeel, A M Dollar, 10.1109/ICAR.2015.72515042015 international conference on advanced robotics (ICAR). IEEE2015</p>
<p>Grounding language in action and perception: From cognitive agents to humanoid robots. A Cangelosi, 10.1016/j.plrev.2010.02.001Physics of Life Reviews. 722010</p>
<p>Unsupervised learning of visual features by contrasting cluster assignments. M Caron, I Misra, J Mairal, P Goyal, P Bojanowski, A ; Joulin, D Cer, Y Yang, Kong Sy, N Hua, N Limtiaco, R S John, N Constant, M Guajardo-Cespedes, S Yuan, C Tar, Y H Sung, B Strope, R Kurzweil, 1803.11175Neural Information Processing Systems (NeurIPS). 2020. 201833Universal sentence encoderarXiv preprint</p>
<p>A study of unsupervised evaluation metrics for practical and automatic domain adaptation. M Chen, Z Gao, S Zhao, Q Qiu, Wang W , Lin B He, X , 2023a</p>
<p>A simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, Proceedings of Machine Learning Research. Machine Learning Research2020119Intl. Conf. on Machine Learning (ICML)</p>
<p>Hardware conditioned policies for multi-robot transfer learning. T Chen, A Murali, A Gupta, Neural Information Processing Systems (NeurIPS), volume. 201831</p>
<p>X Chen, J Djolonga, P Padlewski, B Mustafa, S Changpinyo, J Wu, C R Ruiz, S Goodman, X Wang, Y Tay, S Shakeri, M Dehghani, D Salz, M Lucic, M Tschannen, A Nagrani, H Hu, M Joshi, B Pang, C Montgomery, P Pietrzyk, M Ritter, A Piergiovanni, M Minderer, F Pavetic, A Waters, G Li, I Alabdulmohsin, L Beyer, J Amelot, K Lee, A P Steiner, Y Li, D Keysers, A Arnab, Y Xu, K Rong, A Kolesnikov, M Seyedhosseini, A Angelova, X Zhai, N Houlsby, R Soricut, 2305.18565On scaling up a multilingual vision and language model. PaLI-X2023barXiv preprint</p>
<p>Quantifying the reality gap in robotic manipulation tasks. J Collins, Howard D Leitner, J , 10.1109/ICRA.2019.8793591IEEE Intl. Conf. on Robotics and Automation (ICRA). 2019a</p>
<p>Quantifying the reality gap in robotic manipulation tasks. J Collins, Howard D Leitner, J , 10.1109/ICRA.2019.8793591IEEE Intl. Conf. on Robotics and Automation (ICRA). 2019b</p>
<p>The cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, Roth S Schiele, B , 10.1109/CVPR.2016.350IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2016</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, 2016</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, 2016-2021</p>
<p>Online and markerless motion retargeting with kinematic constraints. B Dariush, M Gienger, A Arumbakkam, C Goerick, Y Zhu, K Fujimura, 10.1109/IROS.2008.4651104IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS). 2008</p>
<p>Imitation in Animals and Artifacts. Dautenhahn K and Nehaniv CL2002MIT PressCambridge, MA, USA</p>
<p>The effects of negative adaptation in model-agnostic meta-learning. T Deleu, Y Bengio, 1812.021592018arXiv preprint</p>
<p>Imagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L J Li, Li K , Fei-Fei L , 10.1109/CVPR.2009.5206848IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2009</p>
<p>Learning compliant movement primitives through demonstration and statistical generalization. M Deniša, A Gams, A Ude, T Petrič, 10.1109/TMECH.2015.2510165IEEE/ASME Trans. on Mechatronics. 2152016</p>
<p>D Driess, F Xia, Msm Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, W Huang, Y Chebotar, P Sermanet, D Duckworth, S Levine, V Vanhoucke, K Hausman, M Toussaint, K Greff, A Zeng, I Mordatch, P Florence, 2303.03378PaLM-E: An embodied multimodal language model. 2023arXiv preprint</p>
<p>A general approach for the automation of hydraulic excavator arms using reinforcement learning. P Egli, M Hutter, 10.1109/LRA.2022.3152865IEEE Robotics and Automation Letters. 722022</p>
<p>Regularized multi-task learning. T Evgeniou, M Pontil, 10.1145/1014052.1014067Proc. of the ACM SIGKDD Intl Conf. on Knowledge Discovery and Data Mining. of the ACM SIGKDD Intl Conf. on Knowledge Discovery and Data Mining2004</p>
<p>Probabilistic policy reuse for inter-task transfer learning. F Fernández, J García, M Veloso, 10.1016/j.robot.2010.03.007Robotics and Autonomous Systems. 5872010</p>
<p>Model-agnostic metalearning for fast adaptation of deep networks. C Finn, P Abbeel, S Levine, Proceedings of Machine Learning Research. Machine Learning Research201770Intl. Conf. on Machine Learning (ICML)</p>
<p>Clip on wheels: Zero-shot object navigation as object localization and exploration. S Y Gadre, M Wortsman, G Ilharco, L Schmidt, S Song, 2203.104212022arXiv preprint</p>
<p>Hyperbolic contrastive learning for visual representations beyond objects. S Ge, S Mishra, S Kornblith, C L Li, D Jacobs, 10.1109/CVPR52729.2023.00661IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2023</p>
<p>Recent advances in open set recognition: A survey. C Geng, S J Huang, Chen S , 10.1109/TPAMI.2020.2981604IEEE Transactions on Pattern Analysis and Machine Intelligence. 43102021</p>
<p>Mimicry and automatic imitation are not correlated. O Genschow, S Van Den Bossche, E Cracco, L Bardi, D Rigoni, M Brass, PloS one. 129e01837842017</p>
<p>Bayesian meta-learning for few-shot policy adaptation across robotic platforms. A Ghadirzadeh, X Chen, P Poklukar, C Finn, M Björkman, D Kragic, 10.1109/IROS51168.2021.9636628IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS). 2021</p>
<p>Generating humanlike motion for robots. M J Gielniak, C K Liu, A L Thomaz, 10.1177/0278364913490533Intl. Journal of Robotics Research. 32112013</p>
<p>A kernel two-sample test. A Gretton, K M Borgwardt, M J Rasch, B Schölkopf, A Smola, Journal of Machine Learning Research. 1312012</p>
<p>Learning mixedcurvature representations in products of model spaces. A Gu, F Sala, B Gunel, C Ré, Intl. Conf. on Learning Representations (ICLR). 2019</p>
<p>Momentum contrast for unsupervised visual representation learning. K He, H Fan, Y Wu, S Xie, R Girshick, 10.1109/CVPR42600.2020.00975IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2020</p>
<p>Causes and consequences of imitation. C Heyes, 10.1016/s1364-6613(00)01661-2Trends in cognitive sciences. 562001</p>
<p>Sim2real in robotics and automation: Applications and challenges. S Höfer, K Bekris, A Handa, J C Gamboa, M Mozifian, F Golemo, C Atkeson, D Fox, K Goldberg, J Leonard, Karen Liu, C Peters, J Song, S Welinder, P White, M , 10.1109/TASE.2021.3064065Prepared using sagej.cls 398-400. 202118</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. J Huang, A Gretton, K Borgwardt, B Schölkopf, A ; Smola, W Huang, P Abbeel, D Pathak, I Mordatch, Neural Information Processing Systems (NeurIPS), volume. 2006. 202219Proceedings of Machine Learning Research</p>
<p>Movement imitation with nonlinear dynamical systems in humanoid robots. A Ijspeert, J Nakanishi, S Schaal, 10.1109/ROBOT.2002.1014739IEEE Intl. Conf. on Robotics and Automation (ICRA. 22002</p>
<p>Dynamical Movement Primitives: Learning Attractor Models for Motor Behaviors. A J Ijspeert, J Nakanishi, H Hoffmann, P Pastor, S Schaal, 10.1162/NECOa00393Neural Computation. 2522013</p>
<p>Analysis and transfer of human movement manipulability in industry-like activities. N Jaquier, L Rozo, S Calinon, 10.1109/IROS45743.2020.9341353IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS). 2020</p>
<p>Bringing robotics taxonomies to continuous domains via GPLVM on hyperbolic manifolds. N Jaquier, L Rozo, M González-Duque, V Borovitskiy, T Asfour, Intl. Conf. on Machine Learning (ICML). 2024</p>
<p>Transferlearning-library. J Jiang, B Chen, B Fu, M Long, 2020</p>
<p>Vima: General robot manipulation with multimodal prompts. Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, Fan L , 2210.030942022arXiv preprint</p>
<p>The kit object models database: An object model database for object recognition, localization and manipulation in service robotics. A Kasper, Z Xue, R Dillmann, 10.1177/0278364912445831The International Journal of Robotics Research. 3182012</p>
<p>Selfcorrecting quadratic programming-based robot control. F Khadivar, K Chatzilygeroudis, A Billard, IEEE Transactions on Systems, Man, and Cybernetics: Systems. 2023</p>
<p>Simple but effective: Clip embeddings for embodied AI. A Khandelwal, L Weihs, R Mottaghi, A Kembhavi, 10.1109/CVPR52688.2022.01441IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2022</p>
<p>Learning stable nonlinear dynamical systems with Gaussian mixture models. Khansari-Zadeh Sm Billard, A , 10.1109/TRO.2011.2159412IEEE Trans. on Robotics. 2752011</p>
<p>Robocup: The robot world cup initiative. H Kitano, M Asada, Y Kuniyoshi, I Noda, E Osawa, 10.1145/267658.267738Proceedings of the first international conference on Autonomous agents. the first international conference on Autonomous agents1997</p>
<p>A riemannian take on human motion analysis and retargeting. H Klein, N Jaquier, A Meixner, T Asfour, 10.1109/IROS47612.2022.99821272022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2022</p>
<p>Transfer of contact skills to new environmental conditions. A Kramberger, A Gams, B Nemec, C Schou, D Chrysostomou, O Madsen, A Ude, 10.1109/HUMANOIDS.2016.7803346IEEE-RAS 16th International Conference on Humanoid Robots (Humanoids). 2016. 2016</p>
<p>Embodiment as a necessary a priori of general intelligence. D Kremelberg, 10.1007/978-3-030-27005-613Krizhevsky A. Hammer P, Agrawal P, Goertzel B and Iklé M2019. 2009Springer International PublishingDepartment of Computer Science, University of Toronto URLMaster's thesis</p>
<p>Object-action complexes: Grounded abstractions of sensory-motor processes. N Krueger, C Geib, J Piater, R Petrick, M Steedman, F Woergoe ¶tter, A Ude, T Asfour, D Kraft, D Omrčen, A Agostini, R Dillmann, 10.1016/j.robot.2011.05.009Robotics and Autonomous Systems. 59102011</p>
<p>On information and sufficiency. S Kullback, R A Leibler, The Annals of Mathematical Statistics. 2211951</p>
<p>Adapting rapid motor adaptation for bipedal robots. A Kumar, Z Li, J Zeng, D Pathak, K Sreenath, J Malik, 10.1109/IROS47612.2022.9981091IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS). 2022</p>
<p>Kinematic analysis and position/force control of the anthrobot dextrous hand. K J Kyriakopoulos, J Van Riper, A Zink, H E Stephanou, 10.1109/3477.552188IEEE Trans. on Systems, Man, and Cybernetics. 2711997Part B (Cybernetics)</p>
<p>Learning to learn with the informative vector machine. N D Lawrence, J C Platt, Intl. Conf. on Machine Learning (ICML). 2004</p>
<p>Learning quadrupedal locomotion over challenging terrain. J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, 10.1126/scirobotics.abc5986Science Robotics. 54759862020a</p>
<p>Making sense of vision and touch: Learning multimodal representations for contactrich tasks. M A Lee, Y Zhu, P Zachares, M Tan, K Srinivasan, S Savarese, L Fei-Fei, A Garg, J Bohg, IEEE Transactions on Robotics. 3632020b</p>
<p>Crossdomain video concept detection: A joint discriminative and generative active learning approach. H Li, Y Shi, Y Liu, A G Hauptmann, Z Xiong, 10.1016/j.eswa.2012.04.054Expert Systems with Applications. 39152012</p>
<p>Task generalization with stability guarantees via elastic dynamical system motion policies. T Li, N Figueroa, Conference on Robot Learning (CoRL). 2023</p>
<p>ADAPT: Vision-language navigation with modality-aligned action prompts. B Lin, Y Zhu, Z Chen, X Liang, J Liu, X Liang, 10.1109/CVPR52688.2022.01496IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2022</p>
<p>Microsoft coco: Common objects in context. T Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Dollár, C L Zitnick, 10.1007/978-3-319-10602-1_48European Conference on Computer Vision (ECCV). Springer International Publishing2014</p>
<p>Enabling visual action planning for object manipulation through latent space roadmap. M Lippi, P Poklukar, M C Welle, A Varava, H Yin, A Marino, D Kragic, 10.1109/TRO.2022.3188163IEEE Trans. on Robotics. 3912023</p>
<p>Imitation from observation: Learning to imitate behaviors from raw video via context translation. Y Liu, A Gupta, P Abbeel, S Levine, 10.1109/ICRA.2018.8462901IEEE Intl. Conf. on Robotics and Automation (ICRA). 2018</p>
<p>Skill transfer learning for autonomous robots and human-robot cooperation: A survey. Y Liu, Z Li, H Liu, Z Kan, 10.1016/j.robot.2020.103515Robotics and Autonomous Systems. 1281035152020</p>
<p>EDO-Net: Learning elastic properties of deformable objects from graph dynamics. M Long, Z Cao, Wang J Yu, Ps ; Longhini, A Moletta, M Reichlin, A Welle, M C Held, D Erickson, Z Kragic, D , 2209.08996Neural Information Processing Systems (NeurIPS), volume. 2017. 202230arXiv preprintLearning multiple tasks with multilinear relationship networks</p>
<p>Combining reinforcement learning and lazy learning for faster fewshot transfer learning. Z Lončarević, M Simonič, A Ude, A Gams, 10.1109/Humanoids53995.2022.10000095IEEE/RAS Intl. Conf. on Humanoid Robots (Humanoids). 2022</p>
<p>Symmetric spaces for graph embeddings: A Finsler-Riemannian approach. F López, B Pozzetti, S Trettel, M Strube, A Wienhard, Proceedings of Machine Learning Research. Machine Learning Research2021139Intl. Conf. on Machine Learning (ICML)</p>
<p>Acquiring and generalizing the embodiment mapping from human observations to robot skills. G Maeda, M Ewerton, D Koert, J Peters, 10.1109/LRA.2016.2525038IEEE Robotics and Automation Letters. 122016</p>
<p>Learning ambidextrous robot grasping policies. J Mahler, M Matl, V Satish, M Danielczuk, B Derose, Mckinley S Goldberg, K , 10.1126/scirobotics.aau4984Science Robotics. 42649842019</p>
<p>GTI: Learning to generalize across longhorizon tasks from human demonstrations. A Majumdar, K Yadav, S Arnaud, Y J Ma, C Chen, S Silwal, A Jain, Vincent-Pierre Berges, P A Batra, D Lin, Y Maksymets, O , Rajeswaran A Meier, F ; Mandlekar, A Xu, D Martín-Martín, R Savarese, S , Fei-Fei L , Robotics: Science and Systems (R:SS). 2023. 2020Where are we in the search for an artificial visual cortex for</p>
<p>Eaton E (2023) How to reuse and compose knowledge for a lifetime of tasks: A survey on continual learning and functional composition. J A Mendez, E Eaton, Intl. Conf. on Learning Representations (ICLR). 2021Transactions on Machine Learning Research URL</p>
<p>ORBIT: A unified simulation framework for interactive robot learning environments. M Mittal, C Yu, Q Yu, J Liu, N Rudin, D Hoeller, J L Yuan, P P Tehrani, R Singh, Y Guo, H Mazhar, A Mandlekar, B Babich, G State, M Hutter, A Garg, 2301.041952023arXiv preprint</p>
<p>Rethinking the compositionality of point clouds through regularization in the hyperbolic space. A Montanaro, D Valsesia, E Magli, Neural Information Processing Systems (NeurIPS), volume. 202235</p>
<p>Learning object affordances: From sensory-motor coordination to imitation. L Montesano, M Lopes, A Bernardino, Santos-Victor J , 10.1109/TRO.2007.914848IEEE Transactions on Robotics. 2412008</p>
<p>Robot learning from randomized simulations: A review. F Muratore, F Ramos, G Turk, W Yu, M Gienger, J Peters, 10.3389/frobt.2022.799893Frontiers in Robotics and AI. 92022</p>
<p>Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. S Nair, E Mitchell, K Chen, S Savarese, C Finn, Proceedings of Machine Learning Research. Machine Learning Research2022164Conference on Robot Learning (CoRL)</p>
<p>Y Narang, K Storey, I Akinola, M Macklin, P Reist, L Wawrzyniak, Y Guo, A Moravanszky, G State, M Lu, H Ankur, D Fox, 2205.03532Fast contact for robotic assembly. 2022arXiv preprint</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. S Narvekar, B Peng, M Leonetti, J Sinapov, M E Taylor, P Stone, Journal of Machine Learning Research. 211812020</p>
<p>On first-order metalearning algorithms. A Nichol, J Achiam, J Schulman, arXiv:1803.029992018arXiv preprint</p>
<p>Poincaré Embeddings for Learning Hierarchical Representations. M Nickel, D Kiela, Neural Information Processing Systems (NeurIPS), volume. 2017. 202330Universal scene description</p>
<p>A Padalkar, A Pooley, A Jain, A Bewley, A Herzog, A Irpan, A Khazatsky, A Rai, A Singh, A Brohan, A Raffin, A Wahid, B Burgess-Limerick, B Kim, B Schölkopf, B Ichter, C Lu, C Xu, C Finn, C Xu, C Chi, C Huang, C Chan, C Pan, C Fu, C Devin, D Driess, D Pathak, D Shah, D Büchler, D Kalashnikov, D Sadigh, E Johns, F Ceola, F Xia, F Stulp, G Zhou, G S Sukhatme, G Salhotra, G Yan, G Schiavi, H Su, H S Fang, H Shi, H B Amor, H I Christensen, H Furuta, H Walke, H Fang, I Mordatch, I Radosavovic, I Leal, J Liang, J Kim, J Schneider, J Hsu, J Bohg, J Bingham, J Wu, J Wu, J Luo, J Gu, J Tan, J Oh, J Malik, J Tompson, J Yang, J J Lim, J Silvério, J Han, K Rao, K Pertsch, K Hausman, K Go, K Gopalakrishnan, K Goldberg, K Byrne, K Oslund, K Kawaharazuka, K Zhang, K Majd, K Rana, K Srinivasan, L Y Chen, L Pinto, L Tan, L Ott, L Lee, M Tomizuka, M Du, M Ahn, M Zhang, M Ding, M K Srirama, M Sharma, M J Kim, N Kanazawa, N Hansen, N Heess, N J Joshi, N Suenderhauf, N D Palo, Nmm Shafiullah, O Mees, O Kroemer, P R Sanketi, P Wohlhart, P Xu, P Sermanet, P Sundaresan, Q Vuong, R Rafailov, R Tian, R Doshi, R Martín-Martín, R Mendonca, R Shah, R Hoque, R Julian, S Bustamante, S Kirmani, S Levine, S Moore, S Bahl, S Dass, S Song, S Xu, S Haldar, S Adebola, S Guist, S Nasiriany, S Schaal, S Welker, S Tian, S Dasari, S Belkhale, T Osa, T Harada, T Matsushima, T Xiao, T Yu, T Ding, T Davchev, T Z Zhao, T Armstrong, T Darrell, V Jain, V Vanhoucke, W Zhan, W Zhou, W Burgard, X ; Chen, Z Xu, Z J Cui, 2310.08864Open X-Embodiment: Robotic learning datasets and RT-X models. Wang X, Zhu X, Li X, Lu Y, Chebotar Y, Zhou Y, Zhu Y, Xu Y, Wang Y, Bisk Y, Cho Y, Lee Y, Cui Y, hua Wu Y, Tang Y, Zhu Y, Li Y, Iwasawa Y, Matsuo Y,2023arXiv preprint</p>
<p>Cross-domain sentiment classification via spectral feature alignment. S J Pan, X Ni, J T Sun, Yang Q , Chen Z , 10.1145/1772690.1772767Proceedings of the Intl Conf. on Worldwide Web. the Intl Conf. on Worldwide Web2010</p>
<p>A survey on transfer learning. S J Pan, Q Yang, 10.1109/TKDE.2009.191IEEE Transactions on knowledge and data engineering. 22102010</p>
<p>Probabilistic movement primitives. A Paraschos, C Daniel, J R Peters, G Neumann, 10.5555/2999792.2999904Advances in Neural Information Processing Systems. 262013</p>
<p>The surprising effectiveness of representation learning for visual imitation. J Pari, N M Shafiullah, S P Arunachalam, L Pinto, Robotics: Science and Systems. 2022R:SS</p>
<p>URL. </p>
<p>The (un)surprising effectiveness of pre-trained vision models for control. S Parisi, A Rajeswaran, S Purushwalkam, A Gupta, Intl. Conf. on Machine Learning (ICML). 2022162</p>
<p>Multi-fingered telemanipulation-mapping of a human hand to a three finger gripper. A Peer, S Einenkel, M Buss, 10.1109/ROMAN.2008.4600710IEEE Intl. Symposium on Robot and Human Interactive Communication (RO-MAN). 2008</p>
<p>Robust realtime whole-body motion retargeting from human to humanoid. L Penco, B Clement, V Modugno, Mingo Hoffman, E Nava, G Pucci, D Tsagarakis, N G Mouret, J B Ivaldi, S , 10.1109/HUMANOIDS.2018.8624943IEEE/RAS Intl. Conf. on Humanoid Robots (Humanoids). 2018</p>
<p>Film: Visual reasoning with a general conditioning layer. E Perez, F Strub, De Vries, H Dumoulin, V Courville, A , AAAI Conf. on Artificial Intelligence. 201832</p>
<p>Transfer of learning. D N Perkins, G Salomon, The International Encyclopedia of Education. T Husén, T N Postlethwaite, 1992</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, Ramesh A Goh, G Agarwal, S Sastry, G Askell, A Mishkin, P Clark, J , International conference on machine learning. PMLR2021</p>
<p>A motion retargeting method for effective mimicry-based teleoperation of robot arms. D Rakita, B Mutlu, M Gleicher, ACM/IEEE Intl. Conf. on Human-Robot Interaction (HRI). 2017</p>
<p>Recent advances in robot learning from demonstration. H Ravichandar, A S Polydoros, S Chernova, A Billard, 10.1146/annurev-control-100819-063206Robotics, and Autonomous Systems. 312020Annual Review of Control</p>
<p>Animal and human innovation: novel problems and novel solutions. S M Reader, J Morand-Ferron, Flynn E , 10.1098/rstb.2015.0182Philosophical Transactions of the Royal Society B: Biological Sciences. 371201501822016. 1690</p>
<p>Leveraging language for accelerated learning of tool manipulation. A Z Ren, B Govil, T Y Yang, K R Narasimhan, A Majumdar, Conference on Robot Learning (CoRL). 2023</p>
<p>. PMLR. </p>
<p>To transfer or not to transfer. M T Rosenstein, Z Marx, L P Kaelbling, T G Dietterich, NIPS Workshop on Transfer Learning. 2005898</p>
<p>Learning physical collaborative robot behaviors from human demonstrations. L Rozo, S Calinon, D G Caldwell, P Jiménez, C Torras, 10.1109/TRO.2016.2540623IEEE Trans. on Robotics. 3232016</p>
<p>Tokenlearner: Adaptive space-time tokenization for videos. M Ryoo, A Piergiovanni, A Arnab, M Dehghani, A ; Angelova, K Saenko, B Kulis, Fritz M , Darrell T , 10.1007/978-3-642-15561-116Neural Information Processing Systems (NeurIPS). Springer2021. 201034European Conference on Computer Vision (ECCV)</p>
<p>Tune it the right way: Unsupervised validation of domain adaptation via soft neighborhood density. K Saito, D Kim, P Teterwak, S Sclaroff, Darrell T Saenko, K , Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>Is imitation learning the route to humanoid robots?. 10.1016/S1364-6613(99)01327-3cls Schaal S. 19993Prepared using sagej</p>
<p>Transfer of movement control in motor skill learning. R A Schmidt, Young De, 10.1016/b978-0-12-188950-0.50009-6Transfer of Learning: Contemporary Research and Applications. 1987</p>
<p>LM-nav: Robotic navigation with large pre-trained models of language, vision, and action. D Shah, B Osinski, B Ichter, S Levine, Proceedings of Machine Learning Research. Machine Learning Research2023205Conference on Robot Learning (CoRL)</p>
<p>RRL: Resnet as representation for reinforcement learning. R Shah, V Kumar, Proceedings of Machine Learning Research. Machine Learning Research2021139Intl. Conf. on Machine Learning (ICML)</p>
<p>A framework for few-shot policy transfer through observation mapping and behavior cloning. Y Shukla, B Kesari, S Goel, Wright R Sinapov, J , 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023</p>
<p>Acute: Automatic curriculum transfer from simple to complex environments. Y Shukla, C Thierauf, R Hosseini, G Tatiya, J Sinapov, Proc. of the Intl Conf. on Autonomous Agents and MultiAgent Systems (AAMAS). of the Intl Conf. on Autonomous Agents and MultiAgent Systems (AAMAS)2022</p>
<p>. Sidefx, 2022</p>
<p>Playing games in the dark: An approach for cross-modality transfer in reinforcement learning. R Silva, M Vasco, F S Melo, A Paiva, M Veloso, Proc. of the Intl Conf. on Autonomous Agents and MultiAgent Systems (AAMAS). of the Intl Conf. on Autonomous Agents and MultiAgent Systems (AAMAS)2020</p>
<p>Towards remote robotic competitions: An internet-connected task board and dashboard. P So, J Wittmann, P Ruhkamp, A Sarabakha, S Haddadin, 2201.095652022arXiv preprint</p>
<p>Experience selection using dynamics similarity for efficient multi-source transfer learning between robots. M J Sorocky, S Zhou, A P Schoellig, 10.1109/ICRA40945.2020.9196744IEEE Intl. Conf. on Robotics and Automation (ICRA). 2020</p>
<p>To share or not to share? Performance guarantees and the asymmetric nature of cross-robot experience transfer. M J Sorocky, S Zhou, A P Schoellig, 10.1109/lcsys.2020.3005886IEEE Control Systems Letters. 532021</p>
<p>Open-world object manipulation using pre-trained vision-language models. A Stone, T Xiao, Y Lu, K Gopalakrishnan, K H Lee, Q Vuong, P Wohlhart, B Zitkovich, F Xia, C Finn, K Hausman, Conference on Robot Learning (CoRL). 2023</p>
<p>Universal scene description. P U Studio, 2023</p>
<p>Transferring implicit knowledge of non-visual object properties across heterogeneous robot morphologies. M Tan, Q ; Le, G Tatiya, Francis J Sinapov, J , 10.1109/ICRA48891.2023.10160811Proceedings of Machine Learning Research. Machine Learning ResearchIEEE Intl2019. 202397on Robotics and Automation (ICRA)</p>
<p>Haptic knowledge transfer between heterogeneous robots using kernel manifold alignment. G Tatiya, Y Shukla, M Edegware, J Sinapov, 10.1109/IROS45743.2020.9340770IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS). 2020</p>
<p>Transfer learning for reinforcement learning domains: A survey. M E Taylor, P Stone, Journal of Machine Learning Research. 1072009</p>
<p>Transfer learning via intertask mappings for temporal difference learning. M E Taylor, P Stone, Y Liu, Journal of Machine Learning Research. 892007</p>
<p>Learning to learn: Introduction and overview. S Thrun, L Pratt, 10.1007/978-1-4615-5529-211998Springer</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 10.1109/IROS.2012.6386109IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS). 2012</p>
<p>. M Towers, J K Terry, A Kwiatkowski, J U Balis, Cola Gd, T Deleu, M Goulão, A Kallinteris, A Kg, M Krimmel, R Perez-Vicente, A Pierré, S Schulhoff, J J Tai, Atj Shen, O G Younis, 10.5281/zenodo.81270262023</p>
<p>Task-specific generalization of discrete and periodic dynamic movement primitives. A Ude, A Gams, T Asfour, J Morimoto, 10.1109/TRO.2010.2065430IEEE Transactions on Robotics. 2652010</p>
<p>(2023) Where to start? Transferring simple skills to complex environments. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Neural Information Processing Systems (NeurIPS), volume. 201730Proceedings of Machine Learning Research</p>
<p>A Walqui, Contextual factors in second language acquisition. ERIC Digest URL. 2000</p>
<p>A Wang, T Kurutach, K Liu, P Abbeel, Tamar A , Learning robotic manipulation through visual planning and acting URL. 2019</p>
<p>Whiten A and Ham R (1992) On the nature and evolution of imitation in the animal kingdom: Reappraisal of a century of research. Wang Sj, A M Johnson, 10.1016/S0065-3454(08)60146-1Conference on Learning for Dynamics and Control (L4DC), Proceedings of Machine Learning Research. Academic Press2021144Carnegie Mellon UniversityPhD Thesis(2021) Mitigating negative transfer for better generalization and efficiency in transfer learning</p>
<p>How do apes ape?. A Whiten, V Horner, C A Litchfield, Marshall-Pescini S , 10.3758/BF03196005Animal Learning &amp; Behavior. 322004</p>
<p>Unsupervised feature learning via non-parametric instance discrimination. Z Wu, Y Xiong, Yu Sx, Lin D , 10.1109/CVPR.2018.00393IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2018</p>
<p>Learning to see before learning to act: Visual pre-training for manipulation. Q Yang, Y Zhang, W Dai, Pan Sj ; Yen-Chen, L Zeng, A Song, S Isola, P Lin, T Y , 10.1017/9781139061773DOI:10.1109/ICRA40945.2020on Robotics and Automation (ICRA). IEEE Intl2020. 2020Transfer Learning</p>
<p>Multimodal dance style transfer. Machine Vision and Applications. W Yin, H Yin, K Baraka, D Kragic, M Björkman, 10.1007/s00138-023-01399-x20233448</p>
<p>Universal domain adaptation. K You, M Long, Z Cao, Wang J Jordan, M I , 10.1109/CVPR.2019.00283IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2019</p>
<p>Xirl: Cross-embodiment inverse reinforcement learning. K Zakka, A Zeng, P Florence, J Tompson, J Bohg, D Dwibedi, Proceedings of Machine Learning Research. Machine Learning Research2022164Conference on Robot Learning (CoRL)</p>
<p>Predicting simto-real transfer with probabilistic dynamics models. L M Zhang, M Plappert, W Zaremba, 2020. 2009. 12864arXiv preprint</p>
<p>Bridging theory and algorithm for domain adaptation. Y Zhang, T Liu, Long M , Jordan M , International conference on machine learning. PMLR. 2019</p>
<p>Sim-to-real transfer in deep reinforcement learning for robotics: a survey. W Zhao, J P Queralta, T Westerlund, 10.1109/SSCI47803.2020.9308468IEEE Symposium Series on Computational Intelligence. 2020</p>
<p>Movement primitive learning and generalization: Using mixture density networks. Y Zhou, J Gao, T Asfour, 10.1109/MRA.2020.2980591IEEE Robotics &amp; Automation Magazine. 2722020</p>
<p>Learning sparse rewarded tasks from sub-optimal demonstrations. Z Zhu, K Lin, B Dai, J Zhou, arXiv:2004.005302020aarXiv preprint</p>
<p>Off-policy imitation learning from observations. Z Zhu, K Lin, B Dai, J Zhou, Advances in neural information processing systems. 332020b</p>
<p>Transfer learning in deep reinforcement learning: A survey. Z Zhu, K Lin, A K Jain, J Zhou, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2023</p>
<p>A comprehensive survey on transfer learning. F Zhuang, Z Qi, K Duan, D Xi, Y Zhu, H Zhu, H Xiong, Q He, 10.1109/JPROC.2020.3004555Proceedings of the IEEE. 10912020Prepared using sagej.cls</p>            </div>
        </div>

    </div>
</body>
</html>
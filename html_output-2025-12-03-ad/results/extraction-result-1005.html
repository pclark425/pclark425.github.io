<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1005 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1005</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1005</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-207033829813aadc2f2dca8f93279352d39de759</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/207033829813aadc2f2dca8f93279352d39de759" target="_blank">Learning Neural Causal Models from Unknown Interventions</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper provides a general framework based on continuous optimization and neural networks to create models for the combination of observational and interventional data and establishes strong benchmark results on several structure learning tasks.</p>
                <p><strong>Paper Abstract:</strong> Promising results have driven a recent surge of interest in continuous optimization methods for Bayesian network structure learning from observational data. However, there are theoretical limitations on the identifiability of underlying structures obtained from observational data alone. Interventional data provides much richer information about the underlying data-generating process. However, the extension and application of methods designed for observational data to include interventions is not straightforward and remains an open problem. In this paper we provide a general framework based on continuous optimization and neural networks to create models for the combination of observational and interventional data. The proposed method is even applicable in the challenging and realistic case that the identity of the intervened upon variable is unknown. We examine the proposed method in the setting of graph recovery both de novo and from a partially-known edge set. We establish strong benchmark results on several structure learning tasks, including structure recovery of both synthetic graphs as well as standard graphs from the Bayesian Network Repository.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1005.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1005.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SDI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural Discovery from Interventions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous-optimization, score-based neural method that jointly learns a soft adjacency (structure) and functional conditional models from observational and interventional data, explicitly handling unknown (unidentified) soft interventions by predicting intervention targets and scoring sampled graph configurations on interventional batches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Structural Discovery from Interventions (SDI)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>SDI parametrizes structure with real-valued matrix gamma where sigma(gamma_ij) is belief in edge j->i, and models each variable's conditional with an MLP masked by sampled adjacency configurations. Training alternates: (Phase 1) train functional parameters on observational data under sampled adjacency masks (edge-dropout); (Phase 2) when an (unknown) intervention occurs in the black-box, sample graph configurations and score them on interventional batches while masking the intervened variable's contribution; predict intervention target by per-variable log-likelihood deterioration if target is unknown; (Phase 3) aggregate interventional scores across sampled graphs and apply a REINFORCE-like gradient estimator to update gamma, with L1 sparsity and an acyclicity (length-2 cycle) regularizer. Final graph is obtained by thresholding sigma(gamma).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Black-box SCM virtual lab (synthetic MLP SCMs and BnLearn benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A simulated black-box SCM that supports ancestral sampling and single soft interventions (target chosen uniformly at random in experiments); interactive in that interventions occur and SDI can sample observational and interventional batches, but SDI does not control which interventions occur (passive observation of interventions). Evaluations include synthetic MLP-based SCMs and real Bayesian-network benchmarks (BnLearn).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detection of intervened variable via per-variable log-likelihood deterioration; downweighting/masking the intervened variable's contribution to scoring and blocking gradient into its functional parameters; structural L1 sparsity regularizer to discourage spurious edges; acyclicity regularizer; edge-dropout (sampling adjacency masks) during functional training to reduce reliance on spurious inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Unknown/uncertain (unidentified) interventions that would otherwise produce spurious correlations, irrelevant/spurious edges (statistical correlations not causal), distributional shifts due to interventions (soft or hard); note: latent confounders are assumed absent (causal sufficiency), so latent confounding is not handled.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Heuristic: compute average log-likelihood (under functional parameters trained on observational data) per variable on a small interventional sample; the variable with largest deterioration (worst predictive log-likelihood) is predicted as the intervention target.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Mask (zero out) the intervened variable's contribution to the total log-likelihood when scoring graph configurations, and block gradients into that variable's functional model so the learner does not adapt functional parameters to explain intervened values; use L1 penalty on structure to shrink spurious edges; use sampled adjacency (edge dropout) to avoid overfitting to spurious dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Score competing graph configurations on interventional batches (excluding intervened variable contribution), aggregate per-configuration per-variable log-likelihoods into a REINFORCE-like gradient estimator that increases beliefs in graph configurations consistent with interventional data and decreases beliefs in inconsistent edges; threshold sigma(gamma) for final refutation of spurious edges.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>SDI recovers many synthetic DAGs perfectly for M=3..13 (except dense full graphs); AUROC for edge probabilities reaches 1.0 for many graphs (M<=8). Table 1: SDI SHD=0 on Asia and many synthetic graphs; SDI SHD=6 on Sachs (11 vars). Generalization test (Table 2): under previously unseen interventions, SDI test log-likelihoods are higher than a non-causal baseline (e.g. fork3 baseline -0.5036 vs SDI -0.4502). Intervention prediction accuracies (Table 3): 95% (3 vars), 93% (4 vars), 85% (5 vars), 71% (8 vars).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Non-causal variant (all c_ij = 1) performs worse on transfer under interventions (Table 2 baseline values): chain3 baseline -0.4562 vs SDI -0.3801; collider3 baseline -0.5082 vs SDI -0.4677. Ablations: not predicting intervention or predicting randomly significantly slows or prevents learning (Figure 11).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit detection and masking of intervened variables plus structural regularization and edge-dropout allow SDI to disentangle causal structure from spurious correlations induced by unknown interventions; the intervention-prediction heuristic closely tracks ground-truth-leaked training and is essential — not predicting the intervention harms learning; SDI outperforms several baselines (ICP, non-linear ICP, DAG-GNN, NO TEARS variants) on SHD and AUROC in benchmarks; however dense graphs (maximally connected) remain challenging and sample-complexity scales with number of edges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Neural Causal Models from Unknown Interventions', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1005.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1005.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Intervention Prediction Heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intervention Target Prediction via Per-Variable Log-Likelihood Deterioration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic used within SDI to infer which variable was intervened upon when the intervention target is unknown, by identifying the variable whose predictive log-likelihood (under observationally-trained functional models) deteriorates most under the intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Per-variable log-likelihood deterioration heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Draw a small interventional sample; for each variable compute average log-likelihood under functional models trained on observational data across sampled graph configurations; select variable with largest drop/worst log-likelihood as intervention target; subsequently mask that variable's contribution in scoring and block its functional-gradient updates.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Black-box SCM virtual lab with unknown single interventions</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Passive setting where single soft interventions randomly occur on a single variable and the learner must infer the target with limited interventional samples.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detects intervention-induced spurious signals by finding the variable whose predictive likelihood degrades most; then removes its influence during scoring to avoid attributing spurious changes to functional parameters or edges.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations and distributional shift localized to the intervened variable (unknown/uncertain intervention).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Compute per-variable average log-likelihood on interventional batches using functional models trained on observational data; pick argmax deterioration.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Mask the identified variable's log-likelihood contribution and block gradient into its functional model while scoring candidate graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>By eliminating the intervened variable's contribution, the procedure prevents false credit assignment to edges and allows scoring to more reliably refute edges inconsistent with interventional effects.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Heuristic yields high intervention-prediction accuracy in experiments (Table 3): 95% for 3-variable graphs, 93% for 4-variable, 85% for 5-variable, 71% for 8-variable; training curves with prediction closely track training with leaked true target (Figure 11).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>No prediction or random prediction causes training to be slower or fail (Figure 11); performance drops substantially compared to using the heuristic.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A simple log-likelihood deterioration heuristic is effective and critical in practice for handling unknown-target interventions; identifying and masking the intervened variable prevents spurious gradient updates and preserves structural learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Neural Causal Models from Unknown Interventions', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1005.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1005.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mask-and-Block</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Masking Intervened Variable Contribution and Blocking Gradients</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique to avoid attributing intervention-induced outcomes to learned conditional models by removing the intervened variable's contribution when scoring graphs and preventing gradient updates to its functional parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Masking & Gradient Blocking for Intervened Variables</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>When scoring candidate graphs on interventional data, the contribution to the log-likelihood from the intervened (or predicted-intervened) variable is zeroed out, and gradient backpropagation into that variable's functional model is disabled so scoring reflects causal consistency of other variables and prevents the method from 'explaining away' intervention effects via functional parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Black-box SCM virtual lab</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Passive interventional environment where interventions may be unknown and must not corrupt functional parameter estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Downweighting by masking/removal of the intervened variable's statistical signal from scoring; prevents spurious adaptation to intervention-induced values.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Intervention-induced distributional shifts and local spurious signals on single variables.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Used in conjunction with the intervention-prediction heuristic (or leaked target); requires knowledge or prediction of intervened variable.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Zero-out log-likelihood term for the intervened variable, block gradients into its functional model.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Removes confounding effect of intervention during scoring so graph configurations inconsistent with interventional behavior receive lower aggregate scores, enabling structural parameters to be updated to refute spurious edges.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Critical for effective Phase 2 scoring; experiments show that masking is necessary to avoid poor/failing training when interventions are unknown (ablation described qualitatively and via Figure 11).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Without masking or with incorrect prediction of the intervened variable, training degrades markedly or fails to converge to correct structure (Figure 11).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Masking and gradient blocking for the intervened variable is essential to prevent misattribution of intervention effects to the learned mechanisms, thereby reducing spurious updates and improving graph recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Neural Causal Models from Unknown Interventions', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1005.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1005.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Edge Dropout (Adjacency Sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge Dropout via Sampling Adjacency Matrices from Bernoulli(gamma)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>During functional-parameter training on observational data, SDI samples adjacency matrices from Bernoulli(sigma(gamma)) and uses these masks as input dropout, producing an ensemble of conditional models and preventing over-reliance on spurious inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Edge-dropout via sampled adjacency masks</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Sample configurations C ~ Bernoulli(sigma(gamma)) and mask inputs to each variable's MLP according to c_ij so that the functional models are trained under many possible parent subsets; this acts analogously to dropout and reduces chance of fitting spurious correlations present in observational data.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Black-box SCM (observational pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Observational data regime used to pretrain functional conditionals before interventional scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Regularization via stochastic masking of candidate parents (dropout-like) to prevent learned conditional models from relying on spurious predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant variables and spurious predictive correlations arising in observational data.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Implicit: edges with low gamma probabilities are effectively dropped during functional training, reducing their influence.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Ablation (Section 7.14) shows that training functional parameters without edge-dropout fails to recover previously-recoverable graphs (chain3, fork3, confounder3), demonstrating substantial robustness gains when using adjacency sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Without dropout, functional training overfits to spurious correlations and SDI fails to recover correct structure on small graphs (Figure 15).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Edge-dropout (sampling adjacency) during functional learning is necessary: it prevents spurious dependencies from being learned as causal mechanisms and is crucial for downstream correct structural recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Neural Causal Models from Unknown Interventions', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1005.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1005.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REINFORCE-like gamma estimator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REINFORCE-like Gradient Estimator for Structural Parameters (from Bengio et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A credit-assignment estimator used to backpropagate interventional scoring information through the discrete sampling of graph configurations into continuous structural beliefs gamma.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A meta-transfer objective for learning to disentangle causal mechanisms</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>REINFORCE-like estimator for gamma (Bengio et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Given K sampled configurations C^(k) ~ Bernoulli(sigma(gamma)), compute per-variable log-likelihoods L_{C,i}^{(k)} on interventional data; estimate gradient g_ij proportional to sum_k (sigma(gamma_ij) - c_ij^(k)) * L_{C,i}^{(k)} normalized by sum_k L_{C,i}^{(k)}; use this estimator to update gamma with SGD/Adam, thereby assigning credit to edges based on how configurations with/without that edge explain interventional outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Interventional scoring phase in SDI</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Used during Phase 3 to propagate scores obtained from scoring sampled graphs on interventional batches back to continuous structural parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Aggregates evidence from interventional batches across sampled graph configurations so that edges that consistently reduce likelihood under interventions are downweighted in gamma; indirectly refutes spurious edges inconsistent with interventional data.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Intervention-induced shifts and spurious correlations that make certain edges less predictive under interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Not a detector per se, but uses interventional log-likelihood as signal to determine which sampled configurations are penalized or rewarded.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Edges absent in higher-scoring configurations (relative to sampled ones) see their sigma(gamma) reduced via the estimator term (sigma(gamma)-c_ij) weighted by log-likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Edges that lead to lower explanatory power on interventional data are progressively downweighted in gamma, enabling refutation of spurious edges over training.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Used as part of SDI which achieves strong SHD/AUROC results; estimator enables learning from unknown interventions by translating interventional evidence into structural belief updates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>If structural credit assignment is disabled or not properly normalized, SDI cannot reliably update gamma from discrete configuration scores; ablations imply estimator choice matters though precise numeric ablation of estimator not separately tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sampling-based credit assignment through a REINFORCE-like estimator is an effective mechanism to update continuous structural beliefs from interventional scoring, enabling learning even when interventions are unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Neural Causal Models from Unknown Interventions', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1005.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1005.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eaton & Murphy uncertain interventions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Belief net structure learning from uncertain interventions / Exact bayesian structure learning from uncertain interventions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bayesian approaches that model uncertain/unknown interventions by augmenting the model (e.g., adding 'shadow' variables) to represent intervention uncertainty, enabling learning from imperfectly known interventions but with severe scaling costs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Belief net structure learning from uncertain interventions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Eaton & Murphy's uncertain-intervention Bayesian methods</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Model uncertain intervention targets by introducing auxiliary/shadow variables that capture whether an intervention affected each variable and perform Bayesian structure learning (exact or via dynamic programming / MCMC) over the augmented model; capable of representing imperfect/uncertain targets but increases problem dimensionality (d -> 2d) and memory/time complexity (O(d 2^d)).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Interventional learning with uncertain targets (conceptual / small-scale problems)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Intended for datasets where intervention targets are imperfectly known; not designed for large interactive virtual labs due to computational blowup.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Explicit latent/shadow variables to model uncertainty about which variables were intervened, thereby accounting for intervention-induced spurious signals in a Bayesian framework.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Uncertain/unclean interventions that corrupt observed distributions; intervention-induced spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Model-based: represent intervention uncertainty explicitly and infer intervention target assignments during Bayesian inference.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Probability mass is allocated across possible intervention assignments; edges that only explain intervention artifacts are downweighted in posterior inference.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Bayesian model selection / marginalization over intervention assignments refutes edges inconsistent with majority posterior support.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Can correctly handle uncertain interventions for small d; paper reports correct behavior on small graphs but suffers severe memory/time scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Not applicable; method explicitly models uncertainty rather than ignoring it — but if not used, unknown interventions can mislead structure learning.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Modeling intervention uncertainty explicitly can account for intervention-induced spurious signals, but the 'shadow variable' approach scales poorly (runs out of memory for d>20) making it impractical for large virtual labs or many-variable SCMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Neural Causal Models from Unknown Interventions', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1005.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1005.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Backshift</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Backshift: Learning causal cyclic graphs from unknown shift interventions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that leverages unknown shift/shift-like interventions across environments to learn causal structure, including certain cyclic graphs, by exploiting differences in covariance/shift patterns rather than requiring known intervention targets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Backshift: Learning causal cyclic graphs from unknown shift interventions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Backshift (Rothenhäusler et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses the structure in distributional shifts across environments (unknown shifts) to infer causal relations by relating changes in second-order statistics (covariance) to unknown shift interventions and solving linear equations to recover causal structure; particularly aimed at cyclic graphs under shift interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple-environment / shift-intervention setups (passive environments with unknown shifts)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Works with multiple datasets/environments produced by unknown shifts (e.g., changes in noise distributions) rather than controlled interventions; not an active experimental design method.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Exploit invariance and shift patterns across environments to attribute distributional changes to shifts rather than spurious correlations within a single environment.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Unknown shift/perturbation interventions (distributional shifts) that could otherwise be mistaken for causal effects.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects shifts by comparing second-moment (covariance) changes across environments and uses that structure to solve for causal relations.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Edges inconsistent with observed shift-structure across environments are rejected when solving the linear systems that recover the causal graph.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Effective on problems with unknown shift interventions; cited in SDI related work as relevant prior approach for unknown interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Backshift shows that unknown shift-type interventions can be exploited by studying distributional differences across environments; complements techniques that rely on direct intervention identification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title_alternate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Neural Causal Models from Unknown Interventions', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1005.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1005.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICP / non-linear ICP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Causal Prediction (ICP) and Non-linear ICP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that exploit invariance of causal conditionals across environments to identify causal parents and rule out spurious predictors by requiring that conditional distributions remain stable across different experimental conditions or environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal inference by using invariant prediction: identification and confidence intervals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Causal Prediction (ICP) and non-linear ICP</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>ICP searches for sets of predictors whose conditional distribution for a target remains invariant across multiple environments (interventions/contexts), using hypothesis tests on invariance to eliminate spurious predictors; non-linear ICP extends the idea to nonlinear models.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multi-environment / multi-context datasets (known or unknown interventions producing environments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Methods require multiple environments with different interventions or contexts; typically passive observation of environments but can be applied when interventions are known or when environments are treated as distinct.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detection and refutation via invariance tests: variables whose conditional distributions change across environments are considered non-causal (distractors) for the target.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations that are not invariant across environments, distributional shifts, and intervention-induced artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Statistical tests for equality/invariance of conditional distributions across environments (e.g., conditional independence or test of residuals), extended to nonlinear settings in non-linear ICP.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Rejects predictor sets not satisfying invariance constraints, thereby refuting spurious relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>ICP is robust when sufficient diverse environments exist; SDI experimentally outperforms ICP on several benchmarks per Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Invariance-based approaches provide principled ways to detect and rule out spurious predictors across environments, but may face scalability challenges if applied naively over the super-exponential graph space.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Neural Causal Models from Unknown Interventions', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Causal inference by using invariant prediction: identification and confidence intervals <em>(Rating: 2)</em></li>
                <li>Belief net structure learning from uncertain interventions <em>(Rating: 2)</em></li>
                <li>Backshift: Learning causal cyclic graphs from unknown shift interventions <em>(Rating: 2)</em></li>
                <li>A meta-transfer objective for learning to disentangle causal mechanisms <em>(Rating: 2)</em></li>
                <li>DAGs with NO TEARS: Continuous optimization for structure learning <em>(Rating: 2)</em></li>
                <li>Invariant causal prediction for nonlinear models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1005",
    "paper_id": "paper-207033829813aadc2f2dca8f93279352d39de759",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "SDI",
            "name_full": "Structural Discovery from Interventions",
            "brief_description": "A continuous-optimization, score-based neural method that jointly learns a soft adjacency (structure) and functional conditional models from observational and interventional data, explicitly handling unknown (unidentified) soft interventions by predicting intervention targets and scoring sampled graph configurations on interventional batches.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Structural Discovery from Interventions (SDI)",
            "method_description": "SDI parametrizes structure with real-valued matrix gamma where sigma(gamma_ij) is belief in edge j-&gt;i, and models each variable's conditional with an MLP masked by sampled adjacency configurations. Training alternates: (Phase 1) train functional parameters on observational data under sampled adjacency masks (edge-dropout); (Phase 2) when an (unknown) intervention occurs in the black-box, sample graph configurations and score them on interventional batches while masking the intervened variable's contribution; predict intervention target by per-variable log-likelihood deterioration if target is unknown; (Phase 3) aggregate interventional scores across sampled graphs and apply a REINFORCE-like gradient estimator to update gamma, with L1 sparsity and an acyclicity (length-2 cycle) regularizer. Final graph is obtained by thresholding sigma(gamma).",
            "environment_name": "Black-box SCM virtual lab (synthetic MLP SCMs and BnLearn benchmarks)",
            "environment_description": "A simulated black-box SCM that supports ancestral sampling and single soft interventions (target chosen uniformly at random in experiments); interactive in that interventions occur and SDI can sample observational and interventional batches, but SDI does not control which interventions occur (passive observation of interventions). Evaluations include synthetic MLP-based SCMs and real Bayesian-network benchmarks (BnLearn).",
            "handles_distractors": true,
            "distractor_handling_technique": "Detection of intervened variable via per-variable log-likelihood deterioration; downweighting/masking the intervened variable's contribution to scoring and blocking gradient into its functional parameters; structural L1 sparsity regularizer to discourage spurious edges; acyclicity regularizer; edge-dropout (sampling adjacency masks) during functional training to reduce reliance on spurious inputs.",
            "spurious_signal_types": "Unknown/uncertain (unidentified) interventions that would otherwise produce spurious correlations, irrelevant/spurious edges (statistical correlations not causal), distributional shifts due to interventions (soft or hard); note: latent confounders are assumed absent (causal sufficiency), so latent confounding is not handled.",
            "detection_method": "Heuristic: compute average log-likelihood (under functional parameters trained on observational data) per variable on a small interventional sample; the variable with largest deterioration (worst predictive log-likelihood) is predicted as the intervention target.",
            "downweighting_method": "Mask (zero out) the intervened variable's contribution to the total log-likelihood when scoring graph configurations, and block gradients into that variable's functional model so the learner does not adapt functional parameters to explain intervened values; use L1 penalty on structure to shrink spurious edges; use sampled adjacency (edge dropout) to avoid overfitting to spurious dependencies.",
            "refutation_method": "Score competing graph configurations on interventional batches (excluding intervened variable contribution), aggregate per-configuration per-variable log-likelihoods into a REINFORCE-like gradient estimator that increases beliefs in graph configurations consistent with interventional data and decreases beliefs in inconsistent edges; threshold sigma(gamma) for final refutation of spurious edges.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "SDI recovers many synthetic DAGs perfectly for M=3..13 (except dense full graphs); AUROC for edge probabilities reaches 1.0 for many graphs (M&lt;=8). Table 1: SDI SHD=0 on Asia and many synthetic graphs; SDI SHD=6 on Sachs (11 vars). Generalization test (Table 2): under previously unseen interventions, SDI test log-likelihoods are higher than a non-causal baseline (e.g. fork3 baseline -0.5036 vs SDI -0.4502). Intervention prediction accuracies (Table 3): 95% (3 vars), 93% (4 vars), 85% (5 vars), 71% (8 vars).",
            "performance_without_robustness": "Non-causal variant (all c_ij = 1) performs worse on transfer under interventions (Table 2 baseline values): chain3 baseline -0.4562 vs SDI -0.3801; collider3 baseline -0.5082 vs SDI -0.4677. Ablations: not predicting intervention or predicting randomly significantly slows or prevents learning (Figure 11).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Explicit detection and masking of intervened variables plus structural regularization and edge-dropout allow SDI to disentangle causal structure from spurious correlations induced by unknown interventions; the intervention-prediction heuristic closely tracks ground-truth-leaked training and is essential — not predicting the intervention harms learning; SDI outperforms several baselines (ICP, non-linear ICP, DAG-GNN, NO TEARS variants) on SHD and AUROC in benchmarks; however dense graphs (maximally connected) remain challenging and sample-complexity scales with number of edges.",
            "uuid": "e1005.0",
            "source_info": {
                "paper_title": "Learning Neural Causal Models from Unknown Interventions",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Intervention Prediction Heuristic",
            "name_full": "Intervention Target Prediction via Per-Variable Log-Likelihood Deterioration",
            "brief_description": "A heuristic used within SDI to infer which variable was intervened upon when the intervention target is unknown, by identifying the variable whose predictive log-likelihood (under observationally-trained functional models) deteriorates most under the intervention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Per-variable log-likelihood deterioration heuristic",
            "method_description": "Draw a small interventional sample; for each variable compute average log-likelihood under functional models trained on observational data across sampled graph configurations; select variable with largest drop/worst log-likelihood as intervention target; subsequently mask that variable's contribution in scoring and block its functional-gradient updates.",
            "environment_name": "Black-box SCM virtual lab with unknown single interventions",
            "environment_description": "Passive setting where single soft interventions randomly occur on a single variable and the learner must infer the target with limited interventional samples.",
            "handles_distractors": true,
            "distractor_handling_technique": "Detects intervention-induced spurious signals by finding the variable whose predictive likelihood degrades most; then removes its influence during scoring to avoid attributing spurious changes to functional parameters or edges.",
            "spurious_signal_types": "Spurious correlations and distributional shift localized to the intervened variable (unknown/uncertain intervention).",
            "detection_method": "Compute per-variable average log-likelihood on interventional batches using functional models trained on observational data; pick argmax deterioration.",
            "downweighting_method": "Mask the identified variable's log-likelihood contribution and block gradient into its functional model while scoring candidate graphs.",
            "refutation_method": "By eliminating the intervened variable's contribution, the procedure prevents false credit assignment to edges and allows scoring to more reliably refute edges inconsistent with interventional effects.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Heuristic yields high intervention-prediction accuracy in experiments (Table 3): 95% for 3-variable graphs, 93% for 4-variable, 85% for 5-variable, 71% for 8-variable; training curves with prediction closely track training with leaked true target (Figure 11).",
            "performance_without_robustness": "No prediction or random prediction causes training to be slower or fail (Figure 11); performance drops substantially compared to using the heuristic.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "A simple log-likelihood deterioration heuristic is effective and critical in practice for handling unknown-target interventions; identifying and masking the intervened variable prevents spurious gradient updates and preserves structural learning.",
            "uuid": "e1005.1",
            "source_info": {
                "paper_title": "Learning Neural Causal Models from Unknown Interventions",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Mask-and-Block",
            "name_full": "Masking Intervened Variable Contribution and Blocking Gradients",
            "brief_description": "A technique to avoid attributing intervention-induced outcomes to learned conditional models by removing the intervened variable's contribution when scoring graphs and preventing gradient updates to its functional parameters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Masking & Gradient Blocking for Intervened Variables",
            "method_description": "When scoring candidate graphs on interventional data, the contribution to the log-likelihood from the intervened (or predicted-intervened) variable is zeroed out, and gradient backpropagation into that variable's functional model is disabled so scoring reflects causal consistency of other variables and prevents the method from 'explaining away' intervention effects via functional parameter updates.",
            "environment_name": "Black-box SCM virtual lab",
            "environment_description": "Passive interventional environment where interventions may be unknown and must not corrupt functional parameter estimation.",
            "handles_distractors": true,
            "distractor_handling_technique": "Downweighting by masking/removal of the intervened variable's statistical signal from scoring; prevents spurious adaptation to intervention-induced values.",
            "spurious_signal_types": "Intervention-induced distributional shifts and local spurious signals on single variables.",
            "detection_method": "Used in conjunction with the intervention-prediction heuristic (or leaked target); requires knowledge or prediction of intervened variable.",
            "downweighting_method": "Zero-out log-likelihood term for the intervened variable, block gradients into its functional model.",
            "refutation_method": "Removes confounding effect of intervention during scoring so graph configurations inconsistent with interventional behavior receive lower aggregate scores, enabling structural parameters to be updated to refute spurious edges.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Critical for effective Phase 2 scoring; experiments show that masking is necessary to avoid poor/failing training when interventions are unknown (ablation described qualitatively and via Figure 11).",
            "performance_without_robustness": "Without masking or with incorrect prediction of the intervened variable, training degrades markedly or fails to converge to correct structure (Figure 11).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Masking and gradient blocking for the intervened variable is essential to prevent misattribution of intervention effects to the learned mechanisms, thereby reducing spurious updates and improving graph recovery.",
            "uuid": "e1005.2",
            "source_info": {
                "paper_title": "Learning Neural Causal Models from Unknown Interventions",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Edge Dropout (Adjacency Sampling)",
            "name_full": "Edge Dropout via Sampling Adjacency Matrices from Bernoulli(gamma)",
            "brief_description": "During functional-parameter training on observational data, SDI samples adjacency matrices from Bernoulli(sigma(gamma)) and uses these masks as input dropout, producing an ensemble of conditional models and preventing over-reliance on spurious inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Edge-dropout via sampled adjacency masks",
            "method_description": "Sample configurations C ~ Bernoulli(sigma(gamma)) and mask inputs to each variable's MLP according to c_ij so that the functional models are trained under many possible parent subsets; this acts analogously to dropout and reduces chance of fitting spurious correlations present in observational data.",
            "environment_name": "Black-box SCM (observational pretraining)",
            "environment_description": "Observational data regime used to pretrain functional conditionals before interventional scoring.",
            "handles_distractors": true,
            "distractor_handling_technique": "Regularization via stochastic masking of candidate parents (dropout-like) to prevent learned conditional models from relying on spurious predictors.",
            "spurious_signal_types": "Irrelevant variables and spurious predictive correlations arising in observational data.",
            "detection_method": null,
            "downweighting_method": "Implicit: edges with low gamma probabilities are effectively dropped during functional training, reducing their influence.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Ablation (Section 7.14) shows that training functional parameters without edge-dropout fails to recover previously-recoverable graphs (chain3, fork3, confounder3), demonstrating substantial robustness gains when using adjacency sampling.",
            "performance_without_robustness": "Without dropout, functional training overfits to spurious correlations and SDI fails to recover correct structure on small graphs (Figure 15).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Edge-dropout (sampling adjacency) during functional learning is necessary: it prevents spurious dependencies from being learned as causal mechanisms and is crucial for downstream correct structural recovery.",
            "uuid": "e1005.3",
            "source_info": {
                "paper_title": "Learning Neural Causal Models from Unknown Interventions",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "REINFORCE-like gamma estimator",
            "name_full": "REINFORCE-like Gradient Estimator for Structural Parameters (from Bengio et al.)",
            "brief_description": "A credit-assignment estimator used to backpropagate interventional scoring information through the discrete sampling of graph configurations into continuous structural beliefs gamma.",
            "citation_title": "A meta-transfer objective for learning to disentangle causal mechanisms",
            "mention_or_use": "use",
            "method_name": "REINFORCE-like estimator for gamma (Bengio et al.)",
            "method_description": "Given K sampled configurations C^(k) ~ Bernoulli(sigma(gamma)), compute per-variable log-likelihoods L_{C,i}^{(k)} on interventional data; estimate gradient g_ij proportional to sum_k (sigma(gamma_ij) - c_ij^(k)) * L_{C,i}^{(k)} normalized by sum_k L_{C,i}^{(k)}; use this estimator to update gamma with SGD/Adam, thereby assigning credit to edges based on how configurations with/without that edge explain interventional outcomes.",
            "environment_name": "Interventional scoring phase in SDI",
            "environment_description": "Used during Phase 3 to propagate scores obtained from scoring sampled graphs on interventional batches back to continuous structural parameters.",
            "handles_distractors": true,
            "distractor_handling_technique": "Aggregates evidence from interventional batches across sampled graph configurations so that edges that consistently reduce likelihood under interventions are downweighted in gamma; indirectly refutes spurious edges inconsistent with interventional data.",
            "spurious_signal_types": "Intervention-induced shifts and spurious correlations that make certain edges less predictive under interventions.",
            "detection_method": "Not a detector per se, but uses interventional log-likelihood as signal to determine which sampled configurations are penalized or rewarded.",
            "downweighting_method": "Edges absent in higher-scoring configurations (relative to sampled ones) see their sigma(gamma) reduced via the estimator term (sigma(gamma)-c_ij) weighted by log-likelihoods.",
            "refutation_method": "Edges that lead to lower explanatory power on interventional data are progressively downweighted in gamma, enabling refutation of spurious edges over training.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Used as part of SDI which achieves strong SHD/AUROC results; estimator enables learning from unknown interventions by translating interventional evidence into structural belief updates.",
            "performance_without_robustness": "If structural credit assignment is disabled or not properly normalized, SDI cannot reliably update gamma from discrete configuration scores; ablations imply estimator choice matters though precise numeric ablation of estimator not separately tabulated.",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Sampling-based credit assignment through a REINFORCE-like estimator is an effective mechanism to update continuous structural beliefs from interventional scoring, enabling learning even when interventions are unknown.",
            "uuid": "e1005.4",
            "source_info": {
                "paper_title": "Learning Neural Causal Models from Unknown Interventions",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Eaton & Murphy uncertain interventions",
            "name_full": "Belief net structure learning from uncertain interventions / Exact bayesian structure learning from uncertain interventions",
            "brief_description": "Bayesian approaches that model uncertain/unknown interventions by augmenting the model (e.g., adding 'shadow' variables) to represent intervention uncertainty, enabling learning from imperfectly known interventions but with severe scaling costs.",
            "citation_title": "Belief net structure learning from uncertain interventions",
            "mention_or_use": "mention",
            "method_name": "Eaton & Murphy's uncertain-intervention Bayesian methods",
            "method_description": "Model uncertain intervention targets by introducing auxiliary/shadow variables that capture whether an intervention affected each variable and perform Bayesian structure learning (exact or via dynamic programming / MCMC) over the augmented model; capable of representing imperfect/uncertain targets but increases problem dimensionality (d -&gt; 2d) and memory/time complexity (O(d 2^d)).",
            "environment_name": "Interventional learning with uncertain targets (conceptual / small-scale problems)",
            "environment_description": "Intended for datasets where intervention targets are imperfectly known; not designed for large interactive virtual labs due to computational blowup.",
            "handles_distractors": true,
            "distractor_handling_technique": "Explicit latent/shadow variables to model uncertainty about which variables were intervened, thereby accounting for intervention-induced spurious signals in a Bayesian framework.",
            "spurious_signal_types": "Uncertain/unclean interventions that corrupt observed distributions; intervention-induced spurious correlations.",
            "detection_method": "Model-based: represent intervention uncertainty explicitly and infer intervention target assignments during Bayesian inference.",
            "downweighting_method": "Probability mass is allocated across possible intervention assignments; edges that only explain intervention artifacts are downweighted in posterior inference.",
            "refutation_method": "Bayesian model selection / marginalization over intervention assignments refutes edges inconsistent with majority posterior support.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "Can correctly handle uncertain interventions for small d; paper reports correct behavior on small graphs but suffers severe memory/time scaling.",
            "performance_without_robustness": "Not applicable; method explicitly models uncertainty rather than ignoring it — but if not used, unknown interventions can mislead structure learning.",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Modeling intervention uncertainty explicitly can account for intervention-induced spurious signals, but the 'shadow variable' approach scales poorly (runs out of memory for d&gt;20) making it impractical for large virtual labs or many-variable SCMs.",
            "uuid": "e1005.5",
            "source_info": {
                "paper_title": "Learning Neural Causal Models from Unknown Interventions",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Backshift",
            "name_full": "Backshift: Learning causal cyclic graphs from unknown shift interventions",
            "brief_description": "A method that leverages unknown shift/shift-like interventions across environments to learn causal structure, including certain cyclic graphs, by exploiting differences in covariance/shift patterns rather than requiring known intervention targets.",
            "citation_title": "Backshift: Learning causal cyclic graphs from unknown shift interventions",
            "mention_or_use": "mention",
            "method_name": "Backshift (Rothenhäusler et al.)",
            "method_description": "Uses the structure in distributional shifts across environments (unknown shifts) to infer causal relations by relating changes in second-order statistics (covariance) to unknown shift interventions and solving linear equations to recover causal structure; particularly aimed at cyclic graphs under shift interventions.",
            "environment_name": "Multiple-environment / shift-intervention setups (passive environments with unknown shifts)",
            "environment_description": "Works with multiple datasets/environments produced by unknown shifts (e.g., changes in noise distributions) rather than controlled interventions; not an active experimental design method.",
            "handles_distractors": true,
            "distractor_handling_technique": "Exploit invariance and shift patterns across environments to attribute distributional changes to shifts rather than spurious correlations within a single environment.",
            "spurious_signal_types": "Unknown shift/perturbation interventions (distributional shifts) that could otherwise be mistaken for causal effects.",
            "detection_method": "Detects shifts by comparing second-moment (covariance) changes across environments and uses that structure to solve for causal relations.",
            "downweighting_method": null,
            "refutation_method": "Edges inconsistent with observed shift-structure across environments are rejected when solving the linear systems that recover the causal graph.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "Effective on problems with unknown shift interventions; cited in SDI related work as relevant prior approach for unknown interventions.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Backshift shows that unknown shift-type interventions can be exploited by studying distributional differences across environments; complements techniques that rely on direct intervention identification.",
            "citation_title_alternate": null,
            "uuid": "e1005.6",
            "source_info": {
                "paper_title": "Learning Neural Causal Models from Unknown Interventions",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "ICP / non-linear ICP",
            "name_full": "Invariant Causal Prediction (ICP) and Non-linear ICP",
            "brief_description": "Methods that exploit invariance of causal conditionals across environments to identify causal parents and rule out spurious predictors by requiring that conditional distributions remain stable across different experimental conditions or environments.",
            "citation_title": "Causal inference by using invariant prediction: identification and confidence intervals",
            "mention_or_use": "mention",
            "method_name": "Invariant Causal Prediction (ICP) and non-linear ICP",
            "method_description": "ICP searches for sets of predictors whose conditional distribution for a target remains invariant across multiple environments (interventions/contexts), using hypothesis tests on invariance to eliminate spurious predictors; non-linear ICP extends the idea to nonlinear models.",
            "environment_name": "Multi-environment / multi-context datasets (known or unknown interventions producing environments)",
            "environment_description": "Methods require multiple environments with different interventions or contexts; typically passive observation of environments but can be applied when interventions are known or when environments are treated as distinct.",
            "handles_distractors": true,
            "distractor_handling_technique": "Detection and refutation via invariance tests: variables whose conditional distributions change across environments are considered non-causal (distractors) for the target.",
            "spurious_signal_types": "Spurious correlations that are not invariant across environments, distributional shifts, and intervention-induced artifacts.",
            "detection_method": "Statistical tests for equality/invariance of conditional distributions across environments (e.g., conditional independence or test of residuals), extended to nonlinear settings in non-linear ICP.",
            "downweighting_method": null,
            "refutation_method": "Rejects predictor sets not satisfying invariance constraints, thereby refuting spurious relationships.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "ICP is robust when sufficient diverse environments exist; SDI experimentally outperforms ICP on several benchmarks per Table 1.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Invariance-based approaches provide principled ways to detect and rule out spurious predictors across environments, but may face scalability challenges if applied naively over the super-exponential graph space.",
            "uuid": "e1005.7",
            "source_info": {
                "paper_title": "Learning Neural Causal Models from Unknown Interventions",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Causal inference by using invariant prediction: identification and confidence intervals",
            "rating": 2
        },
        {
            "paper_title": "Belief net structure learning from uncertain interventions",
            "rating": 2
        },
        {
            "paper_title": "Backshift: Learning causal cyclic graphs from unknown shift interventions",
            "rating": 2
        },
        {
            "paper_title": "A meta-transfer objective for learning to disentangle causal mechanisms",
            "rating": 2
        },
        {
            "paper_title": "DAGs with NO TEARS: Continuous optimization for structure learning",
            "rating": 2
        },
        {
            "paper_title": "Invariant causal prediction for nonlinear models",
            "rating": 1
        }
    ],
    "cost": 0.01974125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning Neural Causal Models from Unknown Interventions</h1>
<p>Nan Rosemary $\mathrm{Ke}^{<em> 1,2}$, Olexa Bilaniuk ${ }^{</em> 1}$, Anirudh Goyal ${ }^{1}$, Stefan Bauer ${ }^{5}$, Hugo Larochelle ${ }^{4}$, Bernhard Schölkopf ${ }^{5}$, Michael C. Mozer ${ }^{4}$, Chris Pal ${ }^{1,2,3}$, Yoshua Bengio ${ }^{1 \dagger}$<br>${ }^{1}$ Mila, Université de Montréal, ${ }^{2}$ Mila, Polytechnique Montréal, ${ }^{3}$ Element AI<br>${ }^{4}$ Google AI, ${ }^{5}$ Max Planck Institute for Intelligent Systems, ${ }^{1}$ CIFAR Senior Fellow.<br>* Authors contributed equally, rosemary.nan.ke@gmail.com</p>
<h4>Abstract</h4>
<p>Promising results have driven a recent surge of interest in continuous optimization methods for Bayesian network structure learning from observational data. However, there are theoretical limitations on the identifiability of underlying structures obtained from observational data alone. Interventional data provides much richer information about the underlying data-generating process. However, the extension and application of methods designed for observational data to include interventions is not straightforward and remains an open problem. In this paper we provide a general framework based on continuous optimization and neural networks to create models for the combination of observational and interventional data. The proposed method is even applicable in the challenging and realistic case that the identity of the intervened upon variable is unknown. We examine the proposed method in the setting of graph recovery both de novo and from a partially-known edge set. We establish strong benchmark results on several structure learning tasks, including structure recovery of both synthetic graphs as well as standard graphs from the Bayesian Network Repository.</p>
<h2>1 Introduction</h2>
<p>Structure learning concerns itself with the recovery of the graph structure of Bayesian networks from data. When Bayesian networks are used to model cause-effect relationships and are augmented with the notion of interventions and counterfactuals, they can be represented as structural causal models (SCM). While Bayesian networks can uncover statistical correlations between factors, SCMs can be used to answer higher-order questions of cause-and-effect, up in the ladder of causation [1]. Causal structure learning using SCMs has been attempted in several disciplines including biology [2, 3], weather forecasting [4] and medicine [5, 6].
Structure can be learned from data samples drawn from observational or interventional distributions. The data can be more or less revelatory about the underlying structure, which affects a structure learning method's ability to identify structure using that data. Observational data is sampled from the distribution without interventions; alone, it contains only limited information about the underlying causal graph and hence structure learning methods generally cannot do more than identify the causal graph up to a Markov equivalence class [7]. In order to fully identify the true causal graph, a method must either make restrictive assumptions about the underlying data-generating process, such as linear but non-Gaussian data [8], or must access enough data from outside the observational distribution (i.e., from interventions). Under certain assumptions about the number, diversity, and nature of the interventions, the true underlying causal graph is always identifiable, given that the method knows the intervention performed [9]. However, in the real world, interventions can often be performed</p>
<p>by other agents, which would make them unknown interventions. A few works have attempted to learn structures from unknown-intervention data [10, 11, 12, 13]. Although there is no theoretical guarantee that the true causal graph can be identified in that setting, evidence so far points to that still being the case.</p>
<p>Another common setting is when the graph structure is partially provided, but must be completed. An example is protein structure learning in biology, where we may have definite knowledge about some parts of the protein-protein interaction structure and have to fill out other parts. We will call this setting partial graph completion. This is an easier task compared to learning the entire graph, since it limits the number of edges that have to be learned.</p>
<p>Recently, a flurry of work on structure learning using continuous optimization methods has appeared [14, 15]. These methods operate on observational data and perform competitively to other methods. There are theoretical limitations on how well the causal graph can be identified from only observational data and it would be interesting to apply these methods on interventional data, which offers more information about the underlying causal structure. However, it is not straightforward to apply continuous optimization methods to structure learning from interventional data. Contributions, we experimentally answer the following questions:</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: In many areas of science, such as biology, we try to infer the underlying mechanisms and structure through experiments. We can obtain observational data plus interventional data through known (e.g. by targeting a certain variable) or unknown interventions (e.g. when it is unclear where the effect of the intervention will be). Knowledge of existing edges e.g. through previous experiments can likewise be included and be considered a special case of causal induction.</p>
<ol>
<li>Can the proposed model recover true causal structure? Yes, see Figure $\S 4$.</li>
<li>How does the proposed model compare against state of the art causal methods on real-world datasets? Favourably; see $\S 5.4$ and Table $\S 1$.</li>
<li>Does a proposed model generalize well to unseen interventions? Yes, see $\S 5.5$.</li>
<li>How does the proposed model perform on partial graph recovery? It scales to $\sim 50$ variables; see $\S 5.7$.</li>
</ol>
<h1>2 Preliminaries</h1>
<p>Causal modeling. A Structural Causal Model (SCM) [16] over a finite number $M$ of random variables $X_{i}$ is a set of structural assignments</p>
<p>$$
X_{i}:=f_{i}\left(X_{p a(i, C)}, N_{i}\right), \quad \forall i \in{0, \ldots, M-1}
$$</p>
<p>where $N_{i}$ is jointly-independent noise and $p a(i, C)$ is the set of parents (direct causes) of variable $i$ under hypothesized configuration $C$ of the SCM directed acyclic graph, i.e., $C \in{0,1}^{M \times M}$, with $c_{i j}=1$ if node $i$ has node $j$ as a parent (equivalently, $X_{j} \in X_{p a(i, C)}$; i.e. $X_{j}$ is a direct cause of $X_{i}$ ).</p>
<p>Identifiability. In a purely-observational setting, it is known that causal graphs can be distinguished only up to a Markov equivalence class. In order to identify the true causal graph structure, intervention data is needed [17].</p>
<p>Intervention. There are several types of common interventions which may be available [18]. These are: No intervention: only observational data is obtained from the ground truth model. Hard/perfect: the value of a single or several variables is fixed and then ancestral sampling is performed on the other variables. Soft/imperfect: the conditional distribution of the variable on which the intervention is performed is changed. Uncertain: the learner is not sure of which variable exactly the intervention affected directly. Here we make use of soft intervention because they include hard intervention as a limiting case and hence are more general.</p>
<p>Structure discovery using continuous optimization. Structure discovery is a super-exponential search problem that searches though all possible directed acyclic graphs (DAGs). Previous continuousoptimization structure learning works $[14,15,19]$ mitigate the problem of searching in the superexponential set of graph structures by considering the degree to which a hypothesis graph violates</p>
<p>"DAG-ness" as an additional penalty to be optimized. If there are $M$ such variables, the strategy of considering all the possible structural graphs as separate hypotheses is not feasible because it would require maintaining $O\left(2^{M^{2}}\right)$ models of the data.</p>
<h1>3 Related Work</h1>
<p>The recovery of the underlying structural causal graph from observational and interventional data is a fundamental problem [20, 21, 7]. Different approaches have been studied: score-based, constraintbased, asymmetry-based and continuous optimization methods. Score-based methods search through the space of all possible directed acyclic graphs (DAGs) representing the causal structure based on some form of scoring function for network structures [9, 22, 23, 24, 25, 26, 27]. Constraint-based methods [7, 28, 29, 30, 27] infer the DAG by analyzing conditional independences in the data. Eaton and Murphy [31] use dynamic programming techniques to accelerate Markov Chain Monte Carlo (MCMC) sampling in a Bayesian approach to structure learning for discrete variable DAGs. Asymmetry-based methods [8, 32, 33, 34, 35, 36] assume asymmetry between cause and effect in the data and try to use this information to estimate the causal structure. Peters et al. [37], Ghassami et al. [38], Rojas-Carulla et al. [39] exploit invariance across environments to infer causal structure, which faces difficulty scaling due to the iteration over the super-exponential set of possible graphs. Mooij et al. [11] propose a modelling framework that leverages existing methods while being more powerful and applicable to a wider range of settings. Recently, [14, 15, 19] framed the structure search as a continuous optimization problem, however, the methods only uses observational data and are non-trivial to extend to interventional data. In our paper, we present a method that uses continuous optimization methods that works on both observational and interventional data.
For interventional data, it is often assumed that the models have access to full intervention information, which is rare in the real world. Rothenhäusler et al. [13] have investigated the case of additive shift interventions, while Eaton and Murphy [18] have examined the situation where the targets of experimental interventions are imperfect or uncertain. This is different from our setting where the intervention is unknown to start with and is assumed to arise from other agents and the environment. Bengio et al. [40] propose a meta-learning framework for learning causal models from interventional data. However, the method [40] explicitly models every possible set of parents for every child variable and attempts to distinguish the best amongst the combinatorially many such parent sets. It cannot scale beyond trivial graphs and only 2 variable experiments are presented in the paper.
Learning based methods have been proposed [41, 42, 43] and there also exist recent approaches using the generalization ability of neural networks to learn causal signals from purely observational data [44, 45]. Neural network methods equipped with learned masks, such as [46, 47, 48, 49], exist in the literature, but only a few [44] have been adapted to causal inference. This last work is, however, tailored for causal inference on continuous variables and from observations only. Adapting it to a discrete-variable setting is made difficult by its use of a Generative Adversarial Network (GAN) [50] framework.</p>
<h2>4 Structure Discovery from Interventions Method</h2>
<h3>4.1 Scope of Applicability and Objective</h3>
<p>The proposed method, like any structure learning algorithm, assumes the availability of a datagenerating process based on ancestral sampling of a black-box SCM of $M$ variables, which can be queried for samples. Because the method supports interventions, we further assume that the black-box supports applying and retracting interventions, although they may not be visible outside the black-box. The black-box can support infinite- or finite-data as well as infinite- or finite-intervention regimes.
The objective is, then, to "open up" the black-box, and learn the SCM's concealed edge structure from the glimpses that each intervention affords into the black-box's behaviour.</p>
<h3>4.2 Problem Setting and Assumptions</h3>
<p>In this paper, we restrict the problem setting to specific, but still broad classes of SCMs and interventions. In particular, we assume that:</p>
<ul>
<li>
<p>Data is discrete-valued. The SCM's random variables are all categorical.</p>
</li>
<li>
<p>Data is fully observed. For every sample, the value of all random variables are available.</p>
</li>
<li>Interventions are sparse. They affect only a single random variable (but which one may not be known). This is realistic because a large-scale, coordinated intervention on a broad subset of a realistic system's causal mechanisms is implausible for any single agent.</li>
<li>Interventions are soft. An intervention does not necessarily pin its target random variable to a fixed value (although it may).</li>
<li>Interventions do not stack. Before a new intervention is made, the previous one is fully retracted. This prevents the black-box's behaviour from wandering away from the observational configuration after a long series of interventions.</li>
<li>No control over interventions. The structure learning algorithm has control neither of the target, nor the nature of the next intervention inside the black box.</li>
</ul>
<p>For a detailed description of the interventions, refer to $\S 7.2$.</p>
<h1>4.3 Variations and Prior Knowledge</h1>
<p>In the problem setting above, the black-box is completely opaque. However, there exist two interesting relaxations of the black-box formulation to a more translucent, "gray" box:</p>
<ul>
<li>Complete or partial graph recovery: We may already know the existence of certain causeeffect edges and non-edges within the black-box SCM. If such prior information is available, it may greatly speed up the search for the true causal graph, turning a complete graph recovery problem into one of partial graph recovery.</li>
<li>Known or unknown-target interventions: The black-box might "leak" the target of an intervention. If so, we are not required to predict the target of the intervention.</li>
</ul>
<p>We demonstrate that the proposed method can naturally incorporate this prior information to improve its performance.</p>
<h3>4.4 Method Overview</h3>
<p>The proposed method is a score-based, iterative, continuousoptimization method in three phases that flow into each other (See Figure 2). During the three-phase procedure, a structural representation of a DAG and a functional representation of a set of independent causal mechanisms are trained jointly until convergence. Because the structural and functional parameters are not independent and do influence each other, we train them in alternating phases, a form of block coordinate descent optimization.</p>
<h3>4.4.1 Parametrization</h3>
<p>We distinguish two sets of parameters: The structural parameters $\gamma$ and the functional parameters $\theta$. Given a graph of $M$ variables, we parametrize the structure $\gamma$ as a matrix $\mathbb{R}^{M \times M}$ such that $\sigma\left(\gamma_{i j}\right)$ is our belief in random variable $X_{j}$ being a direct cause of $X_{i}$, where $\sigma(x)=1 /(1+\exp (-x))$ is the sigmoid function. The matrix $\sigma(\gamma)$ is thus a softened adjacency matrix.</p>
<p>The set of all functional parameters $\theta$ comprises the parameters $\theta_{i}$ that model the conditional probability distribution of $X_{i}$ given its parent set $X_{\mathrm{pa}(i, C)}$, with $C \sim \operatorname{Ber}(\sigma(\gamma))$ a hypothesized configuration of the SCM's DAG.</p>
<h3>4.4.2 Phase 1: Graph Fitting on Observational Data</h3>
<p>During Phase 1, the functional parameters $\theta$ are trained to maximize the likelihood of randomly drawn observational data under graphs randomly drawn from our current beliefs about the edge structure. We draw graph configurations $C_{i j} \sim \operatorname{Ber}\left(\sigma\left(\gamma_{i j}\right)\right)$ and batches of observational data from the unintervened black-box SCM, then maximize the log-likelihood of the batch under that configuration using SGD. The use of graph configurations sampling from Bernoulli distributions is</p>
<p>analogous to dropout on the inputs of the functional models (in our implementation, MLPs), giving us an ensemble of neural networks that can model the observational data.</p>
<h1>4.4.3 Phase 2: Graph Scoring on Interventional Data</h1>
<p>During Phase 2, a number of graph configurations are sampled from the current edge beliefs parametrized by $\gamma$, and scored on data samples drawn from the intervened black-box SCM.
Intervention applied: At the beginning of Phase 2, an intervention is applied to the black-box SCM. This intervention is not under the control of the method. In our implementation, and unbeknownst to the model, the target variable is chosen uniformly randomly from all $M$ variables throughout the optimization process.
Intervention predicted: If the target of the intervention is not known, it is predicted using a simple heuristic. A small number of interventional data samples are drawn from the black-box and more graphs are sampled from our current edge beliefs. The average log-likelihood of each individual variable $X_{i}$ across the samples is then computed using the functional model parameters $\theta$ fine-tuned on observational data in Phase 1. The variable $X_{i}$ showing the greatest deterioration in log-likelihood is assumed to be the target because the observational distribution most poorly predicts that variable.
If the target of the intervention leaks from the black-box, then this is taken as ground-truth knowledge for the purpose of subsequent steps, and no prediction is done.
Graphs Sampled and Scored: A new set of interventional data samples and graph configurations are now drawn from the intervened black-box and edge beliefs respectively. The log-likelihood of the data batches under the hypothesized configurations is computed, with one modification: The contribution to the total log-likelihood of a sample $X$ coming from the intervened (or predictedintervened) random variable $X_{i}$ is masked. Because $X_{i}$ was intervened upon (in the manner of a Pearl do-operation, soft or hard), the values one gets for that variable should be taken as givens, not as contributors to the total log-likelihood of the sample, and nor should a correction gradient propagate through it (because the variable's CPT or MLP wasn't actually responsible for the outcome).
Intervention retracted: After Phase 2, the intervention is retracted, per our modelling assumptions.</p>
<h3>4.4.4 Phase 3: Credit Assignment to Structural Parameters</h3>
<p>During Phase 3, the scores of the interventional data batches over various graph configurations are aggregated into a gradient for the structural parameters $\gamma$. Because a discrete Bernoulli random sampling process was used to sample graph configurations under which the log-likelihoods were computed, we require a gradient estimator to propagate gradient through to the $\gamma$ structural parameters. Several alternatives exist, but we adopt for this purpose the REINFORCE-like gradient estimator $g_{i j}$ with respect to $\gamma_{i j}$ proposed by Bengio et al. [40]:</p>
<p>$$
g_{i j}=\frac{\sum_{k}\left(\sigma\left(\gamma_{i j}\right)-c_{i j}^{(k)}\right) \mathcal{L}<em k="k">{C, i}^{(k)}(X)}{\sum</em>, \quad \forall i, j \in{0, \ldots, M-1}
$$} \mathcal{L}_{C, i}^{(k)}(X)</p>
<p>where the ${ }^{(k)}$ superscript indicates the values obtained for the $k$-th draw of $C$ under the current edge beliefs parametrized by $\gamma$. Therefore, $\mathcal{L}<em i="i">{C, i}^{(k)}(X)$ can be read as the log-likelihood of variable $X</em>$, drawn from our edge beliefs. Using the estimated gradient, we then update $\gamma$ with SGD, and return to Phase 1 of the continuous optimization process.
Acyclic Constraint: We include a regularization term $J_{\mathrm{DAG}}(\gamma)$ that penalizes length-2 cycles in the learned adjacency matrix $\sigma(\gamma)$, with a tunable strength $\lambda_{\text {DAG }}$. The regularization term is $J_{\mathrm{DAG}}(\gamma)=$ $\sum_{i \neq j} \cosh \left(\sigma\left(\gamma_{i j}\right) \sigma\left(\gamma_{j i}\right)\right), \quad \forall i, j \in{0, \ldots, M-1}$ and is derived from Zheng et al. [14]. The details of the derivation are in the Appendix. We explore several different values of $\lambda_{\text {DAG }}$ and their effects in our experimental setup. Suppression of longer-length cycles was not found to be worthwhile for the increased computational expense.}$ in the data sample $X$ under the $k$ 'th configuration, $C^{(k)</p>
<h2>5 Experimental Setup and Results</h2>
<p>We first evaluate the proposed method on a synthetic dataset where we have control over the number of variables and causal edges in the ground-truth SCM. This allows us to analyze the performance of the proposed method under various conditions. We then evaluate the proposed method on real-world</p>
<p>datasets from the BnLearn dataset repository. We also consider the two variations of $\S 4.3$ : Recovering only part of the graph (when the rest is known), and exploiting knowledge of the intervention target.</p>
<p>The summary of our findings is: 1) We show strong results for graph recovery for all synthetic graphs in comparisons with other baselines, measured by Hamming distance. 2) The proposed method achieves high accuracy on partial graph recovery for large, real-world graphs. 3) The proposed method's intervention target prediction heuristic closes the gap between the known- and unknown-target intervention scenarios. 4) The proposed method generalizes well to unseen interventions. 5) The proposed method's time-to-solution scaling appears to be driven by the number of edges in the groundtruth graph moreso than the number of variables.</p>
<h1>5.1 Model Description</h1>
<p>Learner model. Without loss of generality, we let $\theta_{i}=\left{\mathrm{WO}<em i="i">{i}, \mathrm{BO}</em>$ can be readily used to mask the inputs of MLP $i$, as shown in Figure 3.}, \mathrm{~W} 1_{i}, \mathrm{~B} 1_{i}\right}$ define a stack of $M$ one-hidden-layer MLPs, one for each random variable $X_{i}$. A more appropriate model, such as a CNN, can be chosen using domainspecific knowledge; the primary advantage of using MLPs is that the hypothesized DAG configurations $c_{i j</p>
<p>To force the structural equation $f_{i}$ corresponding to $X_{i}$ to rely exclusively on its direct ancestor set $\mathrm{pa}(i, C)$ under hypothesis adjacency matrix $C$ (See Eqn. 1), the one-hot input vector $X_{j}$ for variable $X_{i}$ 's MLP is masked by the Boolean element $c_{i j}$. An example of the multi-MLP architecture with $M=3$ categorical variables of $N=2$ categories is shown in Figure 3. For more details, refer to Appendix 7.4.
Ground-truth model. Ground-truth SCM models are parametrized either as CPTs with parameters from BnLearn (in the case of real-world graphs), or as a second stack of MLPs similar to the learner model, with randomly-initialized functional parameters $\theta_{\mathrm{GT}}$ and the desired adjacency matrix $\gamma_{\mathrm{GT}}$.
Interventions. In all experiments, at most one (soft) intervention is concurrently performed. To simulate a soft intervention on variable $X_{i}$, we reinitialize its ground-truth conditional distribution's MLP parameters or CPT table randomly, while leaving the other variables untouched. For more details about the interventions, please refer to Appendix 7.2.</p>
<h3>5.2 Synthetic Datasets Experiments</h3>
<p>We first evaluate the model's performance on several randomly-initialized SCMs with specific, representative graph structures. Since the number of possible DAGs grows super-exponentially with the number of variables, for $M=4$ up to 13 a selection of representative and edgecase DAGs are chosen. chainM and fullM ( $M=3-13$ ) are the minimally- and maximallyconnected $M$-variable DAGs, while treeM and jungleM are tree-like intermediate graphs. colliderM is the $(M-1) \rightarrow 1$ collider graph. The details of the setup is in Appendix 7.6.</p>
<p>Results. The model can recover most synthetic DAGs with high accuracy, as measured by Structural Hamming Distance (SHD) between learned
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Cross entropy (CE) and Area-Under-Curve (AUC/AUROC) for edge probabilities of learned graph against ground-truth for synthetic SCMs. Error bars represent $\pm 1 \sigma$ over PRNG seeds 1-5. Left to right: chainM, jungleM, fullM, $M=3 \ldots 13$. Graphs (3-13 variables) all learn perfectly with AUROC reaching 1.0. However, denser graphs (fullM) take longer to converge.
and ground-truth DAGs. Table 1 shows our proposed method outperforming all other baseline methods, and learns all graphs perfectly for 3 to 13 variables (excepting full). For DAGs ranging</p>
<p>from 3 to 8 variables, the AUROCs all eventually reach 1.0 (indicating perfect classification into edge/non-edge; Refer to Figure 4). For both large $(M&gt;10)$ and dense DAGs (e.g. full13) the model begins encountering difficulties, as shown in Table 1 and Appendix $\S$ 7.6.1.</p>
<p>Small graphs ( $M&lt;10$ ) are less sensitive than larger ones to our hyperparameters, notably the sparsity and acyclic regularization ( $\S 4.4 .4$ ) terms. In $\S 7.5$, we perform an analysis of these hyperparameters.</p>
<p>Table 1: Baseline comparisons: Structural Hamming Distance (SHD) (lower is better) for learned and ground-truth edges on various graphs from both synthetic and real datasets, compared to [37], [51], [18], [15] and [14]. The proposed method (Structural Discovery from Interventions (SDI)) is run on random seeds $1-5$ and we pick the worst performing model out of the random seeds in the table. OOM: out of memory. Our proposed method correctly recovers the true causal graph, with the exception of Sachs and full13, and it significantly outperforms all other baseline methods.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: right;">Asia</th>
<th style="text-align: right;">Sachs</th>
<th style="text-align: right;">collider</th>
<th style="text-align: right;">chain</th>
<th style="text-align: right;">jungle</th>
<th style="text-align: right;">collider</th>
<th style="text-align: right;">full</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$M$</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">13</td>
</tr>
<tr>
<td style="text-align: left;">Zheng et al. [14]</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">22</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">39</td>
<td style="text-align: right;">22</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">71</td>
</tr>
<tr>
<td style="text-align: left;">Yu et al. [15]</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">19</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">77</td>
</tr>
<tr>
<td style="text-align: left;">Heinze-Deml et al. [51]</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">28</td>
</tr>
<tr>
<td style="text-align: left;">Peters et al. [37]</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">16</td>
</tr>
<tr>
<td style="text-align: left;">Eaton and Murphy [10]</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">OOM</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">OOM</td>
<td style="text-align: right;">OOM</td>
<td style="text-align: right;">OOM</td>
<td style="text-align: right;">OOM</td>
</tr>
<tr>
<td style="text-align: left;">Proposed Method (SDI)</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">7</td>
</tr>
</tbody>
</table>
<h1>5.3 Real-World Datasets: BnLearn</h1>
<p>The Bayesian Network Repository is a collection of commonly-used causal Bayesian networks from the literature, suitable for Bayesian and causal learning benchmarks. We evaluate the proposed method on the Earthquake [6], Cancer [6], Asia [5] and Sachs [2] datasets ( $M=5,5,8$ and 11-variables respectively, maximum in-degree 3) in the BnLearn dataset repository.</p>
<p>Results. As shown in Table 1, the proposed method perfectly recovers the DAG of Asia, while making a small number of errors (SHD=6) for Sachs (11-variables). It thus significantly outperforms all other baselines models. Figures $8 \&amp; 9$ visualize what the model has learned at several stages of learning. Results for Cancer and Asia can be found in the appendices, Figure 16 and 17.</p>
<h3>5.4 Comparisons with other methods</h3>
<p>As shown in Table 1, we compared the proposed SDI method to ICP ([37]), non-linear ICP ([51]), and $[18,14,15]$ on Asia [5], Sachs [2] and representative synthetic graphs. Eaton and Murphy [18] handles uncertain interventions and Peters et al. [37], Heinze-Deml et al. [51] handles unknown interventions. However, neither attempts to predict the intervention. As shown in Table 1, we significantly outperform ICP, non-linear ICP, and the methods in [15] and [14]. Furthermore, Eaton and Murphy [18] runs out of memory for graphs larger than $M=10$ because modelling of uncertain interventions is done using "shadow" random variables (as suggested by the authors), and thus recovering the DAG internally requires solving a $d=2 M$-variable problem. Their method's extremely poor time- and space-scaling of $O\left(d 2^{d}\right)$ makes it unusable beyond $d&gt;20$.
For SDIs, we threshold our edge beliefs at $\sigma(\gamma)=0.5$ to derive a graph, but the continued decrease of the cross-entropy loss (Figure 4) hints at SDI's convergence onto the correct causal model. Please refer to Appendix $\S 7.8$ for full details and results.</p>
<h3>5.5 Generalization to Previously Unseen Interventions</h3>
<p>It is often argued that machine learning approaches based purely on capturing joint distributions do not necessarily yield models that generalize to unseen experiments, since they do not explicitly model changes through</p>
<p>Table 2: Evaluating the consequences of a previously unseen intervention: (test log-likelihood under intervention)</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">fork3</th>
<th style="text-align: center;">chain3</th>
<th style="text-align: center;">confounder3</th>
<th style="text-align: center;">collider3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">-0.5036</td>
<td style="text-align: center;">-0.4562</td>
<td style="text-align: center;">-0.3628</td>
<td style="text-align: center;">-0.5082</td>
</tr>
<tr>
<td style="text-align: center;">SDI</td>
<td style="text-align: center;">-0.4502</td>
<td style="text-align: center;">-0.3801</td>
<td style="text-align: center;">-0.2819</td>
<td style="text-align: center;">-0.4677</td>
</tr>
</tbody>
</table>
<p>interventions. By way of contrast, causal models use the concept of interventions to explicitly model changing environments and thus hold the promise of robustness even under distributional shifts [21, 52, 16]. To test the robustness of causal modelling to previously unseen interventions (new values for an intervened variable), we evaluate a well-trained causal model against a variant, non-causal model trained with $c_{i j}=1, i \neq j$.</p>
<p>An intervention is performed on the black-box SCM, fresh interventional data is drawn from it, and the models, with knowledge of the intervention target, are asked to predict the other variables given their parents. The average log-likelihoods of the data under both models are computed and contrasted. The intervention variable's contribution to the log-likelihood is masked. For all 3-variable graphs (chain3, fork3, collider3, confounder3), the causal model attributes higher log-likelihood to the intervention distribution's samples than the non-causal variant, thereby demonstrating causal models' superior generalization ability in transfer tasks. Table 2 collects these results.</p>
<h1>5.6 Variant: Predicting interventions</h1>
<p>In Phase 2 (\$4.4.3), we use a simple heuristic to predict the intervention target variable. Experiments show that this heuristic functions well in practice, yielding correct predictions far more often than by chance alone (Table 3). Guessing the intervention variable randomly, or not guessing it at all, leads to a significant drop in the model performance, even for 3-variable graphs (Figure 11 Left). Training SDI with intervention prediction closely tracks training with leaked knowledge of the ground-truth intervention on larger, 7-variable graphs (Figure 11 Right).</p>
<h3>5.7 Variant: Partial Graph Recovery</h3>
<p>Instead of learning causal structures de novo, we may have partial information about the black-box SCM and may only need to fill in missing information (\$4.3). An example is protein structure discovery in</p>
<p>Table 3: Intervention Prediction Accuracy: (identify on which variable the intervention took place)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">3 variables</th>
<th style="text-align: left;">4 variables</th>
<th style="text-align: left;">5 variables</th>
<th style="text-align: left;">8 variables</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$95 \%$</td>
<td style="text-align: left;">$93 \%$</td>
<td style="text-align: left;">$85 \%$</td>
<td style="text-align: left;">$71 \%$</td>
</tr>
</tbody>
</table>
<p>biology, where some causal relationships have been definitely established and others remain open hypotheses. This is an easier task compared to full graph recovery, since the model only has to search for missing edges. We evaluate the proposed method on Barley [54] $(M=48)$ and Alarm [53] $(M=37)$ from the BnLearn repository. The model is asked to predict 50 edges from Barley and 40 edges from Alarm. The model reached $\geq 90 \%$ accuracy on both datasets, as shown in Table 4.</p>
<h3>5.8 Ablation and analysis</h3>
<p>As shown in Figure 12, larger graphs (such as $M&gt;6$ ) and denser graphs (such as full8) are progressively more difficult to learn. For denser graphs, the learned models have higher sample complexity, higher variance and slightly worse results. Refer to Appendix $\S 7.9$ for complete results on all graphs.
Hyperparameters. Hyperparameters for all experiments were kept identical unless otherwise stated. We study the effect of DAG and sparsity penalties in the following paragraph. For more details on hyperparameter setup, please refer to Appendix $\S 7.5$.</p>
<p>Table 4: Partial Graph Recovery on Alarm [53] and Barley [54]. The model is asked to predict 50 edges in Barley and 40 edges in Alarm. The accuracy is measured in Structural Hamming Distance (SHD). SDI achieved over $90 \%$ accuracy on both graphs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Graph</th>
<th style="text-align: center;">Alarm</th>
<th style="text-align: center;">Barley</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Number of variables</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">48</td>
</tr>
<tr>
<td style="text-align: left;">Total Edges</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">84</td>
</tr>
<tr>
<td style="text-align: left;">Edges to recover</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">Recovered Edges</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">45</td>
</tr>
<tr>
<td style="text-align: left;">Errors (in SHD)</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5</td>
</tr>
</tbody>
</table>
<p>Importance of regularization. Valid configurations $C$ for a causal model are expected to be a) sparse and b) acyclic. To promote such solutions, we introduce DAG and sparsity regularization with tunable hyperparameters. For all experiments, we set the DAG penalty to 0.5 and sparsity penalty to 0.1 . We run ablation studies on different values of the regularizers and study their effect. We find that smaller graphs are less sensitive to different values of regularizer than larger graphs. For details, refer to Appendix $\S 7.13$.</p>
<p>Importance of dropout. To train functional parameter for an observational distribution, sampling adjacency matrices is required. We "drop out" each edge (with a probability of $\sigma(\gamma)$ ) in our experiments during functional parameter training of the conditional distributions of the SCM. Please refer to Appendix $\S 7.14$ for a more detailed analysis.</p>
<h2>6 Conclusion</h2>
<p>In this work, we introduced an experimentally successful method (SDI) for causal structure discovery using continuous optimization, combining information from both observational and interventional</p>
<p>data. We show in experiments that it can recover true causal structure, that it generalizes well to unseen interventions, that it compares very well against the start-of-the-art causal discovery methods on real world datasets, and that it scales even better on problems where only part of the graph is known.</p>
<h1>Acknowledgements</h1>
<p>The authors would like to acknowledge the support of the following agencies for research funding and computing support: NSERC, Compute Canada, the Canada Research Chairs, CIFAR, and Samsung. We would also like to thank the developers of Pytorch for developments of great frameworks. We wish to thank Sébastien Lachapelle for referring us to [14] and help to extend the acyclic regularization term $J_{D A G}$ to the current from. We would like to thank Lars Buesing, Bernhard Schölkopf, Nasim Rahaman, Jovana Mitrović and Rémi Le Priol for useful feedback and discussions.</p>
<h2>References</h2>
<p>[1] Judea Pearl and Dana Mackenzie. The book of why: the new science of cause and effect. Basic Books, 2018.
[2] Karen Sachs, Omar Perez, Dana Pe'er, Douglas A Lauffenburger, and Garry P Nolan. Causal protein-signaling networks derived from multiparameter single-cell data. Science, 308(5721): $523-529,2005$.
[3] Steven M Hill, Laura M Heiser, Thomas Cokelaer, Michael Unger, Nicole K Nesser, Daniel E Carlin, Yang Zhang, Artem Sokolov, Evan O Paull, Chris K Wong, et al. Inferring causal molecular networks: empirical assessment through a community-based effort. Nature methods, 13(4):310-318, 2016.
[4] Bruce Abramson, John Brown, Ward Edwards, Allan Murphy, and Robert L Winkler. Hailfinder: A bayesian system for forecasting severe weather. International Journal of Forecasting, 12(1): $57-71,1996$.
[5] Steffen L Lauritzen and David J Spiegelhalter. Local computations with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society: Series B (Methodological), 50(2):157-194, 1988.
[6] Kevin B Korb and Ann E Nicholson. Bayesian artificial intelligence. CRC press, 2010.
[7] Peter Spirtes, Clark N Glymour, Richard Scheines, David Heckerman, Christopher Meek, Gregory Cooper, and Thomas Richardson. Causation, prediction, and search. MIT press, 2000.
[8] Shohei Shimizu, Patrik O Hoyer, Aapo Hyvärinen, and Antti Kerminen. A linear non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(Oct):2003-2030, 2006.
[9] David Heckerman, Dan Geiger, and David M Chickering. Learning bayesian networks: The combination of knowledge and statistical data. Machine learning, 20(3):197-243, 1995.
[10] Daniel Eaton and Kevin Murphy. Belief net structure learning from uncertain interventions. J Mach Learn Res, 1:1-48, 2007.
[11] Joris M Mooij, Sara Magliacane, and Tom Claassen. Joint causal inference from multiple contexts. arXiv preprint arXiv:1611.10351, 2016.
[12] Robert Tillman and Peter Spirtes. Learning equivalence classes of acyclic models with latent and selection variables from multiple datasets with overlapping variables. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 3-15, 2011.
[13] Dominik Rothenhäusler, Christina Heinze, Jonas Peters, and Nicolai Meinshausen. Backshift: Learning causal cyclic graphs from unknown shift interventions. In Advances in Neural Information Processing Systems, pages 1513-1521, 2015.
[14] Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. DAGs with NO TEARS: Continuous optimization for structure learning. In Advances in Neural Information Processing Systems, pages 9472-9483, 2018.
[15] Yue Yu, Jie Chen, Tian Gao, and Mo Yu. Dag-gnn: Dag structure learning with graph neural networks. arXiv preprint arXiv:1904.10098, 2019.</p>
<p>[16] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. MIT press, 2017.
[17] Frederick Eberhardt, Clark Glymour, and Richard Scheines. On the number of experiments sufficient and in the worst case necessary to identify all causal relations among n variables. arXiv preprint arXiv:1207.1389, 2012.
[18] Daniel Eaton and Kevin Murphy. Exact bayesian structure learning from uncertain interventions. In Artificial Intelligence and Statistics, pages 107-114, 2007.
[19] Sébastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradientbased neural dag learning. arXiv preprint arXiv:1906.02226, 2019.
[20] Judea Pearl. Causal diagrams for empirical research. Biometrika, 82(4):669-688, 1995.
[21] Judea Pearl. Causality. Cambridge university press, 2009.
[22] David Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine learning research, 3(Nov):507-554, 2002.
[23] Ioannis Tsamardinos, Laura E Brown, and Constantin F Aliferis. The max-min hill-climbing bayesian network structure learning algorithm. Machine learning, 65(1):31-78, 2006.
[24] Alain Hauser and Peter Bühlmann. Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs. Journal of Machine Learning Research, 13(Aug):2409-2464, 2012.
[25] Olivier Goudet, Diviyan Kalainathan, Philippe Caillou, Isabelle Guyon, David Lopez-Paz, and Michèle Sebag. Causal generative neural networks. arXiv preprint arXiv:1711.08936, 2017.
[26] Gregory F. Cooper and Changwon Yoo. Causal Discovery from a Mixture of Experimental and Observational Data. In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI'99, pages 116-125, San Francisco, CA, USA, 1999.
[27] Shengyu Zhu and Zhitang Chen. Causal discovery with reinforcement learning. arXiv preprint arXiv:1906.04477, 2019.
[28] Xiaohai Sun, Dominik Janzing, Bernhard Schölkopf, and Kenji Fukumizu. A kernel-based causal learning algorithm. In Proceedings of the 24th international conference on Machine learning, pages 855-862. ACM, 2007.
[29] Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Kernel-based conditional independence test and application in causal discovery. arXiv preprint arXiv:1202.3775, 2012.
[30] Ricardo Pio Monti, Kun Zhang, and Aapo Hyvarinen. Causal discovery with general non-linear relationships using non-linear ica. arXiv preprint arXiv:1904.09096, 2019.
[31] Daniel Eaton and Kevin Murphy. Bayesian structure learning using dynamic programming and MCMC. In Uncertainty in Artificial Intelligence, pages 101-108, 2007.
[32] Patrik O Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Schölkopf. Nonlinear causal discovery with additive noise models. In Advances in neural information processing systems, pages 689-696, 2009.
[33] J. Peters, J. M. Mooij, D. Janzing, and B. Schölkopf. Identifiability of causal graphs using functional models. In Proceedings of the 27th Annual Conference on Uncertainty in Artificial Intelligence (UAI), pages 589-598, 2011.
[34] Povilas Daniusis, Dominik Janzing, Joris Mooij, Jakob Zscheischler, Bastian Steudel, Kun Zhang, and Bernhard Schölkopf. Inferring deterministic causal relations. arXiv preprint arXiv:1203.3475, 2012.
[35] Kailash Budhathoki and Jilles Vreeken. Causal inference by stochastic complexity. arXiv:1702.06776, 2017.
[36] Jovana Mitrovic, Dino Sejdinovic, and Yee Whye Teh. Causal inference via kernel deviance measures. In Advances in Neural Information Processing Systems, pages 6986-6994, 2018.
[37] Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947-1012, 2016.</p>
<p>[38] AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, and Kun Zhang. Learning causal structures using regression invariance. In Advances in Neural Information Processing Systems, pages 3011-3021, 2017.
[39] Mateo Rojas-Carulla, Bernhard Schölkopf, Richard Turner, and Jonas Peters. Invariant models for causal transfer learning. The Journal of Machine Learning Research, 19(1):1309-1342, 2018.
[40] Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, Sébastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle causal mechanisms. arXiv preprint arXiv:1901.10912, 2019.
[41] Isabelle Guyon. Cause-effect pairs kaggle competition, 2013. URL https://www. kaggle. com/c/cause-effect-pairs, page 165, 2013.
[42] Isabelle Guyon. Chalearn fast causation coefficient challenge, 2014. URL https://www. codalab. org/competitions/1381, page 165, 2014.
[43] David Lopez-Paz, Krikamol Muandet, Bernhard Schölkopf, and Iliya Tolstikhin. Towards a learning theory of cause-effect inference. In International Conference on Machine Learning, pages 1452-1461, 2015.
[44] Diviyan Kalainathan, Olivier Goudet, Isabelle Guyon, David Lopez-Paz, and Michèle Sebag. Sam: Structural agnostic model, causal discovery and penalized adversarial learning. arXiv preprint arXiv:1803.04929, 2018.
[45] Olivier Goudet, Diviyan Kalainathan, Philippe Caillou, Isabelle Guyon, David Lopez-Paz, and Michele Sebag. Learning functional causal models with generative neural networks. In Explainable and Interpretable Models in Computer Vision and Machine Learning, pages 39-80. Springer, 2018.
[46] Oleg Ivanov, Michael Figurnov, and Dmitry Vetrov. Variational autoencoder with arbitrary conditioning. arXiv preprint arXiv:1806.02382, 2018.
[47] Yang Li, Shoaib Akbar, and Junier B Oliva. Flow models for arbitrary conditional likelihoods. arXiv preprint arXiv=1909.06319, 2019.
[48] Jinsung Yoon, James Jordon, and Mihaela Van Der Schaar. Gain: Missing data imputation using generative adversarial nets. arXiv preprint arXiv:1806.02920, 2018.
[49] Laura Douglas, Iliyan Zarov, Konstantinos Gourgoulias, Chris Lucas, Chris Hart, Adam Baker, Maneesh Sahani, Yura Perov, and Saurabh Johri. A universal marginalizer for amortized inference in generative models. arXiv preprint arXiv:1711.00695, 2017.
[50] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672-2680, 2014.
[51] Christina Heinze-Deml, Jonas Peters, and Nicolai Meinshausen. Invariant causal prediction for nonlinear models. Journal of Causal Inference, 6(2), 2018.
[52] Bernhard Schölkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij. On causal and anticausal learning. In J. Langford and J. Pineau, editors, Proceedings of the 29th International Conference on Machine Learning (ICML), pages 1255-1262, New York, NY, USA, 2012. Omnipress.
[53] Ingo A Beinlich, Henri Jacques Suermondt, R Martin Chavez, and Gregory F Cooper. The alarm monitoring system: A case study with two probabilistic inference techniques for belief networks. In AIME 89, pages 247-256. Springer, 1989.
[54] Kristian Kristensen and Ilse A Rasmussen. The use of a bayesian network in the design of a decision support system for growing malting barley without use of pesticides. Computers and Electronics in Agriculture, 33(3):197-217, 2002.
[55] Christina Heinze-Deml, Marloes H Maathuis, and Nicolai Meinshausen. Causal structure learning. Annual Review of Statistics and Its Application, 5:371-391, 2018.
[56] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<h1>Algorithm 1 Training Algorithm</h1>
<p>1: procedure Training(SCM Black Box Distribution $D$, with $M$ nodes and $N$ categories)
2: Let $i$ an index from 0 to $M-1$
3: for $I$ iterations, or until convergence, do
4: for $F$ functional parameter training steps do
5: $\quad X \sim D$
6: $\quad C \sim \operatorname{Ber}(\sigma(\gamma))$
7: $\quad L=-\log P(X \mid C ; \theta)$
8: $\quad \theta_{t+1} \leftarrow \operatorname{Adam}\left(\theta_{t}, \nabla_{\theta} L\right)$
9: for $Q$ interventions do
10: $\quad \mathrm{I} _\mathrm{N} \leftarrow \operatorname{randint}(0, M-1)$
$D_{\text {int }}:=D$ with intervention on node $\mathrm{I} _\mathrm{N}$
12: if predicting intervention then
$L_{i} \leftarrow 0 \quad \forall i$
14: for $N_{P}$ prediction steps do
15: $\quad X \sim D_{\text {int }}$
16: for $C_{P}$ configurations do
17: $\quad C \sim \operatorname{Ber}(\sigma(\gamma))$
18: $\quad L_{i} \leftarrow L_{i}-\log P_{i}\left(X \mid C_{i} ; \theta_{\text {slow }}\right) \forall i$
19: $\quad \mathrm{I} _\mathrm{N} \leftarrow \operatorname{argmax}\left(L_{i}\right)$
20: gammagrads, logregrets = [], []
21: for $N_{S}$ scoring steps do
22: $\quad X \sim D_{\text {int }}$
23: gammagrad, logregret $=0,0$
24: for $C_{S}$ configurations do
25: $\quad C \sim \operatorname{Ber}(\sigma(\gamma))$
26: $\quad L_{i}=-\log P_{i}\left(X \mid C_{i} ; \theta_{\text {slow }}\right) \quad \forall i$
27: gammagrad $<em>=\sigma(\gamma)-C$
28: $\quad$ logregret $</em>=\sum_{i \neq L_{i} S} L_{i}$
gammagrads.append(gammagrad)
logregrets.append(logregret)
29: $\quad g_{i j}=\frac{\sum_{k}\left(\sigma\left(\gamma_{i j}\right)-c_{i j}^{(k)}\right) \mathcal{L}<em k="k">{C, i}^{(k)}(X)}{\sum</em>} \mathcal{L<em _gamma="\gamma">{C, i}^{(k)}(X)}$
30: $\quad g \leftarrow g+\nabla</em>(\gamma)\right)$
$\gamma_{t+1} \leftarrow \operatorname{Adam}\left(\gamma_{t}, g\right)$
$\triangleright$ Phase 2 Scoring}\left(\lambda_{\text {sparse }} L_{\text {sparse }}(\gamma)+\lambda_{\text {DAG }} L_{\text {DAG }</p>
<h2>7 Annexes</h2>
<h3>7.1 Training Algorithm</h3>
<p>Algorithm 1 shows the pseudocode of the method described in $\S 4$. Typical values for the loop trip counts are found in $\S 7.11$.</p>
<h3>7.2 Preliminaries</h3>
<p>Interventions. In a purely-observational setting, it is known that causal graphs can be distinguished only up to a Markov equivalence class. In order to identify the true causal graph intervention data is needed [17]. Several types of common interventions may be available [18]. These are: No intervention: only observational data is obtained from the ground truth causal model. Hard/perfect: the value of a single or several variables is fixed and then ancestral sampling is performed on the other variables. Soft/imperfect: the conditional distribution of the variable on which the intervention is performed is changed. Uncertain: the learner is not sure of which variable exactly the intervention affected directly. Here we make use of soft interventions for several reasons: First, they include hard</p>
<p>interventions as a limiting case and hence are more general. Second, in many real-world scenarios, it is more difficult to perform a hard intervention compared to a soft one. We also deal with a special case of uncertain interventions, where the variable selected for intervention is random and unknown. We call these unidentified or unknown interventions.</p>
<p>Intervention setup. For our experiments, the groundtruth models of the synthetic datasets are modeled by neural networks as described in section 7.6. Each neural network models the relationship of the causal parents and a variable. We perform our intervention by first randomly selecting which variable to intervene on, then soft-intervening on it. The selected variable is sampled from a uniform distribution. The soft intervention is a reinitialization of its neural network's parameters.</p>
<p>Causal sufficiency. The inability to distinguish which causal graph, within a Markov equivalence class, is the correct one in the purely-observational setting is called the identifiability problem. In our setting, all variables are observed (there are no latent confounders) and all interventions are random and independent. Hence, within our setting, if the interventions are known, then the true causal graph is always identifiable in principle [17, 55]. We also consider here situations where a single variable is randomly selected and intervened upon with a soft or imprecise intervention, its identity is unknown and must be inferred. In this case, there is no theoretical guarantee that the causal graph is identifiable. However, there is existing work Peters et al. [37] that handles this scenario and the proposed method is also proven to work empirically.</p>
<p>Faithfulness. It is possible for causally-related variables to be probabilitistically independent purely by happenstance, such as when causal effects along multiple paths cancel out. This is called unfaithfulness. We assume that faithfulness holds, since the $\gamma$ gradient estimate is extracted from shifts in probability distributions. However, because of the "soft" nature of our interventions and their infinite variety, it would be exceedingly unlikely for cancellation-related unfaithfulness to persist throughout the causal-learning procedure.</p>
<h1>7.3 Experimental setup</h1>
<p>For all datasets, the weight parameters for the learned model is initialized randomly. In order to not bias the structural parameters, all $\sigma(\gamma)$ are initialized to 0.5 in the beginning of training. Details of hyperparameters of the learner model are described in Section 7.5. The experimental setup for the groundtruth model for the synthetic data can be found in Section 7.6 and the details for the real world data are described in Section 7.7.</p>
<h3>7.4 Model setup</h3>
<p>As discussed in section 4, we model the $M$ variables in the graph using $M$ independent MLPs, each possesses an input layer of $M \times N$ neurons (for $M$ one-hot vectors of length $N$ each), a single hidden layer chosen arbitrarily to have $\max (4 M, 4 N)$ neurons with a LeakyReLU activation of slope 0.1 , and a linear output layer of $N$ neurons representing the unnormalized log-probabilities of each category (a softmax then recovers the conditional probabilities from these logits). To force $f_{i}$ to rely exclusively on the direct ancestor set $p a(i, C)$ under adjacency matrix $C$ (See Eqn. 2), the one-hot input vector $X_{j}$ for variable $X_{i}$ 's MLP is masked by the Boolean element $c_{i j}$. The functional parameters of the MLP are the set $\theta=\left{\mathrm{WO}<em h="h" i="i">{i h j n}, \mathrm{BO}</em>\right}$. An example of the multi-MLP architecture with $M=3$ categorical variables of $N=2$ categories is shown in Figure 3.}, \mathrm{~W} 1_{i n h}, \mathrm{BI}_{i n</p>
<h3>7.5 Hyperparameters</h3>
<p>Learner model. All experiments on the synthetic graphs of size 3-8 use the same hyperparameters. Both the functional and structural parameters are optimized using the Adam optimizer [56]. We use a learning rate of $5 e-2$ with alpha of 0.9 for the functional parameters, and we use a learning rate of $5 e-3$ with alpha of 0.1 for the structural parameters. We perform 5 runs of each experiment with random seeds $1-5$ and error bars are plotted for various graphs from size 3 to 8 in Figure 4. We use a batch size of 256 . The L1 norm regularizer is set to 0.1 and the $D A G$ regularizer is set to 0.5 for all experiments. For each $\gamma$ update step, we sample 25 structural configurations from the current $\gamma$. In all experiments, we use 100 batches from the interventional distribution to predict the intervened node.</p>
<h1>7.6 Synthetic data</h1>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Every possible 3-variable connected DAG.</p>
<p>Synthetic datasets. The synthetic datasets in the paper are modeled by neural networks. All neural networks are 2 layered feed forward neural networks (MLPs) with Leaky ReLU activations between layers. The parameters of the neural network are initialized orthogonally within the range of $(-2.5,2.5)$. This range was selected such that they output a non-trivial distribution. The biases are initialized uniformly between $(-1.1,1.1)$.
SCM with $n$ variables are modeled by $n$ feedforward neural networks (MLPs) as described in $\S 5.1$. We assume an acyclic causal graph so that we may easily sample from them. Hence, given any pair of random variables $A$ and $B$, either $A \rightarrow B, B \rightarrow A$ or $A$ and $B$ are independent.
The MLP representing the ground-truth SCM has its weights $\theta$ initialized use orthogonal initialization with gain 2.5 and the biases are initialized using a uniform initialization between -1.1 and 1.1, which was empirically found to yield "interesting" yet learnable random SCMs.
We study a variety of SCMs with different ground-truth edge structures $\gamma$. Our selection of synthetic graphs explores various extremes in the space of DAGs, stress-testing SDI. The chain graphs are the sparsest connected graphs possible, and are relatively easy to learn. The bidiag graphs are extensions of chain where there are 2-hops as well as single hops between nodes, doubling the number of edges and creating a meshed chain of forks and colliders. The jungle graphs are binary-tree-like graphs, but with each node connected directly to its grandparent in the tree as well. Half the nodes in a jungle graph are leaves, and the out-degree is up to 6 . The collider graphs deliberately collide independent $M-1$ ancestors into the last node; They stress maximum in-degree. Lastly, the full graphs are the maximally dense DAGs. All nodes are direct parents of all nodes below them in the topological order. The maximum in- and out-degree are both $M-1$. These graphs are depicted in Figure 6 .</p>
<h3>7.6.1 Synthetic data results</h3>
<p>The model can recover correctly all synthetic graphs with 10 variables or less, as shown in Figure 10 and Table 1. For graphs larger than 10 variables, the model found it more challenging to recover the denser graphs (e.g. fullM), as shown in Table 1. Plots of the training curves showing average cross entropy (CE) and Area-Under-Curve(AUC/AUCROC) for edge probabilities of the learned graph against the ground-truth graph for synthetic SCMs with 3-13 variables are available in Figure 10.</p>
<h3>7.7 BnLearn data repository</h3>
<p>The repo contains many datasets with various sizes and structures modeling different variables. We evaluate the proposed method on 3 of the datasets in the repo, namely the Earthquake [6], Cancer [6] and Asia [5] datasets. The ground-truth model structure for the Cancer [6] and Earthquake [6] datasets are shown in Figure 7. Note that even though the structure for the 2 datasets seems to be the same, the conditional probability tables (CPTs) for these 2 datasets are very different and hence results in different structured causal models (SCMs) for the 2 datasets.
Given that some of the CPTs contain very unlikely events, we have found it necessary to add a temperature parameter in order to make them more frequent. The near-ground-truth MLP model's logit outputs are divided by the temperature before being used for sampling. Temperatures above 1 result in more uniform distributions for all causal variables; Temperatures below 1 result in less uniform, sharper distributions that peak around the most likely value. We find empirically that a temperaure of about 2 is required for our BnLearn benchmarks.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: Figures for various synthetic graphs. chain, collider, bidiagonal, full and jungle graph.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Left to right: Ground Truth SCM for Cancer, Groundtruth SCM for Earthquake, Groundtruth SCM for Asia.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: Learned edges at three different stages of training. Left: chain4 (chain graph with 4 variables). Right: full4 (tournament graph with 4 variables).</p>
<h1>7.8 Comparisons to other methods</h1>
<p>As described in section 5.4, we compare to 5 other methods. The full comparison between SDIs and other methods on various graphs can be found in Table 1.
One of these methods, DAG-GNN Yu et al. [15], outputs 3 graphs based on different criteria: best mean square error (MSE), best negative loglikelihood (NLL) and best evidence lower bound (ELBO). We report performance of all outputs of DAG-GNN Yu et al. [15] in Table 6, and the best one is selected for Table 1.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: right;">Asia</th>
<th style="text-align: right;">chain8</th>
<th style="text-align: right;">jungle8</th>
<th style="text-align: right;">collider7</th>
<th style="text-align: right;">collider8</th>
<th style="text-align: right;">full8</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">[14]</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">21</td>
</tr>
<tr>
<td style="text-align: left;">[15]</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">25</td>
</tr>
<tr>
<td style="text-align: left;">[51]</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">28</td>
</tr>
<tr>
<td style="text-align: left;">[37]</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">16</td>
</tr>
<tr>
<td style="text-align: left;">[10]</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;">SDIs</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
<p>Table 5: Baseline comparisons: Hamming distance (lower is better) for learned and ground-truth edges on various graphs from both synthetic and real datasets, compared to [37], [51], [18], [15] and [14]. The proposed SDI is run on random seeds $1-5$ and we pick the worst performing model out of the random seeds in the table.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 9: Left: Earthquake: Learned edges at three different stages of training. Right: Asia: Learned edges at three different stages of training.</p>
<h1>7.9 Sparsity of Ground-Truth Graph</h1>
<p>We evaluated the performance of SDI on graphs of various size and sparsity to better understand the performance of the model. We evaluated the proposed model on 4 representative types of graphs in increasing order of density. They are the chain, jungle, bidiag and full graphs. As shown in the results in figure 12, for graphs of size 5 or smaller, there is almost no difference in the final results in terms of variance and sample complexity. However, as the graphs gets larger (than 6), the denser graphs (full graphs) gets progressively more difficult to learn compared to the sparser graphs (chain, jungle and bidiag). The models learned for denser graphs have higher complexity, higher variance and slightly worse results.</p>
<h3>7.10 Predicting interventions</h3>
<p>In Phase 2, we score graph configurations based on how well they fit the interventional data. We find that it is necessary to avoid disturbing the learned parameters of intervened variables, and to ignore its contribution to the total negative log-likelihood of the sample. Intuitively, this is because, having been intervened upon, that variable should be taken as a given. It should especially not be interpreted as a poorly-learned variable requiring a tuning of its functional parameters, because those functional parameters were not responsible for the value of that variable; The extrinsic intervention was.
Since an intervened variable is likely to be unusually poorly predicted, we heuristically determine that the most poorly predicted variable is the intervention variable. We then zero out its contribution to the log-likelihood of the sample and block gradient into its functional parameters.
Figure 11 illustrates the necessity of this process. When using the prediction heuristic, the training curve closely tracks training with ground-truth knowledge of the identity of the intervention. If no prediction is made, or a random prediction is made, training proceeds much more slowly, or fails entirely.</p>
<h3>7.11 Sample complexity</h3>
<p>Our method is heavily reliant on sampling of configurations and data in Phases 1 and 2. We present here the breakdown of the sample complexity. Let</p>
<ul>
<li>$I$ be the number of iterations of the method,
(typical: 500-2000)</li>
<li>$B$ the number of samples per batch,
(typical: 256)</li>
<li>$F$ the number of functional parameter training iterations in Phase 1,
(typical: 10000)</li>
<li>$Q$ the number of interventions performed in Phase 2,
(typical: 100)</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">SDI</th>
<th style="text-align: center;">Best MSE</th>
<th style="text-align: center;">Best NLL</th>
<th style="text-align: center;">Best Elbo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Asia</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: left;">chain8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: left;">jungle8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: left;">collider7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">collider8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: left;">full8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">27</td>
</tr>
</tbody>
</table>
<p>Table 6: Baseline comparisons: Hamming distance (lower is better) for learned and ground-truth edges on Asia and various synthetic graphs. compared to DAG-GNN Yu et al. [15]. DAG-GNN outputs 3 graphs according to different criterion. We show results on all outputs in this table and we show the best performing result in Table 1.</p>
<p>|  | Chain |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | Full |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | </p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 11: Ablation Study of Intervention Prediction Cross-entropy loss over time on multiple graphs and intervention prediction modes. Left: All 3-variable graphs. Solid/dashed lines: Ground-truth \&amp; Prediction strategies. Dotted lines: Random- \&amp; No-Prediction strategies. Training with prediction closely tracks ground-truth. Right: Comparison for 7-variable graphs, ground-truth against prediction strategy. Training with prediction still closely tracks ground-truth at larger scales.</p>
<p>Then the total number of interventions performed, and configurations and samples drawn, over an entire run are:</p>
<p>$$
\begin{aligned}
\text { Interventions } &amp; =I Q=\gamma \text { updates } \
\text { Samples } &amp; =I(\underbrace{F}<em P="P">{\text {Phase 1 }}+\underbrace{Q\left(N</em>}+N_{S}\right)<em 1="1" _Phase="{Phase" _text="\text">{\text {Phase 2 }}) B \
\text { Configurations } &amp; =I(\underbrace{F}</em>)
\end{aligned}
$$}}+\underbrace{Q\left(C_{P} N_{P}+C_{S} N_{S}\right)}_{\text {Phase 2 </p>
<p>Because of the multiplicative effect of these factors, the number of data samples required can quickly spiral out of control. For typical values, as many as $500 \times 10000 \times 256=1.28 \mathrm{e} 9$ observational and $500 \times 100 \times(100+10) \times 256=1.408 \mathrm{e} 9$ interventional samples are required. To alleviate this problem slightly, we limit the number of samples generated for each intervention; This limit is usually 500-2000.</p>
<h1>7.13 Effect of regularization</h1>
<p>Importance of sparsity regularizer. We use a $L 1$ regularizer on the structure parameters $\gamma$ to encourage a sparse representation of edges in the causal graph. In order to better understand the effect of the $L 1$ regularizer, we conducted ablation studies on the $L 1$ regularizer. It seems that the regularizer has an small effect on rate of converges and that the model converges faster with the regularizer, This is shown in Figure 13. However, this does not seem to affect the final value the model converges to, this is shown in Table 7.</p>
<p>Importance of DAG regularizer. We use an acyclic regularizer to discourage length-2 cycles in the learned model. We found that for small models ( $\leq 5$ variables), the acyclic regularizer helps with faster convergence, without improving significantly the final cross-entropy. This is illustrated for the 3 -variable graphs in Figure 14. However, for graphs larger than 5 variables, the acyclic regularizer starts playing an important role in encouraging the model to learn the correct structure. This is shown in the ablation study in Table 7.</p>
<h3>7.14 Importance of dropout</h3>
<p>To train the functional parameters on an observational distribution, one would need sampling adjacency matrices. One may be tempted to make these "complete directed graph" (all-ones except for a zero diagonal), to give the MLP maximum freedom to learn any potential causal relations itself. We demonstrate that functional parameter training cannot be carried out this way, and that it is necessary to "drop out" each edge (with probability of the current $\gamma$ value in our experiments) during pretraining of the conditional distributions of the SCM. We attempt to recover the previously-recoverable graphs chain3, fork3 and confounder3 without dropout, but fail to do so, as shown in Figure 15.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 12: Left to right, top to bottom Average cross-entropy loss of edge beliefs $\sigma(\gamma)$ and Area-Under-Curve throughout training for the synthetic graphs chainN, jungleN, colliderN and fullN, $N=3-13$, grouped by graph size. Error bars represent $\pm 1 \sigma$ over PRNG seeds $1-5$.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 13: Effect of sparsity (lsparse) regularizer : On 5 variable, 6 variable and 8 variable Nodes
SDI Eaton and Murphy [18]</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Asia</th>
<th style="text-align: left;">0</th>
<th style="text-align: left;">0</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">chain8</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">jungle8</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">collider7</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">7</td>
</tr>
<tr>
<td style="text-align: left;">collider8</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">7</td>
</tr>
<tr>
<td style="text-align: left;">full8</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">1</td>
</tr>
</tbody>
</table>
<p>Table 8: Comparisons: Structured hamming distance (SHD) on learned and ground-truth edges on asia and various synthetic graphs. Eaton and Murphy [18] can not scale to larger variables graphs as shown in Table 1, hence, we compare to the largest graph that [18] can scale up to. SDI is compared to [18] for collider7, collider8 and full8, [10] asserts with $100 \%$ confidence a no-edge where there is one (false negative). For comparisons with all other methods 1.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 14: Ablations study results on all possible 3 variable graphs. Graphs show the cross-entropy loss on learned vs ground-truth edges over training time. Comparisons of model trained with and without DAG regularizer ( $L_{\mathrm{DAG}}$ ), showing that DAG regularizer helps convergence.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 15: Edge CE loss for 3-variable graphs with no dropout when training functional parameters, showing the importance of this dropout.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 16: Cross-entropy for edge probability between learned and ground-truth SCM for Cancer at varying temperatures.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 17: Cross-entropy for edge probability between learned and ground-truth SCM. Left: The Earthquake dataset with 6 variables. Right: The Asia dataset with 8 variables</p>            </div>
        </div>

    </div>
</body>
</html>
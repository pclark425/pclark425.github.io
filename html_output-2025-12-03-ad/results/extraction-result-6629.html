<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6629 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6629</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6629</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-f288e2238ac8725baa7ca9874bbc3fed1e89a632</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f288e2238ac8725baa7ca9874bbc3fed1e89a632" target="_blank">Data Engineering for Scaling Language Models to 128K Context</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K, which outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K.</p>
                <p><strong>Paper Abstract:</strong> We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular \textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the \textit{quantity} and \textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \textit{domain balance} and \textit{length upsampling}. Concretely, we find that naively upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important. We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. Our recipe outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Llemma: An open language model for mathematics <em>(Rating: 2)</em></li>
                <li>Measuring massive multitask language understanding <em>(Rating: 2)</em></li>
                <li>Evaluating large language models trained on code <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6629",
    "paper_id": "paper-f288e2238ac8725baa7ca9874bbc3fed1e89a632",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Llemma: An open language model for mathematics",
            "rating": 2
        },
        {
            "paper_title": "Measuring massive multitask language understanding",
            "rating": 2
        },
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 1
        }
    ],
    "cost": 0.00559825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Data Engineering for Scaling Language Models to 128K Context</h1>
<p>Yao Fu<em> Rameswar Panda</em> Xinyao Niu ${ }^{\text {p }}$ Xiang Yue<em> Hannaneh Hajishirzi</em> Yoon Kim ${ }^{\text {L }}$ Hao Peng ${ }^{\text {L }}$<br>*University of Edinburgh ${ }^{\text {n}}$ MIT-IBM Watson AI Lab ${ }^{\text {p }}$ University of Melbourne ${ }^{\text {n }}$ Ohio State University<br>${ }^{\sigma}$ University of Washington ${ }^{\text {L }}$ MIT ${ }^{\text {LIUC }}$<br>yao.fu@ed.ac.uk yoonkim@mit.edu haopeng@illinois.edu<br>https://github.com/FranxYao/Long-Context-Data-Engineering</p>
<h4>Abstract</h4>
<p>We study the continual pretraining recipe for scaling language models' context lengths to 128 K , with a focus on data engineering. We hypothesize that long context modeling, in particular the ability to utilize information at arbitrary input locations, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training (e.g., 4 K to 128 K ) through lightweight continual pretraining on appropriate data mixture. We investigate the quantity and quality of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128 K context; (2) for quality, our results equally emphasize domain balance and length upsampling. Concretely, we find that naïvely upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important. We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128 K . Our recipe outperforms strong open-source longcontext models and closes the gap to frontier models like GPT-4 128K.</p>
<h2>1. Introduction</h2>
<p>A context window of 128 K tokens enables large language models to perform tasks that significantly beyond existing paradigm, such as multi-document question answering (Caciularu et al., 2023), repository-level code understanding (Bairi et al., 2023), long-history dialog modeling (Mazumder \&amp; Liu, 2024), and language model-powered autonomous agents (Weng, 2023). A popular testbed for
whether models can actually utilize long context length is the recent Needle-in-a-Haystack test (Kamradt, 2023), which asks the model to precisely recite the information in a given sentence where the sentence (the "needle") is placed in an arbitrary location of a 128 K long document (the "haystack"). In the open-source space, although works like LongLoRA (Chen et al., 2023b) and YaRN-Mistral (Peng et al., 2023) theoretically support 100K context, they are not able to pass this test at such context lengths, as shown in Fig. 1. Currently, only closed-source frontier models like GPT-4 128K have demonstrated strong performance on the Needle-in-a-Haystack test.</p>
<p>This work investigates data engineering methods for scaling language models' context lengths. Our objective is to continue pretraining the language model on appropriate data mixtures such that it can pass the Needle-in-a-Haystack test at 128 K length. Given that most existing models are trained on less than 4 K context length (Touvron et al., 2023a) and that attention has quadratic complexity, continual pretraining with full attention on much longer context lengths (we train on $64 \mathrm{~K}-80 \mathrm{~K}$ context lengths) may seem prohibitively costly at a first glance. However, we show that this is feasible under academic-level resources (see Table 2). We use LLaMA-2 7B and 13B as our base models. We do not make any significant change to model architecture other than adjusting the base of RoPE, as in Xiong et al. (2023). Our major focus is the data recipe: what and how much data is able to well-adapt a model to pass the Needle-in-a-Haystack test at 128 K context length.</p>
<p>We hypothesize that the capability to utilize information at arbitrary locations within long context length is (mostly) already acquired during pretraining, even for models pretrained on substantially shorter 4 K contexts. This hypothesis is in contrast to existing works like Xiong et al. (2023); XVerse (2024), which perform continual pretraining on a large amount of data ( 400 B tokens) to inject long-contextmodeling capabilities; in this strategy, the cost can be as high as pre-training from scratch. In this work we show that continual pretraining on a small amount of long-context data, in our case, 1-5B tokens, can "unlock" a 7B model's</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Needle-in-a-Haystack test (Kamradt, 2023) performance comparison. The x-axis denotes the length of the document (the "haystack"); the y-axis indicates the position that the "needle" (a short sentence) is located within the document, from 1K to 128K tokens. For example, 50% indicates that the needle is placed in the middle of the document. A red cell means the the language model cannot recite the information in the needle, and a green cell means the model can. A white dashed line means models' continual pretraining (or finetuning for instruction-tuned models) context length; thus the area to its right indicates length generalization. Most existing open-source models make mistakes when the document is long. Our post-training recipe demonstrates strong performance up to about 100K length.</p>
<p>capability of precise retrieval over much longer context lengths than seen in original pretraining.</p>
<p>We further show that upsampling long sequences while retaining the domain mixture of the pretraining corpora is crucial for context scaling, yet overlooked by existing works (Table 1). Most existing works are based on the following intuition: to model long-range dependencies one needs long sequence data, which domains like books provide; therefore, it is necessary to upsample domains containing long sequences in the data mixture, as done by LongChat 32K (Li et al., 2023a) and YaRN Mistral 128K (Peng et al., 2023). However, we show that this intuitive solution is suboptimal because, as we observe, this results in perplexity degradations in other domains (Table 5). Instead, a data mixture that keeps the domain mixture ratio the same as the pretraining mixture, and then upsampling long sequences within each domain gives the most stable performance gain. We give evidence that this is the primary reason our solution improves long context tasks while maintaining short context performance, compared with strong baselines like YaRN-Mistral 128K (Peng et al., 2023) and LongLoRA 100K (Chen et al., 2023b).</p>
<p>In summary, we propose a concrete data recipe for scaling language model context length to 128K, specifically, which involves continual pretrain the full-attention model on 1-5B tokens of per-source-length upsampled data. We show that our recipe results in 7B and 13B LLaMA-2 of strong long-context performance, substantially closing the gap to frontier models like GPT-4 128K on the Needle-in-a-Haystack test, opening future possibilities of studying long-context modeling under academic budgets.</p>
<h2>2. Background</h2>
<p>Frontier language models feature extremely long context lengths, such as OpenAI's GPT-4 Turbo 128K (Nov 2023) and Anthropic's Claude-100K (May 2023). In the regime of 100K, a wide array of new applications emerge, such as repo-level code understanding (Bairi et al., 2023), long-history dialog modeling (Mazumder &amp; Liu, 2024), and language model-powered autonomous agents (Weng, 2023). A recent testbed for long-range capabilities is the Needle-in-a-Haystack benchmark, first proposed by Kamradt (2023) in Nov 2023. This benchmark asks the language model to recite the information in a "needle" sentence ("The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day") that is randomly inserted at an arbitrary location in a long essay. Since its release, it has become a popular sandbox for testing whether models can utilize 100K+ context lengths, as it differentiates models that can precisely recite the given information at arbitrary input location versus those models that cannot. So far there is neither public knowledge nor open-source work about achieving precise retrieval anywhere at this scale to the best of our knowledge.</p>
<p>This work improves the long-context capability over strong open-source baselines, and closes the gap to GPT-4 on the Needle-in-a-Haystack benchmark, as demonstrated in Fig. 1. Our baselines here include LMSys' LongChat v1.5 32K (Li et al., 2023a), Together Compute's LLaMA-2 32K (Together, 2023), YaRN Mistral 128K (Peng et al., 2023), and</p>
<p>Table 1. Major differences between our method v.s. existing work from a data perspective, which we view as critical for extending context length. Our data mixture differs from previous work in three ways: (1) length of continual pretraining data: we use 80 K compared to Together's 32K, which does not generalizes beyond 32K; (2) data mixture: we use SlimPajama which has balanced domains compared to YaRN, which uses book-only PG19; (3) length upsampling: we upsample long sequences compared to LongLoRA, which does not. Despite the subtlety of these details (e.g., many of these details are just mentioned as a single line in previous works), we find that these details are crucial for performance on long-range retrieval.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Existing data solution</th>
<th style="text-align: left;">Our solution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Together LLaMA-2 32K</td>
<td style="text-align: left;">Train on 32K length</td>
<td style="text-align: left;">Train on 80K length</td>
</tr>
<tr>
<td style="text-align: left;">LongChat v1.5 32K</td>
<td style="text-align: left;">Train on 32K length dialog data</td>
<td style="text-align: left;">Train on 80K length-upsampled SlimPajama</td>
</tr>
<tr>
<td style="text-align: left;">YaRN Mistral 7B 128K</td>
<td style="text-align: left;">Train on PG19 book-only</td>
<td style="text-align: left;">Trained on length-upsampled SlimPajama</td>
</tr>
<tr>
<td style="text-align: left;">LongLoRA 100K</td>
<td style="text-align: left;">Train on Redpajama no length upsampling</td>
<td style="text-align: left;">With length upsampling</td>
</tr>
</tbody>
</table>
<p>LongLoRA (Chen et al., 2023b), which are so far the top open-source long-context language models. These works focus on different aspects of long-context language modeling. For example, YaRN (Peng et al., 2023) focuses on positional embeddings, LongLoRA (Chen et al., 2023b) focuses on efficient attention, and Together (Together, 2023) focuses on a full-stack solution. Our work focuses on data-engineering, and identifies critical data aspects for extending language models' long-term dependency modeling to much longer contexts than seen during regular pretraining.</p>
<p>The major differences between this work and existing work are listed on Table 1. Together's LLaMA-2 is trained on 32 K , but only generalizes to about 40 K length. YaRN Mistral is trained on book-only data; later in Table 5 we will show that improvements on one domain has limited transfer to other domains, indicating that one should consider a balanced mixture of different domains. LongLoRA does not upsample long sequences; later in Fig. 4 we will show that upsampling long sequences is critical for precise retrieval. These details are relatively hard to notice, yet we believe that they are the important for improved performance. Before the Needle-in-a-Haystack test, most existing works use test negative log-likelihood as evaluation, effectively concealing the underlying differences beyond low loss. We show that, similar test loss could result in substantially different behavior when performing precise retrieval (Fig. 4).</p>
<p>Another important related work is the previous LLaMA Long (Xiong et al., 2023) work and the concurrent XVERSE (XVerse, 2024) work, which continue pretraining the model on 32 K sequences for about 500 billion tokens. These works are implicitly motivated by the view that longcontext modeling is a new capability that must be "injected" through large-scale training. We instead hypothesize that the base model has mostly already acquired this capability through large-scale pretraining, and thus a lightweight continual pretraining on relatively small data (e.g., 5B tokens) is enough to extend these capabilities to much longer context lengths (Fig. 3).</p>
<h2>3. Long Context Data Composition</h2>
<p>We use the SlimPajama (Soboleva et al., 2023) dataset for continual pretraining. This dataset is an open-source reproduction of the LLaMA (Touvron et al., 2023a) pretraining data mixture, consisting of $82 \%$ web data ( $67 \%$ from CommonCrawl and 15\% from C4), 4.5\% code (Github), $4.5 \%$ Wikipedia, $4.5 \%$ books, $2.5 \%$ Arxiv, and $2.0 \%$ StackExchange. Since this dataset closely mirrors that used to pretrain the LLaMA models, there is less concern of distribution shift during continual pretraining; it is therefore used by many recent works like Fuzhao Xue \&amp; You (2023).</p>
<p>The documents' lengths and their source domains are two closely related confounding factors in data engineering because long data usually come from particular sources. Our examination shows books and Github code are the longest sources, followed by Arxiv. Webpages like C4 and StackExchange tend to be shorter. Directly upsampling long data changes the domain mixture, e.g., upsampling sequences longer than 100 K will increase the portion of the books domain. Likewise, changes in the domain mixture will result in shifts of the length distribution. Our guideline is to decompose these two factors step by step, as is shown in Fig. 2. We consider the following methods:</p>
<p>Cut at 4K: This truncates documents into 4K-length chunks. This approach inherits the original data mixture and is used by most pretraining works, such as (Touvron et al., 2023a; Hoffmann et al., 2022). Since there are about $30 \%$ documents that are naturally longer than 4 K , this approach breaks such naturally-existing long-range dependencies.</p>
<p>Cut at 128K: This preserves most of the naturallyexisting long-range dependencies without changing the domain mixture. LongLoRA (Chen et al., 2023b) follows this approach. However, we will show later that only using the naturally-existing long dependencies is inadequate (Fig. 4).</p>
<p>Per-source Upsampling: This retains the domain mixture, then upsamples long documents within each domain. This approach upsamples long documents without chang-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Length and domain distributions of the various data mixture strategies. We use SlimPajama <em>(Soboleva et al., 2023)</em>, an open-source reproduction of LLaMA <em>(Touvron et al., 2023a)</em> pretraining data mixture, as the source dataset. The original data mixture has about 30% of documents that are naturally longer than 4K. Cutting documents at 4K, the common practice of pretraining like <em>Touvron et al. (2023a)</em>, breaks such long-range dependencies. Cutting documents at 128K retains the naturally-existing long-range dependencies. Global upsampling longer sequences slightly changes the data mixture. Per-source upsampling longer sequences increases the portion of long sequences while keeping the domain mixture the same. Upsampling Arxiv / Book / Github data simultaneously changes domain and length distributions.</p>
<p>ing the domain mixture. We recommend this approach as our experiments suggest that this gives the most balanced performance gain.</p>
<p><strong>Global Upsampling:</strong> This upsamples long documents while ignoring their source domains, and consequently slightly changes the domain mixture. Together (2023) uses this approach.</p>
<p><strong>Upsample Arxiv/ Book/ Github:</strong> This intentionally upsamples Arxiv/Book/Github data, assuming that these domains are more likely to contain naturally-occurring long documents. For example, YaRN Mistral <em>(Peng et al., 2023)</em> and MPT-storyteller <em>(Team, 2023)</em> upsamples books. This approach changes both the domain and length distributions.</p>
<p>The above approaches generally cover most data recipes used in prior works discussed in section 2. In our experiments, we will show that: (1) using the original mixture cutting at 128K is insufficient for precise retrieval over long-context (Fig. 4), and one need to upsample long sequences; (2) improved performance in one domain may not transfer and could even hurt another domain (Table 5), showing that one needs to balance the mixture ratio of different domains.</p>
<h2>4. Infrastructure and Engineering</h2>
<p>We continue pretraining the model on the data mixture yielded by the "Per-source Upsampling" strategy as introduced in section 3, where we train with a 80K sequence length for 7B models, and 64K for 13B models. A "brute force" training on 80K context may seem impractical at first glance given the quadratic complexity of attention. However, in our initial tests, we found this to be feasible, and the actual wallclock time was far from quadratic. This is due to the fact that most of the time is spent on data transfer from CPU to GPU (since we use offloading), from one GPU to another GPU via NVLink (since we use Zero3, see Rajbhandari et al., 2020) and from GPU High-Bandwidth Memory (HBM) to Streaming Multiprocessors (SM) (as in FlashAttention, see Dao, 2023). These IO operations are all constant or linear in sequence length. The actual quadratic attention computation, when performed on SM, is highly parallelized, and thus largely hidden by the linear IO cost. Consequently, training on 80K is only 3x slower than training on 4K.</p>
<p>Our specific configuration is listed on Table 2. We note that this configuration is substantially cheaper than previous work <em>(Xiong et al., 2023)</em>, as they continue pretraining the model on more than 400B tokens. However this configuration seems to be the limit of Huggingface-DeepSpeed framework, and setting even longer context leads to memory overflow. Since Zero Optimizer is data parallel, increasing number of GPUs cannot further increase context length, and most of the GPU memory is taken by the KV cache. We refer the reader to more advanced sequence parallelism techniques in <em>(Li et al., 2021; Jacobs et al., 2023)</em> for training on even longer sequences (e.g., 200K), and we leave the exploration of training on 200K context for future work.</p>
<p>For training, we use a constant learning rate 2e-5. We modify the base of RoPE positional encoding to adjust it to longer context, as in <em>Xiong et al. (2023)</em>. We pack all data to 80K chunks regardless of the document boundary, following common practice <em>(Raffel et al., 2020; Touvron et al., 2023a)</em>. We set the batch size to be 4M tokens. Note that this batch size is the same as training on 4K context length, as we increase the length of a chunk but decrease the number of chunks in a batch. We train the model on 5B tokens, which translates to 5B (size of data) / 4M (batch size) = 2000 optimization steps.</p>
<h2>5. Experimental Results</h2>
<p>We continue pretraining the base LLaMA 2 7B/ 13B models on data produced by our "Per-source Upsampling" strategy.</p>
<p>Table 2. We show that long context continual pretraining is feasible under academic-level resources. Our configuration on $8 \times 80 \mathrm{G}$ A100s, as listed below, takes 5 days, which is about $1 \%$ budget than existing works such as Xiong et al. (2023), which trains on 400B tokens.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Framework</th>
<th style="text-align: left;">Huggingface Transformers <br> + DeepSpeed Zero 3 <br> + FlashAttention 2 <br> + Gradient Checkpointing <br> + CPU Offloading</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Hardware</td>
<td style="text-align: left;">$8 \times 80$ G A100</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA 2 7B</td>
<td style="text-align: left;">Ctx. 4K 3 days / 10B tokens</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ctx. 80K 10 days / 10B tokens</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA 2 13B</td>
<td style="text-align: left;">Ctx. 4K 5 days / 10B tokens</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ctx. 64K 13 days / 10B tokens</td>
</tr>
<tr>
<td style="text-align: left;">Hardware</td>
<td style="text-align: left;">$2 \times 8 \times 80$ G A100</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA 2 7B</td>
<td style="text-align: left;">Ctx. 4K 2 days / 10B tokens</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ctx. 80K 7 days / 10B tokens</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA 2 13B</td>
<td style="text-align: left;">Ctx. 4K 4 days / 10B tokens</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ctx. 64K 10 days / 10B tokens</td>
</tr>
</tbody>
</table>
<p>(Sec. 5.3). The number of training tokens is 5B. We first show that our method outperforms strong open-source baselines like YaRN Mistral 7B 128K (Peng et al., 2023) and LongLoRA 7B 100K (Chen et al., 2023b), and closes the gap to GPT-4 Turbo 128K. Then we dissect the ingredients of our data recipe. In section 5.2, we discuss data sufficiency (the 5B part) and show that as we increase the amount of data from 100 M to 1 B , the model gradually surfaces the ability to precisely retrieve given information at arbitrary locations within 128 K documents. In section 5.3, we discuss the data mixture (the per-source length upsampled part), and show that the original mixture of SlimPajama is insufficient (although it has about $30 \%$ data longer than 4 K ), and one needs to upsample long sequences for the model to acquire the long context capability. We also show that the improvements on one upsampled domain (e.g., Books), have limited transfer to others, and can even hurt them. The Book and Code domains are a particular example that improvements on one come with detrimental impact on the other.</p>
<p>Existing work heavily uses validation loss as the primary evaluation (Chen et al., 2023a;b; Xiao et al., 2023; Peng et al., 2023). We show that using only the validation loss is insufficient because two data recipes that result in similar loss may have substantially different retrieval capabilities, as we show in Fig. 4. In addition to Needle-in-a-Haystack, we use a real-world book-long question answering benchmark (Zhang et al., 2023) for our evaluation. There are earlier long context benchmarks such as Zeroscrolls (Shaham et al., 2023), longbench (Bai et al., 2023b) and L-Eval (An et al., 2023). We do not evaluate on them because their</p>
<p>Table 3. Our method achieves better long context task performance (Needle. performance) without compromising short context performance (MMLU).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Ctx.</th>
<th style="text-align: center;">Needle.</th>
<th style="text-align: center;">MMLU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Non-LLaMA Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-4-Turbo</td>
<td style="text-align: center;">128 K</td>
<td style="text-align: center;">$\mathbf{8 7 . 1}$</td>
<td style="text-align: center;">$\mathbf{8 6 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-Turbo</td>
<td style="text-align: center;">16 K</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">67.3</td>
</tr>
<tr>
<td style="text-align: left;">YaRN Mistral 7B</td>
<td style="text-align: center;">128 K</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">59.4</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-2 7B Based Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Together LLaMA-2 7B</td>
<td style="text-align: center;">32 K</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">$\mathbf{4 4 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">LongChat v1.5 7B</td>
<td style="text-align: center;">32 K</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">42.3</td>
</tr>
<tr>
<td style="text-align: left;">LongLoRA 7B</td>
<td style="text-align: center;">100 K</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">37.9</td>
</tr>
<tr>
<td style="text-align: left;">Ours LLaMA-2 7B</td>
<td style="text-align: center;">80 K</td>
<td style="text-align: center;">$\mathbf{8 8 . 0}$</td>
<td style="text-align: center;">43.3</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-2 13B Based Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">LongLoRA 13B</td>
<td style="text-align: center;">64 K</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">50.1</td>
</tr>
<tr>
<td style="text-align: left;">Ours LLaMA-2 13B</td>
<td style="text-align: center;">64 K</td>
<td style="text-align: center;">$\mathbf{9 0 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 2 . 4}$</td>
</tr>
</tbody>
</table>
<p>lengths are mostly around 10 K , which were considered long at their release time, but substantially shorter than the 128 K regime, which is the focus of the present work.</p>
<h3>5.1. Overall Performance</h3>
<p>Figure 1 compares our method with Together AI's LLaMA2 32K (Together, 2023), LMSys' LongChat v1.5 32K (Li et al., 2023a), YaRN Mistral 7B 128K (Peng et al., 2023), and LongLoRA 100K (Chen et al., 2023b). We choose these models as our baseline because they are so far the top open-source long-context language models (as evidenced by thousands of GitHub stars). LongChat v1.5 32K does not perform well even for sequence length $16 \mathrm{~K}-32 \mathrm{~K}$, and we hypothesize that this is because the model has not gone through enough continual pretraining, but directly goes to finetuning. Together AI's LLaMA-2 7B's data mixture is similar to ours, which we believe to be the reason that they perform well within 32 K . Yet their model does not generalize beyond 32 K , while ours generalizes from 80 K to 128 K . For LongLoRA, we believe the two major problems are that they use sparse attention and that they do not upsample long sequence data, which we show is problematic in Fig. 4. For YaRN Mistral, we hypothesize that its underperformance is due to its being only trained on PG19 book (Rae et al., 2019); we show that it is important to use a balanced domain mixture in Table 5.</p>
<p>In Table 3 we show that our method not only improves precise retrieval, but maintains short context performance, evidenced by strong MMLU (Hendrycks et al., 2020) score, which is a widely-accepted benchmark for testing the general capabilities (within short context) of language models, and is used by Chinchilla (Hoffmann et al., 2022), LLama (Touvron et al., 2023a;b), Mistral (Jiang et al., 2023),</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. As we increase the number of trained tokens, the model's retrieval performance increases with converged validation loss. We further note two observations with regard to data quantity: (1) 500M tokens are enough to unlock most of the retrieval accuracy. Since we are not explicitly training the model to perform retrieval, this potentially suggests that the model already has the precise retrieval capability, and that we are simply extending this capability with lightweight continual pretraining; (2) the model's retrieval performance saturates at about 5B tokens, and further scaling to 10B tokens does not improve length generalization.</p>
<p>QWen (Bai et al., 2023a), and Baichuan (Yang et al., 2023a).</p>
<p>Table 4 further compares 128K context language models on a book-long question answering benchmark recently proposed by Zhang et al. (2023). This task conditions language models on a book and then asks questions about the plot. Our method outperforms LongLoRA and Yarn Mistral (even though Mistral 7B is a stronger base model than LLaMA 2 7B we use). Our 13B model performance closes the gap to GPT-4 128K, and we anticipate that future scaling and instruction tuning will further improve performance. While there are other long-context benchmarks in InfiniBench (Zhang et al., 2023), in our initial experiments we found that models often had trouble understanding the instruction (because they are not instruction tuned). Hence we focus on the BookQA benchmark where base LLMs performed reasonably without instruction tuning.</p>
<h3>5.2. Data Quantity</h3>
<p>Now we consider data quantity, i.e., how much data enables the language model to utilize the information at arbitrary locations within 128K context. Our hypothesis is that precise retrieval over long-range context is an intrinsic capability obtained by large-scale pretraining, even when the pretraining context length is substantially shorter (4K in many cases). If this hypothesis is true, then lightweight continual pretraining should be enough to extend this capability to much longer context lengths than see in training. That is, we would not need data-intensive continual pretraining as used by Xiong et al. (2023) and XVerse (2024).</p>
<p>Table 4. The performance gain shown in Needle-in-a-Haystack further translates to improvements on downstream BookQA. We report the results on 128K length InfiniBench long book question answering task (Zhang et al., 2023). Our method outperforms top open-source models and closes the gap to GPT-4.</p>
<table>
<thead>
<tr>
<th>Model / Train Len</th>
<th>Test Len</th>
<th>BookQA</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4-Turbo 128K</td>
<td>128K</td>
<td>37.4</td>
</tr>
<tr>
<td>LongLoRA 7B 100K</td>
<td>128K</td>
<td>24.3</td>
</tr>
<tr>
<td>Ours LLaMA-2 7B 80K</td>
<td>128K</td>
<td>27.4</td>
</tr>
<tr>
<td>LongLoRA 13B 64K</td>
<td>128K</td>
<td>24.6</td>
</tr>
<tr>
<td>YaRN Mistral 7B 128K</td>
<td>128K</td>
<td>26.3</td>
</tr>
<tr>
<td>Ours LLaMA-2 13B 64K</td>
<td>128K</td>
<td>31.1</td>
</tr>
</tbody>
</table>
<p>Fig. 3 shows how data scaling gradually unlocks the model's retrieval capability. When the model is continually pretrained on 100M to 300M tokens, the model's loss gradually decreases, but it has not converged yet. At 500M to 1B tokens, the model achieves relatively good performance within its continually pretrained 80K context, but does not generalize to 80K-128K range. After 5B tokens, the model performs well on 0-80K, and can generalize to unseen lengths 80K-128K. At 10B token, the model seems to overfit on its 80K training range, and the length generalization starts to decrease. Along this process, we see a gradual decrease in loss, which correlates with the increase in the retrieval performance. This result provides more evidence for our hypothesis, as only 500M tokens are required to enable a model's retrieval capability with 80K context.</p>
<p>Continually pretraining a 7B LLaMA-2 takes about 5 days on 8×80G A100 GPUs, and the cost is reasonable compared</p>
<p>Table 5. How different data engineering methods improves (or does not improve) the original data mixture, where we show results from continual pretraining of a 7B LLaMA-2 on 5B tokens packed to 80K length. We consider how changing the original data mixture to long-context upsampled data mixture results in different loss across domains, and report the loss differences to the original data mixture (e.g., the loss of per-source length upsampling minus the loss of original mixture). We view a loss difference larger than 0.01 significant, marked by a red shade indicating performance decrease of larger than +0.01 loss; or a green shade indicating performance improvements of smaller than -0.01 loss, or a grey shade indicating no significant differences. Although upsampling book / code / arxiv improves in-domain performance for both short and long context, such improvements do not transfer to all domains. Instead, upsampling one domain, e.g., code, may even harm another domain, e.g., book. Per-source length upsampling is the most balanced mixture with almost no significant increase of loss across domains.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">C4</th>
<th style="text-align: center;">CC</th>
<th style="text-align: center;">Stack</th>
<th style="text-align: center;">Arxiv</th>
<th style="text-align: center;">Wiki</th>
<th style="text-align: center;">Book</th>
<th style="text-align: center;">Github</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0 - 4K Context Length</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">2.038</td>
<td style="text-align: center;">1.760</td>
<td style="text-align: center;">1.519</td>
<td style="text-align: center;">1.660</td>
<td style="text-align: center;">1.424</td>
<td style="text-align: center;">2.085</td>
<td style="text-align: center;">0.907</td>
</tr>
<tr>
<td style="text-align: center;">v.s. Per-source</td>
<td style="text-align: center;">$+.002$</td>
<td style="text-align: center;">$+.008$</td>
<td style="text-align: center;">$-.001$</td>
<td style="text-align: center;">$-.008$</td>
<td style="text-align: center;">.040</td>
<td style="text-align: center;">$-.065$</td>
<td style="text-align: center;">$-.008$</td>
</tr>
<tr>
<td style="text-align: center;">v.s. Global</td>
<td style="text-align: center;">$+.008$</td>
<td style="text-align: center;">$+.010$</td>
<td style="text-align: center;">$+.015$</td>
<td style="text-align: center;">$-.020$</td>
<td style="text-align: center;">$-.020$</td>
<td style="text-align: center;">$-.140$</td>
<td style="text-align: center;">$+.015$</td>
</tr>
<tr>
<td style="text-align: center;">v.s. Code $\uparrow$</td>
<td style="text-align: center;">$+.010$</td>
<td style="text-align: center;">$+.016$</td>
<td style="text-align: center;">$+.010$</td>
<td style="text-align: center;">$+.006$</td>
<td style="text-align: center;">$-.026$</td>
<td style="text-align: center;">$+.030$</td>
<td style="text-align: center;">$-.023$</td>
</tr>
<tr>
<td style="text-align: center;">v.s. Book $\uparrow$</td>
<td style="text-align: center;">$+.010$</td>
<td style="text-align: center;">$+.016$</td>
<td style="text-align: center;">$+.021$</td>
<td style="text-align: center;">$+.000$</td>
<td style="text-align: center;">$-.010$</td>
<td style="text-align: center;">$-.175$</td>
<td style="text-align: center;">$+.029$</td>
</tr>
<tr>
<td style="text-align: center;">v.s. Arxiv $\uparrow$</td>
<td style="text-align: center;">$+.006$</td>
<td style="text-align: center;">$+.016$</td>
<td style="text-align: center;">$+.013$</td>
<td style="text-align: center;">$-.060$</td>
<td style="text-align: center;">$-.030$</td>
<td style="text-align: center;">$+.040$</td>
<td style="text-align: center;">$+.025$</td>
</tr>
<tr>
<td style="text-align: center;">4K - 128K Context Length</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">1.560</td>
<td style="text-align: center;">1.650</td>
<td style="text-align: center;">0.786</td>
<td style="text-align: center;">1.075</td>
<td style="text-align: center;">1.313</td>
<td style="text-align: center;">1.852</td>
<td style="text-align: center;">0.447</td>
</tr>
<tr>
<td style="text-align: center;">v.s. Per-source</td>
<td style="text-align: center;">$-.010$</td>
<td style="text-align: center;">$-.010$</td>
<td style="text-align: center;">$-.006$</td>
<td style="text-align: center;">$-.011$</td>
<td style="text-align: center;">.044</td>
<td style="text-align: center;">$-.014$</td>
<td style="text-align: center;">$+.002$</td>
</tr>
<tr>
<td style="text-align: center;">v.s. Global</td>
<td style="text-align: center;">$-.010$</td>
<td style="text-align: center;">$-.006$</td>
<td style="text-align: center;">$-.001$</td>
<td style="text-align: center;">$-.016$</td>
<td style="text-align: center;">$-.040$</td>
<td style="text-align: center;">$-.018$</td>
<td style="text-align: center;">$-.007$</td>
</tr>
<tr>
<td style="text-align: center;">v.s. Code $\uparrow$</td>
<td style="text-align: center;">$-.008$</td>
<td style="text-align: center;">$-.002$</td>
<td style="text-align: center;">$-.003$</td>
<td style="text-align: center;">$-.007$</td>
<td style="text-align: center;">$-.042$</td>
<td style="text-align: center;">$-.010$</td>
<td style="text-align: center;">$-.029$</td>
</tr>
<tr>
<td style="text-align: center;">v.s. Book $\uparrow$</td>
<td style="text-align: center;">$-.010$</td>
<td style="text-align: center;">$-.006$</td>
<td style="text-align: center;">$+.001$</td>
<td style="text-align: center;">$-.007$</td>
<td style="text-align: center;">.037</td>
<td style="text-align: center;">$-.030$</td>
<td style="text-align: center;">$+.000$</td>
</tr>
<tr>
<td style="text-align: center;">v.s. Arxiv $\uparrow$</td>
<td style="text-align: center;">$-.008$</td>
<td style="text-align: center;">$-.002$</td>
<td style="text-align: center;">$+.002$</td>
<td style="text-align: center;">$-.036$</td>
<td style="text-align: center;">$-.039$</td>
<td style="text-align: center;">$-.010$</td>
<td style="text-align: center;">$-.004$</td>
</tr>
</tbody>
</table>
<p>to large-scale pretraining (which requires months of compute on thousands of GPUs). Our results suggest that for supervised finetuning, since training on long-context is substantially cheaper than previously thought, future work may dive deeper on the solutions for 100K length finetuning and reasoning, which so far has almost no open-source work to our knowledge. For pretraining research, currently there is no definite answer as to whether long-context continual pretraining should be combined with other capabilities, such as math (Azerbayev et al., 2023) and code (Chen et al., 2021), which typically require hundreds of billions of tokens. Our results suggest that long-context continual pretraining could be a separate stage after code and math pretraining.</p>
<h3>5.3. Data Mixture</h3>
<p>Now we discuss why per-source length upsampling is necessary when constructing the data mixture. Recall that this strategy keeps the mixture ratio of the data sources the same as the original data, i.e., $67 \%$ CommonCrawl (CC), 15\% C4, 4.5\% Github, 4.5\% Wikipedia, 4.5\% books, 2.5\% Arxiv and 2.0\% StackExchange for SlimPajama. Then in each of the domains, we upsample sequences longer than 4 K from about $30 \%$ to about $70 \%$. Doing this enables us to keep the domain mixture ratio fixed, only changing the length
distribution of the training documents. In contrast, globally upsampling long sequences (without considering their domain), or intentionally upsampling code/ book/ Arxiv (since they are long) changes both the domain mixture and the length distribution.</p>
<p>We compare all the data mixtures with the original one packed to 80 K / 64 K for the continual pretraining of the 7B / 13B LLaMA-2. This baseline approach effectively never changes the original data, but keeps the naturally existing long sequence that would otherwise been cut to 4 K (recall Sec. 3). If a data mixture is effective, it should be at least better than this baseline "do nothing" approach.</p>
<p>Table 5 compares the per-domain loss differences of all the data mixture against the baseline original mixture. We report the differences of the validation loss, where a more than 0.01 loss change is considered significant, following common pretraining practice (Kaplan et al., 2020; Peng et al., 2023; Hoffmann et al., 2022). We observe: (1) for $0-4 \mathrm{~K}$ short context data, most of the data mixtures have a negative impact on webpages (C4, CC and StackExchange), except for the per-source approach; (2) performance improvements from domains like Book may not transfer to others, and even hurt code performance; (3) per-source length upsampling is the most balanced mixture that improves 4K-128K</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. The importance of length upsampling. The validation loss, a common measure for long-context language modeling used in Xiong et al. (2023); Chen et al. (2023b), cannot reveal longcontext retreival capability, but Needle-in-a-Haystack can. The original data mixture, i.e. without length upsampling, despite giving very close loss, performs badly on precise retrieval. Persource length upsampling significantly improves precise retrieval.
context losses without much sacrificing short-context losses, whereas all other methods show more or less performance tradeoff across domains. Combining these observations, we recommend the per-source length-upsampling strategy.</p>
<p>Since per-source length upsampling keeps the domain mixture ratio while increasing tokens from long sequences, its major difference to the original data mixture is the distribution of sequence length. Fig. 4 makes a close comparison between the two data strategies. Specifically, we train two LLaMA-2 7B models with a 80 K context length using original / per-source upsampled data mixture, and report their per-length validation loss and Needle-in-a-Haystack performance. Note that LongLoRA (Chen et al., 2023b) uses the original data mixture without length upsampling, so our results also explains why we achieve better performance than LongLoRA (Fig. 1). We see that the original data mixture without length upsampling, despite achieving a very close loss, underperforms on precise retrieval. Per-source length upsampling significantly improves precise retrieval. This observation also serves as strong evidence why only using test loss, the evaluation used in most prior work (Chen et al., 2023a; Peng et al., 2023; Chen et al., 2023b; Xiao et al., 2023; Anthropic, 2023), may conceal the underlying model differences. The necessity of length upsampling is demonstrated by the Needle-in-a-Haystack test.</p>
<h2>6. Discussion</h2>
<p>We attribute our improvements over strong open-source baselines, as is detailed in section 5, to our careful treatments of data engineering. Our results echo the recent wisdom that in large language model research, data engineering is equally important as modeling and algorithmic innovations (Kaplan et al., 2020; Hoffmann et al., 2022; Brown et al., 2020). As we list in Table 1, many of the data details are crucial for strong long-context performance, yet may be easily overlooked. We also acknowledge that our research is made possible through utilizing the latest innovations in machine learning systems research, particularly FlashAttention (Dao, 2023) as it reduces the memory usage of the attention mechanism from quadratic to linear. Incorporating further work on sequence parallelism (Li et al., 2021; Jacobs et al., 2023) could enable brute force training of even longer context (e.g., 200K) models.</p>
<p>The Transformer's use of position embeddings makes it difficult to generalize significantly beyond contexts seen during training even with relative positional encodings (Press et al., 2021; Sun et al., 2022; Li et al., 2023b), thus necessitating (at least a little bit of) continual pretraining. There has much recent work on RNN-like architectures, which implicitly encode positional information through their hidden states, that perform competitively with Transformers (Sun et al., 2023; Qin et al., 2023; Gu \&amp; Dao, 2023; Yang et al., 2023b). It would be interesting to test whether such models can generalize zero-shot to longer contexts than seen during training on the Needle-in-a-Haystack benchmark.</p>
<p>Long-context language model research at the 100K-level is still a developing research area. This work only studies continual pretraining, and research on instruction finetuning language models on tasks of 100K context length (e.g., repolevel code understanding) is still limited. So far there seems to no open-source instruction-finetuned 100K context language models. We hope our work serve as a basis for future work on 100K-level long context superivsed finetuning.</p>
<h2>7. Conclusion</h2>
<p>This work studies a continual pretraining recipe for scaling language models' context length to 128 K tokens. We demonstrate that the ability to utilize information at arbitrary locations within the 128 K input is already mostly acquired by large-scale pretraining, even for models pretrained on substantially shorter 4 K context. We show that continual pretraining of the full model on 1-5 billion tokens from per-source length-upsampled data gives the most balanced performance improvements. Our work closes the gap to frontier models like GPT-4 128K on the retrieval task and serves as the foundation for future long-context instruction finetuning research.</p>
<h2>References</h2>
<p>An, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong, L., and Qiu, X. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088, 2023.</p>
<p>Anthropic. Model card and evaluations for claude models, July 2023. URL https://www.anthropic.com/ product.</p>
<p>Azerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D., McAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and Welleck, S. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023.</p>
<p>Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023a.</p>
<p>Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023b.</p>
<p>Bairi, R., Sonwane, A., Kanade, A., Iyer, A., Parthasarathy, S., Rajamani, S., Ashok, B., Shet, S., et al. Codeplan: Repository-level coding using llms and planning. arXiv preprint arXiv:2309.12499, 2023.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877-1901, 2020.</p>
<p>Caciularu, A., Peters, M. E., Goldberger, J., Dagan, I., and Cohan, A. Peek across: Improving multi-document modeling via cross-document question-answering. arXiv preprint arXiv:2305.15387, 2023.</p>
<p>Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023a.</p>
<p>Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023b.</p>
<p>Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023.</p>
<p>Fuzhao Xue, Zian Zheng, Y. F. J. N. Z. Z. W. Z. and You, Y. Openmoe: Open mixture-of-experts language models. https://github.com/XueFuzhao/OpenMoE, 2023.</p>
<p>Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. 2023. URL https://api. semanticscholar.org/CorpusID:265551773.</p>
<p>Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.</p>
<p>Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.</p>
<p>Jacobs, S. A., Tanaka, M., Zhang, C., Zhang, M., Song, L., Rajbhandari, S., and He, Y. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023.</p>
<p>Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.</p>
<p>Kamradt, G. Needle in a haystack - pressure testing llms. https://github.com/gkamradt/LLMTest_ NeedleInAHaystack, 2023.</p>
<p>Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.</p>
<p>Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J. E., Stoica, I., Ma, X., and Zhang, H. How long can open-source llms truly promise on context length?, June 2023a. URL https://lmsys.org/blog/ 2023-06-29-longchat.</p>
<p>Li, S., Xue, F., Baranwal, C., Li, Y., and You, Y. Sequence parallelism: Long sequence training from system perspective. arXiv preprint arXiv:2105.13120, 2021.</p>
<p>Li, S., You, C., Guruganesh, G., Ainslie, J., Ontanon, S., Zaheer, M., Sanghai, S., Yang, Y., Kumar, S., and Bhojanapalli, S. Functional interpolation for relative positions improves long context transformers. arXiv preprint arXiv:2310.04418, 2023b.</p>
<p>Mazumder, S. and Liu, B. Lifelong and Continual Learning Dialogue Systems. Springer Nature, 2024.</p>
<p>Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language models, 2023.</p>
<p>Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.</p>
<p>Qin, Z., Yang, S., and Zhong, Y. Hierarchically gated recurrent neural network for sequence modeling. CoRR, abs/2311.04823, 2023. doi: 10.48550/ARXIV. 2311.04823. URL https://doi.org/10.48550/arXiv. 2311.04823.</p>
<p>Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., and Lillicrap, T. P. Compressive transformers for longrange sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/1911.05507.</p>
<p>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.</p>
<p>Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1-16. IEEE, 2020.</p>
<p>Shaham, U., Ivgi, M., Efrat, A., Berant, J., and Levy, O. Zeroscrolls: A zero-shot benchmark for long text understanding, 2023.</p>
<p>Soboleva, D., Al-Khateeb, F., Myers, R., Steeves, J. R., Hestness, J., and Dey, N. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-2023. URL https://huggingface.co/datasets/ cerebras/SlimPajama-627B.</p>
<p>Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X., and Wei, F. A length-extrapolatable transformer. arXiv preprint arXiv:2212.10554, 2022.</p>
<p>Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023.</p>
<p>Team, M. N. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL www. mosaicml.com/blog/mpt-7b. Accessed: 2023-03-28.</p>
<p>Together. Llama-2-7b-32k-instruct - and fine-tuning for llama-2 models with together api, August 2023. URL https://www.together.ai/blog/ llama-2-7b-32k-instruct.</p>
<p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Weng, L. Llm-powered autonomous agents. lilianweng.github.io, Jun 2023. URL https://lilianweng. github.io/posts/2023-06-23-agent/.</p>
<p>Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.</p>
<p>Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023.</p>
<p>XVerse. Xverse-13b, January 2024. URL https://github.com/xverse-ai/XVERSE-13B/ blob/main/README_EN.md.</p>
<p>Yang, A., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin, C., Lv, C., Pan, D., Wang, D., Yan, D., et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023a.</p>
<p>Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023b.
version-of-regpajama.
Zhang, X., Chen, Y., Hu, S., Wu, Q., Chen, J., Xu, Z., Dai, Z., Han, X., Wang, S., Liu, Z., and Sun, M. Infinitebench: 128k long-context benchmark for language models, 2023.</p>            </div>
        </div>

    </div>
</body>
</html>
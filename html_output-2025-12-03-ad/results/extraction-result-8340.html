<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8340 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8340</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8340</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278886171</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.17653v1.pdf" target="_blank">GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Geometric spatial reasoning forms the foundation of many applications in artificial intelligence, yet the ability of large language models (LLMs) to operate over geometric spatial information expressed in procedural code remains underexplored. In this paper, we address this gap by formalizing the Program-to-Geometry task, which challenges models to translate programmatic drawing code into accurate and abstract geometric reasoning. To evaluate this capability, we present GeoGramBench, a benchmark of 500 carefully refined problems organized by a tailored three-level taxonomy that considers geometric complexity rather than traditional mathematical reasoning complexity. Our comprehensive evaluation of 17 frontier LLMs reveals consistent and pronounced deficiencies: even the most advanced models achieve less than 50% accuracy at the highest abstraction level. These results highlight the unique challenges posed by program-driven spatial reasoning and establish GeoGramBench as a valuable resource for advancing research in symbolic-to-spatial geometric reasoning. Project page: https://github.com/LiAuto-DSR/GeoGramBench.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8340.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8340.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source proprietary large language model evaluated via official API in this paper; tested on program-to-geometry (procedural drawing code) problems in GeoGramBench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary closed-source API model evaluated by the authors using the official API and standardized prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeoGramBench: Program-to-Geometry (procedural geometry problems)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>procedural-code-driven geometry problems requiring diagram construction and multi-step spatial reasoning from drawing code (Asymptote/Matplotlib).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot setting: model input = problem text + procedural drawing code; prompt template includes 'Let's think step by step and output the final answer within \boxed{}'. For each instance 8 responses sampled at temperature 0.6; final accuracy = mean over 8 samples.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Models parse drawing code (Asymptote/Matplotlib), attempt chain-of-thought style reasoning, iterative self-reflection and verification; rely on internal symbolic parsing of code into geometric primitives and composition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primitive: 39.81% ; Local Relation Composition: 21.29% ; Global Abstract Integration: 4.96% ; ALL: 21.40% (accuracy percentages as reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative outputs show attempts to parse code ('Now, looking at the Asymptote code'), but extremely low accuracy on Global Abstract Integration indicates poor global spatial integration; CoT trajectories rarely correct internal geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against other closed- and open-source models in Table 1; GPT-4o performs substantially worse on GeoGramBench than many other closed-source and open-source models (much lower accuracies across levels).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Severe performance drop as geometric complexity increases; fails to form coherent global diagrammatic representations; chain-of-thought often cycles and does not resolve spatial relations; vulnerable to directional mistakes (e.g., CCW/ CW interpretation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8340.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8340.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-o1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-o1 (series)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source GPT-o1 series model evaluated on GeoGramBench; performs well on primitive tasks but degrades on abstract program-to-geometry tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-o1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary closed-source model from the GPT-o1 series evaluated via API with identical prompting; multiple variants (preview/mini) also evaluated separately.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeoGramBench: Program-to-Geometry (procedural geometry problems)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>procedural-code-driven geometry problems requiring diagram construction and multi-step spatial reasoning from drawing code (Asymptote/Matplotlib).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot; input = problem text + procedural code; standardized prompt 'Let's think step by step...' ; 8 samples per problem, temperature 0.6, mean accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Parsing of procedural drawing code into geometric elements; chain-of-thought style reasoning and iterative self-reflection used in generated solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primitive: 86.76% ; Local Relation Composition: 76.02% ; Global Abstract Integration: 43.35% ; ALL: 70.92%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Models frequently reference code in reasoning steps and reconstruct local primitives; substantial drop on Global Abstract Integration indicates difficulties with global spatial composition despite local parsing ability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Per Table 1, GPT-o1 achieves among the highest ALL and Primitive accuracies; shows a smaller gap vs. some open-source models at primitive level but retains advantage at abstract level compared to e.g., DeepSeek-R1.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Accuracy declines markedly with problem complexity; struggles with integrating local constructions into coherent global representations; CoT often insufficient to resolve spatial ambiguities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8340.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8340.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source GPT variant evaluated on GeoGramBench showing strong performance on primitive and compositional levels but reduced performance on abstract geometry tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary closed-source model accessed via official API and evaluated with the shared prompt/temperature sampling protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeoGramBench: Program-to-Geometry</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>procedural-code-driven geometry problems requiring spatial diagram construction and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot; problem text + code as input; sample 8 outputs per instance (temperature 0.6); final answer formatting requested in \boxed{}.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Code parsing, CoT-style stepwise reasoning, textual description of geometric primitives and relations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primitive: 84.33% ; Local Relation Composition: 75.66% ; Global Abstract Integration: 42.16% ; ALL: 70.00%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Model explicitly cites parsing code and attempts to visualize diagrams in its chain-of-thought; performs well on primitive recognition indicating ability to extract local geometry from code.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Comparable to GPT-o1 on simpler tasks; loses ground relative to top closed/open models at Global Abstract Integration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles to reliably form globally consistent spatial representations; single local errors cascade into incorrect global inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8340.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8340.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-Pro-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-Pro-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source multimodal-capable model evaluated on GeoGramBench; exhibits limited ability on program-driven geometry tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-Pro-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary model (Gemini family) evaluated through API calls with identical prompting; included as a closed-source baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B (as implied in name)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeoGramBench: Program-to-Geometry</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>procedural geometry problems requiring parsing of drawing code and spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot, problem text + code, 8 samples per problem (temperature 0.6), CoT encouraged in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Parsing of code, chain-of-thought reasoning, attempts at diagram reconstruction in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primitive: 49.26% ; Local Relation Composition: 31.79% ; Global Abstract Integration: 15.92% ; ALL: 31.64%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Some parsing behavior observed but low scores indicate weak spatial abstraction and composition capabilities on program-driven tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperforms relative to top closed-source and many open-source models on GeoGramBench; comparatively low across all levels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Substantial failure on abstract integration; difficulty resolving CCW/CW and other directional code semantics; iterative CoT rarely yields correct final diagram.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8340.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8340.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source reasoning-focused model evaluated extensively; strong at primitive and compositional geometry but limited on abstract integration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source model emphasized for reasoning (paper references DeepSeek-R1 work); evaluated across GeoGramBench with standardized prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeoGramBench: Program-to-Geometry</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>procedural geometry problems (diagram construction from Asymptote/Matplotlib code) requiring local and global spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot; model given problem text + drawing code; 8 outputs per instance sampled at temperature 0.6; prompt asks for stepwise reasoning and boxed final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Parses procedural code into primitives, composes local relations, uses chain-of-thought and iterative self-checking; attempts to visualize and reason about arcs, radii, directions in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primitive: 85.66% ; Local Relation Composition: 75.27% ; Global Abstract Integration: 40.38% ; ALL: 69.17%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High primitive and compositional scores and qualitative excerpts (e.g., parsing arcs and midpoint constructions) indicate ability to extract local spatial structure; fails in robust global integration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Comparable primitive/compositional performance to GPT-o1 but falls behind at Global Abstract Integration; open-source top models show size-scaling trends.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Drops sharply on abstract tasks (<50%); local parsing errors propagate; CoT often repetitive and fails to update global diagram correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8340.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8340.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 32B open-source, reasoning-oriented LLM achieving strong overall GeoGramBench performance, especially at primitive and compositional levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 32B model described as reasoning-oriented in the paper; achieved state-of-the-art among open-source models on GeoGramBench.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeoGramBench: Program-to-Geometry</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>procedural-code geometry problems requiring translation of drawing code to internal spatial representation and multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot evaluation using problem text + drawing code (Asymptote/Matplotlib); 8 sampled responses per instance at temperature 0.6; standardized prompt encourages stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Parses procedural code into geometric primitives, composes local relations, uses chain-of-thought reasoning for stepwise integration; robust parsing across drawing languages (Asymptote vs Matplotlib).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primitive: 85.17% ; Local Relation Composition: 73.12% ; Global Abstract Integration: 37.92% ; ALL: 67.20%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Minimal language/syntax sensitivity: QwQ-32B shows <1% absolute accuracy difference between Asymptote and Matplotlib versions, supporting that failures are due to deeper spatial abstraction limits rather than code language unfamiliarity; qualitative CoT traces show parsing and visualization attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Top-performing open-source model; compares favorably to many 32B academic models (s1.1-32B, LIMO-32B) and is competitive vs closed-source in primitive/compositional tasks but still below best closed-source on abstract tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Does not exceed 50% on Global Abstract Integration (37.92%); struggles with global abstraction, recursion, parameterized transformations; CoT often fails to converge to correct global representation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8340.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8340.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-v3-0324</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-v3-0324</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source model variant evaluated on GeoGramBench; performs well on primitives and composition but much lower on abstract integration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-v3-0324</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source DeepSeek family variant evaluated at scale in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeoGramBench: Program-to-Geometry</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>procedural geometry problems requiring parsing of drawing code and multi-step spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot, text+code input; 8 responses sampled per instance (temp 0.6); standardized prompt with CoT encouragement.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Code parsing, primitive extraction, chain-of-thought reasoning, iterative verification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primitive: 80.57% ; Local Relation Composition: 68.89% ; Global Abstract Integration: 27.67% ; ALL: 62.05%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Model often reconstructs local geometric elements from code but fails to reliably assemble global abstractions; qualitative traces show stepwise parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperformed by DeepSeek-R1 and QwQ-32B on most metrics; drops notably at abstract level compared to primitive/compositional.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Large drop on abstract tasks; local mis-assignments and directional errors cascade into incorrect problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8340.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8340.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>s1.1-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>s1.1-32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 32B academic model included as a baseline; moderate primitive/compositional performance, limited abstract capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>s1.1-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Academic open-source 32B model evaluated across GeoGramBench levels.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeoGramBench: Program-to-Geometry</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>procedural code-based geometry problems requiring spatial parsing and integration.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot; problem text plus drawing code; 8 outputs per instance sampled at temperature 0.6; mean accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Extracts primitives from code, composes local relations, uses CoT but with limited global synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primitive: 75.37% ; Local Relation Composition: 58.96% ; Global Abstract Integration: 26.58% ; ALL: 54.60%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Capable of primitive recognition but frequently fails at global composition; qualitative failure modes described in behavior analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs below QwQ-32B and DeepSeek variants; shows expected size/architecture-linked limitations versus reasoning-focused models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low abstract-level accuracy; suffers from local-to-global integration bottleneck described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8340.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8340.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LIMO-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LIMO-32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 32B open-source model evaluated on GeoGramBench with similar behavior to other academic 32B models: reasonable primitive parsing, poor abstract integration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LIMO-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 32B model evaluated in the study; included for scale comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeoGramBench: Program-to-Geometry</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>procedural geometry problems requiring diagram reconstruction and spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot; text+code input; 8 sampled outputs per instance; prompt requires stepwise reasoning and boxed final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Parsing drawing code, stepwise decomposition, CoT-based reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primitive: 76.59% ; Local Relation Composition: 59.63% ; Global Abstract Integration: 25.53% ; ALL: 54.98%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Shows extraction of local primitives but poor global abstraction; behavior consistent with the hypothesized multi-stage internal representation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Similar to other academic 32B models; underperforms reasoning-optimized QwQ-32B.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails to exceed ~26% on abstract problems; local parsing errors propagate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8340.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8340.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bespoke-Stratos-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bespoke-Stratos-32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 32B open-source model included as a baseline; lower performance on all GeoGramBench levels compared to top 32B models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bespoke-Stratos-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 32B model evaluated in the benchmark; included among academic baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeoGramBench: Program-to-Geometry</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>procedural geometry problems requiring spatial parsing and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot; problem text and code provided; 8 output samples per instance; mean accuracy computed.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Parses primitives from code, attempts compositional reasoning and CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primitive: 62.50% ; Local Relation Composition: 42.56% ; Global Abstract Integration: 17.02% ; ALL: 40.55%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Limited primitive recognition and poor composition; qualitative examples of failure modes in paper apply.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs worse than other 32B models like QwQ-32B and DeepSeek variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Large drop with increased geometric complexity; cannot reliably form global representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8340.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8340.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sky-T1-mini-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sky-T1-mini-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B open-source model evaluated on GeoGramBench showing moderate primitive recognition but limited higher-level spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sky-T1-mini-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source smaller-scale 7B model evaluated to study size scaling effects on program-to-geometry tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeoGramBench: Program-to-Geometry</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>procedural geometry problems requiring code-to-diagram translation and spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot; input includes problem text and drawing code; 8 responses per instance sampled at temperature 0.6; mean accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Parses drawing code into primitives, uses CoT in outputs, but limited compositional and abstract integration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primitive: 71.45% ; Local Relation Composition: 57.75% ; Global Abstract Integration: 24.79% ; ALL: 52.70%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Good primitive recognition relative to model size but declines on compositional/abstract tasks, supporting a size-linked capability trend.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs worse than 32B models; follows the observed trend that accuracy decreases with smaller model size.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails to integrate local structures globally; global abstract tasks remain very difficult at this scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8340.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8340.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepScaleR-1.5B-preview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepScaleR-1.5B-preview</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1.5B open-source model evaluated as a low-parameter baseline; shows limited program-to-geometry capability beyond primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepScaleR-1.5B-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 1.5B model evaluated by the authors; included to assess low-parameter performance on GeoGramBench.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeoGramBench: Program-to-Geometry</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>procedural geometry problems requiring parsing of code into geometric primitives and composing spatial relations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot; problem text + code input; 8 outputs sampled per instance (temperature 0.6); mean accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Attempts to parse code and reason in chain-of-thought but is constrained by model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primitive: 65.44% ; Local Relation Composition: 47.89% ; Global Abstract Integration: 15.76% ; ALL: 43.83%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Some primitive recognition present, but major deficits in composition/abstract integration consistent with capacity limits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs better than some 1.5B variants but worse than larger models; highlights size-performance scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails on Global Abstract Integration; CoT less effective at low parameter counts; prone to repeating symbolic steps without spatial consolidation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? <em>(Rating: 2)</em></li>
                <li>GeoSense: Evaluating identification and application of geometric principles in multimodal reasoning <em>(Rating: 2)</em></li>
                <li>Euclid: Supercharging multimodal llms with synthetic high-fidelity visual descriptions <em>(Rating: 2)</em></li>
                <li>Lego-puzzles: How good are mllms at multi-step spatial reasoning? <em>(Rating: 2)</em></li>
                <li>AIME24 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8340",
    "paper_id": "paper-278886171",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A closed-source proprietary large language model evaluated via official API in this paper; tested on program-to-geometry (procedural drawing code) problems in GeoGramBench.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Proprietary closed-source API model evaluated by the authors using the official API and standardized prompt templates.",
            "model_size": null,
            "puzzle_name": "GeoGramBench: Program-to-Geometry (procedural geometry problems)",
            "puzzle_type": "procedural-code-driven geometry problems requiring diagram construction and multi-step spatial reasoning from drawing code (Asymptote/Matplotlib).",
            "task_setup": "Zero-shot setting: model input = problem text + procedural drawing code; prompt template includes 'Let's think step by step and output the final answer within \\boxed{}'. For each instance 8 responses sampled at temperature 0.6; final accuracy = mean over 8 samples.",
            "mechanisms_or_strategies": "Models parse drawing code (Asymptote/Matplotlib), attempt chain-of-thought style reasoning, iterative self-reflection and verification; rely on internal symbolic parsing of code into geometric primitives and composition.",
            "performance_metrics": "Primitive: 39.81% ; Local Relation Composition: 21.29% ; Global Abstract Integration: 4.96% ; ALL: 21.40% (accuracy percentages as reported in Table 1).",
            "evidence_of_spatial_reasoning": "Qualitative outputs show attempts to parse code ('Now, looking at the Asymptote code'), but extremely low accuracy on Global Abstract Integration indicates poor global spatial integration; CoT trajectories rarely correct internal geometry.",
            "comparisons": "Compared against other closed- and open-source models in Table 1; GPT-4o performs substantially worse on GeoGramBench than many other closed-source and open-source models (much lower accuracies across levels).",
            "limitations_or_failure_cases": "Severe performance drop as geometric complexity increases; fails to form coherent global diagrammatic representations; chain-of-thought often cycles and does not resolve spatial relations; vulnerable to directional mistakes (e.g., CCW/ CW interpretation).",
            "uuid": "e8340.0",
            "source_info": {
                "paper_title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-o1",
            "name_full": "GPT-o1 (series)",
            "brief_description": "A closed-source GPT-o1 series model evaluated on GeoGramBench; performs well on primitive tasks but degrades on abstract program-to-geometry tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-o1",
            "model_description": "Proprietary closed-source model from the GPT-o1 series evaluated via API with identical prompting; multiple variants (preview/mini) also evaluated separately.",
            "model_size": null,
            "puzzle_name": "GeoGramBench: Program-to-Geometry (procedural geometry problems)",
            "puzzle_type": "procedural-code-driven geometry problems requiring diagram construction and multi-step spatial reasoning from drawing code (Asymptote/Matplotlib).",
            "task_setup": "Zero-shot; input = problem text + procedural code; standardized prompt 'Let's think step by step...' ; 8 samples per problem, temperature 0.6, mean accuracy reported.",
            "mechanisms_or_strategies": "Parsing of procedural drawing code into geometric elements; chain-of-thought style reasoning and iterative self-reflection used in generated solutions.",
            "performance_metrics": "Primitive: 86.76% ; Local Relation Composition: 76.02% ; Global Abstract Integration: 43.35% ; ALL: 70.92%.",
            "evidence_of_spatial_reasoning": "Models frequently reference code in reasoning steps and reconstruct local primitives; substantial drop on Global Abstract Integration indicates difficulties with global spatial composition despite local parsing ability.",
            "comparisons": "Per Table 1, GPT-o1 achieves among the highest ALL and Primitive accuracies; shows a smaller gap vs. some open-source models at primitive level but retains advantage at abstract level compared to e.g., DeepSeek-R1.",
            "limitations_or_failure_cases": "Accuracy declines markedly with problem complexity; struggles with integrating local constructions into coherent global representations; CoT often insufficient to resolve spatial ambiguities.",
            "uuid": "e8340.1",
            "source_info": {
                "paper_title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-o3-mini",
            "name_full": "GPT-o3-mini",
            "brief_description": "A closed-source GPT variant evaluated on GeoGramBench showing strong performance on primitive and compositional levels but reduced performance on abstract geometry tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-o3-mini",
            "model_description": "Proprietary closed-source model accessed via official API and evaluated with the shared prompt/temperature sampling protocol.",
            "model_size": null,
            "puzzle_name": "GeoGramBench: Program-to-Geometry",
            "puzzle_type": "procedural-code-driven geometry problems requiring spatial diagram construction and reasoning.",
            "task_setup": "Zero-shot; problem text + code as input; sample 8 outputs per instance (temperature 0.6); final answer formatting requested in \\boxed{}.",
            "mechanisms_or_strategies": "Code parsing, CoT-style stepwise reasoning, textual description of geometric primitives and relations.",
            "performance_metrics": "Primitive: 84.33% ; Local Relation Composition: 75.66% ; Global Abstract Integration: 42.16% ; ALL: 70.00%.",
            "evidence_of_spatial_reasoning": "Model explicitly cites parsing code and attempts to visualize diagrams in its chain-of-thought; performs well on primitive recognition indicating ability to extract local geometry from code.",
            "comparisons": "Comparable to GPT-o1 on simpler tasks; loses ground relative to top closed/open models at Global Abstract Integration.",
            "limitations_or_failure_cases": "Struggles to reliably form globally consistent spatial representations; single local errors cascade into incorrect global inferences.",
            "uuid": "e8340.2",
            "source_info": {
                "paper_title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Gemini-Pro-1.5",
            "name_full": "Gemini-Pro-1.5",
            "brief_description": "A closed-source multimodal-capable model evaluated on GeoGramBench; exhibits limited ability on program-driven geometry tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-Pro-1.5",
            "model_description": "Proprietary model (Gemini family) evaluated through API calls with identical prompting; included as a closed-source baseline.",
            "model_size": "1.5B (as implied in name)",
            "puzzle_name": "GeoGramBench: Program-to-Geometry",
            "puzzle_type": "procedural geometry problems requiring parsing of drawing code and spatial reasoning.",
            "task_setup": "Zero-shot, problem text + code, 8 samples per problem (temperature 0.6), CoT encouraged in prompt.",
            "mechanisms_or_strategies": "Parsing of code, chain-of-thought reasoning, attempts at diagram reconstruction in text.",
            "performance_metrics": "Primitive: 49.26% ; Local Relation Composition: 31.79% ; Global Abstract Integration: 15.92% ; ALL: 31.64%.",
            "evidence_of_spatial_reasoning": "Some parsing behavior observed but low scores indicate weak spatial abstraction and composition capabilities on program-driven tasks.",
            "comparisons": "Underperforms relative to top closed-source and many open-source models on GeoGramBench; comparatively low across all levels.",
            "limitations_or_failure_cases": "Substantial failure on abstract integration; difficulty resolving CCW/CW and other directional code semantics; iterative CoT rarely yields correct final diagram.",
            "uuid": "e8340.3",
            "source_info": {
                "paper_title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DeepSeek-R1",
            "name_full": "DeepSeek-R1",
            "brief_description": "An open-source reasoning-focused model evaluated extensively; strong at primitive and compositional geometry but limited on abstract integration.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1",
            "model_description": "Open-source model emphasized for reasoning (paper references DeepSeek-R1 work); evaluated across GeoGramBench with standardized prompts.",
            "model_size": null,
            "puzzle_name": "GeoGramBench: Program-to-Geometry",
            "puzzle_type": "procedural geometry problems (diagram construction from Asymptote/Matplotlib code) requiring local and global spatial reasoning.",
            "task_setup": "Zero-shot; model given problem text + drawing code; 8 outputs per instance sampled at temperature 0.6; prompt asks for stepwise reasoning and boxed final answer.",
            "mechanisms_or_strategies": "Parses procedural code into primitives, composes local relations, uses chain-of-thought and iterative self-checking; attempts to visualize and reason about arcs, radii, directions in text.",
            "performance_metrics": "Primitive: 85.66% ; Local Relation Composition: 75.27% ; Global Abstract Integration: 40.38% ; ALL: 69.17%.",
            "evidence_of_spatial_reasoning": "High primitive and compositional scores and qualitative excerpts (e.g., parsing arcs and midpoint constructions) indicate ability to extract local spatial structure; fails in robust global integration.",
            "comparisons": "Comparable primitive/compositional performance to GPT-o1 but falls behind at Global Abstract Integration; open-source top models show size-scaling trends.",
            "limitations_or_failure_cases": "Drops sharply on abstract tasks (&lt;50%); local parsing errors propagate; CoT often repetitive and fails to update global diagram correctly.",
            "uuid": "e8340.4",
            "source_info": {
                "paper_title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "QwQ-32B",
            "name_full": "QwQ-32B",
            "brief_description": "A 32B open-source, reasoning-oriented LLM achieving strong overall GeoGramBench performance, especially at primitive and compositional levels.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "QwQ-32B",
            "model_description": "Open-source 32B model described as reasoning-oriented in the paper; achieved state-of-the-art among open-source models on GeoGramBench.",
            "model_size": "32B",
            "puzzle_name": "GeoGramBench: Program-to-Geometry",
            "puzzle_type": "procedural-code geometry problems requiring translation of drawing code to internal spatial representation and multi-step reasoning.",
            "task_setup": "Zero-shot evaluation using problem text + drawing code (Asymptote/Matplotlib); 8 sampled responses per instance at temperature 0.6; standardized prompt encourages stepwise reasoning.",
            "mechanisms_or_strategies": "Parses procedural code into geometric primitives, composes local relations, uses chain-of-thought reasoning for stepwise integration; robust parsing across drawing languages (Asymptote vs Matplotlib).",
            "performance_metrics": "Primitive: 85.17% ; Local Relation Composition: 73.12% ; Global Abstract Integration: 37.92% ; ALL: 67.20%.",
            "evidence_of_spatial_reasoning": "Minimal language/syntax sensitivity: QwQ-32B shows &lt;1% absolute accuracy difference between Asymptote and Matplotlib versions, supporting that failures are due to deeper spatial abstraction limits rather than code language unfamiliarity; qualitative CoT traces show parsing and visualization attempts.",
            "comparisons": "Top-performing open-source model; compares favorably to many 32B academic models (s1.1-32B, LIMO-32B) and is competitive vs closed-source in primitive/compositional tasks but still below best closed-source on abstract tasks.",
            "limitations_or_failure_cases": "Does not exceed 50% on Global Abstract Integration (37.92%); struggles with global abstraction, recursion, parameterized transformations; CoT often fails to converge to correct global representation.",
            "uuid": "e8340.5",
            "source_info": {
                "paper_title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DeepSeek-v3-0324",
            "name_full": "DeepSeek-v3-0324",
            "brief_description": "An open-source model variant evaluated on GeoGramBench; performs well on primitives and composition but much lower on abstract integration.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-v3-0324",
            "model_description": "Open-source DeepSeek family variant evaluated at scale in this benchmark.",
            "model_size": null,
            "puzzle_name": "GeoGramBench: Program-to-Geometry",
            "puzzle_type": "procedural geometry problems requiring parsing of drawing code and multi-step spatial reasoning.",
            "task_setup": "Zero-shot, text+code input; 8 responses sampled per instance (temp 0.6); standardized prompt with CoT encouragement.",
            "mechanisms_or_strategies": "Code parsing, primitive extraction, chain-of-thought reasoning, iterative verification.",
            "performance_metrics": "Primitive: 80.57% ; Local Relation Composition: 68.89% ; Global Abstract Integration: 27.67% ; ALL: 62.05%.",
            "evidence_of_spatial_reasoning": "Model often reconstructs local geometric elements from code but fails to reliably assemble global abstractions; qualitative traces show stepwise parsing.",
            "comparisons": "Outperformed by DeepSeek-R1 and QwQ-32B on most metrics; drops notably at abstract level compared to primitive/compositional.",
            "limitations_or_failure_cases": "Large drop on abstract tasks; local mis-assignments and directional errors cascade into incorrect problem solving.",
            "uuid": "e8340.6",
            "source_info": {
                "paper_title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "s1.1-32B",
            "name_full": "s1.1-32B",
            "brief_description": "An open-source 32B academic model included as a baseline; moderate primitive/compositional performance, limited abstract capability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "s1.1-32B",
            "model_description": "Academic open-source 32B model evaluated across GeoGramBench levels.",
            "model_size": "32B",
            "puzzle_name": "GeoGramBench: Program-to-Geometry",
            "puzzle_type": "procedural code-based geometry problems requiring spatial parsing and integration.",
            "task_setup": "Zero-shot; problem text plus drawing code; 8 outputs per instance sampled at temperature 0.6; mean accuracy reported.",
            "mechanisms_or_strategies": "Extracts primitives from code, composes local relations, uses CoT but with limited global synthesis.",
            "performance_metrics": "Primitive: 75.37% ; Local Relation Composition: 58.96% ; Global Abstract Integration: 26.58% ; ALL: 54.60%.",
            "evidence_of_spatial_reasoning": "Capable of primitive recognition but frequently fails at global composition; qualitative failure modes described in behavior analysis.",
            "comparisons": "Performs below QwQ-32B and DeepSeek variants; shows expected size/architecture-linked limitations versus reasoning-focused models.",
            "limitations_or_failure_cases": "Low abstract-level accuracy; suffers from local-to-global integration bottleneck described in the paper.",
            "uuid": "e8340.7",
            "source_info": {
                "paper_title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LIMO-32B",
            "name_full": "LIMO-32B",
            "brief_description": "A 32B open-source model evaluated on GeoGramBench with similar behavior to other academic 32B models: reasonable primitive parsing, poor abstract integration.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LIMO-32B",
            "model_description": "Open-source 32B model evaluated in the study; included for scale comparisons.",
            "model_size": "32B",
            "puzzle_name": "GeoGramBench: Program-to-Geometry",
            "puzzle_type": "procedural geometry problems requiring diagram reconstruction and spatial reasoning.",
            "task_setup": "Zero-shot; text+code input; 8 sampled outputs per instance; prompt requires stepwise reasoning and boxed final answer.",
            "mechanisms_or_strategies": "Parsing drawing code, stepwise decomposition, CoT-based reasoning.",
            "performance_metrics": "Primitive: 76.59% ; Local Relation Composition: 59.63% ; Global Abstract Integration: 25.53% ; ALL: 54.98%.",
            "evidence_of_spatial_reasoning": "Shows extraction of local primitives but poor global abstraction; behavior consistent with the hypothesized multi-stage internal representation pipeline.",
            "comparisons": "Similar to other academic 32B models; underperforms reasoning-optimized QwQ-32B.",
            "limitations_or_failure_cases": "Fails to exceed ~26% on abstract problems; local parsing errors propagate.",
            "uuid": "e8340.8",
            "source_info": {
                "paper_title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Bespoke-Stratos-32B",
            "name_full": "Bespoke-Stratos-32B",
            "brief_description": "A 32B open-source model included as a baseline; lower performance on all GeoGramBench levels compared to top 32B models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Bespoke-Stratos-32B",
            "model_description": "Open-source 32B model evaluated in the benchmark; included among academic baselines.",
            "model_size": "32B",
            "puzzle_name": "GeoGramBench: Program-to-Geometry",
            "puzzle_type": "procedural geometry problems requiring spatial parsing and reasoning.",
            "task_setup": "Zero-shot; problem text and code provided; 8 output samples per instance; mean accuracy computed.",
            "mechanisms_or_strategies": "Parses primitives from code, attempts compositional reasoning and CoT.",
            "performance_metrics": "Primitive: 62.50% ; Local Relation Composition: 42.56% ; Global Abstract Integration: 17.02% ; ALL: 40.55%.",
            "evidence_of_spatial_reasoning": "Limited primitive recognition and poor composition; qualitative examples of failure modes in paper apply.",
            "comparisons": "Performs worse than other 32B models like QwQ-32B and DeepSeek variants.",
            "limitations_or_failure_cases": "Large drop with increased geometric complexity; cannot reliably form global representations.",
            "uuid": "e8340.9",
            "source_info": {
                "paper_title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Sky-T1-mini-7B",
            "name_full": "Sky-T1-mini-7B",
            "brief_description": "A 7B open-source model evaluated on GeoGramBench showing moderate primitive recognition but limited higher-level spatial reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Sky-T1-mini-7B",
            "model_description": "Open-source smaller-scale 7B model evaluated to study size scaling effects on program-to-geometry tasks.",
            "model_size": "7B",
            "puzzle_name": "GeoGramBench: Program-to-Geometry",
            "puzzle_type": "procedural geometry problems requiring code-to-diagram translation and spatial reasoning.",
            "task_setup": "Zero-shot; input includes problem text and drawing code; 8 responses per instance sampled at temperature 0.6; mean accuracy reported.",
            "mechanisms_or_strategies": "Parses drawing code into primitives, uses CoT in outputs, but limited compositional and abstract integration.",
            "performance_metrics": "Primitive: 71.45% ; Local Relation Composition: 57.75% ; Global Abstract Integration: 24.79% ; ALL: 52.70%.",
            "evidence_of_spatial_reasoning": "Good primitive recognition relative to model size but declines on compositional/abstract tasks, supporting a size-linked capability trend.",
            "comparisons": "Performs worse than 32B models; follows the observed trend that accuracy decreases with smaller model size.",
            "limitations_or_failure_cases": "Fails to integrate local structures globally; global abstract tasks remain very difficult at this scale.",
            "uuid": "e8340.10",
            "source_info": {
                "paper_title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DeepScaleR-1.5B-preview",
            "name_full": "DeepScaleR-1.5B-preview",
            "brief_description": "A 1.5B open-source model evaluated as a low-parameter baseline; shows limited program-to-geometry capability beyond primitives.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepScaleR-1.5B-preview",
            "model_description": "Open-source 1.5B model evaluated by the authors; included to assess low-parameter performance on GeoGramBench.",
            "model_size": "1.5B",
            "puzzle_name": "GeoGramBench: Program-to-Geometry",
            "puzzle_type": "procedural geometry problems requiring parsing of code into geometric primitives and composing spatial relations.",
            "task_setup": "Zero-shot; problem text + code input; 8 outputs sampled per instance (temperature 0.6); mean accuracy reported.",
            "mechanisms_or_strategies": "Attempts to parse code and reason in chain-of-thought but is constrained by model capacity.",
            "performance_metrics": "Primitive: 65.44% ; Local Relation Composition: 47.89% ; Global Abstract Integration: 15.76% ; ALL: 43.83%.",
            "evidence_of_spatial_reasoning": "Some primitive recognition present, but major deficits in composition/abstract integration consistent with capacity limits.",
            "comparisons": "Performs better than some 1.5B variants but worse than larger models; highlights size-performance scaling.",
            "limitations_or_failure_cases": "Fails on Global Abstract Integration; CoT less effective at low parameter counts; prone to repeating symbolic steps without spatial consolidation.",
            "uuid": "e8340.11",
            "source_info": {
                "paper_title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?",
            "rating": 2,
            "sanitized_title": "mathverse_does_your_multimodal_llm_truly_see_the_diagrams_in_visual_math_problems"
        },
        {
            "paper_title": "GeoSense: Evaluating identification and application of geometric principles in multimodal reasoning",
            "rating": 2,
            "sanitized_title": "geosense_evaluating_identification_and_application_of_geometric_principles_in_multimodal_reasoning"
        },
        {
            "paper_title": "Euclid: Supercharging multimodal llms with synthetic high-fidelity visual descriptions",
            "rating": 2,
            "sanitized_title": "euclid_supercharging_multimodal_llms_with_synthetic_highfidelity_visual_descriptions"
        },
        {
            "paper_title": "Lego-puzzles: How good are mllms at multi-step spatial reasoning?",
            "rating": 2,
            "sanitized_title": "legopuzzles_how_good_are_mllms_at_multistep_spatial_reasoning"
        },
        {
            "paper_title": "AIME24",
            "rating": 1
        }
    ],
    "cost": 0.01857525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs
23 May 2025</p>
<p>Shixian Luo 
Zezhou Zhu 
Yu Yuan 
Yuncheng Yang 
Lianlei Shan 
Yong Wu wuyong5@lixiang.com 
Li Auto 
GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs
23 May 2025545D8C635129035BC27C56736E3D4C07arXiv:2505.17653v1[cs.AI]
Geometric spatial reasoning forms the foundation of many applications in artificial intelligence, yet the ability of large language models (LLMs) to operate over geometric spatial information expressed in procedural code remains underexplored.In this paper, we address this gap by formalizing the Program-to-Geometry task, which challenges models to translate programmatic drawing code into accurate and abstract geometric reasoning.To evaluate this capability, we present GeoGram-Bench, a benchmark of 500 carefully refined problems organized by a tailored three-level taxonomy that considers geometric complexity rather than traditional mathematical reasoning complexity.Our comprehensive evaluation of 17 frontier LLMs reveals consistent and pronounced deficiencies: even the most advanced models achieve less than 50% accuracy at the highest abstraction level.These results highlight the unique challenges posed by program-driven spatial reasoning and establish GeoGramBench as a valuable resource for advancing research in symbolic-to-spatial geometric reasoning.Project page: https://github.com/LiAuto-DSR/GeoGramBench.</p>
<p>Introduction</p>
<p>Spatial reasoning is fundamental to both human cognition and artificial intelligence, supporting applications ranging from robotics and autonomous navigation to automated design [3].With the rise of large language models (LLMs), interest has grown in evaluating their ability to interpret geometric transformations and spatial relations in complex environments [30,25].</p>
<p>Mathematical geometric spatial reasoning is a specialized subdomain of spatial reasoning, requiring models to comprehend intricate geometric relationships and perform deep spatial reasoning.Researchers have recently developed multiple benchmarks including Mathverse [34], GeoSense [29], and Euclid [33] to assess LLMs' capabilities in visual geometry comprehension.Another emerging direction leverages procedural geometric code, such as Asymptote code, as a symbolic and structured interface for expressing geometry problems and probing spatial reasoning.While some existing benchmarks (e.g., AIME24 [20], MATH-500 [34]) include subsets containing Asymptote code, there is a lack of systematic, dedicated benchmarks specifically designed to evaluate LLMs' ability to perform program-driven spatial geometric reasoning.In this work, we formalize this unique setting as the Program-to-Geometry task, referring to the translation and abstraction process from procedural code to internal spatial representations.</p>
<p>Preliminary studies [21] have shown that current LLMs struggle to bridge procedural geometry code to spatial reasoning.We expanded these investigations on a broader range of models further corroborate these observations, confirming this pronounced deficiency.For example, as shown in Figure 1, advanced models such as DeepSeek-R1 [5] suffer substantial drops in accuracy-23.5%</p>
<p> We conduct an extensive evaluation of 17 models, providing accuracy metrics and detailed behavior analyses aligned with our research questions.Our results highlight persistent weaknesses in geometric program reasoning, establishing GeoGramBench as a novel evaluation axis and fostering future advancements in spatially-grounded, symbolically-rich model training and analysis.</p>
<p>Related Works</p>
<p>Visual Geometric Perception To study visual geometric reasoning, several benchmarks such as Euclid [33], MM-Math [24], GeoSense [29], MathVerse [34], and MathVista [18] have been introduced, each incorporating visual geometric content.These datasets measure large multi-modal models' comprehension of visual geometric concepts and their handling of mathematical problems with visual components.Their focus is mainly on diagram interpretation rather than procedural geometric code understanding, which represents a different but equally important aspect of geometric spatial reasoning.</p>
<p>Mathematical Reasoning Benchmarks A diverse array of benchmarks has been developed to evaluate the mathematical reasoning abilities of large language models (LLMs).Datasets such as GSM8K [2], MATH-500 [15], OlympiadBench [6], Minerva-MATH [12], CollegeMath [26], MMLU-STEM [7], and AIME24 [20] primarily focus on algebraic, arithmetic, and word-problem reasoning.Many of these benchmarks target complex multi-step solutions, ranging from advanced high school mathematics to the level of international mathematical olympiads.</p>
<p>3 Program-to-Geometry</p>
<p>Task Definition</p>
<p>We define Program-to-Geometry as the task in which a model interprets procedural code to construct mathematical geometric representations, and subsequently reasons over these representations to solve geometry problems.This paradigm provides a comprehensive assessment of two fundamental capabilities: (a) the ability to accurately construct mathematical geometric diagrams from symbolic instructions, and (b) the ability to perform spatial reasoning and mathematical problem solving based on these constructed diagrams.</p>
<p>Taxonomy</p>
<p>Taxonomies for problem categorization are widely used across various fields, often focusing on dimensions such as topological complexity [35], logical intricacy [16], or the extent of required reasoning complexity (e.g., high school, graduate, olympiad-level) [20,23,8].The Program-to-Geometry task fundamentally differs from these settings: it specifically examines the ability to map geometric code representations to geometric diagram understanding.Our preliminary analyses reveal that existing categorization schemes fail to capture the unique aspects and challenges of this space.Consequently, we propose a tailored taxonomy that better reflects the core competencies required for Program-to-Geometry translation.As shown in Figure 2, based on reasoning difficulty annotations provided by the MATH-500 [15] dataset, difficulty is similarly distributed between text-only and text+code geometry problems, yet model performance diverges sharply.For instance, models like QwQ-32B perform worse on the easiest text+code problems than on the hardest, suggesting that reasoning complexity alone is not the determining factor.</p>
<p>Instead, we propose a taxonomy whose primary principle is the construction of increasingly complex mathematical geometric diagrams from code.Our three-level categories are determined chiefly by the types and number of geometric elements involved, while also reflecting the depth of spatial reasoning required for each problem (see Figure 3):</p>
<p> Primitive Recognition: Problems involving procedural code that specify only one or two geometric primitives (e.g., points, lines, arcs, circles, polygons), focusing on basic mathematical properties such as length, area, or angle.</p>
<p> Local Relation Composition: Problems with multiple local geometric elements, requiring the recognition, integration, and composition of spatial relationships among subcomponents of the diagram.</p>
<p> Global Abstract Integration: Items demanding spatial direction, parameterization, recursion, 3D objects, composite structures, or advanced geometric operations (e.g., rotation, folding, projection), thus requiring not only the construction of complex diagrams but also global and stepwise spatial reasoning across the entire configuration.</p>
<p>Research Questions</p>
<p>Based on this task definition and taxonomy, we articulate the following research questions to structure our analysis of LLMs behavior in the Program-to-Geometry context:</p>
<p>Benchmark Construction</p>
<p>In this section, we present the systematic construction process of GeoGramBench, a dedicated benchmark for Program-to-Geometry reasoning.We first introduce a critical challenge inherent to this task domain-answer leakage-before detailing our comprehensive data construction pipeline that forms the foundation of our benchmark (more details in Appendix D).Left-Direct leakage, where the answer is explicitly given by a coordinate value in the Asymptote code (here, we rescale the coordinates to preserve the geometric shape); Right-Indirect leakage, where the answer can be computed from code parameters (in this case, we modify the procedural code to mask such critical information).</p>
<p>Answer Leakage Challenges</p>
<p>In the Program-to-Geometry task, a significant challenge arises from the potential for answer leakage within the code itself.The program code that generates geometric figures often contains precise numerical specifications that directly or indirectly reveal the answers sought.Benchmark like Math-500 [15], we discovered numerous instances where answers were directly embedded in the Asymptote code.Similar issues persist across various open-source geometry problem collections we collected.As illustrated in Figure 4, we categorize two types of answer leakage in the procedural code.Direct leakage occurs when the answer is explicitly encoded as a coordinate value in the Asymptote code (e.g., a circle's radius or segment's length).Indirect leakage occurs when the answer can be computed from code parameters or formulas.</p>
<p>Collection and Preprocessing</p>
<p>We first aggregated approximately 905K candidate problems from three open-source mathematics datasets, including NuminaMath-1.5 [14], HARP [32], and Omni-MATH [4], with a focus on sources rich in geometry content.We filtered for problems containing embedded Asymptote code by searching for [asy] and [/asy] tags, resulting in a subset comprising about 1% (9,260 problems).We then deduplicated this subset using an n-gram (n = 8) similarity approach [21], reducing the set to 1,782 unique items.Finally, by following the schema from s1 [21] and leveraging GPT-4o [9] for prompt-based classification, we selected only geometry problems, yielding 1,247 geometry-focused items for subsequent curation.</p>
<p>Human Refinement and Verification</p>
<p>To ensure data quality and suitability for geometry code understanding tasks, we implemented a two-stage manual verification process, conducted by a team of four experts (each holding a master's degree or higher in mathematics or related fields).The first round aimed to standardize problem types and formats, while the second round focused on enhancing overall problem quality.</p>
<p>In the first round, we performed initial screening and format normalization: (a) non-relevant questions (such as hyperlink chains, multi-part items, and proofs) were filtered out according to best practices from BigMath [1]; (b) convertible multiple-choice questions were transformed into open-form computation problems by removing options, while those not amenable to conversion were discarded entirely; and (c) answers were standardized into consistent L A T E X format.At the end of this screening, 547 candidate problems remained.</p>
<p>In the second round, we implemented a rigorous three-pronged refinement process to improve problem quality:</p>
<p> Decontamination: To minimize community-sourced contamination, we systematically revised problem statements by removing redundant descriptive information that might enable direct textual inference.Additionally, we adjusted problem conditions and modified corresponding answers to maintain mathematical consistency.Furthermore, we adjusted the answer requirements (such as replacing queries about lengths with those about area, volume, or ratios) to further reduce the risk of leakage and promote authentic geometric reasoning.</p>
<p> Answer Leakage Prevention: As detailed in Section 4.1, to address this task-specific vulnerability, we implemented two targeted strategies: systematically rescaling coordinates while preserving geometric relationships for direct leakage, and modifying or masking code parameters for indirect leakage.These interventions ensure that answers cannot be derived through mere code inspection (see Figure 4).</p>
<p> Accuracy Verification: Each answer was manually checked for correctness; items with ambiguous, unverifiable, or doubtful solutions were removed.</p>
<p>Through this thorough process, we ultimately obtained 392 high-quality, contamination-free geometry problems for augmentation and evaluation.</p>
<p>Benchmark Augmentation</p>
<p>To enhance difficulty balance and problem diversity, we supplemented GeoGramBench with additional items: 5 geometry problems from AIME24 [20], 42 from MATH-500 [15], and 61 geometric problems adapted from Mathverse [34].For the Mathverse subset, we selected representative solid geometry problems and manually transcribed diagrams into matplotlib code to diversify the procedural drawing code within the dataset.Our experiments indicate minimal impact from the choice of drawing language (see Appendix A).Altogether, GeoGramBench comprises 500 geometry problems, supporting robust evaluation across a variety of geometric phenomena.</p>
<p>Difficulty Categorization</p>
<p>Building on our theoretical and empirical insights in Section 3.2, we categorize all 500 GeoGram-Bench problems into three ascending difficulty levels-Primitive Recognition, Local Relation Composition, and Global Abstract Integration-based on the type and number of geometric elements and the spatial relationships involved (see Figure 3).The categorization is implemented through a combination of GPT-4o [9] assisted classification and thorough human expert review.The final distribution comprises 102, 279, and 119 problems for each category, respectively.GeoGramBench thus stands as the largest and most diverse Program-to-Geometry benchmark to date, establishing a rigorous testbed for spatially grounded language model evaluation.</p>
<p>Experiment</p>
<p>We benchmark 17 popular LLMs on GeoGramBench, providing a broad comparative analysis in this section.Section 5.1 details our evaluation framework and prompt engineering strategies.Section 5.2 introduces the tested models, followed by quantitative comparisons in Section 5.3.</p>
<p>Evaluation Protocols</p>
<p>For open-source models, we adopt the [19] framework for evaluation, while for closed-source models, we utilize official APIs with identical prompt templates(Let's think step by step and output the final answer within \boxed{}.).All result parsing is standardized using [19], with assistance from GPT-4o when necessary.Each problem is evaluated in a zero-shot setting: the model input consists strictly of the problem text and the procedural geometry drawing code.For each problem instance, we sample 8 responses using temperature 0.6, and report final accuracy as the mean over these 8 outputs, which balances model stochasticity and answer reliability.</p>
<p>Evaluation Models</p>
<p>We evaluate a total of 17 mainstream LLMs, including both proprietary APIs and leading opensource systems.The closed-source models include GPT-4o [9], GPT-o3-mini [22], the GPT-o1 series [10], and Gemini-Pro-1.5 [27].The open-source models cover a wide range of scales, including DeepSeek-R1 [5], DeepSeek-v3-0324 [17], and QwQ-32B [28], as well as other prominent models from 32B down to 1.5B parameters: DeepSeek-R1-Distill variants [5], Bespoke-Stratos-32B [11], s1.1-32B [21], LIMO-32B [31], Sky-T1-mini-7B [13], and DeepScaleR-1.5B-preview[19].</p>
<p>Main Results</p>
<p>As shown in Table 1, all tested LLMs perform strongly on the Primitive Recognition, but accuracy drops steadily as geometric complexity increases.This downward trend is evident at the Local Relation Composition level and becomes most pronounced on Global Abstract Integration, where the highest accuracy is only 43.35% across all models.</p>
<p>Although GPT-o1 achieves a similar accuracy (86.76%) to DeepSeek-R1 (85.66%) in the Primitive Recognition category, a substantial performance gap emerges when evaluating Global Abstract Integration.GPT-o1 scores 43.35% in this more complex domain, whereas DeepSeek-R1 reaches only 40.38%.This significant difference may indicate a unique strength of closed-source models over open-source models in handling the most challenging tasks that require complex, abstract reasoning.</p>
<p>For open-source models, we observe a clear downward trend in performance as the model size decreases from 32B to 7B parameters.At the 32B scale, the reasoning-oriented QwQ-32B model achieves state-of-the-art results.However, academic models of similar scale, such as s1.1-32B and LIMO-32B, still exhibit a notable gap in performance compared to QwQ-32B.</p>
<p>Behavior Analysis of LLMs</p>
<p>We address our RQs through both quantitatively and qualitatively analyses base on benchmarking results and detailed model responses.</p>
<p>RQ1: Is there evidence that LLMs can understand and represent basic geometric elements from program code?</p>
<p>RQ1 investigates the fundamental ability of LLMs to recognize basic geometry elements, which can be quantitatively measured by the evaluation results of Primitive Recognition.As shown in Table 1, most of the models achieve 60% accuracy on the Primitive Recognition level, suggesting that they can effectively parse and build basic geometric scenes from procedural codes.Qualitatively, some of the model responses explicitly reveal the capability to interpret and reconstruct geometric information.As shown in Figure 5, models frequently examine the procedural code for geometry understanding: "Now, looking at the Asymptote code", "Let me parse the Asymptote code a bit", and "maybe I should try to visualize this".They can also identify simple geometric relationships according to the procedural code.For example, "c is (2,0), so c/2 is (1,0).So the inner arc is between points a/2 and c/2", and "path inner = arc(d, a/2, c/2, CW);...path outer = arc(d, c, a, CCW);".These behavior demonstrate that LLMs are intent and capable to map procedural code into internal geometric structures.In conclusion, modern LLMs are able to construct basic geometric representations from procedural code.</p>
<p>RQ2: How effectively can LLMs compose and abstract geometric elements into coherent spatial configurations as specified by program code?Response:Okay, so I need to find the measure of angle ADC in degrees,... Hmm, let's start by understanding the problem step by step.First, the larger circle has a radius of 2, ... Now, looking at the Asymptote code, it seems like the shaded regions are two parts... Wait, actually, looking at the Asymptote code... Let me parse the Asymptote code a bit...between the two arcs?Wai t, maybe I should try to visualize this.Since both circles are centered at D,... Since both circles are centered at D, the inner circle has radius 1, and the outer has radius 2. Points A and C are on the outer circle..., then along the outer arc back to C and then to A? Wait, maybe I'm getting confused here.Let me think again.... Since a is a point on the outer circle (r adius 2), a/2 would be the same direction but radius 1.Similarly, c is (2,0), so c/2 is (1,0) , path inner = arc(d, a/2, c/2, CW);... path outer = arc(d, c, a, CCW); going counterclockwise.....So that's an arc from a/2 to c/2 going (which would be the other direction compared to the inner counterclockwise path before), and then connecting back to D and forming a triangle?...Hmm, this is a bit confusing without seeing the actual diagram.Since I can't see the diagram, maybe I should proceed with the information give...Alternatively, maybe the shaded area is the area of the larger sector minus the area of the sm aller sector, plus some other regions?Hmm, perhaps I need to break it down.Maybe I should approach it mathematic ally.Wait, perhaps it's the area of the larger segment minus the smaller segment?...Alternatively, let me try to compute the area step by step.Fir st, Hmm, not sure...So starting at a/2, going counterclockwise ...Wait, but then why is it shaded?Maybe region2 is actually the triangle?Wait, no, because the path is the arc and then the two radii, so it's a sector.Hmm.Wai t, but is that correct?..,Wait, but maybe I miscomputed region1.according to the Asymptote code, region1 is (a-a/2)..inner..(c/2--c)..outer..cycle.So that path starts at A, goes to a/2 (midpoint?),... maybe I made a mistake in calculating the area of region1.Let me recast the problem... follows the inner arc to c/2, then to c, then follows the outer arc back to A. So that area is like a closed loop that goes from A down to the inner circle Let me check again.... Essentially, it's the area of the larger sector ( from C to A counterclockwise) minus the area of the sm aller sector (from C' to A' clockwise).Hmm, but let me check again ...Let me see the Asymptote code again: region2 is arc(d, a/2, c/2, CCW)..(c/2--d--a/2)..cycle.Hmm, that seem s straightforward, but let me check again... Alternatively , maybe I got the direction of the angle wrong?Let me think: ...Thus, the measure of ( \angle ADC ) is ( \boxed{150}).</p>
<p>It exemplifies the model's motivation to move from code parsing to spatial interpretation.</p>
<p>Although the model repeatedly attempted to reason, engaging in multiple rounds of self-questioning and re-examination of the Asymptote code, it was ultimately unable to arrive at the correct approach.According to the results in Table 1, all models experience a significant drop in accuracy from Compositional problems to Global Abstract Integration.For example, GPT-o1 drops from 76.02% to 43.35%, and DeepSeek-R1 drops from 75.27% to 40.38%.These results indicate that current LLMs may lack of compositional and spatial abstraction ability to solve complex geometry problems.Qualitatively, while models can often parse and assemble some local structures, small errors in local constructions frequently appear, preventing LLMs to construct a complete and coherent global representation.As illustrated in Figure 5, a model may read a piece of code like "path inner = arc(d, a/2, c/2, CW)" and reason about directions ("which would be the other direction compared to the inner counterclockwise path before"), but a single mistake in local spatial assignment may generate downstream confusion: "maybe I got the direction of the angle wrong?... the actual angle between the points is \theta, so the area calculations still hold.".This phenomena suggests that modern LLMs may not good at capturing complex compositional geometry relationships for high level spatial reasoning.In summary, although LLMs have made progress in local geometric parsing, their ability to synthesize and reason over globally consistent spatial structures in Program-to-Geometry tasks remains limited.</p>
<p>RQ3: How does chain-of-thought (CoT) reasoning influence LLMs' spatial geometric reasoning abilities with program code?</p>
<p>Quantitatively, we observe a clear downward trend in accuracy as structural complexity rises.Since our benchmark taxonomy is based on geometric complexity rather than reasoning steps, this result suggests that most of the LLMs have difficulty in solving mathematical geometry problems with CoT.Qualitatively, while models frequently perform iterative self-reflection and verification of code ("Let me check again"), and repeatedly parse diagram instructions, their CoT trajectories rarely correct or update internal geometric understanding.For instance, the model may cycle through algebraic steps and verbalize uncertainty ("Hmm, this is a bit confusing without seeing the actual diagram.Since I can't see the diagram, maybe I should proceed with the information given."),yet consistently fails to resolve spatial relationships or integrate local shapes into a whole.This observation illustrates that CoT may lead LLMs fall into repetitive symbolic reasoning.Such repetitiveness does not beneficial for LLMs to construct high level spatial representations as a whole, even leading to confusion about complex geometry relationships.Although CoT improves LLM in mathematical reasoning, its ability to drive and update internal geometry understanding in complex spatial tasks remains fundamentally limited.</p>
<p>Discussion</p>
<p>A Hypothesis on Internal Geometric Representations in LLMs</p>
<p>Drawing on both quantitative results and behavior analyses, we hypothesize that large language models confronted with procedural geometry code engage in a multi-stage internal reasoning process closely aligned with the pipeline illustrated in Figure 6.</p>
<p>The process begins with the extraction of local geometric features or substructures ({z 1 , z 2 , . . .}) from the input text and code ({T, C}), corresponding to the abilities probed in RQ1.Our evidence shows that models are generally able to parse and represent these local primitives with high accuracy in simpler cases.</p>
<p>The next critical stage involves integrating these local elements into a coherent, global representation (Z 1 ), reflecting the compositional reasoning explored in RQ2.This is where we observe a pronounced bottleneck: small errors or ambiguities in local geometry can disrupt subsequent steps, making it difficult for models to build a structurally correct and complete diagram as complexity increases.</p>
<p>Subsequently, models iteratively attempt to update and refine their global geometric understanding, often through chain-of-thought (CoT) reasoning or self-reflective steps, in hopes of reconciling inconsistencies and clarifying spatial relationships.Despite such iterative efforts, our analysis of model outputs indicates that most fail to achieve robust global integration, as highlighted by the continued drop in accuracy and recurring spatial confusion on the most complex tasks (RQ3).</p>
<p>Finally, the model produces an answer (A), leveraging whatever spatial structure has been successfully constructed and refined.Our overall findings suggest that while LLMs can recognize and extract local geometric information, and to some extent initiate the integration process, there remain significant limitations in aggregating and refining these components into a globally consistent geometric representation for accurate problem solving.Overcoming these integration and synthesis difficulties is likely to be a key research frontier for closing the gap in Program-to-Geometry spatial reasoning.</p>
<p>These findings point to the need for future research on more robust scene composition and iterative spatial integration mechanisms in LLMs, as well as the development of benchmarks and training strategies tailored to these specific bottlenecks.</p>
<p>{T, C} {z
1 , z 2 , . . . } Z 1 Z 2 A  RQ1  RQ2  RQ3</p>
<p>Conclusion</p>
<p>This work introduces the Program-to-Geometry task, which tests the capability of LLMs to map program code into geometric space, and GeoGramBench as a systematic benchmark for evaluating such geometric spatial reasoning abilities.Through a comprehensive analysis of 17 leading LLMs, we find that while models perform well on simple geometric constructions, their accuracy declines sharply for problems with higher geometric complexity-none surpassing 50% on the most advanced level.Our results highlight persistent challenges in complex geometric reasoning and emphasize the need for targeted advances in model design and training.GeoGramBench provides a robust foundation for future research on symbolic-to-geometric understanding in AI.</p>
<p>A Effect of Drawing Language on Program-to-Geometry Performance</p>
<p>A key motivation for our investigation is to determine to what extent challenges in Program-to-Geometry reasoning arise from the logic of geometric construction itself, rather than from surface-level code syntax or unfamiliarity with specific drawing languages.To test this, we translated 5 geometry questions containing Asymptote code from AIME24 and 42 questions from MATH-500 into equivalent Python matplotlib code, holding geometric content constant while varying only the programmatic language.As shown in Figure 7, QwQ-32B exhibits less than 1% difference in absolute accuracy between the Asymptote and Matplotlib versions on both benchmarks.This minimal gap provides strong evidence that the principal bottleneck in Program-to-Geometry task performance is not due to the choice of drawing language, but rather stems from deeper difficulties in spatial abstraction and geometric reasoning from code.This result reinforces our conclusion that surface syntax is not the main limiting factor for LLMs in this domain.
plt.plot([E[0], D[0], A[0], B[0], C[0], E[0]], [E[1], D[1], A[1], B[1], C[1], E[1]], color='black') plt.plot([E[0], H[0], G[0], F[0], C[0]], [E[1], H[1], G[1], F[1], C[1]],</p>
<p>B Taxonomy Classification Prompt Details</p>
<p>In constructing the GeoGramBench taxonomy, we categorized all 500 problems into three ascending difficulty levels-Primitive Recognition, Local Relation Composition, and Global Abstract Integration-based primarily on the geometric and spatial complexity of each problem.This classification process was conducted through a combination of large language model (GPT-4o) assisted clustering and meticulous human expert correction.The initial clustering enabled an efficient, scalable filtering of geometry problems, while human review ensured rigor, consistency, and alignment with the intended definitions of each difficulty level.</p>
<p>To ensure reproducibility and transparency, we provide below the actual LLM prompt used in the taxonomy assignment stage:</p>
<p>Given a geometry problem and its drawing code of diagram: There are three categories of geometry problems:</p>
<p>C Preventing Information Leakage in Procedural Geometry Code</p>
<p>A critical aspect of dataset curation for Program-to-Geometry evaluation is the prevention of information leakage through the procedural drawing code.In this context, information leakage refers to situations where the answer to a geometry problem is either explicitly or implicitly encoded in the program, enabling a model (or human) to bypass genuine geometric reasoning and instead extract the solution directly from code inspection.</p>
<p>We identify two primary forms of leakage:</p>
<p> Direct leakage: The answer appears explicitly in the code, for example as a coordinate, length, or parameter value (e.g., a circle radius or segment described directly in the Asymptote code).</p>
<p> Indirect leakage: The answer can be inferred by performing simple calculations or extracting formula results from the parameters or structure of the code, even though it is not written verbatim.</p>
<p>To mitigate these risks, we systematically reviewed all procedural code in the dataset.For direct leakage, critical coordinates and parameters are rescaled or randomized while preserving the diagram's structure.For indirect leakage, problem variables and code formulas are modified or masked to preclude simple reverse engineering of the answer.</p>
<p>Below we present concrete examples comparing original and mitigated code for selected problems.Each example includes its problem statement and paired Asymptote code, annotated as "before" and "after" modification.A--E); draw(B--F); label(\"$A$\",A,N); label(\"$B$\",B,W); label(\"$C$\",C,dir(0)); label(\"$E$\",E,S); label(\"$F$\",F,NE); label(\"$G$\",G,SE);</p>
<p>Answer: 8</p>
<p>Before modification (Leakage present): size(4cm); draw((0,4)-(0,0)-(6,0)-(6,8) -(0,8)-(0,4)-(6,8)-(0,0)); label(\"$A$\", (0,0), SW); label(\"$B$\", (6, 0), SE); label(\"$C$\", (6,8), NE); label(\"$D$\", (0, 8), NW); label(\"$M$\", (0, 4), W);</p>
<p>After modification (Leakage mitigated): size(4cm); draw((0,2)--(0,0)--(3,0)--(3,4) --(0,4)--(0,2)--(3,4)--(0,0)); label("$A$", (0,0), SW); label(\"$B$\", (3, 0), SE); label(\"$C$\", (3,4), NE); label(\"$D$\", (0, 4), NW); label(\"$M$\", (0, 2), W);</p>
<p>D Detailed Benchmark Curation</p>
<p>We assemble a team of four experts (each holding a Master's degree or higher in mathematics or related fields) to ensure data quality.Our team manually verifies and refines samples from three aspects: question reformulation and standardization, decontamination, answer verification and leakage prevention.</p>
<p>D.1 Question reformulation and answer standardization</p>
<p>Question reformulation The formulation of each sample in GeoGramBench should be simple QA pairs for convenient evaluation.To achieve this, we start to deal with multiple choice questions, proofbased questions and multi-part problems, which are not in QA format.Multiple choice questions can be transformed into open-ended computation problems by preserving the correct choice as the answer and removing all other choices.Some of the proof-based questions can be transformed into computation problems (like "Prove that P A = 4P B" can be rewrite to "Compute the ratio between P A and P B"), whereas others are not suitable for such transformation (like "Prove that AB  3P R).Multi-part problem always consists of several sub-problems, which can be simplified into a single question format by retaining one of the computable sub-questions.Questions amenable to conversion can be retained and reformulated into new QA samples, while others may be excluded from the benchmark.According to the aforementioned rules, our team members carefully assess the formulation of each question and perform corresponding modifications and deletion.</p>
<p>Answer standardization Considering the diversity and complexity of mathematical expressions, answer standardization is crucial for accurately evaluating model-generated responses.Our team manually modify the answer of each question by removing arithmetic operators (like +, ), letters and characters that irrelevant for computation and evaluation (like \text{cm^2}), and standardize each answer into L A T E X format as simple as possible (like simplify \frac{28}{\sqrt{7}}} to 4\sqrt{7}).The above operations successfully ensure the consistency of question formulation and answer standardization, which benefits subsequent data processing and contributes reliable benchmarking.The resulting subset contains 547 candidate samples.</p>
<p>D.2 Decontamination</p>
<p>Most of the samples we collected originates from public datasets and internet resources, which indicates a high possibility that these data has already been included in the LLM's pre-training corpora.Besides, current data samples contains a certain degree of redundancy and unnecessary information, which may introduce unexpected bias to benchmarking.To mitigate the above influences as much as possible, our team manually perform data decontamination for all the 547 samples from three aspects:</p>
<p>Extraneous information removal We believe hyperlinks and code comments are not only unnecessary information for mathematic geometry spatial reasoning, but also introduce text bias for mathematic geometry problem reasoning.As a result, each member in our team carefully examine and delete all these contents in each question;</p>
<p>Problem statement rephrasing To prevent samples from being solved solely based on question statement, encourage LLM focus on mathematic geometry spatial reasoning, we reduce some comprehensive and specific mathematical expressions in question text.To minimize the overlap between LLM pre-training corpora and benchmarking samples, our team modifies the given condition and question objective of some samples;</p>
<p>Coordinate modification In some samples, the coordinates used to generate pictures are identical to the given conditions in the problem statement, which may enable LLM to derive answer through algebraic geometry reasoning based on text solely.Such problem solving approach cannot effectively evaluate the mathematic geometry spatial reasoning ability of LLM.To decrease the possibility of LLM using algebraic geometry problem solving approach, we adjust the coordinates in each samples program code, which maintains the geometric shape and relationship of the original picture.The above decontamination methods ensures each item in GeoGramBench is a completely new sample, contributing to valuable and reliable mathematic geometry spatial reasoning benchmarking.</p>
<p>D.3 Answer Verification and Leakage Prevention</p>
<p>Answer verification We observe that some of the original answers are wrong to the corresponding questions after decontamination.To avoid such circumstances, we carefully verify the answer of each sample one by one by both referencing the original question from the Internet and calculate answer by ourself.The QA pairs that cannot be searched on the Internet are removed.</p>
<p>Answer leakage prevention We find some of the correct answers are already leaked in the code of samples during verification.As shown in Figure 9, 10, the answer can explicitly equals to the answer, or implicitly computed according to the code for generating image.This situation may allow LLM access the answer in advance, which harm to the evaluation of mathematic geometry spatial reasoning.To prevent answer leakage, our team manually revised the code for all samples once again by rescaling coordinates and masking codes with numbers.Answer verification and leakage prevention guarantee the correctness of all the samples and the fairness of benchmarking.</p>
<p>After human verification and refinement, we ultimately obtained 392 high-quality, contamination-free geometry problems for later augmentation and evaluation.</p>
<p>D.4 Augmentation</p>
<p>We introduce additional samples to enhance difficulty and diversity of GeoGramBench: 5 geometry problems from AIME24 [20], 42 from MATH-500 [15], and 61 geometric problems adapted from Mathverse [34].The 47 samples from AIME24 and MATH-500 are retained without modification dur to their high quality.For the Mathverse subset, we first filter 119 samples with two key words: Vision Intensive and Solid Geometry.These samples focus on solid geometry questions, with the majority of problem solving information presented in image.This advantages makes them highly suitable for mathematic geometry spatial reasoning evaluation.However, Mathverse only provides the original images without the plotting code for reproducing the picture.Thus, our team decide to write python matplotlib code with our own to construct new evaluation samples in GemGramBench.Notably, we do not ask for multimodal models (like GPT-4o) for help because such models performs poorly when transforming solid geometry picture to matplotlib code.</p>
<p>Altogether, GeoGramBench comprises 500 hand-crafted geometry problems, which contributes to valuable and reliable mathematic geometry spatial reasoning evaluation.7, -2)--cycle); label("$A$", A, SW); label("$B$", B, NW); label("$C$", C, NE); label("$D$", D, SE); pair E, F; E = (4.5-.2,1-.2);F = (5, 3); draw(A--E--D); draw(A--F--D); label("$E$", E, N); label("$F$", F, NW); dot(A);dot(B);dot(C);dot(D);dot(E);dot(F); label("$x$", (1, 1.5), S); label("$x$", (2, 1), S+W); label("$x$", (2, -1), N+N+N+W); label("$y$", (5.5+.3, .5-.3), S); label("$y$", (6.5+.3, 0)); label("$y$", (5+.5, -1.5+.3)); label("$110^{\circ}$",(2.5,3.5));label("$100^{\circ}$",(6.5-.2,3.5));, then angles at A and D would need to be 360 -210 = 150 u00b0, so angle BAD + angle CDA = 150 u00b0, so if angle BAD is trisected into 3x and angle CDA into 3y, then 3x + 3y = 150 u00b0, so x + y = 50 u00b0, then angle AFD would be 180 -x -y = 130 u00b0?Wait, no, in triangle AFD, angles at A and D would be x and y, so angle AFD = 180 -(x + y) = 180 -50=130 u00b0, but that contradicts coordinate.Alternatively, maybe angle at B is 110 u00b0, angle at C is 100 u00b0, then angles at A and D would need to be 360 -210 = 150 u00b0, so angle BAD + angle CDA = 150 u00b0, so if angle BAD is trisected into 3x and angle CDA into 3y, then 3x + 3y = 150 u00b0, so x + y = 50 u00b0, then angle AFD would be 180 -x -y = 130 u00b0?Wait, no, in triangle AFD, angles at A and D would be x and y, so angle AFD = 180 -(x + y) = 180 -50=130 u00b0, but that contradicts coordinate.Alternatively, maybe angle AFD is an external angle or something else ...... which is 80.  Okay, let's see.I need to find the length of BH in the figure provided...Hmm, I should start by trying to understand the coordinates of the points based on the Asymptote code given.First, looking at the Asymptote code, they define point A at (0,0), B at (3,0), C at (6,0), D at (9,0), E is labeled as Ep which is (9,3), and G is at (6,3).So, let me note down these coordinates: -A: (0, 0) -B: (3, 0) -C: (6, 0) -D: (9, 0) -E: (9, 3) -G: (6, 3) ... Alternatively, maybe the square is oriented differently?But since the problem didn't specify orientation, but the Asymptote code had coordinates with C at (6,0), D at (9,0), E at (9,3), G at (6,3).Wait, but maybe it's better to compute coordinates step by step....Alternatively, since BEFH is a rectangle, then H must be such that BH is equal and parallel to EF, and FH is equal and parallel to BE.Alternatively, since BE is from B to E, then to get to F from E, we move in the direction perpendicular to BE ... Alternatively, perhaps I chose the direction incorrectly.Let me try with the other perpendicular direction.If instead of (3,4), we take (-3,-4).Then vector EF would be (-3,-4).Then point F would be E(3,3) + (-3,-4) = (0,-1).Then vector FH would be equal to BE? Wait, no.Wait, in a rectangle, opposite sides are equal.So FH should be equal and parallel to BE. Hmm, perhaps I need to think differently.Alternatively, since BE is one side, and EF is another side, then FH should be equal and parallel to BE, and HB should be equal and parallel to EF.So if vector BE is (-4,3), then vector FH should also be (-4,3).Starting from F, adding vector BE gives H = F + BE.Wait  (6,3) and (12,6), which are in the same direction (since (12,6)=2<em>(6,3)).So the angle between them is zero, so the bisector is the same line.Hence, the angle bisector is undefined, or the line itself.So H0 is along that line.... Ah, yes!Because vector BE is (6,3), and vector HB is (3/5, -6/5).Their dot product is indeed zero.So the calculation is correct.Therefore, the answer is 1 .</em><em>Final Answer</em><em> 1 Given quadrilateral (CDEG) is a square with (CD = 3) and quadrilateral (BEFH) is a rectangle with (BE = 5), we need to find the length of (BH). 1. </em><em>Coordinates and Setup</em>*: -In the Asymptote code, points (C), (D), (E), and (G) form a square with (CD = 3).The coordinates are (C(6,0)), (D(9,0)), (E(9,3)), and (G(6,3)).</p>
<p>E More Behavior Analysis of LLMs
() =  F A     F D |  F A||    F D| = 5  34  29 = 5 986
-The problem states (BE = 5), but in the Asymptote coordinates, (BE) is calculated as ( (9  3) 2 + (3  0) 2 =  45 = 3  5).To scale this to (BE = 5), the scaling factor is ( 5
3  5 = 5
3 ).2. <strong>Finding Coordinates of (H)</strong>: -In the Asymptote figure, line (AG) is from (A(0,0)) to (G(6,3)).The parametric form of (AG) is ((6t, 3t)).-Point (F) on (AG) is found by ensuring (EF) is perpendicular to (BE).Solving, (F) is at ( 42 5 , 21  5 ).-Point (H) is determined by the rectangle properties, resulting in ( 125 , 6 5 ).</p>
<p>3 ) is applied to the Asymptote length of (BH), resulting in ( 3</p>
<p>F Limitation and Future Work</p>
<p>Although GeoGramBench currently focuses on procedural code in geometry, the framework and insights developed here may generalize to broader domains where procedural descriptions interact with spatial or relational reasoning.Our present analysis is largely empirical and focuses on observable model behavior, without providing deeper theoretical explanations for these shortcomings.In future work, we plan to conduct more in-depth investigations into the underlying causes of failures on Program-to-Geometry tasks using this dataset, and to explore reinforcement learning as well as other targeted training strategies to explicitly enhance spatial reasoning and abstraction in LLMs.We encourage further research to expand upon this benchmark, develop more sophisticated probing methods, and systematically explore model behaviors under diverse procedural spatial contexts, ultimately advancing a deeper understanding of spatial reasoning capabilities in large language models.</p>
<p>Problem:</p>
<p>The area of ABC is 6 square centimeters.  . = 4.What is the number of square centimeters in the area of  ?[asy] draw((-.3,-3)--(.1,1)--(-1,0)\ --(3,0)--cycle); label("$A$",(.1,1),N);label("$B$",(-1,0),W); label("$C$",(0,0),NE); label("$D$",(3,0),E); label("$E$",(-.3,-3),S);[/asy] Answer: 54 visualize (a) Example of a problem from PT C in MATH-500.</p>
<p>Accuracy comparison of models on PT vs. PT C in AIME24.</p>
<p>Accuracy comparison of models on PT vs. PT C in MATH-500.</p>
<p>Figure 1 :
1
Figure 1: Overview and performance analysis on text-only (P T ) and text+code (P T C ) geometry problems.(a) The procedural code is wrapped with [asy][/asy] and its geometric figure is visualized to facilitate understanding.(b) and (c) show accuracy comparisons of models on P T and P T C subsets in AIME24 (|P T C | = 5, |P T | = 25) and MATH-500 (|P T C | = 42, |P T | = 458), respectively.In both benchmarks, accuracy consistently drops for problems with procedural code.</p>
<p>Figure 2 :
2
Figure 2: Distribution of problem difficulty levels and QwQ-32B accuracy for text-only (P T ) vs. text+code (P T C ) geometry problems on MATH-500.</p>
<p>RQ1: 5 .
5
Is there evidence that LLMs can understand and represent basic geometric elements from program code?RQ2: How effectively can LLMs compose and abstract geometric elements into coherent spatial configurations as specified by program code?RQ3: How does chain-of-thought (CoT) reasoning influence LLMs' spatial geometric reasoning abilities with program code?Problem: In , suppose  = 3,  = 27, and  = 48.What is the value of ? [asy] pair A=origin, B=(14,0), C=(10,6); draw(A--B--C--cycle); label("$A$ ", A, SW); label(" $B$ ", B, SE); label(" $C$ ", C, N); ... [/asy] Answer: 35 visualize Problem: Square  has side length 4, and  is the midpoint of  ... What is the distance from  to ? television screens have an aspect ratio of 4  3 , meaning the width is to the height as 4 is to 3 ...  have area 240 .Points  ,  , and  ...such that Find the area of  .[asy] pair A=(0,15),B=(0,-5),C=(25,0.5),X=origin, ... label(" $A$ ",A,N); label(" $B$ ",B,S); label(" $C$ ",C,E); ... [/asy] Answer: 122 visualize Problem: The pattern in the figure below continues inward infinitely.The base of the largest triangle is 1 , ...Find the total shaded area.[asy] pen blu = rgb(0,112,191); real r=sqrt(3); fill((8,0)--(0,8r)--(-8,0) --cycle, blu); fill((0,0)--(4,4r)--(-4,4r) --cycle, white); ... [/asy] Answer: * # visualize Problem: The centers of the faces of the right rectangular prism shown below are connected to form an octahedron.What is the volume of this octahedron?</p>
<p>Figure 3 :
3
Figure 3: Representative examples from GeoGramBench illustrating the three ascending Program-to-Geometry difficulty levels: Primitive Recognition, Local Relation Composition, and Global Abstract Integration.Each category is exemplified by two sampled problems, highlighting the increasing spatial complexity and abstraction across levels.</p>
<p>Problem: ...What is the height, in inches, of each black bar? visualize Problem: ...Calculate the area of the bounding rectangle.</p>
<p>Figure 4 :
4
Figure 4: Illustration of two types of answer leakage in procedural code (highlighted in yellow): Left-Direct leakage, where the answer is explicitly given by a coordinate value in the Asymptote code (here, we rescale the coordinates to preserve the geometric shape); Right-Indirect leakage, where the answer can be computed from code parameters (in this case, we modify the procedural code to mask such critical information).</p>
<p>Question: In the diagram, two circles ... How many degree s are in the measure of (the smal ler)  ?[asy] ... guide region1 = (a-a/2)..inner..(c/2--c)..outer..cycle; guide region2 = arc(d, a/2, c/2,CCW).. (c/2--d--a/2)..cycle; fill(region1, gray(0.75));fill(region2, gray(0.75));... [/asy] Answer: 120</p>
<p>The model demonstrates a clear weakness in mapping code to spatial imagery, struggling to correctly interpret directions such as counterclockwise (CCW).</p>
<p>Figure 5 :
5
Figure 5: Illustrative solution process generated by the QwQ-32B model on a Local Relation Composition problem.The model initially attempts to construct spatial representations from the provided code, then interprets geometric elements such as direction and region, exhibiting behavior aligned with all three research questions (RQ1-RQ3): local construction, compositional integration, and chain-of-thought-based refinement.Multiple rounds of reflection and verification are observed, although these iterative steps do not consistently yield correct or fully integrated solutions.</p>
<p>Figure 6 :
6
Figure 6: Illustration of the hypothesized multi-stage internal geometry representations process in LLMs for Program-to-Geometry tasks.The model first extracts local geometric substructures ({z 1 , z 2 , . ..}) from the problem statement ({T, C}), then integrates these into a coherent global structure (Z 1 ), which is further iteratively refined and updated (Z 2 , . ..), before finally predicting the answer (A).Each stage corresponds to a core research question: RQ1 (local construction), RQ2 (compositional integration), and RQ3 (global abstraction and reasoning).Dashed arrows indicate how both input information and intermediate representations propagate throughout the process.</p>
<p>Figure 7 :
7
Figure 7: Comparison of QwQ-32B accuracy on equivalent geometry problems expressed in Asymptote versus Matplotlib code (AIME24 and MATH-500).The negligible performance gap demonstrates that Program-to-Geometry capability is independent of drawing language syntax.Example Problem Statement: Rectangles ABCD and EF GH are drawn such that D, E, C, F are collinear.Also, A, D, H, G all lie on a circle.If BC = 16, AB = 107, F G = 17, and EF = 184, what is the length of CE?</p>
<p>Figure 8 :
8
Figure 8: Visualization generated from the drawing code</p>
<p>Example 1 :
1
Problem Statement:In ABC, point F divides side AC in the ratio 1 : 2. Let E be the point of intersection of side BC and AG where G is the midpoint of BF .The length of EC divided by the length of BE is ?(B--F,A--E); draw(A--B--C--cycle); draw(A--E); draw(B--F); label(\"$A$\",A,N); label(\"$B$\",B,W); label(\"$C$\",C,dir(0)); label(\"$E$\",E,S); label(\"$F$\",F,NE); label(\"$G$\",G,SE); After modification (Leakage mitigated): (B--F,A--E); draw(A--B--C--cycle); draw(</p>
<p>Figure 9 :
9
Figure 9: Side-by-side comparison of Asymptote code: before (left) and after (right) information leakage mitigation.</p>
<p>Figure 10 :
10
Figure 10: Side-by-side comparison of Asymptote code: before (left) and after (right) information leakage mitigation.</p>
<p>Problem statement: In quadrilateral ABCD, angle BAD and angle CDA are trisected as shown.What is the degree measure of angle AF D? Answer: 80 Geometric Code: size(150); pair A , B, C, D; A = (0,0); B = (2, 4); C = (7,4); D = (7, -2); draw( (0,0)--(2,4) --(7,4) --(</p>
<p>Figure 11 :
11
Figure 11: Visualization generated from the drawing code</p>
<p>8 u00b0, so the closest integer is 81 u00b0.Since the problem likely expects an integer, I'll go with 81 as the answer.<strong>Final Answer</strong> 81 To find the degree measure of angle AF D in quadrilateral ABCD where angles BAD and CDA are trisected, we start by analyzing the given coordinates and using vector calculations.The coordinates of the points are: -A = (0, 0) -B = (2, 4) -C = (7, 4) -D = (7, 2) -F = (5, 3) We need to determine the angle at point F between points A, F , and D. This angle is formed by the vectors  F A and    F D. 1. <strong>Calculate vectors:</strong> - F A = A  F = (0  5, 0  3) = (5, 3) -   F D = D  F = (7  5, 2  3) = (2, 5) 2. <strong>Compute the dot product:</strong>  F A     F D = (5)(2) + (3)(5) = 10 + 15 = 5 3. <strong>Compute the magnitudes of the vectors:</strong> -|  F A| = (5) 2 + (3) <strong>Calculate the cosine of the angle:</strong> cos</p>
<p>5 .Answer: 1 4 5Figure 12 :
5412
Figure 12: Visualization generated from the drawing code</p>
<p>, but maybe I need to think of the rectangle as B -&gt; E -&gt; F -&gt; H -&gt; B ... In the Asymptote figure, points are: A(0,0), B(3,0), C(6,0), D(9,0), E(9,3), G(6,3) We need to find H's coordinates.H is extension(B, H0, A, G).Let's try to compute H0 and then the line.Which is point B. So line BH0 is the line from B(3,0) to H0(3,0), which is a point.Not helpful... Vector Ep -B = (9-3, 3-0)=(6,3) Vector Ep -(2*B -Ep) = Ep -(-3,-3) = (9 -(-3), 3 -(-3)) = (12,6) So the two vectors are</p>
<ol>
<li><strong>Calculating (BH) in Asymptote Coordinates</strong>: -Coordinates of (B(3,0)) and ( <em>Scaling to Problem's Coordinates</em>*: -Scaling factor ( </li>
</ol>
<p>length of (BH) is ( 1 ).</p>
<p>Table 1 :
1ModelPrimitive Compositional Abstract ALLClosed-source ModelsGPT-o3-mini84.3375.6642.1670.00GPT-o186.7676.0243.3570.92GPT-o1-preview74.7955.9826.2053.15GPT-o1-mini79.6263.2129.0958.94GPT-4o39.8121.294.9621.40Gemini-Pro-1.549.2631.7915.9231.64Open-source ModelsDeepSeek-R185.6675.2740.3869.17DeepSeek-v3-032480.5768.8927.6762.05QwQ-32B85.1773.1237.9267.20DeepSeek-R1-Distill-Qwen-32B79.7867.8335.9262.68Bespoke-Stratos-32B62.5042.5617.0240.55s1.1-32B75.3758.9626.5854.60LIMO-32B76.5959.6325.5354.98DeepSeek-R1-Distill-Qwen-7B72.7958.7424.1653.38Sky-T1-mini-7B71.4557.7524.7952.70DeepSeek-R1-Distill-Qwen-1.5B60.2939.0211.0336.70DeepScaleR-1.5B-preview65.4447.8915.7643.83
Primitive: Primitive Recognition, Compositional: Local Relation Composition, Abstract: Global Abstract Integration.Accuracy (%) of selected closed-source and open-source LLMs on GeoGramBench across three difficulty levels.All models show a marked drop in performance on Abstract tasks, with no model exceeding 50% accuracy at this level.The highest results in both Closed and Open sources models are shown in bold.</p>
<p>(7,4)nse from QwQ-32B <think> Okay, so I need to find the measure of angle AFD in this quadrilateral ABCD where angles BAD and CDA are trisected ...... First, let me note down the coordinates again to get a better idea: -A: (0,0) -B: (2,4) -C: (7,4) -D: (7,-2) .So quadrilateral ABCD has vertices at these points.Let me sketch this mentally.Starting at A (0,0), going to B (2,4), then to C(7,4), then to D (7,-2), and back to A. So AB is a line going up to the right, BC is a horizontal line to the right, CD goes down to the right, and DA goes straight left down to A. Wait, actually, D is at (7,-2), so DA would go from (7,-2) back to (0,0).Hmm, so DA is a line from the lower right back to the origin.Angles BAD and CDA are being trisected ......Alternatively, since both E and F are connected from A to D, but through different paths?Maybe E is on one trisection and F on another?Hmm, this is getting a bit confusing without seeing the actual figure.Let me try to reconstruct based on coordinates.First, let's compute angles BAD and CDA to see their measures, which can then be trisected.Starting with angle BAD.This is the angle at vertex A between points B, A, and D.
Coordinates: -Point B: (2,4) -Point A: (0,0) -Point D: (7,-2) So vectors AB and AD can be calculated...... Now, arccos(0.1592). Let me use a calculator for better precision. Using a calculator,arccos(0.1592): cos(80 u00b0) u2248 0.1736 cos(81 u00b0) u2248 0.1564 So 0.1592 isbetween 80 and 81 degrees. Let me compute the difference: At 80 u00b0: 0.1736 At 81u00b0: 0.1564 Difference between 80 and 81 u00b0 is 1 u00b0, and the difference in cos is-0.0172 per degree... Alternatively, maybe angle at B is 110 u00b0, angle at C is 100 u00b0</p>
<p>Big-math: A large-scale, high-quality math dataset for reinforcement learning in language models. Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, arXiv:2502.173872025arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, Oct 2021Cornell University -arXiv,Cornell University -arXiv</p>
<p>The theory of multiple intelligences. Katie Davis, Joanna Christodoulou, Scott Seider, Howard Earl Gardner ; Davis, K Christodoulou, J Seider, S Gardner, H , Cambridge Handbook of Intelligence. &amp; Rj Sternberg, Sb Kaufman, 2011. 2011The theory of multiple intelligences</p>
<p>Omni-math: A universal olympiad level mathematic benchmark for large language models. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, arXiv:2410.079852024arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, arXiv:2402.140082024arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>Bespoke-stratos: The unreasonable effectiveness of reasoning distillation. Bespoke Labs, 2025</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Advances in Neural Information Processing Systems. 202235</p>
<p>Dacheng Li, Shiyi Cao, Chengkun Cao, Xiuyu Li, Shangyin Tan, Kurt Keutzer, Jiarong Xing, Joseph E Gonzalez, Ion Stoica, arXiv:2502.14382S*: Test time scaling for code generation. 2025arXiv preprint</p>
<p>Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Q Jiang, Ziju Shen, Hugging Face repository. 1392024</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, The Twelfth International Conference on Learning Representations. Bowen Baker, Teddy LeeJan. 2023</p>
<p>Zebralogic: On the scaling limits of llms for logical reasoning. Ronan Bill Yuchen Lin, Kyle Le Bras, Ashish Richardson, Radha Sabharwal, Peter Poovendran, Yejin Clark, Choi, arXiv:2502.011002025arXiv preprint</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao, Mathvista, arXiv:2310.02255Evaluating mathematical reasoning of foundation models in visual contexts. 2023arXiv preprint</p>
<p>DeepScaleR: Surpassing o1-preview with a 1.5b model by scaling rl. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca , Ada Popa, Ion Stoica, ion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling -RL-19681902c1468005bed8ca303013a4e2. 2025</p>
<p>American invitational mathematics examination -aime. in american invitational mathematics examination -aime 2024. MAA. February 2025</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cands, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Understanding and benchmarking artificial intelligence: Openai's o3 is not agi. Rolf Pfister, Hansueli Jud, arXiv:2501.074582025arXiv preprint</p>
<p>Challenging the boundaries of reasoning: An olympiad-level math benchmark for large language models. Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, Lei Fang, Ji-Rong Wen, arXiv:2503.213802025arXiv preprint</p>
<p>Mm-math: Advancing multimodal math evaluation with process evaluation and fine-grained classification. Kai Sun, Yushi Bai, Ji Qi, Lei Hou, Juanzi Li, arXiv:2404.050912024arXiv preprint</p>
<p>Kexian Tang, Junyao Gao, Yanhong Zeng, Haodong Duan, Yanan Sun, Zhening Xing, Wenran Liu, Kaifeng Lyu, Kai Chen, arXiv:2503.19990Lego-puzzles: How good are mllms at multi-step spatial reasoning?. 2025arXiv preprint</p>
<p>Mathscale: Scaling instruction tuning for mathematical reasoning. Zhengyang Tang, Xingxing Zhang, Benyou Wang, Furu Wei, arXiv:2403.028842024arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>Qwq-32b: Embracing the power of reinforcement learning. Qwen Team, March 2025</p>
<p>Geosense: Evaluating identification and application of geometric principles in multimodal reasoning. Liangyu Xu, Yingxiu Zhao, Jingyun Wang, Yingyao Wang, Bu Pi, Chen Wang, Mingliang Zhang, Jihao Gu, Xiang Li, Xiaoyong Zhu, arXiv:2504.125972025arXiv preprint</p>
<p>Thinking in space: How multimodal large language models see, remember, and recall spaces. Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, Saining Xie, arXiv:2412.141712024arXiv preprint</p>
<p>Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, Pengfei Liu, Limo, arXiv:2502.03387Less is more for reasoning. 2025arXiv preprint</p>
<p>Lovish Albert S Yue, Ted Madaan, Moskovitz, Aaditya K Dj Strouse, Singh, arXiv:2412.08819Harp: A challenging human-annotated math reasoning benchmark. 2024arXiv preprint</p>
<p>Euclid: Supercharging multimodal llms with synthetic high-fidelity visual descriptions. Jiarui Zhang, Ollie Liu, Tianyu Yu, Jinyi Hu, Willie Neiswanger, arXiv:2412.087372024arXiv preprint</p>
<p>Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, European Conference on Computer Vision. Springer2024</p>
<p>Yang Zhou, Hongyi Liu, Zhuoming Chen, Yuandong Tian, Beidi Chen, arXiv:2502.05252Gsm-infinite: How do your llms behave over infinitely increasing context length and reasoning complexity?. 2025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
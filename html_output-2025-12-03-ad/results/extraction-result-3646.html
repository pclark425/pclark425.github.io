<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3646 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3646</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3646</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-88.html">extraction-schema-88</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-269282809</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.12494v3.pdf" target="_blank">BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Predictive models often need to work with incomplete information in real-world tasks. Consequently, they must provide reliable probability or confidence estimation, especially in large-scale decision-making and planning tasks. Current large language models (LLMs) are insufficient for accurate estimations, but they can generate relevant factors that may affect the probabilities, produce coarse-grained probabilities when the information is more complete, and help determine which factors are relevant to specific downstream contexts. In this paper, we make use of these capabilities of LLMs to provide a significantly more accurate probabilistic estimation. We propose BIRD, a novel probabilistic inference framework that aligns a Bayesian network with LLM abductions and then estimates more accurate probabilities in a deduction step. We show BIRD provides reliable probability estimations that are 30% better than those provided directly by LLM baselines. These estimates further contribute to better and more trustworthy decision making.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3646.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3646.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BIRD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Inference from Abduction and Deduction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses LLMs to abductively generate natural-language factors and coarse probability judgements, fits an interpretable Bayesian network (via a constrained optimization over factor-level conditional probabilities) to those coarse judgements, and then deductively computes calibrated outcome probabilities for novel contexts via LLM entailment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-70b-instruct, Llama-3.1-70b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLaMA-family models with ~70B parameters used for factor generation, coarse verbal probability outputs, factor-outcome classification, and entailment. Llama-3.1 is reported to produce more diverse factors in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Estimate P(O_i | C) for binary decision outcomes under partial information across real-world-style scenarios (commonsense reasoning, planning, temporal reasoning) — e.g., whether to build a charging station at a location given partial context.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Two-step LLM+Bayes approach: (1) Abduction: LLMs generate factors F_j and for sampled full-factor combinations f prompt the LLM for coarse verbal probabilities PLLM(O|f). (2) Fit learnable per-factor-value probabilities P(O|f_j) under a multiplicative opinion-pool approximation (Eq.4) by minimizing MSE between P_estimated(O|f) (computed from P(O|f_j)) and PLLM(O|f) plus a margin-ranking regularizer. At inference, compute P(O|C)=Σ_f P(O|f)P(f|C) where P(f_j|C) is obtained by LLM entailment (observed factors get prob 1, unobserved values uniform).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Converted subsets of COMMON2SENSE (216 scenarios, 3,822 instances), PLASMA (279 scenarios, 1,395 instances), and TODAY (1,000 instances) into the P(O|C) decision setting used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primary: preference-based pairwise F1 (three-way: C1 better / C2 better / equal); Decision accuracy when using argmax on estimated probabilities. Reported: BIRD optimized average F1 ≈ 0.592 (Llama-2) and ≈ 0.588 (Llama-3.1) on the sampled COMMON2SENSE evaluation (350 instances). Decision-making accuracies (BIRD optimized) reported per-dataset (examples): Llama-2: TODAY ~73.9%, PLASMA ~74.0%, COMMON2SENSE ~86.9%; Llama-3.1: improvements shown especially on TODAY and PLASMA (see paper Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>BIRD substantially outperforms direct LLM probability outputs (vanilla verbalization, logit token-prob, CoT) on probability-quality F1 (≈30% absolute improvement reported over GPT-4 verbal probabilities in the preference F1 evaluation) and yields better or comparable decision-making accuracy relative to Chain-of-Thought (CoT) baselines; when unfair baseline EC (explicit comparison seeing both conditions) is used, BIRD still outperforms GPT-4 EC by ~5% in some settings. Ablations demonstrate the constrained optimization yields better CPTs than neutral heuristics (1/2, 1/n, or fixed-init).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires sufficiently comprehensive factor generation (errors: incomplete factors cause the largest failure mode ~56.7% of analyzed errors); entailment/mapping of conditions to factor values can be incorrect (~30%); residual inaccuracies in the final probability calculation (~13.3%). BIRD can output "unknown" when a condition maps to no factor values. Method depends on (i) quality of LLM-generated coarse probabilities and factors, (ii) choice of mapping from verbal categories to numeric probabilities (paper uses fixed mapping), and (iii) sampling (paper uses 128 sampled full-factor instances per scenario).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>LLMs are comparatively good at abductive factor generation and coarse-grained probability judgements but poor at producing calibrated numeric probabilities; aligning these coarse judgements with an explicit Bayesian model via constrained optimization (BIRD) yields substantially more reliable and interpretable probability estimates that improve downstream decision making and can serve as soft supervision to fine-tune smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3646.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3646.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-70b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2 70B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70-billion-parameter instruction-tuned LLaMA-family model used in the paper for factor generation, verbal coarse probability outputs, factor-outcome classification, and entailment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-70b-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned ~70B parameter LLaMA variant (used via HuggingFace checkpoints referenced in paper) employed for all LLM-side tasks in experiments: generating additional sentences/factors, coarse probability verbalization, factor-to-outcome classification, and entailment.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Direct baselines: verbalized probability estimation for each condition; also used inside BIRD to produce PLLM(O|f) and entailment P(f_j|C).</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>As baseline: direct verbalization (Vanilla), logit token-prob, and Chain-of-Thought prompting with self-consistency; as core BIRD component: provide coarse P_llm(O|f) on sampled full-factor contexts and participate in factor generation and entailment.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>COMMON2SENSE, PLASMA, TODAY (paper's converted evaluation datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline direct methods (Llama-2): Vanilla average F1 ≈ 0.311 (preference F1, Table 1), Logit ≈ 0.228; as part of BIRD: optimized average F1 ≈ 0.592 and decision accuracy improvements (e.g., TODAY ~73.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Llama-2 direct verbalization/logit underperformed vs BIRD when used standalone for probability estimation; Llama-2 used within BIRD achieved much higher F1 and usable probabilities for decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Direct numeric probability verbalization from Llama-2 is noisy and over/under-confident; factor generation is less diverse than Llama-3.1 in this work, which affects downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>When combined with BIRD's Bayesian alignment, Llama-2's coarse outputs can be converted into reliable probability estimates that improve downstream decisions and generate preferred follow-up questions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3646.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3646.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-70b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1 70B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ~70B instruction-tuned LLaMA-family model (version 3.1) evaluated alongside Llama-2 and reported to yield more diverse and accurate abductive factors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-70b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned ~70B parameter LLaMA variant (Llama-3.1) used for the same roles as Llama-2; authors report it generates more diverse and accurate factors, improving BIRD's performance.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Same binary decision probability estimation tasks (COMMON2SENSE/PLASMA/TODAY), both as baseline LLM (Vanilla/CoT/Logit) and as the LLM inside BIRD.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Used in baseline prompting (Vanilla, Logit, CoT) and inside BIRD for factor generation, coarse probability labeling of sampled full-factor contexts, and entailment mapping of contexts to factor values.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>COMMON2SENSE, PLASMA, TODAY (paper conversions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>As part of BIRD: optimized average F1 ≈ 0.588; decision-making accuracies show stronger improvements on the hardest dataset (TODAY) relative to CoT; baselines (vanilla/CoT) had lower F1 (vanilla ~0.303, CoT ~0.309 per Table 1 averages).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>BIRD with Llama-3.1 outperforms direct Llama-3.1 verbal probabilities and often outperforms CoT decision accuracy on more complex tasks (TODAY); ablations show factor-generation quality (better in Llama-3.1) materially affects final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Same structural limitations as BIRD that depend on factor completeness and accurate entailment; improvements are contingent on the base LLM's ability to produce comprehensive factors.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Higher-quality factor generation (as observed with Llama-3.1) improves BIRD's calibrated probability estimates and downstream decisions, highlighting the importance of factor diversity and quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3646.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3646.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-0613 (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-0613 (OpenAI GPT-4 variant used for annotation/EC baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 variant used by the authors for dataset annotation and as an explicit-comparison (EC) baseline in which the model sees both candidate conditions at once.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 family model variant used in the paper for annotation work and as a baseline; employed in EC (explicit comparison) experiments where it selects which of two conditions better supports an outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>As EC baseline: decide which of two provided conditions better supports a target outcome; used by authors for dataset verification/annotation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>EC baseline: explicit pairwise comparison (no numeric probabilities); GPT-4 also appears in CoT baseline runs reported in Table 1 (CoT verbalized probabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>COMMON2SENSE preference-based pairwise evaluation (350 instances used for BIRD intrinsic evaluation) and other converted datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In the preference-based F1 evaluation, GPT-4 EC achieved average F1 ≈ 0.540 (EC is an unfair baseline since it sees both conditions simultaneously). GPT-4 CoT verbalization average F1 was ≈ 0.289 (much lower).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>GPT-4 EC (seeing both conditions) performs well but BIRD still outperforms GPT-4 EC in the authors' evaluation by ~5% in some comparisons; GPT-4 CoT direct probability verbalization performs poorly relative to BIRD.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Direct verbalized probabilities from GPT-4 are reported as unreliable/overconfident in previous work and in this paper's baselines; EC setting is not directly comparable as it benefits from seeing both conditions simultaneously.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Even strong instruction models like GPT-4 can be outperformed in calibrated probability estimation by BIRD's Bayesian alignment of abductive factors and coarse LLM judgements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3646.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3646.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vanilla verbalization baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct probability verbalization (Vanilla)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline where models are prompted to directly verbalize estimated numeric probabilities for each outcome given a single condition, with self-consistency sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to Llama-2, Llama-3.1, GPT-3.5/GPT-4 in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting-based baseline: ask the LLM to output numeric probability estimates (verbalized) for each outcome; sampling (temperature 0.7), majority vote across responses used.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Estimate probabilities P(O|C) directly from the LLM for single conditions, independently for each condition.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Direct prompting for numeric probabilities; sometimes combined with self-consistency (multiple samples, majority vote).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>COMMON2SENSE/PLASMA/TODAY converted decision datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Vanilla Llama-2 average preference F1 ≈ 0.311, Llama-3.1 vanilla ≈ 0.303 (Table 1); significantly lower than BIRD's F1 ≈ 0.59.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Vanilla verbalization underperforms BIRD; EC baseline (explicit comparison) can sometimes perform better because it sees both contexts at once, but EC is not directly comparable to the single-condition probability setting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLMs tend to be inaccurate and overconfident when asked for direct numeric probabilities; performance suffers in fine-grained probability comparisons between similar contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Direct verbalization is insufficient for reliable probability estimation in mission-critical decision tasks; converting coarse verbal outputs via an explicit Bayesian framework yields much better alignment with human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3646.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3646.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) verbalization with self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline where LLMs produce a chain-of-thought reasoning trace and then verbalize probability estimates; self-consistency sampling used to aggregate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to Llama-2, Llama-3.1, GPT-3.5/GPT-4 in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chain-of-thought prompting that elicits step-by-step reasoning before asking for numeric confidence; self-consistency (multiple sampled reasoning chains) used to aggregate an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Produce probability estimates and final decisions grounded in an explicit reasoning chain for each condition.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>CoT prompting to generate reasoning then numerical verbalization; self-consistency (sampling multiple CoT outputs) and majority vote.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>COMMON2SENSE/PLASMA/TODAY converted decision datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CoT baselines (e.g., GPT-4 CoT) report low preference F1 (e.g., GPT-4 CoT average ≈ 0.289); Llama CoT averages ~0.294–0.309 (Table 1); decision accuracies vary and in some datasets CoT achieves high hard-label accuracy but poorer calibrated probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>CoT can help induce correct hard decisions in some cases but its verbalized probabilities remain poorly calibrated compared to BIRD; BIRD often outperforms CoT in decision-making accuracy on harder datasets and yields better-calibrated probability outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>CoT can produce plausible-sounding but uncalibrated probability estimates; its hard-label decisions can sometimes succeed due to spurious signals rather than calibrated probabilistic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Chain-of-thought improves explanation but not necessarily numeric calibration; structured Bayesian alignment (BIRD) gives better-calibrated probabilities for downstream decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3646.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3646.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logit baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logit-probability baseline (token probability)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline that uses the model's normalized output token probability (logit) associated with the decision token as a proxy for the probability estimate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to Llama-2, Llama-3.1 in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Extract normalized probability of the decision token (greedy decoding, temperature = 0) and use this as the model's probability estimate for the outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Estimate probability of an outcome using model token probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Use output-token probability (logit) after greedy decoding as numerical probability.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>COMMON2SENSE/PLASMA/TODAY converted decision datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Logit baselines performed worse in preference F1 (e.g., Llama-2 logit average F1 ≈ 0.228, Llama-3.1 logit ≈ 0.269, Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Logit-based estimates underperform BIRD and are generally worse than verbalization baselines for the calibrated probability task.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Token-prob based estimates are not reliable calibrations of true outcome probabilities and can be biased by output tokenization and prompt phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Simple use of logits is insufficient for trustworthy probability estimation in these decision settings; structured approaches like BIRD are needed to improve calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3646.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3646.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EC baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit Comparison (EC) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline where the model is given both candidate conditions simultaneously and asked to pick which one better supports the outcome (no numeric probabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to Llama-2, Llama-3.1, GPT-4 in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting format that presents both contexts at once and asks the model to choose which condition better supports a target outcome; not directly comparable to single-condition probability estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Pairwise comparison: which of two additional conditions better supports the given outcome in a scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Not a probability method — performs direct argmax/selection between two conditions when both are observed.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>COMMON2SENSE pairwise evaluation and other converted datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>EC achieves relatively high preference F1 (e.g., GPT-4 EC average F1 ≈ 0.540; Llama3.1 EC ≈ 0.511; Llama2 EC ≈ 0.503 in Table 1) because it sees both contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>EC performs well but is an unfair baseline for single-condition probability estimation since it benefits from simultaneous visibility of both contexts; BIRD still outperforms EC in some comparisons despite not seeing both contexts together.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>EC cannot be used in settings where only one condition is available at a time (the single-condition probability estimation setting) and therefore is not directly comparable to BIRD's use-case.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Even though EC is advantaged (seeing both contexts), BIRD's calibrated single-condition probabilities can still yield competitive or superior alignment with human preference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs <em>(Rating: 2)</em></li>
                <li>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback <em>(Rating: 2)</em></li>
                <li>DeLLMa: Decision making under uncertainty with large language models <em>(Rating: 2)</em></li>
                <li>ThinkSum: Probabilistic reasoning over sets using large language models <em>(Rating: 2)</em></li>
                <li>Decomposing uncertainty for large language models through input clarification ensembling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3646",
    "paper_id": "paper-269282809",
    "extraction_schema_id": "extraction-schema-88",
    "extracted_data": [
        {
            "name_short": "BIRD",
            "name_full": "Bayesian Inference from Abduction and Deduction",
            "brief_description": "A framework that uses LLMs to abductively generate natural-language factors and coarse probability judgements, fits an interpretable Bayesian network (via a constrained optimization over factor-level conditional probabilities) to those coarse judgements, and then deductively computes calibrated outcome probabilities for novel contexts via LLM entailment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-70b-instruct, Llama-3.1-70b-Instruct",
            "model_description": "Instruction-tuned LLaMA-family models with ~70B parameters used for factor generation, coarse verbal probability outputs, factor-outcome classification, and entailment. Llama-3.1 is reported to produce more diverse factors in this work.",
            "prediction_task": "Estimate P(O_i | C) for binary decision outcomes under partial information across real-world-style scenarios (commonsense reasoning, planning, temporal reasoning) — e.g., whether to build a charging station at a location given partial context.",
            "method_of_probability_estimation": "Two-step LLM+Bayes approach: (1) Abduction: LLMs generate factors F_j and for sampled full-factor combinations f prompt the LLM for coarse verbal probabilities PLLM(O|f). (2) Fit learnable per-factor-value probabilities P(O|f_j) under a multiplicative opinion-pool approximation (Eq.4) by minimizing MSE between P_estimated(O|f) (computed from P(O|f_j)) and PLLM(O|f) plus a margin-ranking regularizer. At inference, compute P(O|C)=Σ_f P(O|f)P(f|C) where P(f_j|C) is obtained by LLM entailment (observed factors get prob 1, unobserved values uniform).",
            "dataset_or_benchmark": "Converted subsets of COMMON2SENSE (216 scenarios, 3,822 instances), PLASMA (279 scenarios, 1,395 instances), and TODAY (1,000 instances) into the P(O|C) decision setting used for evaluation.",
            "performance_metrics": "Primary: preference-based pairwise F1 (three-way: C1 better / C2 better / equal); Decision accuracy when using argmax on estimated probabilities. Reported: BIRD optimized average F1 ≈ 0.592 (Llama-2) and ≈ 0.588 (Llama-3.1) on the sampled COMMON2SENSE evaluation (350 instances). Decision-making accuracies (BIRD optimized) reported per-dataset (examples): Llama-2: TODAY ~73.9%, PLASMA ~74.0%, COMMON2SENSE ~86.9%; Llama-3.1: improvements shown especially on TODAY and PLASMA (see paper Table 2).",
            "comparison_to_baselines": "BIRD substantially outperforms direct LLM probability outputs (vanilla verbalization, logit token-prob, CoT) on probability-quality F1 (≈30% absolute improvement reported over GPT-4 verbal probabilities in the preference F1 evaluation) and yields better or comparable decision-making accuracy relative to Chain-of-Thought (CoT) baselines; when unfair baseline EC (explicit comparison seeing both conditions) is used, BIRD still outperforms GPT-4 EC by ~5% in some settings. Ablations demonstrate the constrained optimization yields better CPTs than neutral heuristics (1/2, 1/n, or fixed-init).",
            "limitations_or_challenges": "Requires sufficiently comprehensive factor generation (errors: incomplete factors cause the largest failure mode ~56.7% of analyzed errors); entailment/mapping of conditions to factor values can be incorrect (~30%); residual inaccuracies in the final probability calculation (~13.3%). BIRD can output \"unknown\" when a condition maps to no factor values. Method depends on (i) quality of LLM-generated coarse probabilities and factors, (ii) choice of mapping from verbal categories to numeric probabilities (paper uses fixed mapping), and (iii) sampling (paper uses 128 sampled full-factor instances per scenario).",
            "notable_findings": "LLMs are comparatively good at abductive factor generation and coarse-grained probability judgements but poor at producing calibrated numeric probabilities; aligning these coarse judgements with an explicit Bayesian model via constrained optimization (BIRD) yields substantially more reliable and interpretable probability estimates that improve downstream decision making and can serve as soft supervision to fine-tune smaller models.",
            "uuid": "e3646.0",
            "source_info": {
                "paper_title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Llama-2-70b-Instruct",
            "name_full": "Llama-2 70B Instruct",
            "brief_description": "A 70-billion-parameter instruction-tuned LLaMA-family model used in the paper for factor generation, verbal coarse probability outputs, factor-outcome classification, and entailment.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2-70b-instruct",
            "model_description": "Instruction-tuned ~70B parameter LLaMA variant (used via HuggingFace checkpoints referenced in paper) employed for all LLM-side tasks in experiments: generating additional sentences/factors, coarse probability verbalization, factor-to-outcome classification, and entailment.",
            "prediction_task": "Direct baselines: verbalized probability estimation for each condition; also used inside BIRD to produce PLLM(O|f) and entailment P(f_j|C).",
            "method_of_probability_estimation": "As baseline: direct verbalization (Vanilla), logit token-prob, and Chain-of-Thought prompting with self-consistency; as core BIRD component: provide coarse P_llm(O|f) on sampled full-factor contexts and participate in factor generation and entailment.",
            "dataset_or_benchmark": "COMMON2SENSE, PLASMA, TODAY (paper's converted evaluation datasets).",
            "performance_metrics": "Baseline direct methods (Llama-2): Vanilla average F1 ≈ 0.311 (preference F1, Table 1), Logit ≈ 0.228; as part of BIRD: optimized average F1 ≈ 0.592 and decision accuracy improvements (e.g., TODAY ~73.9%).",
            "comparison_to_baselines": "Llama-2 direct verbalization/logit underperformed vs BIRD when used standalone for probability estimation; Llama-2 used within BIRD achieved much higher F1 and usable probabilities for decision-making.",
            "limitations_or_challenges": "Direct numeric probability verbalization from Llama-2 is noisy and over/under-confident; factor generation is less diverse than Llama-3.1 in this work, which affects downstream performance.",
            "notable_findings": "When combined with BIRD's Bayesian alignment, Llama-2's coarse outputs can be converted into reliable probability estimates that improve downstream decisions and generate preferred follow-up questions.",
            "uuid": "e3646.1",
            "source_info": {
                "paper_title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Llama-3.1-70b-Instruct",
            "name_full": "Llama-3.1 70B Instruct",
            "brief_description": "A ~70B instruction-tuned LLaMA-family model (version 3.1) evaluated alongside Llama-2 and reported to yield more diverse and accurate abductive factors.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-70b-Instruct",
            "model_description": "Instruction-tuned ~70B parameter LLaMA variant (Llama-3.1) used for the same roles as Llama-2; authors report it generates more diverse and accurate factors, improving BIRD's performance.",
            "prediction_task": "Same binary decision probability estimation tasks (COMMON2SENSE/PLASMA/TODAY), both as baseline LLM (Vanilla/CoT/Logit) and as the LLM inside BIRD.",
            "method_of_probability_estimation": "Used in baseline prompting (Vanilla, Logit, CoT) and inside BIRD for factor generation, coarse probability labeling of sampled full-factor contexts, and entailment mapping of contexts to factor values.",
            "dataset_or_benchmark": "COMMON2SENSE, PLASMA, TODAY (paper conversions).",
            "performance_metrics": "As part of BIRD: optimized average F1 ≈ 0.588; decision-making accuracies show stronger improvements on the hardest dataset (TODAY) relative to CoT; baselines (vanilla/CoT) had lower F1 (vanilla ~0.303, CoT ~0.309 per Table 1 averages).",
            "comparison_to_baselines": "BIRD with Llama-3.1 outperforms direct Llama-3.1 verbal probabilities and often outperforms CoT decision accuracy on more complex tasks (TODAY); ablations show factor-generation quality (better in Llama-3.1) materially affects final performance.",
            "limitations_or_challenges": "Same structural limitations as BIRD that depend on factor completeness and accurate entailment; improvements are contingent on the base LLM's ability to produce comprehensive factors.",
            "notable_findings": "Higher-quality factor generation (as observed with Llama-3.1) improves BIRD's calibrated probability estimates and downstream decisions, highlighting the importance of factor diversity and quality.",
            "uuid": "e3646.2",
            "source_info": {
                "paper_title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4-0613 (used)",
            "name_full": "GPT-4-0613 (OpenAI GPT-4 variant used for annotation/EC baseline)",
            "brief_description": "GPT-4 variant used by the authors for dataset annotation and as an explicit-comparison (EC) baseline in which the model sees both candidate conditions at once.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4-0613",
            "model_description": "OpenAI GPT-4 family model variant used in the paper for annotation work and as a baseline; employed in EC (explicit comparison) experiments where it selects which of two conditions better supports an outcome.",
            "prediction_task": "As EC baseline: decide which of two provided conditions better supports a target outcome; used by authors for dataset verification/annotation tasks.",
            "method_of_probability_estimation": "EC baseline: explicit pairwise comparison (no numeric probabilities); GPT-4 also appears in CoT baseline runs reported in Table 1 (CoT verbalized probabilities).",
            "dataset_or_benchmark": "COMMON2SENSE preference-based pairwise evaluation (350 instances used for BIRD intrinsic evaluation) and other converted datasets.",
            "performance_metrics": "In the preference-based F1 evaluation, GPT-4 EC achieved average F1 ≈ 0.540 (EC is an unfair baseline since it sees both conditions simultaneously). GPT-4 CoT verbalization average F1 was ≈ 0.289 (much lower).",
            "comparison_to_baselines": "GPT-4 EC (seeing both conditions) performs well but BIRD still outperforms GPT-4 EC in the authors' evaluation by ~5% in some comparisons; GPT-4 CoT direct probability verbalization performs poorly relative to BIRD.",
            "limitations_or_challenges": "Direct verbalized probabilities from GPT-4 are reported as unreliable/overconfident in previous work and in this paper's baselines; EC setting is not directly comparable as it benefits from seeing both conditions simultaneously.",
            "notable_findings": "Even strong instruction models like GPT-4 can be outperformed in calibrated probability estimation by BIRD's Bayesian alignment of abductive factors and coarse LLM judgements.",
            "uuid": "e3646.3",
            "source_info": {
                "paper_title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Vanilla verbalization baseline",
            "name_full": "Direct probability verbalization (Vanilla)",
            "brief_description": "Baseline where models are prompted to directly verbalize estimated numeric probabilities for each outcome given a single condition, with self-consistency sampling.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to Llama-2, Llama-3.1, GPT-3.5/GPT-4 in experiments",
            "model_description": "Prompting-based baseline: ask the LLM to output numeric probability estimates (verbalized) for each outcome; sampling (temperature 0.7), majority vote across responses used.",
            "prediction_task": "Estimate probabilities P(O|C) directly from the LLM for single conditions, independently for each condition.",
            "method_of_probability_estimation": "Direct prompting for numeric probabilities; sometimes combined with self-consistency (multiple samples, majority vote).",
            "dataset_or_benchmark": "COMMON2SENSE/PLASMA/TODAY converted decision datasets.",
            "performance_metrics": "Vanilla Llama-2 average preference F1 ≈ 0.311, Llama-3.1 vanilla ≈ 0.303 (Table 1); significantly lower than BIRD's F1 ≈ 0.59.",
            "comparison_to_baselines": "Vanilla verbalization underperforms BIRD; EC baseline (explicit comparison) can sometimes perform better because it sees both contexts at once, but EC is not directly comparable to the single-condition probability setting.",
            "limitations_or_challenges": "LLMs tend to be inaccurate and overconfident when asked for direct numeric probabilities; performance suffers in fine-grained probability comparisons between similar contexts.",
            "notable_findings": "Direct verbalization is insufficient for reliable probability estimation in mission-critical decision tasks; converting coarse verbal outputs via an explicit Bayesian framework yields much better alignment with human preferences.",
            "uuid": "e3646.4",
            "source_info": {
                "paper_title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "CoT baseline",
            "name_full": "Chain-of-Thought (CoT) verbalization with self-consistency",
            "brief_description": "Baseline where LLMs produce a chain-of-thought reasoning trace and then verbalize probability estimates; self-consistency sampling used to aggregate outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to Llama-2, Llama-3.1, GPT-3.5/GPT-4 in experiments",
            "model_description": "Chain-of-thought prompting that elicits step-by-step reasoning before asking for numeric confidence; self-consistency (multiple sampled reasoning chains) used to aggregate an answer.",
            "prediction_task": "Produce probability estimates and final decisions grounded in an explicit reasoning chain for each condition.",
            "method_of_probability_estimation": "CoT prompting to generate reasoning then numerical verbalization; self-consistency (sampling multiple CoT outputs) and majority vote.",
            "dataset_or_benchmark": "COMMON2SENSE/PLASMA/TODAY converted decision datasets.",
            "performance_metrics": "CoT baselines (e.g., GPT-4 CoT) report low preference F1 (e.g., GPT-4 CoT average ≈ 0.289); Llama CoT averages ~0.294–0.309 (Table 1); decision accuracies vary and in some datasets CoT achieves high hard-label accuracy but poorer calibrated probabilities.",
            "comparison_to_baselines": "CoT can help induce correct hard decisions in some cases but its verbalized probabilities remain poorly calibrated compared to BIRD; BIRD often outperforms CoT in decision-making accuracy on harder datasets and yields better-calibrated probability outputs.",
            "limitations_or_challenges": "CoT can produce plausible-sounding but uncalibrated probability estimates; its hard-label decisions can sometimes succeed due to spurious signals rather than calibrated probabilistic reasoning.",
            "notable_findings": "Chain-of-thought improves explanation but not necessarily numeric calibration; structured Bayesian alignment (BIRD) gives better-calibrated probabilities for downstream decision-making.",
            "uuid": "e3646.5",
            "source_info": {
                "paper_title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Logit baseline",
            "name_full": "Logit-probability baseline (token probability)",
            "brief_description": "Baseline that uses the model's normalized output token probability (logit) associated with the decision token as a proxy for the probability estimate.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to Llama-2, Llama-3.1 in experiments",
            "model_description": "Extract normalized probability of the decision token (greedy decoding, temperature = 0) and use this as the model's probability estimate for the outcome.",
            "prediction_task": "Estimate probability of an outcome using model token probabilities.",
            "method_of_probability_estimation": "Use output-token probability (logit) after greedy decoding as numerical probability.",
            "dataset_or_benchmark": "COMMON2SENSE/PLASMA/TODAY converted decision datasets.",
            "performance_metrics": "Logit baselines performed worse in preference F1 (e.g., Llama-2 logit average F1 ≈ 0.228, Llama-3.1 logit ≈ 0.269, Table 1).",
            "comparison_to_baselines": "Logit-based estimates underperform BIRD and are generally worse than verbalization baselines for the calibrated probability task.",
            "limitations_or_challenges": "Token-prob based estimates are not reliable calibrations of true outcome probabilities and can be biased by output tokenization and prompt phrasing.",
            "notable_findings": "Simple use of logits is insufficient for trustworthy probability estimation in these decision settings; structured approaches like BIRD are needed to improve calibration.",
            "uuid": "e3646.6",
            "source_info": {
                "paper_title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "EC baseline",
            "name_full": "Explicit Comparison (EC) baseline",
            "brief_description": "Baseline where the model is given both candidate conditions simultaneously and asked to pick which one better supports the outcome (no numeric probabilities).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to Llama-2, Llama-3.1, GPT-4 in experiments",
            "model_description": "Prompting format that presents both contexts at once and asks the model to choose which condition better supports a target outcome; not directly comparable to single-condition probability estimation.",
            "prediction_task": "Pairwise comparison: which of two additional conditions better supports the given outcome in a scenario.",
            "method_of_probability_estimation": "Not a probability method — performs direct argmax/selection between two conditions when both are observed.",
            "dataset_or_benchmark": "COMMON2SENSE pairwise evaluation and other converted datasets.",
            "performance_metrics": "EC achieves relatively high preference F1 (e.g., GPT-4 EC average F1 ≈ 0.540; Llama3.1 EC ≈ 0.511; Llama2 EC ≈ 0.503 in Table 1) because it sees both contexts.",
            "comparison_to_baselines": "EC performs well but is an unfair baseline for single-condition probability estimation since it benefits from simultaneous visibility of both contexts; BIRD still outperforms EC in some comparisons despite not seeing both contexts together.",
            "limitations_or_challenges": "EC cannot be used in settings where only one condition is available at a time (the single-condition probability estimation setting) and therefore is not directly comparable to BIRD's use-case.",
            "notable_findings": "Even though EC is advantaged (seeing both contexts), BIRD's calibrated single-condition probabilities can still yield competitive or superior alignment with human preference.",
            "uuid": "e3646.7",
            "source_info": {
                "paper_title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs",
            "rating": 2,
            "sanitized_title": "can_llms_express_their_uncertainty_an_empirical_evaluation_of_confidence_elicitation_in_llms"
        },
        {
            "paper_title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
            "rating": 2,
            "sanitized_title": "just_ask_for_calibration_strategies_for_eliciting_calibrated_confidence_scores_from_language_models_finetuned_with_human_feedback"
        },
        {
            "paper_title": "DeLLMa: Decision making under uncertainty with large language models",
            "rating": 2,
            "sanitized_title": "dellma_decision_making_under_uncertainty_with_large_language_models"
        },
        {
            "paper_title": "ThinkSum: Probabilistic reasoning over sets using large language models",
            "rating": 2,
            "sanitized_title": "thinksum_probabilistic_reasoning_over_sets_using_large_language_models"
        },
        {
            "paper_title": "Decomposing uncertainty for large language models through input clarification ensembling",
            "rating": 1,
            "sanitized_title": "decomposing_uncertainty_for_large_language_models_through_input_clarification_ensembling"
        }
    ],
    "cost": 0.021041749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BIRD: A TRUSTWORTHY BAYESIAN INFERENCE FRAMEWORK FOR LARGE LANGUAGE MODELS
3 Apr 2025</p>
<p>Yu Feng fengyu1@seas.upenn.edu 
University of Pennsylvania</p>
<p>Ben Zhou 
Arizona State University</p>
<p>Weidong Lin 
University of Pennsylvania</p>
<p>Dan Roth 
University of Pennsylvania</p>
<p>BIRD: A TRUSTWORTHY BAYESIAN INFERENCE FRAMEWORK FOR LARGE LANGUAGE MODELS
3 Apr 2025A8162E7A445B746858853E536428F44BarXiv:2404.12494v3[cs.CL]
Predictive models often need to work with incomplete information in real-world tasks.Consequently, they must provide reliable probability or confidence estimation, especially in large-scale decision-making and planning tasks.Current large language models (LLMs) are insufficient for accurate estimations, but they can generate relevant factors that may affect the probabilities, produce coarse-grained probabilities when the information is more complete, and help determine which factors are relevant to specific downstream contexts.In this paper, we make use of these capabilities of LLMs to provide a significantly more accurate probabilistic estimation.We propose BIRD, a novel probabilistic inference framework that aligns a Bayesian network with LLM abductions and then estimates more accurate probabilities in a deduction step.We show BIRD provides reliable probability estimations that are 30% better than those provided directly by LLM baselines.These estimates further contribute to better and more trustworthy decision making.</p>
<p>INTRODUCTION</p>
<p>Accurate, interpretable, and unbiased probabilistic estimation is crucial to artificial intelligence (AI) systems in mission-critical tasks that require trustworthy decision making.This is because, in many real-world applications, AI systems have to work with partial and incomplete observations that are insufficient for deductive and deterministic decisions (McCarthy &amp; Hayes, 1981).Consider the charging station planning scenario in Fig. 1, where we want to use large language models to decide where to build a certain number of charging stations over a region, subject to some constraints.Since we cannot always observe all the necessary information regarding a potential location, we can only provide probability or confidence estimates on whether we should use the candidate location.If the number of planned stations is significant, we need to perform an optimization process such as ILP (Roth &amp; Yih, 2005) that maximizes the estimated probabilities of each individual station.A simple ranking of the candidates would not be sufficient in this scenario, as it may yield a suboptimal solution illustrated in Fig. 1.However, current LLMs are not ideal for such probability estimation tasks since 1) LLMs tend to be inaccurate and overconfident when estimating numerical confidences and probabilities (Xiong et al., 2024) and 2) they cannot provide an out-of-the-box interpretable and controllable process on how their estimates are derived (Li et al., 2024).This prevents us from using LLMs' decision-making capabilities in large-scale automatic decision-making tasks.Consequently, there is a need to find ways to infer better probabilities from these models.</p>
<p>In this work, we propose BIRD (Bayesian Inference from Abduction and Deduction).BIRD builds on three observations.The first observation is that although LLMs cannot produce convincing numerical probabilities directly, they can generate neutral and comprehensive real-world factors relevant to these probabilistic estimates.Moreover, we observe that they can make relatively correct coarse probability estimations when using these factors as contexts that provide more complete information.The last observation is that LLMs can decide how specific contexts and conditions are related to these factors via entailment classifications.Together, these observations motivate BIRD, where we approximate a Bayesian network (Friedman et al., 1997) using LLM-generated factors, optimize its internal conditional probabilities by sampling LLM coarse predictions on combinations of values of all the factors, and infer more accurate probabilities for specific downstream conditions by leveraging LLMs to map conditions to factors.Figure 1: An example of mission-critical tasks.We first ask OpenAI o1 to separately predict the probability of building a charging station at each specific location.It estimates the same probabilities for two different conditions twice, while BIRD (ours) successfully distinguishes the difference and can thus help the user make a more informed decision.This further demonstrates that while LLMs are capable of coarse estimations, they struggle to generate accurate probabilities.We further show through OpenAI o1 ranking that although it can successfully rank all four locations based on how likely it should be used to build a new charging station, it is still insufficient to complete the task because of ties, i.e., (1,4) and (2,3) are the same from ranking perspectives.</p>
<p>As illustrated in Fig. 2, we formulate the probability estimation problem as deciding P(O i |C), where O i is a potential outcome (e.g., the government should/should not build a charging station here) and C is the downstream condition or context (e.g., the location is on a busy highway) under a decision-making scenario S (e.g., deciding whether to build a charging station at the location).Given S, BIRD first generates a comprehensive set of real-world factors and values f 1 , f 2 , ..., f n that may contribute to the probabilistic estimations.Then, it approximates a Bayesian network structure using the generated factors and the outcomes {O}.We propose a novel method for estimating the conditional probability table (CPT) for the outcome, i.e., P(O i |f ), f ∈ F where F is the product space of value sets for all factors.1 Specifically, we assume a set of learnable parameters P(O i |f j ) representing the conditional probability of each possible outcome given a single variable f j .We then sample f ∈ F and acquire LLM's coarse probability estimations P LLM (O i |f ).Next, we approximate the conditional probability P estimated (O i |f ) in the CPT using the learnable parameters P(O i |f j ) based on a derived approximation formula and optimize these parameters by minimizing the distributional distance between P estimated (O i |f ) and the LLM's coarse predicted probability P LLM (O i |f ).From a high level, this abduction step aligns language models' internal probability estimation with Bayesian constraints and has better error tolerance.The resulting Bayesian network is generic to all possible conditions in the scenario S. In the following deduction step, we use the trained Bayesian parameters to estimate the probability of any possible conditions and observations P(O i |C).To do this, we employ an entailment process to determine P(f i |C), as shown by "LLM Entailment" in Fig. 2, and the outcome we are interested in, P(O i |C) = f ∈F P(O i |f )P(f |C).</p>
<p>As we show in §4, the inferred probabilities from BIRD are more reliable, consistent with human judgments on 30% more instances than those directly predicted by LLMs.We further show that BIRD probability predictions are good enough to be directly used for decision making (i.e., by computing an argmax on the predicted probabilities of each label for inference), outperforming standard chain-ofthought (CoT) (Wei et al., 2022) on several reasoning benchmarks.More importantly, BIRD achieves better probability estimations by optimizing an interpretable Bayesian network with natural-languagebased variables.There are two additional benefits of BIRD.First, we show that BIRD-aligned probabilities can serve as more accurate and fine-grained supervision signals for training smaller models.In addition, we demonstrate that BIRD's factor generation process is neutral and unbiased to the specific conditions, so it can be used to generate better follow-up questions to improve decisionmaking confidence.Combining these findings, we show that BIRD improves trustworthiness -a crucial ability for mission-critical applications such as medical chatbots, where we need reliable decisions supported with enough evidence for high-confidence predictions.2, e.1, f.2)), and minimizing the distributional distance between an approximated P estimated (Oi|f ) and PLLM(Oi|f ).At inference time, each context C (S+U1/S+U2/...) is mapped to some fj via entailment, and a probability is derived using P estimated .BIRD can further generate follow-up questions.</p>
<p>RELATED WORK</p>
<p>Direct Inference in LLMs.Our paper is first related to LLM direct inference methods (Wei et al., 2022;Wang et al., 2023), and decomposition-based reasoning (Wolfson et al., 2020;Tafjord et al., 2022;Zhou et al., 2022;Kassner et al., 2023).Compared to these works, BIRD considers using abduction and deduction to facilitate probabilistic induction, which aligns with the slow-thinking systems (McGlynn, 2014;Lin et al., 2023).Our work is also related to chain-of-thought via latentvariable inference.Given a question-answer pair (X, Y ), existing methods (Hoffman et al., 2023;Hu et al., 2024) aim to find latent chains of thought -token sequences Z that contribute the most to the conditional likelihood.While BIRD has a more structured Z and focuses more on the proper probability estimation instead of merely finding the optimal Z. Uncertainty Estimations and Decision making under Uncertainty in LLMs.Most current works focus more on sampling to access the model's uncertainty of factual knowledge (Kuhn et al., 2023;Yadkori et al., 2024;Lin et al., 2024).Some works verbalize the uncertainty of LLMs (Tian et al., 2023;Xiong et al., 2024).However, Xiong et al. (2024) shows that LLMs are often overconfident when directly verbalizing their confidence.Hou et al. (2024) is most related to us, as it also incorporates the intuition of decomposition, but it focuses more on addressing aleatoric uncertainty due to input ambiguity without rigorous probability calculation.This work is also related to decision making with uncertainty (Ren et al., 2023;Han et al., 2024;Liu et al., 2025) while all methods use LLM's direct uncertainty estimation such as token logits or verbalization.BIRD introduces external reliable Bayesian modeling to more accurately calibrate uncertainty in LLMs for decision making.</p>
<p>Probabilistic Inference with LMs.Chen et al. (2020) is one of the first papers to estimate a probability out of an NLI problem while with no explanation.Ozturkler et al. (2023) follows a similar intuition to ours, where the model first thinks by retrieving relevant associations, and then conducts probabilistic reasoning.Lew et al. (2020); Wong et al. (2023) discuss translating from natural language to the probabilistic language of thought.However, none of the papers tackle the real-world complex settings under incomplete information as comprehensively as we do with the level of interpretability to users of any level.</p>
<p>MODELING</p>
<p>BAYESIAN INFERENCE FROM ABDUCTION AND DEDUCTION</p>
<p>On a high level, given a decision Y and partial information X, instead of direct inference using LLMs (induction : X → Y ), BIRD conceptualizes the general scenario behind X into intermediate factors (abduction : X → Z), and then fits a Bayesian model with Z.The model then estimates probabilities for Y (deduction : X, Z → Y ).Our problem setting is a generic contextual inference problem of the type inspired by Feng et al. (2023).Given the context C, which includes a general scenario S and a corresponding downstream condition U , our task is to estimate
P(O i |C), i = 1, 2, C = (S, U ),
where {O} is denoted as O 1 and O 2 .We decompose the overall context C into the general scenario S and downstream conditions U because we want to build a neutral intermediate space Z based on only S, so that the probability estimation is unbiased towards any specific conditions.This would mitigate models relying on incorrect signals, such as charging stations are normally built in higher-income neighborhoods, but more generic factors like accessibility for trustworthy decision making.</p>
<p>Instead of directly making a prediction, we approximate a controllable and interpretable inference process based on explicit Bayesian modeling (Gelman et al., 1995) containing natural-language-based variables.For a scenario S, we conceptualize N factors {F j } N j=1 based on S that are relevant to the outcome O, which can take on two possible values, O i , i = 1, 2, similar to the structure found in Bayesian networks (Koller &amp; Friedman, 2009).Here the real-world factors and the outcome are equivalent to the variables in the network.Therefore, the choice between outcomes O 1 and O 2 depends on the factors, with each discrete factor F j having a set of possible values as F j , containing all possible details of the factor required for deciding between O 1 and O 2 .We use F = N j=1 F j to denote the product space of value sets for all factors.For an instance in the space, each factor F j should be attributed a specific value f j .We assume such derived factors are complete at this stage.That is, they include all the possible individual factors/angles that may affect the distribution of the outcomes.2As a result, F is a complete information space for decisions.As in Fig. 2, (factor a value 1 a 1 , factor b value 1 b 1 , factor c value 1 c 1 , factor d value 1 d 1 , factor e value 1 e 1 , factor f value 1 f 1 ) is an element f in the product space F where there are 2 6 elements in total.We then calculate the conditional probability table (CPT) as P(O i |f ), ∀f ∈ F, f = (f 1 , f 2 , ..., f N ).</p>
<p>We assume 1) {F j } N j=1 is complete, so O does not depend on any other intermediate factors, and 2) only one value can be assigned for each factor for a given C. The predictive probability for the outcome is thus obtained by marginalizing over the product space F for a context C (details in §A.1):
P(O i |C) = f ∈F P(O i |f )P(f |C)(1)
where i = 1, 2, C = (S, U ), ∀f ∈ F, f = (f 1 , f 2 , ..., f N ), f j ∈ F j , j = 1, ..., N .F is the product space of value sets for all factors.f is a possible instance of complete information in the space.</p>
<p>Assumption on Conditional Independence of Variables.To provide relaxation for real-world applications, and since the factors should cover as many different aspects as possible, we assume factors are conditionally independent given the context.Therefore,
P(O i |C) = f ∈F P(O i |f ) N j=1 P(f j |C)(2)
In the following sections, we describe the missing implementation details of the framework: (1) the generation of the factors {F j } N j=1 ( §3.2); (2) the assignment of the conditional probability table P(O i |f ) ( §3.3), and (3) the computation of observations P(f j |C), j = 1, ..., N ( §3.4).</p>
<p>ABDUCTIVE FACTOR GENERATION</p>
<p>Large language models, although they cannot solve some complicated cases, are shown to possess parametric knowledge that enables them to solve more common cases that can be directly found in their memory (Kambhampati et al., 2024;Li et al., 2024;Zhou et al., 2024).Our method for generating the factors follows this similar intuition.For a scenario S, we first do abstractions with LLMs to derive N factors {F j } N j=1 .The most straightforward approach involves utilizing LLMs to directly obtain a list of factors and their potential values.However, direct generation is effective but not sufficiently comprehensive, and we want to fully utilize past similar occurrences that the model memorizes in their parametric knowledge.</p>
<p>Therefore, to encourage models to fully explore the range of alternatives, we divide the task into two stages: 1) generating sentences describing familiar situations that would increase the likelihood of an outcome, and 2) summarizing these sentences into factors with corresponding values.We show the prompts in Appendix Fig. 5 and Fig. 6.We further utilize LLMs to perform classification to assess how the value of each generated factor affect the decision, i.e., deciding the value supports which outcome, so that we can filter out factors that are unrelated to the outcomes.The prompt is in Appendix Fig. 7.In the example depicted in Fig. 2, a factor like "The color of the vehicle" will be eliminated since all potential values of this factor consistently support the same outcome, i.e., neutral.</p>
<p>ASSIGNMENT OF P(O i |f ) AS A CONSTRAINED OPTIMIZATION PROBLEM</p>
<p>We now calculate each P(O i |f ), i = 1, 2, ∀f ∈ F in the conditional probability table of the Bayesian network.Since we do not have any observed data for approximation and LLMs alone cannot directly give an accurate assignment on P(O i |f ), we adopt the formula proposed in Bordley (1982), which derived a version of the logarithmic opinion pool using axioms from the theory of additive conjoint measurement.This formula is proved to be the only continuous (over the open interval (0, 1)) group probability formula under the assumption detailed in §A.2 that the decision maker has an intuitive weak ordering on the set of factor odds ratios and a "noninteraction" property of factors.Our setting fits the assumption as the weak ordering of decisions is the most fundamental condition of consistency in decision making and we assume the conditional independence of factors.Therefore,
P(O i |f ) = N j=1 ( P(Oi|fj ) P(Oi) ) wj P(O i ) N j=1 ( P(Oi|fj ) P(Oi) ) wj P(O i ) + N j=1 ( 1−P(Oi|fj ) 1−P(Oi) ) wj (1 − P(O i )) (3)
To determine the weights w j ,3 , we adopt the second case scenario mentioned in Bordley (1982), i.e., N j=1 w j &gt; 1.In this case, the decision maker with complete information is more certain than the decision maker with only one piece of information through acquiring all different pieces of information from different factors.Since there isn't a priori information that suggests another choice exists, we use equal weight w j = 1, j = 1, ..., N and assume P(O i ) = 50%, i = 1, 2. The final estimation formula can be rewritten as:
P estimated (O i |f ) = P estimated (O i |f 1 , f 2 , ...f N ) ≈ N j=1 P(O i |f j ) N j=1 P(O i |f j ) + N j=1 (1 − P(O i |f j )) (4)
Constrained optimization for estimating P(O i |f ).From Eq. 4, it is not hard to see that each P(O i |f ), ∀f ∈ F can now be approximated using only P(O i |f j ), ∀f j ∈ F j , j = 1, ..., N , which we view as learnable parameters.As discussed in §1, LLMs can perform relatively correct coarse estimation under complete information, which refers to contexts with all factors' values present (i.e.f ∈ F).Consequently, we prompt the LLMs to output verbalized probability estimations for outcome O i given f .This approximates LLM's predictions of P LLM (O i |f ).With P estimated and P LLM , we can now form a constraint optimization problem that finds the values of parameters P(O i |f j ) that minimize the distributional distance between the Bayesian estimated probabilities P estimated (O i |f ) and LLM estimated probabilities P LLM (O i |f ), ∀f ∈ F, while conforming to the structure in Eq. 4 and preliminary assessments about each P(O i |f j ), ∀f j ∈ F j , j = 1, ..., N provided by the LLM classification as mentioned in §3.2.</p>
<p>we give a general description here.As the parameter initialization, we utilize the LLM classification as mentioned in §3.2, where we direct the LLMs to ascertain which outcome is more likely supported by the value of each factor:
P init (O i |f j ) = 75% f j supports outcome i 50% f j is neutral 25% f j supports opposite outcome ¬i (5)
For the training data, given an instance of complete information f
′ = (f ′ 1 , f ′ 2 , ..., f ′ N
), we prompt LLM to output a verbalized probability score for each outcome ∈ {very unlikely, unlikely, somewhat unlikely, neutral, somewhat likely, likely, very likely} which we use as P LLM (O i |f ′ ).The detailed prompt can be found in Appendix Fig. 10.We then assign a specific probability value for each coarse estimation, i.e., very unlikely: 0%, unlikely: 20%, somewhat unlikely: 40%, neutral: 50%, somewhat likely: 60%, likely: 80%, very likely: 100%, as supervision signals to learn the correct value of P(O i |f ′ j ), j = 1, ..., N for each f ′ j in f ′ using Eq.4.We randomly sample 128 instances in the space of complete information as the training data for each scenario.Note that we manually assign probability values for LLM coarse decisions, but they can be changed as hyperparameters.</p>
<p>We adopt two loss functions: 1) we use the regular mean squared error loss for optimization and denote the loss as
ℓ MSE (P estimated (O i |f ′ ), P LLM (O i |f ′ ))
; 2) we use the margin ranking loss to ensure the final trained individual probability preserve the original direction of its supported outcome and denote the loss as ℓ M R :
ℓ MR = max(0, −y target (O i |f j )(P trained (O i |f j ) − 0.5) + ϵ) y target (O i |f j ) = sgn(P init (O i |f j ) − 50%) P trained (O i |f j ) = f * ∈F * P(O i |f * ∪ f j )P(f * |f j ) = 1 card(F * ) f * ∈F * P(O i |f * ∪ f j ) (6)
∀f j ∈ F j , j = 1, ..., N .We apply probability marginalization to get the trained individual probability and approximate it with the mean values since there are no explicit priors.We use F * to denote the product space of the remaining factors except F j , which is a subspace of F and f * = (f 1 , f 2 , ..., f j−1 , f j+1 , ..., f N ), f * ∈ F * .Therefore, the final loss function we use for training is defined as follows:</p>
<p>L = ℓ MSE + αℓ MR (7) where α balances the two losses.We use SGD as the optimization function.As a result, we can estimate any P(O i |f ), ∀f ∈ F with the learned P(O i |f j ), ∀f j ∈ F j with Eq.4.</p>
<p>Customization to human preference.A benefit of our design is that we can explicitly ask for human preferences on how a particular factor value will affect the outcome, i.e., manually setting P(O i |f j ).Such preferences are desirable in certain applications with known constraints, and they can be easily integrated in Eq.4 by replacing P(O i |f j ) with the human preferred value.</p>
<p>COMPUTATION OF P(f j |C) WITH LLM ENTAILMENT</p>
<p>This section discusses how we compute P(f j |C).As noted in §3.1, C is the context containing the general scenario S, and a specific additional condition U .Given C, we employ an entailment task formulation to find which factors and their corresponding values are implied or mapped by C. Specifically, we adopt the prompt in Appendix Fig. 8 where we directly ask if the context entails a value from a factor.We conduct another round of self-correction using the prompt in Appendix Fig. 9 to check if the model believes all the previously identified values are actually entailed.</p>
<p>We define the notion of observed factors, which refers to a subset of factors among all factors that are being implied by the context.For example, as shown in Fig. 2, context S + U 1 implies a 1 , c 1 , and d 1 , S + U 2 implies c 1 and d 1 , while the additional condition S + U 3 implies d 1 and f 2 .We assume only one value can be chosen per factor.We write the product value space of the implied factors as
F ′ = M m=1 F jm ,
which is a subspace of F. We define the conditional probabilities of the values of these observed factors to be 1 if the value is implied by C, a 0 if not:
P(f jm |C) = 1 f jm = f * jm , f jm ∈ F jm 0 f jm ̸ = f * jm , f jm ∈ F jm(8)
Where the implied f * jm ∈ F jm , ∀m = 1, ..., M .For the unobserved factors, we have no information regarding which value to be selected.In order to be neutral and unbiased, we assume that, in an unobserved factor, each value has an equal probability of being selected.We use F ′ ⊥ = N m=M +1 F jm to denote the space of unobserved factors, which is the orthogonal complement space of F ′ in F,
P(f jm |C) = 1 card(F jm ) , f jm ∈ F jm(9)
Where ∀m = M + 1, ..., N .We can therefore calculate the final probability as presented in Eq.1, utilizing the formulations provided in Eqs.2 through 9.</p>
<p>EXPERIMENTS 4.1 DATASETS AND SETTINGS</p>
<p>We convert three datasets focusing on reasoning and planning to our problem setting including O, S, and U , as described in §3.1.We detail this process in §A.4.COMMON2SENSE (Singh et al., 2021) focuses on commonsense reasoning, and we gather 216 scenarios and 3822 instances, where 9 additional conditions on average support each outcome.PLASMA (Brahman et al., 2023) focuses on planning, and we gather 279 scenarios and 1395 instances, where 5 additional conditions support the less common outcome.TODAY (Feng et al., 2023) focuses on temporal reasoning, where there are 1000 instances.Among the three datasets, the most challenging task is TODAY and the easiest is COMMON2SENSE, judged by baseline model performances.The complexity of the scenario and conditions is highest in TODAY.The conditions support the less common outcome in PLASMA and COMMON2SENSE presents general decision making with common sense.</p>
<p>We conduct our experiments with Llama-2-70b-instruct and Llama-3.1-70b-Instruct. 4For the factor generation task in §3.2, we generate 10 sentences for each outcome before summarization.For the LLM classification task for factor-outcome mapping, we generate 3 times and conduct a majority vote.For the LLM entailment task in §3.4,we sample 3 times and conduct a majority vote with 1 round of self-reflection.We set learning rate = 1e-02, epoch = 20, batch size = 4, ϵ = 0, α = 10 for the learning algorithm mentioned in §3.3.</p>
<p>INTRINSIC EVALUATIONS OF THE BIRD FRAMEWORK</p>
<p>Reliability of BIRD's Estimated Probabilities.We first propose an evaluation method that directly benchmarks the quality of probability estimations.We adopt a similar setting as mentioned in §3.1.
P (O 1 |C 1 ) = P (O 1 |C 2 ) &gt; P (O 2 |C 1 ) = P (O 2 |C 2 ).
This task is significantly more difficult than providing two contexts that support different outcomes, as systems will have to understand the nuances in probability estimation.</p>
<p>We collect human gold labels for which context better supports the gold outcome with rigorous quality control using MTurk.We explain our choice of this relative evaluation over asking human annotators to provide absolute probabilities from a psychological perspective, along with the details of human annotation in §A.5.We evaluate on 350 randomly sampled instances from COMMON2SENSE when the conditions can be mapped to at least one factor and therefore BIRD does not produce "unknown"7 .</p>
<p>Published as a conference paper at ICLR 2025 We report the F1 score as the evaluation metric.We further explain how the F1 score is computed in §A.5.</p>
<p>We compare the following methods that directly acquire probability estimations from LLMs as baselines.1) Vanilla (Wang et al., 2023): models directly verbalize the estimated probabilities.2) Logit: we select the normalized token probability of the decision.3) CoT (Wei et al., 2022): models output a reasoning process followed by verbalized estimated probabilities.4) EC: models select which additional condition better supports the outcome without outputting any probabilities.Note that this formulation is unfair to BIRD and other baselines that individually estimate the probabilities.</p>
<p>To demonstrate the effectiveness of our learning algorithm in §3.We utilize the initial probability of Eq.5 to directly estimate P(O i |f ) with Eq.4.The details of the baselines are presented also in §A.6.</p>
<p>Table 1 lists the results.Our framework exhibits superior alignment with human preference, with both models achieving an F1 of 59%, more than 30% higher than GPT-4.Such results demonstrate that BIRD produces reliable probability estimation without additional human supervision.Notably, such probability estimation is done by judging each condition separately (i.e., BIRD assigns one probability to condition 1, and later another probability to condition 2).We find that vanilla language models do much better when both conditions are provided at the same time (i.e., the EC setting) where the model can perform argmax operations.However, even under this setting that is unfair to our framework, BIRD still outperforms GPT-4 by 5%.All results suggest that vanilla LLMs cannot properly estimate probabilities, and BIRD effectively improves them.In addition, by outperforming all the ablation settings, we demonstrate that our constrained optimization algorithm proposed in §3.3 effectively approximates better Bayesian network's conditional probabilities.</p>
<p>Applying BIRD's Probabilities in Decision Making.In this section, we evaluate the performance of applying the predicted probabilities from BIRD in decision-making tasks.and the task is to decide which outcome is more likely given C, similar to Fig. 2. The inference process we use for BIRD is simply taking the argmax between P(O 1 |C) and P(O 2 |C).We remove all instances where they cannot be mapped to any factor values(i.e., BIRD's decision is "unknown").This does not undermine our experiment setting because such removal is label-agnostic, and we can always fall back to other methods such as CoT when BIRD predicts "unknown".We compare our framework with chain-of-thought inference with self-consistency on the same instances where BIRD predictions are not "unknown".We present the prompt for CoT in Appendix Fig. 14.</p>
<p>The results are shown in Table 2. Our first observation is that BIRD's probability estimations are accurate enough for direct decision making.Our proposed method outperforms the chain-of-thought baseline with Llama-3.1 on all benchmarks and performs comparably using Llama-2 as the base model.This is partially because Llama-3.1 generates more diverse and accurate factors, and we analyze this in §A.7.At the same time, we see that BIRD performs better on the most challenging TODAY dataset on both models.This demonstrates that BIRD is a desirable solution for more complex scenarios where CoT tends to fail more.We also observe the inferior performance of our method compared to CoT may be attributed to the fundamental differences between probability estimation and hard label classification.Table 6 in the appendix shows an example that CoT sometimes does correctly due to the spurious signals and biases in the conditions, which highlights the importance of our controllable inference approach.We can conclude that BIRD outperforms direct inference methods like CoT in most cases and is at least comparable in decision making while offering significantly better interpretability and controllability throughout the process.</p>
<p>Ablation Studies on BIRD's Components.We conduct an ablation study regarding factor generation.</p>
<p>We compare the final decision-making accuracy based on our proposed abductive sampling method and the direct factor generation method mentioned in §3.2.Appendix Table 7 shows the results: we see that our proposed method, on average, surpasses the direct method by 4.4%/3.9% in accuracy for Llama-2/ Llama-3.1, indicating the effectiveness of our proposed factor generation that collects more comprehensive signals from models' parametric knowledge and memorization.We also conduct ablation studies on different ways of estimating the conditional probabilities of our Bayesian network (i.e., 1/2, 1/n, and fixed initial prob).Results in Tables 1 and 2 show that our proposed constrained optimization learning algorithm effectively finds more accurate estimations by outperforming all ablation baselines.We demonstrate that both losses in Eq.7 are crucial in §A.9.</p>
<p>EXTRINSIC APPLICATIONS OF THE BIRD FRAMEWORK</p>
<p>The Usage of the Reliably Estimated Probability.As shown in §4.2, BIRD can produce reliable probabilities that contribute to better decision making.Such findings motivate us to investigate whether these probabilities can be used as supervision signals, as models may benefit from seeing more than the discrete classification labels (Zhou et al., 2020).Specifically, we use BIRD to estimate probabilities of some decision-making training instances and finetune a T5-large model using these generated probabilities.We then evaluate its performance on cross-domain datasets.We fully replicate the experimental settings described in Feng et al. (2023) and the details can be referred to in §A.10.We use instances from COMMON2SENSE, where the argmax of the estimated probabilities from our framework using Llama-2 supports the correct outcome, as additional training data, besides the originally proposed training data in that paper.We test on two general temporal reasoning datasets, MATRES (Ning et al., 2018) and TRACIE (Zhou et al., 2021), as well as two commonsense reasoning and planning datasets, TODAY, and PLASMA.The results are shown in Table 3.We see that models achieve an average performance increase of 1.3% compared with BIRD-generated probabilities as additional signals.We use cross-entropy loss with class probabilities as the target for our soft probability setting and cross-entropy loss with class labels as the target for the hard-label setting.Such results suggest another effective application of the BIRD framework, where smaller models can benefit from soft probability alignment and make better overall predictions.Table 3: System performances under different supervision data across three binary temporal benchmarks and one binary planning benchmark.For simplicity, we use "hard label" representing that we use COMMON2SENSE supervision data with explicit binary labels, and "BIRD prob" representing that we use COMMON2SENSE supervision data with estimated probabilities.TODAY (exp) uses gold explanations during evaluation.</p>
<p>Using BIRD to Generate Better Follow-up Questions.Generating accurate and informative follow-up questions is an important goal for LLMs to be applied in mission-critical tasks that require unambiguous decision making.A natural benefit of our BIRD framework is that it analyzes what needs to be known through the comprehensive and unbiased abduction step (i.e., factor generation) and interprets how new information will change the probability or confidence of a desired decision during the deduction step (i.e., probability estimation).We conduct a follow-up question generation experiment to demonstrate this benefit.For a given decision-making problem, we ask both BIRD and LLMs to generate binary (yes/no) follow-up questions.For BIRD, we identify implied factors, choose the most important unmentioned one, adopt the most important value aligning with the current outcome, and rephrase it as a question.We directly ask LLM to generate questions for better decision making.The details can be referred to in §A.11.We then recruit crowd-workers on MTurk to provide pairwise preference between the BIRD generated question and the question generated directly by the LLM.We do this on 250 examples from COMMON2SENSE.An example can be seen in Appendix Fig. 4 and the details can also be seen in §A.11.Appendix Table 9 shows that the follow-up questions generated by BIRD are more often preferred: questions generated by BIRD are preferred 52.8% of the time, while those directly generated by LLM are preferred 32.8% of the time; in the remaining 14.4%, both are considered equally useful.This result motivates future work on applying BIRD in an interactive agent system for more efficient and controlled decision making.</p>
<p>CONCLUSION</p>
<p>In this paper, we propose BIRD, a Bayesian inference framework for large language models.BIRD utilizes LLMs' existing reliable capabilities such as abductive thinking and coarse-grained classification to optimize an explicit Bayesian network for more reliable probability estimation.We show that BIRD can output accurate probabilities, outperforming LLM vanilla probabilties by 30% on direct estimation tasks, and outperform chain-of-thought on decision-making tasks.At the same time, BIRD's outputs are explainable, because of its natural-language-based variables in the fully-interpretable Bayesian network.We also show that BIRD can be used in several other settings, such as generating fine-grained supervision signals and asking informative follow-up questions, to further improve the trustworthiness and controllability of LLMs when applied to real-world applications.We derive the outcome probability for the given context C of partial information, i.e., any additional condition U under the scenario S, based on the complete information space F of scenario S:
P(O i |C) = f ∈F P(O i |f, C)P(f |C) = f ∈F P(O i |f, C)P(f |C)(1{f ∩ C = f } + 1{f ∩ C ̸ = f }) = f ∈F ∩{f ∩C=f } P(O i |f, C)P(f |C) + f ∈F ∩{f ∩C̸ =f } P(O i |f, C)P(f |C) = f ∈F ∩{f ∩C=f } P(O i |f )P(f |C) + f ∈F ∩{f ∩C̸ =f } P(O i |f, C)P(f |C) = f ∈F ∩{f ∩C=f } P(O i |f )P(f |C) + f ∈F ∩{f ∩C̸ =f } P(O i |f )P(f |C) = f ∈F P(O i |f )P(f |C) (10)
Where the fourth line follows from that 1−P(Oi|fj ) , j = 1, ..., N , the intuitive ordering ≥ E is defined as:
P(O i |f, C) = P(O i |f ) since f ∩ C = f . And the second last line is derived from that P(f |C) = 0 if f ∩ C ̸ = f(o 1 , o 2 , ..., o N ) ≥ E (o * 1 , o * 2 , ..., o * N )(• (a) either (o 1 , o 2 , ..., o N ) ≥ E (o * 1 , o * 2 , ..., o * N ) or (o * 1 , o * 2 , ..., o * N ) ≥ E (o 1 , o 2 , ..., o N ), • (b) if (o 1 , o 2 , ..., o N ) ≥ E (o * 1 , o * 2 , ..., o * N ) and if (o * 1 , o * 2 , ..., o * N ) ≥ E (o * * 1 , o * * 2 , ..., o * * N ) then (o 1 , o 2 , ..., o N ) ≥ E (o * * 1 , o * * 2 , ..., o * * N ).
"Noninteraction" property of factors.The non-interaction condition says that the decision maker's interpretation of how one factor affects the outcome does not depend on how other factors affect the outcome.</p>
<p>A.3 LEARNING ALGORITHM FOR ESTIMATING P(O i |f )</p>
<p>We present the complete algorithm in Appendix Algorithm 1.Since P(O 1 ) and P(O 2 ) are complementary, we only need to learn P(O 1 |f ja ), f ja ∈ F j , j = 1, ..., N, a = 1, ..., card(F j ), and
P(O 2 |f ja ) = 1 − P(O 1 |f ja ).
As a result, for a given scenario S, the number of learnable parameters is the total number of values for all the generated factors, i.e., N j=1 card(F j ).</p>
<p>All training instances for a scenario are drawn from the complete information space for the same scenario S and possible outcomes {O} and will share the same factors {F j } N j=1 .We randomly sample 128 instances in the space of complete information as the training data for each scenario.This represents the minimum number of instances required for effective training.Ideally, increasing the number of sampled instances would improve the model's alignment with the underlying LLM.For a scenario with fewer than 128 instances in the space of complete information, we employ a resampling technique, selecting instances repeatedly until we reach a total of 128 samples.</p>
<p>The value of P init (O i |f j ) in Eq.5 is assigned as it represents random initialization.To determine the mappings of rankings to probabilities in the algorithm, we consulted two psychology experts and adopted the Likert scale theory.The finalized mapping between verbalized probabilities and numerical probabilities reflects typical human behavior and effectively distinguishes between verbalized probabilities in an unbiased manner.Additionally, slight adjustments to the mappings do not noticeably affect overall performance.All training hyperparameters for the algorithm are optimized using a grid search with a hold-out validation set from PLASMA.</p>
<p>A.4 EXPERIMENT DATASET DESCRIPTION</p>
<p>We convert three datasets focusing on reasoning and planning to our problem setting.</p>
<p>• COMMON2SENSE (Singh et al., 2021): focuses on commonsense reasoning and planning.The original task constitutes a multi-domain complementary commonsense reasoning benchmark and comprises natural language true/false statements.We exclusively focus on instances of comparative reasoning where a pre-trained smaller model exhibits low confidence in its prediction logits.Using GPT-48 , we rewrite each instance statement into its opposite, creating two distinct outcomes.We then outline a general scenario based on the two statements and generate 10 additional sentences for each statement, with detailed conditions that would support the statement.Additionally, we implement a reverse verification step to eliminate any incorrect additional sentences to ensure dataset quality.We gather 216 scenarios and 3822 instances.9 additional conditions on average support each outcome.</p>
<p>• TODAY (Feng et al., 2023): focuses on temporal reasoning.The dataset is designed to study how introducing an extra sentence to a specific scenario affects the temporal relation, aligning well with the context of our task.There are 1000 instances.</p>
<p>• PLASMA (Brahman et al., 2023): focuses on planning.The original task is to generate or revise a plan based on a given goal while adhering to an additional condition.We've observed that the key variations between the initial and the revised plans given the additional condition typically focus on a specific step within the plan.We then utilize GPT-4 to identify the step and adapt the description of the step in the original and revised plans into two potential outcomes.We manually identify the less common outcome in the 279 scenarios and use GPT-4 to generate 5 additional sentences for the outcome, with detailed conditions that would support this outcome instead of the other.We gather 279 scenarios and 1395 instances with 5 additional conditions for each scenario.</p>
<p>For the two GPT-4 generated datasets, we recruited the authors and their colleagues to conduct a human quality check on 100 randomly selected instances from each dataset, with three people reviewing each instance.All three reviewers agreed on the label for 91% of the generated data.For 94% of the data, at least two reviewers agreed on the label.</p>
<p>A.5 HUMAN EVALUATION ON BIRD ESTIMATED PROBABILITIES</p>
<p>We first explain our choice of relative evaluation over asking human annotators to provide absolute probabilities from a psychological perspective.TVERSKY &amp; KAHNEMAN (1974) shows that humans use heuristic principles to make probability judgments, which often suffer from systematic and predictable errors due to cognitive biases.Kahneman (2003) further discusses how human judgment in probabilistic situations is generally better in relative than absolute terms due to cognitive biases and limitations in processing complex information.We conducted a pilot study with human
(O i |f ja ), f ja ∈ F j , j = 1, ..., N, a = 1, ..., card(F j ), i = 1, 2.</p>
<h1>Get verbalized probability scores from the LLM.</h1>
<p>1 for f ′ ∈ {f } do 2 P LLM (O 1 |f ′ ), P LLM (O 2 |f ′ ) ← M (f ′ , O 1 , O 2 ) 3 end</p>
<h1>Initialize probabilities and calculate target for the margin ranking loss.annotators on MTurk, asking them to provide absolute probabilities for each additional condition separately.However, we were unable to reach a consensus among the 7 annotators.</h1>
<p>4 for j = 1 to N do 5 for a = 1 to card(F j ) do 6 P init (O 1 |f ja ) = 75% f ja supports outcome 1 50% f ja is neutral 25% f ja supports opposite outcome 2 7 y target (O 1 |f ja ) = 1 P init (O 1 |f ja ) &gt; 50% 0 P init (O 1 |f ja ) = 50% −1 P init (O 1 |f ja ) &lt; 50%P trained (O 1 |f ja ) = f * ∈F * P estimated (O 1 |f * ∪ f ja )P(f * |f ja ) = 1 card(F * ) f * ∈F * P estimated (O 1 |f * ∪ f ja ) 20 ℓ ja MR = max(0, −y target (O 1 |f ja )(P trained (O 1 |f ja ) − 0.5) + ϵ) 21 end 22 end 23 ℓ MR = 1 N N j=1 1 card(Fj ) card(Fj ) a=1 ℓ ja MR # Compute MR loss. 24 L = ℓ MSE + α * ℓ MR # Compute final loss.
We then introduce the human annotation process for pairwise comparison.We collect human gold labels with rigorous quality control for the preference-based pairwise evaluation of the estimated probabilities.We restrict each annotator to take 5 samples at most and there are 386 distinct annotators.We only allow participants who have 98% or more HITs acceptance rate and are located in the US.We also require annotators to spend at least 1 minute for each instance (the hourly salary is $15).We show the experiment interfaces in Appendix Fig. 3.We first carry out the human annotation process five times on 450 randomly sampled instances from COMMON2SENSE when the condition can be mapped to at least one factor and therefore BIRD does not produce "unknown" during our experiments, and both conditions support one target outcome.If the 5 annotations have a less than 80% agreement, i.e., less than 4 out of the 5 annotators agree on the same condition label, we invite 2 additional annotators to reassess this instance.If the agreement of the 7 annotators is more than 70%, i.e., at least 5 out of the 7 annotators agree on the same condition label, we accept the instance.Other than the previously accepted instances, we also accept an instance as a neutral label if all three labels have been chosen during the annotations and the agreement for the neutral label is more than or equal 40% and the agreement for any of the remaining two labels is less than or equal 40%.We get 350 instances in total, where 12.3% of the instances have a neutral label where condition 1 and condition 2 equally support the outcome, 44.0% of the instances have a label towards condition 1 where condition 1 supports the corresponding outcome better than condition 2, 43.7% of the instances have a label towards condition 2 where condition 2 supports the corresponding outcome better than condition 1.</p>
<p>We now introduce how precision and recall are calculated.Given a testing instance, assuming the gold outcome is O * and the opposite outcome is O ¬ , there will be 3 model output labels: 1) P (O * |C1) &gt; P (O * |C2), i.e., the model thinks C1 better support the gold outcome; 2) P (O * |C2) &gt; P (O * |C1), i.e., the model thinks C2 better support the gold outcome; 3) P (O * |C2) = P (O * |C1), i.e., the model thinks both C 1 and C 2 equally support the gold outcome.We want to check if the model's label is the same as the human annotator's label.Therefore, this can further be seen as a 3-label classification problem.</p>
<p>Error Type Frequency Percentage</p>
<p>The generated factors and their values are not comprehensive enough to cover all elements of the conditions.The selected values only imply partial information in the additional condition.</p>
<p>56.7%</p>
<p>The additional condition is mapped to incorrect values of the factors that it does not necessarily imply.</p>
<p>30%</p>
<p>The calculation of the final probability is inaccurate.13.3%</p>
<p>Table 4: Error analysis for Llama-2-70b-instruct for direct decision making.</p>
<p>Scenario: the ease of carrying a cup is being compared between one person and six people.</p>
<p>Outcome 1: It would be easier for one person to carry the cup, than it would be for six people to carry the cup.</p>
<p>Outcome 2: It would be easier for six people to carry the cup, than it would be for one person to carry the cup.</p>
<p>Llama-2-70b-Instruct Generated Factors Table 5: An example where Llama-3.1 generates better factors than Llama-2.</p>
<p>We then give insights into LLM direct performance.We present an example from the COMMON2SENSE dataset in Appendix Table 6.where the LLM accurately infers the correct outcome based predominantly on inductive biases, ignoring the other primary information provided in the condition, i.e., the peppers are used in large quantities, which will be captured in our framework.</p>
<p>Example</p>
<p>Scenario: The spiciness of a hot sauce is being discussed in relation to the choice between using habanero peppers or jalapeno peppers.Condition: The hot sauce recipe uses a significant amount of habanero peppers relative to the quantity of other ingredients.Outcome 1: A hot sauce is going to be hotter if it uses habanero peppers rather than jalapeno.Outcome 2: A hot sauce is going to be hotter if it uses jalapeno peppers rather than habanero.</p>
<p>Llama-2 Generated Deicision Making Process</p>
<p>Habanero peppers are generally much hotter than jalapeno peppers.This is measured on the Scoville scale, which is a measure of the pungency (spiciness or heat) of chili peppers and other spicy foods.So the condition better supports Outcome 1: A hot sauce is going to be hotter if it uses habanero peppers rather than jalapeno.We conduct an ablation study on the loss function in Eq.7 for decision making, as detailed in Appendix Sentences: #1 A longer cord provides more flexibility and allows for a greater range of movement while using the cell phone.This is because the additional length of the six-foot cord gives the user a larger radius of movement, enabling them to comfortably use their phone while it is charging without feeling restricted or confined to a specific location.Outcome 2: You can move around more freely with your cell phone if it is being charged with a one-foot cord rather than a six-foot cord.</p>
<p>Sentences: #2 If the cell phone is plugged into a portable power bank or a USB port on a computer, a one-foot cord provides greater mobility because it is shorter and less likely to get tangled or caught on objects while moving.#3 If the cell phone is constantly being used while charging and the user prefers to keep the phone close to the charger at all times, a one-foot cord allows for easier mobility and reduces the risk of tripping over a longer cord."</p>
<p>AS S I S T A N T</p>
<p>{ "The cell phone's charging method":</p>
<p>["The charger is portable", "The charger is unmovable"], "The user's movement range": ["The user stays very close to the charger", "The user has a large radius of movement"], "The location of the phone charger":</p>
<p>["The user leaves the charger somewhere", "The user carries the charger"]} Outcome 1: You can move around more freely with your cell phone with a one-foot cord rather than a six-foot cord.</p>
<p>Outcome 2: You can move around more freely with your cell phone with a six-foot cord rather than a one-foot cord.</p>
<p>AS S I S T A N T</p>
<p>Given that the user is carrying a portable charger, a shorter cord like one foot would indeed be far more manageable, making it easier for the user to move around freely.Given the same conditions, a longer cord like six feet might become an impediment, making it more challenging for the user who is carrying the charger to move around freely due to the possibility of tangling or managing the extra length.Therefore, Outcome 1: Likely Outcome 2: Unlikely You are an AI assistant that helps people make decisions.The user will give you a question.Your task is to answer as faithfully as you can.While answering, think step-by-step and justify your answer.You are asked to provide a probability for each outcome being correct (0% to 100%) based on the scenario and its corresponding additional condition.Ensure that P(outcome 1) + P(outcome 2) = 1 .US E R Scenario: ... Additional Condition: ... Outcome 1: ... Outcome 2: ...</p>
<p>AS S I S T A N T</p>
<p>Explanation: [step-by-step analysis here] Confidence: P(outcome 1) = X%, P(outcome 2) = Y%.You are an AI assistant that helps people make decisions.The user will give you a question.Your task is to answer as faithfully as you can.While answering, think step-by-step and justify your answer.You are asked to decide which condition makes the outcome more likely to be true given the scenario.US E R Scenario: ... Outcome: ... Condition 1: ... Condition 2: ...</p>
<p>AS S I S T A N T</p>
<p>Explanation: [step-by-step analysis here] Answer: Condition i/Both.and justify your answer.Given a scenario and an additional condition, decide which of the two outcomes is supported.</p>
<p>US E R</p>
<p>Scenario: The government is planning the locations for building charging stations.</p>
<p>Condition: The location is on a busy highway with no existing charging stations.</p>
<p>Outcome 1: The government should build a charging station here.</p>
<p>Outcome 2: The government should not build a charging station here.</p>
<p>AS S I S T A N T</p>
<p>The scenario presents a busy highway with no existing charging stations, and the government is planning where to build new ones.Given the high traffic volume and the absence of charging infrastructure, building a station here would fill a critical gap, support electric vehicle drivers, and encourage the transition to sustainable transportation.</p>
<p>Based on the available information, Outcome 1: The government should build a charging station here is more supported.and justify your answer.Ask a follow-up yes or no question so that you can better decide which outcome is better supported by the scenario and the condition.You can only ask for a specific situation.</p>
<p>US E R</p>
<p>Scenario: You want to move around with your cell phone when it is being charged.Condition: The charger is portable and the user carries the charger.</p>
<p>Outcome 1: You can move around more freely with your cell phone with a one-foot cord rather than a six feet cord.Outcome 2: You can move around more freely with your cell phone with a six-foot cord rather than a one feet cord.</p>
<p>AS S I S T A N T</p>
<p>In this scenario, the primary factor to consider is cord length, in most situations, a greater cord length allows more movement freedom, as you can be further from the outlet and still keep your device charged.However, considering that the user carries the charger, it might bring about the confusion between the two outcomes.Thus, to further ascertain which outcome is supported better, I'd ask the following question: Does the portable charger need to be plugged into an outlet to charge the phone?</p>
<p>Figure 2 :
2
Figure 2: Overview of BIRD.Given a scenario S, LLMs generate the factors F (a/b/...) with potential values (f1 ∈ {a.1,a.2}).BIRD approximates a Bayesian network parameterized by P(Oi|fj), and optimizes by sampling LLM coarse predictions on PLLM(Oi|f ), f ∈ F (F is the set of all value combinations of F , e.g., f = (a.1,b.2, c.1, d.2, e.1, f.2)), and minimizing the distributional distance between an approximated P estimated (Oi|f ) and PLLM(Oi|f ).At inference time, each context C (S+U1/S+U2/...) is mapped to some fj via entailment, and a probability is derived using P estimated .BIRD can further generate follow-up questions.</p>
<p>as for a given C, only one value can be chosen for each factor as in Fig.2.A.2 ASSUMPTIONS FOR ESTIMATING P(O i |f )Weak ordering of the set of factor odds ratios.Suppose given factors f 1 , f 2 , ..., f N , we assess odds of o 1 , o 2 , ..., o N , respectively in favor of the outcome O i , i = 1, 2 occurring.o j = P(Oi|fj )</p>
<p>10 for epoch = 1 to E do 11 for each batch of B samples from the training set do 12 for b ∈ 1, ..., B(in parallel) do 13 P estimated (O 1 |f b ) = P LLM (O 1 |f b ) − P estimated (O 1 |f b )) 2 # Compute MSE loss for each sample, for f ja ∈ F j , f ja ̸ = f b j ,</p>
<p>17</p>
<p>for j = 1 to N do 18 for a = 1 to card(F j ) do 19</p>
<p>Figure 3 :
3
Figure 3: The interface for human evaluation on preference-based pairwise evaluation of the estimated probabilities.</p>
<p>Figure 10 :
10
Figure 10: Example Prompt for Outputting a Verbalized Probability Score</p>
<p>Figure 12 :
12
Figure 12: Example Prompt for Direct Probability Verbalization with CoT Baseline</p>
<p>Figure 13 :
13
Figure 13: Example Prompt for Explicit Comparison Baseline</p>
<p>Figure 14 :
14
Figure 14: Example COT for Decision Making Prompt</p>
<p>Figure 15 :
15
Figure 15: Example Direct Yes-or-no Follow-up Question Generation Prompt</p>
<p>Table 1 :
1
Preference-based pairwise evaluation using F1 score for predicted probabilities across various model settings.Different 1 indicates the F1 score for a category where C 1 supports the gold outcome better than C 2 , and vice versa for Different 2 .Same refers to the F1 score of the category where C
ModelDifferent1 Different2 Same AverageRandom Guessing0.3330.3330.3330.333GPT3.5 CoT0.3060.3060.2420.283GPT4 CoT0.3120.3570.2160.289Llama2-70b Instruct Logit0.2630.2280.2050.228Llama2-70b Instruct Vanilla0.3750.3330.2430.311Llama2-70b Instruct CoT0.3150.3230.2540.294Llama3.1-70b Instruct Logit0.3000.2820.2420.269Llama3.1-70b Instruct Vanilla0.3650.3010.2510.303Llama3.1-70b Instruct CoT0.3730.3510.1870.309Llama2-70b Instruct EC<em>0.5300.5290.2070.503Llama3.1-70b Instruct EC</em>0.5350.5380.2860.511GPT4 EC<em>0.5880.5330.3000.540Llama3.1 BIRD (ablation w 1/2 assumption)0.5270.5320.1960.480Llama3.1 BIRD (ablation w 1/n assumption)0.5720.5840.2720.532Llama3.1 BIRD (ablation w fixed initial prob)0.6140.5970.3370.568Llama2 BIRD (ours w optimized prob)0.6140.6240.4500.592Llama3.1 BIRD (ours w optimized prob)0.6120.6250.3820.588
1 and C 2 equally support the outcome.Average refers to calculating F1 metrics globally by counting the total true positives, false negatives, and false positives.EC</em> settings see both C 1 and C 2 at the same time instead of predicting individual probabilities, hence not directly comparable.</p>
<p>Table 2 :
2
Performance comparisons of BIRD and baselines on decision-making benchmarks.BIRD optimized is our final model; chain-of-thought with self-consistency (CoT) is our main baseline; others are ablation baselines.
DatasetBIRD 1/2 BIRD 1/n BIRD fixed prob BIRD optimized (ours) CoTLlama-2-70b-instructTODAY73.772.873.473.971.5PLASMA72.872.372.774.076.8COMMON2SENSE86.986.887.589.093.8Llama-3.1-70b-InstructTODAY65.568.965.574.372.6PLASMA71.366.565.773.071.5COMMON2SENSE78.185.486.792.390.8
In the general decisionmaking setting, we similarly assume a context C = S + U , two possible outcomes O 1 and O 2 ,</p>
<p>Ben Zhou, QiangNing, Daniel Khashabi, and Dan Roth.Temporal commonsense acquisition with minimal supervision.In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.7579-7589, 2020.Ben Zhou, Kyle Richardson, Qiang Ning, Tushar Khot, Ashish Sabharwal, and Dan Roth.Temporal reasoning on implicit events from distant supervision.In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1361-1371, Online, June 2021.Association for Computational Linguistics.doi: 10.18653/v1/2021.naacl-main.107.URL https://aclanthology.org/2021.naacl-main.107.Ben Zhou, Kyle Richardson, Xiaodong Yu, and Dan Roth.Learning to decompose: Hypothetical question decomposition based on comparable texts.In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.),Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp.2223-2235, Abu Dhabi, United Arab Emirates, December 2022.Association for Computational Linguistics.doi: 10.18653/v1/2022.emnlp-main.142.URL https://aclanthology.org/2022.emnlp-main.142.
A APPENDIXA.1 DERIVATION OF THE EQUATION FOR UNCERTAINTY ESTIMATIONBen Zhou, Hongming Zhang, Sihao Chen, Dian Yu, Hongwei Wang, Baolin Peng, Dan Roth,and Dong Yu. Conceptual and unbiased reasoning in language models. arXiv preprintarXiv:2404.00205, 2024.</p>
<p>11) if and only if the decision maker feels at least as sure about the outcome O i , occurring when given factors f 1 , f 2 , ..., f N we assess odds o 1 , o 2 , ..., o N -as when given factors f 1 , f 2 , ..., f N we assess odds o * 1 , o * 2 , ..., o * N .The ordering ≥ E is a Weak Ordering if and only if for any (o 1 , o 2 , ..., o N ),(o *
1 , o  *  2 , ..., o  *  N ),(o  *  *  1 , o  *  *  2 , ..., o  *  *  N ), we have:</p>
<p>Algorithm 1: Training single probabilities to estimate P(O i |f ) .input :Training set of complete information {f }, f ∈ F, outcome O i , i = 1, 2, LLM M , learning rate η, number of epochs E, batch size B. output :Trained probabilities P</p>
<p>Table 6 :
6
An example where the LLM correctly makes the decision but with inadequate emphasis on the context.A.8 ABLATION STUDY TOWARDS ABDUCTIVE FACTOR GENERATIONWe compare the final decision-making accuracy based on two different factor generation methods and present the results in Appendix Table7.We see that our proposed method performs much better than the direct generation method, indicating the effectiveness of utilizing the model's inductive bias inherited from training data.
DatasetMethodBIRD llama2 %Unknown BIRD llama3.1 %UnknownTODAYDirect70.974.168.946.3Abduction73.954.074.334.0PLASMADirect68.640.769.024.1Abduction74.031.473.019.0C2SDirect84.254.989.929.0Abduction89.034.692.327.1</p>
<p>Table 7 :
7
An ablation study towards different factor generation methods.Direct refers to the direct factor generation method.Abduction refers to our abductive factor generation.
A.9 ABLATION STUDY TOWARDS THE EFFECT OF THE TWO LOSS FUNCTIONS IN THELEARNING ALGORITHM</p>
<p>Table 8 ,
8
and demonstrate that both components of the loss function are crucial for final performance.
Datasetw/o MR Loss w/o MSE Loss BIRDTODAY65.565.274.3PLASMA60.065.773.0C2S77.286.892.3</p>
<p>Table 8 :
8
An ablation study towards the effect of the two loss functions in Eq.7 for decision making with Llama-3.1-70b-Instruct.
EXAMPLE ADDITIONAL SENTENCE SAMPLING PROMPTSY S T E MYou are given a scenario and an accompanying hypothesis. Generate5 sentences covering different conditions that would add objectiveinformation relevant to the hypothesis such that the hypothesis ismore likely to hold true. The information should not definitivelyimply the hypothesis. You must follow the below structure to justgenerate sentences with no explanations.US E RScenario: You want to move around with your cell phone when it isbeing charged.Hypothesis: You can move around more freely with your cell phoneif it is being charged with a one-foot cord rather than a six-footcord.AS S I S T A N T# The cell phone is being charged with a portable power banklocated in your pocket, allowing you to move around without beingtethered to a fixed outlet.# The user is working in a compact space where longer cords couldeasily snag on furniture or equipment, thus a one-foot cord couldminimize this risk.# The phone is needed for tasks that require frequent handling andclose proximity to the user, making a shorter cord more practicalto avoid excessive dangling.# The charging setup includes a small desktop charger that keepsthe phone elevated and stable, limiting the practicality of alonger cord.# The user is in a busy environment like a kitchen or workshop,where shorter cords can reduce the hazard of tripping or catchingon moving objects.Figure 5: Example Additional Sentence Sampling Prompt
Outcome 1: You can move around more freely with your cell phone if it is being charged with a six feet cord rather than a one feet cord.</p>
<p>Based on a scenario, delete value in the list if the key does not imply value.You should output an explanation and then a json.You must generate double quotes.Here are rules that you must follow: -Ensure that retained values align with the key's implications, do not assume causality or infer a connection if the value is not clearly stated in the key.Avoid over-generalizations and ensure logical consistency in your decisions.-Belenientwith deletions.Only remove a value if the key clearly does not imply the value.-Providea clear explanation for each deletion or retention.As an AI assistant, your role is to respond accurately to user queries.While answering think step-by-step and justify your answer.Analyze the given scenario and condition to determine the likelihood of the outcomes.Use only the information provided, without relying on prior knowledge.Assess the probability using the specified terms: 'very likely', 'likely', 'somewhat likely', 'neutral', 'somewhat unlikely', 'unlikely', 'very unlikely'.Ensure that your assessments are complementary: if one outcome is deemed 'likely', the other must be 'unlikely', and so on.You should first give your reasons and then format your final answer.US E R Scenario: You want to move around with your cell phone when it is being charged.Condition: The charger is portable.The user stays very close to the charger.The user carries the charger.
EXAMPLE PROMPT FOR OUTPUTTING A VERBALIZED PROBABILITY SCORESY S T E MEXAMPLE PROMPT FOR REEVALUATING IMPLIED VALUESY S T E MUS E RScenario: The government is planning the locations for buildingcharging stations.{"The location is on a busy highway with no existing chargingstations.":["No nearby charging stations","Location is on a major travel route, serving long-distance EVtravelers","Nearby amenities like restaurants, shops, and rest areas for userswhile charging"]}AS S I S T A N TExplanation: "No nearby charging stations" was kept because thekey explicitly mentions the absence of existing charging stationsin the area. Similarly, "Location is on a major travel route,serving long-distance EV travelers" was retained because being on abusy highway strongly implies that it serves as a key travel routefor such travelers. However, "Nearby amenities like restaurants,shops, and rest areas for users while charging" was removed sincethe key does not directly mention or suggest the presence of theseamenities.List: {"The location is on a busy highway with no existingcharging stations.":["No nearby charging stations","Location is on a major travel route, serving long-distance EVtravelers"]}Figure 9: Example Prompt for Reevaluating Implied Value. Explanation is optional.Figure 6: Example Factor Summarization from Additional Sentences Prompt
F is the set of all possible combinations of different assignments of values to all factors (e.g., selecting a value from each of the factors a/b/c/d/e/f in Fig.
). Refer to §3.1.
Note that our framework can later accommodate any additional factors by selectively recalculating specific parts as needed.
Weights here refer to a set of "hyperparameters" that decides how highly the final decision regards each factor and how correlated the factors are considered, not the learnable parameters in our Bayesian network.
https://huggingface.co/upstage/Llama-2-70b-instruct and https:// huggingface.co/meta-llama/Llama-3.1-70B-Instruct
Here O1 will be the gold outcome. During evaluation, the two conditions can also support O2 over O1.
For proper evaluation, systems predict probabilities one at a time, without seeing other contexts.
Although the setting may seem to favor our proposed method, it is a fair comparison, because "unknown" predictions are label-agnostic, and we can use the same direct inference approach, such as CoT, for such cases.
We use GPT-4-0613 when utilizing GPT-4 for annotations.
ACKNOWLEDGEMENTSThis work was sponsored by the Army Research Office and was accomplished under Grant Number W911NF-20-1-0080.The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government.The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.This work was also supported by the Office of Naval Research [ONR N00014-23-1-2364].We thank Kyurae Kim, Ollie Liu, Vivek Gupta and colleagues at CogComp UPenn for valuable discussions.Weight and Size of the Cup:The cup is light and small The cup is heavy and large The cup is of medium weight and size Material of the Cup:The cup is fragile The cup is sturdy The cup is made of moderately durable material Strength of the People:The person is able-bodied and strong enough to carry the cup The combined strength of six people is greater than that of one person Experience of the Person or People Involved:The person has more experience in carrying objects The six people have more experience in lifting objects Formation of the People:The six people carry the cup in a specific formation designed to distribute the weight more evenly The six people do not carry the cup in a specific formation Llama-3.1-70b-Instruct Generated Factors Cup Size and Weight:The cup is small and light The cup is large and heavy The cup is small and heavy The cup is large and light Cup Material:The cup is fragile The cup is sturdy The cup is constructed from a material of moderate durability Cup Design and Features:The cup has a comfortable grip and balanced ergonomics The cup has an uncomfortable grip and unbalanced ergonomics The cup has a neutral grip and average ergonomics Carrier's Physical Abilities:The one carrier has more strength and endurance than the six carries The combined strength of the six carries is stronger than the one carrier Group Dynamics and Coordination:The group is well-coordinated and works together seamlessly The group is disorganized and has difficulty working together The group has average coordination and works together moderately well Physical Demands of the Task: The task requires exceptional strength and endurance The task requires minimal strength and endurance The task requires moderate strength and endurance The Size and Shape of the Carrying Area: The carrying area is narrow and confined The carrying area is wide and openA.6 BASELINE DESCRIPTIONWe adopt the following baselines to evaluate our BIRD framework.• Vanilla Verbalization with Self-Consistency (Vanilla).(Wang et al., 2023)Given the scenario, an additional condition, and two corresponding outcomes, the model is asked to directly verbalize an estimated probability for each outcome.We show the Prompt in Appendix Fig.11.The above process is carried out independently for each of the two additional conditions.The additional conditions with higher output probabilities for the gold outcome are the predicted answers.We use temperature = 0.7 to generate 3 responses and take the majority vote answer as the final answer.• Logit-based Probability.We directly use the output token probability for the decision.We use greedy decoding here to set the temperature = 0.• Chain-of-Thought Verbalization with Self-consistency (CoT).(Wei et al., 2022)We ask for a reasoning process to think step by step before verbalizing an estimated probability.The remaining process is the same as vanilla verbalization with SC.We show the Prompt in Appendix Fig.12.• Explicit Comparison (EC).Instead of evaluating each additional condition separately, both conditions are presented simultaneously to the model.Given the scenario and the gold outcome, the model is asked to select which additional condition better supports the outcome without outputting any probabilities.Note that the task is less challenging for this particular baseline than the others due to the explicit formatting of inference in this baseline.As a result, performances under the EC setting are not directly comparable with our model and other baselines' performances since they only see one condition at a time and cannot perform argmax operations.We show the Prompt in Appendix Fig.13.• 1/2 Assumption.Instead of utilizing the learning algorithm mentioned in §3.3 to estimate P(O i |f ).We adopt a neutral assumption.An outcome is determined based on the factors only if, except the values of the factors that are neutral, the values of all remaining factors uniformly support that particular outcome.Conversely, if the values of the factors indicate different outcomes and there is no prior information, we cannot conclusively determine the outcome.The intuition behind this assumption is that as we focus solely on the present context, if the current information supports both outcomes and no additional information is available to finalize the decision, it becomes impossible to predict which outcome is more likely to occur.• 1/N Assumption.Instead of utilizing the learning algorithm mentioned in §3.3 to estimate P(O i |f ), we adopt another commonly used neutral assumption.The outcome O i 's probability is determined by the aggregate count of factors whose values support outcome O i .• Fixed Initial Prob.Instead of training with the algorithm to get an estimated prob for each P(O i |f j ) in §3.3, we utilize the initial probability of Eq.5 to directly estimate P(O i |f ) with Eq.4.A.7 ANALYSIS OF DIRECT DECISION MAKINGWe first conduct error analysis for Llama-2-70b-instruct on direct decision making where BIRD gets the wrong outcomes while CoT makes the correct decisions.We detail the type of errors with its frequency percentage in Appendix Table4.We observe that our proposed method outperforms the chain-of-thought baseline with Llama-3.1 rather than using Llama-2.This is partially because Llama-3.1 generates more diverse and accurate factors, and we present an example in Appendix Table5.A.10 EXPERIMENT SETTINGS FOR USING ESTIMATED PROBABILITIES AS ADDITIONAL TRAINING DATAThe T5-large model and PatternTime(Zhou et al., 2021)are specialized temporal reasoning models, as outlined inFeng et al. (2023).Both models were specifically fine-tuned on temporal reasoning datasets using the original temporal training data fromFeng et al. (2023).In contrast, the additional training data from COMMON2SENSE in §4.3 focuses on commonsense reasoning.A.11 YES-OR-NO FOLLOWUP QUESTION GENERATIONWe experiment to see if BIRD can further facilitate decision making in a controllable way, where we use BIRD to generate a yes-or-no follow-up question about an unobserved factor in the context.Specifically, given a scenario and an additional condition, we first identify which factors and their corresponding values are implied by the condition, we then select a factor that is not mentioned and is most important for the decision in the remaining unmentioned factors, as the target question area.We adopt the factor value that better aligns with the outcome direction suggested by the current implied factor values and has the largest condition probability.We then rewrite the statement of the factor value into its corresponding question.Simultaneously, we ask Llama2-70b-instruct to directly generate a yes-or-no follow-up question.The prompt is in Appendix Fig.15.We use MTurk and ask annotators to provide pairwise preference between the BIRD generated question and LLM directly generated question.We adopt the same Mturk setting as mentioned in §A.5.we carry out the human annotation process three times, using the majority vote to find the preferred generated question.We show the experiment interfaces and an example in Appendix Fig.4. Appendix Table9shows that the follow-up questions generated by BIRD are more often preferred.US E RHere is a scenario: ... Sentence: ... List: "factor": [ "value 1", ... ] AS S I S T A N T {"explanation": ..., "answer": a value/None}
A multiplicative formula for aggregating probability assessments. Robert F Bordley, Management science. 28101982</p>
<p>Plasma: Making small language models better procedural knowledge models for (counterfactual) planning. Faeze Brahman, Chandra Bhagavatula, Valentina Pyatkin, Jena D Hwang, Lorraine Xiang, Hirona J Li, Soumya Arai, Keisuke Sanyal, Xiang Sakaguchi, Yejin Ren, Choi, 2023</p>
<p>Uncertain natural language inference. Tongfei Chen, Zhengping Jiang, Adam Poliak, Keisuke Sakaguchi, Benjamin Van Durme, 10.18653/v1/2020.acl-main.774Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Generic temporal reasoning with differential analysis and explanation. Yu Feng, Ben Zhou, Haoyu Wang, Helen Jin, Dan Roth, 10.18653/v1/2023.acl-long.671Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Bayesian network classifiers. Nir Friedman, Dan Geiger, Moises Goldszmidt, Machine learning. 291997</p>
<p>Bayesian Data Analysis. Andrew B Gelman, John B Carlin, Hal S Stern, Donald B Rubin, 1995Chapman and Hall/CRCBoca Ratan, Florida</p>
<p>Towards uncertainty-aware language agent. Jiuzhou Han, Wray Buntine, Ehsan Shareghi, 2024</p>
<p>Training chain-of-thought via latent-variable inference. Matthew Douglas Hoffman, Du Phan, Sholto David Dohan, Tuan Douglas, Anh Le, Aaron T Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, Rif A Saurous, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Decomposing uncertainty for large language models through input clarification ensembling. Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, Yang Zhang, Proceedings of the 41st International Conference on Machine Learning. Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, Felix Berkenkamp, the 41st International Conference on Machine LearningPMLRJul 2024235</p>
<p>Amortizing intractable inference in large language models. J Edward, Moksh Hu, Eric Jain, Younesse Elmoznino, Guillaume Kaddar, Yoshua Lajoie, Nikolay Bengio, Malkin, The Twelfth International Conference on Learning Representations. 2024</p>
<p>A perspective on judgment and choice: mapping bounded rationality. Daniel Kahneman, 10.1037/0003-066X.58.9.697The American psychologist. 2003. 200358</p>
<p>Position: LLMs can't plan, but can help planning in LLM-modulo frameworks. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Paul Saldyt, Anil B Murthy, Forty-first International Conference on Machine Learning. 2024</p>
<p>Association for Computational Linguistics. Nora Kassner, Oyvind Tafjord, Ashish Sabharwal, Kyle Richardson, Hinrich Schuetze, Peter Clark, 10.18653/v1/2023.emnlp-main.877Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeDecember 2023Language models with rationality</p>
<p>Probabilistic Graphical Models: Principles and Techniques -Adaptive Computation and Machine Learning. Daphne Koller, Nir Friedman, 2009The MIT Press</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Leveraging unstructured statistical knowledge in a probabilistic language of thought. Michael Henry Alexander K Lew, Tessler, Joshua B Vikash K Mansinghka, Tenenbaum, Proceedings of the annual conference of the cognitive science society. the annual conference of the cognitive science society2020</p>
<p>Deceptive semantic shortcuts on reasoning chains: How far can models go without hallucination?. Bangzheng Li, Ben Zhou, Fei Wang, Xingyu Fu, Dan Roth, Muhao Chen, 10.18653/v1/2024.naacl-long.424Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Kevin Duh, Helena Gomez, Steven Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJune 20241</p>
<p>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. Yicheng Bill Yuchen Lin, Karina Fu, Prithviraj Yang, Faeze Ammanabrolu, Shiyu Brahman, Chandra Huang, Yejin Bhagavatula, Xiang Choi, Ren, ArXiv, abs/2305.173902023</p>
<p>Generating with confidence: Uncertainty quantification for black-box large language models. Zhen Lin, Shubhendu Trivedi, Jimeng Sun, 2024</p>
<p>DeLLMa: Decision making under uncertainty with large language models. Ollie Liu, Deqing Fu, Dani Yogatama, Willie Neiswanger, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Some philosophical problems from the standpoint of artificial intelligence. John Mccarthy, Patrick J Hayes, Readings in artificial intelligence. Elsevier1981</p>
<p>Thinking fast and slow. N F Mcglynn, Australian veterinary journal. 92N212014</p>
<p>A multi-axis annotation scheme for event temporal relations. Qiang Ning, Hao Wu, Dan Roth, 10.18653/v1/P18-1122Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational LinguisticsJuly 20181</p>
<p>ThinkSum: Probabilistic reasoning over sets using large language models. Batu Ozturkler, Nikolay Malkin, Zhen Wang, Nebojsa Jojic, 10.18653/v1/2023.acl-long.68Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Robots that ask for help: Uncertainty alignment for large language model planners. Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, Anirudha Majumdar, 7th Annual Conference on Robot Learning. 2023</p>
<p>Integer linear programming inference for conditional random fields. Dan Roth, Wen-Tau Yih, Proceedings of the 22nd international conference on Machine learning. the 22nd international conference on Machine learning2005</p>
<p>COM2SENSE: A commonsense reasoning benchmark with complementary sentences. Shikhar Singh, Nuan Wen, Yu Hou, Pegah Alipoormolabashi, Te-Lin Wu, Xuezhe Ma, Nanyun Peng, 10.18653/v1/2021.findings-acl.78Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational LinguisticsAugust 2021</p>
<p>Entailer: Answering questions with faithful and truthful chains of reasoning. Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark, 10.18653/v1/2022.emnlp-main.134Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher Manning, 10.18653/v1/2023.emnlp-main.330Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Judgment under uncertainty: Heuristics and biases. Amos Tversky, Daniel Kahneman, 10.1126/science.185.4157.1124Science. 1974185</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Break it down: A question understanding benchmark. Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, Jonathan Berant, 10.1162/tacla00309Transactions of the Association for Computational Linguistics. 82020</p>
<p>From word models to world models: Translating from natural language to the probabilistic language of thought. Lionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, K Vikash, Jacob Mansinghka, Joshua B Andreas, Tenenbaum, 2023</p>
<p>Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, The Twelfth International Conference on Learning Representations. 2024</p>
<p>To believe or not to believe your llm. Yasin Abbasi Yadkori, Ilja Kuzborskij, András György, Csaba Szepesvári, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
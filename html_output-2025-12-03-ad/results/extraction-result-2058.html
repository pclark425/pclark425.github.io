<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2058 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2058</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2058</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-52.html">extraction-schema-52</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training machine learning models or RL agents using circuit simulators (including analog, digital, RF, mixed-signal, or power circuits), with particular focus on simulator fidelity levels, sim-to-real transfer experiments, and comparisons of different simulation approaches.</div>
                <p><strong>Paper ID:</strong> paper-279119261</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.03122v1.pdf" target="_blank">AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation</a></p>
                <p><strong>Paper Abstract:</strong> Analog circuit topology synthesis is integral to Electronic Design Automation (EDA), enabling the automated creation of circuit structures tailored to specific design requirements. However, the vast design search space and strict constraint adherence make efficient synthesis challenging. Leveraging the versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel reinforcement learning (RL)-based framework for automated analog circuit synthesis. The framework operates in two phases: instruction tuning, where an LLM learns to generate circuit topologies from structured prompts encoding design constraints, and RL refinement, which further improves the instruction-tuned model using reward models that evaluate validity, efficiency, and output voltage. The refined model is then used directly to generate topologies that satisfy the design constraints. Empirical results show that AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by ~14% compared to the best baselines, while reducing duplicate generation rates by ~38%. It achieves over 60% success in synthesizing valid circuits with limited training data, demonstrating strong generalization. These findings highlight the framework's effectiveness in scaling to complex circuits while maintaining efficiency and constraint adherence, marking a significant advancement in AI-driven circuit design.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2058.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2058.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training machine learning models or RL agents using circuit simulators (including analog, digital, RF, mixed-signal, or power circuits), with particular focus on simulator fidelity levels, sim-to-real transfer experiments, and comparisons of different simulation approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AUTOCIRCUIT-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AUTOCIRCUIT-RL (AC-RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-phase framework that instruction-tunes a Large Language Model (LLM) to generate circuit netlists and then refines the LLM with reinforcement learning (PPO) using learned reward models for validity, efficiency, and output voltage; includes an iterative adaptation stage using high-quality samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>circuit_type</strong></td>
                            <td>power converter (switching power converter topologies)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_used</strong></td>
                            <td>Ngspice</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_fidelity_details</strong></td>
                            <td>Ngspice time-domain switching simulations were used to identify valid circuits and to measure output voltage and efficiency. The paper does not report modeling of parasitic capacitances/resistances/inductances beyond what is inherent in component models, nor explicit modeling of thermal effects, noise, process corners, aging, or full electromagnetic coupling. Simulations use fixed duty cycles and fixed device parameters; no mention of explicit parasitic extraction or advanced layout-level effects.</td>
                        </tr>
                        <tr>
                            <td><strong>component_models</strong></td>
                            <td>Fixed-parameter device models in NGSpice: capacitors (10 µF), inductors (10 µH), and MOSFETs with fixed unspecified parameter sets; treated as the SPICE device models available in NGSpice for the chosen devices, with no reported BSIM or foundry process corners or added parasitics.</td>
                        </tr>
                        <tr>
                            <td><strong>ml_model_type</strong></td>
                            <td>Transformer LLM (GPT-Neo, StableLM, Llama-3, MPT) fine-tuned with supervised instruction tuning then refined with PPO (policy-gradient RL) using reward models (RLAIF).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Topology generation / automated circuit topology synthesis for switching power converters (generate netlists that meet component, efficiency, and output-voltage constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_transfer</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_performance</strong></td>
                            <td>AUTOCIRCUIT-RL yields ~12% more valid circuits and ~14% higher efficiency than the best LLM baselines; reduces duplicate generation rate by ~38%; achieves >98% component-pool adherence; >60% success generating valid circuits for 6–10 components (few-shot); per-design generation time ~2–3.5 seconds on two NVIDIA V100 GPUs. Evaluations reported using NGSpice simulator and classifier/regressor estimators.</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>randomization_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>physics_informed_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>physics_constraints_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_claims</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Yes — compares (a) instruction-tuned LLMs vs vanilla fine-tuning vs RL-refinement (AC-RL) showing substantial gains with RL refinement; (b) Gumbel-Max multi-objective fine-tuning (improves over standard fine-tuning but lags AC-RL by ~9% in validity/efficiency); (c) iterative adaptation: removing iterative adaptation causes ~8% decrease in circuit efficiency score. No ablations on simulator fidelity (parasitics, noise, process corners) were performed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2058.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2058.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training machine learning models or RL agents using circuit simulators (including analog, digital, RF, mixed-signal, or power circuits), with particular focus on simulator fidelity levels, sim-to-real transfer experiments, and comparisons of different simulation approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ngspice</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ngspice (Ngspice users manual version 23)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source SPICE circuit simulator used to simulate generated netlists, determine validity, and collect output voltage and efficiency values for evaluation and dataset labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ngspice users manual version 23.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>circuit_type</strong></td>
                            <td>power converter switching circuits (used to simulate datasets of 4–10 component switching converters)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_used</strong></td>
                            <td>Ngspice</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_fidelity_details</strong></td>
                            <td>Standard Ngspice circuit-level transient simulations were used to compute Vout and efficiency across specified duty cycles. The paper does not describe adding advanced fidelity features such as extracted parasitics, process corners, thermal coupling, noise modeling, device aging, or electromagnetic effects; simulations used fixed device parameter sets and idealized component values (e.g., fixed capacitor and inductor values).</td>
                        </tr>
                        <tr>
                            <td><strong>component_models</strong></td>
                            <td>SPICE device models present in Ngspice with fixed parameter selections; capacitors and inductors treated as ideal components with fixed values (10 µF, 10 µH respectively); MOSFETs simulated with unspecified built-in NGSpice transistor models and fixed parameters (no explicit mention of BSIM-level process modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>ml_model_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used as the ground-truth simulator/evaluator to label dataset samples (valid/invalid) and measure efficiency and Vout for training reward models and post-generation validation.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_transfer</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_performance</strong></td>
                            <td>Used to compute per-netlist validity and simulated efficiency/Vout metrics; specific simulator numeric performance metrics are the same as reported for AUTOCIRCUIT-RL (used as evaluation oracle).</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>randomization_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>physics_informed_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>physics_constraints_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_claims</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>No ablation studies on simulator fidelity reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2058.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2058.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training machine learning models or RL agents using circuit simulators (including analog, digital, RF, mixed-signal, or power circuits), with particular focus on simulator fidelity levels, sim-to-real transfer experiments, and comparisons of different simulation approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRAPH-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphVAE (variational autoencoder for graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based VAE baseline that models circuit netlists as undirected graphs and generates topologies from continuous embeddings; used here as a non-LLM baseline for topology generation and compared against AUTOCIRCUIT-RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>circuit_type</strong></td>
                            <td>power converter topologies (used for 6–10 component circuits comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_used</strong></td>
                            <td>Ngspice (used for measuring validity and efficiency of generated samples)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_fidelity_details</strong></td>
                            <td>Evaluations of GRAPH-VAE generated topologies were performed with Ngspice as above; the GRAPH-VAE itself does not include a physics simulator but its outputs are evaluated with Ngspice transient simulations. No differentiation of simulator fidelity levels was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>component_models</strong></td>
                            <td>GRAPH-VAE generates graph netlists (abstract); component electrical behavior is realized during Ngspice evaluation using the same fixed-parameter models as the rest of the study.</td>
                        </tr>
                        <tr>
                            <td><strong>ml_model_type</strong></td>
                            <td>Variational Autoencoder (graph VAE) with graph convolutional encoder and fully connected decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Topology generation conditioned on prompt (for 6–10 component circuits) and evaluated for validity and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_transfer</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_performance</strong></td>
                            <td>Reported success rates for 6–10 component circuits are much lower than AC-RL (e.g., GRAPHVAE overall success rates: 21.5% (6C) down to 12.8% (10C) as evaluated with Ngspice).</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>randomization_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>physics_informed_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>physics_constraints_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_claims</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>No simulator-fidelity ablations; comparisons are between generation methods (GRAPH-VAE vs LLM-based methods) using the same Ngspice evaluation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2058.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2058.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training machine learning models or RL agents using circuit simulators (including analog, digital, RF, mixed-signal, or power circuits), with particular focus on simulator fidelity levels, sim-to-real transfer experiments, and comparisons of different simulation approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CIRCUITSYNTH-GUMBEL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CircuitSynth variant (Gumbel-Max fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline LLM-based method that fine-tunes an LLM with additional objectives for circuit validity and efficiency using a Gumbel-softmax relaxation to enable gradient-based optimization over discrete netlist outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Circuitsynth: Leveraging large language models for circuit topology synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>circuit_type</strong></td>
                            <td>power converter topologies (4–5 component circuits and comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_used</strong></td>
                            <td>Ngspice (used to label training data and to evaluate validity/efficiency for comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_fidelity_details</strong></td>
                            <td>Uses the same Ngspice-based dataset and evaluations as AUTOCIRCUIT-RL. The paper does not report additional fidelity distinctions for this baseline; the Gumbel approach optimizes against classifier/regressor estimates derived from Ngspice-labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>component_models</strong></td>
                            <td>Same fixed-parameter component models as the dataset (capacitors 10 µF, inductors 10 µH, MOSFETs with fixed parameters) used during Ngspice evaluation and classifier/regressor training.</td>
                        </tr>
                        <tr>
                            <td><strong>ml_model_type</strong></td>
                            <td>LLM fine-tuning with Gumbel-softmax (continuous relaxation) for multi-objective optimization (validity and efficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Topology generation with multi-objective optimization for validity and efficiency using Gumbel-based fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_transfer</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_performance</strong></td>
                            <td>Improves over standard fine-tuning but underperforms AUTOCIRCUIT-RL by ~9% in circuit validity and efficiency metrics (as measured by Ngspice and classifier/regressor estimates).</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>randomization_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>physics_informed_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>physics_constraints_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_claims</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Compared against standard fine-tuning and AC-RL; no ablation that isolates simulator features.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2058.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2058.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training machine learning models or RL agents using circuit simulators (including analog, digital, RF, mixed-signal, or power circuits), with particular focus on simulator fidelity levels, sim-to-real transfer experiments, and comparisons of different simulation approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Search-based algorithms (Fan et al., Zhao & Zhang)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Traditional search-based topology synthesis methods (tree sampling, genetic algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work employing search-based methods (e.g., tree sampling, genetic algorithms) that rely on many SPICE simulation queries to find valid topologies and are used here as a computational-cost baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>circuit_type</strong></td>
                            <td>power converter topology synthesis (automatic power converter design)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_used</strong></td>
                            <td>Typically SPICE-based simulators in prior work (the paper references Fan et al., 2021 which used many simulation queries), but this paper does not itself run those simulators beyond referencing their high-simulation-cost behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_fidelity_details</strong></td>
                            <td>Not detailed in this paper; referenced works reportedly use SPICE evaluations (hundreds of simulations per design) to evaluate candidate designs. No fidelity-level comparisons provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>component_models</strong></td>
                            <td>Not specified in this paper for the referenced methods.</td>
                        </tr>
                        <tr>
                            <td><strong>ml_model_type</strong></td>
                            <td>Not ML in the same sense — search heuristics, tree sampling, genetic programming; some prior works combine RL for sizing but are referenced mainly for simulation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Topology search via heuristic or evolutionary search requiring many SPICE evaluations per candidate to meet constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_transfer</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_performance</strong></td>
                            <td>Reported as computationally expensive: prior methods require hundreds of seconds (hundreds of SPICE queries) per design and are contrasted with AC-RL's ~2–3.5s per design.</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>randomization_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>physics_informed_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>physics_constraints_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_claims</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>From specification to topology: Automatic power converter design via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Circuitsynth: Leveraging large language models for circuit topology synthesis <em>(Rating: 2)</em></li>
                <li>Auto-spice: Leveraging llms for dataset creation via automated spice netlist extraction from analog circuit diagrams <em>(Rating: 1)</em></li>
                <li>Language-model-based topology generation for analog integrated circuits <em>(Rating: 1)</em></li>
                <li>GraphVAE <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2058",
    "paper_id": "paper-279119261",
    "extraction_schema_id": "extraction-schema-52",
    "extracted_data": [
        {
            "name_short": "AUTOCIRCUIT-RL",
            "name_full": "AUTOCIRCUIT-RL (AC-RL)",
            "brief_description": "A two-phase framework that instruction-tunes a Large Language Model (LLM) to generate circuit netlists and then refines the LLM with reinforcement learning (PPO) using learned reward models for validity, efficiency, and output voltage; includes an iterative adaptation stage using high-quality samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "circuit_type": "power converter (switching power converter topologies)",
            "simulator_used": "Ngspice",
            "simulator_fidelity_details": "Ngspice time-domain switching simulations were used to identify valid circuits and to measure output voltage and efficiency. The paper does not report modeling of parasitic capacitances/resistances/inductances beyond what is inherent in component models, nor explicit modeling of thermal effects, noise, process corners, aging, or full electromagnetic coupling. Simulations use fixed duty cycles and fixed device parameters; no mention of explicit parasitic extraction or advanced layout-level effects.",
            "component_models": "Fixed-parameter device models in NGSpice: capacitors (10 µF), inductors (10 µH), and MOSFETs with fixed unspecified parameter sets; treated as the SPICE device models available in NGSpice for the chosen devices, with no reported BSIM or foundry process corners or added parasitics.",
            "ml_model_type": "Transformer LLM (GPT-Neo, StableLM, Llama-3, MPT) fine-tuned with supervised instruction tuning then refined with PPO (policy-gradient RL) using reward models (RLAIF).",
            "task_description": "Topology generation / automated circuit topology synthesis for switching power converters (generate netlists that meet component, efficiency, and output-voltage constraints).",
            "sim_to_real_transfer": false,
            "real_hardware_details": null,
            "simulation_performance": "AUTOCIRCUIT-RL yields ~12% more valid circuits and ~14% higher efficiency than the best LLM baselines; reduces duplicate generation rate by ~38%; achieves &gt;98% component-pool adherence; &gt;60% success generating valid circuits for 6–10 components (few-shot); per-design generation time ~2–3.5 seconds on two NVIDIA V100 GPUs. Evaluations reported using NGSpice simulator and classifier/regressor estimators.",
            "real_world_performance": null,
            "fidelity_comparison": false,
            "fidelity_comparison_results": "",
            "domain_randomization": false,
            "randomization_details": "",
            "physics_informed_approach": null,
            "physics_constraints_used": "",
            "transfer_failure_cases": "",
            "minimal_fidelity_claims": "",
            "ablation_studies": "Yes — compares (a) instruction-tuned LLMs vs vanilla fine-tuning vs RL-refinement (AC-RL) showing substantial gains with RL refinement; (b) Gumbel-Max multi-objective fine-tuning (improves over standard fine-tuning but lags AC-RL by ~9% in validity/efficiency); (c) iterative adaptation: removing iterative adaptation causes ~8% decrease in circuit efficiency score. No ablations on simulator fidelity (parasitics, noise, process corners) were performed.",
            "uuid": "e2058.0"
        },
        {
            "name_short": "Ngspice",
            "name_full": "Ngspice (Ngspice users manual version 23)",
            "brief_description": "Open-source SPICE circuit simulator used to simulate generated netlists, determine validity, and collect output voltage and efficiency values for evaluation and dataset labeling.",
            "citation_title": "Ngspice users manual version 23.",
            "mention_or_use": "use",
            "circuit_type": "power converter switching circuits (used to simulate datasets of 4–10 component switching converters)",
            "simulator_used": "Ngspice",
            "simulator_fidelity_details": "Standard Ngspice circuit-level transient simulations were used to compute Vout and efficiency across specified duty cycles. The paper does not describe adding advanced fidelity features such as extracted parasitics, process corners, thermal coupling, noise modeling, device aging, or electromagnetic effects; simulations used fixed device parameter sets and idealized component values (e.g., fixed capacitor and inductor values).",
            "component_models": "SPICE device models present in Ngspice with fixed parameter selections; capacitors and inductors treated as ideal components with fixed values (10 µF, 10 µH respectively); MOSFETs simulated with unspecified built-in NGSpice transistor models and fixed parameters (no explicit mention of BSIM-level process modeling).",
            "ml_model_type": "",
            "task_description": "Used as the ground-truth simulator/evaluator to label dataset samples (valid/invalid) and measure efficiency and Vout for training reward models and post-generation validation.",
            "sim_to_real_transfer": false,
            "real_hardware_details": null,
            "simulation_performance": "Used to compute per-netlist validity and simulated efficiency/Vout metrics; specific simulator numeric performance metrics are the same as reported for AUTOCIRCUIT-RL (used as evaluation oracle).",
            "real_world_performance": null,
            "fidelity_comparison": false,
            "fidelity_comparison_results": "",
            "domain_randomization": false,
            "randomization_details": "",
            "physics_informed_approach": null,
            "physics_constraints_used": "",
            "transfer_failure_cases": "",
            "minimal_fidelity_claims": "",
            "ablation_studies": "No ablation studies on simulator fidelity reported.",
            "uuid": "e2058.1"
        },
        {
            "name_short": "GRAPH-VAE",
            "name_full": "GraphVAE (variational autoencoder for graphs)",
            "brief_description": "A graph-based VAE baseline that models circuit netlists as undirected graphs and generates topologies from continuous embeddings; used here as a non-LLM baseline for topology generation and compared against AUTOCIRCUIT-RL.",
            "citation_title": "",
            "mention_or_use": "use",
            "circuit_type": "power converter topologies (used for 6–10 component circuits comparison)",
            "simulator_used": "Ngspice (used for measuring validity and efficiency of generated samples)",
            "simulator_fidelity_details": "Evaluations of GRAPH-VAE generated topologies were performed with Ngspice as above; the GRAPH-VAE itself does not include a physics simulator but its outputs are evaluated with Ngspice transient simulations. No differentiation of simulator fidelity levels was reported.",
            "component_models": "GRAPH-VAE generates graph netlists (abstract); component electrical behavior is realized during Ngspice evaluation using the same fixed-parameter models as the rest of the study.",
            "ml_model_type": "Variational Autoencoder (graph VAE) with graph convolutional encoder and fully connected decoder.",
            "task_description": "Topology generation conditioned on prompt (for 6–10 component circuits) and evaluated for validity and efficiency.",
            "sim_to_real_transfer": false,
            "real_hardware_details": null,
            "simulation_performance": "Reported success rates for 6–10 component circuits are much lower than AC-RL (e.g., GRAPHVAE overall success rates: 21.5% (6C) down to 12.8% (10C) as evaluated with Ngspice).",
            "real_world_performance": null,
            "fidelity_comparison": false,
            "fidelity_comparison_results": "",
            "domain_randomization": false,
            "randomization_details": "",
            "physics_informed_approach": null,
            "physics_constraints_used": "",
            "transfer_failure_cases": "",
            "minimal_fidelity_claims": "",
            "ablation_studies": "No simulator-fidelity ablations; comparisons are between generation methods (GRAPH-VAE vs LLM-based methods) using the same Ngspice evaluation.",
            "uuid": "e2058.2"
        },
        {
            "name_short": "CIRCUITSYNTH-GUMBEL",
            "name_full": "CircuitSynth variant (Gumbel-Max fine-tuning)",
            "brief_description": "A baseline LLM-based method that fine-tunes an LLM with additional objectives for circuit validity and efficiency using a Gumbel-softmax relaxation to enable gradient-based optimization over discrete netlist outputs.",
            "citation_title": "Circuitsynth: Leveraging large language models for circuit topology synthesis.",
            "mention_or_use": "use",
            "circuit_type": "power converter topologies (4–5 component circuits and comparisons)",
            "simulator_used": "Ngspice (used to label training data and to evaluate validity/efficiency for comparisons)",
            "simulator_fidelity_details": "Uses the same Ngspice-based dataset and evaluations as AUTOCIRCUIT-RL. The paper does not report additional fidelity distinctions for this baseline; the Gumbel approach optimizes against classifier/regressor estimates derived from Ngspice-labeled data.",
            "component_models": "Same fixed-parameter component models as the dataset (capacitors 10 µF, inductors 10 µH, MOSFETs with fixed parameters) used during Ngspice evaluation and classifier/regressor training.",
            "ml_model_type": "LLM fine-tuning with Gumbel-softmax (continuous relaxation) for multi-objective optimization (validity and efficiency).",
            "task_description": "Topology generation with multi-objective optimization for validity and efficiency using Gumbel-based fine-tuning.",
            "sim_to_real_transfer": false,
            "real_hardware_details": null,
            "simulation_performance": "Improves over standard fine-tuning but underperforms AUTOCIRCUIT-RL by ~9% in circuit validity and efficiency metrics (as measured by Ngspice and classifier/regressor estimates).",
            "real_world_performance": null,
            "fidelity_comparison": false,
            "fidelity_comparison_results": "",
            "domain_randomization": false,
            "randomization_details": "",
            "physics_informed_approach": null,
            "physics_constraints_used": "",
            "transfer_failure_cases": "",
            "minimal_fidelity_claims": "",
            "ablation_studies": "Compared against standard fine-tuning and AC-RL; no ablation that isolates simulator features.",
            "uuid": "e2058.3"
        },
        {
            "name_short": "Search-based algorithms (Fan et al., Zhao & Zhang)",
            "name_full": "Traditional search-based topology synthesis methods (tree sampling, genetic algorithms)",
            "brief_description": "Prior work employing search-based methods (e.g., tree sampling, genetic algorithms) that rely on many SPICE simulation queries to find valid topologies and are used here as a computational-cost baseline comparison.",
            "citation_title": "",
            "mention_or_use": "mention",
            "circuit_type": "power converter topology synthesis (automatic power converter design)",
            "simulator_used": "Typically SPICE-based simulators in prior work (the paper references Fan et al., 2021 which used many simulation queries), but this paper does not itself run those simulators beyond referencing their high-simulation-cost behavior.",
            "simulator_fidelity_details": "Not detailed in this paper; referenced works reportedly use SPICE evaluations (hundreds of simulations per design) to evaluate candidate designs. No fidelity-level comparisons provided here.",
            "component_models": "Not specified in this paper for the referenced methods.",
            "ml_model_type": "Not ML in the same sense — search heuristics, tree sampling, genetic programming; some prior works combine RL for sizing but are referenced mainly for simulation cost.",
            "task_description": "Topology search via heuristic or evolutionary search requiring many SPICE evaluations per candidate to meet constraints.",
            "sim_to_real_transfer": null,
            "real_hardware_details": null,
            "simulation_performance": "Reported as computationally expensive: prior methods require hundreds of seconds (hundreds of SPICE queries) per design and are contrasted with AC-RL's ~2–3.5s per design.",
            "real_world_performance": null,
            "fidelity_comparison": false,
            "fidelity_comparison_results": "",
            "domain_randomization": null,
            "randomization_details": "",
            "physics_informed_approach": null,
            "physics_constraints_used": "",
            "transfer_failure_cases": "",
            "minimal_fidelity_claims": "",
            "ablation_studies": "",
            "uuid": "e2058.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "From specification to topology: Automatic power converter design via reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Circuitsynth: Leveraging large language models for circuit topology synthesis",
            "rating": 2
        },
        {
            "paper_title": "Auto-spice: Leveraging llms for dataset creation via automated spice netlist extraction from analog circuit diagrams",
            "rating": 1
        },
        {
            "paper_title": "Language-model-based topology generation for analog integrated circuits",
            "rating": 1
        },
        {
            "paper_title": "GraphVAE",
            "rating": 1
        }
    ],
    "cost": 0.012985,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation
3 Jun 2025</p>
<p>Prashanth Vijayaraghavan <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#112;&#114;&#97;&#115;&#104;&#97;&#110;&#116;&#104;&#118;&#64;&#105;&#98;&#109;&#46;&#99;&#111;&#109;">&#112;&#114;&#97;&#115;&#104;&#97;&#110;&#116;&#104;&#118;&#64;&#105;&#98;&#109;&#46;&#99;&#111;&#109;</a> 
IBM Almaden Research Center
95120San JoseCAUSA</p>
<p>Luyao Shi <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#108;&#117;&#121;&#97;&#111;&#46;&#115;&#104;&#105;&#64;&#105;&#98;&#109;&#46;&#99;&#111;&#109;">&#108;&#117;&#121;&#97;&#111;&#46;&#115;&#104;&#105;&#64;&#105;&#98;&#109;&#46;&#99;&#111;&#109;</a> 
IBM Almaden Research Center
95120San JoseCAUSA</p>
<p>Ehsan Degan 
IBM Almaden Research Center
95120San JoseCAUSA</p>
<p>Vandana Mukherjee 
IBM Almaden Research Center
95120San JoseCAUSA</p>
<p>Xin Zhang 
IBM Thomas J. Watson Research Center
10598Yorktown HeightsNY</p>
<p>AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation
3 Jun 20258417F908285AD83272B26102472F345BarXiv:2506.03122v1[cs.CL]
Analog circuit topology synthesis is integral to Electronic Design Automation (EDA), enabling the automated creation of circuit structures tailored to specific design requirements.However, the vast design search space and strict constraint adherence make efficient synthesis challenging.Leveraging the versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL, a novel reinforcement learning (RL)-based framework for automated analog circuit synthesis.The framework operates in two phases: instruction tuning, where an LLM learns to generate circuit topologies from structured prompts encoding design constraints, and RL refinement, which further improves the instruction-tuned model using reward models that evaluate validity, efficiency, and output voltage.The refined model is then used directly to generate topologies that satisfy the design constraints.Empirical results show that AUTOCIRCUIT-RL generates ∼12% more valid circuits and improves efficiency by ∼14% compared to the best baselines, while reducing duplicate generation rates by ∼38%.It achieves over 60% success in synthesizing valid circuits with limited training data, demonstrating strong generalization.These findings highlight the framework's effectiveness in scaling to complex circuits while maintaining efficiency and constraint adherence, marking a significant advancement in AI-driven circuit design.</p>
<p>Introduction</p>
<p>AI and machine learning have been applied to various circuit design tasks, including parameter optimization (Wang et al., 2020) and physical design (Hakhamaneshi et al., 2019), which focus on circuit optimization with a fixed circuit topology.Analog circuit topology synthesis (Bengio et al., 2013) is a fundamental aspect of EDA, where the configuration and interconnection of components directly influence circuit functionality and performance.Despite years of EDA advancements, automation of analog circuit topology synthesis has remained underexplored until recently.</p>
<p>The key challenge in circuit topology synthesis stems from the exponential growth of the design space with the number of components, making high-quality designs rare and hard to discover.This sparsity makes it difficult to satisfy specific performance and design constraints.Manual topology design remains time-consuming and requires significant expertise, while brute-force or random search methods are computationally infeasible due to the vastness of the space.</p>
<p>Existing AI methods fall into two categories: (a) AI-based search-based algorithms, including rule-based systems, heuristics, genetic algorithms (McConaghy et al., 2011), and tree-based search (Fan et al., 2021;Zhao &amp; Zhang, 2020).While partially effective, these methods struggle with scalability, efficiency, and adaptability to evolving design requirements.They often require numerous queries and long runtimes to find circuits that meet targets.For example, Fan (Fan et al., 2021) uses tree sampling for automated topology design but faces scalability and practical challenges when handling diverse performance needs, requiring over 400 simulation queries per design.Similarly, other search-based methods (Zhao &amp; Zhang, 2020) demand extensive simulations for new specifications.(b) Generative AI-based frameworks, including graph-based and LLM-based generative methods.Graph generative models use VAEs to produce netlists as undirected (Simonovsky &amp; Komodakis, 2018) or directed graphs (Dong et al., 2023;Zhang et al., 2019), but lack precise control over component count, efficiency, or power conversion ratio.Recently, LLMs have been applied to automated circuit topology synthesis (Vijayaraghavan et al., 2024;Chang et al., 2024;Lai et al., 2025), leveraging their pattern learning and design generation abilities.Unlike search-based methods, LLMs produce circuits from a single prompt after training, enabling faster generation.</p>
<p>However, most LLM approaches are limited in scale and flexibility.CircuitSynth (Vijayaraghavan et al., 2024) and similar works (Chang et al., 2024) target small circuits (up to six components).AnalogCoder (Lai et al., 2025) generates PySpice code via prompt engineering but depends on a fixed synthesis library and lacks iterative refinement, limiting exploration of novel or complex topologies, particularly for power converter circuits requiring high efficiency and specific output voltage constraints.This limitation makes it less extensible for optimizing circuit performance or handling diverse design constraints.Artisan (Chen et al., 2024) is another recent effort focusing on operational amplifier design using domain-specific LLMs.While valuable, it is highly specialized and does not generalize to other circuit families such as power converters.LaMAGIC (Chang et al., 2024) fine-tunes LLMs for netlist generation but omits iterative or performance-driven optimization, restricting adaptability to multi-objective constraints.Auto-SPICE (Bhandari et al., 2024) automates large-scale SPICE netlist generation from textbook schematics (e.g., Masala-CHAI) but focuses on data creation rather than optimization or refinement.Recent advances like AnalogXpert (Zhang et al., 2024) and Atelier (Shen et al., 2024) incorporate domain knowledge, subcircuit libraries, Chain-of-Thought prompting, and agentbased coordination.Nonetheless, these methods lack reinforcement learning for iterative, performance-driven refinement, focusing instead on structured generation and error correction.Our work advances beyond prior efforts by synthesizing more complex circuits while optimizing both topology and performance metrics.</p>
<p>In this work, we present AUTOCIRCUIT-RL (AC-RL), a reinforcement learning (RL)-based framework that refines LLM-generated circuit topologies to optimize design objectives.Our method employs two training phases: instruction tuning to generate diverse topologies from prompts, followed by RL-refinement using AI-based reward models that estimate validity, efficiency, and output voltage.This enables scaling, generalization, and multi-objective optimization with minimal manual effort.RL-refinement occurs only during training, not at inference.Empirical results show a ∼12% improvement in validity and ∼14% gain in efficiency over the best-performing LLM baselines, with few-shot generalization beyond 6 components and support for circuits with up to 10 components.Key contributions include: LLM with RL Refinement: We propose AUTOCIRCUIT-RL, a novel RL framework for analog circuit synthesis that targets constraint-driven design.Superior Performance Evaluation: Our framework's evaluations on 4 and 5-component analog circuits demonstrate superior performance in generating circuits that meet design constraints more effectively than other baseline approaches.</p>
<p>Scalability and Generalizability: Using few-shot finetuning, our framework generalizes to 6-10 components even with limited data, highlighting its scalability and adaptability in practical design scenarios.</p>
<p>Problem Statement and Dataset</p>
<p>Given an input instruction, our goal is to produce a netlist with components and their connections.Each entry in the netlist corresponds to a node in an undirected graph G, with edges indicating the connections between these nodes as in the netlist.For the choices of encoding the netlist textually, we adopt the "Incident" encoding strategy, recognized for its effectiveness in various graph-related tasks (Fatemi et al., 2024).The center part of Figure 1 illustrates an example of how a netlist is encoded.This research investigates different model variations that refine Language Models (LMs) to generate the circuit topology netlist.We compare two representation approaches: generating the netlist as lists or employing the text representation with the incident encoding method.Through empirical evaluation, our objective is to evaluate the efficacy of these LM variations in accurately and efficiently synthesizing circuit topologies from natural language instructions.We generated a dataset of switching power converter topologies with 4-10 components (Figure 2(a)).Using Random Search (RS) (Fan et al., 2021), we generated numerous unique netlists.Multiple netlists can represent the same topology by changing component order or node indices, so we report unique designs.The design space for 4-and 5component circuits is small, allowing near-exhaustive exploration; for 6+ components, exhaustive search is impractical.Therefore, we collected 10,000 unique netlists for each component count from 6 to 10.Each netlist was simulated at 5 duty cycles: 0.1, 0.3, 0.5, 0.7, and 0.9, yielding 5 times the number of unique netlists as total samples (Figure 2(a)).We used NGSpice (Nenzi &amp; Vogt, 2011) to identify valid circuits and collect output voltage and efficiency values.The circuit includes five external signal ports: Vin, Vout, GND, N-type gate signal, and P-type gate signal (renamed as ' IN','OUT','0','GATEN',and 'GATEP',respectively).Devices considered are capacitors, inductors, n-type MOSFET (FET-A), and p-type MOSFET (FET-B).Capacitors and inductors have two ports, while MOSFETs have four (drain, gate, source, body).To simplify the design space and accelerate RS generation, FET-A connects gate/body to GATEN/0, and FET-B to GATEP/IN.Devices are numbered, with shared ports using one index (Figure 1).Capacitors (10µF), inductors (10µH), and MOSFETs use fixed parameters; switching frequency is 200 kHz, input voltage 2V.</p>
<p>For training, we randomly sample approximately 100,000 unique netlists (for 4-and 5-component circuits), each with varying efficiency and output voltage values.The data is categorized into four groups: (a) Group 1 with low efficiency (efficiency &lt; 0.05), (b) Group 2 with moderate efficiency (efficiency between 0.05 and 0.7), (c) Group 3 with high efficiency but minimal output voltage difference from input, and (d) Group 4 with optimal Vout and efficiency.An example for 4-component circuits is shown in Figure 2(b).We apply a weighted sampling strategy in each batch, prioritizing Group 4 (weight 0.4) and giving the lowest priority to Group 1 (weight 0.1).This ensures that batches are enriched with data from Group 4, improving model learning on the most desirable conditions.Instruction prompts are constructed in three categories based on design constraints: (a) Component constraint (only the list of components), (b) Efficiency constraint (components with expected efficiency), and (c) Output Voltage constraint (components with input voltage and expected output voltage).</p>
<p>Proposed Approach</p>
<p>Our proposed framework, AUTOCIRCUIT-RL, is depicted in Figure 2(c).It consists of two primary phases: instruction tuning and RL refinement.In the instruction tuning phase, we fine-tune a large language model (LLM) using super-vised learning techniques.This phase focuses on training the model to comprehend instruction prompts that specify the component pool and design constraints, facilitating the efficient generation of valid circuit topologies.To further optimize the circuit topology generation process and ensure compliance with all design constraints, we incorporate reinforcement learning with AI feedback (RLAIF) (Bai et al., 2022;Lee et al., 2023).The RL refinement phase enhances circuit topology generation by integrating feedback from constraint-specific AI models in three steps: reward modeling, RL training, and iterative adaptation.</p>
<p>Instruction Tuning</p>
<p>The instruction tuning phase can be considered as a standard supervised finetuning (SFT) step, where instruction prompts specifying the component pool and design constraints are provided as input to the model, and the corresponding circuit topology generations are produced as output.We represent the pairs of instruction-circuit topologies as
D = {(X i , Y i )} T i=1
, where X i denotes the instruction prompt and Y i represents the valid circuit topology netlist using the incident encoding method concatenated with the corresponding duty cycle.This phase involves training an autoregressive language model p θ parameterized by θ to minimize the negative log-likelihood of the desired circuit topology represented using the incident encoding method.Formally,
L SFT = −E (X,Y )∼D r log π θ (y t |X, y &lt;t ) (1)
where π θ represents the LLM policy and y &lt;t denotes all tokens before the t th token in the circuit topology Y .This objective aims to ensure that the model learns to comprehend the instruction and generate the circuit topology in the incident encoding method.However, the generated circuit topology may not satisfy all the design constraints related to components, efficiency, validity, and expected output voltage.To meet such constraints, we utilize Reinforcement Learning with AI Feedback (RLAIF), which learns to refine the circuit topology generation process by maximizing rewards associated with specific constraints of interest.</p>
<p>RL-Refinement with AI Feedback</p>
<p>The RL-refinement phase aims to enhance the circuit topology generation process by leveraging feedback from constraint-specific AI models.Using the remaining data in D and the trained estimators, we define a reward function r(X, Ŷ ) that assigns a reward to an LLM-generated circuit topology Ŷ as follows:
r(X, Ŷ ) =      −1, if s valid &lt; 0.6 1, if s eff or s vout meets constraints s eff , otherwise(2)
Invalid topologies receive a negative reward.Valid topologies meeting output voltage or efficiency constraints get a reward of 1; otherwise, the efficiency estimate is used as the reward to maximize efficiency.</p>
<p>RL TUNING</p>
<p>To enhance the LLM for generating circuit topologies that better meet the constraints, we employ a reward function r(X, Ŷ ) and Proximal Policy Optimization (PPO) (Schulman et al., 2017).The base model for this refinement is the LLM fine-tuned with the instruction tuning technique discussed in Section 3.1, following the common practice in Reinforcement Learning with Human Feedback (RLHF) (Ouyang et al., 2022).Standard PPO training procedures are then applied to optimize the base model using the following reward objective function:
L RL = r(X, Ŷ ) − ηKL(π RLAIF ( Ŷ |X)||π θ ( Ŷ |X)) (3)
Here, KL represents the Kullback-Leibler divergence, and η is a hyperparameter controlling the penalty for divergence.This penalty helps prevent the model from getting trapped in local optima or straying too far from the original distribution of the supervised instruction-tuned model.</p>
<p>ITERATIVE ADAPTATION (IA)</p>
<p>Additionally, we explore the concept of iterative adaptation as a means to refine the circuit topology generation process further.Leveraging the circuits generated in the previous steps, we aim to enhance the overall synthesis task by iteratively adapting the model based on the sampled valid and highly efficient synthesized circuit topologies.Utilizing the nucleus sampling approach, we specifically target circuit topologies with an efficiency score (s eff ) exceeding 0.7.This criterion ensures that the selected circuits not only satisfy validity constraints but also exhibit high operational efficiency.To initiate the iterative adaptation process, we synthesize a dataset comprising 10,000 such samples.Starting from the RL-tuned model described in Section 3.2.2,we iteratively refine the model using Equation 3, incorporating insights from high-quality topologies into the RL-refined model from the previous iteration.Upon completion, the final model is used directly for inference, with no further tuning.In our experiments, we evaluate this iterative adaptation's effectiveness to enhance the circuit topology generation process.</p>
<p>Experiments and Results</p>
<p>In this study, we utilized the following Language Models (LMs) for generating circuit topologies: GPT-Neo-2.7 (Black et al., 2021), StableLM-3B-4E1T, Llama-3-8b (Grattafiori et al., 2024), and MPT-7b (Team et al., 2023).More details on the baseline models and implementation are provided in the Appendix A, and B.</p>
<p>Baselines</p>
<p>Our primary focus is on leveraging LLM-based generative methods for circuit design synthesis and benchmarking their potential.We compare different LLM-based methods as baselines and include GraphVAE (Simonovsky &amp; Komodakis, 2018), a non-LLM baseline, to highlight the superiority of our method in both efficiency and performance.While recent LLM-based frameworks like Analog-Coder (Lai et al., 2025) and Artisan (Chen et al., 2024) provide valuable contributions, their methodologies and application domains differ from ours.AnalogCoder employs a training-free prompt strategy with a fixed synthesis library, focusing on general analog circuits, and does not specifically address power converters, which require nuanced handling of constraints such as efficiency and output voltage.Artisan is tailored for operational amplifier design using a domain-specific LLM.In contrast, AUTOCIRCUIT-RL is currently trained using power converter designs and combines instruction tuning with RL refinement to handle diverse user prompts and optimization goals.Although these prior works are not directly applicable to our constraint-driven setting, future adaptations could make comparisons more feasible.</p>
<p>Given these considerations, we evaluate AUTOCIRCUIT-RL against the following baselines: Zero-Shot Generation: Prompts with component pools and design constraints are directly fed into large language models (LLMs) like Llama-2 (13b) and Flan-UL2 (20b) without fine-tuning, aiming to generate circuit topologies; In-Context Learning (ICL): This approach uses circuit generation demonstrations, combining input prompts with component pools, design constraints, and corresponding output circuits within the prompts.It leverages the in-context learning ability of LLMs such as Llama-2 (13b) and Flan-UL2 (20b), exploring different numbers of examples (j ∈ {5, 10, 20}) and experimenting with incident encoding and netlist structures; Prompt Tuning: This method fine-tunes LLMs like Llama-2 (13b) and Flan-UL2 (20b) for circuit topology generation using a Prompt-tuned Model (Lester et al., 2021), which learns task-specific soft prompts while keeping model parameters unchanged.We test with 100 trainable soft prompt tokens (p100); Vanilla Fine-Tuning: Standard fine-tuning is conducted on the LMs listed above.The primary objective is to minimize the negative log-likelihood for generating circuit topologies; Gumbel-Max Fine-Tuning: Drawing ideas from a prior study (Vijayaraghavan et al., 2024), we integrate multiple objectives optimizing for circuit validity and efficiency using the Gumbel-Max trick.This approach refines models fine-tuned using Llama-3 (8b) and MPT-7b architectures, allowing for more efficient circuit generation while maintaining structural validity constraints; GRAPH-VAE: This method models circuit netlists as undirected graphs and uses a variational autoencoder (VAE) to generate graphs from continuous embeddings (Simonovsky &amp; Komodakis, 2018).We did not implement DAG-based methods (Dong et al., 2023;Zhang et al., 2019), because a noticeable subset of circuits in our datasets are not DAGs.For circuits with 6-10 components, the generation process is conditioned on a SentenceBERT-encoded label vector (Reimers, 2019) derived from the input prompt, enabling controlled and guided sampling during inference; AUTOCIRCUIT-RL: We introduce AUTOCIRCUIT-RL, a comprehensive framework designed to enhance the circuit topology generation process using RL and iterative adaptation.</p>
<p>Metrics</p>
<p>In our evaluation setup, we report different metrics based on sampling 500 unique circuit topologies from each of the trained models.(a) Circuit Validity Score represents the fraction of unique circuit topologies estimated as valid,</p>
<p>RL Convergence Analysis</p>
<p>We evaluate the learning dynamics of the proposed AUTOCIRCUIT-RL framework by analyzing the convergence behavior of two key metrics during RL refinement:</p>
<p>(a) circuit efficiency and (b) success ratio, defined as the proportion of generated circuits satisfying functional and design constraints.Figure 3 illustrates these convergence curves for circuits composed of 4 and 5 components, trained using the Llama-3 backbone over ∼ 25, 000 training steps with a batch size of 16.The training exhibits several characteristic phases.In the initial phase, both efficiency and success ratio increase gradually with noticeable oscillations, reflecting the model's early adaptation to reward signals.This is followed by a phase of rapid improvement, where the RL agent learns effective circuit design strategies.Intermediate fluctuations arise as the model explores diverse topologies while balancing validity, constraint satisfaction, and efficiency.Eventually, the curves stabilize and plateau, indicating convergence to a policy that consistently generates valid, high-quality circuit topologies.These results show AUTOCIRCUIT-RL steadily improves design capabilities and sustains robust performance as circuit complexity grows, with only minor efficiency degradation observed from 4-to 5-component circuits.</p>
<p>Results Overview</p>
<p>The evaluation results, summarized in Table 1, show that AUTOCIRCUIT-RL outperforms all baselines across various metrics for circuit topology synthesis.Notably, smaller language models tuned with our method outperform larger prompt tuning-based models, generating unique designs faster with a higher success rate in meeting design constraints.Our LLM-based approach is efficient, requiring ∼ 2.7 seconds per design generation once trained (refer Section D for runtime analysis).In contrast, traditional search-based algorithms (Fan et al., 2021) are computationally expensive and time-consuming, often taking hundreds of seconds to converge on a target design.Usability of Zero-Shot and ICL Methods We observe a notable disparity in performance between zero-shot generation techniques and fine-tuned methodologies.Zero-shot generation struggles to produce valid netlist-like structures essential for subsequent classification or simulation tasks.Despite using in-context learning (ICL), where the model is exposed to sample prompts and corresponding circuit generations, the improvement in generating comprehensive netlist-like structures is minimal.Additionally, increasing the number of in-context examples j yielded diminishing returns.As a result, these incomplete structures cannot be used for accurate metric computation, so we exclude the zero-shot and ICL results in Table 1.</p>
<p>Error Analysis</p>
<p>We conduct a qualitative error analysis of the AUTOCIRCUIT-RL framework by evaluating the modelgenerated circuits through post-hoc SPICE simulations.The analysis focuses on two primary failure modes: validity errors, where circuits fail to simulate due to structural violations such as incorrect node assignments or connectivity issues, and efficiency constraint errors, where the generated circuits do not meet the efficiency thresholds specified in the generation prompts.This systematic evaluation enables the identification of recurring failure patterns and provides insight into the model's behavior near constraint boundaries, highlighting specific areas where the model lacks fine-grained control.</p>
<p>Our analysis reveals that validity failures predominantly arise from minor inconsistencies in node assignments rather than fundamental topological errors, and such failures can typically be resolved with minimal structural modifications.</p>
<p>Regarding efficiency constraint errors, most of the circuits that fail to meet the specified thresholds do so by a relatively small margin, suggesting that the model generally approximates the desired performance metrics.These deviations often reflect inherent trade-offs with other design parameters such as output voltage and duty cycle.A limited number of outlier cases exhibit larger deviations from the efficiency targets, typically caused by complex interactions between circuit components.Collectively, these findings indicate that while the model internalizes key principles of both structural integrity and performance-aware design, there remains scope for further improvements in constraint calibration and optimization.Detailed examples and further discussion can be found in Appendix C.</p>
<p>Effectiveness of AUTOCIRCUIT-RL</p>
<p>Impact of Prompt Tuning Experiments with prompt tuning of Flan-UL2/Llama-2 models (approximately 20b parameters) with a restricted dataset, facilitated the production of netlist-like structures.This represents a substantial enhancement compared to the performance of the same models under zero-shot or in-context learning (ICL) conditions.Nevertheless, these fine-tuned models lag behind smaller language models (such as GPT-Neo/StableLM) fine-tuned using our method in terms of both efficiency and validity of the generated circuits.Additionally, our models demonstrate a higher success rate in meeting constraints compared to the larger prompt-tuned language models.Despite the advantages of fine-tuning these larger models, we emphasize that fine-tuned models with lower capacity can still achieve effective performance relative to larger prompt tuning-based models for circuit topology synthesis.</p>
<p>Comparison with Vanilla Fine-tuning When contrasting our methodology, which entails iterative refinement via reinforcement learning, with the basic fine-tuning of different architectures, we note a significant enhancement in the effectiveness of our approach.Our methodology, which entails iterative refinement via reinforcement learning, demonstrates a significant improvement in effectiveness compared to basic fine-tuning of GPT-Neo and StableLM architectures.This improvement is evident in generating valid circuits that meet design constraints, underscoring the efficacy of our approach in synthesizing valid topologies and accelerating the discovery of unique designs.</p>
<p>Effect of Using Gumbel-Max Trick for Multi-Objective</p>
<p>Optimization Building on a prior study (Vijayaraghavan et al., 2024), we optimize circuit validity and efficiency  using the Gumbel-Max trick, yielding notable gains over standard fine-tuned models.However, its performance still lags behind AUTOCIRCUIT-RL by ∼ 9% in circuit validity and efficiency.A key limitation of the Gumbel-Max trick is its inability to adaptively optimize multiple objectives.Unlike reinforcement learning (RL), which refines strategies through iterative AI feedback, Gumbel-based methods don't adjust based on prior evaluations and lack a mechanism to balance competing objectives, often leading to suboptimal designs.In contrast, our RL approach navigates these trade-offs effectively, explores a broader design space, and avoids premature convergence, demonstrating superior performance in circuit validity and efficiency.</p>
<p>Effect of Iterative Adaptation</p>
<p>To assess the significance of the iterative adaptation strategy for our task, we conducted an experiment where we sampled circuit topology generations from a model trained using reward models but without incorporating the iterative adaptation strategy.Our results reveal that the iterative refinement enabled by iterative adaptation substantially improves the overall model performance across various metrics.Notably, the lack of an iterative adaptation strategy leads to a more pronounced decline in performance, particularly notable in the circuit efficiency score, where we observed a ∼ 8% decrease.</p>
<p>Adherence to Design Constraints</p>
<p>This section examines how different models meet three key design constraints for 4-and 5-component circuits.We evaluate success rate (σ) for component usage (C), efficiency (C+E), and output voltage (C+V), using a 20-40-40 prompt split per constraint.AUTOCIRCUIT-RL excels across constraints, demonstrating RL's effectiveness in circuit topology synthesis.</p>
<p>Component Pool Adherence (C) Ensuring adherence to the component pool constraint is pivotal for practical applicability, achievable with minimal tuning.Generally, models fine-tuned or optimized with reward models exhibit superior adherence to the specified component pool.For instance, prompt tuning-based models, even with minimal tuning, achieve moderate success rates, with σ values ranging from ∼ 81% to ∼ 84%.Remarkably, models trained using our complete AUTOCIRCUIT-RL approach showcase the highest success rates, with σ values surpassing ∼ 98%.Efficiency (E) &amp; Output Voltage (V) Adherence Meeting efficiency and expected output voltage constraints is challenging in comparison to the component pool constraint, particularly when expected values are specified in the prompts.However, models optimized with reward models show notable improvements in meeting constraints.Particularly, models trained using AUTOCIRCUIT-RL exhibit superior performance in satisfying efficiency and expected output voltage requirements compared to their counterparts.</p>
<p>Generalization to Complex Scenarios</p>
<p>To assess the generalization capability of our framework, we evaluated it on circuits with 6-10 components.Following the procedure in Section 2, we constructed a limited dataset and applied few-shot fine-tuning with k = {250, 500, 1000} examples.The base model for fine-tuning was the AC-RL model previously trained on 4-and 5-component circuits (see Table 1).We adopted a sampling strategy similar to that used for 4/5-component circuits, as shown in Figure 2(b).We compared our top-performing AUTOCIRCUIT-RL (Llama-3) approach against GRAPHVAE and prompt-tuned Flan-ul-20b, focusing on circuit validity and efficiency, measured by the NGSpice simulator.As shown in Figure 4, AUTOCIRCUIT-RL showed consistent improvements with increasing k, achieving validity and efficiency scores nearing 50% with fewer than 500 examples.Table 2 summarizes the success rates for all methods with k = 1000, highlighting the superior performance of AUTOCIRCUIT-RL.The higher success rate underscores the effectiveness of reward models in optimizing efficiency and validity.These findings show AUTOCIRCUIT-RL's potential to scale to higher component circuits using minimal fine-tuning, though addressing more complex design constraints for different circuit types remains a future research area.</p>
<p>Analysis of Success Rates &amp; Sampling Strategies</p>
<p>Comparing success rates in Tables 1 and 2, we observe a performance decline in AC-RL as circuit complexity increases.However, this drop is due to differences in training and evaluation setups, as the model was extensively trained on 4C and 5C circuits, whereas 6C and beyond used a few-shot setup with only 1,000 samples.Despite this, the model achieves &gt;60% success with minimal training data, demonstrating good generalization.As shown in Figure 4, increasing training data improves performance, reinforcing the model's ability to learn from limited data.We also analyzed Suc-cessRate@m, where multiple generations per prompt (m) increase the likelihood of success.As seen in Figure 5, Suc-cessRate@3 and SuccessRate@5 consistently outperform SuccessRate@1, showing that additional sampling improves results.The average generation times for m = 3 and m = 5 are ∼ 7 and ∼ 9 seconds, respectively, achieving higher success rates with minimal added computation and remaining significantly faster than traditional AI-based search methods, which require hundreds of seconds.</p>
<p>Discussion</p>
<p>Our results demonstrate the effectiveness of proposed AC-RL in advancing circuit topology synthesis by iteratively refining circuit generation using reinforcement learning.Its key advantages are as follows:</p>
<p>RL for Improved Validity and Efficiency Unlike traditional methods, AC-RL adapts circuit generation using reward feedback.This iterative process yields higher validity and efficiency, outperforming zero-shot and ICL baselines.Its ability to optimize multiple objectives while adhering to constraints highlights its robustness.Accelerated Discovery of Novel Circuits AC-RL reduces duplicate generation rates (DGR) by ∼ 38% compared to fine-tuning methods, enabling faster convergence to unique and effective circuit topologies.This accelerates design space exploration while minimizing redundant computations, making it highly effective for circuit automation.</p>
<p>Enhanced Constraint Adherence With up to 80% success rates in constraint adherence, AC-RL efficiently balances efficiency and output voltage while ensuring feasibility.Unlike heuristic-based methods that require extensive computational resources, its reward-driven approach adapts to complex constraints with significantly better performance.Scalability with Limited Training Data AC-RL achieves over &gt;60% success rates generating valid circuits with only ∼ 1, 000 training examples, demonstrating strong gener-alization.This ability to perform well with limited data distinguishes it from conventional methods relying on extensive labeled datasets.Generalization to Increasing Circuit Complexity AC-RL scales effectively to circuits with 6-10 components, outperforming baselines like GRAPHVAE and prompt-tuned FLAN-UL-20B.While complexity increases, reducing success rates, more training data significantly enhances performance.Additionally, SuccessRate@m analysis shows that generating multiple circuits per prompt improves convergence with minimal added computation.</p>
<p>Extending AC-RL to diverse circuit architectures and integrating RL with advanced sampling could further optimize high-dimensional design spaces.Addressing constraints like power consumption and component parameter estimation via adaptive reward modeling remains a promising avenue.Overall, AUTOCIRCUIT-RL offers a transformative approach to circuit topology synthesis, excelling in efficiency, constraint adherence, and generalization with minimal data.Further enhancements could solidify its role as a cornerstone in AI-driven electronic design automation.</p>
<p>Conclusion</p>
<p>In</p>
<p>Impact Statement</p>
<p>The proposed AUTOCIRCUIT-RL framework significantly advances the field of analog circuit topology synthesis by integrating large language models (LLMs) with reinforcement learning (RL) for constrained topology generation.Our approach addresses key challenges in circuit synthesis, such as scalability, efficiency, and adaptability to diverse design specifications, which are critical in modern electronic design automation (EDA).By overcoming the limitations of traditional search-based and generative methods, our framework enables efficient circuit generation while reducing the number of simulation queries.This not only enhances the practicality of AI-driven circuit design but also provides a scalable solution capable of generalizing to circuits with 6 to 10 components using few-shot fine-tuning.Our results demonstrate superior performance in generating circuits that meet design constraints more effectively than existing approaches, paving the way for future research in topologyaware AI design methods.</p>
<p>Beyond technical improvements, our work has broader societal and ethical implications.By automating complex aspects of circuit design, AUTOCIRCUIT-RL reduces reliance on extensive human expertise, potentially lowering the barrier to entry for new designers and expanding accessibility to hardware design in low-resource environments.This democratization of circuit synthesis could foster innovation and accelerate technological advancements across industries.However, it is crucial to ensure that AI-driven design tools remain transparent and do not introduce unintended biases in circuit topology selection, which could lead to over-reliance on specific architectures.Additionally, given the increasing energy demands of AI-driven automation, future work should consider optimizing the computational efficiency of training and inference phases to minimize environmental impact.By addressing these aspects, our work contributes to responsible AI adoption in EDA, ensuring that automation enhances creativity and inclusivity in circuit design rather than reinforcing existing barriers.</p>
<p>B. Appendix: Implementation Details</p>
<p>We provide the implementation details of our experiments conducted with the official PyTorch v2.2.0 release binary package, compiled with CUDA 11.8, utilizing NVIDIA V100 GPUs with 32 GB of memory.</p>
<p>• Two-Phase Training: This setup applies to both instruction tuning and RL-refinement phases.We plot the training data on a Vout-Efficiency graph (an example for 5-component circuits is shown in Figure 6) and categorize the data into four main groups: (a) Group 1 with low efficiency (efficiency lower than 0.05), (b) Group 2 with moderate efficiency (efficiency between 0.05 and 0.7), (c) Group 3 high efficiency but the output voltage shows minimal difference from input voltage, and (d) Group 4 with optimal Vout and efficiency.To optimize the training process, we apply a weighted sampling strategy in each batch, giving the highest priority to Group 4, which represents the optimal conditions, and the lowest priority to Group 1, which has the least efficiency.The sampling weights are allocated as follows: 0.1 for Group 1, 0.25 for Group 2, 0.25 for Group 3, and 0.4 for Group 4. This approach ensures that batches are enriched with data from Group 4, allowing the model to better learn from the most critical and desirable conditions.</p>
<p>Training is conducted over 4-6 epochs using shuffled data from the training split, with model checkpoints saved based on the best performance on the validation split.To manage memory efficiently, we employ gradient checkpointing.</p>
<p>We use the AdamW optimizer (Loshchilov &amp; Hutter, 2017), setting beta parameters to 0.9 and 0.95, and an epsilon value of 1.0e-8.The learning rate is set to 0.95e-5, and the seed is fixed at 42 to ensure reproducibility.During training, we assess performance by evaluating a subset of 100 sample generations, using consistent evaluation settings.If the performance in the current epoch exceeds that of the previous one, we save the checkpoint.• Iterative Adaptation: The iterative adaptation, explained in Section 3.2.3, is performed over 3-5 iterations depending on the base model used.Each iteration involves 2-4 epochs of RL refinement and uses a dataset of 10,000 high-quality circuit samples obtained via nucleus sampling.This process improves performance progressively with each stage.The final adapted model is directly used for inference without further tuning.</p>
<p>• Inference: We assess all models by generating 1,000 unique sample circuit topologies from each, using a combination of nucleus sampling and top-k sampling techniques.These generated topologies are then analyzed using validity, efficiency, and output voltage estimators to identify the designs that are both valid and efficient, meeting the specified design criteria.</p>
<p>C. Error Analysis</p>
<p>C.1. Validity Errors</p>
<p>We begin our error analysis by examining one common failure mode encountered in generated circuit topologies: validity errors, as identified by SPICE simulation.A netlist is considered invalid if it violates essential electrical constraints such as correct node referencing, grounding, or connectivity, which causes simulation failures.</p>
<p>Despite the strong overall performance of our model, a small subset of generated netlists exhibit such validity issues.These typically stem from improper or inconsistent node assignments rather than errors in component selection or overall topology.</p>
<p>A representative example of an invalid netlist by the model is:
[['FET-B-1', 'IN', '6'], ['FET-A-0', '0', 'IN'], ['FET-B-0', 'OUT', '7'], ['inductor-0', '6', '7']]
This netlist fails validation primarily due to ambiguous or incorrect node labeling, which disrupts the circuit's connectivity and prevents successful simulation.However, with minimal adjustments, specifically refining the node assignments to properly reference outputs and grounds, the netlist can be made valid:
[['FET-B-1', 'IN', '6'], ['FET-A-0', '0', 'IN'], ['FET-B-0', 'OUT', '0'], ['inductor-0', '6', 'OUT']]
This minor correction preserves the original component arrangement and topology while resolving the node reference ambiguities, enabling successful simulation.This pattern indicates that the model effectively learns appropriate component placements and connectivity patterns but occasionally struggles with precise node labeling.These validity errors are therefore close to valid designs and could be mitigated with improved node management during generation.</p>
<p>C.2. Efficiency Constraint Errors</p>
<p>In addition to validity, our generated circuits are evaluated against performance constraints such as minimum required efficiency, as specified in the generation prompt.These constraints are verified post-hoc using SPICE simulation.While the majority of the model outputs achieve or closely approximate the requested efficiency thresholds, a few circuits fall short.Consider the following example, where the prompt required an efficiency greater than 0.7: This topology, under a generated duty cycle of 0.1, achieved a simulated efficiency of 0.625, missing the constraint by a relatively small margin.The component usage and connectivity suggest that the model has captured many of the structural features that contribute to efficiency, although precise performance can depend on subtle circuit-level interactions.Such cases demonstrate that the model generates solutions near the constraint boundary and could be improved further with targeted refinement.</p>
<p>However, there also exist a few outlier cases where the efficiency gap is more pronounced, such as outputs scoring well below the target (for example, less than 0.3 when the required efficiency was greater than 0.7).These larger discrepancies are typically associated with difficult trade-offs between duty cycle, output voltage, and interactions among other components.In these cases, the model may prioritize satisfying voltage or topological structure over efficiency, reflecting the inherent challenge of simultaneously satisfying multiple constraints.Nevertheless, the proximity of many failing cases to the desired efficiency threshold, along with the model's ability to produce performance-aware topologies, supports the conclusion that these errors arise from nuanced constraint balancing rather than fundamental model shortcomings.</p>
<p>D. Runtime Complexity Analysis</p>
<p>A key strength of AUTOCIRCUIT-RL lies in its computational efficiency, particularly when compared with traditional search-based topology synthesis methods.These traditional methods, such as genetic algorithms and tree-based search (Fan et al., 2021;Zhao &amp; Zhang, 2020), often rely on hundreds of SPICE simulations per design iteration.As a result, they typically require several minutes (hundreds of seconds) to produce a single valid circuit design, especially when adapting to new specifications or exploring diverse topologies.</p>
<p>In contrast, our method significantly reduces synthesis time by leveraging a two-phase RL approach.The LLM quickly generates initial candidate topologies, while the RL-based refinement performs iterative optimization using a learned reward model that captures circuit validity, efficiency, and output voltage.On average, AUTOCIRCUIT-RL generates a complete and optimized circuit in approximately 2-3.5 seconds using two NVIDIA V100 GPUs, offering over 50x improvement in design time over traditional approaches.</p>
<p>To assess how the choice of language model affects runtime, we evaluated AUTOCIRCUIT-RL with two LLMs: MPT-7B and LLaMA-3 8B.As expected, model size influences generation latency.The MPT-7B variant achieves faster runtimes (2.4-3.5 seconds for 4-10 component circuits), whereas the LLaMA-3 8B variant incurs slightly higher runtimes (2.8-5 seconds), reflecting the increased computational overhead of the larger model.Despite this, both configurations maintain runtimes well below traditional baselines, making them viable for real-time or interactive design applications.These results demonstrate that AUTOCIRCUIT-RL achieves an effective balance between computational cost and output quality, offering a scalable and practical alternative to traditional methods in analog circuit synthesis.</p>
<p>Figure 2 .
2
Figure 2. ((a) Statistics of our Circuit dataset.b) Vout-Efficiency graph of 4-component circuits in the training data.(c) Illustration of the AUTOCIRCUIT-RL framework.Dashed lines depict probability flow, while the dotted line represents iterative adaptation post RLAIF tuning.A prompt guides topology generation, then rewards are calculated with KL-divergence to a reference model to preserve the original distribution.PPO updates model parameters using rewards, and iterative adaptation further improves the model.</p>
<p>Figure 3 .
3
Figure 3. RL convergence curves for AUTOCIRCUIT-RL with Llama-3 over ∼25,000 training steps.Top: Efficiency for 4and 5-component circuits; Bottom: Success ratio of valid and constraint-satisfying topologies.</p>
<p>Figure 4 .
4
Figure 4. Few-shot fine-tuning results for 6-10 components, with models tuned using k-examples of varying circuit topologies.</p>
<p>Figure 5 .
5
Figure 5. Plot of SuccessRate@m of AC-RL for m ∈ {1, 3, 5}.</p>
<p>Figure 6 .
6
Figure 6.Vout-Efficiency graph of 5-component circuits in the training data.</p>
<p>Figure 7 .
7
Figure7.Runtime Complexity of AUTOCIRCUIT-RL Using MPT-7B vs. LLaMA-3 8B.The runtime per circuit increases with the number of components but remains significantly faster than traditional search-based methods.MPT-7B offers lower latency due to its smaller model size, while LLaMA-3 8B provides marginally higher runtimes due to increased model capacity.</p>
<p>RLAIF Model Reference Model Reward Circuit Validity Circuit Efficiency Output Voltage KL Regularization RL Algorithm: PPO Trainer Prompt Probabilities Reward Circuit Topology Generation Iterative adaptation Circuit Topology Weight Updates
It consists of 3 main steps:3.2.1. REWARD MODELINGIn this step, our reward model evaluates the appropriate-ness of a generated circuit topology based on the instructionprompt. Canonical reinforcement learning with AI feed-</p>
<p>Table 1 .
1
Evaluation results for all methods on 4-component (4C) and 5-component (5C) circuits.Circuit validity and efficiency are measured using both classifier (column 2 and 4) and simulator (column 3 and 5).The DGR ρ and success rate σ for different categories of constraints: component (C), efficiency (C+E), output voltage (C+V) and overall (O) are also reported.Models E(f valid (ŷ)) E(fS valid (ŷ)) E(f eff (ŷ)) E(fS eff (ŷ)) DGR ρ
Success Rate σ (%)
denoted as E(f clf (ŷ)).Here, clf ∈ {valid, S valid } refers to the validity estimated by the classifier or the simulator, respectively.We consider a circuit valid if its validity score from the classifier exceeds 0.6; (b) Circuit Efficiency Score refers to average efficiency of the generated circuits using our efficiency regressor or the NGSpice simulator, denoted as E(f eff (ŷ)) and E(f S eff (ŷ)), respectively.</p>
<p>Table 2 .
2
Overall success rate of GRAPHVAE, Prompt Tuning and AUTOCIRCUIT-RL for 6-10 component circuits with k = 1000.
Models6C7C8C9C10CGRAPHVAE21.5 20.6 16.1 15.9 12.8PROMPT TUNING 39.7 36.3 35.9 33.2 32.1AC-RL65.5 63.8 63.2 60.4 58.5
A. Appendix: Baseline Model DetailsWe compare AUTOCIRCUIT-RL against various baseline models, including both LLM-based and non-LLM approaches, to assess its efficiency and performance in circuit topology synthesis.• GPT-Neo-2.7(Black et al., 2021)1 : A 2.7B parameter transformer-based model developed by EleutherAI, following GPT-3's architecture.It was trained on 420B tokens over 400,000 steps using masked autoregressive modeling with cross-entropy loss.• StableLM-3B-4E1T 2 : A 3B parameter decoder-only model pre-trained on 1T tokens over 4 epochs from diverse English and code datasets.Referred to as StableLM in our work.• Llama-3-8B 3 : An 8B parameter model from the Meta Llama 3 family, incorporating supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to enhance alignment with human preferences.• MPT-7B 4 : A 7B parameter decoder-style transformer trained on 1T tokens of English text and code.It employs a modified transformer architecture optimized by MosaicML for efficient training and inference.• CIRCUITSYNTH-GUMBEL: A variant of(Vijayaraghavan et al., 2024)incorporating:1. Training a circuit validity and efficiency classifier to estimate the probability of a generated circuit being valid.2. Fine-tuning an LLM to generate circuit topologies.3. Refining outputs using the classifier while enforcing circuit validity and efficiency constraints.The training objective combines standard negative log-likelihood loss (LL LM ) with circuit validity and efficiency loss.Since LLMs operate in discrete spaces, we employ Gumbel-softmax(Jang et al., 2016)for continuous relaxation, enabling gradient-based optimization.• GraphVAE (Simonovsky &amp; Komodakis, 2018): A non-LLM baseline for circuit generation.The encoder consists of two graph convolutional layers (32 and 64 channels) with identity connections, batch normalization, and ReLU activation, followed by a fully connected (FC) layer producing a 256-dimensional latent representation.The decoder has three FC layers (256, 512, 1024 channels) with batch normalization and ReLU, followed by a parallel triplet of FC layers to output graph tensors.Training was performed for 50-75 epochs using Adam (learning rate 1e −3 , β 1 = 0.5).A sentence transformer(Reimers, 2019)processes natural language prompts, followed by an FC layer to align its representation with the encoder output size.Prompt Type Prompt Sample CircuitComponent Constraint
Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Estimating or propagating gradients through stochastic neurons for conditional computation. Y Bengio, N Léonard, A Courville, arXiv:1308.34322013arXiv preprint</p>
<p>Auto-spice: Leveraging llms for dataset creation via automated spice netlist extraction from analog circuit diagrams. J Bhandari, V Bhat, Y He, S Garg, H Rahmani, R Karri, arXiv:2411.142992024arXiv preprint</p>
<p>S Black, G Leo, P Wang, C Leahy, S Biderman, Gpt-Neo, 10.5281/zenodo.5297715Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. March 2021</p>
<p>Language-modelbased topology generation for analog integrated circuits. C.-C Chang, Y Shen, S Fan, J Li, S Zhang, N Cao, Y Chen, X Zhang, Lamagic, the 41st International Conference on Machine Learning (ICML). 2024</p>
<p>Artisan: Automated operational amplifier design via domain-specific large language model. Z Chen, J Huang, Y Liu, F Yang, L Shang, D Zhou, X Zeng, Proceedings of the 61st ACM/IEEE Design Automation Conference. the 61st ACM/IEEE Design Automation Conference2024</p>
<p>Cktgnn: Circuit graph neural network for electronic design automation. Z Dong, W Cao, M Zhang, D Tao, Y Chen, X Zhang, International Conference on Learning Representations (ICLR). 2023</p>
<p>From specification to topology: Automatic power converter design via reinforcement learning. S Fan, N Cao, S Zhang, J Li, X Guo, X Zhang, IEEE/ACM International Conference On Computer Aided Design (ICCAD). 2021</p>
<p>Talk like a graph: Encoding graphs for large language models. B Fatemi, J Halcrow, B Perozzi, The Twelfth International Conference on Learning Representations. 2024</p>
<p>The llama 3 herd of models. A Grattafiori, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Vaughan, arXiv:2407.217832024arXiv preprint</p>
<p>Bagnet: Berkeley analog generator with layout optimizer boosted with deep neural networks. K Hakhamaneshi, N Werblun, P Abbeel, V Stojanović, IEEE/ACM International Conference on Computer-Aided Design. IC-CAD2019</p>
<p>E Jang, S Gu, B Poole, arXiv:1611.01144Categorical reparameterization with gumbel-softmax. 2016arXiv preprint</p>
<p>Analog circuit design via training-free code generation. Y Lai, S Lee, G Chen, S Poddar, M Hu, D Z Pan, P Luo, Analogcoder, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>H Lee, S Phatale, H Mansoor, K Lu, T Mesnard, C Bishop, V Carbune, A Rastogi, Rlaif, arXiv:2309.00267Scaling reinforcement learning from human feedback with ai feedback. 2023arXiv preprint</p>
<p>The power of scale for parameter-efficient prompt tuning. B Lester, R Al-Rfou, N Constant, arXiv:2104.086912021arXiv preprint</p>
<p>I Loshchilov, F Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Trustworthy Genetic Programming-Based Synthesis of Analog Circuit Topologies Using Hierarchical Domain-Specific Building Blocks. T Mcconaghy, P Palmers, M Steyaert, G Gielen, IEEE Transactions on Evolutionary Computation. 1941- 00261542011</p>
<p>Ngspice users manual version 23. Experiments/ ngspice23-manual. pdf. P Nenzi, H Vogt, 2011</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 202235</p>
<p>N Reimers, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019arXiv preprint</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Atelier: An automated analog circuit design framework via multiple large language model-based agents. J Shen, Z Chen, J Zhuang, J Huang, F Yang, L Shang, Z Bi, C Yan, D Zhou, X Zeng, M Simonovsky, N Komodakis, Artificial Neural Networks and Machine Learning-ICANN 2018: 27th International Conference on Artificial Neural Networks. Rhodes, GreeceSpringer2024. October 4-7, 2018. 2018Proceedings, Part I 27</p>
<p>Introducing mpt-7b: A new standard for open-source, commercially usable llms. M N Team, 2023</p>
<p>Circuitsynth: Leveraging large language models for circuit topology synthesis. P Vijayaraghavan, L Shi, E Degan, X Zhang, 2024 IEEE LLM Aided Design Workshop (LAD). IEEE2024</p>
<p>Gcn-rl circuit designer: Transferable transistor sizing with graph neural networks and reinforcement learning. H Wang, K Wang, J Yang, L Shen, N Sun, H Lee, S Han, ACM/IEEE Design Automation Conference (DAC). 2020</p>
<p>H Zhang, S Sun, Y Lin, R Wang, J Bian, arXiv:2412.19824Analogxpert: Automating analog topology synthesis by incorporating circuit design expertise into large language models. 2024arXiv preprint</p>
<p>Dvae: A variational autoencoder for directed acyclic graphs. M Zhang, S Jiang, Z Cui, R Garnett, Y Chen, Advances in neural information processing systems. 201932</p>
<p>An Automated Topology Synthesis Framework for Analog Integrated Circuits. Z Zhao, L Zhang, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. 1937-415139122020</p>
<p>Generate a 4-component circuit with n-type MOSFETs. FET-A-0, FET-A-1 and FET-A-2</p>
<p>FET-A-2type MOSFET: FET-B-0; representing different nodes. ' Fet-A-2, ' Fet-B-0, ' , ' Fet-A-1, ' , ' Fet-A-, FET-B-0', '5', 'OUT'], ['FET-A-1', '0', 'IN'], ['FET-A-0', '5', 'OUT'</p>
<p>Output Voltage Constraint Generate a 4-component circuit with capacitors: capacitor-0 and capacitor-1; n-type MOSFET: FET-A-0; and p-type MOSFET: FET-B-0 representing different nodes: ['capacitor-1','FET-A-0','capacitor-0', 'FET-B-0'] with Vout less than 1.5V when Vin equals 2V. capacitor-1IN', '10'], ['FET-A-0', 'OUT', 'IN'], ['capacitor-0', 'IN', '0'], ['FET-B-0', 'OUT', '10'</p>
<p>Efficiency Constraint Generate a 4-component circuit with inductor: inductor-0; n-type MOSFET: FET-A-0. </p>
<p>FET-A-0','FET-B-0', 'capacitor-0'] with efficiency greater than 0. inductor-0MOSFET: FET-B-0; and capacitor: capacitor-0; representing different nodes: ['inductor-0. FET-A-0capacitor-0', 'OUT', '0'</p>
<p>Sample prompts of each prompt type and their corresponding sample circuit topologies. </p>            </div>
        </div>

    </div>
</body>
</html>
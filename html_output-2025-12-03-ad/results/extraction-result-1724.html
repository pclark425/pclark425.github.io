<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1724 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1724</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1724</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-274776885</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.10402v1.pdf" target="_blank">TANGO: Training-free Embodied AI Agents for Open-world Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated excellent capabilities in composing various modules together to create programs that can perform complex reasoning tasks on images. In this paper, we propose TANGO, an approach that extends the program composition via LLMs already observed for images, aiming to integrate those capabilities into embodied agents capable of observing and acting in the world. Specifically, by employing a simple PointGoal Navigation model combined with a memory-based exploration policy as a foundational primitive for guiding an agent through the world, we show how a single model can address diverse tasks without additional training. We task an LLM with composing the provided primitives to solve a specific task, using only a few in-context examples in the prompt. We evaluate our approach on three key Embodied AI tasks: Open-Set ObjectGoal Navigation, Multi-Modal Lifelong Navigation, and Open Embodied Question Answering, achieving state-of-the-art results without any specific fine-tuning in challenging zero-shot scenarios.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1724.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1724.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (used as planner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A foundation large language model used as the high-level planner in TANGO: given a natural-language instruction plus 15 in-context examples it generates step-by-step pseudo-programs composed of symbolic primitives that are executed by the embodied agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Large autoregressive / instruction-following LLM used as a symbolic planner that emits pseudo-code composed of high-level primitives (e.g., explore_scene, detect, navigate_to, answer). The LLM is prompted with 15 in‑context examples and is not fine-tuned on the embodied tasks; its outputs are parsed by the TANGO Program Interpreter to invoke pre-trained perception and navigation modules.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>large-scale natural language text / foundation LLM corpora (internet-scale text)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper beyond the general statement that LLMs are trained on "vast internet-scale datasets" [paper cites foundational LLM work]. The TANGO paper does not provide dataset names or sizes for GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-Set ObjectGoal Navigation, Multi-Modal Lifelong Navigation (GOAT-Bench), Open Embodied Question Answering (OpenEQA)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Photorealistic 3D indoor navigation and question-answering tasks run in Habitat/HM3D environments: (1) Open-set ObjectGoal Navigation — find an object category instance within 500 steps; (2) GOAT Bench — sequentially find 5–10 targets specified as names, descriptions, or images (lifelong/memory requirements); (3) Active Embodied QA — autonomously explore and gather visual evidence to answer open-vocabulary questions within 500 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural-language tokens and instructions; the LLM emits high-level symbolic program instructions (primitives) rather than low-level motor commands.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation and viewpoint actions exposed to the agent: Forward (0.25 m steps), Move Left, Move Right, Look Up, Look Down, Turn (30° increments), and Stop; navigation is implemented via a PointGoal waypoint controller that consumes waypoints (relative distance & heading) and egocentric depth.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Symbolic mapping: the LLM emits pseudo-code lines invoking named primitives; the Program Interpreter parses those lines and dispatches to pre-implemented modules. Navigation primitives are mapped to waypoints and executed by a pre-trained PointGoal policy; perception primitives call pre-trained detectors/classifiers (Owlv2/DETR/BLIP2) and similarity modules (SuperGlue). Memory-guided exploration primitives update/use a feature-map based memory for lifelong goals.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images (640×480 or 360×640 depending on task), egocentric depth, (optionally) GPS+compass or derived pose; pretrained detectors/classifiers used include Owlv2 and DETR for object detection, BLIP2 for VQA/answering and language grounding, SuperGlue for instance-image matching. Value maps are computed from RGB/depth and vision-language embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Using the TANGO system with GPT-4o as planner (no task-specific fine-tuning): HM3D-OVON (val unseen) OBJNAV: Success Rate (SR) = 35.5% ± 0.3, SPL = 19.5% ± 0.3; OpenEQA (A-EQA) LLM-Match score S = 37.2 ± 1.8; GOAT-Bench: reported +2.6% gain in success over prior state-of-the-art methods (numbers shown in paper tables). These results are the end-to-end TANGO system numbers (LLM planner + pretrained perception/navigation modules).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not directly reported in this paper for ablations that remove LLM pretraining; the paper compares to baselines and prior methods but does not provide a pure "no-LLM" TANGO ablation performance. The paper notes that end-to-end RL PointNav training (DD-PPO) can require very large-scale training (e.g., referenced 2.5 billion frames) but does not provide a direct task-level numeric baseline for a no-text-pretraining variant.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Zero-shot / few-shot setup: the LLM planner is used without fine-tuning; TANGO supplies 15 in-context examples to GPT-4o. No additional task-specific training episodes are required for the planner. The PointGoal policy used as a low-level controller was pre-trained (on HM3D training set) prior to experiments; the paper does not provide counts of episodes used to pretrain the LLM (foundation LLM training is external).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported for a no-LLM training baseline in the paper. The paper references prior art (DD-PPO) which required ~2.5 billion frames to train a near-perfect PointGoal navigator as an illustration of typical heavy sample complexity for end-to-end RL approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitative and task-level: TANGO avoids task-specific RL/fine-tuning by leveraging a pretrained LLM planner and pretrained perception/navigation modules (i.e., effectively zero-shot for planner); this saves the heavy sample/training cost associated with end-to-end RL (e.g., days / billions of frames). No quantitative factor (e.g., X× fewer samples) is provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Strong language priors and compositional reasoning of the LLM that allow decomposition of tasks into primitives; modular architecture that uses off-the-shelf pretrained perception and navigation modules; explicit program interpreter that maps symbolic primitives to reliable low-level controllers (PointGoal); a memory-based feature map enabling lifelong recall of object locations; few-shot in-context examples guiding correct program generation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Perception failures (object detector false positives / missed detections) dominate many failures; LLM-generated pseudo-code errors (incorrect primitive ordering or ambiguous targets) produce failures ~11–18% in failure analysis; ambiguous or underspecified natural language prompts degrade planner output; incorrect memorized target locations can reduce path efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A foundation LLM (GPT-4o) can be used as a training-free high-level planner for 3D embodied tasks by emitting executable pseudo-programs that invoke pretrained perception and navigation primitives. With only 15 in-context examples and no task-specific fine-tuning, the LLM-driven system (TANGO) attains competitive / state-of-the-art results across OBJNAV, GOAT lifelong navigation, and OpenEQA benchmarks, while avoiding the massive sample complexity of end-to-end RL. Main limitations arise from perception module errors and occasional incorrect or ambiguous LLM pseudo-code; the modular program-execution design gives strong interpretability and enables targeted debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TANGO: Training-free Embodied AI Agents for Open-world Tasks', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1724.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1724.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLIP2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLIP2 (Bootstrapping Language-Image Pre-training 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained multimodal vision-language model used in TANGO as an answer module for EQA and to produce language-grounded value maps for exploration guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>BLIP2</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pretrained vision-language model employed for multi-modal tasks (VQA, captioning, image-text retrieval). In TANGO BLIP2 is used both as the 'answer' module for EQA and for generating language-grounded value maps that guide exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multimodal image-text pretraining (vision + language pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper; the authors refer to BLIP2 as a pretrained vision-language model used off-the-shelf. The paper does not list BLIP2's pretraining datasets or sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open Embodied Question Answering (OpenEQA), used in language-grounded exploration/value-map construction for navigation tasks (OVON, GOAT).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>EQA: agent must navigate a 3D environment to collect evidence and answer open-vocabulary natural language questions within a step budget; BLIP2 supplies answers and aids in grounding language queries to visual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Pretraining involved text (captions, VQA text), not a text-action environment; in TANGO BLIP2 is used to produce language-vision embeddings rather than actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Same embodied discrete navigation actions as the system (Forward, Move Left, Move Right, Look Up, Look Down, Turn, Stop); BLIP2 does not directly output low-level actions but supplies perception/semantic outputs used by modules that do.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>BLIP2 produces semantic outputs (captions, embeddings) and language-grounded value maps used by the exploration primitive; the Program Interpreter calls BLIP2 to evaluate/answer questions and to compute image-text similarity for value mapping, which then informs the exploration frontiers used by the navigation controller.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images and their embeddings; BLIP2 uses image encoders + language model components and is used alongside depth and detector outputs in TANGO.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>BLIP2-based 'answer' module contributes to TANGO's OpenEQA score of 37.2 ± 1.8 (system-level metric) and supports value-map computation used in navigation tasks; paper reports TANGO closely matches SoA on OpenEQA (gap < 1%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not reported. The paper does not provide an ablation where BLIP2 is removed/replaced to measure isolated effect.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Used off-the-shelf without task-specific fine-tuning; zero additional task-specific samples for BLIP2 in TANGO.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitative: using a pretrained VL model (BLIP2) enabled language-grounded perception and VQA capabilities without extra embodied-task training; the paper does not quantify a numeric sample-efficiency gain.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>BLIP2 supplies rich multimodal embeddings and VQA capability that directly map language questions to visual observations; compatibility with other modular primitives and real-time inference.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Perception noise in synthetic 3D scenes (false positives) requires downstream classification/verification; BLIP2's outputs depend on visual input quality and can be confounded by cluttered or ambiguous scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A pretrained vision-language model (BLIP2) can be reused without fine-tuning as both a VQA answerer and a language-grounding component to produce value maps for embodied exploration; this enables effective zero-shot EQA and language-guided navigation when combined with an LLM planner and robust low-level navigation primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TANGO: Training-free Embodied AI Agents for Open-world Tasks', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1724.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1724.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Socratic LLMs (OpenEQA baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Socratic LLMs using frame captions / scene-graph captions (baseline methods cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline zero-shot EQA methods (cited in paper) that use foundation LLMs to interpret sequences of frame or scene-graph captions and answer EQA queries without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Socratic LLMs (frame-captions / scene-graph captions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Family of approaches that feed temporal sequences of image-derived captions or scene-graph descriptions to an LLM to produce answers for embodied question answering in a zero-shot manner; cited as comparative baselines in OpenEQA.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Foundation LLM pretraining on natural language text; at inference time fed with image-derived textual captions (frame captions or scene-graph captions).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper; the baseline is reported as a zero-shot approach leveraging pre-existing LLMs (reference provided in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open Embodied Question Answering (OpenEQA)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Open-vocabulary EQA where the agent must explore and gather frames to answer natural language questions; Socratic LLMs operate by reasoning over caption streams rather than by planning navigation directly.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Textual reasoning over captions (no text-action environment exposure in the cited baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>In the baseline, the mapping to embodied actions is typically performed by separate perception/navigation components (not the LLM); the paper cites these methods as baselines and does not detail their low-level action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Cited methods use LLM reasoning over captioned frames rather than producing executable pseudo-code; they do not generally output explicit low-level navigation programs in the same way as TANGO's planner.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Frame captions or scene-graph captions derived from visual observations; requires underlying frame captioning / scene-graph systems.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported in paper Table 2 as baselines: Socratic LLMs w/ Frame Captions: Score = 38.1 ± 1.8; Socratic LLMs w/ Scene-Graph Captions: Score = 34.4 ± 1.8 (these are cited baseline metrics on OpenEQA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not applicable here (these are zero-shot LLM-based baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Zero-shot evaluation; no task-specific fine-tuning reported in the cited baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitative: baseline demonstrates that feeding captioned visual observations to LLMs can yield non-trivial EQA performance without embodied-task training; paper does not quantify sample efficiency beyond reporting zero-shot scores.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>LLM reasoning over symbolic textual summaries of frames leverages language priors to produce answers without requiring navigation-guided planning from the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Relying on captions can miss or mis-describe critical visual details; lacks direct LLM-guided navigation, which can reduce ability to seek specific evidence in the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited LLM-baseline approaches that reason over image captions can achieve competitive zero-shot EQA scores, but differ from TANGO in that they do not use the LLM to explicitly plan and steer navigation in the 3D environment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TANGO: Training-free Embodied AI Agents for Open-world Tasks', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1724.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1724.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLFM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VLFM (Vision-Language Frontier Maps)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method (cited) that builds language-grounded value maps using a pretrained vision-language model to guide frontier-based exploration; TANGO extends this exploration policy with a memory-feature-map.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>VLFM</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Method that constructs occupancy maps and a language-grounded 'value map' from RGB/depth and pretrained vision-language embeddings to guide exploration frontiers in open-vocabulary semantic navigation (cited as [52] in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Vision-language pretrained models / image-text supervision (as used by the VLFM method)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not given in the TANGO paper; VLFM is cited as a prior that uses BLIP2 or similar vision-language models to compute value maps.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-Set ObjectGoal Navigation (OVON) and related zero-shot semantic navigation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Frontier-based exploration in 3D indoor environments where a language-specified target guides behavior via a computed value map that ranks map pixels by language-vision similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not applicable to VLFM pretraining specifically; VLFM uses language queries as inputs to compute value maps.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Frontier-based navigation primitives mapped to low-level discrete movement actions in the simulator (same action set as TANGO's navigator).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Compute a value map from vision-language model embeddings and use it to score frontiers; select frontier with highest expected value and map that choice to waypoints executed by a PointGoal controller.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB, depth; pretrained vision-language model (e.g., BLIP2) for grounding language to visual observations; occupancy mapping from depth.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Cited in paper Table 1: VLFM reported SR = 35.2, SPL = 19.6 on HM3D-OVON (val unseen) as a strong comparative method.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not reported in this paper for VLFM.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>VLFM is a zero-shot / pretrained approach in the cited work; this paper does not report VLFM's training sample counts.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not quantified in paper; VLFM is cited as an example of a pretrained vision-language approach that yields strong zero-shot navigation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Language-grounded embeddings enable scoring of map locations for semantic relevance; frontier-based exploration provides coverage and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Value-map correctness depends on quality of vision-language embeddings and captioning; may be sensitive to detector/embedding noise.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TANGO extends VLFM-style exploration by adding a persistent per-pixel feature-memory map to enable efficient lifelong navigation and recall of previously seen targets, demonstrating that memory augmentation improves sequential goal performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TANGO: Training-free Embodied AI Agents for Open-world Tasks', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
                <li>Vision-language frontier maps for zero-shot semantic navigation <em>(Rating: 2)</em></li>
                <li>Visual programming: Compositional visual reasoning without training <em>(Rating: 1)</em></li>
                <li>ViperGPT: Visual Inference via Python Execution for Reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1724",
    "paper_id": "paper-274776885",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (used as planner)",
            "brief_description": "A foundation large language model used as the high-level planner in TANGO: given a natural-language instruction plus 15 in-context examples it generates step-by-step pseudo-programs composed of symbolic primitives that are executed by the embodied agent.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "GPT-4o",
            "model_agent_description": "Large autoregressive / instruction-following LLM used as a symbolic planner that emits pseudo-code composed of high-level primitives (e.g., explore_scene, detect, navigate_to, answer). The LLM is prompted with 15 in‑context examples and is not fine-tuned on the embodied tasks; its outputs are parsed by the TANGO Program Interpreter to invoke pre-trained perception and navigation modules.",
            "pretraining_data_type": "large-scale natural language text / foundation LLM corpora (internet-scale text)",
            "pretraining_data_details": "Not specified in this paper beyond the general statement that LLMs are trained on \"vast internet-scale datasets\" [paper cites foundational LLM work]. The TANGO paper does not provide dataset names or sizes for GPT-4o.",
            "embodied_task_name": "Open-Set ObjectGoal Navigation, Multi-Modal Lifelong Navigation (GOAT-Bench), Open Embodied Question Answering (OpenEQA)",
            "embodied_task_description": "Photorealistic 3D indoor navigation and question-answering tasks run in Habitat/HM3D environments: (1) Open-set ObjectGoal Navigation — find an object category instance within 500 steps; (2) GOAT Bench — sequentially find 5–10 targets specified as names, descriptions, or images (lifelong/memory requirements); (3) Active Embodied QA — autonomously explore and gather visual evidence to answer open-vocabulary questions within 500 steps.",
            "action_space_text": "Natural-language tokens and instructions; the LLM emits high-level symbolic program instructions (primitives) rather than low-level motor commands.",
            "action_space_embodied": "Discrete navigation and viewpoint actions exposed to the agent: Forward (0.25 m steps), Move Left, Move Right, Look Up, Look Down, Turn (30° increments), and Stop; navigation is implemented via a PointGoal waypoint controller that consumes waypoints (relative distance & heading) and egocentric depth.",
            "action_mapping_method": "Symbolic mapping: the LLM emits pseudo-code lines invoking named primitives; the Program Interpreter parses those lines and dispatches to pre-implemented modules. Navigation primitives are mapped to waypoints and executed by a pre-trained PointGoal policy; perception primitives call pre-trained detectors/classifiers (Owlv2/DETR/BLIP2) and similarity modules (SuperGlue). Memory-guided exploration primitives update/use a feature-map based memory for lifelong goals.",
            "perception_requirements": "RGB images (640×480 or 360×640 depending on task), egocentric depth, (optionally) GPS+compass or derived pose; pretrained detectors/classifiers used include Owlv2 and DETR for object detection, BLIP2 for VQA/answering and language grounding, SuperGlue for instance-image matching. Value maps are computed from RGB/depth and vision-language embeddings.",
            "transfer_successful": true,
            "performance_with_pretraining": "Using the TANGO system with GPT-4o as planner (no task-specific fine-tuning): HM3D-OVON (val unseen) OBJNAV: Success Rate (SR) = 35.5% ± 0.3, SPL = 19.5% ± 0.3; OpenEQA (A-EQA) LLM-Match score S = 37.2 ± 1.8; GOAT-Bench: reported +2.6% gain in success over prior state-of-the-art methods (numbers shown in paper tables). These results are the end-to-end TANGO system numbers (LLM planner + pretrained perception/navigation modules).",
            "performance_without_pretraining": "Not directly reported in this paper for ablations that remove LLM pretraining; the paper compares to baselines and prior methods but does not provide a pure \"no-LLM\" TANGO ablation performance. The paper notes that end-to-end RL PointNav training (DD-PPO) can require very large-scale training (e.g., referenced 2.5 billion frames) but does not provide a direct task-level numeric baseline for a no-text-pretraining variant.",
            "sample_complexity_with_pretraining": "Zero-shot / few-shot setup: the LLM planner is used without fine-tuning; TANGO supplies 15 in-context examples to GPT-4o. No additional task-specific training episodes are required for the planner. The PointGoal policy used as a low-level controller was pre-trained (on HM3D training set) prior to experiments; the paper does not provide counts of episodes used to pretrain the LLM (foundation LLM training is external).",
            "sample_complexity_without_pretraining": "Not reported for a no-LLM training baseline in the paper. The paper references prior art (DD-PPO) which required ~2.5 billion frames to train a near-perfect PointGoal navigator as an illustration of typical heavy sample complexity for end-to-end RL approaches.",
            "sample_complexity_gain": "Qualitative and task-level: TANGO avoids task-specific RL/fine-tuning by leveraging a pretrained LLM planner and pretrained perception/navigation modules (i.e., effectively zero-shot for planner); this saves the heavy sample/training cost associated with end-to-end RL (e.g., days / billions of frames). No quantitative factor (e.g., X× fewer samples) is provided in the paper.",
            "transfer_success_factors": "Strong language priors and compositional reasoning of the LLM that allow decomposition of tasks into primitives; modular architecture that uses off-the-shelf pretrained perception and navigation modules; explicit program interpreter that maps symbolic primitives to reliable low-level controllers (PointGoal); a memory-based feature map enabling lifelong recall of object locations; few-shot in-context examples guiding correct program generation.",
            "transfer_failure_factors": "Perception failures (object detector false positives / missed detections) dominate many failures; LLM-generated pseudo-code errors (incorrect primitive ordering or ambiguous targets) produce failures ~11–18% in failure analysis; ambiguous or underspecified natural language prompts degrade planner output; incorrect memorized target locations can reduce path efficiency.",
            "key_findings": "A foundation LLM (GPT-4o) can be used as a training-free high-level planner for 3D embodied tasks by emitting executable pseudo-programs that invoke pretrained perception and navigation primitives. With only 15 in-context examples and no task-specific fine-tuning, the LLM-driven system (TANGO) attains competitive / state-of-the-art results across OBJNAV, GOAT lifelong navigation, and OpenEQA benchmarks, while avoiding the massive sample complexity of end-to-end RL. Main limitations arise from perception module errors and occasional incorrect or ambiguous LLM pseudo-code; the modular program-execution design gives strong interpretability and enables targeted debugging.",
            "uuid": "e1724.0",
            "source_info": {
                "paper_title": "TANGO: Training-free Embodied AI Agents for Open-world Tasks",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "BLIP2",
            "name_full": "BLIP2 (Bootstrapping Language-Image Pre-training 2)",
            "brief_description": "A pretrained multimodal vision-language model used in TANGO as an answer module for EQA and to produce language-grounded value maps for exploration guidance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "BLIP2",
            "model_agent_description": "Pretrained vision-language model employed for multi-modal tasks (VQA, captioning, image-text retrieval). In TANGO BLIP2 is used both as the 'answer' module for EQA and for generating language-grounded value maps that guide exploration.",
            "pretraining_data_type": "Multimodal image-text pretraining (vision + language pairs)",
            "pretraining_data_details": "Not specified in this paper; the authors refer to BLIP2 as a pretrained vision-language model used off-the-shelf. The paper does not list BLIP2's pretraining datasets or sizes.",
            "embodied_task_name": "Open Embodied Question Answering (OpenEQA), used in language-grounded exploration/value-map construction for navigation tasks (OVON, GOAT).",
            "embodied_task_description": "EQA: agent must navigate a 3D environment to collect evidence and answer open-vocabulary natural language questions within a step budget; BLIP2 supplies answers and aids in grounding language queries to visual observations.",
            "action_space_text": "Pretraining involved text (captions, VQA text), not a text-action environment; in TANGO BLIP2 is used to produce language-vision embeddings rather than actions.",
            "action_space_embodied": "Same embodied discrete navigation actions as the system (Forward, Move Left, Move Right, Look Up, Look Down, Turn, Stop); BLIP2 does not directly output low-level actions but supplies perception/semantic outputs used by modules that do.",
            "action_mapping_method": "BLIP2 produces semantic outputs (captions, embeddings) and language-grounded value maps used by the exploration primitive; the Program Interpreter calls BLIP2 to evaluate/answer questions and to compute image-text similarity for value mapping, which then informs the exploration frontiers used by the navigation controller.",
            "perception_requirements": "RGB images and their embeddings; BLIP2 uses image encoders + language model components and is used alongside depth and detector outputs in TANGO.",
            "transfer_successful": true,
            "performance_with_pretraining": "BLIP2-based 'answer' module contributes to TANGO's OpenEQA score of 37.2 ± 1.8 (system-level metric) and supports value-map computation used in navigation tasks; paper reports TANGO closely matches SoA on OpenEQA (gap &lt; 1%).",
            "performance_without_pretraining": "Not reported. The paper does not provide an ablation where BLIP2 is removed/replaced to measure isolated effect.",
            "sample_complexity_with_pretraining": "Used off-the-shelf without task-specific fine-tuning; zero additional task-specific samples for BLIP2 in TANGO.",
            "sample_complexity_without_pretraining": "Not reported.",
            "sample_complexity_gain": "Qualitative: using a pretrained VL model (BLIP2) enabled language-grounded perception and VQA capabilities without extra embodied-task training; the paper does not quantify a numeric sample-efficiency gain.",
            "transfer_success_factors": "BLIP2 supplies rich multimodal embeddings and VQA capability that directly map language questions to visual observations; compatibility with other modular primitives and real-time inference.",
            "transfer_failure_factors": "Perception noise in synthetic 3D scenes (false positives) requires downstream classification/verification; BLIP2's outputs depend on visual input quality and can be confounded by cluttered or ambiguous scenes.",
            "key_findings": "A pretrained vision-language model (BLIP2) can be reused without fine-tuning as both a VQA answerer and a language-grounding component to produce value maps for embodied exploration; this enables effective zero-shot EQA and language-guided navigation when combined with an LLM planner and robust low-level navigation primitives.",
            "uuid": "e1724.1",
            "source_info": {
                "paper_title": "TANGO: Training-free Embodied AI Agents for Open-world Tasks",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Socratic LLMs (OpenEQA baselines)",
            "name_full": "Socratic LLMs using frame captions / scene-graph captions (baseline methods cited)",
            "brief_description": "Baseline zero-shot EQA methods (cited in paper) that use foundation LLMs to interpret sequences of frame or scene-graph captions and answer EQA queries without task-specific fine-tuning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "Socratic LLMs (frame-captions / scene-graph captions)",
            "model_agent_description": "Family of approaches that feed temporal sequences of image-derived captions or scene-graph descriptions to an LLM to produce answers for embodied question answering in a zero-shot manner; cited as comparative baselines in OpenEQA.",
            "pretraining_data_type": "Foundation LLM pretraining on natural language text; at inference time fed with image-derived textual captions (frame captions or scene-graph captions).",
            "pretraining_data_details": "Not specified in this paper; the baseline is reported as a zero-shot approach leveraging pre-existing LLMs (reference provided in paper).",
            "embodied_task_name": "Open Embodied Question Answering (OpenEQA)",
            "embodied_task_description": "Open-vocabulary EQA where the agent must explore and gather frames to answer natural language questions; Socratic LLMs operate by reasoning over caption streams rather than by planning navigation directly.",
            "action_space_text": "Textual reasoning over captions (no text-action environment exposure in the cited baseline).",
            "action_space_embodied": "In the baseline, the mapping to embodied actions is typically performed by separate perception/navigation components (not the LLM); the paper cites these methods as baselines and does not detail their low-level action spaces.",
            "action_mapping_method": "Cited methods use LLM reasoning over captioned frames rather than producing executable pseudo-code; they do not generally output explicit low-level navigation programs in the same way as TANGO's planner.",
            "perception_requirements": "Frame captions or scene-graph captions derived from visual observations; requires underlying frame captioning / scene-graph systems.",
            "transfer_successful": null,
            "performance_with_pretraining": "Reported in paper Table 2 as baselines: Socratic LLMs w/ Frame Captions: Score = 38.1 ± 1.8; Socratic LLMs w/ Scene-Graph Captions: Score = 34.4 ± 1.8 (these are cited baseline metrics on OpenEQA).",
            "performance_without_pretraining": "Not applicable here (these are zero-shot LLM-based baselines).",
            "sample_complexity_with_pretraining": "Zero-shot evaluation; no task-specific fine-tuning reported in the cited baseline.",
            "sample_complexity_without_pretraining": "Not applicable.",
            "sample_complexity_gain": "Qualitative: baseline demonstrates that feeding captioned visual observations to LLMs can yield non-trivial EQA performance without embodied-task training; paper does not quantify sample efficiency beyond reporting zero-shot scores.",
            "transfer_success_factors": "LLM reasoning over symbolic textual summaries of frames leverages language priors to produce answers without requiring navigation-guided planning from the LLM.",
            "transfer_failure_factors": "Relying on captions can miss or mis-describe critical visual details; lacks direct LLM-guided navigation, which can reduce ability to seek specific evidence in the environment.",
            "key_findings": "Cited LLM-baseline approaches that reason over image captions can achieve competitive zero-shot EQA scores, but differ from TANGO in that they do not use the LLM to explicitly plan and steer navigation in the 3D environment.",
            "uuid": "e1724.2",
            "source_info": {
                "paper_title": "TANGO: Training-free Embodied AI Agents for Open-world Tasks",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "VLFM",
            "name_full": "VLFM (Vision-Language Frontier Maps)",
            "brief_description": "A prior method (cited) that builds language-grounded value maps using a pretrained vision-language model to guide frontier-based exploration; TANGO extends this exploration policy with a memory-feature-map.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "VLFM",
            "model_agent_description": "Method that constructs occupancy maps and a language-grounded 'value map' from RGB/depth and pretrained vision-language embeddings to guide exploration frontiers in open-vocabulary semantic navigation (cited as [52] in the paper).",
            "pretraining_data_type": "Vision-language pretrained models / image-text supervision (as used by the VLFM method)",
            "pretraining_data_details": "Not given in the TANGO paper; VLFM is cited as a prior that uses BLIP2 or similar vision-language models to compute value maps.",
            "embodied_task_name": "Open-Set ObjectGoal Navigation (OVON) and related zero-shot semantic navigation tasks",
            "embodied_task_description": "Frontier-based exploration in 3D indoor environments where a language-specified target guides behavior via a computed value map that ranks map pixels by language-vision similarity.",
            "action_space_text": "Not applicable to VLFM pretraining specifically; VLFM uses language queries as inputs to compute value maps.",
            "action_space_embodied": "Frontier-based navigation primitives mapped to low-level discrete movement actions in the simulator (same action set as TANGO's navigator).",
            "action_mapping_method": "Compute a value map from vision-language model embeddings and use it to score frontiers; select frontier with highest expected value and map that choice to waypoints executed by a PointGoal controller.",
            "perception_requirements": "RGB, depth; pretrained vision-language model (e.g., BLIP2) for grounding language to visual observations; occupancy mapping from depth.",
            "transfer_successful": null,
            "performance_with_pretraining": "Cited in paper Table 1: VLFM reported SR = 35.2, SPL = 19.6 on HM3D-OVON (val unseen) as a strong comparative method.",
            "performance_without_pretraining": "Not reported in this paper for VLFM.",
            "sample_complexity_with_pretraining": "VLFM is a zero-shot / pretrained approach in the cited work; this paper does not report VLFM's training sample counts.",
            "sample_complexity_without_pretraining": "Not reported.",
            "sample_complexity_gain": "Not quantified in paper; VLFM is cited as an example of a pretrained vision-language approach that yields strong zero-shot navigation performance.",
            "transfer_success_factors": "Language-grounded embeddings enable scoring of map locations for semantic relevance; frontier-based exploration provides coverage and robustness.",
            "transfer_failure_factors": "Value-map correctness depends on quality of vision-language embeddings and captioning; may be sensitive to detector/embedding noise.",
            "key_findings": "TANGO extends VLFM-style exploration by adding a persistent per-pixel feature-memory map to enable efficient lifelong navigation and recall of previously seen targets, demonstrating that memory augmentation improves sequential goal performance.",
            "uuid": "e1724.3",
            "source_info": {
                "paper_title": "TANGO: Training-free Embodied AI Agents for Open-world Tasks",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2,
            "sanitized_title": "language_models_as_zeroshot_planners_extracting_actionable_knowledge_for_embodied_agents"
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 2,
            "sanitized_title": "inner_monologue_embodied_reasoning_through_planning_with_language_models"
        },
        {
            "paper_title": "Vision-language frontier maps for zero-shot semantic navigation",
            "rating": 2,
            "sanitized_title": "visionlanguage_frontier_maps_for_zeroshot_semantic_navigation"
        },
        {
            "paper_title": "Visual programming: Compositional visual reasoning without training",
            "rating": 1,
            "sanitized_title": "visual_programming_compositional_visual_reasoning_without_training"
        },
        {
            "paper_title": "ViperGPT: Visual Inference via Python Execution for Reasoning",
            "rating": 1,
            "sanitized_title": "vipergpt_visual_inference_via_python_execution_for_reasoning"
        }
    ],
    "cost": 0.018803,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TANGO: Training-free Embodied AI Agents for Open-world Tasks
5 Dec 2024</p>
<p>Filippo Ziliotto 
University of Padova</p>
<p>Fondazione Bruno Kessler</p>
<p>Tommaso Campari 
Fondazione Bruno Kessler</p>
<p>Luciano Serafini 
Fondazione Bruno Kessler</p>
<p>Lamberto Ballan 
University of Padova</p>
<p>TANGO: Training-free Embodied AI Agents for Open-world Tasks
5 Dec 202450AF660F7754A5AF11CD5E828C3701E2arXiv:2412.10402v1[cs.AI]
Large Language Models (LLMs) have demonstrated excellent capabilities in composing various modules together to create programs that can perform complex reasoning tasks on images.In this paper, we propose TANGO, an approach that extends the program composition via LLMs already observed for images, aiming to integrate those capabilities into embodied agents capable of observing and acting in the world.Specifically, by employing a simple PointGoal Navigation model combined with a memory-based exploration policy as a foundational primitive for guiding an agent through the world, we show how a single model can address diverse tasks without additional training.We task an LLM with composing the provided primitives to solve a specific task, using only a few in-context examples in the prompt.We evaluate our approach on three key Embodied AI tasks: Open-Set ObjectGoal Navigation, Multi-Modal Lifelong Navigation, and Open Embodied Question Answering, achieving state-of-the-art results without any specific fine-tuning in challenging zero-shot scenarios.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have gained significant attention in the field of AI due to their remarkable capability to generalize across unseen tasks [5,23,45,46].Systems like VisProg and ViperGPT [18,44] demonstrated strong performance on a broad range of vision-and-language tasks by just relying on a few numbers of in-context examples to generate complex compositional programs without requiring any specific training.Nevertheless, the evaluation of these concepts to Embodied AI, particularly for navigation tasks, remains largely unexplored.This approach may be crucial in building agents capable to navigate and operate efficiently in unfamiliar environments.</p>
<p>Previously, methods such as Neural Module Networks (NMN) [2,19] have demonstrated promising compositional properties for high-level tasks like visual question answering, via end-to-end training of networks and specialized, differentiable neural modules.However, they required the combination of semantic parsers and predefined templates to learn to solve the compositional tasks.Alternative approaches leverage LLMs through predefined task modules or APIs for action execution [21,22,31,49,55].While effective, these methods often lack a compositional structure to address multiple tasks or primarily focus on manipulation and robotic planning, with limited emphasis on navigation.</p>
<p>In this work, we present TANGO (see Figure 1) a novel neuro-symbolic compositional approach which utilizes primitives and employs a LLM as a planner to sequence these primitives within photorealistic 3D environ-ments, where agents must perceive and act.This framework integrates high-level planning with low-level action execution, without the need for training.TANGO makes use of diverse modules designed for visual navigation and question-answering tasks, resulting in a system that seamlessly addresses both challenges while achieving SoA performance without requiring any prior adjustments.By providing a few in-context examples that show how to tackle multiple tasks, TANGO is capable of generalizing to the specific task at hand.This is facilitated by the LLM, which effectively combines the individual modules available within the TANGO system.Moreover, it extends [52] exploration policy by incorporating a memory mechanism in the form of a stored feature map that retains information about previously explored areas, supporting for efficient life-long navigation tasks.</p>
<p>To demonstrate the flexibility of the proposed framework, TANGO has been tested on three popular benchmarks: namely, i) Open-Vocabulary ObjectGoal Navigation [53], ii) Multi-Modal Lifelong Navigation [25], and iii) Embodied Question Answering [15,33].Results match or surpass previous approaches, without requiring specialized training.Summing up, the contributions of this paper are as follows:</p>
<p>• Introduction of a neuro-symbolic compositional LLMbased framework for EAI leveraging specialized primitive modules.• Demonstration of robust generalization capabilities across multiple tasks without the need for any specific training or fine-tuning.• Extension of the exploration policy presented in [52] to multi-goal scenarios through the incorporation of a memory mechanism stored as a feature vector map.• Achievement of state-of-the-art results, underscoring the effectiveness of the proposed framework.</p>
<p>Related Works</p>
<p>Our work takes inspiration from modular approaches for visual tasks, like the seminal Neural Module Networks [2,19], as well as the recent VisProg [18] and ViperGPT [44] frameworks.The main contribution of this paper is to extend these ideas to embodied visual navigation, that usually require heavy end-to-end learning, although modular approaches have recently shown promising results [7,17].Therefore, we discuss prior work in the area of embodied AI, the use of language models in robotics, and program generation approaches for image recognition tasks.</p>
<p>Embodied AI and Visual Navigation.The field of Embodied AI has recently undergone a paradigm shift, fuelled by the emergence of highly efficient simulators [26,35,[41][42][43].These simulators enable processing numerous parallel simulations in photorealistic indoor environments, fa-cilitating large-scale testing that was otherwise challenging in classical robotics.Alongside these simulators, a large variety of tasks (and benchmarks) has been proposed: PointGoal Navigation [47], where an agent navigates from point A to point B in an unknown environment; Object-Goal Navigation [3], requiring the agent to locate and navigate to an object in the scene; Instance-Image Goal Navigation [27], akin to ObjectGoal Navigation, but in which the agent should find a specific object depicted in a given image; Multi-Modal Lifelong Navigation [25] involving navigating to a sequence of target objects that can be specified through labels, images, or textual descriptions; Embodied Question Answering [15,33], in which an agent navigates an environment to gather information needed to answer a question, and Vision and Language Navigation (VLN) [1].</p>
<p>Various approaches have been proposed to tackle these tasks, primarily falling into three categories: end-toend [6,50,51], modular [11-13, 28, 38, 39] and LLMbased [16,29,[54][55][56].However, the main limitation of these paradigms is their dependence on task-specific architectures.In the case of end-to-end approaches, a model is trained for a specific problem, often requiring days of training, such as in [47], where a policy for PointGoal Navigation was trained for 2.5 billion steps.To adapt the learned model to a new problem, adjustments and retraining are necessary, and this comes with a high cost.Modular approaches share a similar challenge, usually relying on policies designed for specific tasks, although common elements, such as exploration and navigation modules, are often shared [11,12,28,38,39].Adapting these solutions to different tasks requires a manual adjustment of the approach to fit the new domain, often involving the addition or modification of modules.</p>
<p>Language models for Robotics In robotics, foundation models (LLMs) trained on vast internet-scale datasets [5,23,45,46] have the potential to equip robots with realworld priors and advanced reasoning abilities without the need for extensive task-specific training.Early approaches equipped agents with learned language embeddings, requiring large amounts of training data [4,31].Recent studies, on the other hand, have explored zero-shot and few-shot solutions mainly focusing on robotic planning and manipulation tasks [21,22,31].In the context of visual navigation, LLM-based approaches leverage the powerful LLMs priors and reasoning capabilities to guide navigation within the environment [16,29,[54][55][56]; however, despite their remarkable contributions, they often lack a modular design that allows for flexible integration and extensibility, as they are typically optimized for single tasks rather than a comprehensive, adaptable framework.</p>
<p>In contrast, our work deviates from these paradigms by relying on a series of diverse pre-trained modules that, thanks to a LLM, can be combined to potentially solve various tasks, compositionally.None of our modules are fine-tuned explicitly for the target problems, placing us in a zero-shot setting.Moreover, TANGO does not only rely on an exploration policy based entirely on LLM output, as in [16,55,56].Instead, it combines a frontier exploration policy with LLM priors and memory guidance to enhance navigation performance.To achieve this, we use a pre-trained PointGoal policy as the base waypoint navigation module of TANGO and extend the exploration policy introduced in [52] by equipping the agent with a memory mechanism stored as a feature vector for each pixel of the map.</p>
<p>Modular vision and program composition for visual tasks.Neural Module Networks (NMN) [2,19] introduced modular and compositional methodologies for visual question answering (VQA).NMNs integrate neural modules into an end-to-end differentiable network; the approach originally relied on pre-existing parsers [2], whereas more recent methods [19,20,24] have evolved to learn the layout generation model concurrently with the neural modules, employing reinforcement learning [48] and weak supervision.Stack-NMN [20] extends N2NMN by transitioning from discrete to soft layout generation, incorporating a weighted average of predictions from all modules at each step, determined by a layout generation network.</p>
<p>Recently, [18] introduced VisProg, a framework that offers two key advantages over NMNs.Firstly, it constructs high-level programs that invoke state-of-the-art neural models and Python functions at intermediate steps, diverging from the conventional approach of generating end-to-end neural networks.This design facilitates the integration of symbolic, non-differentiable modules.Secondly, it capitalizes on the in-context learning ability of large language models (LLMs) [5] to generate programs.This is achieved by prompting the LLM with a natural language instruction (or a visual question or a statement to be verified), along with a few examples of similar instructions and their corresponding programs.This approach eliminates the need to train specialized program generators for each task.Almost concurrently, ViperGPT [44] presented a similar approach to VisProg, but in this case the model directly generates Python code, instead of composing pre-defined modules and routines.In contrast, in our framework, the LLM is provided only with high-level knowledge of the functions of each primitive, which is encoded directly in the primitive's name.Therefore, it is built similarly to VisProg, as directly creating code to solve EAI tasks is a much harder problem than composing pre-defined primitives.It is noteworthy that both VisProg and ViperGPT were tested on images, while our model is specifically designed for embodied agents operating in an interactive environment, relying on sensor-based perception.</p>
<p>Method</p>
<p>Neuro-symbolic approaches offer the possibility to address a broad range of diverse complex tasks efficiently.Systems like VisProg [18] operate on images and have demonstrated excellent results in zero-shot settings.Understanding how a similar approach works in an action-oriented scenario is key to enable robots to navigate autonomously in novel environments.Therefore, we develop this paradigm further into EAI, where the integration with action execution adds several challenges.Inspired by [18], given an input prompt, we rely on a LLM to generate synthetic pseudo-programs that can be executed by the embodied agent in the environment.Our TANGO framework is therefore able to generalize to new tasks, without any direct training on taskspecific datasets, thus effectively mitigating the heavy computational training costs inherent to embodied navigation tasks.The procedure that enables generating these new programs is based on "in-context examples" that are fed to the LLM, alongside a natural language instruction.An outline of TANGO is shown in Figure 2.</p>
<p>TANGO Interpreter.The first key component of the proposed framework is referred to as "Program Interpreter".It comprises visual recognition modules that can be used by the agent to extract the semantics of the scene, as well as to provide an understanding of the visual context.</p>
<p>As shown in Figure 1, users can ask questions or give specific tasks to the agent; the natural language prompt is then processed by the LLM (in our implementation GPT-4o [5]), which serves as a planner and outputs a step-bystep executable program.To ensure the LLM delivers a reasonable output, it is fed with 15 "in-context examples" across diverse tasks.This enables the LLM to make use of its reasoning capabilities effectively, identifying the most suitable planning for the required task.Moreover, the examples remain the same regardless of the task identified in the prompt; it is the LLM's responsibility to output the specific program target for the given question.Programs use a higher level of abstraction than previous modular attempts such as Neural Module Networks (NMN) [2,19].Each program is constructed as a sequence of primitives (e.g., detect, answer, match, etc.) that invoke corresponding TANGO modules.These modules are either powered by pre-trained state-of-the-art vision models or implemented as simple Python subroutines (e.g., count, is found, eval, etc.), with additional navigation-specific modules designed to "steer" the agent's movements (e.g.navigate to, return, turn, etc.). Figure 3 provides a comprehensive list of all the currently implemented modules.</p>
<p>Figure 2 shows an Embodied Question Answering (EQA) example for the user's question: "on the left of the toilet there's a shower, is the shower curtain open or closed?".The LLM, which acts as a planner, transforms the  user's initial prompt into navigation subtasks by leveraging its reasoning abilities and powerful priors.This decomposition enables the model to break down complex queries into more manageable steps, guiding the agent's navigation process effectively.Each line of the generated program corresponds to a module serving a specific task.The agent must then execute the generated program.The LLM is also guided to comment on its steps directly within the generated pseudocode.</p>
<p>All the given primitives are equipped with methods to: i) parse lines in order to extract input argument names and values, as well as the output variable name; ii) execute the module in the environment, and update the program state with the output variable name and value.The outputs for each step and the comments in the generated pseudocode can also be used to better understand what is happening under the hood and why the LLM generated a specific line, thus enabling a good interpretability of the system behavior.</p>
<p>TANGO modules utilize open-source software and models, readily downloadable from the web.They can also be effortlessly updated with newer, more efficient models as they become available.</p>
<p>Navigation Module.In order to navigate the environment, we define a module employing a PointGoal navigation agent as our foundational module [39,52].This model achieved nearly perfect PointGoal performance, both in terms of success (∼ 99%) and efficiency (a forward pass that takes a fraction of a second) on standard datasets [9,37].The agent starts the exploration of the environment until it locates its target goal.Once the target is identified, the focus of exploration transitions to reaching the designated goal.This PointNav policy exclusively uses the egocentric depth image and the robot's relative distance and heading towards the desired goal point as input.</p>
<p>Exploration Policy.Exploration is performed using the policy outlined in [52].This method builds occupancy maps based on depth observations to identify exploration frontiers.It further leverages RGB observations and a pretrained vision-language model (BLIP2 [30]) to generate a language-grounded "value map".This value map is then used to guide the exploration, facilitating an efficient search for instances of a given target.Notably, as in [52], the agent performs a 360 • turn at the start of navigation to initialize frontiers.</p>
<p>We extend this policy for sequential goals by incorporat- Modules span a variety of inputs and outputs.Orange modules use Python subroutines, while blue modules use pre-trained computer vision models (similarly to [18]).The navigate to and explore scene modules, in green, both implement our foundational PointNav module; however, only explore scene integrates the memory mechanism.</p>
<p>ing a memory mechanism, represented as a "feature map" in which each pixel of the value map is encoded as a vector and updated at each step.This feature map then updates the language-grounded original value map when a new target is specified, by calculating the cosine similarity between each pixel's vector and the new target embeddings (either from text or image).We sample the highest value in the updated value map; if this value exceeds a predefined threshold, indicating the agent has previously encountered and "remembers" the target's location, the agent navigates to it, supporting lifelong navigation.The process is also computationally efficient, as the feature map similarity calculations are performed only if the target changes, rather than at each step during navigation.</p>
<p>Navigation Tasks with TANGO</p>
<p>TANGO leverages pre-trained multimodal vision-language and vision-only models as foundational components to extract semantic information from the scene, making it wellsuited for several Embodied AI tasks.</p>
<p>(Open-set) ObjectGoal Navigation [3,53], in particular, emerges as a suitable testbed to evaluate the efficacy of our method.A key module to tackle this problem involves utilizing an object detector (Owlv2 in our implementation [34] or DETR [8] for target objects that fall within the COCO classes [32]) to identify objects within the image.Subsequently, navigation towards the detected objects is facilitated by waypoints, leveraging depth distance calculation to guide the agent effectively.For simplicity, we use the center of the bounding box to determine the target waypoint.Hence, our approach effectively addresses this task without requiring prior knowledge of target labels for previously encountered objects, making it suitable for real-world applications.</p>
<p>TANGO also integrates a specialized module for matching target objectives with ongoing visual observations in image-based scenarios, resulting in accurate navigation towards an instance image.To evaluate our agent's performance in this context, we use SuperGlue network [40], optimized for indoor environments (i.e. using hyperparame-ters recommended in the respective paper).We rely on Su-perGlue because of its real-time matching capabilities and effortless integration into Embodied systems.</p>
<p>TANGO being task-agnostic, can process any type of navigation target, regardless of the order in which it is presented.In this context, the GOAT benchmark [25] is wellsuited for evaluating our system, as it operates in scenarios where targets are specified through images, text, or descriptive phrases, provided in random sequence.It also heavily relies on memory to find previously seen targets when sequential goals are given.</p>
<p>An agent should also be able to answer user queries like "can you check if the kitchen table is clean?".Hence, EQA serves as an excellent benchmark to assess TANGO's capabilities and robustness.EQA consists of three phases: understanding the semantic structure of the prompt, locating the target object(s), and analyzing the visual semantics to generate an accurate response.To this end, TANGO integrates a specific answer module, relying on BLIP2 [30] as its core foundation.</p>
<p>Experiments</p>
<p>TANGO provides a flexible framework that can be applied to various embodied navigation problems.We evaluate our approach on three popular tasks that require a wide range of capabilities, including efficient environment exploration, path planning, scene and context understanding, and image similarity comparison.</p>
<p>Experimental Setup</p>
<p>Agent Configuration.Prior research on visual navigation commonly uses different agent configurations depending on the considered task [15,25,53].The configuration typically employed for (OPEN-SET) OBJNAV closely resembles that of a LoCoBot, with an agent height of 0.88m, a radius of 0.18m, and a single 640 × 480 RGB sensor with a 79 • hfov, positioned 0.88m above the ground.</p>
<p>In the context of GOAT Benchmark, some of the default settings differ, featuring a camera situated 1.31m above the ground.The agent has a height of 1.41m and is given 360 × 640 RGB images.Both the above tasks' configurations use a step size of 0.25m and a left and right turning angle of 30 • .Lastly, the settings for EQA mirror those of PointGoal Navigation as specified in the Habitat-Lab configuration files.All other settings remain consistent with those of [15].</p>
<p>Datasets and Evaluation Protocol.We assess the TANGO performance across task-specific datasets using the Habitat simulator [42].As the system is not trained in any way (except for the PointGoal Navigation model, which has been pre-trained on the training set of HM3D, never used in our tests), each scene within episodes is novel to the agent.Therefore, the entire validation set can be categorized as "unseen".We now describe in detail the tasks and datasets used:</p>
<p>• OPEN VOCABULARY OBJECT NAVIGATION (OVON) [53]: a large-scale benchmark featuring over 15,000 annotated household objects across 379 categories, derived from real-world 3D scans [37].The agent is initialized at a random location within a scene and tasked with navigating to a target object category within a time limit of 500 steps.To ensure comparability with other state-of-the-art (SoA) methods, we evaluate our approach across all episodes included in the "val unseen" set.</p>
<p>• MULTI-MODAL LIFELONG NAVIGATION (GOAT-BENCH) [25]: an agent is tasked with sequentially navigating to five to ten target objects identified by category names, descriptions, or images.Each target represents a subtask executed within an open-vocabulary framework that spans over 312 categories.The agent is required to reach a goal within a specified time constraint and is assigned new targets upon the completion of each subtask.We evaluate our approach across all episodes included in the "val unseen" set.• EMBODIED QUESTION ANSWERING (OPENEQA) [33]: containing over 1,600 questionanswer pairs sourced from more than 180 real-world environments and scans [14,37].It is divided into two task categories: Episodic Memory-EQA (EM-EQA) and Active-EQA (A-EQA).Our focus is on the latter, in which the agent must autonomously navigate and answer questions within a time constraint of 500 steps.Questions span over seven categories, including world knowledge, attribute recognition, spatial reasoning, and object localization.Examples of questions from the episodes include: "Is the microwave door propped open?" and "What is left of the kitchen pass-through?".We evaluate our approach across all episodes belonging to the A-EQA task category.</p>
<p>Evaluation Metrics.In all tasks, we use the standard metrics as in prior works [3,15,33,53].Namely: • Success Rate (SR): measures the ratio of episodes where the agent succesfully reached its target (Open ObjNav, GOAT-Bench).• Success weighted by Path Length (SPL): measures the optimality of the path taken by the agent w.r.t. the optimal path (Open ObjNav, GOAT-Bench).• Distance to Goal (DTG): measures the average distance to goal of the agent at the end of the episode (Open Obj-Nav, GOAT-Bench).• Score (LLM-Match): an LLM compares the ground-truth GT i answers with model output A i given a question Q i and assigns a score σ i on a scale of 1 to 5. On this scale, 1 indicates an incorrect response, 5 represents a correct response, and intermediate values reflect varying levels of similarity.For example, the answer to the question "What color is the bed" could be correctly answered either as "white" or "the bed is white".The final results aggregation is as follows:
S = 1 N N i σ i − 1 4 × 100%(1)
• Answer Accuracy: the average accuracy of the answers provided by the agent for the EQA task.This metric, used in [15], is presented with the related results in the supplementary material.</p>
<p>Implemented Modules.TANGO contains descriptive and easily understandable names for the modules, arguments, and variables to facilitate the LLM comprehension of the input and output.Figure 3 showcases a list of all available modules.Each module exclusively outputs predefined object variables to the next one, enabling the possibility to monitor the progression of the agent at each step.Therefore, dramatically enhancing the explainability of failure cases, as it allows for a deep examination of the agent's decision-making process through its interactions with the environment.Figure 6 provides a general overview of the failure analysis in EQA task.As presented in Section 3, "navigate to" and "explore scene" modules are specifically engineered to navigate the environment, leveraging inputs from depth and RGB sensors as well as the PointGoal GPS + compass sensors.If this sensor is not available for a specific task, it can be derived from the current agent pose and the exploration goal location.The agent is provided with the actions: Forward, Move Left, Move Right, Look Up, Look Down and Stop.</p>
<p>For the "detect" module, we employ Owlv2 [34] object detector for general classes and use DETR [8] for categories within the COCO classes [32].Notably, in synthetic 3D scenes, we noticed numerous false positives.To address the problem, bounding boxes produced by the detector are forwarded to a "classify" module.Within this module, each detection undergoes classification to differentiate between categories of similar instances.For example, the class "chair" encompasses diverse subcategories such as "armchair", "couch", and "other".Subclasses are outputted under the hood directly by the LLM requiring no human annotation.We utilize a CLIP-based classifier in its default configuration [36].</p>
<p>Embodied Question Answering (EQA) task uses an "answer" module based on BLIP2 [30], capable of performing various multi-modal tasks, including Visual Question Answering, Image-Text retrieval, and Image Captioning.Success: The answer has a LLM-Match of 5 out of 5</p>
<p>Step 208</p>
<p>Step 1</p>
<p>Step 10</p>
<p>Step 100</p>
<p>Step Notably, the module is also used when goals are specified through images, to capture the class of the input image before starting the navigation.Furthermore, for image-based tasks we exploit a "match" module to evaluate the similarity between the agent's observation and the instance image target.This module performs feature matching between the two, as described in Section 3.1, to aid in navigation.Hence, it assists the agent in determining whether what it perceives as the current target is indeed the correct goal.</p>
<p>Experimental Results</p>
<p>Open-set ObjectGoal Navigation We evaluated our method in the standard Open-Set OBJNAV setting, with targets sampled directly from objects in the scene.As shown in Table 1, our approach achieves performance on par with state-of-the-art methods (rows 4-5) on the validation-unseen split both in SR (35.5%) and SPL (19.5%).Instead of focusing solely on numerical results, these findings highlight TANGO 's flexibility in solving navigation tasks in unfamiliar environments, underscoring the potential of our approach within the OBJNAV framework.</p>
<p>Embodied Question Answering.In EQA, the agent is queried with a natural language question and must autonomously explore the environment and gather information in order to answer accordingly.In recent literature, this task shifted from being regarded as a purely classification problem aimed at determining the most suitable answer  from a set of pre-defined possibilities (i.e., class labels) to an open-vocabulary benchmark where the agent must answer with natural language [15,33].We compare TANGO results, using the score from Eq. 1, against other zero-shot approaches.Table 2 shows that our method ranks second and closely matches the performance of the leading SoA method, with a gap of less than 1%.Notably, these approaches [33] utilize LLMs to interpret scene objects stepby-step (rows 3-4), but they do not leverage LLMs to effectively guide navigation towards specific targets.Hence, our approach demonstrates that SoA results can still be achieved by pre-planning a path that prioritizes relevant areas for a given question.Moreover, the results indicate that human agents achieve a significantly higher score of 85%, which remains well above the performance achieved by using large language models (LLMs), particularly in open-world settings.This gap underscores the challenges LLMs face in handling the complexity and variability of open-world scenarios compared to human agents.</p>
<p>GOAT.We assess our method within the GOAT-Bench setting, where the agent is spawned randomly and tasked with sequential targets.Each goal can be specified either by its category name, a description, or an image (e.g. the input could be "gas boiler", "the gas boiler on the corner of the room.The gas boiler is located on the left of  Step 48</p>
<p>Step 78</p>
<p>Step 115</p>
<p>Step 239 the washing machine and freezer" or an image of the "gas boiler").Table 3 compares our approach to the methods in [10].TANGO significantly outperforms other state-ofthe-art techniques (rows 1-4), achieving a +2.6% gain in the success metric.Additionally, it ranks second in navigation efficiency, with only a slight −0.7% difference from the top-performing method.We primarily attribute these results to the LLM's ability to accurately transform target descriptions into a sequence of selected goals, rather than relying solely on potentially "noisy" text embeddings.Moreover, the use of a memory mechanism is essential for the task (Fig. 5); however, if the memorized target object is incorrect, the path efficiency can significantly decrease.</p>
<p>TANGO Failure analysis</p>
<p>Due to the high explainability of our system, we conducted an analysis of the failures encountered during EQA episodes (Fig. 6).We extracted a significant subsample for the task and manually classified the instances where the model failed.We considered scores below 3 as failures, i.e. incorrect answers.We observed that the main cause of failure can be attributed to the "detect" module (stopped at wrong object or Ignored goal object in the image).Furthermore, the exploration policy appears to perform well  given the targets produced by the LLM, as it fails in only ∼ 10% of the cases (labeled as: didn't see target goal in the image).The LLM generates incorrect pseudo-code around 18% of the time, with 6.9% failures leading to ambiguous or incorrect targets, and 11.2% due to incorrect primitive ordering or usage which instead leads directly to episode failure.Notably, only 11.2% of errors are actually due to the LLM generating incorrect code, while the remaining is attributable to prompt-related issues.</p>
<p>Conclusion</p>
<p>We conducted a systematic analysis of our compositional approach, offering valuable insights into the future of modular, neuro-symbolic systems.Moreover, TANGO highlights the versatility of simple PointGoal Navigation agents equipped with specific task-oriented modules, yielding promising results in zero-shot scenarios across all considered tasks.TANGO effectively handles diverse multi-modal prompts, following instructions to complete tasks, underscoring the potential of LLMs in robotic navigation.Its integrated memory supports lifelong navigation, suggesting improved capabilities with increased exploration.However, it may struggle with overly complex or ambiguous prompts, which can limit the LLM's ability to identify the correct primitives or targets.Future work could extend this system to tasks such as Visual Language Navigation (VLN) and explore the use of open-source LLMs.Additionally, refining the memory mechanism-particularly through improved sampling strategies for identifying high-value pixels linked to objects' memory-could further enhance the system's effectiveness.</p>
<p>Figure 1 .
1
Figure1.We introduce TANGO, a modular neuro-symbolic system for compositional embodied visual navigation.Given a few examples of natural language instructions and the corresponding programs composed of action primitives, TANGO can generate executable programs, enabling the agent to perform multiple tasks within a 3D environment.</p>
<p>on the left of the toilet there's a shower, is the shower curtain open or closed?Instruction: How many lights are above the bathroom mirror?Program: # search for the bathroom explore_scene() bathroom = detect(target = "bathroom") if is_found(bathroom): # search for the mirror in the bathroom explore_scene(loc = bathroom) mirror = detect(target = "mirror") if is_found(object = mirror): # navigate to it and count how many lights navigate_to(waypoint = mirror) # look up above the mirror and count look_up() lights = detect(target = "light") n_lights = count(object = lights) # convert to string number = eval(" '{n_lights}' if {n_lights} &gt; 0 else '0' ") return number LLM as Planner Program Interpreter Program: # assuming the shower is usually in the bathroom explore_scene() room = detect(target = "bathroom") if is_found(object = room): # search for the toilet explore_scene(loc = room) target = detect(target = "toilet") if is_found(object = target): navigate_to(waypoint = target) # look on the left for the shower turn_left(degrees = 40) # answer the question ans = answer(question = "is the shower curtain open?") return ans Execution in the 3D environment</p>
<p>Figure 2 .
2
Figure 2. Overview of the program generation in TANGO.Given few "in-context examples", the LLM provide a detailed sequence of steps to be executed by the agent in the given environment.The LLM is instructed to comment its output to allow for explainability.</p>
<p>Figure 3 .
3
Figure3.Overview of TANGO modules.Modules span a variety of inputs and outputs.Orange modules use Python subroutines, while blue modules use pre-trained computer vision models (similarly to[18]).The navigate to and explore scene modules, in green, both implement our foundational PointNav module; however, only explore scene integrates the memory mechanism.</p>
<p>Does the bathroom door open "into" or "out" the bathroom?GT Answer: "out" Answer: it opens out</p>
<p>Figure 4 .
4
Figure 4. Examples from OpenEQA [33].The top section illustrates a successful episode where TANGO is able to understands the input query, correctly specifying the sequential targets.The lower section illustrates a failure caused by overly general directions from the LLM, which TANGO struggled to resolve.</p>
<p>20 Figure 5 .
205
Figure 5. Multi-Modal Lifelong Navigation Success Example.(top) RGB observation of the target during STOP action (step ti+steps).(middle) Value map for the specific target recomputed from the memory map (step ti+steps).(bottom) Memory map after target changes (step ti).</p>
<p>Figure 6 .
6
Figure 6.Causes of system failure in EQA tasks.The analysis was manually conducted on a significant sample of failed episodes, facilitated by the explanaibility of TANGO .Answers were deemed as "incorrect" if they received a score below three.</p>
<p>Table 1 .
1
±0.3 7.5 ±0.2 BCRL [53] 8.0 ±0.2 2.8 ±0.1 DAgRL [53] 18.3 ±0.3 7.9 ±0.1 * VLFM [52, 53] 35.2 19.6 DAgRL+OD [53] 37.1 ±0.2 19.9 ±0.3 TANGO (ours) 35.5 ±0.3 19.5 ±0.3 HM3D-OVON.Performance comparison of different methods on VAL UNSEEN split.* deterministic method.± 1.7 Socratic LLMs w/ Frame Captions [33] 38.1 ± 1.8 Socratic LLMs w/ Scene-Graph Captions [33] 34.4 ± 1.8
VAL UNSEENMethodSR (↑)SPL (↑)RL [53] 18.6 MethodScore (↑)Human Agent [33]85.1 ± 1.1Blind LLMs [33] 35.5 TANGO (ours) 37.2 ± 1.8</p>
<p>Table 2 .
2
OpenEQA</p>
<p>results.Comparison of TANGO with the SoA on the OpenEQA dataset.All methods involve zero-shot approaches.</p>
<p>Table 3 .
3
GOAT-Bench.Performance comparison of different methods on the VAL UNSEEN split.
GOAL: RefrigeratorGOAL: carpet in the room. it isGOAL: Freezerlocated near the rack, freezer,and refrigerator, towards thebottom right corner of theRGBframe.Value MapFrontier BestFrontier BestFrontier BestFrontier BestMemory MapNo Memory for the first subtask goalMemory TargetMemory TargetNo memory of target object, explore using frontiers
GOAL:Image DTG: 0.13 SR: 1.00 SPL: 0.60 DTG: 0.11 SR: 1.00 SPL: 0.16 DTG: 0.09 SR: 1.00 SPL: 0.53</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2018</p>
<p>Neural module networks. Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition201623</p>
<p>Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets, Roozbeh Mottaghi, Manolis Savva, Alexander Toshev, Erik Wijmans, arXiv:2006.13171ObjectNav revisited: On evaluation of embodied agents navigating to objects. 2020. 2, 5, 6arXiv preprint</p>
<p>Meta-reinforcement learning via language instructions. Zhenshan Bing, Alexander Koch, Xiangtong Yao, Kai Huang, Alois Knoll, 2023 IEEE International Conference on Robotics and Automation (ICRA). 2023</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Proc. of Advances in Neural Information Processing Systems (NeurIPS). of Advances in Neural Information essing Systems (NeurIPS)Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish202023</p>
<p>Exploiting scene-specific features for object goal navigation. Tommaso Campari, Paolo Eccher, Luciano Serafini, Lamberto Ballan, Proc. of the European Conference on Computer Vision Workshops (ECCVW). of the European Conference on Computer Vision Workshops (ECCVW)2020</p>
<p>Online learning of reusable abstract models for object goal navigation. Tommaso Campari, Leonardo Lamanna, Paolo Traverso, Luciano Serafini, Lamberto Ballan, Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>End-toend object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, European conference on computer vision. Springer202056</p>
<p>Matterport3D: Learning from RGB-D data in indoor environments. X Angel, Angela Chang, Thomas Dai, Maciej Funkhouser, Matthias Halber, Manolis Niessner, Shuran Savva, Andy Song, Yinda Zeng, Zhang, Proc. of the International Conference on 3D Vision (3DV). of the International Conference on 3D Vision (3DV)2017</p>
<p>Goat: Go to any thing. Matthew Chang, Theophile Gervet, Mukul Khanna, Sriram Yenamandra, Dhruv Shah, So Yeon Min, Kavit Shah, Chris Paxton, Saurabh Gupta, Dhruv Batra, arXiv:2311.064302023arXiv preprint</p>
<p>Learning to explore using active neural SLAM. Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, Ruslan Salakhutdinov, Proc. of the International Conference on Learning Representations (ICLR). of the International Conference on Learning Representations (ICLR)2019</p>
<p>Object goal navigation using goal-oriented semantic exploration. Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, Ruslan Salakhutdinov, Proc. of Advances in Neural Information Processing Systems (NeurIPS). of Advances in Neural Information essing Systems (NeurIPS)2020</p>
<p>Neural topological slam for visual navigation. Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav Gupta, Saurabh Gupta, Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020</p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Niessner, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2017</p>
<p>Embodied question answering. Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra, Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2018. 2, 5, 6, 7</p>
<p>Can an embodied agent find your "catshaped mug"? llm-based zero-shot object navigation. Sashank Vishnu, James F Dorbala, Dinesh MullenJr, Manocha, IEEE Robotics and Automation Letters. 232023</p>
<p>Navigating to objects in the real world. Theophile Gervet, Soumith Chintala, Dhruv Batra, Jitendra Malik, Devendra Singh, Chaplot , 2023. 2Science Robotics. 879</p>
<p>Visual programming: Compositional visual reasoning without training. Tanmay Gupta, Aniruddha Kembhavi, Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2023. 1, 2, 3, 5</p>
<p>Learning to reason: End-to-end module networks for visual question answering. Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Kate Saenko, Proc. of the IEEE/CVF International Conference on Computer Vision (ICCV). of the IEEE/CVF International Conference on Computer Vision (ICCV)201723</p>
<p>Explainable neural computation via stack neural module networks. Ronghang Hu, Jacob Andreas, Trevor Darrell, Kate Saenko, Proc. of the European Conference on Computer Vision (ECCV). of the European Conference on Computer Vision (ECCV)2018</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International conference on machine learning. PMLR20221</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.0560820221arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.0682520231Mistral 7B. arXiv preprint</p>
<p>. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, Ross B , </p>
<p>Inferring and executing programs for visual reasoning. Girshick, Proc. of the IEEE/CVF International Conference on Computer Vision (ICCV). of the IEEE/CVF International Conference on Computer Vision (ICCV)2017</p>
<p>Devendra Singh Chaplot, Dhruv Batra, and Roozbeh Mottaghi. Goat-bench: A benchmark for multi-modal lifelong navigation. Mukul Khanna, Ram Ramrakhya, Gunjan Chhablani, Sriram Yenamandra, Theophile Gervet, Matthew Chang, Zsolt Kira, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024. 2, 5, 6, 8</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, arXiv:1712.05474Ai2-thor: An interactive 3d environment for visual ai. 2017arXiv preprint</p>
<p>Instance-specific image goal navigation: Training embodied agents to find object instances. Jacob Krantz, Stefan Lee, Jitendra Malik, Dhruv Batra, Devendra Singh, Chaplot , arXiv:2211.158762022arXiv preprint</p>
<p>Navigating to objects specified by images. Jacob Krantz, Theophile Gervet, Karmesh Yadav, Austin Wang, Chris Paxton, Roozbeh Mottaghi, Dhruv Batra, Jitendra Malik, Stefan Lee, Devendra Singh, Chaplot , Proc. of the IEEE/CVF International Conference on Computer Vision (ICCV). of the IEEE/CVF International Conference on Computer Vision (ICCV)2023</p>
<p>Tina: Think, interaction, and action framework for zero-shot vision language navigation. Dingbang Li, Wenzhou Chen, Xin Lin, arXiv:2403.088332024arXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. 19730-19742. PMLR, 2023. 4, 5, 6</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE20231</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, Lawrence Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerSeptember 6-12, 2014. 201456Proceedings, Part V 13</p>
<p>Embodied question answering in the era of foundation models. Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202467</p>
<p>Scaling open-vocabulary object detection. Matthias Minderer, Alexey Gritsenko, Neil Houlsby, Advances in Neural Information Processing Systems. 2024366</p>
<p>. Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, So Yeon Min, Vladimír Vondruš, Theophile Gervet, Vincent-Pierre Berges, John M Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh Mottaghi2023Habitat 3.0: A co-habitat for humans, avatars and robots</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>K Santhosh, Aaron Ramakrishnan, Erik Gokaslan, Oleksandr Wijmans, Alex Maksymets, John Clegg, Eric Turner, Wojciech Undersander, Andrew Galuba, Angel X Westbury, Chang, arXiv:2109.08238Habitat-matterport 3D dataset (HM3D): 1000 large-scale 3D environments for embodied AI. 202146arXiv preprint</p>
<p>Poni: Potential functions for objectgoal navigation with interactionfree learning. K Santhosh, Devendra Ramakrishnan, Ziad Singh Chaplot, Jitendra Al-Halah, Kristen Malik, Grauman, Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Mopa: Modular object navigation with pointgoal agents. Sonia Raychaudhuri, Tommaso Campari, Unnat Jain, Manolis Savva, Angel X Chang, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision202424</p>
<p>Superglue: Learning feature matching with graph neural networks. Paul-Edouard Sarlin, Daniel Detone, Tomasz Malisiewicz, Andrew Rabinovich, 2020</p>
<p>Manolis Savva, Angel X Chang, Alexey Dosovitskiy, Thomas Funkhouser, Vladlen Koltun, Minos, arXiv:1712.03931Multimodal indoor simulator for navigation in complex environments. 2017arXiv preprint</p>
<p>Habitat: A platform for embodied AI research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Proc. of the IEEE/CVF International Conference on Computer Vision (ICCV). of the IEEE/CVF International Conference on Computer Vision (ICCV)2019</p>
<p>Lyne Tchapmi, et al. iGibson 1.0: a simulation environment for interactive tasks in large realistic scenes. Bokui Shen, Fei Xia, Chengshu Li, Roberto Martín-Martín, Linxi Fan, Guanzhi Wang, Claudia Pérez-D'arpino, Shyamal Buch, Sanjana Srivastava, Proc. of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)2021</p>
<p>ViperGPT: Visual Inference via Python Execution for Reasoning. Dídac Surís, Sachit Menon, Carl Vondrick, Proc. of the IEEE/CVF International Conference on Computer Vision (ICCV). of the IEEE/CVF International Conference on Computer Vision (ICCV)202323</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.1180520231arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.0928820231arXiv preprint</p>
<p>DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames. Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra, Proc. of the International Conference on Learning Representations (ICLR). of the International Conference on Learning Representations (ICLR)2019</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. Williams Ronald, Machine learning. 831992</p>
<p>Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan, arXiv:2307.01848Embodied task planning with large language models. 2023arXiv preprint</p>
<p>Auxiliary Tasks Speed Up Learning PointGoal Navigation. Joel Ye, Dhruv Batra, Erik Wijmans, Abhishek Das, Proc. of the International Conference on Robot Learning (CoRL). of the International Conference on Robot Learning (CoRL)2020</p>
<p>Auxiliary Tasks and Exploration Enable ObjectNav. Joel Ye, Dhruv Batra, Abhishek Das, Erik Wijmans, Proc. of the IEEE/CVF International Conference on Computer Vi-(ICCV). of the IEEE/CVF International Conference on Computer Vi-(ICCV)2021</p>
<p>Vlfm: Vision-language frontier maps for zero-shot semantic navigation. Naoki Yokoyama, Sehoon Ha, Dhruv Batra, Jiuguang Wang, Bernadette Bucher, 2024 IEEE International Conference on Robotics and Automation (ICRA). 2024. 2, 3, 4, 7</p>
<p>Hm3d-ovon: A dataset and benchmark for open-vocabulary object goal navigation. Naoki Yokoyama, Ram Ramrakhya, Abhishek Das, Dhruv Batra, Sehoon Ha, arXiv:2409.142962024. 2, 5, 6, 7arXiv preprint</p>
<p>L3mvn: Leveraging large language models for visual target navigation. in 2023 ieee. Bangguo Yu, Hamidreza Kasaei, Ming Cao, RSJ International Conference on Intelligent Robots and Systems (IROS). </p>
<p>Navgpt: Explicit reasoning in vision-and-language navigation with large language models. Gengze Zhou, Yicong Hong, Qi Wu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202413</p>
<p>Navgpt-2: Unleashing navigational reasoning capability for large vision-language models. Gengze Zhou, Yicong Hong, Zun Wang, Xin , Eric Wang, Qi Wu, European Conference on Computer Vision. Springer202523</p>            </div>
        </div>

    </div>
</body>
</html>
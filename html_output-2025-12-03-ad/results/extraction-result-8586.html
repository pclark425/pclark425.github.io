<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8586 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8586</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8586</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278237785</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.00001v1.pdf" target="_blank">Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are primarily trained on high-resource natural languages, limiting their effectiveness in low-resource settings and in tasks requiring deep logical reasoning. This research introduces Rosetta-PL, a benchmark designed to evaluate LLMs’ logical reasoning and generalization capabilities in a controlled environment. We construct Rosetta-PL by translating a dataset of logical propositions from Lean into a custom logical language, which is then used to fine-tune an LLM (e.g., GPT-4o). Our experiments analyze the impact of the size of the dataset and the translation methodology on the performance of the model. Our results indicate that preserving logical relationships in the translation process significantly boosts precision, with accuracy plateau-ing beyond roughly 20,000 training samples. These insights provide valuable guidelines for optimizing LLM training in formal reasoning tasks and improving performance in various low-resource language applications.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8586.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8586.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o on Rosetta-PL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o fine-tuned and evaluated on the Rosetta-PL propositional-logic benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper fine-tunes GPT-4o on Rosetta-PL, a 25,214-entry propositional logic dataset translated from the Lean Workbook, and evaluates its logical reasoning on seen and unseen problems under different translation strategies and dataset sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A closed-source large language model referenced in the paper as a high-performing LLM on reasoning benchmarks (MMLU, GSM8K, Big Bench Hard); used here as the base for fine-tuning. The authors note it is closed-source and may have pretraining exposure to Lean-like syntax.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Rosetta-PL (propositional logic benchmark derived from Lean Workbook)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A custom propositional language benchmark constructed by translating 25,214 problems from the Lean Workbook into a controlled propositional language; problems are labeled true/false and arranged in conversational-format items. Designed to test pattern discovery and strict propositional logical reasoning (connectives, structured expressions) without relying on natural-language ambiguity or predefined inference templates.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning GPT-4o on the translated Rosetta-PL dataset with controlled experiments varying (1) translation strategy (Translation Key 1 preserving logical relationships vs Translation Key 2 arbitrarily disrupting them) and (2) training set size (25,214; 20,000; 10,000). Evaluation used 'seen' (samples from training set) and 'unseen' (MiniF2F-lean4) splits; base (unfine-tuned) GPT-4o was evaluated as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Quantitative: On 'seen' translated data average accuracy 95.97% (±0.33%) in custom propositional language vs 76.08% (±0.36%) in Lean (untranslated). On 'unseen' data: Lean 99.89% (±0.06%) vs Translation Key 1 97.56% (±0.44%). Translation Key 2 yielded much lower accuracy: 64.1% (±0.75%). Increasing training from 10k to 20k improved seen accuracy by ~2.7% and unseen by ~0.3%; accuracy plateaued beyond ~20k examples.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to the base (unfine-tuned) GPT-4o, fine-tuned models outperform on seen translated examples and on unseen translated examples; for seen Lean data the base model retains strong pretraining Lean knowledge and achieves results similar to some fine-tuned variants. The model fine-tuned with Translation Key 2 performed worse than base on unseen Lean.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Failure cases include severe degradation when logical structure is disrupted (Translation Key 2), signs of overfitting with larger training sets, performance plateau around ~20k samples, possible dataset redundancy and bias in unseen data, and dependence on the model's pretraining familiarity with Lean-like syntax (better performance on Lean unseen).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Preserving relational/logical structure in input representation materially improves LLM logical reasoning; fine-tuning on structured translated data helps generalize to both translated and Lean formats; there are diminishing returns beyond ~20k samples, suggesting attention to data diversity and efficiency; arbitrary symbol mappings harm learnability and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8586.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8586.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (base)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unfine-tuned / base GPT-4o (evaluated as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors evaluated the base (unfine-tuned) GPT-4o as a baseline to compare the effect of fine-tuning on Rosetta-PL; the base model retains pretraining knowledge of Lean-like syntax.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (base, unfine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The standard pretrained GPT-4o model (closed-source) used as a baseline in experiments; authors note it likely has prior exposure to Lean-like syntax from pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Rosetta-PL (baseline evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Evaluated on seen and unseen splits in both Lean and translated formats to measure retention of pretraining knowledge vs benefits of fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Direct evaluation (no fine-tuning) on the same seen/unseen datasets, including translated variants, to serve as baseline for finetuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported qualitatively: base GPT-4o performs well on Lean (untranslated) unseen data—sometimes similar to fine-tuned models—whereas fine-tuned models outperform the base on seen translated examples; exact per-dataset base-model numbers are summarized in Table 4 (not fully enumerated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Serves as the baseline; fine-tuned GPT-4o models outperform base on translated seen/unseen examples, but base retains strengths on Lean due to pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Base model can be outperformed on translated data if fine-tuning preserves logical structure; base performance depends on pretraining exposure to similar formal languages (e.g., Lean).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Comparisons indicate that fine-tuning on structured translations improves task-specialized reasoning beyond the base model, but pretraining familiarity with a formal language (Lean) provides a strong starting point for reasoning on untranslated problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8586.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8586.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (mentioned as example of an LLM that could be fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT is referenced in the paper’s introduction as an example of an LLM that might be fine-tuned on translated logical data, though experimental work reported uses GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A widely known conversational LLM referenced in introductory discussion; the paper mentions fine-tuning ChatGPT in one place but does not present experiments using it.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Rosetta-PL (proposed potential subject for fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same Rosetta-PL benchmark described above; ChatGPT is only suggested as an example and not actually evaluated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Mentioned (inconsistent) suggestion of fine-tuning conversational LLMs on translated propositional datasets; no experimental procedure given for ChatGPT here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No experimental data in this paper for ChatGPT; mention is illustrative only and the authors actually fine-tune GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>The paper implies that conversational LLMs (e.g., ChatGPT) could be adapted to formal logical benchmarks via translation and fine-tuning, but provides no empirical results for ChatGPT itself.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8586.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8586.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOGIC-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LOGIC-LM (Logic-LM: empowering LLMs with symbolic solvers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work cited that demonstrates LLMs can solve logic puzzles when aided by external symbolic solvers; included in related work to contrast with Rosetta-PL's focus on autonomous pattern discovery without external tooling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (various, in LOGIC-LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Work that integrates LLMs with external symbolic solvers to improve faithfulness of logical reasoning; the specific LLMs used are not detailed in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Logic-LM benchmark / approach</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Logic puzzles and tasks where symbolic solvers are combined with LLM outputs to achieve faithful logical reasoning; emphasizes coupling LLMs with solvers rather than relying solely on the model to discover patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Neuro-symbolic approach: LLMs augmented with external symbolic solvers to verify or guide reasoning (cited as prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not reported in this paper; LOGIC-LM is cited as demonstrating improved problem solving when aided by external solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used as a contrast: Rosetta-PL aims to test whether LLMs can autonomously discover logical patterns without reliance on external solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>The paper notes that approaches relying on external solvers may not demonstrate autonomous pattern discovery by LLMs (i.e., may rely on tooling rather than internalized reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Paper positions Rosetta-PL as complementary to neuro-symbolic work like LOGIC-LM by evaluating internal pattern-learning in a strictly formal language rather than solver-assisted reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8586.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8586.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOGIGLUE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LOGIGLUE benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited benchmark that evaluates multi-step reasoning using predefined inference templates; used in the paper's related work to highlight differences from Rosetta-PL's aim of testing autonomous discovery of logical rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (various on LOGIGLUE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LOGIGLUE evaluates LLM logical reasoning tasks structured around inference templates; specific model instances are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LOGIGLUE</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A benchmark for analyzing logical reasoning capabilities that typically relies on predefined reasoning steps and templates to evaluate multi-step logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Template-based evaluation of multi-step reasoning; often evaluated with prompting strategies like chain-of-thought in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not specified in this paper; LOGIGLUE is cited primarily to contrast methodological approach.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Contrasted with Rosetta-PL: LOGIGLUE's reliance on predefined inference steps makes it harder to assess whether models autonomously discover rules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper notes that benchmarks relying on predefined steps may conflate following templates with genuine rule discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Rosetta-PL is designed to avoid predefined inference templates and instead assess whether LLMs can infer novel logical patterns in a strict propositional language.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8586.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8586.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic Bench (Logicbench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited evaluation suite that assesses logical reasoning on known logical patterns; referenced to highlight that Rosetta-PL differs by requiring inference of novel patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (various on Logic Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Logic Bench evaluates systematic logical reasoning ability of LLMs on benchmark tasks with known patterns; the paper cites it as part of the landscape of symbolic reasoning evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Logic Bench</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A benchmark for systematic evaluation of logical reasoning where models are tested on canonical logical pattern tasks and known inference structures.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Benchmarking of LLMs on known logical patterns; some approaches integrate symbolic verification or tool use in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not provided here; cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used to contrast Rosetta-PL’s objective which is to measure discovery of new patterns rather than evaluation on pre-known templates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Benchmarks using known templates may not measure a model’s ability to discover novel logical relations.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Rosetta-PL complements existing benchmarks like Logic Bench by focusing on model-driven pattern discovery in a controlled propositional language.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>Towards logiglue: A brief survey and a benchmark for analyzing logical reasoning capabilities of language models <em>(Rating: 2)</em></li>
                <li>Logicbench: Towards systematic evaluation of logical reasoning ability of large language models <em>(Rating: 2)</em></li>
                <li>Faithful logical reasoning via symbolic chain-of-thought <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>A closer look at logical reasoning with llms: The choice of tool matters <em>(Rating: 1)</em></li>
                <li>Evaluating step-by-step reasoning through symbolic verification <em>(Rating: 1)</em></li>
                <li>Lean workbook: A large-scale lean problem set formalized from natural language math problems <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8586",
    "paper_id": "paper-278237785",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "GPT-4o on Rosetta-PL",
            "name_full": "GPT-4o fine-tuned and evaluated on the Rosetta-PL propositional-logic benchmark",
            "brief_description": "This paper fine-tunes GPT-4o on Rosetta-PL, a 25,214-entry propositional logic dataset translated from the Lean Workbook, and evaluates its logical reasoning on seen and unseen problems under different translation strategies and dataset sizes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "A closed-source large language model referenced in the paper as a high-performing LLM on reasoning benchmarks (MMLU, GSM8K, Big Bench Hard); used here as the base for fine-tuning. The authors note it is closed-source and may have pretraining exposure to Lean-like syntax.",
            "model_size": null,
            "reasoning_task_name": "Rosetta-PL (propositional logic benchmark derived from Lean Workbook)",
            "reasoning_task_description": "A custom propositional language benchmark constructed by translating 25,214 problems from the Lean Workbook into a controlled propositional language; problems are labeled true/false and arranged in conversational-format items. Designed to test pattern discovery and strict propositional logical reasoning (connectives, structured expressions) without relying on natural-language ambiguity or predefined inference templates.",
            "method_or_approach": "Fine-tuning GPT-4o on the translated Rosetta-PL dataset with controlled experiments varying (1) translation strategy (Translation Key 1 preserving logical relationships vs Translation Key 2 arbitrarily disrupting them) and (2) training set size (25,214; 20,000; 10,000). Evaluation used 'seen' (samples from training set) and 'unseen' (MiniF2F-lean4) splits; base (unfine-tuned) GPT-4o was evaluated as a baseline.",
            "performance": "Quantitative: On 'seen' translated data average accuracy 95.97% (±0.33%) in custom propositional language vs 76.08% (±0.36%) in Lean (untranslated). On 'unseen' data: Lean 99.89% (±0.06%) vs Translation Key 1 97.56% (±0.44%). Translation Key 2 yielded much lower accuracy: 64.1% (±0.75%). Increasing training from 10k to 20k improved seen accuracy by ~2.7% and unseen by ~0.3%; accuracy plateaued beyond ~20k examples.",
            "baseline_comparison": "Compared to the base (unfine-tuned) GPT-4o, fine-tuned models outperform on seen translated examples and on unseen translated examples; for seen Lean data the base model retains strong pretraining Lean knowledge and achieves results similar to some fine-tuned variants. The model fine-tuned with Translation Key 2 performed worse than base on unseen Lean.",
            "limitations_or_failures": "Failure cases include severe degradation when logical structure is disrupted (Translation Key 2), signs of overfitting with larger training sets, performance plateau around ~20k samples, possible dataset redundancy and bias in unseen data, and dependence on the model's pretraining familiarity with Lean-like syntax (better performance on Lean unseen).",
            "insights_or_conclusions": "Preserving relational/logical structure in input representation materially improves LLM logical reasoning; fine-tuning on structured translated data helps generalize to both translated and Lean formats; there are diminishing returns beyond ~20k samples, suggesting attention to data diversity and efficiency; arbitrary symbol mappings harm learnability and generalization.",
            "uuid": "e8586.0",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GPT-4o (base)",
            "name_full": "Unfine-tuned / base GPT-4o (evaluated as baseline)",
            "brief_description": "The authors evaluated the base (unfine-tuned) GPT-4o as a baseline to compare the effect of fine-tuning on Rosetta-PL; the base model retains pretraining knowledge of Lean-like syntax.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o (base, unfine-tuned)",
            "model_description": "The standard pretrained GPT-4o model (closed-source) used as a baseline in experiments; authors note it likely has prior exposure to Lean-like syntax from pretraining.",
            "model_size": null,
            "reasoning_task_name": "Rosetta-PL (baseline evaluation)",
            "reasoning_task_description": "Evaluated on seen and unseen splits in both Lean and translated formats to measure retention of pretraining knowledge vs benefits of fine-tuning.",
            "method_or_approach": "Direct evaluation (no fine-tuning) on the same seen/unseen datasets, including translated variants, to serve as baseline for finetuned models.",
            "performance": "Reported qualitatively: base GPT-4o performs well on Lean (untranslated) unseen data—sometimes similar to fine-tuned models—whereas fine-tuned models outperform the base on seen translated examples; exact per-dataset base-model numbers are summarized in Table 4 (not fully enumerated in text).",
            "baseline_comparison": "Serves as the baseline; fine-tuned GPT-4o models outperform base on translated seen/unseen examples, but base retains strengths on Lean due to pretraining.",
            "limitations_or_failures": "Base model can be outperformed on translated data if fine-tuning preserves logical structure; base performance depends on pretraining exposure to similar formal languages (e.g., Lean).",
            "insights_or_conclusions": "Comparisons indicate that fine-tuning on structured translations improves task-specialized reasoning beyond the base model, but pretraining familiarity with a formal language (Lean) provides a strong starting point for reasoning on untranslated problems.",
            "uuid": "e8586.1",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ChatGPT (mentioned)",
            "name_full": "ChatGPT (mentioned as example of an LLM that could be fine-tuned)",
            "brief_description": "ChatGPT is referenced in the paper’s introduction as an example of an LLM that might be fine-tuned on translated logical data, though experimental work reported uses GPT-4o.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChatGPT",
            "model_description": "A widely known conversational LLM referenced in introductory discussion; the paper mentions fine-tuning ChatGPT in one place but does not present experiments using it.",
            "model_size": null,
            "reasoning_task_name": "Rosetta-PL (proposed potential subject for fine-tuning)",
            "reasoning_task_description": "Same Rosetta-PL benchmark described above; ChatGPT is only suggested as an example and not actually evaluated in this work.",
            "method_or_approach": "Mentioned (inconsistent) suggestion of fine-tuning conversational LLMs on translated propositional datasets; no experimental procedure given for ChatGPT here.",
            "performance": "",
            "baseline_comparison": "",
            "limitations_or_failures": "No experimental data in this paper for ChatGPT; mention is illustrative only and the authors actually fine-tune GPT-4o.",
            "insights_or_conclusions": "The paper implies that conversational LLMs (e.g., ChatGPT) could be adapted to formal logical benchmarks via translation and fine-tuning, but provides no empirical results for ChatGPT itself.",
            "uuid": "e8586.2",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LOGIC-LM",
            "name_full": "LOGIC-LM (Logic-LM: empowering LLMs with symbolic solvers)",
            "brief_description": "A prior work cited that demonstrates LLMs can solve logic puzzles when aided by external symbolic solvers; included in related work to contrast with Rosetta-PL's focus on autonomous pattern discovery without external tooling.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (various, in LOGIC-LM)",
            "model_description": "Work that integrates LLMs with external symbolic solvers to improve faithfulness of logical reasoning; the specific LLMs used are not detailed in this paper's discussion.",
            "model_size": null,
            "reasoning_task_name": "Logic-LM benchmark / approach",
            "reasoning_task_description": "Logic puzzles and tasks where symbolic solvers are combined with LLM outputs to achieve faithful logical reasoning; emphasizes coupling LLMs with solvers rather than relying solely on the model to discover patterns.",
            "method_or_approach": "Neuro-symbolic approach: LLMs augmented with external symbolic solvers to verify or guide reasoning (cited as prior work).",
            "performance": "Not reported in this paper; LOGIC-LM is cited as demonstrating improved problem solving when aided by external solvers.",
            "baseline_comparison": "Used as a contrast: Rosetta-PL aims to test whether LLMs can autonomously discover logical patterns without reliance on external solvers.",
            "limitations_or_failures": "The paper notes that approaches relying on external solvers may not demonstrate autonomous pattern discovery by LLMs (i.e., may rely on tooling rather than internalized reasoning).",
            "insights_or_conclusions": "Paper positions Rosetta-PL as complementary to neuro-symbolic work like LOGIC-LM by evaluating internal pattern-learning in a strictly formal language rather than solver-assisted reasoning.",
            "uuid": "e8586.3",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LOGIGLUE",
            "name_full": "LOGIGLUE benchmark",
            "brief_description": "A cited benchmark that evaluates multi-step reasoning using predefined inference templates; used in the paper's related work to highlight differences from Rosetta-PL's aim of testing autonomous discovery of logical rules.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (various on LOGIGLUE)",
            "model_description": "LOGIGLUE evaluates LLM logical reasoning tasks structured around inference templates; specific model instances are not detailed in this paper.",
            "model_size": null,
            "reasoning_task_name": "LOGIGLUE",
            "reasoning_task_description": "A benchmark for analyzing logical reasoning capabilities that typically relies on predefined reasoning steps and templates to evaluate multi-step logical inference.",
            "method_or_approach": "Template-based evaluation of multi-step reasoning; often evaluated with prompting strategies like chain-of-thought in prior work.",
            "performance": "Not specified in this paper; LOGIGLUE is cited primarily to contrast methodological approach.",
            "baseline_comparison": "Contrasted with Rosetta-PL: LOGIGLUE's reliance on predefined inference steps makes it harder to assess whether models autonomously discover rules.",
            "limitations_or_failures": "Paper notes that benchmarks relying on predefined steps may conflate following templates with genuine rule discovery.",
            "insights_or_conclusions": "Rosetta-PL is designed to avoid predefined inference templates and instead assess whether LLMs can infer novel logical patterns in a strict propositional language.",
            "uuid": "e8586.4",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Logic Bench",
            "name_full": "Logic Bench (Logicbench)",
            "brief_description": "A cited evaluation suite that assesses logical reasoning on known logical patterns; referenced to highlight that Rosetta-PL differs by requiring inference of novel patterns.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (various on Logic Bench)",
            "model_description": "Logic Bench evaluates systematic logical reasoning ability of LLMs on benchmark tasks with known patterns; the paper cites it as part of the landscape of symbolic reasoning evaluations.",
            "model_size": null,
            "reasoning_task_name": "Logic Bench",
            "reasoning_task_description": "A benchmark for systematic evaluation of logical reasoning where models are tested on canonical logical pattern tasks and known inference structures.",
            "method_or_approach": "Benchmarking of LLMs on known logical patterns; some approaches integrate symbolic verification or tool use in prior work.",
            "performance": "Not provided here; cited as related work.",
            "baseline_comparison": "Used to contrast Rosetta-PL’s objective which is to measure discovery of new patterns rather than evaluation on pre-known templates.",
            "limitations_or_failures": "Benchmarks using known templates may not measure a model’s ability to discover novel logical relations.",
            "insights_or_conclusions": "Rosetta-PL complements existing benchmarks like Logic Bench by focusing on model-driven pattern discovery in a controlled propositional language.",
            "uuid": "e8586.5",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "Towards logiglue: A brief survey and a benchmark for analyzing logical reasoning capabilities of language models",
            "rating": 2,
            "sanitized_title": "towards_logiglue_a_brief_survey_and_a_benchmark_for_analyzing_logical_reasoning_capabilities_of_language_models"
        },
        {
            "paper_title": "Logicbench: Towards systematic evaluation of logical reasoning ability of large language models",
            "rating": 2,
            "sanitized_title": "logicbench_towards_systematic_evaluation_of_logical_reasoning_ability_of_large_language_models"
        },
        {
            "paper_title": "Faithful logical reasoning via symbolic chain-of-thought",
            "rating": 2,
            "sanitized_title": "faithful_logical_reasoning_via_symbolic_chainofthought"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "A closer look at logical reasoning with llms: The choice of tool matters",
            "rating": 1,
            "sanitized_title": "a_closer_look_at_logical_reasoning_with_llms_the_choice_of_tool_matters"
        },
        {
            "paper_title": "Evaluating step-by-step reasoning through symbolic verification",
            "rating": 1,
            "sanitized_title": "evaluating_stepbystep_reasoning_through_symbolic_verification"
        },
        {
            "paper_title": "Lean workbook: A large-scale lean problem set formalized from natural language math problems",
            "rating": 2,
            "sanitized_title": "lean_workbook_a_largescale_lean_problem_set_formalized_from_natural_language_math_problems"
        }
    ],
    "cost": 0.01184475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning</p>
<p>Shaun Baek shaun.baek@emory.edu 
Emory University</p>
<p>Shaun Esua-Mensah 
Algoverse AI Research</p>
<p>Cyrus Tsui 
Algoverse AI Research</p>
<p>Sejan Vigneswaralingam 
Algoverse AI Research</p>
<p>Abdullah Alali 
Algoverse AI Research</p>
<p>Michael Lu 
Algoverse AI Research</p>
<p>Vasu Sharma 
Algoverse AI Research</p>
<p>Kevin Zhu 
Algoverse AI Research</p>
<p>Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning
76E3863139914C393341030F8B24C1A6
Large Language Models (LLMs) are primarily trained on high-resource natural languages, limiting their effectiveness in low-resource settings and in tasks requiring deep logical reasoning.This research introduces Rosetta-PL, a benchmark designed to evaluate LLMs' logical reasoning and generalization capabilities in a controlled environment.We construct Rosetta-PL by translating a dataset of logical propositions from Lean into a custom logical language, which is then used to fine-tune an LLM (e.g., GPT-4o).Our experiments analyze the impact of the size of the dataset and the translation methodology on the performance of the model.Our results indicate that preserving logical relationships in the translation process significantly boosts precision, with accuracy plateauing beyond roughly 20,000 training samples.These insights provide valuable guidelines for optimizing LLM training in formal reasoning tasks and improving performance in various low-resource language applications.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), such as OpenAI's GPT models (Brown et al., 2020), Google's Gemini models (Team et al., 2024), and Meta's Llama models (Touvron et al., 2023), are typically trained on high-resource natural languages (e.g., English, Spanish, and Chinese).This focus on high-resource languages disadvantages speakers of low-resource languages, as training models for these languages are more challenging due to their inherent complexity (Team et al., 2022).Furthermore, semantic ambiguity, grammatical complexities, and contextual dependencies in natural languages can limit the capabilities of an LLM in precise logical reasoning.Since natural language often relies on implied meaning, subtle cues, and flexible syntax, models trained primarily on data using these principles * These authors contributed equally to this work.may struggle to follow strict rules needed for logical reasoning (Asher et al., 2023).</p>
<p>To isolate these reasoning abilities from language-specific challenges, we propose the evaluation of LLMs within a controlled setting using formal logical language.Logical languages, characterized by strict syntax and precise semantics, eliminate many of the extraneous factors present in natural languages, allowing us to focus squarely on pattern recognition and problem solving.Although prior benchmarks, such as LOGIGLUE (Luo et al., 2024), provide structured reasoning tasks, these typically rely on predefined reasoning steps, making it challenging to determine whether an LLM can autonomously identify and apply logical rules.In contrast, our benchmark, Rosetta-PL, evaluates whether LLMs can discover logical patterns within a propositional language, thereby measuring reasoning ability without relying on predefined inference steps or extraneous linguistic factors.Research on applying LLMs to logic-based problem solving is relatively scarce, and while chain-ofthought (CoT) prompting has gained popularity in natural language tasks (Wei et al., 2023), its effectiveness in logical or symbolic contexts remains largely unexplored (Creswell et al., 2022).</p>
<p>We address this gap by constructing Rosetta-PL by translating the Lean Workbook dataset (Ying et al., 2024) into our own propositional language and fine-tuning ChatGPT (Brown et al., 2020) using the translated dataset.We evaluate logical accuracy in our custom language while varying training data parameters such as training set size and the method of translation.Our experiments point towards potentially effective training strategies and provide preliminary estimates on the dataset size needed to approach benchmark-level logical understanding.By setting aside language-specific factors, we focus on the relationship between pattern recognition and data requirements, offering insights that impact language training in both high-arXiv:2505.00001v1[cs.CL] 25 Mar 2025 and low-resource settings.</p>
<p>Background</p>
<p>Large Language Models (LLMs) have excelled at tasks involving unstructured natural language, yet their capacity for structured logical reasoning remains underexplored (Creswell et al., 2022).The inherent ambiguities of natural language, such as polysemy and idiomatic expressions, can obscure true reasoning capabilities.In contrast, formal logical languages, defined by strict syntax and unambiguous semantics, offer a controlled testbed for evaluating pattern recognition and rule-based inference (Barcelo et al., 2023).</p>
<p>Propositional logic, a fundamental component of formal logic, employs connectives (e.g., ∧, ∨, ¬) to combine atomic propositions into complex expressions whose truth values are fully determined by their parts (Niu et al., 2024).This clarity makes it an ideal framework for assessing whether LLMs can autonomously learn and generalize logical rules-a skill central to disciplines like mathematics and programming (Nye et al., 2021;Polu and Sutskever, 2020).</p>
<p>Recent benchmarks have begun to probe the symbolic reasoning of LLMs.For example, LOGIC-LM demonstrates that LLMs can solve logic puzzles when aided by external symbolic solvers (Pan et al., 2023).Meanwhile, LOGIGLUE (Luo et al., 2024) andLogic Bench (Parmar et al., 2024) evaluate multi-step reasoning based on predefined inference templates, and chain-of-thought prompting has been shown to improve arithmetic performance (Wei et al., 2023).Other studies have further enriched this landscape: for example, the SymbCoT framework integrates symbolic expressions and logic rules directly into chain-of-thought (CoT) thereby boosting reasoning fidelity (Xu et al., 2024), while research examining the impact of symbolic solver choices has revealed that tool selection (e.g., Z3, Prover9, or Pyke) can cause performance variations of up to 50% (Lam et al., 2024).Furthermore, work on step-by-step symbolic verification has demonstrated that automated checks of intermediate reasoning steps can substantially enhance overall accuracy (Zhang et al., 2024).However, these approaches tend to rely on surface-level statistical correlations rather than genuine discovery of novel logical patterns (Creswell et al., 2022).</p>
<p>To bridge this gap, our work translates natural language logic problems into a propositional lan-guage, thereby eliminating linguistic complexities and focusing solely on intrinsic pattern recognition.Building on formal frameworks such as Lean4 (Ying et al., 2024), we investigate how well LLMs can learn and generalize new logical structures-a capability that also carries implications for improving training strategies in low-resource language settings (Team et al., 2022).</p>
<p>Method</p>
<p>Objective</p>
<p>The primary objective of this experiment is to evaluate the logical accuracy and pattern recognition capabilities of LLMs in a newly created propositional language.By removing linguistic complexities to focus solely on logical problem-solving, we aim to determine how well these models generalize and adapt in a structured, logic-based environment under varying dataset sizes, and whether this process reveals or rectifies discrepancies in their understanding of formal languages.</p>
<p>Dataset</p>
<p>We derived Rosetta-PL from the Lean Workbook (Ying et al., 2024), which is a dataset of logical problems translated into the formal language of Lean.Each problem was translated into our custom propositional language using a predefined translation key, resulting in a training dataset of 25,214 problems.Each dataset entry was written in a conversation-like structure with system, user, assistant, function, and message content, containing a logical problem (a statement) in our custom language and its corresponding truth value, indicating whether the statement is true or not.In contrast to benchmarks such as LOGIGLUE (Luo et al., 2024) and LOGIC-LM (Pan et al., 2023), which focus on logical problems with predefined inference steps, Rosetta-PL is designed to test an LLM's ability to discover new patterns.Unlike Logic Bench (Parmar et al., 2024), which evaluates performance on known logical patterns, our dataset requires the model to infer novel patterns.</p>
<p>Experimental Methodology</p>
<p>Our experimental setup involved building a data pipeline for fine-tuning GPT-4o on formal logical tasks.We opted to use GPT-4o primarily due to its performance on a range of reasoning benchmarks such as MMLU (Massive Multitask Language Understanding), GSM8K, and Big Bench Hard, al-lowing us to compare with one of the highest performers for LLMs in formal logic tasks.Because GPT-4o is closed-source, there is an inherent risk of leakage challenges.However, by translating the Lean Workbook into our own custom propositional language, we altered the original problems in an unorthodox way that makes direct overlap in GPT-4o's training far less likely.</p>
<p>Each entry in our training dataset was verified to conform to the required format-ensuring valid roles such as system, user, and assistant, and is passed on to GPT-4o for fine-tuning.From this same dataset, we also extracted "seen" testing subsets by randomly selecting 500 entries.We also extracted "unseen" testing subsets by randomly selecting 200 problems from an entirely different source: the Minif2f-lean4 dataset (Zheng et al., 2022), which does not overlap with the training dataset.We aim to measure the model's ability to both retain learned information and generalize its logical understanding to novel patterns through the "seen" and "unseen" datasets respectively.</p>
<p>Throughout these experiments, all fine-tuning and testing were conducted using NVIDIA A100 GPUs.Overall, GPT-4o underwent four separate fine-tuning runs, during which we kept parameter settings constant (e.g., learning rate, number of epochs) while varying the size of the training dataset (25,214, 20,000, and 10,000) and which one out of the two translation keys used.These translation keys altered how the logical problems from the Lean Workbook were mapped into our custom language, effectively creating multiple languages with varying logical structures.</p>
<p>Original Example:
xyz : N ⊢ (x 2 + 1) * (y 2 + 1) * (z 2 + 1) = (x + y + z) 2 − 2 * (x * y + y * z + z * x) + (x * y + y * z + z * x) 2 − 2 * x * y * z * (x + y + z) + x 2 * y 2 * z 2 + 1 (1)
Translation Strategies: To investigate the effect of symbolic representation on logical reasoning, we employ two distinct translation strategies.The first strategy maintains the inherent logical relationships by carefully mapping symbols, while the second intentionally disrupts these patterns through arbitrary transformations.These contrasting approaches allow us to assess how preserving or altering logical structure influences model performance.</p>
<p>• Translation Key 1 Strategy (Focused Key):</p>
<p>Translation Key 1 replaces Lean symbols with other symbols (see appendix).This method preserves logical relationships by ensuring that related symbols are consistently mapped.For instance, the symbols "&gt;" and "&lt;" are translated into "»" and "«", respectively, preserving their comparative meaning.This is to mimic spoken language, where symbols and phrases are logically related.Additionally, the sentence structure is encrypted using a scrambling function that adds a reversed duplicate of the sentence at the end, with a few additional symbols in between, in order to mimic the variations in sentence structures across different languages.An example of an entry translated with Key 1 is shown below:
xyz¬N##|−|−|−x ∧ ∧2 ∧ ∧1|−e|−|−|−y ∧ ∧2 ∧ ∧1|− e|−|−|−z ∧ ∧2 ∧ ∧1|− == |−|−|−x ∧ ∧y ∧ ∧z| − ∧ ∧2 2 e|−|−|−xey ∧ ∧yez ∧ ∧zx|− ∧ ∧|−|−|−xey ∧ ∧yez ∧ ∧zex|− ∧ ∧2 2 exeyeze|−|−|−x ∧ ∧y ∧ ∧z| − ∧ ∧x ∧ ∧2ey ∧ ∧2ez ∧ ∧2 ∧ ∧1(2)
• Translation Key 2 Strategy (Random Key): In contrast, this method removes logical structure by shifting the ASCII values of each character by 10, resulting in an entirely arbitrary transformation.As a result, the translated expression loses any recognizable logical patterns.Additionally, statements are inverted around logical operators such as -&gt;, &gt;, &lt;, &gt;=, and &lt;=.For example, an expression of the form "A &gt; B &gt; C" would be translated into "C T(&gt;) B T(&gt;) A", where T(&gt;) represents the transformed version of the "&gt;" symbol.An example of an entry translated with Key 2 is provided below:
"y!z!{!;!\u2125\u000b\u22a3!)y!<em>!3!, !2<em>!+!)z!_!3!,!2</em>!+!){!</em>!3!,!2<em>!&gt;\u000b !!!!)y!,!z!,!{</em>!<em>!3!.!3!+!)y!+!z!,!z!+! {!,!{!+!y<em>!,!)y!+!z!,!z!+!{!,!{!+!y</em>!</em> !3!.!3!+!y!+!z!+!{!+!)y!,!z!,!{*!, \u000b!!!!!!!!y!<em>!3!+!z!</em>!3!+! !,\u000b!!!!!!2"|(3)
Evaluation Procedure: We conducted four finetuning runs on GPT-4o, keeping all hyperparameters constant, and evaluated five models (four finetuned and one base model with no fine-tuning) using 12 distinct datasets.These datasets are organized into two main categories:</p>
<p>• Seen Data: Six datasets were created by randomly selecting problems from the training set-three datasets containing 500 problems each in the original Lean format and three datasets with 500 problems each using the same translation key employed during finetuning.</p>
<p>• Unseen Data: To assess generalization, six additional datasets were formed by randomly selecting 200 problems each from the independent Mini-f2f dataset (Zheng et al., 2022).Like the seen data, these were split into two groups of three datasets: one in Lean and the other using the corresponding translated format.</p>
<p>Overall accuracy was computed by averaging the results across all testing sets, with accuracy defined as the number of correctly answered queries divided by the total number of queries in each set.</p>
<p>Results</p>
<p>Figure 1 displays the comparative performance of four fine-tuned GPT-4o models evaluated on both "seen" and "unseen" datasets.Specifically, models were fine-tuned with 25,214, 20,000, and 10,000 distinct queries using Translation Key 1, and with 25,214 queries using Translation Key 2. Additionally, Lean (untranslated) versions of both testing sets serve as benchmarks.</p>
<p>Our experiments demonstrate that GPT-4o exhibits superior problem-solving performance in our custom propositional language compared to Lean on average.On the "seen" dataset, GPT-4o achieved an average accuracy over all tests in of 95.97% in our propositional language versus 76.08% in Lean, with a small uncertainty of ± 0.33% and ± 0.36% respectively.</p>
<p>In contrast, on the "unseen" dataset, GPT-4o performed better when tested in Lean than in our custom language-attaining 99.89% accuracy with Lean compared to 97.56% with Translation Key 1 (± 0.06% and ± 0.44% respectively).As expected, Translation Key 2 yielded a substantially lower accuracy of 64.1% (± 0.75%) due to its arbitrary mapping.The model was fine-tuned solely on translated data, so it specializes in those patterns, resulting in high performance on seen translated examples but poor performance on seen Lean examples.For unseen data, it falls back on its broader pre-training, which helps it perform better on unseen Lean problems.</p>
<p>Additionally, our experiments indicate that GPT-4o solves problems more accurately with Translation Key 1 than with Translation Key 2, with average accuracies of 92.68% compared to 80.36% respectively-highlighting the importance of preserving logical relationships in the translation process.Table 1 provides a detailed summary of results from testing with Translation Key 1, and Table 3 provides a detailed summary of results from testing with Translation Key 2. Furthermore, training set size influenced performance.Increasing the training set from 10,000 to 20,000 samples improved accuracy by 2.7% on the "seen" dataset and by 0.3% on the "unseen" dataset, while further increases up to 25,214 samples did not yield additional gains.This suggests that the training set size threshold for stable performance lies below 20,000 samples.</p>
<p>For seen data in the custom translated format, the fine-tuned GPT-4o consistently achieves higher accuracy by specializing in the patterns and syntax introduced during fine-tuning, outperforming the base model.In contrast, on seen Lean data, the base GPT-4o retains its general Lean knowledge from pre-training and achieves similar results to the fine-tuned model.</p>
<p>When it comes to unseen data, the fine-tuned GPT-4o expectedly outperforms the base model on unseen translated examples.Table 4 provides a detailed summary of the results from testing using the base GPT-4o model.However, for unseen Lean data, the GPT-4o fine-tuned using Translation Key 2 performed significantly worse than its Translation Key 1 counterparts and also the base models.Focusing on Lean data (untranslated), all 4 finetuned models outperform the base models in both the unseen and seen data, except for the model finetuned in Translation Key 2 which showed worse comparative performance in the unseen lean data.</p>
<p>Tables 1, 3, and 4 provides a detailed summary of all dataset permutations and average performance metrics, shedding light on any potential anomalies.</p>
<p>Discussion</p>
<p>Our findings align with previous studies (Kojima et al., 2023;Wei et al., 2023), demonstrating that the accuracy of logical reasoning depends significantly on prompt formulation and task representation.The use of translation keys in our experiments illustrates that preserving inherent logical relationships-as in Translation Key 1-yields better performance than employing arbitrary mappings.This is analogous to natural language, where inverse or comparable relationships between symbols facilitate comprehension.</p>
<p>Our results also reveal a general trend where accuracy increases with training set size, echoing prior research that shows LLMs can perform well even with limited data (Brown et al., 2020).However, as shown in Figure 1, this trend is not strictly linear.There are occasions where smaller datasets outperformed larger datasets, such as the "seen" dataset in our propositional language having a 0.467% greater accuracy with 10000 samples compared to 20,000 samples.We attribute these fluctuations to certain factors, such as overfitting in larger training sets.Unlike earlier studies that evaluated existing models (liu et al., 2023), our approach using a custom propositional language uncovers unique aspects of pattern recognition in LLMs.</p>
<p>Notably, our analysis revealed that GPT-4o's performance on unseen data is better in Lean than it is in our custom language.We attribute this to GPT-4o's prior exposure to Lean-like syntax during pre-training Lean, as a formal proof assistant, shares structural similarities with theorem-proving and programming languages.In contrast, the custom language, especially under Translation Key 2, disrupted logical structure, thereby impeding generalization.This suggests that fine-tuning benefits significantly when the training data preserves logical consistency, aligning with the model's pretraining experience.This is further reinforced by the observation that models fine-tuned with Translation Key 1 performed better across all testing sets than those finetuned with Translation Key 2. Additionally, the fine-tuned models-especially those with Translation Key 1-consistently exhibited superior performance on both seen and unseen data, and this performance improved with larger training set sizes.This demonstrates GPT's ability to generalize logical information.The LLM extracted logical information from our custom language and used it to improve its logical accuracy in Lean.Notably, it performed better with Translation Key 1-which preserves logical relationships-than with Translation Key 2, which disrupts them.</p>
<p>While distinguishing between these effects is challenging, future work could explore fine-tuning an LLM with minimal exposure to Lean syntax to better understand the impact of pre-training familiarity compared to logical structure preservation.Comparing performance across runs provided insights into whether GPT-4o could robustly handle shifts in symbolic representation and how sensitive its performance is to different training configurations.</p>
<p>Our experiments indicate that GPT-4o's performance plateaus at around 20,000 training examples.This plateau may result from dataset redundancy, model capacity limitations, or the relative simplicity of the tasks.When the dataset contains many similar patterns, the model's exposure to novel challenges is limited, and once key patterns are internalized, additional training yields diminishing returns.</p>
<p>In summary, our findings suggest that GPT-4o can achieve high problem-solving accuracy in a propositional language when fine-tuned appropriately.The choice of translation key, dataset characteristics, and training set size must be managed carefully to mitigate overfitting and ensure robust generalization beyond seen patterns.</p>
<p>Conclusion</p>
<p>Our investigation confirms that fine-tuning GPT-4o on a custom propositional language not only facilitates high-level logical reasoning but also underscores the critical role of maintaining relational integrity within training data.Specifically, our work shows that using structured translation strategies significantly enhances model performance.This improvement is achieved by aligning the training data with the inherent logical patterns familiar from the model's pre-training, allowing GPT-4o to generalize more effectively, particularly when transitioning from seen to unseen examples.</p>
<p>Furthermore, our analysis highlights that an optimally balanced training set is essential: while increased dataset size improves performance up to a threshold (around 20,000 examples), additional data yields diminishing returns, suggesting the need for more efficient data utilization methods.These findings not only validate the importance of structured prompts and contextual cues but also offer practical guidelines for optimizing LLM training in both high-and low-resource language scenarios.</p>
<p>Collectively, our results contribute to a deeper understanding of how targeted data curation and translation methodologies can bolster logical reasoning in large language models.</p>
<p>Future Research</p>
<p>Future work should investigate dataset design principles.The high accuracy observed on our unseen dataset may reflect biases, such as overrepresentation of certain problem types or cultural premises, which should be systematically addressed.Synthetically balanced datasets that incorporate tiered complexity levels (e.g., single-step versus multi-step reasoning) could help disentangle superficial pattern recognition from genuine logical understanding.Additionally, although formatting differences (e.g., brackets versus colons) did not hinder performance in our study, systematic evaluations of robustness to syntactic variations are needed to better assess adaptability in low-resource settings.</p>
<p>A potential path to explore would be foregoing fine-tuning GPT-4o on our custom dataset and instead rely on in-context learning.Because GPT-4o may already have some familiarity with Lean from its pre-training, one could design a prompt that includes a few worked examples of Lean problems alongside a call to an external translator function that converts Lean input into the custom propositional language at inference time.Though this may yield lower accuracy than fine-tuning, it avoids the cost of creating and maintaining a large translation corpus.Evaluating GPT-4o in context can reveal how much of its Lean knowledge can be utilized through prompt engineering alone.</p>
<p>Further research should focus on optimizing translation strategies by developing principled approaches, such as semantic alignment of symbols, to enhance learnability.At the same time, exploring data efficiency methods is critical, as our observed performance plateau at approximately 20,000 training examples suggests that smarter data utilization may both reduce data requirements and improve systematicity.</p>
<p>Figure 1 :
1
Figure 1: Comparison of GPT-4o accuracy across datasets ("Seen" and "Unseen") using different translation keys and varying dataset sizes.</p>
<p>A Appendix
Limits for learning with language models. Nicholas Asher, Swarnadeep Bhar, Akshay Chaturvedi, Julie Hunter, Soumya Paul, arXiv:2306.122132023Preprint</p>
<p>Logical languages accepted by transformer encoders with hard attention. Pablo Barcelo, Alexander Kozachinskiy, Anthony Widjaja Lin, Vladimir Podolskii, arXiv:2310.038172023Preprint</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, 202012</p>
<p>Language models are few-shot learners. arXiv:2005.14165Preprint</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, arXiv:2205.097122022Preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162023Preprint</p>
<p>A closer look at logical reasoning with llms: The choice of tool matters. Long Hei, Matthew Lam, Ramya Keerthy Thatikonda, Ehsan Shareghi, arXiv:2406.002842024Preprint</p>
<p>Glore: Evaluating logical reasoning of large language models. Zhiyang Hanmeng Liu, Ruoxi Teng, Jian Ning, Qiji Liu, Yue Zhou, Zhang, arXiv:2310.091072023Preprint</p>
<p>Towards logiglue: A brief survey and a benchmark for analyzing logical reasoning capabilities of language models. Man Luo, Shrinidhi Kumbhar, Ming Shen, Mihir Parmar, Neeraj Varshney, Pratyay Banerjee, Somak Aditya, Chitta Baral, arXiv:2310.008362024Preprint</p>
<p>On grobner-shirshov bases for markov semirings. Xiaohui Niu, Wenxi Li, Zhongzhi Wang, arXiv:2401.057312024Preprint</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena, arXiv:2112.001142021Preprint</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.12295arXiv:2404.15522Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, and Chitta Baral. 2024. Logicbench: Towards systematic evaluation of logical reasoning ability of large language models. 2023Preprint</p>
<p>Generative language modeling for automated theorem proving. Stanislas Polu, Ilya Sutskever, arXiv:2009.033932020Preprint</p>
<p>Gemini: A family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, arXiv:2312.118052024PreprintAngeliki Lazaridou, and 1331 others</p>
<p>No language left behind: Scaling human-centered machine translation. Marta R Nllb Team, James Costa-Jussà, Onur Cross, Maha Çelebi, Kenneth Elbayad, Kevin Heafield, Elahe Heffernan, Janice Kalbassi, Daniel Lam, Jean Licht, Anna Maillard, Skyler Sun, Guillaume Wang, Al Wenzek, Bapi Youngblood, Loic Akula, Gabriel Mejia Barrault, Prangthip Gonzalez, Hansanti, arXiv:2207.04672202220Preprint</p>
<p>Wenyin Fu, and 49 others. 2023. Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, arXiv:2307.09288Preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032023Preprint</p>
<p>Faithful logical reasoning via symbolic chain-of-thought. Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, Wynne Hsu, arXiv:2405.183572024Preprint</p>
<p>Lean workbook: A large-scale lean problem set formalized from natural language math problems. Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, Kai Chen, arXiv:2406.038472024Preprint</p>
<p>Evaluating step-by-step reasoning through symbolic verification. Yi-Fan Zhang, Hanlin Zhang, Li Erran Li, Eric Xing, arXiv:2212.086862024Preprint</p>
<p>Minif2f: a cross-system benchmark for formal olympiad-level mathematics. Kunhao Zheng, Jesse Michael Han, Stanislas Polu, arXiv:2109.001102022Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
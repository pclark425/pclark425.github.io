<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8173 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8173</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8173</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-276929093</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.07604v3.pdf" target="_blank">Implicit Reasoning in Transformers is Reasoning through Shortcuts</a></p>
                <p><strong>Paper Abstract:</strong> Test-time compute is emerging as a new paradigm for enhancing language models' complex multi-step reasoning capabilities, as demonstrated by the success of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on unfixed-pattern data tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8173.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8173.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 (RoPE) trained-from-scratch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>12-layer GPT-2 with RoPE trained from scratch on synthetic multi-step modular arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 12-layer GPT-2 model (RoPE positional embeddings) trained from scratch on a synthetic dataset of 1–5 step modular (mod 23) addition/subtraction templates to study implicit multi-step arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (12-layer, RoPE) trained from scratch</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>12-layer GPT-2 architecture with rotary position embeddings (RoPE) used to learn length generalization; trained from scratch on synthetic multi-step modular addition/subtraction templates (operators +/−, operands mod 23); AdamW optimizer, LR=1e-4, batch=1600.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step modular (mod 23) sequential addition and subtraction (1–5 steps for training; eval includes 6–7 steps OOD).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>When trained on fixed sequential-premise patterns, the model implements a step-by-step internal computation: intermediate results are computed at the end of each step and stored in specific residual-stream regions that are read by later layers; attention carries intermediate-result information along a diagonal across layers/positions; MLPs amplify operator/operand features early and final-layer MLPs amplify output logits.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Activation patching (sliding-window 2×2 residual-stream patching across token positions and layers), attention-window masking (sliding window attention masks to restrict context), component-wise patching to decompose attention vs MLP contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Trained on fixed (forward) premise-order templates: reached ≈100% ID accuracy; OOD generalization: ≈99% accuracy for +1 step, ≈90% for +2 steps (paper wording: 'near-complete accuracy' on ID; '99%' for +1 step; 'nearly 90%' for +2 steps). When trained on unfixed (shuffled) premise-order data, accuracy degraded with step count (example numbers reported: forward-order test accuracies after training on unfixed-order data ~ 1-step:1.00, 2-step:0.87, 3-step:0.57, 4-step:0.43, 5-step:0.23).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>When trained on unfixed premise-order data, the model overfits to a shortcut that chains numbers (treating sequences as associative addition chains), failing strongly when variables appear as subtrahends; accuracy decreases as the number of variable-as-subtrahend equations increases; attention-window restriction to current step (window size=6) abolishes reasoning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Activation-patching residual-stream plots show concentrated patching effects at the end of each step propagating along a diagonal across layers/positions; result-fixed vs result-varied patching comparisons show diminished patching effect when intermediate result is identical (localizing intermediate-result storage); attention/MLP decomposition via patching shows attention modules in middle layers transfer intermediate information while MLPs in early/final layers amplify features.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>When premise order is shuffled in training, step-by-step pattern fails to emerge and the model resorts to chaining-number shortcuts; scaling data (more patterns or templates) and larger models (beyond medium) do not reliably fix failure; initializing from pretrained weights helps somewhat but does not eliminate the 'variable-as-subtrahend' failure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8173.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8173.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT2-RoPE-Medium-Pretrained</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT2-RoPE-Medium initialized from pretrained GPT-2 weights (RoPE positional embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A medium-sized GPT-2 variant initialized from pre-trained weights and using RoPE; used when analyzing mechanistic failures because training-from-scratch models had low accuracy on longer chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-RoPE-Medium (pretrained init)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Larger GPT-2 (medium) with RoPE, initialized from pretrained GPT-2 weights (paper uses this variant to reduce overfitting and for mechanistic analysis on 3-step problems); 1×1 patching used for mechanistic plots in Appendix G.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step modular arithmetic (3-step focused in analysis due to low accuracy on longer chains).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Shows same dichotomy: when no variables are subtrahends, model behavior localizes to output/numeric tokens (consistent with chaining-numbers shortcut); when variable is in subtrahend position, patching shows dependency on the variable token indicating the model must access variable value rather than chaining numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>1×1 activation patching (residual stream), position/operator combination analyses across different premise orders.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used for analysis; reported to still fail on many cases with variables as subtrahends, though absolute accuracy numbers in this specific ablation are not restated beyond qualitative failure on >3-step problems.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Fails on configurations with variables as subtrahends (mechanistic plots show need for variable token when subtrahend present); implication: pretrained-init improves some accuracies but does not escape 'Variable as Subtrahend Plight'.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Patching plots (Appendix G, Figure 16) show distinct pattern for 'number - variable' case with dark patching on variable token; other operator/variable combinations concentrate on output/numeric tokens indicating chaining.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Even with pretrained initialization and larger dataset scaling (many templates), models still fail when most variables are subtrahends; 1×1 patching sufficed to show mechanistic differences for 3-step problems but longer chains remain unresolved.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8173.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8173.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Step-by-step internal reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Step-by-step internal computation of intermediate results</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discovered internal computation pattern where the model computes each intermediate result at the end of its corresponding step and propagates that result to subsequent steps via attention/ residual-stream regions, effectively implementing implicit chain-of-thought without emitting tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 family models trained on fixed sequential-premise patterns</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Observed in 12-layer GPT-2 (RoPE) trained on fixed-sequential templates; pattern characterized by localized residual-stream regions at end of each step and diagonal propagation across layers/tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step modular addition/subtraction (sequential premises).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Representation: intermediate results stored in a localized region of the residual stream at end of each step; attention modules in middle layers transfer that stored result to deeper layers for the next step; MLPs amplify operator/operand features earlier and produce final-likelihood amplification later.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Activation patching (sliding-window 2×2), attention-window masking to block access to previous-step information (sliding window attention masks), result-fixed vs result-varied patching comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When this pattern emerges (fixed-order training), near-100% ID accuracy and strong OOD length generalization (≈99% for +1 step, ≈90% for +2 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>This pattern does not emerge when training data lacks fixed ordering; then the model does not reliably track variables and instead may use shortcuts; restricting attention to current step eliminates the model's ability to use stored intermediate results (accuracy collapses when window size limited to current-step tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Diagonal banding in residual-stream patching plots concentrated at step ends; diminished patching effect when intermediate result kept fixed across corrupted vs clean runs; rapid recovery of accuracy when attention-window includes previous step (window >6 tokens) versus collapse when limited to step-size window.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Fails to appear under unfixed/shuffled-premise training; cannot be induced by mere data scaling or larger model size in the paper's experiments; requires fixed-pattern training to reliably emerge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8173.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8173.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chaining-numbers shortcut</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number-chaining / commutativity shortcut</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Shortcut strategy where the model treats a multi-step expression as if all numeric terms can be chained together (e.g., c = 6 + 2 - 3 + 4) and thus avoids explicit variable-tracking; benefits from additions' commutativity and breaks when subtrahends are variables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 and SoTA LLMs trained/evaluated on unfixed-premise-order data</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Observed behavior across models trained or probed with unfixed premise-order data; models exploit addition commutativity and chain numeric literals while ignoring variable-dependency structure.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step mixed addition/subtraction where operands may be variables or numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Representation: model focuses on numeric tokens and output tokens; ignores variable-tracking representations when possible and effectively collapses expression into a flat numeric chain.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Activation patching shows darker effects localized to numeric and output tokens (not variable tokens) in non-variable-subtrahend cases; operator/position patching contrasts number-variable combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>High accuracy on problems without variable-as-subtrahend even under shuffled premise orders (e.g., forward cases with 0 variables-as-subtrahend often 1.00); but accuracy drops sharply as proportion of variable-as-subtrahend expressions increases (examples: GPT-4o accuracy drops from near 100% to ≈30% in relevant evaluations). Qwen2.5 reported forward 5-step accuracies (0/4..4/4) show drop from 1.00 to 0.40 when variables-as-subtrahend increase.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Fails when subtrahend is a variable (shortcut cannot chain terms), leading to 'Variable as Subtrahend Plight' with steep performance degradation as count of such equations increases; fails to generalize to OOD step counts that require genuine variable-tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Patching plots (Figures 16–17) show numeric/output token localization in non-variable-subtrahend cases and distinct dependency on variable tokens in 'number - variable' cases; accuracy trends plotted vs number of variable-subtrahend equations show sharp decline for shortcut-trained models but not for a true step-by-step simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>When training data enforces fixed sequential premises, models instead learn step-by-step pattern and do not rely on this shortcut; adding more random premise-order patterns or scaling templates does not reliably eliminate shortcut usage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8173.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8173.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Activation patching (sliding 2×2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Activation patching with sliding-window 2×2 residual-stream interventions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Intervention method that replaces a 2×2 region of activations (across current+next token and current+next layer) from a corrupted run into a clean run to measure causal influence on output logits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to GPT-2 models in this paper (and used to probe SoTA LLMs in evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Activation patching implementation follows Vig et al. (2020) with three forward passes (clean, corrupted, patched) and measures normalized logit changes of ground-truth tokens; window size chosen as 2×2 to capture rectangular critical regions.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to probe multi-step modular addition/subtraction tasks and to localize intermediate-result representations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Used to identify residual-stream regions where intermediate results are stored, to decompose contributions of attention vs MLP modules, and to contrast result-fixed vs result-varied settings.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Activation patching itself; variants include metrics (normalized change in clean-run ground-truth logit), sliding-window over layers/tokens, and single-layer (1×1) comparisons in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not an accuracy metric, but patching effect (PE) defined as normalized change in logit of correct token: PE = (Logit_cl(r) − Logit_pt(r)) / Logit_cl(r) (paper uses normalized metric); used to produce localization heatmaps where darker indicates larger causal impact.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Single-layer patching produced marginal effects; 2×2 window essential to reveal regional computations; larger windows inflate localization and were avoided.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Consistent diagonal banding and localization at end-of-step tokens in patching plots demonstrate intermediate-result storage and propagation; decomposition into attention vs MLP patching implicates attention in middle layers and MLPs in early/final layers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Different patching metrics produce similar flows but the authors justify their choice; identification depends on appropriate window size (2×2) — too small misses joint effects, too large may inflate plots.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8173.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8173.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention-window masking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sliding-window attention masking (restricting attention to local window)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intervention that limits each token's attention context to itself plus previous (window_size−1) tokens to test whether the model needs access to previous-step intermediate results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (12-layer RoPE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Model modified so attention scores outside the sliding window are set to −∞ before softmax, preventing access beyond the specified window.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>5-step multi-step modular arithmetic used to test dependence on previous-step information.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Used to show that the model stores intermediate results from the previous step and requires access to them: when window size limited to current step (size=6 tokens), reasoning ability collapses; accuracy recovers rapidly once window >6 to include previous step.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Hard attention-mask intervention applied during inference to ablate access to prior steps; accuracy measured across window sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy collapses to near-zero when attention window only covers the current step (window size = 6 tokens); accuracy recovers rapidly as window size expands to include previous step (exact recovery curve in Figure 4 of paper).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Shows that limited attention prevents stepwise reuse of intermediate results and thus abolishes implicit stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Rapid recovery of accuracy when attention window includes previous step supports hypothesis that intermediate results are stored and read across steps; combined with patching localization, indicates attention transfers intermediate results between steps.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>This ablation demonstrates necessity of cross-step attention for stepwise mechanism, but does not by itself show sufficiency for preventing shortcut behavior when training data encourages shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8173.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8173.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Variable-as-Subtrahend Plight</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variable as Subtrahend Plight (failure mode)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prominent failure mode where models trained on unfixed premise-order data over-rely on number-chaining shortcuts and fail when subtrahends are variables because chaining breaks and genuine variable-tracking is required.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Observed across GPT-2 variants and evaluated SoTA LLMs (GPT-4o, Claude-3.5, Qwen2.5, Llama-3 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Phenomenon arises when training data mixes premise orders (unfixed) so the model learns to ignore variable dependencies and instead chains numeric tokens; when subtrahends are variables the shortcut is invalid and accuracy drops steeply.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step modular addition/subtraction with varying counts of equations where the subtrahend is a variable rather than a numeric literal.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Indicates absence of reliable variable-tracking representation; patched localization shows models rely on numeric/output tokens and do not maintain variable-value representations in residual stream when using shortcut.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Measured via accuracy stratified by number of variable-as-subtrahend equations (Table 2, Table 4, Figure 6, Figure 7); activation patching shows different localization patterns for 'number - variable' operator/position combos.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>For GPT-2 trained on unfixed-order: accuracies drop with step count (example forward-order test: 1-step 1.00, 2-step 0.87, 3-step 0.57, 4-step 0.43, 5-step 0.23). Qwen2.5 (1.5B base) 5-step forward accuracies by #variables-as-subtrahend (0/4..4/4): 1.00, 0.98, 0.79, 0.62, 0.40 respectively. GPT-4o on 3-step natural-language formatted problems (Table 8): forward order accuracies for 0/2,1/2,2/2 variables-as-subtrahend = 0.94, 0.47, 0.28 respectively; Claude-3.5 forward = 0.98, 0.79, 0.35.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Systematic degradation as count of variable-as-subtrahend equations increases; near-complete failure when almost all variables are subtrahends; this effect persists across premise orders (forward/reverse/shuffled) and across different model sizes/architectures tested.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Accuracy-vs-#variable-as-subtrahend plots (Figures 6,7) and patching localization (Appendix G Figures 16–18) showing dependence on variable-token when subtrahend present; randomized-order training ablations and dataset-scaling experiments show persistent failure.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>When models are trained on fixed sequential premises, they avoid this plight by learning stepwise internal computation; scaling data volume, model size, or including more premise orders in training did not reliably eliminate the plight in the experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8173.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8173.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-2024-08-06 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source state-of-the-art LLM evaluated zero-shot on 3-step synthetic arithmetic tasks without chain-of-thought; performance degrades sharply as variable-as-subtrahend equations increase.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-2024-08-06</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4o system card (closed-source SOTA); used zero-shot with temperature 0 and prompts that instruct direct answer (no CoT) on 3-step modular arithmetic problems (natural-language and formula forms).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>3-step modular addition/subtraction problems with varying numbers of variable-as-subtrahend equations; evaluations included forward/reverse/shuffled premise orders.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Behavior consistent with reliance on shortcuts (number chaining) rather than robust internal variable-tracking for implicit reasoning; exact internal representation not probed mechanistically in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Zero-shot evaluation with controlled prompts disallowing CoT and stratified test sets varying ratio of variable-as-subtrahend equations and premise order; no internal interventions possible for closed-source model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 8 (3-step natural-language): forward-order accuracies by #variable-as-subtrahend (0/2,1/2,2/2) = 0.94, 0.47, 0.28 respectively; reverse/shuffled orders similar degrading patterns; text: 'GPT-4o accuracy even drops from nearly 100% to approximately 30%'.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Fails when multiple equations have variable subtrahends (two in 3-step setting), with accuracy approaching chance for worst-case ratios; less affected by premise order than by variable-as-subtrahend proportion.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical accuracy degradation as variable-as-subtrahend proportion increases; comparison versus step-count OOD shows that these failure modes are not solely due to longer chains but to variable-subtrahend structure.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>GPT-4o can still solve long forward-order chains without variable-as-subtrahend (paper notes GPT-4o accuracy on 9-step forward-order problems can exceed accuracy on 3-step problems containing two variable-as-subtrahend equations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8173.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8173.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.5-sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3.5-sonnet-20241022-v2 (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source large model evaluated zero-shot on the same 3-step tasks; shows similar degradation with variable-as-subtrahend equations though slightly higher robustness than GPT-4o in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.5-sonnet-20241022-v2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic Claude-3.5-sonnet variant evaluated zero-shot with temperature 0 and direct-answer instruction; evaluated on 3-step modular arithmetic (natural-language form) across premise orders.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>3-step modular addition/subtraction with variable-as-subtrahend stratification.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Empirical behavior matches shortcut reliance; internal mechanics not probed due to closed-source nature.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Zero-shot prompt-controlled evaluation (no CoT), stratified dataset; direct-answer enforcement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 8 forward-order accuracies for 0/2,1/2,2/2 = 0.98, 0.79, 0.35 respectively; reverse/shuffled orders show degraded performance in some orderings.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Performance drops substantially as variable-as-subtrahend proportion increases; fails on 3-step problems with two variable subtrahends in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Observed accuracy trends across stratified tests mirror those of GPT-4o and the trained GPT-2 models trained on unfixed patterns, supporting shortcut hypothesis across architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Despite somewhat better robustness on some ratios, still fails when multiple subtrahends are variables; premise order less influential than presence of variable-subtrahends.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8173.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8173.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-72B / Qwen2.5-1.5B results</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5 family (evaluated versions: 72B-instruct referenced in setup; 1.5B-Base in Table 6)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source Qwen2.5 variants evaluated; reported numerical accuracies for multi-step (5-step) tests show degradation as variables-as-subtrahend increase, consistent with shortcut behavior but with some variance across orders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5 (evaluated variants including 1.5B-Base and larger instruct variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen2.5 family; paper reports results for Qwen2.5-1.5B-Base (Table 6) on 5-step problems stratified by #variables-as-subtrahend and premise order (forward/reverse/random).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>5-step modular arithmetic (paper uses these to illustrate effect across architectures).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Observed behavior consistent with chaining-numbers shortcut when variables are not subtrahends; patching analysis not performed for Qwen (open-source variant used for accuracy tables).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Zero-shot evaluation / accuracy measurement stratified by variable-subtrahend count and premise order.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 6 (Qwen2.5-1.5B-Base, forward row for 5-step problems with #variables-as-subtrahend 0/4..4/4): 1.00, 0.98, 0.79, 0.62, 0.40. Reverse/random orders reported in same table with different drops.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Accuracy declines steadily as more equations contain a variable subtrahend; larger failure rates for shuffled/random orders in some cases; open-source models lag behind closed-source in implicit reasoning overall per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical accuracy tables show quantitative degradation consistent with shortcut reliance and failure when variable-subtrahend proportion increases.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Performance is somewhat better on reverse order for some subtrahend counts (Table 6), indicating architecture/dataset interactions; but overall the plight remains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8173.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8173.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention vs MLP decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Component decomposition: attention transfers intermediate results; MLPs enhance features</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mechanistic claim that attention modules in middle layers mediate stepwise propagation of intermediate results while MLP modules in early and late layers amplify operator/operand features and final logits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (12-layer RoPE) analyses</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Derived by decomposing activation-patching effects by component (attention vs MLP) across layers and positions to identify causal contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step modular arithmetic (used to understand component roles in stepwise implicit reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Attention modules in middle layers are primary causal pathway for propagating stored intermediate results between steps; early-layer MLPs amplify input/operator features; final-layer MLPs amplify correct-output logits.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Activation patching applied separately to attention outputs and MLP outputs; sliding-window patching plotted for each component across layers/positions (Figures 5b/5e for attention and 5c/5f for MLPs referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not a direct accuracy metric; decomposition produced distinct spatial/temporal localization of causal effects aligned with step boundaries: attention patching peaks in middle layers along diagonal, MLPs show peaks in early/final layers.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Even with this separation of roles, when training encourages shortcuts the attention/MLP pattern aligns with chaining-numbers behavior (attention/MLP focus numeric/output tokens) rather than variable-tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Component-wise patching plots show attention modules in middle layers produce the largest patching effect along the inter-step diagonal, whereas MLP interventions have stronger effects in early and final layers consistent with feature amplification.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Decomposition is correlational via patching; while it localizes roles, it does not by itself produce a full algorithmic proof; results depend on patching window size and experimental choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Activation patching: Investigating gender bias in language models using causal mediation analysis <em>(Rating: 1)</em></li>
                <li>Investigating multi-hop factual shortcuts in knowledge editing of large language models <em>(Rating: 2)</em></li>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>Arithmetic without algorithms: Language models solve math with a bag of heuristics <em>(Rating: 2)</em></li>
                <li>Do large language models latently perform multi-hop reasoning without exploiting shortcuts? <em>(Rating: 2)</em></li>
                <li>How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8173",
    "paper_id": "paper-276929093",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "GPT-2 (RoPE) trained-from-scratch",
            "name_full": "12-layer GPT-2 with RoPE trained from scratch on synthetic multi-step modular arithmetic",
            "brief_description": "A 12-layer GPT-2 model (RoPE positional embeddings) trained from scratch on a synthetic dataset of 1–5 step modular (mod 23) addition/subtraction templates to study implicit multi-step arithmetic reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 (12-layer, RoPE) trained from scratch",
            "model_description": "12-layer GPT-2 architecture with rotary position embeddings (RoPE) used to learn length generalization; trained from scratch on synthetic multi-step modular addition/subtraction templates (operators +/−, operands mod 23); AdamW optimizer, LR=1e-4, batch=1600.",
            "arithmetic_task_type": "Multi-step modular (mod 23) sequential addition and subtraction (1–5 steps for training; eval includes 6–7 steps OOD).",
            "mechanism_or_representation": "When trained on fixed sequential-premise patterns, the model implements a step-by-step internal computation: intermediate results are computed at the end of each step and stored in specific residual-stream regions that are read by later layers; attention carries intermediate-result information along a diagonal across layers/positions; MLPs amplify operator/operand features early and final-layer MLPs amplify output logits.",
            "probing_or_intervention_method": "Activation patching (sliding-window 2×2 residual-stream patching across token positions and layers), attention-window masking (sliding window attention masks to restrict context), component-wise patching to decompose attention vs MLP contributions.",
            "performance_metrics": "Trained on fixed (forward) premise-order templates: reached ≈100% ID accuracy; OOD generalization: ≈99% accuracy for +1 step, ≈90% for +2 steps (paper wording: 'near-complete accuracy' on ID; '99%' for +1 step; 'nearly 90%' for +2 steps). When trained on unfixed (shuffled) premise-order data, accuracy degraded with step count (example numbers reported: forward-order test accuracies after training on unfixed-order data ~ 1-step:1.00, 2-step:0.87, 3-step:0.57, 4-step:0.43, 5-step:0.23).",
            "error_types_or_failure_modes": "When trained on unfixed premise-order data, the model overfits to a shortcut that chains numbers (treating sequences as associative addition chains), failing strongly when variables appear as subtrahends; accuracy decreases as the number of variable-as-subtrahend equations increases; attention-window restriction to current step (window size=6) abolishes reasoning ability.",
            "evidence_for_mechanism": "Activation-patching residual-stream plots show concentrated patching effects at the end of each step propagating along a diagonal across layers/positions; result-fixed vs result-varied patching comparisons show diminished patching effect when intermediate result is identical (localizing intermediate-result storage); attention/MLP decomposition via patching shows attention modules in middle layers transfer intermediate information while MLPs in early/final layers amplify features.",
            "counterexamples_or_challenges": "When premise order is shuffled in training, step-by-step pattern fails to emerge and the model resorts to chaining-number shortcuts; scaling data (more patterns or templates) and larger models (beyond medium) do not reliably fix failure; initializing from pretrained weights helps somewhat but does not eliminate the 'variable-as-subtrahend' failure.",
            "uuid": "e8173.0",
            "source_info": {
                "paper_title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GPT2-RoPE-Medium-Pretrained",
            "name_full": "GPT2-RoPE-Medium initialized from pretrained GPT-2 weights (RoPE positional embeddings)",
            "brief_description": "A medium-sized GPT-2 variant initialized from pre-trained weights and using RoPE; used when analyzing mechanistic failures because training-from-scratch models had low accuracy on longer chains.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT2-RoPE-Medium (pretrained init)",
            "model_description": "Larger GPT-2 (medium) with RoPE, initialized from pretrained GPT-2 weights (paper uses this variant to reduce overfitting and for mechanistic analysis on 3-step problems); 1×1 patching used for mechanistic plots in Appendix G.",
            "arithmetic_task_type": "Multi-step modular arithmetic (3-step focused in analysis due to low accuracy on longer chains).",
            "mechanism_or_representation": "Shows same dichotomy: when no variables are subtrahends, model behavior localizes to output/numeric tokens (consistent with chaining-numbers shortcut); when variable is in subtrahend position, patching shows dependency on the variable token indicating the model must access variable value rather than chaining numbers.",
            "probing_or_intervention_method": "1×1 activation patching (residual stream), position/operator combination analyses across different premise orders.",
            "performance_metrics": "Used for analysis; reported to still fail on many cases with variables as subtrahends, though absolute accuracy numbers in this specific ablation are not restated beyond qualitative failure on &gt;3-step problems.",
            "error_types_or_failure_modes": "Fails on configurations with variables as subtrahends (mechanistic plots show need for variable token when subtrahend present); implication: pretrained-init improves some accuracies but does not escape 'Variable as Subtrahend Plight'.",
            "evidence_for_mechanism": "Patching plots (Appendix G, Figure 16) show distinct pattern for 'number - variable' case with dark patching on variable token; other operator/variable combinations concentrate on output/numeric tokens indicating chaining.",
            "counterexamples_or_challenges": "Even with pretrained initialization and larger dataset scaling (many templates), models still fail when most variables are subtrahends; 1×1 patching sufficed to show mechanistic differences for 3-step problems but longer chains remain unresolved.",
            "uuid": "e8173.1",
            "source_info": {
                "paper_title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Step-by-step internal reasoning",
            "name_full": "Step-by-step internal computation of intermediate results",
            "brief_description": "A discovered internal computation pattern where the model computes each intermediate result at the end of its corresponding step and propagates that result to subsequent steps via attention/ residual-stream regions, effectively implementing implicit chain-of-thought without emitting tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 family models trained on fixed sequential-premise patterns",
            "model_description": "Observed in 12-layer GPT-2 (RoPE) trained on fixed-sequential templates; pattern characterized by localized residual-stream regions at end of each step and diagonal propagation across layers/tokens.",
            "arithmetic_task_type": "Multi-step modular addition/subtraction (sequential premises).",
            "mechanism_or_representation": "Representation: intermediate results stored in a localized region of the residual stream at end of each step; attention modules in middle layers transfer that stored result to deeper layers for the next step; MLPs amplify operator/operand features earlier and produce final-likelihood amplification later.",
            "probing_or_intervention_method": "Activation patching (sliding-window 2×2), attention-window masking to block access to previous-step information (sliding window attention masks), result-fixed vs result-varied patching comparisons.",
            "performance_metrics": "When this pattern emerges (fixed-order training), near-100% ID accuracy and strong OOD length generalization (≈99% for +1 step, ≈90% for +2 steps).",
            "error_types_or_failure_modes": "This pattern does not emerge when training data lacks fixed ordering; then the model does not reliably track variables and instead may use shortcuts; restricting attention to current step eliminates the model's ability to use stored intermediate results (accuracy collapses when window size limited to current-step tokens).",
            "evidence_for_mechanism": "Diagonal banding in residual-stream patching plots concentrated at step ends; diminished patching effect when intermediate result kept fixed across corrupted vs clean runs; rapid recovery of accuracy when attention-window includes previous step (window &gt;6 tokens) versus collapse when limited to step-size window.",
            "counterexamples_or_challenges": "Fails to appear under unfixed/shuffled-premise training; cannot be induced by mere data scaling or larger model size in the paper's experiments; requires fixed-pattern training to reliably emerge.",
            "uuid": "e8173.2",
            "source_info": {
                "paper_title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Chaining-numbers shortcut",
            "name_full": "Number-chaining / commutativity shortcut",
            "brief_description": "Shortcut strategy where the model treats a multi-step expression as if all numeric terms can be chained together (e.g., c = 6 + 2 - 3 + 4) and thus avoids explicit variable-tracking; benefits from additions' commutativity and breaks when subtrahends are variables.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 and SoTA LLMs trained/evaluated on unfixed-premise-order data",
            "model_description": "Observed behavior across models trained or probed with unfixed premise-order data; models exploit addition commutativity and chain numeric literals while ignoring variable-dependency structure.",
            "arithmetic_task_type": "Multi-step mixed addition/subtraction where operands may be variables or numbers.",
            "mechanism_or_representation": "Representation: model focuses on numeric tokens and output tokens; ignores variable-tracking representations when possible and effectively collapses expression into a flat numeric chain.",
            "probing_or_intervention_method": "Activation patching shows darker effects localized to numeric and output tokens (not variable tokens) in non-variable-subtrahend cases; operator/position patching contrasts number-variable combinations.",
            "performance_metrics": "High accuracy on problems without variable-as-subtrahend even under shuffled premise orders (e.g., forward cases with 0 variables-as-subtrahend often 1.00); but accuracy drops sharply as proportion of variable-as-subtrahend expressions increases (examples: GPT-4o accuracy drops from near 100% to ≈30% in relevant evaluations). Qwen2.5 reported forward 5-step accuracies (0/4..4/4) show drop from 1.00 to 0.40 when variables-as-subtrahend increase.",
            "error_types_or_failure_modes": "Fails when subtrahend is a variable (shortcut cannot chain terms), leading to 'Variable as Subtrahend Plight' with steep performance degradation as count of such equations increases; fails to generalize to OOD step counts that require genuine variable-tracking.",
            "evidence_for_mechanism": "Patching plots (Figures 16–17) show numeric/output token localization in non-variable-subtrahend cases and distinct dependency on variable tokens in 'number - variable' cases; accuracy trends plotted vs number of variable-subtrahend equations show sharp decline for shortcut-trained models but not for a true step-by-step simulator.",
            "counterexamples_or_challenges": "When training data enforces fixed sequential premises, models instead learn step-by-step pattern and do not rely on this shortcut; adding more random premise-order patterns or scaling templates does not reliably eliminate shortcut usage.",
            "uuid": "e8173.3",
            "source_info": {
                "paper_title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Activation patching (sliding 2×2)",
            "name_full": "Activation patching with sliding-window 2×2 residual-stream interventions",
            "brief_description": "Intervention method that replaces a 2×2 region of activations (across current+next token and current+next layer) from a corrupted run into a clean run to measure causal influence on output logits.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to GPT-2 models in this paper (and used to probe SoTA LLMs in evaluation)",
            "model_description": "Activation patching implementation follows Vig et al. (2020) with three forward passes (clean, corrupted, patched) and measures normalized logit changes of ground-truth tokens; window size chosen as 2×2 to capture rectangular critical regions.",
            "arithmetic_task_type": "Used to probe multi-step modular addition/subtraction tasks and to localize intermediate-result representations.",
            "mechanism_or_representation": "Used to identify residual-stream regions where intermediate results are stored, to decompose contributions of attention vs MLP modules, and to contrast result-fixed vs result-varied settings.",
            "probing_or_intervention_method": "Activation patching itself; variants include metrics (normalized change in clean-run ground-truth logit), sliding-window over layers/tokens, and single-layer (1×1) comparisons in appendices.",
            "performance_metrics": "Not an accuracy metric, but patching effect (PE) defined as normalized change in logit of correct token: PE = (Logit_cl(r) − Logit_pt(r)) / Logit_cl(r) (paper uses normalized metric); used to produce localization heatmaps where darker indicates larger causal impact.",
            "error_types_or_failure_modes": "Single-layer patching produced marginal effects; 2×2 window essential to reveal regional computations; larger windows inflate localization and were avoided.",
            "evidence_for_mechanism": "Consistent diagonal banding and localization at end-of-step tokens in patching plots demonstrate intermediate-result storage and propagation; decomposition into attention vs MLP patching implicates attention in middle layers and MLPs in early/final layers.",
            "counterexamples_or_challenges": "Different patching metrics produce similar flows but the authors justify their choice; identification depends on appropriate window size (2×2) — too small misses joint effects, too large may inflate plots.",
            "uuid": "e8173.4",
            "source_info": {
                "paper_title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Attention-window masking",
            "name_full": "Sliding-window attention masking (restricting attention to local window)",
            "brief_description": "An intervention that limits each token's attention context to itself plus previous (window_size−1) tokens to test whether the model needs access to previous-step intermediate results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (12-layer RoPE)",
            "model_description": "Model modified so attention scores outside the sliding window are set to −∞ before softmax, preventing access beyond the specified window.",
            "arithmetic_task_type": "5-step multi-step modular arithmetic used to test dependence on previous-step information.",
            "mechanism_or_representation": "Used to show that the model stores intermediate results from the previous step and requires access to them: when window size limited to current step (size=6 tokens), reasoning ability collapses; accuracy recovers rapidly once window &gt;6 to include previous step.",
            "probing_or_intervention_method": "Hard attention-mask intervention applied during inference to ablate access to prior steps; accuracy measured across window sizes.",
            "performance_metrics": "Accuracy collapses to near-zero when attention window only covers the current step (window size = 6 tokens); accuracy recovers rapidly as window size expands to include previous step (exact recovery curve in Figure 4 of paper).",
            "error_types_or_failure_modes": "Shows that limited attention prevents stepwise reuse of intermediate results and thus abolishes implicit stepwise reasoning.",
            "evidence_for_mechanism": "Rapid recovery of accuracy when attention window includes previous step supports hypothesis that intermediate results are stored and read across steps; combined with patching localization, indicates attention transfers intermediate results between steps.",
            "counterexamples_or_challenges": "This ablation demonstrates necessity of cross-step attention for stepwise mechanism, but does not by itself show sufficiency for preventing shortcut behavior when training data encourages shortcuts.",
            "uuid": "e8173.5",
            "source_info": {
                "paper_title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Variable-as-Subtrahend Plight",
            "name_full": "Variable as Subtrahend Plight (failure mode)",
            "brief_description": "A prominent failure mode where models trained on unfixed premise-order data over-rely on number-chaining shortcuts and fail when subtrahends are variables because chaining breaks and genuine variable-tracking is required.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Observed across GPT-2 variants and evaluated SoTA LLMs (GPT-4o, Claude-3.5, Qwen2.5, Llama-3 variants)",
            "model_description": "Phenomenon arises when training data mixes premise orders (unfixed) so the model learns to ignore variable dependencies and instead chains numeric tokens; when subtrahends are variables the shortcut is invalid and accuracy drops steeply.",
            "arithmetic_task_type": "Multi-step modular addition/subtraction with varying counts of equations where the subtrahend is a variable rather than a numeric literal.",
            "mechanism_or_representation": "Indicates absence of reliable variable-tracking representation; patched localization shows models rely on numeric/output tokens and do not maintain variable-value representations in residual stream when using shortcut.",
            "probing_or_intervention_method": "Measured via accuracy stratified by number of variable-as-subtrahend equations (Table 2, Table 4, Figure 6, Figure 7); activation patching shows different localization patterns for 'number - variable' operator/position combos.",
            "performance_metrics": "For GPT-2 trained on unfixed-order: accuracies drop with step count (example forward-order test: 1-step 1.00, 2-step 0.87, 3-step 0.57, 4-step 0.43, 5-step 0.23). Qwen2.5 (1.5B base) 5-step forward accuracies by #variables-as-subtrahend (0/4..4/4): 1.00, 0.98, 0.79, 0.62, 0.40 respectively. GPT-4o on 3-step natural-language formatted problems (Table 8): forward order accuracies for 0/2,1/2,2/2 variables-as-subtrahend = 0.94, 0.47, 0.28 respectively; Claude-3.5 forward = 0.98, 0.79, 0.35.",
            "error_types_or_failure_modes": "Systematic degradation as count of variable-as-subtrahend equations increases; near-complete failure when almost all variables are subtrahends; this effect persists across premise orders (forward/reverse/shuffled) and across different model sizes/architectures tested.",
            "evidence_for_mechanism": "Accuracy-vs-#variable-as-subtrahend plots (Figures 6,7) and patching localization (Appendix G Figures 16–18) showing dependence on variable-token when subtrahend present; randomized-order training ablations and dataset-scaling experiments show persistent failure.",
            "counterexamples_or_challenges": "When models are trained on fixed sequential premises, they avoid this plight by learning stepwise internal computation; scaling data volume, model size, or including more premise orders in training did not reliably eliminate the plight in the experiments reported.",
            "uuid": "e8173.6",
            "source_info": {
                "paper_title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GPT-4o (evaluated)",
            "name_full": "GPT-4o-2024-08-06 (OpenAI)",
            "brief_description": "A closed-source state-of-the-art LLM evaluated zero-shot on 3-step synthetic arithmetic tasks without chain-of-thought; performance degrades sharply as variable-as-subtrahend equations increase.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o-2024-08-06",
            "model_description": "OpenAI GPT-4o system card (closed-source SOTA); used zero-shot with temperature 0 and prompts that instruct direct answer (no CoT) on 3-step modular arithmetic problems (natural-language and formula forms).",
            "arithmetic_task_type": "3-step modular addition/subtraction problems with varying numbers of variable-as-subtrahend equations; evaluations included forward/reverse/shuffled premise orders.",
            "mechanism_or_representation": "Behavior consistent with reliance on shortcuts (number chaining) rather than robust internal variable-tracking for implicit reasoning; exact internal representation not probed mechanistically in paper.",
            "probing_or_intervention_method": "Zero-shot evaluation with controlled prompts disallowing CoT and stratified test sets varying ratio of variable-as-subtrahend equations and premise order; no internal interventions possible for closed-source model.",
            "performance_metrics": "Table 8 (3-step natural-language): forward-order accuracies by #variable-as-subtrahend (0/2,1/2,2/2) = 0.94, 0.47, 0.28 respectively; reverse/shuffled orders similar degrading patterns; text: 'GPT-4o accuracy even drops from nearly 100% to approximately 30%'.",
            "error_types_or_failure_modes": "Fails when multiple equations have variable subtrahends (two in 3-step setting), with accuracy approaching chance for worst-case ratios; less affected by premise order than by variable-as-subtrahend proportion.",
            "evidence_for_mechanism": "Empirical accuracy degradation as variable-as-subtrahend proportion increases; comparison versus step-count OOD shows that these failure modes are not solely due to longer chains but to variable-subtrahend structure.",
            "counterexamples_or_challenges": "GPT-4o can still solve long forward-order chains without variable-as-subtrahend (paper notes GPT-4o accuracy on 9-step forward-order problems can exceed accuracy on 3-step problems containing two variable-as-subtrahend equations).",
            "uuid": "e8173.7",
            "source_info": {
                "paper_title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Claude-3.5-sonnet",
            "name_full": "Claude-3.5-sonnet-20241022-v2 (Anthropic)",
            "brief_description": "A closed-source large model evaluated zero-shot on the same 3-step tasks; shows similar degradation with variable-as-subtrahend equations though slightly higher robustness than GPT-4o in some settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3.5-sonnet-20241022-v2",
            "model_description": "Anthropic Claude-3.5-sonnet variant evaluated zero-shot with temperature 0 and direct-answer instruction; evaluated on 3-step modular arithmetic (natural-language form) across premise orders.",
            "arithmetic_task_type": "3-step modular addition/subtraction with variable-as-subtrahend stratification.",
            "mechanism_or_representation": "Empirical behavior matches shortcut reliance; internal mechanics not probed due to closed-source nature.",
            "probing_or_intervention_method": "Zero-shot prompt-controlled evaluation (no CoT), stratified dataset; direct-answer enforcement.",
            "performance_metrics": "Table 8 forward-order accuracies for 0/2,1/2,2/2 = 0.98, 0.79, 0.35 respectively; reverse/shuffled orders show degraded performance in some orderings.",
            "error_types_or_failure_modes": "Performance drops substantially as variable-as-subtrahend proportion increases; fails on 3-step problems with two variable subtrahends in many cases.",
            "evidence_for_mechanism": "Observed accuracy trends across stratified tests mirror those of GPT-4o and the trained GPT-2 models trained on unfixed patterns, supporting shortcut hypothesis across architectures.",
            "counterexamples_or_challenges": "Despite somewhat better robustness on some ratios, still fails when multiple subtrahends are variables; premise order less influential than presence of variable-subtrahends.",
            "uuid": "e8173.8",
            "source_info": {
                "paper_title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Qwen2.5-72B / Qwen2.5-1.5B results",
            "name_full": "Qwen2.5 family (evaluated versions: 72B-instruct referenced in setup; 1.5B-Base in Table 6)",
            "brief_description": "Open-source Qwen2.5 variants evaluated; reported numerical accuracies for multi-step (5-step) tests show degradation as variables-as-subtrahend increase, consistent with shortcut behavior but with some variance across orders.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5 (evaluated variants including 1.5B-Base and larger instruct variants)",
            "model_description": "Qwen2.5 family; paper reports results for Qwen2.5-1.5B-Base (Table 6) on 5-step problems stratified by #variables-as-subtrahend and premise order (forward/reverse/random).",
            "arithmetic_task_type": "5-step modular arithmetic (paper uses these to illustrate effect across architectures).",
            "mechanism_or_representation": "Observed behavior consistent with chaining-numbers shortcut when variables are not subtrahends; patching analysis not performed for Qwen (open-source variant used for accuracy tables).",
            "probing_or_intervention_method": "Zero-shot evaluation / accuracy measurement stratified by variable-subtrahend count and premise order.",
            "performance_metrics": "Table 6 (Qwen2.5-1.5B-Base, forward row for 5-step problems with #variables-as-subtrahend 0/4..4/4): 1.00, 0.98, 0.79, 0.62, 0.40. Reverse/random orders reported in same table with different drops.",
            "error_types_or_failure_modes": "Accuracy declines steadily as more equations contain a variable subtrahend; larger failure rates for shuffled/random orders in some cases; open-source models lag behind closed-source in implicit reasoning overall per paper.",
            "evidence_for_mechanism": "Empirical accuracy tables show quantitative degradation consistent with shortcut reliance and failure when variable-subtrahend proportion increases.",
            "counterexamples_or_challenges": "Performance is somewhat better on reverse order for some subtrahend counts (Table 6), indicating architecture/dataset interactions; but overall the plight remains.",
            "uuid": "e8173.9",
            "source_info": {
                "paper_title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Attention vs MLP decomposition",
            "name_full": "Component decomposition: attention transfers intermediate results; MLPs enhance features",
            "brief_description": "Mechanistic claim that attention modules in middle layers mediate stepwise propagation of intermediate results while MLP modules in early and late layers amplify operator/operand features and final logits.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (12-layer RoPE) analyses",
            "model_description": "Derived by decomposing activation-patching effects by component (attention vs MLP) across layers and positions to identify causal contributions.",
            "arithmetic_task_type": "Multi-step modular arithmetic (used to understand component roles in stepwise implicit reasoning).",
            "mechanism_or_representation": "Attention modules in middle layers are primary causal pathway for propagating stored intermediate results between steps; early-layer MLPs amplify input/operator features; final-layer MLPs amplify correct-output logits.",
            "probing_or_intervention_method": "Activation patching applied separately to attention outputs and MLP outputs; sliding-window patching plotted for each component across layers/positions (Figures 5b/5e for attention and 5c/5f for MLPs referenced).",
            "performance_metrics": "Not a direct accuracy metric; decomposition produced distinct spatial/temporal localization of causal effects aligned with step boundaries: attention patching peaks in middle layers along diagonal, MLPs show peaks in early/final layers.",
            "error_types_or_failure_modes": "Even with this separation of roles, when training encourages shortcuts the attention/MLP pattern aligns with chaining-numbers behavior (attention/MLP focus numeric/output tokens) rather than variable-tracking.",
            "evidence_for_mechanism": "Component-wise patching plots show attention modules in middle layers produce the largest patching effect along the inter-step diagonal, whereas MLP interventions have stronger effects in early and final layers consistent with feature amplification.",
            "counterexamples_or_challenges": "Decomposition is correlational via patching; while it localizes roles, it does not by itself produce a full algorithmic proof; results depend on patching window size and experimental choices.",
            "uuid": "e8173.10",
            "source_info": {
                "paper_title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Activation patching: Investigating gender bias in language models using causal mediation analysis",
            "rating": 1,
            "sanitized_title": "activation_patching_investigating_gender_bias_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Investigating multi-hop factual shortcuts in knowledge editing of large language models",
            "rating": 2,
            "sanitized_title": "investigating_multihop_factual_shortcuts_in_knowledge_editing_of_large_language_models"
        },
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Arithmetic without algorithms: Language models solve math with a bag of heuristics",
            "rating": 2,
            "sanitized_title": "arithmetic_without_algorithms_language_models_solve_math_with_a_bag_of_heuristics"
        },
        {
            "paper_title": "Do large language models latently perform multi-hop reasoning without exploiting shortcuts?",
            "rating": 2,
            "sanitized_title": "do_large_language_models_latently_perform_multihop_reasoning_without_exploiting_shortcuts"
        },
        {
            "paper_title": "How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
            "rating": 1,
            "sanitized_title": "how_does_gpt2_compute_greaterthan_interpreting_mathematical_abilities_in_a_pretrained_language_model"
        }
    ],
    "cost": 0.020248250000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Implicit Reasoning in Transformers is Reasoning through Shortcuts
2 Jun 2025</p>
<p>Tianhe Lin 
School of Data Science
College of Computer Science and Artificial Intelligence
Fudan University
Fudan University</p>
<p>Jian Xie jianxie22@m.fudan.edu.cn 
School of Data Science
College of Computer Science and Artificial Intelligence
Fudan University
Fudan University</p>
<p>Siyu Yuan syyuan21@m.fudan.edu.cn 
School of Data Science
College of Computer Science and Artificial Intelligence
Fudan University
Fudan University</p>
<p>Deqing Yang yangdeqing@fudan.edu.cn 
School of Data Science
College of Computer Science and Artificial Intelligence
Fudan University
Fudan University</p>
<p>Implicit Reasoning in Transformers is Reasoning through Shortcuts
2 Jun 20253C6CC12F01759DFF24A89C91665D6F07arXiv:2503.07604v3[cs.CL]
Test-time compute is emerging as a new paradigm for enhancing language models' complex multi-step reasoning capabilities, as demonstrated by the success of OpenAI's o1 and o3, as well as DeepSeek's R1.Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens.However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style?In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks.Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning.However, this capability only emerges when trained on fixed-pattern data.2) Conversely, implicit reasoning abilities emerging from training on unfixed-pattern data tend to overfit a specific pattern and fail to generalize further.Notably, this limitation is also observed in state-of-the-art large language models.These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization.Resources are available on the GitHub.</p>
<p>Introduction</p>
<p>Chain-of-Thought (CoT; Wei et al. (2022)) has sparked the development of explicit reasoning in large language models (LLMs).The subsequent rise of large reasoning models (OpenAI, 2024b;Google, 2024;DeepSeek-AI, 2025) based on long CoT demonstrates impressive capabilities across various tasks (Rein et al., 2023;MAA, 2024;Jimenez et al., 2024).Recent works have shown that such reasoning capabilities can even be distilled into smaller models (DeepSeek-AI, 2025). 1ifferent from explicit reasoning, implicit reasoning offers greater inference efficiency by relying on fewer tokens to generate an answer (Deng et al., 2023).Yet, it falls short of the performance achieved by explicit reasoning (Deng et al., 2024;Allen-Zhu and Li, 2024).Why can't implicit reasoning develop advanced reasoning capabilities?</p>
<p>While recent advances in mechanistic interpretability have aimed to demystify the implicit reasoning processes of language models (LMs), most studies are limited to single-step reasoning (Meng et al., 2022;Wang et al., 2023;Nanda et al., 2023), which does not meet the expectation for handling complex reasoning tasks, such as advanced mathematical problems.Meanwhile, for multi-step implicit reasoning, previous work primarily focuses on reasoning over factual knowledge (Yang et al., 2024a;Biran et al., 2024), which may be hindered by issues such as inflated reasoning performance due to memorizing entity co-occurrences in the pretraining data (Elazar et al., 2023;Kang and Choi, 2023;Ju et al., 2024).</p>
<p>In this paper, to minimize the impact of memorization and investigate the underlying reasoning mechanisms, we explore implicit reasoning through the lens of mathematical problems.Mathematical reasoning primarily depends on arithmetic operations that follow strict logical rules, which require algebraic manipulation based on specific operators and operands rather than recalling pre-trained knowledge like entity relationships.Given that the strength of explicit reasoning stems from stepwise rationales, the first question we seek to address is RQ1: Can language models perform stepwise reasoning internally?To investigate this, we train GPT-2 from scratch on our synthetic multi- Stepwise Reasoner m=16-5=11 z=11-m=0 b=z+22=22</p>
<p>Stepwise Reasoner or Figure 1: A failure of generalization in language models trained on data with unfixed patterns, namely "Variable as Subtrahend Plight".When trained on unfixed premise order, the model learns a reasoning shortcut that benefits from addition commutativity.This shortcut enables the model to perform implicit reasoning by chaining numbers, which fails when variables are subtrahends.</p>
<p>step dataset composed of sequential mathematical operations, which means premises are arranged in the same order as they appear in the actual step-bystep calculation process.The experimental results and activation patching (Vig et al., 2020;Meng et al., 2022) plots show that LMs can fully learn to do stepwise reasoning internally and generalize to problems with more steps, provided they are trained on data where all premises are presented sequentially.</p>
<p>However, the premises are not always presented sequentially in real-world reasoning tasks, requiring LMs to organize the information internally.Therefore, based on the findings from RQ1, this paper seeks to answer a more general research question, RQ2: How do language models think internally if the premise order is not fixed?In contrast to the accuracy saturation in RQ1, accuracy drops significantly when the premise order is unfixed.We conduct further analysis and find LMs fail to learn stepwise implicit reasoning when the premise order is not fixed, struggling with "Variable as Subtrahend Plight".Specifically, as shown in Figure 1, models trained on an unfixed premise order overfit to an easy pattern in the data, relying on a shortcut that benefits from addition commutativity.This shortcut allows the model to solve the problem by directly chaining numbers, while the presence of variables in the subtrahend position disrupts this shortcut.Additional mechanistic analysis validates our hypothesis.</p>
<p>Previous work demonstrated that even current state-of-the-art (SoTA) LLMs also struggle with implicit reasoning (Yu, 2024).Based on our previous findings, we aim to investigate RQ3: How do LLMs perform multi-step implicit reasoning?</p>
<p>We find "Variable as Subtrahend Plight" also persists in SoTA LLMs, indicating that these models, trained on diverse unfixed premise corpora, are also relying on shortcuts for multi-step implicit reasoning.This further validates the correctness and generalizability of our findings.</p>
<p>To summarize, in this paper, we investigate the internal mechanisms of implicit reasoning in transformers and uncover why the advanced reasoning capabilities observed in explicit reasoning do not emerge in implicit reasoning.While we reveal that current LMs primarily rely on shortcuts for implicit reasoning, a silver lining is that a stepwise reasoning pattern could indeed emerge through training.Such a pattern underpins the advanced reasoning capabilities of LMs, and we envision that future advanced strategies could help form this pattern.</p>
<p>Related Work</p>
<p>Mechanistic Interpretability of Language Models</p>
<p>Mechanistic interpretability (MI) aims to uncover and explain the internal workings of models.The research of mechanistic interpretability in language models primarily focuses on three key areas: features within model representations (nostalgebraist, 2020;Gurnee et al., 2023;Zhou et al., 2024), circuits connecting these features (Wang et al., 2023;Hanna et al., 2023;Prakash et al., 2024), and universality across diverse models and tasks (Chughtai et al., 2023;Gurnee et al., 2024).Mathematical tasks, due to their significance in representing the reasoning capabilities of language models, have been widely studied in MI (Hanna et al., 2023;Kudo et al., 2024;Zhou et al., 2024).However, most of the existing studies (Stolfo et al., 2023;Yu and Ananiadou, 2024;Zhang et al., 2024;Chen et al., 2024) focus on single-step mathematical reasoning.How LMs perform multi-step mathematical reasoning implicitly remains poorly understood.To bridge this gap, we employ activation patching (Vig et al., 2020) to track the information flow and reverse-engineer the behaviors of LMs in multi-step arithmetic computations.</p>
<p>Multi-step Implicit Reasoning</p>
<p>As opposed to explicit reasoning, implicit reasoning is performed in the hidden states instead of extra tokens.Previous studies typically investigate implicit reasoning in two domains: factual reason-ing (Wang et al., 2024;Yang et al., 2024a,b;Biran et al., 2024) and mathematical reasoning (Stolfo et al., 2023;Nanda et al., 2023;Deng et al., 2024).However, progress in reasoning over factual knowledge risks being inflated by entity co-occurrence learned from pre-training data (Elazar et al., 2023;Kang and Choi, 2023;Ju et al., 2024).While mathematical reasoning is less susceptible to this issue due to the variability of operands and operators, LMs may rely on shortcuts or shallow heuristics to predict the results (Liu et al., 2023;Nikankin et al., 2025;Xie et al., 2024), which are often overlooked in studies on multi-step implicit reasoning.In our study, we scrutinize the impact of shortcuts and represent the internal mechanisms driving the observed phenomenon to the investigation of the multi-step implicit mathematical reasoning abilities in Transformer-based LMs.</p>
<p>General Setup</p>
<p>Task.Focusing on reasoning capability rather than other factors (e.g., factual knowledge memorization), we use mathematical problems as a lens.To further minimize the impact of natural language complexity, we shift our focus to mathematical formulas rather than problem statements in natural language.Specifically, we construct a synthetic dataset of multi-step sequential modular addition and subtraction as our testbed for analysis.As shown in Figure 1, except for the first step, each step of the computation involves a variable from the previous step, a number (we name it operand later), and an operator (i.e., "+" or "−").Following Ye et al. (2024), we consider using arithmetics mod23 to avoid numbers being split into multiple tokens and prevent errors from large number calculations, thereby focusing on reasoning itself rather than calculation.</p>
<p>Data.</p>
<p>For training data, we generate different multi-step calculation templates for questions at each length (ranging from 1 to 5 steps) and then randomly use K different groups of variable names to instantiate each template.2To prevent LMs from memorizing intermediate results from the training set rather than performing actual reasoning to solve the math problems in our test set, we filter out all templates with preceding calculations, apart from the first step, that overlap with the templates of the training set during the test set generation process.</p>
<p>For example, if "f=1+2,s=3-f,s»?" appears in the training set, then "a=1+2,b=3-a,c=b+5,c»?" is not allowed to appear in the test set because the first two steps of the former are the same as the latter regardless of variable names.</p>
<p>We evaluate both in-distribution (ID) and outof-distribution (OOD) performance, which are distinguished by the maximum reasoning steps of the training set, with ID not exceeding the maximum steps of the training set (i.e., 5-step) and OOD being one or two steps more than the maximum steps of the training set (i.e., 6-step or 7-step).ID generalization aims to evaluate whether the model learns the latent rules of the training set, while OOD generalization is designed to assess whether the model genuinely acquires some reasoning skills.</p>
<p>Model &amp; Optimization.Following Ye et al. (2024), we use a standard 12-layer GPT-2 model (Radford et al., 2019) and replace its positional embeddings with rotary embeddings (RoPE) (Su et al., 2024) to enable the model to learn length generalization (i.e., to generalize its ability to solve more steps in multi-step reasoning tasks than those seen during training).We use AdamW (Loshchilov and Hutter, 2019) with learning rate 10 −4 , batch size 1600, weight decay 0.1 and 2000 warm-up steps.</p>
<p>Activation Patching.Activation patching (Vig et al., 2020;Meng et al., 2022) is a strategy for identifying the important modules that causally affect the output by intervening on their latent activations.Specifically, if a module is important, the alteration of its activation will significantly affect the model's output, whereas an unimportant one will have little to no impact.Typically, the method needs two inputs, an original one (e.g., "a=1+4,d=5a,c=1+d,c»?") and another with a slight difference (e.g., "a=6+4,d=5-a,c=1+d,c»?"), and three forward passes: The clean run and the corrupted run take the above two inputs separately and cache activations of the model's components, such as attention or MLP outputs.In the patched run, we run the model on the original input but replace the specific activation with the cached activation from the corrupted run.Following previous work (Zhang et al., 2024), we measure the changes in the output logits of the ground truth tokens.Then, we compute the patching effect (PE) as:
PE = Logit cl (r) − Logit pt (r) Logit cl (r) , (1)
where r is the correct answer of the original input and cl, pt denote the clean and patched run separately.The experiments are conducted on 100 randomly selected samples.We iterate activation patching over a set of activations and compare how much they affect the final output, which allows us to localize which activation matters and ultimately reverse-engineer the underlying circuit.In practice, we utilize sliding window patching (Hase et al., 2023) with window size 2×2, where at each token position, the representations of the 2 × 2 region formed by the current layer and the next layer, along with the current token and the next token, are substituted by the cached activations from the corrupted run. 3</p>
<p>Can Language Models Perform</p>
<p>Stepwise Reasoning Internally?</p>
<p>Previous work found that smaller LMs (∼7B) can hardly do multi-step mathematical reasoning correctly without CoT, while a 70B level model can only achieve an accuracy of about 50% in 4-hop reasoning (Yu, 2024).Since previous work demonstrated that externalizing reasoning step by step enhances performance in mathematical tasks (Wei et al., 2022), a question is: does the poor performance of implicit reasoning arise from the inability to employ this step-by-step reasoning style?We begin our investigation by training our GPT-2 model on the synthetic dataset to learn implicit reasoning.</p>
<p>Results</p>
<p>Language models are able to perform implicit mathematical reasoning with near-complete accuracy when trained.We first analyze whether our model is capable of solving multi-step implicit mathematical reasoning.Figure 2 shows the model's accuracy on both the ID and OOD test data throughout the optimization.The model not only achieves 100% accuracy on implicit reasoning tasks from the same distribution (ID set) but also generalizes effectively to tasks requiring longer reasoning steps in the OOD set.To be specific, the model achieves 99% accuracy in tasks that require an additional step of reasoning and nearly 90% accuracy in tasks that require two more.This implies that the model truly learns some implicit reasoning skills rather than simply memorizing answers, since our model has never seen any training example of the same length as in the test time.</p>
<p>The Working Mechanism of Model</p>
<p>Setting.To investigate whether the language model is based on understanding (i.e., gathering all the information together first and then computing) or reasoning step by step, we use activation patching to reveal the model's internal thought process.Two different experiment setups are used to reveal how the information is transmitted and what information is transmitted separately.</p>
<p>• Tracing the information flow.To gain insights into the working mechanisms of the model, we first need to know the path through which the token's information is transmitted to the output, i.e., how the information of a specific token affects the output.To this end, we change only one operand or operator in the original input and identify the activations that have an influence on the final output by replacing activations.</p>
<p>• Tracking result-related information.The first setting explains how information is transmitted to the output, yet what information is transmitted is still unclear.Therefore, we formulate a variant of the first setting to track the information related to intermediate results (i.e., the value of an intermediate variable).Specifically, we modify a set of operands and compare the differences in patching effects when the intermediate results are either identical or distinct.For example, if we aim to track the related information of "d" in "a=4+6,d=a+5,c=1+d,c»?",We need to modify the operands while keeping the value of "d" fixed at 15 (e.g., "a=4+1, d=a+10,c=1+d,c»?") and compare it with a case where the result changes (e.g., "a=4+1,d=a+4,c=1+d,c»?",where d=9).If the model is performing step-by-step reasoning, the patching effect will be more pronounced in the first two steps due to the change in "a" and will then diminish from the third step onward in the fixed-result setting, as the subsequent results remain unchanged.
Step1 Step2 Step3 Step4 Step5 Step6 Q 0 2
Language models are able to do step-by-step reasoning internally.To trace the information flow, we first examine the residual stream patching plot by only altering one operand. 4The patching effects across layers and positions are shown in Figure 3.We observe that a significant portion of the patching effects concentrate at the end of each step and exhibit a clear trend of gradually propagating along a diagonal line.This pattern forms the foundation of step-by-step reasoning, which implies that each intermediate result builds upon the last.</p>
<p>Based on the discovered information flow, we investigate the information behind these activations by tracking result-related information and adding constraints to ensure the results remain the same when changing the input.By comparing the results of the result-varied setting (Figure 5a) and the result-fixed setting (Figure 5d), we find: The region between Step 2 and Step 3, where the impact diminishes (highlighted by the green box in Figure 5d), aligns precisely with the segment between the second and third steps in the information flow (Figure 3).In the fixed-result setting, the substituted activations retain the same information, leading to a minor patching effect.However, in the unfixed setting, the patching effect is more pronounced.This provides evidence that this area stores information related to the intermediate results.</p>
<p>To further validate that the language model performs step-by-step reasoning by reusing intermediate results stored in a specific area from the last step, we conduct an additional experiment to examine its behavior when the information from previous steps is masked by varying the attention window size (see implementation details in Appendix D).Specifically, each step consists of 6 tokens, and when we scale up the attention window size to bigger than 6, the model is able to access the information stored from the previous step.We present the model's accuracy under different attention window sizes in Figure 4. Our findings show that when the attention is restricted to the current step (i.e., window size = 6), the model completely loses its reasoning ability.However, once the attention is expanded to include the previous step's results, accuracy recovers rapidly.This supports the hypothesis that the model follows a step-by-step reasoning pattern, as evidenced from a different perspective.</p>
<p>To sum up, the model computes the result of each step once it concludes, and this information is then utilized by the subsequent step in the next layers, establishing a step-by-step computation pattern.</p>
<p>Attention mechanism propagates intermediate results, and MLP modules enhance features related to inputs and outputs.Intervening on hidden states only provides us with a glimpse of the information flow, but the roles of various components within the model remain unclear.By decomposing the causal effects of contributions of attention and MLP modules (Figure 5b,5e and Figure 5c,5f), we find a decisive role for attention modules in the middle layers and MLPs in early and final layers.In conjunction with the findings on information flow, we infer that the attention layers are responsible for extracting the information needed in the current step and gradually transferring intermediate computational information to deeper layers.Therefore, a possible explanation of the model's behavior on this task is that the MLP modules enhance features of operators and operands in early layers, then the attention mechanism facilitates the step-by-step propagation of intermediate results, and finally, the MLP modules in the last layers enhance the probabilities of correct predictions.</p>
<p>How Do Language Models Reason</p>
<p>Internally When the Premise Order is Not Fixed?</p>
<p>Based on the above findings, we find that Transformers are able to perform step-by-step reasoning internally when the premise order is fixed.However, in complex reasoning tasks, the premises are not always presented sequentially; they may appear in a random order, requiring LMs to organize the information internally.Can language models still perform implicit reasoning step by step when the premises are shuffled?</p>
<p>Setup.For consistency, we continue to use the original data but randomly shuffle the order of premises 5 , excluding the question.To assess the impact of premise order, we study three different orders, including forward, reverse, and random.Specifically, for the reverse order, we list the premises in reverse order; for the random order, we shuffle the premises randomly.</p>
<p>Result</p>
<p>Language models fail to learn implicit reasoning when the premise order is not fixed.In contrast to the high accuracy scores achieved by the model trained on fixed-order premises, Table 1 shows that the model trained on shuffled premises fails to perform multi-step implicit reasoning correctly.Specifically, as the number of steps increases, the model's accuracy gradually decreases, reaching creased data volume and with different models, yet the phenomenon remains consistent.Please refer to Appendix F for more details.</p>
<p>To explore why models struggle with the "Variable as Subtrahend Plight", we revisit arithmetic expressions.While addition benefits from commutativity (e.g., a+b=b+a), subtraction lacks this property, as swapping the minuend and subtrahend changes the outcome unless a=b.This asymmetry creates challenges for models.For instance, in the sequence "a=6+2,b=a-3,c=4+b", the model might shortcut it as "c=6+2-3+4" (treating subtraction as addition).However, when subtrahends are variables, such a shortcut fails.If "b=3-a", the model can no longer chain terms directly and must compute intermediate results in sequence.As the number of variable subtrahends increases, the model faces greater difficulty in determining the correct order of operations, requiring rigorous step-by-step reasoning instead of relying on shortcuts.</p>
<p>Language models do not think step-by-step when the premise order is not fixed and overfit to an incorrect shortcut.Based on our analysis above, accuracy sharply declines if the model relies on shortcut computation.In contrast, stepby-step computation would result in minimal accuracy variation, as whether variables are subtrahends or not does not significantly affect sequential reasoning.To validate our hypothesis, we plot the accuracy trends against the number of equations with "Variable-as-Subtrahend" in our step-by-step computation model used in Section 4. As shown in Figure 6, there is only a slight variation in the accuracy of the step-by-step computation model while the accuracy of the model trained on problems with varied premise order drops significantly, which verifies our hypothesis. 6o sum up, when the training data follows a fixed pattern, LMs can learn a fixed pattern to store each intermediate result upon completing a step.For instance, in a forward premise order, the model simply follows the operators to compute the results of operands sequentially (i.e., step-by-step reasoning).There is no need to track the variables, as they must come from the previous step.However, when the premise order is shuffled, this shortcut pattern no longer exists, which necessitates the true reasoning capability: first tracking the variables and then performing the computation.More steps involve more complex tracking and computation, which explains why accuracy decreases as the number of steps increases.This implies that when LMs perform implicit reasoning, they are relying on shortcuts rather than engaging in true reasoning.</p>
<p>Furthermore, we find that the premise order does not significantly affect the model trained on an unfixed pattern.This further validates our hypothesis that such a language model relies on shortcuts for reasoning, as there is no difference in reasoning through shortcuts like chaining the numbers directly, whether in forward order (e.g., "c=6+2-3+4") or reverse order (e.g., "c=4-3+6+2").6 How Do LLMs Perform Multi-step Implicit Reasoning?</p>
<p>In the previous section, we found that GPT-2 is unable to perform implicit reasoning when there is no fixed pattern to learn during training.Does this phenomenon also apply to current SoTA LLMs, given that their training data is not always presented in a fixed order?Do these models reason step-bystep, or rely on shortcuts to solve the problem?</p>
<p>Setup.We conduct zero-shot experiments using both open-source and closed-source models, including GPT-4o-2024-08-06 (OpenAI, 2024a), Claude-3.5-sonnet-20241022-v2(Anthropic, 2024), Llama-3-70B-Instruct (AI@Meta, 2024), and Qwen2.5-72B-Instruct(Qwen-Team, 2024).We instruct the model to provide answers directly with the temperature set to 0. To ensure the consistency and fairness of our evaluation: 1) We retain the original data generation method but restrict instances to those with intermediate computation results between 0 and 22, thus eliminating the impact of mod23 on accuracy.2) We focus on 3-step problems, as implicit reasoning for 4-step problems proves too challenging for current LLMs, with low performance that undermines the reliability of our experiments.3) To reduce randomness, we generate 100 problems for each ratio of equations containing a variable as the subtrahend.For each question, we evaluate it with three premise orders, i.e., forward order, reverse order, and shuffled order.4) The accuracy is computed only in cases where the model does not output in CoT format.More details of the experimental setups are in Appendix H. a variable as the subtrahend.We find: 1) As the proportion of expressions with a variable as the subtrahend increases, the accuracy of the LLMs tends to decrease drastically.The accuracy of GPT-4o even drops from nearly 100% to approximately 30% regardless of premise order.2) All the models fail to do 3-step problems containing two equations with a variable as the subtrahend, and open-source LLMs still lag behind closed-source LLMs in implicit reasoning.3) Compared to the influence of variables as the subtrahends, the impact of premise order is not that significant, which aligns with our models trained on unfixed premise order.</p>
<p>Result</p>
<p>We further plot the accuracy of GPT-4o on problems stated in the forward order without any subtrahend being a variable but with different steps of calculations.From Figure 8, we observe that though the accuracy of GPT-4o decreases gradually, the accuracy on 9-step problems even surpasses that on 3-step problems containing two equations with a variable as the subtrahend in Figure 7.</p>
<p>To sum up, the findings suggest that LLMs likely rely on shortcuts for implicit reasoning rather than performing step-by-step reasoning, which aligns with our observations in the GPT-2 model.To speak further, while current LLMs can perform implicit reasoning within a fixed pattern and for a limited number of steps, they cannot generalize beyond these constraints.</p>
<p>Conclusion</p>
<p>In this paper, we investigate the implicit reasoning mechanism to uncover why advanced reasoning capabilities fail to emerge in the implicit reasoning style.We find that language models rely on shortcuts for implicit reasoning, and these shortcuts only work when the training data aligns with a specific pattern that supports directly chaining numbers.As a result, language models struggle with the "Variable as Subtrahend Plight," which requires true reasoning capabilities, such as variable tracking and step-by-step computation, where shortcuts are no longer effective.Experiments with current SoTA LLMs further validate our findings.</p>
<p>We hope this work deepens the understanding of implicit reasoning limitations in LMs and sparks future research to address LMs' key challenges in implicit multi-step reasoning.</p>
<p>Limitations</p>
<p>In this paper, we explore the mechanism of the language models performing multi-step implicit reasoning on synthetic arithmetic problems.To avoid large number operations and decimal operations, we only experiment with two fundamental arithmetic operators.This limitation suggests that future work could expand the scope to include a broader range of mathematical operators.Besides, we only focus on arithmetic reasoning due to the reasons elaborated in Section 1.We leave reasoning beyond arithmetic problems, e.g., commonsense reasoning, for further exploration in future research.</p>
<p>Another limitation lies in the possibility of inflated performance when evaluating LLMs.As their training methodologies and datasets remain proprietary, it is unclear whether these models were exposed to synthetic computational tasks similar to those explored in our study.</p>
<p>A More Details of the Data Generation Process</p>
<p>We provide an overview of the data generation process in Figure 9. First, for the training set, we create 25 000 distinct multi-step calculation templates for questions of each length (2 to 5 steps).</p>
<p>For the 1-step template, we include all the possible combinations to enable the model to learn basic calculations.Then, we use the same method for the test set, but an additional filter mechanism is employed to prevent LMs from utilizing intermediate results from the training set.As shown in Figure 9, the model may directly utilize the result of v 1 from the training data to calculate v 2 by just calculating v 2 = 4 + v 1 .Therefore, the test set retains only the templates whose preceding calculations, apart from the first step, do not overlap with those of the training set.This setting prevents LMs from memorizing intermediate results during training and recall them during testing rather than performing actual reasoning.</p>
<p>During generation, each of the two operators has a 50% probability, and the variable has a 50% probability of appearing before or after the operator (except for the first step).This results in 25% of the steps having the variable as the subtrahend in the original training and test set.</p>
<p>Since the variables in the template are sorted as v 0 , v 1 , ..., to prevent the model from learning the calculation order through the indices, we randomly replace them with the letters a-z.We use K different groups of variable names to instantiate each template in the training set.For the choice of K, please refer to the subsequent subsection.</p>
<p>A.1 Effect of the Number of Template Instantiations</p>
<p>In our early experiments, we find that when K equals 1, the model trained from scratch struggles to generalize effectively to problems outside the training set, even when these problems share the same template but have different variable names.We attempt to adjust the training hyperparameters, including the learning rate and weight decay; however, the situation remained unchanged.After increasing K to 2, the model successfully handles problems with the same template but different variable names, as well as those in the test set.So, we continue to use K = 2 in our experiment to ensure that the failure of generalization is caused by the model rather than our data.</p>
<p>B Choice of Activation Patching Settings</p>
<p>In this section, we study the choice of metrics and window sizes in the discovery of information flow.</p>
<p>B.1 Patching Metrics</p>
<p>Following the notations in Section 3, Logit denotes the output logit at the last token position, r and r ′ are the correct answer of the original input and corrupted input, and cl, *, pt denote the clean, corrupted and patched run separately.</p>
<p>In Figure 10, we compare the effect of several commonly used metrics: a) Logit of the clean run's ground-truth token r:</p>
<p>Logit cl (r) − Logit pt (r).We normalize this by Logit cl (r), and obtain the normalized patching effect as shown in Equation 1; b) Logit of the corrupted run's ground truth token r ′ :</p>
<p>Logit pt (r ′ ) − Logit cl (r ′ ).We do not normalize since Logit cl (r ′ ) can be very small, which may produce noisy localization outcomes.So we use
PE = Logit pt (r ′ ) − Logit cl (r ′ ); (2) c) Logit difference: LD(r, r ′ ) = Logit(r) − Logit(r ′ ).
We normalize this by LD cl (r, r ′ ) − LD * (r, r ′ ), and get so it typically lies in [0, 1].We find there is no significant difference in the discovery of information flow, and we use a) in our experiments for the following reasons: 1) Compared to c), a) can measure the patching effect when the ground truth tokens of the clean run and the corrupted run are the same.2) Compared to b), the patching effect of a) can be normalized to [0, 1] more stably.
PE = LD cl (r, r ′ ) − LD pt (r, r ′ ) LD cl (r, r ′ ) − LD * (r, r ′ ) ,(3)</p>
<p>B.2 Window Sizes</p>
<p>Following the best practices of activation patching (Zhang and Nanda, 2024), we initially employ single-layer interventions to identify crucial model components.However, as illustrated in Figure 11a, individual layer modifications produce only marginal effects, making it difficult to isolate critical hidden states.We speculate that language models may use aggregations from multiple inference pathways (McGrath et al., 2023), using a region rather than a hidden state to perform compu-tations and restore intermediate results.Noting that the critical blocks in Figure 11a frequently exhibit rectangular patterns, we implement a 2 × 2 window size to capture the joint effect of these regions.</p>
<p>Our comparison of different patching window sizes in Figure 11 reveals that different window sizes generally preserve similar information flow characteristics, and our 2 × 2 configuration best captures the information flow.We do not use a larger window size since the 2 × 2 window size is enough, and a larger window size may result in inflated localization plots.</p>
<p>C Information Flow Related to Operators</p>
<p>We present the residual stream patching plot altering the first operator in Figure 12.Similar to changing the operand, the patching effect is still pronounced at the end of each step, with information still propagating downward along the diagonal.</p>
<p>D Implementation Details of Masking Information from Previous Steps</p>
<p>In this experiment, we modify the Transformer model to restrict the attention so that each token can only attend to itself and the preceding window_size − 1 tokens.This is achieved by applying a sliding window mask, as illustrated in Algorithm 1. Specifically, we create an attention mask, where the parts we want to focus on are set to 0, and the remaining positions are set to −∞.The attention mask is then added to the attention score and passed through the softmax function.This procedure ensures the positions outside of the attention window are assigned zero attention after the softmax operation, effectively preventing the model from considering information from these positions.</p>
<p>Through this modification, we can further confirm whether the model only utilizes the information of the current step and the intermediate result from the previous step.</p>
<p>Algorithm 1 Creating Sliding Window Mask</p>
<p>Input: seq_length, window_size Output: mask 1: Initialize mask as a seq_length×seq_length matrix 2: for i ← 0 to seq_length − 1 do 3: In order to enable the model to learn to reason when the order of the premises is not fixed, the training data needs to contain patterns with different premise orders.To this end, in our early experiment, we have tried several data configurations on original GPT-2-Medium to select the best one, and see whether the failure stems from the insufficiency of premise pattern.Specifically, we gradually expand the dataset by controlling the upper limit of the patterns that can be added to the training data for each template (×m indicates that at most m orders for each template will be added to the training data).If the total number of premise orders for a certain template is less than or equal to m, then all order combinations will be added to the dataset; if the total number of orders is greater than m, then for each template, m randomly selected orders of this template will be added to the dataset.We provide the sample size for each dataset in Table 3.As shown in Figure 13, including more patterns does not necessarily improve performance.When all the patterns are added, the loss of the model on the test set basically cannot decrease, and overfitting occurs rapidly.
for j ← 0 to seq_length − 1 do 4: if j &lt; max(0, i − window_size + 1) or j &gt; i then 5: mask[i][j] ← −∞ 6:Dataset ×1 ×5 ×10 All Size 202K 850K 1.4M 7.6M
Table 3: Size of different datasets.We control the upper limit of patterns that can be added to the dataset for each template.For example, ×5 indicates that at most 5 orders for each template will be added to the training data.For the dataset including all the premise orders for every template, we also try another approach that upsample problems with fewer than 5 steps to the same proportion as that in the ×5 dataset.Although the minimum value of the evaluation loss is slightly lower, the loss increases rapidly after reaching the minimum value.We provide the accuracy of both models in Table 4.We find that both models cannot escape "Variable as Subtrahend Plight".Considering training and data efficiency, unless otherwise specified, we set the upper limit of the number of patterns for each step to 5 in all of our experiments.When training from scratch, we also test larger models, i.e., GPT2-RoPE-medium, by increasing the number of layers from 12 to 24, but the accuracy does not improve.We find that our GPT2-RoPE model initiated from a pre-trained GPT-2's weight may alleviate overfitting and have a higher performance, but we only observe this phenomenon on GPT2-RoPE-Medium.As shown in Table 5, despite the increased accuracy, the model still fails when almost all the variables are subtrahends.In addition, investigating model initialization is not the main focus of our paper.Since training from a pre-trained model is often better than training from scratch, in order to better control the experimental variables and illustrate the impact of the model size on the experimental results, we use the original pre-trained GPT-2 series to explore the influence of the model size.In Figure 14, we present the loss curves on the ID test set for different model sizes, ranging from GPT2-Small (124M) to GPT2-XL (1.5B).We find that increasing the model size from Small to Medium can lead to improvements.However, after reaching a certain size (Medium), further increasing the model size does not yield additional gains.</p>
<p>Order</p>
<p>F.2 Model Architecture</p>
<p>In addition to the GPT-2 models (with or without RoPE), we also test other model architecture such as Qwen2.5.Since the performance of the pretrained models is better than those trained from scratch, we initiate from the pre-trained weight.As shown in Table 6, we find that the model still does not escape "Variable as Subtrahend Plight".</p>
<p>F.3 Data Volume</p>
<p>Another potential explanation for the poor generalization could be the limited number of templates used during training.To investigate, we expand the dataset to include 50000 different templates for each step (except for the single-step question).Since the performance of the GPT2-RoPE model used in Table 5 is better than the one used in Table 1, we continue to train from the GPT2-RoPE-Medium model initiated from a pre-trained GPT-2's weight in this experiment.As shown in Table 7, this expanded experiment yields similar results, with models still failing to generalize to problems in which most variables are subtrahends.Furthermore, we scale up the templates for 5-step problems tenfold (simultaneously upsample instances of other step counts to maintain the proportion of different step counts within the dataset), so the training dataset comprises 500K different 5-step templates, hoping the model will thoroughly learn to solve 5-step problems.In addition to the accuracy on 5-step problems in Table 7, we also visualize the model's performance on 6-step problems in Figure 15.Although continuing to scale up the data can slightly boost the model's performance within the sequence lengths seen during training, data scaling does not address the core reasoning flaw that the model cannot genuinely track variables and perform step-bystep calculations, causing it to fall into the "Variable as Subtrahend Plight".G Mechanistic Insights into "Variable as Subtrahend Plight"</p>
<p>Due to the low accuracy of the models trained from scratch, we use the GPT2-RoPE-Medium-Pretrained model from Section F.1 instead for analysis.Since the model can not fully learn to do implicit reasoning on problems requiring more than 3 steps of reasoning, we first restrict our analysis to 3-step problems.We use 1 × 1 patching for this model, since 1 × 1 patching has already had a noticeable impact.</p>
<p>To see why the model fails to handle equations with variables being the subtrahends, we begin our mechanistic exploration by investigating the impact of the position of the variables.Specifically, we analyze four distinct operatorvariable combinations: "number + variable", "variable + number", "variable − number" and "number − variable".As shown in Figure 16, the first three graphs exhibit similar patterns, with the exception of the fourth graph, which shows some differences.We can see that in the first three graphs, the darker-colored areas are exclusively distributed in the output and numerical tokens, which means that the information in the remaining positions has no effect on the output.This phenomenon holds for all premise orders (Figure 17), since premise order does not disturb implicit reasoning through chaining the numbers directly.In contrast, in Fig- ure 16d, we find that the dark color appears on the variable token (i.e., v 0 ), which means the model needs the variable value at the subtrahend position to handle subsequent calculations.We also provide the patching plot on 4-step problems in Figure 18, where a clear difference can also be observed.</p>
<p>These mechanistic findings show that LMs chain the numbers directly when there is no variable as the subtrahend, and explain why the premise order does not significantly affect accuracy, which validates our previous analysis in Section 5 that the model relies on shortcuts to solve the problems.</p>
<p>H More Details of the Experimental</p>
<p>Setup in Section 6</p>
<p>In our preliminary tests, GPT-4o achieved less than 35% accuracy on 4-step problems containing only one variable as the subtrahend, while other opensource models performed only slightly above random guessing.Thus, we only study 3-step problems to ensure meaningful evaluation and better show the decreasing trend of the accuracy.Since the first step of the operation is between numbers, there are at most two equations containing a variable as the subtrahend in 3-step problems.</p>
<p>As the random premise order may still contain the forward order and the reverse order, we specify a fixed shuffled order instead.Specifically, we rearrange the original premise ([step1, step2, step3]) to [step3, step1, step2].The second step is delayed until the end, so the model can only link all the steps together at the last of the problem.</p>
<p>To prevent generic LLMs from using CoT reasoning to answer the question, we carefully craft the prompt to instruct the model to directly output the answer.An example of the prompt used for instructing generic LLMs to think without extra tokens in our task is shown below.</p>
<p>For Qwen and Llama, we use § ¤</p>
<p>¦ ¥</p>
<p>to prevent the model from outputting CoT process.We also test questions in the form of natural language, and reach the same conclusion as shown in Table 8.This indicates that our findings are unrelated to the form of the description.</p>
<p>Figure 3 :
3
Figure 3: Activation patching on residual stream across layers and token positions when changing the first number in the problems.All the premise orders are forward.</p>
<p>Figure 4 :
4
Figure 4: Test accuracy under different attention window sizes on 5-step problems.A window size of n means that a token can focus on itself and its preceding n − 1 tokens.</p>
<p>Figure 5 :
5
Figure 5: Patching effect of different components across layers and token positions.We change the numbers in the first two steps.The result of step 2 is changed in sub-figure (a)(b)(c), while the result is kept unchanged in sub-figure (d)(e)(f).A deeper color indicates the significance of activation at that position.We add a green rectangle in the figure to better illustrate the location where the patching effect first starts to diminish.</p>
<p>Figure 6 :
6
Figure 6: Test accuracies with increasing number of equations containing a variable as the subtrahend.The step-by-step computation model (Step-by-step Model) is evaluated on OOD 7-step problems since the accuracy of this model in both ID ones and OOD 6-step is nearly 100%.The model trained on problems with unfixed premise order (Shortcut Model) is evaluated on ID 5step problems.</p>
<p>Figure 7 :
7
Figure7: Performance comparison on 3-step problems with increasing numbers of equations containing a variable as the subtrahend.The problems in all the figures are the same, except for the order of premises.In a 3-step problem, at most two equations can have a variable as the subtrahend.</p>
<p>Figure 7 Figure 8 :
78
Figure7shows the accuracy of LLMs on problems with different ratios of equations containing</p>
<p>Figure 9 :
9
Figure 9: An overview of the data generation process.</p>
<p>Figure 10 :Figure 11 :
1011
Figure 10: Comparison of different patching metrics.(a) Logit of the clean run's ground truth token r.(b) Logit of the corrupted run's ground truth token r ′ .(c) Logit difference between r and r ′ .Step1 Step2 Step3 Step4 Step5 Step6 Q 0 2 4 6 8 10 Layer</p>
<p>Figure 12 :
12
Figure 12: Activation patching on hidden states across layers and token positions when changing the first operator in the problems.</p>
<p>10: end for 11: return mask E More Details of the Training Premise Pattern</p>
<p>Figure 13 :
13
Figure 13: Loss of the model on the ID test set during training when trained with different data configurations.The y-axis is plotted on a logarithmic scale.</p>
<p>Figure 14 :
14
Figure 14: Loss of the model on the ID test set during training when trained with different model sizes.The y-axis is plotted on a logarithmic scale.</p>
<p>Figure 15 :
15
Figure15: Test accuracies with increasing number of equations containing a variable as the subtrahend.To test genuine reasoning abilities, the premise order used during testing is random.</p>
<p>value of s?You must answer directly .Only output the final result .Begin your answer with "s = xx ".</p>
<p>variable − number v0 = n0 + n1 \n v1 = n2 -v0 \n v2 = n3 -v1 \n number − variable</p>
<p>Figure 16 :
16
Figure 16: Patching effect with different combination of the operator and the position of the variable when changing the first number in the problem.</p>
<p>Figure 17 :
17
Figure 17: Patching effect of different premise order averaged on the same set of problems when changing the first number in the problem.</p>
<p>Figure 18 :
18
Figure 18: Patching effect on 4-step problems when changing the first number.Only the second steps of the problems are different.</p>
<p>Table 2 :
2
Accuracy of the model on questions with different numbers of variables being subtrahends.The accuracy is calculated on 100 instances.Since the first step involves an operation between two numbers, the maximum number of variables as subtrahends is one less than the total number of steps.
Forward1.000.870.570.430.23Reverse1.000.810.510.380.19Random1.000.830.530.370.23Language models struggle with "Variable asSubtrahend Plight." To explore how LMs per-form implicit reasoning after being trained on anunfixed premise order, we conduct further analy-sis and find that the model is more prone to mak-ing mistakes when the premise contains multipleequations with a variable as the subtrahend. De-tailed statistics are provided in Table 2 on howthe model's accuracy varies with the number ofvariables being subtrahends for questions requiringthree to five steps of reasoning. As the number ofvariables being subtracted increases, the model'saccuracy decreases drastically, which is consistentacross different premise orders. We term this phe-
5 More details of data setups are included in Appendix E. Order 2-Step 3-Step 4-Step 5-Step 6-StepTable 1: The accuracy of the model trained with unfixed premise order dataset on the original test set.Each column represents problems with a specific number of steps, and each row represents a premise order used during testing.only∼40% accuracy when five steps of reasoning are required, contrasting the saturated accuracy of the model trained on fixed premise order.nomenon as "Variable as Subtrahend Plight."When almost all the variables in the premise are subtrahends, the model almost fails to solve any of the problems correctly.To rule out the possibility of a special case, we conduct experiments with in-</p>
<p>Table 4 :
4
Accuracy of the models with different training dataset on 5-step problems.</p>
<h1>VARIABLE BEING SUBTRAHEND0/4 1/4 2/4 3/4 4/4×5Forward 1.00 0.90 0.90 0.30 0.05Reverse1.00 0.97 0.98 0.50 0.08Random 1.00 0.83 0.89 0.45 0.23All (Upsample)Forward 1.00 0.87 0.85 0.55 0.08Reverse1.00 0.97 0.95 0.64 0.06Random 1.00 0.86 0.92 0.54 0.16F Extended Experiments on IncreasedData Volume and Different ModelsF.1 Model Size and Initialization</h1>
<p>Table 5 :
5
Accuracy of the model on 5-step questions with different numbers of variables being subtrahends.</p>
<p>Table 6 :
6
Accuracy of the models with different architecture on 5-step problems.
Order#VARIABLE BEING SUBTRAHEND 0/4 1/4 2/4 3/4 4/4Qwen2.5-1.5B-BaseForward 1.00 0.98 0.79 0.62 0.40Reverse0.99 0.97 0.98 0.90 0.69Random 1.00 0.89 0.81 0.74 0.61</p>
<p>Table 7 :
7
Accuracy of the models with different training data volume on 5-step problems.
Order#VARIABLE BEING SUBTRAHEND 0/4 1/4 2/4 3/4 4/450000 TemplatesForward 1.00 0.99 0.97 0.89 0.25Reverse1.00 1.00 0.96 0.85 0.24Random 1.00 0.97 0.92 0.70 0.2650000 Templates w/ further expansionForward 1.00 1.00 1.00 0.92 0.47Reverse1.00 1.00 0.98 0.90 0.44Random 1.00 1.00 0.95 0.81 0.44</p>
<p>Table 8 :
8
Performance comparison on 3-step problems in the form of natural language.The problems in each column are the same, except for the premise order.An example of the natural language form question is shown below.Here, we convert the equations in the original prompt into natural language descriptions resembling grade school math problems.§ ¤ A ' s number of apples equals 4 plus 14. C ' s number of apples equals A ' s number of apples minus 12. S ' s number of apples equals 6 minus C ' s number of apples .How many apples does S have ?Only output the final result .Do not output intermediate results .
Order#VARIABLE BEING SUBTRAHEND0/21/22/2GPT-4oForward 0.940.470.28Reverse0.930.330.21Shuffled 0.880.390.15Claude-3.5-sonnet-v2Forward 0.980.790.35Reverse0.910.670.05Shuffled 0.850.640.20
In this paper, "smaller" is relative to super large LMs like Deepseek R1, which has 671B parameters.
K = 2 in this paper. Please refer to Appendix A for more details about the data generation process.
We show the information flow related to operators in Appendix C.
We also provide mechanistic analysis of "Variable-as-Subtrahend" in Appendix G.
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020. Investigating gender bias in language models using causal mediation analysis. In Advances in Neural Information Processing Systems, volume 33, pages 12388-12401. Curran Associates, Inc.
AcknowledgmentsWe are grateful for the constructive comments from the anonymous reviewers.We would also like to thank Jiangjie Chen from Fudan University for valuable discussions and comments on this project.This work is supported by the Chinese NSF Major Research Plan (No.92270121).
A I , Meta , Llama 3 model card. 2024</p>
<p>Physics of language models: Part 3.2, knowledge manipulation. Zeyuan Allen, -Zhu , Yuanzhi Li, arXiv:2309.144022024Preprint</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2024</p>
<p>Hopping too late: Exploring the limitations of large language models on multihop queries. Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, Amir Globerson, 10.18653/v1/2024.emnlp-main.781Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Junhao Chen, Shengding Hu, Zhiyuan Liu, Maosong Sun, arXiv:2407.11421States hidden in hidden states: Llms emerge discrete state representations implicitly. 2024Preprint</p>
<p>A toy model of universality: Reverse engineering how networks learn group operations. Bilal Chughtai, Lawrence Chan, Neel Nanda, International Conference on Machine Learning. PMLR2023</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Deepseek-Ai , arXiv:2501.129482025Preprint</p>
<p>From explicit cot to implicit cot: Learning to internalize cot step by step. Yuntian Deng, Yejin Choi, Stuart Shieber, arXiv:2405.148382024Preprint</p>
<p>Implicit chain of thought reasoning via knowledge distillation. Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, Stuart Shieber, arXiv:2311.014602023Preprint</p>
<p>Measuring causal effects of data statistics on language model's 'factual' predictions. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schütze, Yoav Goldberg, arXiv:2207.142512023Preprint</p>
<p>Gemini 2.0 flash thinking mode (gemini-2.0-flash-thinking-exp-1219). Google, 2024</p>
<p>Universal neurons in GPT2 language models. Wes Gurnee, Theo Horsley, Zifan Carl Guo, Tara Rezaei Kheirkhah, Qinyi Sun, Will Hathaway, Neel Nanda, Dimitris Bertsimas, Transactions on Machine Learning Research. 2024</p>
<p>Finding neurons in a haystack: Case studies with sparse probing. Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, Dimitris Bertsimas, Transactions on Machine Learning Research. 2023</p>
<p>How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. Michael Hanna, Ollie Liu, Alexandre Variengien, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models. Peter Hase, Mohit Bansal, Been Kim, Asma Ghandeharioun, Thirtyseventh Conference on Neural Information Processing Systems. 2023</p>
<p>SWE-bench: Can language models resolve real-world github issues?. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R Narasimhan, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Investigating multi-hop factual shortcuts in knowledge editing of large language models. Tianjie Ju, Yijin Chen, Xinwei Yuan, Zhuosheng Zhang, Wei Du, Yubin Zheng, Gongshen Liu, 10.18653/v1/2024.acl-long.486Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Impact of co-occurrence on factual knowledge of large language models. Cheongwoong Kang, Jaesik Choi, 10.18653/v1/2023.findings-emnlp.518Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Think-to-talk or talk-to-think? when llms come up with an answer in multi-step reasoning. Keito Kudo, Yoichi Aoki, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Ana Brassard, Keisuke Sakaguchi, Kentaro Inui, arXiv:2412.011132024Preprint</p>
<p>Transformers learn shortcuts to automata. Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 2019</p>
<p>American invitational mathematics examination. MAA. 2024</p>
<p>Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in gpt. Thomas Mcgrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, Shane Legg, arXiv:2307.15771Advances in Neural Information Processing Systems. Kevin Meng, David BauCurran Associates, Inc202335PreprintThe hydra effect: Emergent self-repair in language model computations</p>
<p>Progress measures for grokking via mechanistic interpretability. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Arithmetic without algorithms: Language models solve math with a bag of heuristics. Yaniv Nikankin, Anja Reusch, Aaron Mueller, Yonatan Belinkov, arXiv:2410.21276The Thirteenth International Conference on Learning Representations. nostalgebraist. 2020. interpreting gpt: the logit lens. OpenAI. 2024a. Gpt-4o system card. Preprint. 2025</p>
<p>Fine-tuning enhances existing mechanisms: A case study on entity tracking. Tamar Rott Prakash, Tal Shaham, Yonatan Haklay, David Belinkov, Bau, The Twelfth International Conference on Learning Representations. 2024b. 2024OpenAILearning to reason with llms</p>
<p>Qwen-Team, Qwen2.5: A party of foundation models. 2024</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019OpenAI blog</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, arXiv:2311.120222023Preprint</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, 10.18653/v1/2023.emnlp-main.435Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, Yunfeng Liu, 10.1016/j.neucom.2023.127063Neurocomput. C5682024</p>
<p>Grokking of implicit reasoning in transformers: A mechanistic journey to the edge of generalization. Boshi Wang, Xiang Yue, Yu Su, Huan Sun, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. Kevin Ro, Wang , Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Revealing the barriers of language agents in planning. Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, Yanghua Xiao, arXiv:2410.124092024Preprint</p>
<p>Do large language models latently perform multi-hop reasoning?. Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel, 10.18653/v1/2024.acl-long.550Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024a1</p>
<p>Sohee Yang, Nora Kassner, Elena Gribovskaya, arXiv:2411.16679Sebastian Riedel, and Mor Geva. 2024b. Do large language models perform latent multi-hop reasoning without exploiting shortcuts? Preprint. </p>
<p>Physics of language models: Part 2.1, grade-school math and the hidden reasoning process. Tian Ye, Zicheng Xu, Yuanzhi Li, Zeyuan Allen-Zhu, arXiv:2407.203112024Preprint</p>
<p>Do llms really think step-by-step in implicit reasoning?. Yijiong Yu, arXiv:2411.158622024Preprint</p>
<p>Interpreting arithmetic mechanism in large language models through comparative neuron analysis. Zeping Yu, Sophia Ananiadou, 10.18653/v1/2024.emnlp-main.193Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Towards best practices of activation patching in language models: Metrics and methods. Fred Zhang, Neel Nanda, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, Austria2024. May 7-11, 2024</p>
<p>Interpreting and improving large language models in arithmetic calculation. Wei Zhang, Chaoqun Wan, Yonggang Zhang, Yiu-Ming Cheung, Xinmei Tian, Xu Shen, Jieping Ye, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine LearningPMLR2024Proceedings of Machine Learning Research</p>
<p>Pre-trained large language models use fourier features to compute addition. Tianyi Zhou, Deqing Fu, Sharan Vatsal, Robin Jia, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2054 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2054</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2054</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-281844240</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.04786v1.pdf" target="_blank">Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Humans are good at learning on the job: We learn how to solve the tasks we face as we go along. Can a model do the same? We propose an agent that assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and applies reinforcement learning to continue training the model for its target task. The test-time curriculum avoids time-consuming human curation of datasets by automatically selecting the most task-relevant data from a large pool of available training data. Our experiments demonstrate that reinforcement learning on a test-time curriculum consistently improves the model on its target tasks, across a variety of evaluations and models. Notably, on challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that TTC-RL significantly raises the performance ceiling compared to the initial model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to 43%. Our findings show the potential of test-time curricula in extending the test-time scaling paradigm to continual training on thousands of task-relevant experiences during test-time.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2054.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2054.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TTC-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-Time Curriculum for Targeted Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that self-curates a task-specific curriculum at test-time (via SIFT over a large verifiable corpus) and performs on-policy RL (GRPO) on the selected curriculum to specialize an LLM to given target tasks, achieving faster and higher final accuracy than general-purpose RL post-training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-embedding-based automatic selection (self-curated curriculum via SIFT)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Qwen3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Given a set of target test tasks, the initial model crafts a test-time curriculum by selecting tasks from a large, multi-domain verifiable corpus using SIFT over the model's normalized last-token last-layer embeddings; the selected curriculum trades relevance to the target tasks and diversity via a hyperparameter λ (set to 0.1 in experiments). The agent then attempts tasks from this curriculum in batches, obtains binary/verifier rewards (numeric equality, unit tests, or learned verifier for judged equivalence), and performs on-policy RL updates (GRPO without KL penalty). A variant (A-TTC) further augments selection to prefer tasks of intermediate difficulty by maintaining online success-rate estimates (achievability) and filtering tasks to a target achievability interval (e.g., [0.2,0.6]).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Math (AIME, MATH), Coding (Codeforces, CodeElo, LiveCodeBench), Scientific reasoning (GPQA-Diamond)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Diverse, verifiable-answer domains: math with exact numeric answers; coding with unit-test verification; general reasoning with judged/verifier-model equivalence. Tasks vary widely in difficulty (from simple to competition-level), are largely single-instance (problem solving, not open-ended), require compositional reasoning and domain knowledge (mathematical problem solving or algorithm design), and typically have sparse binary/verifier rewards (correct/incorrect). Not formulated as long-horizon sequential control; rather episodic per-problem planning with potentially long chain-of-thought reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Curriculum selection conditions on the target test tasks and (for A-TTC) on online achievability estimates (per-training-task success rates updated from recent rollouts). SIFT uses the model's latent embeddings of descriptions; A-TTC uses past observed success rates (rolling estimates) to bias selection toward ~50% success-rate tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td>SIFT implements an explicit diversity/relevance trade-off via a regularization coefficient λ: small λ favors diversity, large λ favors relevance; greedy selection minimizes posterior uncertainty in an RKHS kernel induced by embeddings. The selection objective thus encourages covering diverse informative examples and avoids duplicative data.</td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>On-policy RL algorithm GRPO (modified without KL penalty), external verifier models for judged equivalence (1.5B verifier), majority-voting baseline (Maj-TTRL) as a complementary training signal, and dataset decontamination/deduplication pipelines; A-TTC uses online difficulty estimation to complement SIFT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Substantial gains reported using TTC-RL (self-curated curriculum + GRPO) on multiple models/benchmarks. Examples: Qwen3-8B pass@1 improved ~1.8× on AIME25 and ~2.1× on CodeElo (abstract); pass@8 on AIME25 increased from 40% → 62% and on CodeElo from 28% → 43% (abstract / Table 6). TTC-RL also raised pass@k across a wide range of k and improved majority-voting gains. TTC-RL yields faster learning and higher saturation than a general-purpose RL post-training baseline (uniform 1k-task sampling). Per-benchmark averaged plots show consistent improvements over 250 RL steps; per-task TTC-RL sometimes outperforms benchmark-level TTCs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td>TTC-SFT (supervised fine-tuning on human/expert traces) was evaluated and found ill-suited for test-time training: SFT trained directly on test traces caused an initial massive accuracy drop (to near 0%) and required hundreds of gradient steps before accuracy rose, indicating severe distribution-shift/instability (Figure 6, qualitative examples). OpenThinker3-style SFT converged at high accuracy only after extensive training, but exhibited an initial collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>Compared to heuristic baselines: (1) 'RL post-training' baseline sampling 1k uniform tasks from corpus yields a general-purpose model but trains slower and saturates at lower accuracy than TTC-RL with curriculum size 1k; (2) restricting RL post-training to the target domain (domain-filtered sampling) explains only part of TTC-RL's gains on coding tasks, i.e., domain-only heuristic is insufficient (Figure 11); (3) A-TTC (heuristic difficulty-balancing + SIFT) improved learning for much weaker models (e.g., Qwen3-0.6B) by selecting tasks of appropriate difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td>Initial (no test-time training) baselines: Qwen3-8B initial pass@1 values (per Table 6) e.g., AIME24 21.67%, AIME25 23.33%, MATH500 69.55%, Codeforces 20.85%, CodeElo 13.73%, LCB 20.61%, GPQA-D 49.11% — TTC-RL substantially increases these values after training (e.g., AIME24 pass@1 → 50.83%, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>Training corpus (verifiable-corpus) ~265k tasks across math, code, and general reasoning; typical TTCs in experiments use curriculum sizes of 1k (also swept across sizes). SIFT-selected top-10 examples for individual questions are shown as qualitative examples; experiments ablate curriculum sizes (including very small sizes) and show TTC-RL outperforms RL post-training across many sizes (except dataset size 1 where overfitting sometimes harms).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>TTC-RL specializes models strongly to their target tasks: cross-evaluation shows a block-diagonal performance pattern where models trained for a target benchmark perform best on that benchmark and underperform on unrelated benchmarks (Figure 5 right). Models generalize better among related math benchmarks; training on code often generalizes to math better than vice versa. Per-task TTCs (one TTC per AIME25 question) can outperform per-benchmark TTCs, demonstrating transfer/generalization benefits when tightly focused.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>No dollar costs reported; experiments use 'non-thinking' models (responses limited to 8k tokens) for compute reasons; typical RL settings: two episodes, batch size 8, 16 rollouts per train task, avg@4 measured every ten steps, moving average smoothing. Authors note asymptotic cost: growing context length in Transformers is quadratic while TTC-RL's cost (compressing experience into weights) is linear in experience, suggesting compute-efficiency trade-offs, but no wall-clock or monetary costs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Limitations and failure modes documented: (1) Corpus contamination and reward-hacking risks—mitigations include decontamination, removing leaked I/O examples, and manual checks; (2) Fixed training corpus bottleneck: strong models (e.g., Qwen3-4B-Instruct-2507) sometimes improved more via Maj-TTRL, suggesting TTC-RL gains are limited by available relevant training data; (3) Overfitting risk when curriculum very small (dataset size=1 observed worse performance than RL post-training); (4) SFT instability: supervised fine-tuning on expert traces can cause initial collapse; (5) GRPO clipping hyperparameters (clip-high) require tuning to avoid policy entropy collapse — poor settings harm exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Although not a sequential long-horizon control domain, TTC-RL helps short-context LLMs (8k) approach the performance of long-context 'thinking' models (30k) in several domains (coding, GPQA), indicating TTC-RL can compensate for limited context length by encoding strategies into weights. No explicit experiments on multi-step long-horizon planning domains are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>TTC-RL yields large gains on challenging, specialized domains: e.g., on AIME24/AIME25 and CodeElo (competition math and coding) large multiplicative gains in pass@1 and pass@k; gains on easier benchmarks are smaller. Table 5 shows stronger relative improvements for weaker model variants and for harder benchmarks. The approach leverages domain-specific verifiable training signals (unit tests, numeric checks, learned verifier).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Multiple ablations reported: curriculum size sweep (Figure 3) showing TTC-RL outperforms RL post-training across sizes; A-TTC vs TTC shows achievability balancing helps weaker models (Qwen3-0.6B) (Figure 8); GRPO clip-high ablation shows increasing clip-high is essential to prevent entropy collapse and improve learning (Figure 9); per-task vs per-benchmark TTC ablation shows per-task TTC can outperform benchmark-level TTC (Figure 5 left); restricting RL post-training to domain ablation shows domain-only sampling insufficient (Figure 11); combining TTC-RL with Maj-TTRL yields additive gains (Figure 12).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td>Curriculum selection uses Qwen3-8B embeddings across runs for SIFT (authors fixed this for simplicity). Experiments include multiple model checkpoints/sizes for training (Qwen3-8B, Qwen3-4B-Instruct-2507, Qwen3-8B-Base, Qwen3-0.6B), and reported effects vary by model capability. Authors did not present a dedicated study sweeping multiple LLM sizes for curriculum generation itself.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Self-curated, LLM-embedding-based curricula (TTC-RL) substantially accelerate learning and raise final performance ceilings compared to uniform RL post-training and to domain-only heuristics. Balancing relevance and diversity (SIFT λ) and selecting tasks of intermediate difficulty (A-TTC) further improves sample efficiency, especially for weaker models. On-policy RL (GRPO) on self-curated curricula outperforms off-policy SFT on expert traces for test-time training due to SFT's distribution shift/instability. TTC-RL also increases pass@k and complements majority-vote style test-time scaling; however, effectiveness is constrained by the fixed training corpus and requires careful anti-contamination measures and RL hyperparameter tuning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2054.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2054.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SIFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SIFT (Selection by Informative Feature Tracking)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active data-selection method that greedily selects training examples to minimize posterior uncertainty about how to respond to a specific prompt, using a kernel over latent embeddings and a regularization λ to trade relevance vs diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>heuristic-based / active selection (information-theoretic selection over LLM embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Qwen3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>SIFT computes normalized last-token last-layer embeddings ϕ of candidate task descriptions, defines an inner-product kernel k(x,x'), and greedily selects examples minimizing posterior variance σ^2_X(x*). The regularization parameter λ controls the relevance-diversity trade-off. The paper uses SIFT to produce TTCs by selecting tasks most informative for target test tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Same multi-domain verifiable corpus (math, code, general reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>As above: verifiable discrete-answer tasks, mixture of numeric, unit-test, and judged equivalence verification; high task diversity in corpus (~265k tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td>SIFT's objective induces diversity by penalizing redundant examples via the kernelized posterior-variance criterion; λ explicitly trades relevance vs diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Used together with GRPO for RL training, and optionally with A-TTC achievability filtering to control difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Used as the curriculum selection engine in TTC-RL; experiments show SIFT-selected curricula (size 1k) lead to faster learning and higher final accuracy than uniform RL post-training baselines selecting 1k tasks at random.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>SIFT vs simple heuristics: paper shows domain-only sampling (heuristic) underperforms SIFT-based TTC; A-TTC (SIFT constrained by achievability interval) improves performance for weak models versus vanilla SIFT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>Selection uses corpus of ~265k tasks; typical selected curriculum sizes reported at 1k; qualitative top-10 selections shown for specific test questions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>SIFT-selected curricula produce specialized models that generalize best to their target benchmarks and related tasks, as indicated by cross-evaluation matrices (block-diagonal structure).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>SIFT computes embeddings for all candidate descriptions (authors precompute embeddings with Qwen3-8B). No per-call API cost reported. Selection is greedy and thus scales with corpus size; authors decontaminate and deduplicate prior to selection to reduce candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>SIFT depends on quality of embedding space—if embeddings do not represent task relevance well, selection may be suboptimal; λ must be tuned (authors set λ=0.1 and report robustness). Also requires precomputed embeddings and a curated corpus (fixed-corpus limitation).</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>SIFT works across domains because it operates on description embeddings; selection successfully produced coding curricula (unit-test tasks) and math curricula (numeric-verification tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Authors vary λ and report robustness; A-TTC ablation shows combining SIFT with achievability filtering aids weaker models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td>SIFT selection in experiments used embeddings from Qwen3-8B fixed across runs; no multi-size ablation for selection model reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Embedding-based active selection (SIFT) that balances relevance and diversity yields curricula that accelerate RL specialization and give higher asymptotic performance than uniform or domain-only sampling; combining with difficulty-filtering (A-TTC) is beneficial for weaker agents.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2054.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2054.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A-TTC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Achievable Test-Time Curricula (A-TTC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of TTC selection that trades off relevance (SIFT) with online achievability/difficulty estimates to prefer training tasks of appropriate difficulty (e.g., near 50% success rate), improving learning speed for weaker models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>hybrid (LLM-embedding-based selection + online difficulty heuristic)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Qwen3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>A-TTC first computes per-task achievability estimates α_x,t (initial priors from model success rates) and updates them online from batch rewards; it restricts the SIFT selection pool to tasks with α in [a_min, a_max] (authors use [0.2,0.6]) so that selected tasks are both relevant and of intermediate difficulty (targeting maximal GRPO advantage ~50% success rate).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Math (AIME, MATH) primarily evaluated; applicable to other verifiable domains</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Same as TTC-RL; especially useful when dataset difficulty is poorly calibrated for a weaker initial model.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Maintains and conditions on online success-rate (achievability) estimates per training task updated from recent batch rewards; uses these to filter candidate tasks for SIFT.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td>Retains SIFT's diversity objective while limiting candidates to those of intermediate difficulty, implicitly encouraging diverse yet appropriately novel/difficult tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Used with GRPO RL training; complements SIFT selection; requires initial difficulty priors computed from model success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>On a weaker model (Qwen3-0.6B) A-TTC yields higher training reward and improved test scores across MATH benchmarks (AIME24, AIME25, MATH500) compared to vanilla TTC-RL (Figure 8).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>A-TTC can be seen as combining SIFT (learned/embedding selection) with a simple heuristic difficulty filter (achievability interval); experiments show this hybrid outperforms plain SIFT for weaker models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>A-TTC enforces a minimum pool size (authors enforce 1,000 candidates) and then selects via SIFT within the filtered difficulty interval; specific diversity counts not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>A-TTC accelerates learning for weak models and produces competitive specialization; no explicit cross-benchmark transfer matrix reported only for base TTC-RL but trends mirror TTC-RL improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Additional bookkeeping to maintain per-task achievability estimates; authors use a simple recursive update and assume a model for correlated updates—no heavy cost reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Requires reasonable initial difficulty priors; if priors are poor or dataset lacks sufficient tasks in the target difficulty band, filtering can reduce candidate pool and harm training. Hyperparameters [a_min,a_max] must be chosen appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>Shown useful in math benchmarks for weaker models; intended broadly applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Authors ablate the effect on a weaker model (Qwen3-0.6B) and find achievability balancing increases reward and test performance; recommended interval [0.2,0.6].</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td>Evaluated impact primarily on weaker models (0.6B); no large-scale sweep across many LLM sizes for generator reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Balancing relevance with online difficulty estimates provides higher learning signal (maximizes GRPO advantage) and accelerates learning for weaker agents; achievability filtering is most useful when dataset difficulty is misaligned with model capability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2054.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2054.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Maj-TTRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority-Vote Test-Time Reinforcement Learning (Maj-TTRL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A test-time RL approach (from Zuo et al., 2025) that uses majority votes of model rollouts on the target tasks as surrogate rewards to train on the test set directly; applicable only to structured-output environments (e.g., numeric or multiple-choice).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>training-on-target-tasks (no explicit curriculum generation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Maj-TTRL trains the model directly on the target benchmark by using the model's majority prediction among multiple rollouts as a pseudo-reward signal, thereby continually improving agreement with the model's consensus; it does not rely on selecting external training tasks from a corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Math (AIME, MATH) and multiple-choice GPQA evaluated; not applicable to coding benchmarks with unit-test verification.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Works for structured output environments where majority-vote consensus is meaningful (numeric answers, multiple-choice); not applicable when verification requires running unit tests or when outputs are open-form.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Can be combined with TTC-RL (authors show initializing Maj-TTRL from final TTC-RL checkpoint yields best math results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Maj-TTRL yields significant gains on math benchmarks and is competitive or superior on the strongest model (Qwen3-4B-Instruct-2507) for some math tasks (Table 4): Maj-TTRL gave larger improvements on some math benchmarks where TTC-RL was constrained by the fixed corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Maj-TTRL improves performance directly on the target tasks but is less applicable to coding benchmarks where unit-test verification is needed; combining Maj-TTRL after TTC-RL gives additive benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Requires multiple rollouts per target task to compute majority labels; cost scales with k (number of rollouts) and target-set size. Authors used Maj-TTRL as a specialized baseline and as a fine-tuning step after TTC-RL.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Only applicable to structured-output domains where majority voting is a meaningful surrogate; less helpful in GPQA and not applicable to code tasks. Can be limited by the model's initial pass@k ceiling.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>Strong on math benchmarks for strong models; less helpful on GPQA and not applicable for code environments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Authors compare Maj-TTRL alone vs TTC-RL and show Maj-TTRL initialized from TTC-RL checkpoint yields best results on math (Figure 12).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td>Maj-TTRL was most effective on the strongest model (Qwen3-4B-Instruct-2507) among those evaluated; indicates some dependence on base model capability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Maj-TTRL is a complementary, target-task-focused approach that can outperform TTC-RL when the fixed external corpus is insufficient for further improvement; combining Maj-TTRL with TTC-RL yields the strongest math results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2054.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2054.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TTC-SFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-Time Curriculum Supervised Fine-Tuning (TTC-SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant where self-curated curricula (retrieved tasks) are used for supervised fine-tuning (SFT) instead of on-policy RL; prior work and authors' experiments show SFT on expert traces can reduce robustness at test-time and cause initial performance collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>retrieval-based SFT (human/expert traces retrieved or generated)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>QwQ-32B (used to generate expert traces in one experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>TTC-SFT retrieves or constructs expert traces for selected curriculum tasks and fine-tunes the model on these traces (off-policy). The authors tested SFT on the test sets (using expert traces generated by a larger model) and found it unstable for test-time training.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Math (AIME, AMC, GSM8K) evaluated in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Requires high-quality problem solutions / chains-of-thought for supervised targets; susceptible to distribution shift when off-policy traces differ from model's prior behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Compared against TTC-RL (on-policy) and used OpenThinker3 traces (QwQ-32B) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>SFT on test traces is unstable: training directly on test set with SFT caused an initial accuracy drop to near 0% and required hundreds of gradient steps to recover and improve (Figure 6). At convergence, SFT can reach strong performance but is less sample-efficient and less robust than TTC-RL for test-time training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>SFT tends to overfit to trace style and can hurt generalization early in training; evidence from GSM8K SFT experiments showed drop before overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>SFT requires storing/generating expert traces (e.g., QwQ-32B traces) and gradient-based fine-tuning; authors report hundreds of gradient steps needed before improvement is observed when training on test-set traces.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Off-policy distribution shift causes phase transitions in model behavior and initial collapse; requires expert-quality traces and is impractical at scale for many tasks because it demands solution traces (not just verifiable labels).</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>Converged SFT can perform well (OpenThinker3 experiments) but is less robust during initial training; not recommended as a primary TTT mechanism per authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Qualitative and quantitative comparisons to TTC-RL showing SFT's initial drop and later overfitting; GSM8K SFT experiments replicate instability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td>SFT experiments used larger teacher model traces (QwQ-32B) to produce expert solutions for training smaller models; effect depends on trace quality and student model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Off-policy SFT on self-curated curricula is brittle for test-time training: it can cause an initial catastrophic drop in performance due to distribution shift and is less sample-efficient for rapid specialization compared to on-policy TTC-RL.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2054.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2054.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-generated curricula (literature)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-generated training curricula / self-play / automated curriculum generation (literature mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of related contemporary works that study LLMs/agents that autonomously generate training problems, self-play curricula, or goals for improving agents (cited as Zhao et al., Huang et al., Chen et al., Fang et al., etc.), typically aimed at scaling RL/self-play ideas to reasoning and coding domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated / self-play / automated curriculum (various approaches across cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>The paper references recent work that generalizes self-play and automated curriculum learning to LLMs by (a) self-generating training tasks or goals and (b) using self-play to iteratively produce increasingly challenging problems. These approaches are cited as complementary and motivating extensions to TTC-RL for moving beyond a fixed corpus to self-generated curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Mentioned across coding and general reasoning domains; some prior work focused on simpler robotic navigation tasks, others on coding or reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Varied: includes game-like self-play domains (closed systems) as well as open-ended task generation for reasoning/coding; task diversity and open-endedness vary by work.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Often combined with self-play, goal-conditioned RL, or automated difficulty scheduling in cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Paper notes that prior evaluations of self-generated curricula often focused on simpler domains (e.g., robotic navigation) and that extending to challenging reasoning tasks is nontrivial; also highlights fixed-corpus limitation and need for clean self-generation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Cited as promising directions: self-generated curricula and self-play can create curricula without human curation, but prior work has limited evaluation scope; TTC-RL extends the idea with embedding-based selection from a large human-created corpus and shows strong gains on hard reasoning benchmarks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ttrl: Test-time reinforcement learning <em>(Rating: 2)</em></li>
                <li>Serl: Self-play reinforcement learning for large language models <em>(Rating: 2)</em></li>
                <li>Discover: Automated curricula for sparse-reward reinforcement learning <em>(Rating: 2)</em></li>
                <li>R-zero: Self-evolving reasoning llm from zero data <em>(Rating: 1)</em></li>
                <li>Self-improvement in language models: The sharpening mechanism <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2054",
    "paper_id": "paper-281844240",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [
        {
            "name_short": "TTC-RL",
            "name_full": "Test-Time Curriculum for Targeted Reinforcement Learning",
            "brief_description": "An agent that self-curates a task-specific curriculum at test-time (via SIFT over a large verifiable corpus) and performs on-policy RL (GRPO) on the selected curriculum to specialize an LLM to given target tasks, achieving faster and higher final accuracy than general-purpose RL post-training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-embedding-based automatic selection (self-curated curriculum via SIFT)",
            "llm_model_name": "Qwen3-8B",
            "llm_model_size": "8B",
            "curriculum_description": "Given a set of target test tasks, the initial model crafts a test-time curriculum by selecting tasks from a large, multi-domain verifiable corpus using SIFT over the model's normalized last-token last-layer embeddings; the selected curriculum trades relevance to the target tasks and diversity via a hyperparameter λ (set to 0.1 in experiments). The agent then attempts tasks from this curriculum in batches, obtains binary/verifier rewards (numeric equality, unit tests, or learned verifier for judged equivalence), and performs on-policy RL updates (GRPO without KL penalty). A variant (A-TTC) further augments selection to prefer tasks of intermediate difficulty by maintaining online success-rate estimates (achievability) and filtering tasks to a target achievability interval (e.g., [0.2,0.6]).",
            "domain_name": "Math (AIME, MATH), Coding (Codeforces, CodeElo, LiveCodeBench), Scientific reasoning (GPQA-Diamond)",
            "domain_characteristics": "Diverse, verifiable-answer domains: math with exact numeric answers; coding with unit-test verification; general reasoning with judged/verifier-model equivalence. Tasks vary widely in difficulty (from simple to competition-level), are largely single-instance (problem solving, not open-ended), require compositional reasoning and domain knowledge (mathematical problem solving or algorithm design), and typically have sparse binary/verifier rewards (correct/incorrect). Not formulated as long-horizon sequential control; rather episodic per-problem planning with potentially long chain-of-thought reasoning.",
            "state_conditioning": true,
            "state_conditioning_details": "Curriculum selection conditions on the target test tasks and (for A-TTC) on online achievability estimates (per-training-task success rates updated from recent rollouts). SIFT uses the model's latent embeddings of descriptions; A-TTC uses past observed success rates (rolling estimates) to bias selection toward ~50% success-rate tasks.",
            "novelty_mechanism": true,
            "novelty_mechanism_details": "SIFT implements an explicit diversity/relevance trade-off via a regularization coefficient λ: small λ favors diversity, large λ favors relevance; greedy selection minimizes posterior uncertainty in an RKHS kernel induced by embeddings. The selection objective thus encourages covering diverse informative examples and avoids duplicative data.",
            "complementary_systems": "On-policy RL algorithm GRPO (modified without KL penalty), external verifier models for judged equivalence (1.5B verifier), majority-voting baseline (Maj-TTRL) as a complementary training signal, and dataset decontamination/deduplication pipelines; A-TTC uses online difficulty estimation to complement SIFT.",
            "performance_llm_curriculum": "Substantial gains reported using TTC-RL (self-curated curriculum + GRPO) on multiple models/benchmarks. Examples: Qwen3-8B pass@1 improved ~1.8× on AIME25 and ~2.1× on CodeElo (abstract); pass@8 on AIME25 increased from 40% → 62% and on CodeElo from 28% → 43% (abstract / Table 6). TTC-RL also raised pass@k across a wide range of k and improved majority-voting gains. TTC-RL yields faster learning and higher saturation than a general-purpose RL post-training baseline (uniform 1k-task sampling). Per-benchmark averaged plots show consistent improvements over 250 RL steps; per-task TTC-RL sometimes outperforms benchmark-level TTCs.",
            "performance_manual_curriculum": "TTC-SFT (supervised fine-tuning on human/expert traces) was evaluated and found ill-suited for test-time training: SFT trained directly on test traces caused an initial massive accuracy drop (to near 0%) and required hundreds of gradient steps before accuracy rose, indicating severe distribution-shift/instability (Figure 6, qualitative examples). OpenThinker3-style SFT converged at high accuracy only after extensive training, but exhibited an initial collapse.",
            "performance_heuristic_curriculum": "Compared to heuristic baselines: (1) 'RL post-training' baseline sampling 1k uniform tasks from corpus yields a general-purpose model but trains slower and saturates at lower accuracy than TTC-RL with curriculum size 1k; (2) restricting RL post-training to the target domain (domain-filtered sampling) explains only part of TTC-RL's gains on coding tasks, i.e., domain-only heuristic is insufficient (Figure 11); (3) A-TTC (heuristic difficulty-balancing + SIFT) improved learning for much weaker models (e.g., Qwen3-0.6B) by selecting tasks of appropriate difficulty.",
            "performance_no_curriculum": "Initial (no test-time training) baselines: Qwen3-8B initial pass@1 values (per Table 6) e.g., AIME24 21.67%, AIME25 23.33%, MATH500 69.55%, Codeforces 20.85%, CodeElo 13.73%, LCB 20.61%, GPQA-D 49.11% — TTC-RL substantially increases these values after training (e.g., AIME24 pass@1 → 50.83%, etc.).",
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "Training corpus (verifiable-corpus) ~265k tasks across math, code, and general reasoning; typical TTCs in experiments use curriculum sizes of 1k (also swept across sizes). SIFT-selected top-10 examples for individual questions are shown as qualitative examples; experiments ablate curriculum sizes (including very small sizes) and show TTC-RL outperforms RL post-training across many sizes (except dataset size 1 where overfitting sometimes harms).",
            "transfer_generalization_results": "TTC-RL specializes models strongly to their target tasks: cross-evaluation shows a block-diagonal performance pattern where models trained for a target benchmark perform best on that benchmark and underperform on unrelated benchmarks (Figure 5 right). Models generalize better among related math benchmarks; training on code often generalizes to math better than vice versa. Per-task TTCs (one TTC per AIME25 question) can outperform per-benchmark TTCs, demonstrating transfer/generalization benefits when tightly focused.",
            "computational_cost": "No dollar costs reported; experiments use 'non-thinking' models (responses limited to 8k tokens) for compute reasons; typical RL settings: two episodes, batch size 8, 16 rollouts per train task, avg@4 measured every ten steps, moving average smoothing. Authors note asymptotic cost: growing context length in Transformers is quadratic while TTC-RL's cost (compressing experience into weights) is linear in experience, suggesting compute-efficiency trade-offs, but no wall-clock or monetary costs reported.",
            "failure_modes_limitations": "Limitations and failure modes documented: (1) Corpus contamination and reward-hacking risks—mitigations include decontamination, removing leaked I/O examples, and manual checks; (2) Fixed training corpus bottleneck: strong models (e.g., Qwen3-4B-Instruct-2507) sometimes improved more via Maj-TTRL, suggesting TTC-RL gains are limited by available relevant training data; (3) Overfitting risk when curriculum very small (dataset size=1 observed worse performance than RL post-training); (4) SFT instability: supervised fine-tuning on expert traces can cause initial collapse; (5) GRPO clipping hyperparameters (clip-high) require tuning to avoid policy entropy collapse — poor settings harm exploration.",
            "long_horizon_performance": "Although not a sequential long-horizon control domain, TTC-RL helps short-context LLMs (8k) approach the performance of long-context 'thinking' models (30k) in several domains (coding, GPQA), indicating TTC-RL can compensate for limited context length by encoding strategies into weights. No explicit experiments on multi-step long-horizon planning domains are reported.",
            "specialized_domain_performance": "TTC-RL yields large gains on challenging, specialized domains: e.g., on AIME24/AIME25 and CodeElo (competition math and coding) large multiplicative gains in pass@1 and pass@k; gains on easier benchmarks are smaller. Table 5 shows stronger relative improvements for weaker model variants and for harder benchmarks. The approach leverages domain-specific verifiable training signals (unit tests, numeric checks, learned verifier).",
            "ablation_studies": "Multiple ablations reported: curriculum size sweep (Figure 3) showing TTC-RL outperforms RL post-training across sizes; A-TTC vs TTC shows achievability balancing helps weaker models (Qwen3-0.6B) (Figure 8); GRPO clip-high ablation shows increasing clip-high is essential to prevent entropy collapse and improve learning (Figure 9); per-task vs per-benchmark TTC ablation shows per-task TTC can outperform benchmark-level TTC (Figure 5 left); restricting RL post-training to domain ablation shows domain-only sampling insufficient (Figure 11); combining TTC-RL with Maj-TTRL yields additive gains (Figure 12).",
            "model_size_scaling": "Curriculum selection uses Qwen3-8B embeddings across runs for SIFT (authors fixed this for simplicity). Experiments include multiple model checkpoints/sizes for training (Qwen3-8B, Qwen3-4B-Instruct-2507, Qwen3-8B-Base, Qwen3-0.6B), and reported effects vary by model capability. Authors did not present a dedicated study sweeping multiple LLM sizes for curriculum generation itself.",
            "key_findings_curriculum_effectiveness": "Self-curated, LLM-embedding-based curricula (TTC-RL) substantially accelerate learning and raise final performance ceilings compared to uniform RL post-training and to domain-only heuristics. Balancing relevance and diversity (SIFT λ) and selecting tasks of intermediate difficulty (A-TTC) further improves sample efficiency, especially for weaker models. On-policy RL (GRPO) on self-curated curricula outperforms off-policy SFT on expert traces for test-time training due to SFT's distribution shift/instability. TTC-RL also increases pass@k and complements majority-vote style test-time scaling; however, effectiveness is constrained by the fixed training corpus and requires careful anti-contamination measures and RL hyperparameter tuning.",
            "uuid": "e2054.0"
        },
        {
            "name_short": "SIFT",
            "name_full": "SIFT (Selection by Informative Feature Tracking)",
            "brief_description": "An active data-selection method that greedily selects training examples to minimize posterior uncertainty about how to respond to a specific prompt, using a kernel over latent embeddings and a regularization λ to trade relevance vs diversity.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generator_type": "heuristic-based / active selection (information-theoretic selection over LLM embeddings)",
            "llm_model_name": "Qwen3-8B",
            "llm_model_size": "8B",
            "curriculum_description": "SIFT computes normalized last-token last-layer embeddings ϕ of candidate task descriptions, defines an inner-product kernel k(x,x'), and greedily selects examples minimizing posterior variance σ^2_X(x*). The regularization parameter λ controls the relevance-diversity trade-off. The paper uses SIFT to produce TTCs by selecting tasks most informative for target test tasks.",
            "domain_name": "Same multi-domain verifiable corpus (math, code, general reasoning)",
            "domain_characteristics": "As above: verifiable discrete-answer tasks, mixture of numeric, unit-test, and judged equivalence verification; high task diversity in corpus (~265k tasks).",
            "state_conditioning": false,
            "state_conditioning_details": null,
            "novelty_mechanism": true,
            "novelty_mechanism_details": "SIFT's objective induces diversity by penalizing redundant examples via the kernelized posterior-variance criterion; λ explicitly trades relevance vs diversity.",
            "complementary_systems": "Used together with GRPO for RL training, and optionally with A-TTC achievability filtering to control difficulty.",
            "performance_llm_curriculum": "Used as the curriculum selection engine in TTC-RL; experiments show SIFT-selected curricula (size 1k) lead to faster learning and higher final accuracy than uniform RL post-training baselines selecting 1k tasks at random.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": "SIFT vs simple heuristics: paper shows domain-only sampling (heuristic) underperforms SIFT-based TTC; A-TTC (SIFT constrained by achievability interval) improves performance for weak models versus vanilla SIFT.",
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "Selection uses corpus of ~265k tasks; typical selected curriculum sizes reported at 1k; qualitative top-10 selections shown for specific test questions.",
            "transfer_generalization_results": "SIFT-selected curricula produce specialized models that generalize best to their target benchmarks and related tasks, as indicated by cross-evaluation matrices (block-diagonal structure).",
            "computational_cost": "SIFT computes embeddings for all candidate descriptions (authors precompute embeddings with Qwen3-8B). No per-call API cost reported. Selection is greedy and thus scales with corpus size; authors decontaminate and deduplicate prior to selection to reduce candidates.",
            "failure_modes_limitations": "SIFT depends on quality of embedding space—if embeddings do not represent task relevance well, selection may be suboptimal; λ must be tuned (authors set λ=0.1 and report robustness). Also requires precomputed embeddings and a curated corpus (fixed-corpus limitation).",
            "long_horizon_performance": null,
            "specialized_domain_performance": "SIFT works across domains because it operates on description embeddings; selection successfully produced coding curricula (unit-test tasks) and math curricula (numeric-verification tasks).",
            "ablation_studies": "Authors vary λ and report robustness; A-TTC ablation shows combining SIFT with achievability filtering aids weaker models.",
            "model_size_scaling": "SIFT selection in experiments used embeddings from Qwen3-8B fixed across runs; no multi-size ablation for selection model reported.",
            "key_findings_curriculum_effectiveness": "Embedding-based active selection (SIFT) that balances relevance and diversity yields curricula that accelerate RL specialization and give higher asymptotic performance than uniform or domain-only sampling; combining with difficulty-filtering (A-TTC) is beneficial for weaker agents.",
            "uuid": "e2054.1"
        },
        {
            "name_short": "A-TTC",
            "name_full": "Achievable Test-Time Curricula (A-TTC)",
            "brief_description": "A variant of TTC selection that trades off relevance (SIFT) with online achievability/difficulty estimates to prefer training tasks of appropriate difficulty (e.g., near 50% success rate), improving learning speed for weaker models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "hybrid (LLM-embedding-based selection + online difficulty heuristic)",
            "llm_model_name": "Qwen3-8B",
            "llm_model_size": "8B",
            "curriculum_description": "A-TTC first computes per-task achievability estimates α_x,t (initial priors from model success rates) and updates them online from batch rewards; it restricts the SIFT selection pool to tasks with α in [a_min, a_max] (authors use [0.2,0.6]) so that selected tasks are both relevant and of intermediate difficulty (targeting maximal GRPO advantage ~50% success rate).",
            "domain_name": "Math (AIME, MATH) primarily evaluated; applicable to other verifiable domains",
            "domain_characteristics": "Same as TTC-RL; especially useful when dataset difficulty is poorly calibrated for a weaker initial model.",
            "state_conditioning": true,
            "state_conditioning_details": "Maintains and conditions on online success-rate (achievability) estimates per training task updated from recent batch rewards; uses these to filter candidate tasks for SIFT.",
            "novelty_mechanism": true,
            "novelty_mechanism_details": "Retains SIFT's diversity objective while limiting candidates to those of intermediate difficulty, implicitly encouraging diverse yet appropriately novel/difficult tasks.",
            "complementary_systems": "Used with GRPO RL training; complements SIFT selection; requires initial difficulty priors computed from model success rates.",
            "performance_llm_curriculum": "On a weaker model (Qwen3-0.6B) A-TTC yields higher training reward and improved test scores across MATH benchmarks (AIME24, AIME25, MATH500) compared to vanilla TTC-RL (Figure 8).",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": "A-TTC can be seen as combining SIFT (learned/embedding selection) with a simple heuristic difficulty filter (achievability interval); experiments show this hybrid outperforms plain SIFT for weaker models.",
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "A-TTC enforces a minimum pool size (authors enforce 1,000 candidates) and then selects via SIFT within the filtered difficulty interval; specific diversity counts not enumerated.",
            "transfer_generalization_results": "A-TTC accelerates learning for weak models and produces competitive specialization; no explicit cross-benchmark transfer matrix reported only for base TTC-RL but trends mirror TTC-RL improvements.",
            "computational_cost": "Additional bookkeeping to maintain per-task achievability estimates; authors use a simple recursive update and assume a model for correlated updates—no heavy cost reported.",
            "failure_modes_limitations": "Requires reasonable initial difficulty priors; if priors are poor or dataset lacks sufficient tasks in the target difficulty band, filtering can reduce candidate pool and harm training. Hyperparameters [a_min,a_max] must be chosen appropriately.",
            "long_horizon_performance": null,
            "specialized_domain_performance": "Shown useful in math benchmarks for weaker models; intended broadly applicable.",
            "ablation_studies": "Authors ablate the effect on a weaker model (Qwen3-0.6B) and find achievability balancing increases reward and test performance; recommended interval [0.2,0.6].",
            "model_size_scaling": "Evaluated impact primarily on weaker models (0.6B); no large-scale sweep across many LLM sizes for generator reported.",
            "key_findings_curriculum_effectiveness": "Balancing relevance with online difficulty estimates provides higher learning signal (maximizes GRPO advantage) and accelerates learning for weaker agents; achievability filtering is most useful when dataset difficulty is misaligned with model capability.",
            "uuid": "e2054.2"
        },
        {
            "name_short": "Maj-TTRL",
            "name_full": "Majority-Vote Test-Time Reinforcement Learning (Maj-TTRL)",
            "brief_description": "A test-time RL approach (from Zuo et al., 2025) that uses majority votes of model rollouts on the target tasks as surrogate rewards to train on the test set directly; applicable only to structured-output environments (e.g., numeric or multiple-choice).",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generator_type": "training-on-target-tasks (no explicit curriculum generation)",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "Maj-TTRL trains the model directly on the target benchmark by using the model's majority prediction among multiple rollouts as a pseudo-reward signal, thereby continually improving agreement with the model's consensus; it does not rely on selecting external training tasks from a corpus.",
            "domain_name": "Math (AIME, MATH) and multiple-choice GPQA evaluated; not applicable to coding benchmarks with unit-test verification.",
            "domain_characteristics": "Works for structured output environments where majority-vote consensus is meaningful (numeric answers, multiple-choice); not applicable when verification requires running unit tests or when outputs are open-form.",
            "state_conditioning": false,
            "state_conditioning_details": null,
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Can be combined with TTC-RL (authors show initializing Maj-TTRL from final TTC-RL checkpoint yields best math results).",
            "performance_llm_curriculum": "Maj-TTRL yields significant gains on math benchmarks and is competitive or superior on the strongest model (Qwen3-4B-Instruct-2507) for some math tasks (Table 4): Maj-TTRL gave larger improvements on some math benchmarks where TTC-RL was constrained by the fixed corpus.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": "Maj-TTRL improves performance directly on the target tasks but is less applicable to coding benchmarks where unit-test verification is needed; combining Maj-TTRL after TTC-RL gives additive benefit.",
            "computational_cost": "Requires multiple rollouts per target task to compute majority labels; cost scales with k (number of rollouts) and target-set size. Authors used Maj-TTRL as a specialized baseline and as a fine-tuning step after TTC-RL.",
            "failure_modes_limitations": "Only applicable to structured-output domains where majority voting is a meaningful surrogate; less helpful in GPQA and not applicable to code tasks. Can be limited by the model's initial pass@k ceiling.",
            "long_horizon_performance": null,
            "specialized_domain_performance": "Strong on math benchmarks for strong models; less helpful on GPQA and not applicable for code environments.",
            "ablation_studies": "Authors compare Maj-TTRL alone vs TTC-RL and show Maj-TTRL initialized from TTC-RL checkpoint yields best results on math (Figure 12).",
            "model_size_scaling": "Maj-TTRL was most effective on the strongest model (Qwen3-4B-Instruct-2507) among those evaluated; indicates some dependence on base model capability.",
            "key_findings_curriculum_effectiveness": "Maj-TTRL is a complementary, target-task-focused approach that can outperform TTC-RL when the fixed external corpus is insufficient for further improvement; combining Maj-TTRL with TTC-RL yields the strongest math results.",
            "uuid": "e2054.3"
        },
        {
            "name_short": "TTC-SFT",
            "name_full": "Test-Time Curriculum Supervised Fine-Tuning (TTC-SFT)",
            "brief_description": "A variant where self-curated curricula (retrieved tasks) are used for supervised fine-tuning (SFT) instead of on-policy RL; prior work and authors' experiments show SFT on expert traces can reduce robustness at test-time and cause initial performance collapse.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generator_type": "retrieval-based SFT (human/expert traces retrieved or generated)",
            "llm_model_name": "QwQ-32B (used to generate expert traces in one experiment)",
            "llm_model_size": null,
            "curriculum_description": "TTC-SFT retrieves or constructs expert traces for selected curriculum tasks and fine-tunes the model on these traces (off-policy). The authors tested SFT on the test sets (using expert traces generated by a larger model) and found it unstable for test-time training.",
            "domain_name": "Math (AIME, AMC, GSM8K) evaluated in experiments",
            "domain_characteristics": "Requires high-quality problem solutions / chains-of-thought for supervised targets; susceptible to distribution shift when off-policy traces differ from model's prior behavior.",
            "state_conditioning": false,
            "state_conditioning_details": null,
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Compared against TTC-RL (on-policy) and used OpenThinker3 traces (QwQ-32B) in experiments.",
            "performance_llm_curriculum": "SFT on test traces is unstable: training directly on test set with SFT caused an initial accuracy drop to near 0% and required hundreds of gradient steps to recover and improve (Figure 6). At convergence, SFT can reach strong performance but is less sample-efficient and less robust than TTC-RL for test-time training.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": "SFT tends to overfit to trace style and can hurt generalization early in training; evidence from GSM8K SFT experiments showed drop before overfitting.",
            "computational_cost": "SFT requires storing/generating expert traces (e.g., QwQ-32B traces) and gradient-based fine-tuning; authors report hundreds of gradient steps needed before improvement is observed when training on test-set traces.",
            "failure_modes_limitations": "Off-policy distribution shift causes phase transitions in model behavior and initial collapse; requires expert-quality traces and is impractical at scale for many tasks because it demands solution traces (not just verifiable labels).",
            "long_horizon_performance": null,
            "specialized_domain_performance": "Converged SFT can perform well (OpenThinker3 experiments) but is less robust during initial training; not recommended as a primary TTT mechanism per authors' experiments.",
            "ablation_studies": "Qualitative and quantitative comparisons to TTC-RL showing SFT's initial drop and later overfitting; GSM8K SFT experiments replicate instability.",
            "model_size_scaling": "SFT experiments used larger teacher model traces (QwQ-32B) to produce expert solutions for training smaller models; effect depends on trace quality and student model capacity.",
            "key_findings_curriculum_effectiveness": "Off-policy SFT on self-curated curricula is brittle for test-time training: it can cause an initial catastrophic drop in performance due to distribution shift and is less sample-efficient for rapid specialization compared to on-policy TTC-RL.",
            "uuid": "e2054.4"
        },
        {
            "name_short": "Self-generated curricula (literature)",
            "name_full": "Self-generated training curricula / self-play / automated curriculum generation (literature mentions)",
            "brief_description": "A set of related contemporary works that study LLMs/agents that autonomously generate training problems, self-play curricula, or goals for improving agents (cited as Zhao et al., Huang et al., Chen et al., Fang et al., etc.), typically aimed at scaling RL/self-play ideas to reasoning and coding domains.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated / self-play / automated curriculum (various approaches across cited works)",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "The paper references recent work that generalizes self-play and automated curriculum learning to LLMs by (a) self-generating training tasks or goals and (b) using self-play to iteratively produce increasingly challenging problems. These approaches are cited as complementary and motivating extensions to TTC-RL for moving beyond a fixed corpus to self-generated curricula.",
            "domain_name": "Mentioned across coding and general reasoning domains; some prior work focused on simpler robotic navigation tasks, others on coding or reasoning.",
            "domain_characteristics": "Varied: includes game-like self-play domains (closed systems) as well as open-ended task generation for reasoning/coding; task diversity and open-endedness vary by work.",
            "state_conditioning": null,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "Often combined with self-play, goal-conditioned RL, or automated difficulty scheduling in cited works.",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": false,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Paper notes that prior evaluations of self-generated curricula often focused on simpler domains (e.g., robotic navigation) and that extending to challenging reasoning tasks is nontrivial; also highlights fixed-corpus limitation and need for clean self-generation methods.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Cited as promising directions: self-generated curricula and self-play can create curricula without human curation, but prior work has limited evaluation scope; TTC-RL extends the idea with embedding-based selection from a large human-created corpus and shows strong gains on hard reasoning benchmarks.",
            "uuid": "e2054.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ttrl: Test-time reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Serl: Self-play reinforcement learning for large language models",
            "rating": 2
        },
        {
            "paper_title": "Discover: Automated curricula for sparse-reward reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "R-zero: Self-evolving reasoning llm from zero data",
            "rating": 1
        },
        {
            "paper_title": "Self-improvement in language models: The sharpening mechanism",
            "rating": 1
        }
    ],
    "cost": 0.027238,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LEARNING ON THE JOB: TEST-TIME CURRICULA FOR TARGETED REINFORCEMENT LEARNING
6 Oct 2025</p>
<p>Jonas Hübotter jonas.huebotter@inf.ethz.ch. 
ETH Zürich
Switzerland</p>
<p>Leander Diaz-Bone 
ETH Zürich
Switzerland</p>
<p>Ido Hakimi 
ETH Zürich
Switzerland</p>
<p>Andreas Krause 
ETH Zürich
Switzerland</p>
<p>Moritz Hardt 
Max Planck Institute for Intelligent Systems
TübingenGermany</p>
<p>LEARNING ON THE JOB: TEST-TIME CURRICULA FOR TARGETED REINFORCEMENT LEARNING
6 Oct 2025318F7B7F1DF59DE6E98689AE0AE2FCD2arXiv:2510.04786v1[cs.LG]
Humans are good at learning on the job: We learn how to solve the tasks we face as we go along.Can a model do the same?We propose an agent that assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and applies reinforcement learning to continue training the model for its target task.The test-time curriculum avoids time-consuming human curation of datasets by automatically selecting the most task-relevant data from a large pool of available training data.Our experiments demonstrate that reinforcement learning on a test-time curriculum consistently improves the model on its target tasks, across a variety of evaluations and models.Notably, on challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B by approximately 1.8x on AIME25 and 2.1x on CodeElo.Moreover, we find that TTC-RL significantly raises the performance ceiling compared to the initial model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to 43%.Our findings show the potential of testtime curricula in extending the test-time scaling paradigm to continual training on thousands of task-relevant experiences during test-time.</p>
<p>Figure 1: Test-time curricula (TTCs) lead to remarkable improvements in math and coding by practicing on self-curated task-related problems at test-time.The plots show the pass@1 test accuracy of Qwen3-8B throughout its test-time training.Our method, TTC-RL (solid red line), consistently improves performance, learning faster and achieving a higher final accuracy than standard RL post-training (dashed gray line).Notably, the final pass@1 accuracy of TTC-RL approaches the model's initial pass@8 performance (dotted gray line), which represents a proxy for the performance ceiling of the initial model.The stars indicate the final pass@8 values after TTC-RL, demonstrating a significant improvement over the initial pass@8, which indicates that the model learns new solution strategies at test-time.</p>
<p>INTRODUCTION</p>
<p>We study how large language models (LLMs) can continually improve at reasoning on their target tasks at test-time.Increasing test-time compute, for example, by extended use of context as scratch space, has recently emerged as a key direction for improving LLMs on challenging tasks such as math and coding (Jaech et al., 2024;Guo et al., 2025;Kimi et al., 2025).Test-time scaling has been driven primarily by extensive general-purpose reinforcement learning (RL; Guo et al., 2025), where the LLM learns how to effectively use its context for reasoning.However, since the context of LLMs is bounded and becomes exceedingly expensive to expand, an LLM cannot learn in-context from experience over long timeframes.</p>
<p>One promising technique for overcoming this challenge is test-time training (TTT; Sun et al., 2020;Hardt &amp; Sun, 2024), which continues training the model at test-time after being given a task.</p>
<p>Previous work has studied TTT via supervised fine-tuning on human-created or expert data, either retrieved (Hardt &amp; Sun, 2024;Hübotter et al., 2025) or provided as few-shot examples (Akyürek et al., 2025).Other work has instead focused on TTT in the context of recurrent neural networks (Sun et al., 2025;von Oswald et al., 2025;Zhang et al., 2025b), aiming to replace the costly attention-based context in Transformers (Vaswani et al., 2017) with a fixed-size state (i.e., the model itself), but losing some of the advantages of reasoning over an uncompressed scratchpad.We explore a complementary approach to test-time scaling, where an LLM is continually trained on self-curated training tasks related to its target task, while practicing on each individual training task in-context.This leverages the Transformer's attention as an uncompressed scratchpad for short-term ideation, while meta-learning strategies for leveraging that context across long-term, task-specific experience.</p>
<p>Figure 2: TTC-RL performs targeted practice on similar problems to the target task at test-time.The agent is given a target task (red) and self-curates a curriculum of related tasks (blue).It then explores solution strategies on this curriculum, reinforcing successful approaches (✓).This experience enables the agent to more effectively solve the original, more difficult target task.</p>
<p>We propose a test-time curriculum (TTC) agent that automatically designs its own curriculum of training tasks by selecting the relevant tasks for the job from a large corpus of existing tasks.The agent then attempts tasks in its curriculum, and compresses the gathered experience into its weights via RL.The automatic self-guided curriculum design avoids laborious human curation of datasets, and enables training on purpose-built curricula at test-time.We find that this reinforcement learning on test-time curricula (TTC-RL) leads to remarkably improved reasoning on target tasks.In particular, we find that TTC-RL improves the pass@1 of several strong LLMs across diverse reasoning tasks, covering competition math, coding, and scientific reasoning (cf. Figure 1).We further identify that TTC-RL is complementary to other means of test-time scaling, effectively improving pass@k and maj@k even at large k.Notably, we find that TTC-RL can overcome the limitation of fixed context windows by observing that a non-thinking model (limited to 8k context tokens) with TTC-RL can perform similarly to the same model thinking for 30k tokens in-context.This demonstrates that during TTC-RL, the model continues learning how to think effectively for its target tasks.Our results suggest such targeted RL as a promising new direction for LLM agents that continually improve at test-time through many interactions with an environment.</p>
<p>We summarize our contributions as follows:</p>
<ol>
<li>
<p>We propose a TTC agent for targeted RL ( §3): We propose a test-time curriculum agent which at test-time when given a target task, self-selects related training tasks from a diverse corpus.The agent then learns from its own experience of attempting those tasks via RL.</p>
</li>
<li>
<p>TTC-RL improves reasoning on target tasks ( §4): Across several models and tasks, TTC-RL consistently improves pass@1 substantially faster than general-purpose RL post-training on standard RL datasets, and saturates at a higher accuracy.Next, we identify that TTC-RL substantially raises the performance ceiling of the model (pass@k) and demonstrate that it is complementary to existing approaches to test-time scaling.Finally, we find that TTC-RL yields strongly specialized models that perform remarkably well on their target tasks, even when compared to models that are allowed to think for tens of thousands of tokens in context.3. Measuring latent improvements in reasoning ( §5): The evaluation of RL-trained models faces the challenge of estimating whether improved scores are due to better reasoning or merely learning the expected output format.We introduce a new metric, latent improvement, which computes a lower bound on the improvement in reasoning due to RL training, and find that TTC-RL leads to substantial improvements in "latent" reasoning.</p>
</li>
</ol>
<p>RELATED WORK</p>
<p>Test-time scaling and general-purpose RL training.A common strategy for improving LLM performance in challenging domains is to allocate additional test-time compute, for instance, through majority voting (Snell et al., 2025), search with a reward model (Lightman et al., 2023;Wang et al., 2024a;Setlur et al., 2025a), or by identifying consistent patterns among parallel rollouts (Wang et al., 2023;Huang et al., 2025a).The potential of such methods is often measured by pass@k, which describes the performance ceiling with k generations (Chen et al., 2025b).More recently, scaling test-time compute via in-context "reasoning" (Brown et al., 2020;Wei et al., 2022) has significantly improved performance in domains like math and coding (Jaech et al., 2024).This capability is commonly enabled by large-scale, general-purpose RL training on diverse tasks (Lambert et al., 2024;Ma et al., 2025;Guo et al., 2025;Kimi et al., 2025), during which models learn to reason within their bounded context (Setlur et al., 2025b), which connects to the broad topic of meta-learning (Schmidhuber, 1987;Duan et al., 2017;Finn et al., 2017).This paradigm is related to goal-conditioned RL (Schaul et al., 2015;Andrychowicz et al., 2017) where several works have studied automatic curriculum learning (Warde-Farley et al., 2018;Pitis et al., 2020;Pong et al., 2020), first proposed by Bengio et al. (2009).In contrast to improving general-purpose models, our work employs RL to train specialized reasoners for a particular target task at test-time.</p>
<p>Self-play.A specialized form of curriculum learning has proven highly successful in domains like games through the use of self-play (Schmidhuber, 1991;Silver et al., 2016), where an agent is repeatedly challenged by playing against itself.Seminal works show that this approach can lead to superhuman performance (e.g., Mnih et al., 2015;Silver et al., 2016;2017;Berner et al., 2019).Several recent works aim to generalize this paradigm to LLMs and more general domains such as coding by self-generating a training curriculum (Zhao et al., 2025;Huang et al., 2025b;Chen et al., 2025a;Fang et al., 2025).While recent work has studied test-time curricula as an extension of self-play to goal-conditioned RL settings (Diaz-Bone et al., 2025), its evaluation has focused on simple robotic navigation tasks.We extend this line of work to challenging reasoning tasks by self-curating a training curriculum, enabling LLMs to continually learn from extensive experience on a single task (Silver &amp; Sutton, 2025;Shen et al., 2025).</p>
<p>Test-time training and test-time RL.</p>
<p>Training a model at test-time for a given input has been widely studied as TTT (Sun et al., 2020), using supervised (Hardt &amp; Sun, 2024;Hübotter et al., 2025;Yu et al., 2025a;Bertolissi et al., 2025;Bagatella et al., 2025a) or self-supervised losses (Sun et al., 2025;Dalal et al., 2025).Several methods perform TTT in a purely unsupervised manner, i.e., without "real-world" data or feedback (Wang et al., 2021;Zhang et al., 2022).Most relevant to our work, Zuo et al. (2025) recently extended unsupervised TTT to perform RL on the test set, leveraging the model's majority votes as pseudo-labels.This connects to a broader theme of unsupervised RL (Zhang et al., 2025a;Shao et al., 2025;Zhou et al., 2025;Prabhudesai et al., 2025) and self-improvement in LLMs (Zelikman et al., 2022;Gulcehre et al., 2023;Lee et al., 2025).</p>
<p>TEST-TIME CURRICULA</p>
<p>We consider the set of target tasks D ⋆ = {x ⋆ 1 , . . ., x ⋆ M } given at test-time, and our goal is to specialize an existing model through further training to those tasks.For training, as in general-purpose RL, we rely on an existing large corpus of training tasks D = {(x i , v i )} N i=1 , for each of which v i (•) ∈ {0, 1} verifies whether an attempt was correct.To specialize, it is common practice to construct a particular subset D ⋆ from D, and we call such a targeted subset a test-time curriculum for D ⋆ .We seek to make test-time training on such a curriculum scalable.To this end, we propose to go beyond human-curated test-time curricula and let the initial model craft its own test-time curriculum.</p>
<p>The previous works of Hardt &amp; Sun (2024) and Hübotter et al. (2025) have studied self-curated testtime curricula with supervised fine-tuning (SFT), and have shown that this can improve language modeling, i.e., lead to lower perplexity.However, this approach is limited since it requires the corpus to specify how training tasks are to be solved-not only to verify whether a solution is correct.Moreover, mirroring recent observations on the robustness of on-policy RL (Shenfeld et al., 2025), we observe that SFT on expert traces often leads to an initial drop in performance on downstream tasks, suggesting that SFT is ill-suited for TTT with LLMs.We provide further details in Appendix A.</p>
<p>AUTOMATIC TTCS FOR TARGETED RL</p>
<p>We therefore focus on on-policy RL and extend the previous work on automatic data selection for TTC-SFT (Hardt &amp; Sun, 2024;Hübotter et al., 2024;2025) to automatic task selection in TTC-RL.We adopt SIFT (Hübotter et al., 2025), which selects those examples from the corpus that the model deems most informative for the target tasks.SIFT has a hyperparameter λ, which reflects the models' ability to learn from the seen examples, and which explicitly trades between diversity of the selected examples and their relevance to the target tasks.We find that our results are robust to the choice of λ and generally set λ = 0.1 in our experiments.To determine which examples are most informative, SIFT leverages a latent representation space ϕ of token sequences for which we use the normalized last-token last-layer embeddings of the initial model.Appendix F gives examples for such self-curated test-time curricula.</p>
<p>Algorithm 1 Test-Time Curriculum for Targeted RL
Require: Test tasks D ⋆ 1: {(x t , v t )} ← SIFT λ,ϕ,T,D (D ⋆ ) ▷ select curriculum 2: for t = 0, 1, . . . , T − 1 do 3: {ŷ t+1,i } ∼ π t (• | x t+1 ) ▷ attempt 4: {r t+1,i } ← v t+1 ({ŷ t+1,i }) ▷ verify 5: θ t+1 ← GRPO(θ t , {ŷ t+1,i }, {r t+1,i }) ▷ RL step 6: end for
This pipeline leverages the semantic understanding of the initial model to self-curate a test-time curriculum for the target tasks.We then train on this test-time curriculum via GRPO (Shao et al., 2024), as shown in Algorithm 1.1 Note that test-time training does not necessitate the model to stay close to its initialization since it needs to generalize only to its target tasks, and hence, we omit the KL penalty of GRPO.We include background on SIFT and GRPO in Appendix B. In an extension, we evaluate a test-time curriculum that automatically selects tasks of the right difficulty, which we show to further accelerate learning on weaker models (cf.Appendix C).</p>
<p>A DIVERSE CORPUS FOR GENERAL-PURPOSE RL POST-TRAINING</p>
<p>To study the effectiveness of our proposed adaptive test-time curriculum, we leverage a large corpus of high-quality verifiable training data, suitable for post-training a model across diverse domains.We assemble a new meta-dataset, which we call the verifiable-corpus and which combines approximately 265k diverse training tasks, spanning three environments:</p>
<p>• Exact answer match / Math: For math problems with a numerical answer, we determine answer equivalence using math-verify.Our corpus contains the training splits of GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b), and the DAPO math dataset (Yu et al., 2025b), covering numerically verifiable math problems for a wide range of difficulties.• Judged answer match / General reasoning: Measuring the validity of complex reasoning requires more robust verification than symbolic equivalence checks.Given a (potentially long) golden answer, we use a 1.5B-parameter verifier model trained by Ma et al. (2025) to determine whether attempted and golden answers are semantically equivalent.Our corpus contains the Webinstruct-verified dataset (Ma et al., 2025), which covers a wide variety of subjects ranging from natural sciences to history.• Unit tests / Code: Finally, we combine several sources of coding tasks.Each coding task is verified by a set of unit tests.Our corpus combines tasks from APPS (Hendrycks et al., 2021a), code contests (Li et al., 2022), TACO (Li et al., 2023), PrimeIntellect (Mattern et al., 2025), Leetcode (Xia et al., 2025), the Codeforces training split (Penedo et al., 2025) and all LiveCodeBench tasks (Jain et al., 2024) prior to February 1, 2025.We perform a filtering step where we remove training tasks with empty answers or less than 5 unit tests, to ensure a reliable training signal.Finally, we deduplicate and decontaminate the corpus, as detailed in Appendix E.1.We openly share the corpus and our environment implementations to support future research.To our knowledge, the verifiable-corpus is one of the first public corpora of high-quality verifiable tasks, spanning several domains and environments.We envision that, building on this work, future efforts will ultimately enable TTC agents to utilize any relevant training tasks they find on the web (similarly to retrieval-augmented generation; Lewis et al., 2019), or to self-generate their own training tasks (see, e.g., Zhao et al., 2025).</p>
<p>RESULTS</p>
<p>We focus our evaluation on a diverse set of target tasks in math, coding, and scientific reasoning.Specifically, we evaluate test-time curricula for high-school-level competition math questions in AIME 24 &amp; 25 and MATH500 (Hendrycks et al., 2021b).We evaluate coding ability on Codeforces (Penedo et al., 2025), CodeElo (Quan et al., 2025), and on LiveCodeBench v6 (Jain et al., 2024), i.e., tasks released after February 1, 2025.Finally, we evaluate scientific reasoning with GPQA-Diamond (Rein et al., 2024) which covers questions in biology, physics, and chemistry.</p>
<p>TTC-RL can be applied to each task within a benchmark individually or to the entire benchmark on aggregate, treating it as a set of target tasks.We primarily evaluate TTC-RL per-benchmark as this yields greater statistical significance under a limited compute budget.We then perform an ablation, indicating that per-task TTCs performs at least on-par with per-benchmark TTCs (cf.Section 4.2).</p>
<p>To ensure that our evaluation is accurate, we adopt evalchemy (Raoof et al., 2025) and synthesize system prompts to be consistent across benchmarks (cf.Appendix E.2).We generally train for two episodes with batch size 8 and 16 rollouts per train task,2 and measure avg@4 on the set of test tasks once every ten steps.To further reduce noise, we compute a moving average across three validation steps.Finally, in our summarized numeric results, we report the highest averaged avg@4, and include detailed plots of avg@4 per step in Appendix D.2.</p>
<p>We perform our main evaluation on the non-thinking models Qwen3-8B (Yang et al., 2025) and the more recent Qwen3-4B-Instruct-2507, whose responses we limit to 8192 tokens.We additionally evaluate on the Qwen3-8B base model.We opt for non-thinking models due to the high computational cost of running thinking models over long contexts, typically of up to 32k tokens.The goal of our TTC framework is to show that models can improve at test-time, even without further expanding their context.We hypothesize that our results extend to thinking models, which simply have a larger maximum response length.Middle: TTC-RL also improves the performance of majority voting (across math and GPQA-D), with the initial pass@1 significantly outperforming maj@64 on the initial model.Right: We evaluate Qwen3-8B in non-thinking and thinking mode, as well as the non-thinking model + TTC-RL.The color indicates the relative accuracy per column.We find that TTC-RL significantly improves the non-thinking model, allowing it to perform close to the thinking variant in several domains, despite reasoning over 8k rather than 30k context tokens.Main results.We summarize our main results in Fig- ure 1 and Table 1.We find that TTC-RL significantly improves accuracy across a range of models and all benchmarks.Notably, it also leads to significant performance gains on top of Qwen3-8B-Base within only relatively few RL steps, indicating that TTCs lead to sample-efficient training.Our main baseline is a model that is trained on 1k uniformly chosen training tasks from the corpus, to which we refer to as standard "RL post-training", since this method yields a general-purpose model.We compare this to TTC-RL with a curriculum of size 1k and find that training on a test-time curriculum accelerates learning significantly and leads to saturation at substantially higher performance. 3Notably, Qwen3-8B with TTC-RL performs on-par with strong closed-source non-thinking models; for example, it approximately matches GPT-4o-2024-08-06 on LCB v6 .In Figure 3, we further ablate the size of the curriculum and find that TTC-RL consistently outperforms general-purpose RL post-training across a wide range of curriculum sizes.Interestingly, at dataset size 1-though performing poorly-the general-purpose RL post-training outperforms TTC-RL.We suspect that this may result from TTC-RL picking a practice task that is very similar to the test tasks, in which case overfitting may harm more than when overfitting to a less related task.</p>
<p>Takeaway 1 TTC-RL substantially improves accuracy on a wide variety of models and benchmarks, compared to a model's initial performance and after (continued) RL post-training on our corpus.</p>
<p>TTCS ARE COMPLEMENTARY TO EXISTING APPROACHES TO TEST-TIME SCALING</p>
<p>Next, we demonstrate that TTC-RL improves the LLM's ability for test-time scaling.</p>
<p>TTCs raise the model's performance ceiling.While the improvement in accuracy demonstrates that during TTC-RL, the model learns to better reason within context, we ask whether the model improves more broadly.A common metric to understand a model's "performance ceiling" for testtime scaling is the pass@k metric, which measures whether any one of k attempts is correct (Chen et al., 2025b).Recent work has repeatedly shown that RL-training tends not to improve pass@k at large k (Yue et al., 2025), leading to the concern that RL-training is simply "distilling" pass@k into pass@1.In Figure 4 (left), we instead observe that TTC-RL significantly improves pass@k across a wide range of k.Similarly, TTC-RL also improves the realized performance gains of majority voting, as can be seen in Figure 4 (middle), and notably increases the pass@1 well beyond the maj@64 after continued RL post-training.Our results indicate that two key factors lead to the performance of TTC-RL: Improvements to the RL training algorithm that also apply to our general-purpose RL-training baseline, as well as the specific data selected by the TTC agent, as indicated by the strong improvement in majority voting.We provide a more detailed discussion in Appendix D.1.Developing a better understanding of the circumstances under which RL-training can "discover new behavior", leading to improved pass@k, is an exciting direction for future research.</p>
<p>TTC-RL with a short-context LLM can perform close to a long-context LLM.We also seek to better understand how TTC-RL relates to reasoning over long contexts.To this end, we evaluate the non-thinking and thinking variants of Qwen3-8B, limited to 8k and 30k tokens per response, respectively.In Figure 4 (right), we find that TTC-RL on the non-thinking model performs close to the thinking model in several domains, particularly in coding and GPQA. 4 Further, note that the asymptotic cost of growing context in a Transformer is quadratic (Vaswani et al., 2017), whereas the asymptotic cost of TTC-RL is linear (since experience is compressed into the model's weights).This suggests that there is a regime in which, given a fixed compute budget, TTC-RL outperforms further scaling of context size.We believe that studying this compute-optimal Pareto frontier is an exciting topic for future research.Our results indicate that to further improve the performance of LLMs, test-time curricula may eventually be advantageous over continued scaling of context size.</p>
<p>Takeaway 2</p>
<p>Test-time curricula substantially increase the pass@k performance ceiling of a model and can perform similarly to models which are reasoning over a much larger context.This indicates the potential of TTCs to complement existing approaches to test-time scaling.</p>
<p>TTCS EFFECTIVELY SPECIALIZE MODELS</p>
<p>To determine whether the test-time curriculum specializes the model to its target tasks, we conduct a straightforward experiment: We evaluate each final checkpoint of TTC-RL on all benchmarks, including those that were not part of the set of target tasks.We summarize the results in Figure 5 (right), with columns corresponding to evaluation and rows corresponding to training.We find that after TTC-RL, models perform best on their target tasks, while severely underperforming on tasks that are unrelated to the target tasks.Moreover, we identify a block-diagonal structure, where models generalize better across mutually related groups of tasks, particularly among similar math benchmarks.We also find that models appear to generalize better from coding to math than vice versa, and models generalize better from code and math to GPQA than vice versa.</p>
<p>TTCs for individual tasks.Aspirationally, we anticipate test-time curricula to enable continual learning for a single test task over a long timeframe.While we focus our main evaluation on the setting where test-time curricula are applied per benchmark, we run an ablation with 30 separate TTCs-one per AIME 25 question.The results in Figure 5 (left) demonstrate that specializing to an individual test task can outperform a broader specialization to a group of test tasks.This shows that TTC-RL does not depend on a larger set of test tasks to implicitly lead to diverse data and robust training, and instead seamlessly extends to a fully test-time setting with only a single task given.We find, however, that more fine-grained specialization does not always lead to further performance gains.We evaluate training separate TTCs for each of biology, physics, and chemistry in GPQA, leading to approximately the same performance as a joint TTC.In our view, gaining a better understanding for "how much" specialization is helpful is an exciting direction for further research.</p>
<p>Takeaway 3</p>
<p>Test-time curricula effectively specialize the model to their target tasks.When applied to an individual target task, TTC-RL can be seen directly as a method for test-time scaling.We perform TTC-RL and maj-TTRL (cf.Section 5.2) on Qwen3-8B, and find that per-task TTC-RL even outperforms the benchmark-level TTC.Middle: TTC-RL improves "correctness" of reasoning, not only learning the answer format.We evaluate the difference in accuracy between TTC-RL and the initial Qwen3-8B, averaged over benchmarks.The latent improvement is a lower bound on the accuracy gain that is not due to merely learning the format (cf.Section 5.1).Right: TTC-RL yields models that are specialized to their target tasks.We plot the accuracy of Qwen3-8B trained for given target tasks (rows) when evaluated on other benchmarks (columns).We normalize accuracies across all evaluations of a particular benchmark.Notably, the model trained via TTC-RL for the "right" target tasks (i.e., the diagonal) always performs best.</p>
<p>FURTHER ANALYSIS</p>
<p>ESTIMATING "REAL" IMPROVEMENT</p>
<p>When evaluating RL-trained models on verifiable tasks, a reasonable concern is whether the model simply learns to adhere to the expected output format.Indeed, we find that if the initial model is not able to consistently produce well-formed responses, RL-training tends to quickly teach the model the expected output format.Therefore, disentangling shallow learning of format from improvements in a model's "latent" reasoning is critical for accurate evaluation.Ideally, we would like to measure whether the model's reasoning improves throughout training-regardless of whether we can automatically parse and evaluate responses.</p>
<p>We propose to measure a model's latent improvement (LI) during RL training as follows.Consider the event of an answer being marked as "accurate" by the verifier, which occurs if it is "well-formed" (i.e., it can be extracted and interpreted) and if the model's latent reasoning is "correct".Based on this, a straightforward lower bound on correctness is simply P(correct) ≥ P(accurate).To measure the improvement in correctness throughout RL training, we make the following intuitive assumption:</p>
<p>Assumption 1.We assume that being well-formed does not reduce the chance of being correct.Formally, we assume P(correct | well-formed) ≥ P(correct), i.e., a non-negative association of formedness and correctness.</p>
<p>Intuitively, this assumption states that an ill-formed response does not increase the likelihood of correct latent reasoning.This yields a straightforward upper bound on the probability of correct latent reasoning: P(correct) ≤ P(accurate)/P(well-formed) if P(well-formed) &gt; 0. Thus, the improvement in correctness after T RL steps is lower bounded as Latent Improvement := P(correct T ) − P(correct 0 ) ≥ P(accurate T ) − P(accurate 0 ) P(well-formed 0 )</p>
<p>.</p>
<p>Measuring latent improvement.We consider a response as ill-formed if we cannot extract an answer, e.g., because the response was truncated at the max-token limit or because the completed response did not contain an extractable answer.We note that to reliably measure LI, it is essential</p>
<p>to ensure that answer extraction is strict. 5In Figure 5 (middle), we measure the latent improvement of Qwen3-8B, and find that under Assumption 1, TTC-RL leads to a substantial latent improvement.We include our complete results in terms of LI in Table 7 of Appendix D.</p>
<p>TOWARDS CONTINUAL SELF-IMPROVEMENT AT TEST-TIME</p>
<p>We consider this work as a first step towards agents that continue learning at test-time and specialize without requiring human supervision.The recent work of Zuo et al. (2025) can also be seen as a step in this direction by proposing to train on the test set directly, using majority votes as surrogate rewards ("maj-TTRL").Since Maj-TTRL relies on majority votes as its training signal, it can be applied only to environments with structured outputs such as our math environment with numerical answers or the multiple choice GPQA.In contrast, our proposed TTCs can be applied in any environment where a reward signal can be defined.We perform a comparison to Zuo et al. (2025) in Table 2 and find that Maj-TTRL leads to significant gains in accuracy across math benchmarks, but helping less in GPQA.We emphasize that Maj-TTRL and test-time curricula are complementary approaches, e.g., one can perform Maj-TTRL directly after TTC-RL, which we find to outperform Maj-TTRL alone (cf. Figure 12 in Appendix D.4).Zhao et al., 2025;Zweiger et al., 2025), which may be combined with or extend TTCs.</p>
<p>ON CONTAMINATION AND REWARD HACKING</p>
<p>The performance gains from TTC-RL are remarkable: for example, in AIME24 and CodeElo, the pass@1 of the strong Qwen3-8B more than doubles within only a few hundred training steps.This naturally raises the question of potential confounding factors.To mitigate this risk, we took several steps: we extensively decontaminated our corpus by removing tasks that overlap with the test sets, implemented safeguards against reward hacking within our code environment, and manually reviewed several model responses.While we base our evaluation on the widely used evalchemy package (Raoof et al., 2025), we found a significant flaw in the evaluation of Codeforces and CodeElo, where some (and frequently all) private test cases were leaked into the prompt as "examples".This enables a strong model to "solve" a task simply by handling each test case individually.To mitigate this, we removed all input/output examples from the prompts of Codeforces and CodeElo, and also ensured that private test cases are not leaked in tasks from our training corpus.</p>
<p>A remaining limitation is that we cannot guarantee the cleanliness of the model's original pretraining data.To account for this possibility, we evaluate on LCB v6 , which consists of coding tasks that were released since February 2025.Hence, TTC-RLs performance gains on LCB makes pre-existing contamination a less likely explanation for our results.Furthermore, we compare TTC-RL to an oracle that trains directly on the test tasks, finding that our method learns slightly more slowly and levels off at a lower accuracy (cf. Figure 14 in Appendix D).We believe our findings on the importance of data selection (cf. Figure 1) and improvements to the RL training algorithm to facilitate exploration (cf.Appendix D.1) offer plausible explanations for these results.We further include qualitative examples demonstrating the improvements in reasoning in Appendix F.</p>
<p>DISCUSSION</p>
<p>We propose a test-time curriculum agent that self-curates a sequence of training tasks to specialize towards a specific target task via reinforcement learning.We demonstrate that TTCs achieve remarkable performance gains across multiple models and diverse reasoning benchmarks, significantly raising the performance ceiling of strong initial models through specialization to their target task.To better evaluate these gains, we introduce the "latent improvement" metric, which measures genuine improvements in reasoning correctness.Our experiments confirm that TTCs yield substantial gains in latent improvement.</p>
<p>This highlights the potential of a currently underutilized compute regime: targeted test-time training, which sits between large-scale general-purpose training and frozen test-time scaling.While standard next-token prediction relies on a model's intuition and reasoning allows it to leverage context for deliberation, our proposed test-time curriculum enables the model to meta-learn how to reason for a particular target task at test-time.Similarly, when humans begin a new job, they often train for weeks or months before being able to solve all required tasks.During this time, they collect experience on dozens of tasks that are similar, becoming more efficient at solving their jobs' target tasks.</p>
<p>In demonstrating the potential of such targeted test-time training, our work opens up several exciting research directions.A natural direction is to move beyond the bottleneck of a fixed task corpus through self-generated TTCs, which may still use human-created tasks as inspiration.Further avenues include improving the sample-and step-efficiency of TTC-RL through advancing methods for RL training.This also raises questions about scaling laws for this new regime: for instance, at what context length does it become more advantageous to scale TTC-RL rather than increasing the context window?Looking beyond single-task specialization, TTCs might be extended to dynamic settings where an agent must adapt to an evolving set of target tasks.Finally, TTC-RL could be used to unconfound benchmark evaluations by providing a standardized method for specializing all models to a test task (Dominguez-Olmedo et al., 2025), enabling a fairer comparison of their core capabilities.While we focus on RL-training with a test-time curriculum, the prior works of Hardt &amp; Sun (2024) and Hübotter et al. (2025) have proposed to instead perform supervised fine-tuning on human-produced data (TTC-SFT), retrieved from a large corpus.Next to being impractical since requiring reasoning traces for training tasks, we make the observation that the distribution-shift of off-policy SFT appears to make it fundamentally ill-suited for test-time training of LLMs.To test this, we train a Qwen2.5-7B-Instructmodel (Qwen et al., 2024) on the test sets of the AMC23 and AIME25 math competitions, using expert traces generated by QwQ-32B (Qwen, 2025) using the SFT pipeline from OpenThinker3 (Guha et al., 2025).OpenThinker3-7B is simply the fine-tuned Qwen2.5-7B-Instruct when trained to convergence on a curated training set of QwQ-32B (Yang et al., 2025) traces (Guha et al., 2025).Although OpenThinker3 demonstrates that at convergence, an SFT-trained Qwen2.5-7B-Instruct can achieve strong performance, Figure 6 shows that even when training directly on the test set, it takes hundreds of gradient steps before the accuracy starts to increase, while initially dropping to close to 0%.Intuitively, even though perplexity decreases smoothly throughout training, the model's behavior undergoes phase transitions, and begins by only reproducing superficial reasoning patterns such as repeatedly generating "Wait, ...":</p>
<p>CONTRIBUTIONS</p>
<p>Excerpts from reasoning traces for AIME 25 after 200 SFT steps . . .be 2025.Wait, actually, actually, actually, actually, actually, actually, actually, actually, actually, actually, . . . . . .numerator.\n\nWait,numerator numerator is numerator denominator * denominator numerator.\n\nWait,numerator numerator . . .This phenomenon is closely related to recent observations that off-policy SFT appears to induce a greater distribution shift of the policy than on-policy RL (Shenfeld et al., 2025), indicating that TTC-SFT is less robust and may even be less efficient than TTC-RL.</p>
<p>Validating this phenomenon with expert solutions in GSM8K.To validate that this behavior is because of the behavior SFT rather than the specifics of QwQ-32Bgenerated reasoning traces, we perform a similar experiment on GSM8K (Cobbe et al., 2021).GSM8K's test set has not only verified numerical answers, but also humangenerated expert reasoning traces.In Figure 7, we train Llama-3.2-1B-Instruct on these expert solutions with SFT and observe that after the first full epoch, performance on the GSM8K test set has dropped.At first this may seem puzzling, but looking at a qualitative example suggests that the model first learns superficial cues (the human-generated GSM8K solutions are very succinct) before in this case overfitting to the SFT data:</p>
<p>Attempt of a GSM8K question after 1 epoch of SFT on the test set</p>
<p>Problem</p>
<p>There are three trees in Eddy's backyard.The shortest tree has a height of 6 feet, and the second tree has a height of 5 feet more than the shortest tree.The height of the tallest tree is twice the height of the two trees combined.How tall is the tallest tree?</p>
<p>Ground Truth</p>
<p>Tallest Tree: 108 feet Middle sized tree:(108/2)-6=48 feet Smallest tree:48/4=12 feet</p>
<p>Initial Answer</p>
<p>The tallest tree is 108 feet.The middle-sized tree is 6 feet less than half the height of the tallest tree.Half of 108 is 54.So the middle-sized tree is 54 -6 = 48 feet tall.The smallest tree is 1/4 the height of the middle tree, which is 1/4 x 48 = 12 feet tall.The final answer is 12</p>
<p>Final Answer</p>
<p>The middle tree is 108/2 -6 = 60 feet tall.The smallest tree is 60/4 = 15 feet tall.The final answer is 15</p>
<p>Hyperparameter Value</p>
<p>Learning rate 1e-5 Batch size 32 Max.sequence length in tokens 16384 Packing No Adam's β-values (0.9, 0.999) Table 3: Hyperparameters for SFT training on the test sets of AMC23 and AIME25.This corresponds to the "micro" configuration of OpenThinker (Guha et al., 2025).</p>
<p>B BACKGROUND B.1 SIFT</p>
<p>Several works studied how to optimally select data for imitation learning, e.g., the early seminal work of MacKay (1992) and recent extensions (Hübotter et al., 2024;2025;Bagatella et al., 2025b).</p>
<p>SIFT is an active learning selection method that accounts for information duplication and optimizes overall information gain to produce diverse and informative examples (Hübotter et al., 2025).</p>
<p>Given a feature map ϕ, we define the inner-product kernel k(x, x ′ ) := ϕ(x) ⊤ ϕ(x ′ ).SIFT greedily selects data from a corpus D to minimize a measure of uncertainty about how to respond to a specific prompt x ⋆ .This uncertainty (posterior variance) given a selected set X is quantified as:
σ 2 X (x ⋆ ) := k(x ⋆ , x ⋆ ) − k ⊤ X (x ⋆ )(K X + λI) −1 k X (x ⋆ ),(2)
where K X is the kernel matrix of X, k X (x ⋆ ) is the vector of kernel evaluations between the inputs in X and x ⋆ , and λ &gt; 0 is a regularization coefficient.</p>
<p>SIFT iteratively selects the next point x n+1 by greedily minimizing this posterior uncertainty:
x n+1 := arg min x∈D σ 2 Xn∪{x} (x * ).(3)
The regularization coefficient λ modulates the trade-off between relevance (favored by large λ) and diversity (favored by small λ).Full details, including theoretical guarantees and empirical results, are presented in the SIFT paper (Hübotter et al., 2025).</p>
<p>B.2 GRPO</p>
<p>For RL-training, we adopt GRPO (Shao et al., 2024) without a KL penalty.For a specific training task x, the behavior policy π θold samples a group of G individual responses {o i } G i=1 .Then, we calculate the advantage of the i-th response by normalizing the group-level rewards {r i } G i=1 :
Âi,t = r i − mean({R i } G i=1 ) std({R i } G i=1 ) .(4)
GRPO then maximizes a clipped objective:
J GRPO (θ) = E x∼ D ⋆ ,{oi} G i=1 ∼π θ old (•|x) 1 G G i=1 1 |o i | |oi| t=1 min w i,t (θ) Âi,t , clip(w i,t (θ), 1 − ϵ low , 1 + ϵ high ) Âi,t ,(5)
with importance weights
w i,t (θ) = π θ (o i,t | x, o i,&lt;t ) π θold (o i,t | x, o i,&lt;t ) . (6)
Maximizing the learning signal in GRPO.When training on a selected dataset we aim to provide maximal learning signal to the model.One simple way to determine whether a provided data sample provides useful information is via the norm of GRPOs gradient.The gradient of the GRPO objective, in the on-policy setting (π θ = π θold ) is given by:
∇ θ J GRPO (θ) = 1 G G i=1 1 |o i | |oi| t=1 Âi,t ∇ θ log π θ (o i,t | x, o i,&lt;t )(7)
This formulation reveals that the advantages Âi,t are closely tied to the gradient norm of GRPO, ∥∇ θ J GRPO (θ)∥.Intuitively, by selecting data with high absolute advantage we maximize the gradient norm and provide a strong learning signal to the model.</p>
<p>In the sparse-reward setting for a fixed question x, the reward is distributed according to a Bernoulli distribution R ∼ Ber(p x ).The expected absolute advantage for this question can be derived as follows, where we assume G → ∞ for simplicity:
E [|A|] = E |R − E[R]| σ(R) = p x 1 − p x σ(R) + (1 − p x ) p x σ(R) = 2 p x (1 − p x )(8)
Therefore, the absolute advantage is maximized for p x = 1 2 .This simple argument suggests that, in order to maximize the learning signal, we should choose questions on which the current model has success rate 50%.</p>
<p>C AUTOBALANCING ACHIEVABILITY WITH TTC'S</p>
<p>The goal of a targeted test-time curriculum is to teach the LLM skills that are directly useful for solving the target tasks.Naively selecting the test-time curriculum, however, may result in training tasks that are either too easy or too hard for the current model.Prior work on curricula for sparsereward reinforcement learning (e.g., Pitis et al., 2020;Zhao et al., 2025;Huang et al., 2025b;Diaz-Bone et al., 2025) has shown that selecting tasks at an appropriate level of difficulty can dramatically accelerate learning.In line with these findings, we demonstrate that balancing task relevance with task difficulty can lead to a better-performing TTC if the model is initially significantly weaker than required to solve most target tasks.Intuitively, a success rate of 50% provides the most detailed differentiation as to which approaches work.Indeed, in expectation, a success rate of 50% leads to the largest possible absolute advantage in GRPO (cf.Appendix B.2), which implies a large gradient norm and a strong and informative learning signal for the model.</p>
<p>Estimating the success rate online.This raises the question of how to estimate the difficulty α x t of a given training task x from the corpus at time t.We assume access to an initial estimate of difficulty α x 0 ∈ (0, 1).We then update α x t recursively to "track" the approximate success rate of the model for each question:
α x t+|B| := r x t+|B| if
x was within the last batch
σ(σ −1 (α x t ) + σ −1 (∆ t+|B| )) otherwise, (9)
where ∆ t+|B| is the mean reward across the batch and σ(z) = 1/(1 + e −z ) the sigmoid function.</p>
<p>Intuitively, if ∆ &gt; 0.5, the achievability estimate of all unseen questions is increased, indicating that tasks are becoming easier for the agent.Conversely, if ∆ &lt; 0.5, the achievability estimates are decreased, reflecting that training tasks are currently too difficult.Trading off achievability &amp; relevance to the test task.</p>
<p>We can now leverage the achievability estimates to ensure that the selected tasks are of an appropriate difficulty.</p>
<p>To this end, we propose Achievable Test-Time Curricula (A-TTCs), which balance relevance to the target tasks, as identified by SIFT, with achievability:
A |B|t ← {(x, v) | α x |B|t ∈ [a min , a max ]} {(x |B|t , v |B|(t+1)−1 )} ← arg min SIFT λ,ϕ,B,A |B|t (D ⋆ )
where [a min , a max ] determines the interval of task difficulty we consider for the task selection with SIFT.This selection strategy offers a simple way to select batches of problems online, which are of the right difficulty while remaining relevant to the target tasks.In practice, we choose [a min , a max ] = [0.2,0.6], with the goal of achieving approximately 50% of tasks over the batch, obtain prior difficulty estimates by computing the success rates of the Qwen3-8B model on all questions and enforce a minimum subset size of 1000 to select from.</p>
<p>The results in Figure 8 show that on the weaker Qwen3-0.6Bmodel trading-off achievability with relevance yields a higher training reward and furthermore improves test score across the three math benchmarks, AIME 24 &amp; 25 and MATH500.We note that this procedure appears useful primarily if the difficulty level in the dataset is wrongly calibrated with respect to the model's capabilities.</p>
<p>Modeling assumptions.To motivate our online achievability estimation, we consider the logits ϕ x t = σ −1 (α x t ) ∈ R of the achievability values and make the assumption that at each time step the change in the logits d t is jointly gaussian across all tasks:
d x t = ϕ x t+1 − ϕ x t (10)d t ∼ N (0, Σ) with Σ = (v − c)I n + c11 ⊤(
11) That is, we consider a fixed variance v for all tasks and assume that the update has constant correlation c among all tasks.After observing the achievabilities for a batch of problems at time t, we can compute the update in the logits for the observed tasks and are able to estimate the update for the unobserved problems.</p>
<p>Consider a batch of problems B = {y 1 , . . ., y m } and an unobserved problem x / ∈ B, then:
E[d x t | d y t , y ∈ B] = c1 ⊤ ((v − c)I |B| + c11 ⊤ ) −1 d B t (12) = c v − c − |B|c 2 (v − c)(v + (|B| − 1)c) y∈B d y t (13) = c v + (|B| − 1)c ψ y∈B d y t (14) ϕ x t+|B| = ϕ x t + ψ y∈B d y t (15)
Under the assumed covariance structure and letting ∆ t+|B| = σ(ψ y∈B d y t ), our update becomes:
α x t+|B| := r x t+|B| if
x was within the last batch
σ(σ −1 (α x t ) + σ −1 (∆ t+|B| )) otherwise. (16)</p>
<p>D EXTENDED RESULTS</p>
<p>In this section, we present additional experiments and ablations.</p>
<p>D.1 INCREASING CLIP-HIGH IN GRPO IS ESSENTIAL FOR LEARNING</p>
<p>Maintaining a sufficient level of entropy in the policy is key for any on-policy exploration method.</p>
<p>When training with GRPO with symmetrical clipping on verifiable rewards it has been observed (Yu et al., 2025b;Luo et al., 2025), that the policy's entropy quickly goes to 0, preventing effective exploration.It has been found that an increase of the clip-high (ϵ high ) parameter in GRPO can lead to a stabilization of the entropy and improved performance during training (Luo et al., 2025).Intuitively, if correct answers are rewarded more strongly than incorrect answers are penalized, the agent is incentivized to maintain higher entropy in its action distribution, promoting exploration.In Figure 9 we evaluate the effect of the clip-high parameter on the policy entropy and test accuracy during training.We find that a symmetric clipping (ϵ high = 0.2) leads to constant decrease in policy entropy and poor performance on the test tasks.When increasing the clip-high parameter, the policy entropy starts increasing, and the test accuracy is dramatically improved.In our preliminary experiments on Codeforces, ϵ high = 0.32 improved significantly over ϵ high = 0.28, which was suggested in Yu et al. (2025b) and used in our other experiments.In Figure 10, we provide further detail on the performance of all models across the main benchmarks.The plots reveal substantial variation in test accuracy development in response to training with the same TTC, indicating that models have varying initial capabilities and potential of training via RL.This is the case, as each model has been subject to different post-training techniques and therefore responds differently to the RL training on the TTC.To address these differences, we propose an algorithm in Appendix C, which aims to calibrate the difficulty of the curriculum to the capabilities of the model.</p>
<p>D.3 "RL POST-TRAINING" BASELINE RESTRICTED TO THE TEST ENVIRONMENT</p>
<p>A simple heuristic to improve a model's domain-specific capabilities is to restrict training to tasks from the target domain.This can be seen as a primitive version of a TTC that conditions on the environment type but ignores instance-level task characteristics.Accordingly, we include a baseline that samples a random subset of the training set-analogous to RL post-training-but restricted to the target domain.Figure 11 demonstrates that filtering the training questions to the code domain is insufficient to achieve comparable performance to TTC-RL on Codeforces and CodeElo.(2025), provides an alternative way to train the model at test time using majority labels as rewards on the target tasks.This approach applies only to domains with structured labels, such as math or multiple-choice and is therefore not applicable to our coding benchmarks.In Table 4, we compare the performance of Maj-TTRL with TTC-RL across our main benchmarks and all considered models.TTC-RL outperforms Maj-TTRL on most benchmarks for Qwen3-8B and Qwen3-4B-Instruct-2507.The only model, where Maj-TTRL achieves higher performance than TTC-RL is the Qwen3-4B-Instruct-2507 model, which is the strongest among all considered models.This reveals the dataset as the main bottleneck for improving performance and suggests to move beyond the bottleneck of a fixed task corpus through self-generated TTCs.</p>
<p>Combining Maj-TTRL with TTC-RL As already highlighted, Maj-TTRL and TTC-RL are two complementary approaches with different strengths.Intuitively, TTC-RL aims to learns from the most relevant tasks in the given corpus to improve on the target tasks, while Maj-TTRL is able to improve the performance on the target tasks directly by continuously aiming to match the majority prediction of the model.Beyond comparing them in isolation, Figure 12 shows that initializing Maj-TTRL from the final TTC-RL checkpoint and training on the target benchmark yields the strongest results on all math benchmarks.Figure 12: Combining TTC-RL and Maj-TTRL combines the strengths of both methods and yields the strongest results on all math benchmarks.We show the results on the Qwen3-8B for math.</p>
<p>D.5 ADDITIONAL BENCHMARKS</p>
<p>While our main evaluation focuses on the most challenging benchmarks in math, code and general reasoning, aiming to push the capabilities of frontier models, we additionally provide implementation and results for a set of simpler benchmarks.These include in the math domain, GMS8K (Cobbe et al., 2021) and AMC23.For coding we add the HumanEval+ (Chen et al., 2021) and MBPP+ (Chen et al., 2021).Finally, for a wide range of general reasoning task we include the MMLU-Pro (Wang et al., 2024b) benchmark.The results in Table 5 show that TTC-RL yields substantial gains on math and coding, especially for the weaker Qwen3-8B-Base model.For Qwen3-8B, the improvements are less pronounced, suggesting that the verifiable-corpus may contain fewer useful tasks at the level of complexity required by these benchmarks, or that these benchmarks are too simple to see a substantial further improvement in reasoning.• In Figure 13, we show the marginal improvement in percentage points throughout training when using TTC-RL over general-purpose RL post-training, and find that this difference remains large throughout training for all models.• In Figure 14, we perform an ablation, comparing to oracle training on the test set.</p>
<p>Model</p>
<p>• In Table 6, we provide a detailed breakdown of values for pass@k.</p>
<p>• In Table 7, we report additional results on latent improvement.</p>
<p>D.7 UNSUCCESSFUL ATTEMPTS</p>
<p>The strong improvements observed when increasing the clip-high parameter ϵ high suggest that the exploration phase requires stabilization of the policy entropy.We evaluated a "cooldown" of entropy via continued training with ϵ high = 0.2.However, in Figure 15, we find that the cooldown appears to slightly improve performance in math, but not generally.For training tasks from Webinstruct-verified, we additionally include a length penalty as proposed by Ma et al. (2025).Denoting the number of tokens in the extracted answer of an attempt by l and the number of tokens of the golden answer by l ⋆ , the length penalty is defined as
ℓ := 0.05 • min{|l − l ⋆ |, 10}.(17)
We set ℓ = 0 for math and code environments.</p>
<p>Our training reward for a given attempt is
r :=    1 − ℓ if the attempt is correct − 1 2
if the attempt is ill-formed and was not truncated 0 otherwise.</p>
<p>(18)</p>
<p>F QUALITATIVE EXAMPLES
≤ n ≤ 10 5 , 1 ≤ k ≤ n•(n−1)
2</p>
<p>).The second line of the input contains n non-negative integer numbers a1, a2, . . ., an (0 ≤ ai ≤ 10 9 )the array itself.It is guaranteed that the sum of n over all test cases does not exceed 10 5 .Output: Print the k-th smallest value obtained over all subarrays of length at least 2. Note: In the first testcase, we have subarrays with their smallest exclusive-or pair as:
[1, 2] : 3, [2, 3] : 1, [3, 4] : 7, [4, 5] : 1, [1, 2, 3] : 1, [2, 3, 4] : 1, [3, 4, 5] : 1, [1, 2, 3, 4] : 1, [2, 3, 4, 5] : 1, [1, 2, 3, 4, 5] : 1.
The sorted order would be: 1, 1, 1, 1, 1, 1, 1, 1, 3, 7. Therefore, the second smallest element would be 1.The following problem is the 26'th task from the AIME25 competition.Additionally, we provide the initial answer of Qwen3-8B for this problem, as well as the answer of the model after 250 training steps on the TTC curated specifically for this specific question.Finally, we list the first 10 selected training problems for this task.</p>
<p>Initial Answer</p>
<p>Problem</p>
<p>Let A 1 A 2 . . .A 11 be a non-convex 11-gon such that The area of
A i A 1 A i+1 is 1 for each 2 ≤ i ≤ 10, cos(∠A i A 1 A i+1 ) = 12 13 for each 2 ≤ i ≤ 10, The perimeter of A 1 A 2 . . . A 11 is 20. If A 1 A 2 + A 1 A 11 can be expressed as m √ n−p q
for positive integers m, n, p, q with n squarefree and gcd(m, p, q) = 1, find m + n + p + q.Please reason step by step, and put your final answer within .</p>
<p>△ ADE.Consider the line ℓ parallel to BC such that ℓ is tangent to ω at a point F and such that ℓ does not intersect Ω.Let ℓ intersect lines AB, AC at the points X, Y , respectively, with XY = 18 and AX = 16.Let the perpendicular bisector of XY meet the circumcircle of △ AXY at P , Q, where the distance from P to F is smaller than the distance from Q toF .Let ray − − → P F meet Ω for the first time at the point Z.If P Z 2 = m n for relatively prime positive integers m, n, find m + n.The solution will be evaluated in a math environment.</p>
<p>Training problem 6</p>
<p>13 LHS Students attend the LHS Math Team tryouts.The students are numbered 1, 2, . . ., 13.Their scores are s 1 , s 2 , . . ., s 13 , respectively.There are 5 problems on the tryout, each of which is given a weight, labeled w 1 , w 2 , . . ., w 5 .Each score s i is equal to the sum of the weights of all problems solved by student i.On the other hand, each weight w j is assigned to be 1 s i</p>
<p>, where the sum is over all the scores of students who solved problem j. (If nobody solved a problem, the score doesn't matter).If the largest possible average score of the students can be expressed in the form √ a b , where a is square-free, find a + b.The solution will be evaluated in a math environment.</p>
<p>Training problem 7</p>
<p>Let ABCDE be a pentagon with area 2017 such that four of its sides AB, BC, CD, and EA have integer length.Suppose that ∠A = ∠B = ∠C = 90 o , AB = BC, and CD = EA.The maximum possible perimeter of ABCDE is a + b √ c, where a, b, and c are integers and c is not divisible by the square of any prime.Find a + b + c.The solution will be evaluated in a math environment.</p>
<p>Training problem 8</p>
<p>Let △ ABC be a triangle with AB = 4 and AC = 7 2 .Let ω denote the A-excircle of △ ABC.Let ω touch lines AB, AC at the points D, E, respectively.Let Ω denote the circumcircle of △ ADE.Consider the line ℓ parallel to BC such that ℓ is tangent to ω at a point F and such that ℓ does not intersect Ω.Let ℓ intersect lines AB, AC at the points X, Y , respectively, with XY = 18 and AX = 16.Let the perpendicular bisector of XY meet the circumcircle of △ AXY at P , Q, where the distance from P to F is smaller than the distance from Q toF .Let ray − − → P F meet Ω for the first time at the point Z.If P Z 2 = m n for relatively prime positive integers m, n, find m + n.The solution will be evaluated in a math environment.</p>
<p>Training problem 9</p>
<p>Point P is in the interior of △ABC.The side lengths of ABC are AB = 7, BC = 8, CA = 9.The three feet of perpendicular lines from P to sides BC, CA, AB are D, E, F respectively.Suppose the minimal value of BC P D + CA P E + AB P F can be written as a b √ c, where gcd(a, b) = 1 and c is square-free, calculate abc.The solution will be evaluated in a math environment.</p>
<p>Training problem 10</p>
<p>Billy the baker makes a bunch of loaves of bread every day, and sells them in bundles of size 1, 2, or 3. On one particular day, there are 375 orders, 125 for each bundle type.As such, Billy goes ahead and makes just enough loaves of bread to meet all the orders.Whenever Billy makes loaves, some get burned, and are not sellable.For nonnegative i less than or equal to the total number of loaves, the probability that exactly i loaves are sellable to customers is inversely proportional to 2 i (otherwise, it's 0).Once he makes the loaves, he distributes out all of the sellable loaves of bread to some subset of these customers (each of whom will only accept their desired bundle of bread), without worrying about the order in which he gives them out.If the expected number of ways Billy can distribute the bread is of the form a b 2 c −1 , find a + b + c.The solution will be evaluated in a math environment.</p>
<p>F.3 TTC FOR CODEELO</p>
<p>In the following, we list the 10 most relevant problems selected by SIFT to improve performance on the CodeElo benchmark.</p>
<p>Training problem 1</p>
<p>There are n monsters standing in a row.The i-th monster has a i health points.</p>
<p>Every second, you can choose one alive monster and launch a chain lightning at it.The lightning deals k damage to it, and also spreads to the left (towards decreasing i) and to the right (towards increasing i) to alive monsters, dealing k damage to each.When the lightning reaches a dead monster or the beginning/end of the row, it stops.A monster is considered alive if its health points are strictly greater than 0.</p>
<p>For example, consider the following scenario: there are three monsters with health equal to [5,2,7], and k = 3.You can kill them all in 4 seconds:</p>
<p>-launch a chain lightning at the 3-rd monster, then their health values are [2, −1, 4]; -launch a chain lightning at the 1-st monster, then their health values are [−1, −1, 4]; -launch a chain lightning at the 3-rd monster, then the . . .</p>
<p>Training problem 2</p>
<p>Eshag has an array a consisting of n integers.</p>
<p>Eshag can perform the following operation any number of times: choose some subsequence of a and delete every element from it which is strictly larger than AV G, where AV G is the average of the numbers in the chosen subsequence.</p>
<p>For example, if a = [1,4,3,2,4] and Eshag applies the operation to the subsequence containing a 1 , a 2 , a 4 and a 5 , then he will delete those of these 4 elements which are larger than a1+a2+a4+a5 4</p>
<p>= 11 4 , so after the operation, the array a will become a = [1, 3, 2].Your task is to find the maximum number of elements Eshag can delete from the array a by applying the operation described above some number (maybe, zero) times.</p>
<p>A sequence b is a subsequence of an array c if b can be obtained from c by deletion of several (possibly, zero or all) elements.The solution will be evaluated in a code environment.</p>
<p>Training problem 3</p>
<p>There are n squares drawn from left to right on the floor.The i-th square has three integers p i , a i , b i , written on it.The sequence p 1 , p 2 , dots, p n forms a permutation.</p>
<p>Each round you will start from the leftmost square 1 and jump to the right.If you are now on the i-th square, you can do one of the following two operations: 1. Jump to the i + 1-th square and pay the cost a i .If i = n, then you can end the round and pay the cost a i .2. Jump to the j-th square and pay the cost b i , where j is the leftmost square that satisfies j &gt; i, p j &gt; p i .If there is no such j then you can end the round and pay the cost b i .</p>
<p>There are q rounds in the game.To make the game more difficult, you need to maintain a square set S (initially it is empty).You must pass through these squares during the round (other squares can also be passed through).The square set S for . . .</p>
<p>Training problem 4</p>
<p>YouKn0wWho has an integer sequence a 1 , a 2 , . . .a n .Now he will split the sequence a into one or more consecutive subarrays so that each element of a belongs to exactly one subarray.Let k be the number of resulting subarrays, and h 1 , h 2 , . . ., h k be the lengths of the longest increasing subsequences of corresponding subarrays.</p>
<p>For example, if we split [2, 5, 3, 1, 4, 3, 2, 2, 5, 1] into [2, 5, 3, 1, 4], [3, 2, 2, 5] YouKn0wWho wonders if it is possible to split the sequence a in such a way that the bitwise XOR of h 1 , h 2 , . . ., h k is equal to 0. You have to tell whether it is possible.</p>
<p>Training problem 5</p>
<p>Eve is a beginner stand-up comedian.Her first show gathered a grand total of two spectators: Alice and Bob.</p>
<p>Eve prepared a 1 + a 2 + a 3 + a 4 jokes to tell, grouped by their type: type 1: both Alice and Bob like them; type 2: Alice likes them, but Bob doesn't; type 3: Bob likes them, but Alice doesn't; type 4: neither Alice nor Bob likes them.</p>
<p>Initially, both spectators have their mood equal to 0. When a spectator hears a joke he/she likes, his/her mood increases by 1.When a spectator hears a joke he/she doesn't like, his/her mood decreases by 1.If the mood of a spectator becomes negative (strictly below zero), he/she leaves.</p>
<p>When someone leaves, Eve gets sad and ends the show.If no one leaves, and Eve is out of jokes, she also ends the show.Thus, Eve wants to arrange her jokes in such a way that the show lasts as long as possible.Help her to calculate the maximum number of jokes she can tell before the show ends.The solution will be evalu . . .</p>
<p>Training problem 6</p>
<p>Solve the following coding problem using the programming language python: zscoder has a deck of n + m custom-made cards, which consists of n cards labelled from 1 to n and m jokers.Since zscoder is lonely, he wants to play a game with himself using those cards.</p>
<p>Initially, the deck is shuffled uniformly randomly and placed on the table.zscoder has a set S which is initially empty.</p>
<p>Every second, zscoder draws the top card from the deck.If the card has a number x written on it, zscoder removes the card and adds x to the set S. If the card drawn is a joker, zscoder places all the cards back into the deck and reshuffles (uniformly randomly) the n + m cards to form a new deck (hence the new deck now contains all cards from 1 to n and the m jokers).Then, if S currently contains all the elements from 1 to n, the game ends.Shuffling the deck doesn't take time at all.</p>
<p>x and column y, is denoted by (x, y).</p>
<p>Every cell contains 0 or 1.It is known that the top-left cell contains 1, and the bottom-right cell contains 0.</p>
<p>We want to know numbers in all cells of the grid.To do so we can ask the following questions:</p>
<p>x 1 y 1 x 2 y 2 ¨, where 1 ≤ x 1 ≤ x 2 ≤ n, 1 ≤ y 1 ≤ y 2 ≤ n, and x 1 + y 1 + 2 ≤ x 2 + y 2 .In other words, we output two different cells (x 1 , y 1 ), (x 2 , y 2 ) of the grid such that we can get from the first to the second by moving only to the right and down, and they aren't adjacent.</p>
<p>As a response to such question you will be told if there exists a path between (x 1 , y 1 ) and (x 2 , y 2 ), going only to the right or down, numbers in cells of which form a palindrome.</p>
<p>For example, paths, shown in gr . . .</p>
<p>Figure 4 :
4
Figure 4: TTC-RL scales test-time compute in way that is complementary to other means of test-time scaling.Left: The pass@k of TTC-RL on Qwen3-8B, averaged over benchmarks, increases substantially for small and large k, indicating that TTC-RL raises the model's performance ceiling.Middle: TTC-RL also improves the performance of majority voting (across math and GPQA-D), with the initial pass@1 significantly outperforming maj@64 on the initial model.Right: We evaluate Qwen3-8B in non-thinking and thinking mode, as well as the non-thinking model + TTC-RL.The color indicates the relative accuracy per column.We find that TTC-RL significantly improves the non-thinking model, allowing it to perform close to the thinking variant in several domains, despite reasoning over 8k rather than 30k context tokens.</p>
<p>Figure 3 :
3
Figure 3: TTC-RL substantially outperforms general-purpose RL post-training for a range of data sizes.We evaluate Qwen3-8B on all seven benchmarks and report the average test accuracy when training for 250 steps.</p>
<p>Figure 5 :
5
Figure5: Left: Per-task TTC-RL outperforms a benchmark-level TTC in AIME25.We perform TTC-RL and maj-TTRL (cf.Section 5.2) on Qwen3-8B, and find that per-task TTC-RL even outperforms the benchmark-level TTC.Middle: TTC-RL improves "correctness" of reasoning, not only learning the answer format.We evaluate the difference in accuracy between TTC-RL and the initial Qwen3-8B, averaged over benchmarks.The latent improvement is a lower bound on the accuracy gain that is not due to merely learning the format (cf.Section 5.1).Right: TTC-RL yields models that are specialized to their target tasks.We plot the accuracy of Qwen3-8B trained for given target tasks (rows) when evaluated on other benchmarks (columns).We normalize accuracies across all evaluations of a particular benchmark.Notably, the model trained via TTC-RL for the "right" target tasks (i.e., the diagonal) always performs best.</p>
<p>Figure 6 :
6
Figure 6: Training on the test set with SFT leads to an initial accuracy drop, indicating that SFT is ill-suited for TTT.</p>
<p>Figure 7 :
7
Figure 7: Training Llama-3.2-1B-Instruct on the GSM8K test set with SFT.After the first full epoch, the performance has dropped significantly compared to the initial model, before then overfitting to the SFT data.</p>
<p>Figure 8 :
8
Figure 8: Comparison of train and test accuracy of standard TTC-RL vs. A-TTC-RL averaged across math benchmarks (MATH500, AIME24, AIME25) on the Qwen3-0.6Bmodel.</p>
<p>Figure 9 :
9
Figure9: Increasing the ϵ high to 0.28 prevents the collapse of policy entropy and leads to strong performance on the test set.We plot the test accuracy and the policy entropy over the course of the training for various values of ϵ high on the Qwen3-8B model trained on the Codeforces dataset.GRPO's default value is ϵ high .</p>
<p>Figure 10
10
Figure10: TTC-RL shows strong improvements over standard RL Post-Training across most considered models on the math and coding benchmarks.We plot the individual performance of all considered models on the main benchmarks.</p>
<p>Figure 11 :
11
Figure 11: Restricting RL post-training to include only problems in a code environment explains only a fraction of the improvement on challenging coding tasks (Codeforces, CodeElo) seen by TTC-RL.</p>
<p>Figure 13 :Figure 14 :
1314
Figure 13: Improvement of TTC-RL over RL post-training across several models.</p>
<p>range(i + 1, n): # Subarray from i to j (inclusive) sub_len = j -i + 1 if sub_len &lt; 2: continue # Extract the subarray sub = a[i:j+1] # Generate all pairs (x, y) with x &lt; y # For this subarray, compute min XOR min_xor = float('inf') # Iterate over all pairs (p, q) in subarray indices for p in range(len(sub)): for q in range(p + 1, len(sub)): current_xor = sub[p] ^sub[q]</p>
<p>, [1], then h = [3, 2, 1].</p>
<p>The longest increasing subsequence (LIS) of a sequence b 1 , b 2 , . . ., b m is the longest sequence of valid indices i 1 , i 2 , . . ., i k such that i 1 , i 2 , . . ., i k and b i1 , b i2 , . . ., b i k .For ex . . .</p>
<p>Table 2
2: The competitive performance of Maj-TTRL on our strongest model suggests that TTC-RL's effectiveness is constrained by its fixed training cor-pus. Combining our approach with self-improvement techniques is therefore an exciting direction for future work.
Notably, the performance gains of Maj-TTRL on the strong Qwen3-4B-Instruct-2507 model in AIME 24 &amp; 25 suggest that the returns from our proposed implementation of TTC-RL are constrained by the scope of its fixed training corpus.This saturation does not imply a ceiling on the model's capabilities; rather, it may indicate a promising opportunity for self-improvement methods such as Maj-TTRL or synthetic data generation (e.g.,</p>
<p>Table 4 :
4
Extended comparison of TTC-RL with Maj-TTRL across models and benchmarks.
Qwen3-8B-Instruct21.6723.3369.5520.8513.73 20.6149.11+ RL post-training41.6738.3382.5027.8322.67 25.9556.47+ Maj-TTRL (Zuo et al., 2025)42.5030.0085.40---51.14+ TTC-RL50.8341.6785.1033.3529.34 27.2958.38Qwen3-4B-Instruct-250752.5040.8372.0026.7020.27 21.5661.93+ RL post-training55.8347.5086.3028.3921.18 25.9562.82+ Maj-TTRL (Zuo et al., 2025)65.8355.8386.80---62.44+ TTC-RL60.0045.8388.5034.9927.20 26.9161.93Qwen3-8B-Base15.8314.1763.109.926.67 11.2629.70+ RL post-training22.5020.8376.8517.469.97 18.5142.77+ Maj-TTRL (Zuo et al., 2025)20.8320.0074.55---29.70+ TTC-RL30.0021.6778.1517.8411.33 17.9445.94</p>
<p>Table 5 :
5
Performance of TTC-RL on easier benchmarks.(<em>) We evaluate the subset of MMLU-Pro, consisting of computer science, law, math, and physics (equally weighted), and train with separate TTCs for each subject.
GSM8KAMC23HumanEval+ MBPP+MMLU-Pro</em>Qwen3-8B83.1963.1279.8844.8866.00+ RL post-training 93.06 + TTC-RL 94.01 +10.8 88.75 +25.6 80.64 +0.8 86.25 82.7763.23 61.64 +16.8 68.71 +2.8 69.30Qwen3-8B-Base73.0946.2535.8238.8345.46+ RL post-training 92.80 + TTC-RL 93.25 +20.2 72.50 +26.3 81.25 +45.4 63.12 81.1060.44 63.56 +24.8 61.86 +16.4 62.21D.6 FURTHER RESULTS AND ABLATIONS</p>
<p>Table 6 :
6
TTC-RL consistently improves the pass@k across math and code for large k.We show the pass@k for Qwen3-8B before and after the TTC-RL training on our main benchmarks.Continued training with a decreased clip-high parameter (ϵ high = 0.2) does not yield improved performance.We plot the average performance averaged over the main math, code and general reasoning benchmarks on the Qwen3-8B model.
Qwen3-8BAIME24AIME25 MATH500 CodeforcesCodeEloLCBGPQA-DPass@1 21.67/50.83 23.33/41.67 69.55/85.10 20.85/33.35 13.73/29.34 20.61/27.29 49.11/58.38Pass@2 31.87/52.10 28.31/48.37 77.57/86.91 24.96/31.82 17.71/33.75 23.55/28.74 60.94/64.45Pass@4 39.11/60.45 34.11/56.01 82.63/88.34 29.61/35.32 23.11/38.90 27.10/31.03 72.04/73.49Pass@8 46.47/67.43 40.13/62.10 85.68/89.37 33.57/38.31 28.28/43.01 30.12/33.06 80.60/80.67Pass@16 53.21/73.19 45.91/68.27 87.65/90.22 37.06/40.65 32.88/46.39 32.22/34.75 86.49/85.94Pass@32 58.98/77.06 51.52/73.78 89.09/90.91 40.09/42.45 36.75/49.20 33.25/35.92 90.09/89.33Pass@64 63.23/79.03 56.67/78.51 90.10/91.43 42.57/43.74 39.74/51.43 33.79/36.73 92.37/91.43Test Accuracy0.50 0.55 0.60Policy Entropy0.25 0.50 0.75250300350275300325350Training StepTraining StepMathGPQAFigure 15:</p>
<p>Table 7 :
7
On most benchmarks and models TTC-RL yields strong latent improvement, which normalized for learning the correct output format.</p>
<p>In this section we provide qualitative examples of single runs, which showed interesting behavior and provide examples of parts of the curricula used for training for various code and math problems.You have an array of non-negative integers a1, a2, ..., an.The value of a sub-array of length ≥ 2, a[l, r] = [a l , a l+1 , ..., ar] is the minimum value of ai ⊕ aj such that l ≤ i &lt; j ≤ r, where ⊕ is the xor (exclusive-or) operator.You have to find the k-th smallest value over all sub-arrays of length ≥ 2.Input: The first line of the input contains multiple test cases t (1 ≤ t ≤ 2 • 10 4 ).The first line of each test case contains integer numbers n and k (2
F.1 CODEELO, QUESTION 85ProblemDescription:
Algorithm 1 abstracts that we perform each RL step over a batch of training tasks and that we perform RL training for multiple episodes.
We summarize all training hyperparameters in Appendix E.3.
In Appendix D.3, we additionally compare to an "RL post-training" baseline that only samples training tasks from the test environment and show that this yields comparable results.
In MATH500, non-thinking Qwen3-8B + TTC-RL (85%) even outperformed the thinking variant (77%).
If answers are extracted, which are not intended as answers by the model, this artificially inflates LI and violates Assumption 1. To ensure this, we only extract the contents of \boxed{} or the contents wrapped in "' "', for math and code, respectively.
That is, any consecutive sequence of 12 tokens.
ACKNOWLEDGMENTSWe thank Akira Yoshiyama who supported our implementation of curricula (i.e., dynamic datasets) in verl.We further thank Matthias Otth who developed our results with SFT on GSM8K, indicating that SFT is ill-suited for TTT in LLMs and motivating this project.Finally, we thank Yu Sun for helpful discussions and Marco Bagatella for feedback on an early version of this paper.This project was supported through the Swiss AI compute grant a156.JH was supported by the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545.IH was supported by an ETH AI Center Postdoctoral fellowship.E EXPERIMENT DETAILS E.1 DATASETWe curate a multi-domain training corpus from math (DAPO-Math-17k, Hendrycks MATH, GSM8K), code (LiveCodeBench up until August 1, 2024, TACO, PrimeIntellect, Codeforces train, CodeContests, LeetCode), and WebInstruct-verified.All samples are cast into a unified schema with fields kind, dataset, description, problem, answer, and tests, with light task-specific preprocessing (e.g., GSM8K answer extraction).For simplicity we compute embeddings for SIFT using Qwen3-8B across all runs.Decontamination.We decontaminate our entire corpus except for Webinstruct-verified against our held-out evaluation benchmarks using a single, conservative procedure:1.Text normalization: Lowercase, whitespace collapse, and answer normalization by removing TeX wrappers such as \boxed{}.2. Candidate pruning via small n-grams: We tokenize benchmark texts and index 12-gram shingles 6 to retrieve a small candidate set for each training item.3. Contamination tests: An item is marked contaminated if it either (i) shares any exact 32gram shingle with a benchmark item or (ii) achieves a sequence-similarity ratio of at least 0.75 (difflib-style) with any candidate.4. Removal: For math, we additionally require the normalized training answer to match the benchmark answer before removal.For code, if a training item matches multiple distinct benchmark tasks from a single benchmark, we keep it to avoid removing generic boilerplate or templates.Deduplication.Within-domain duplicates are removed via fast token-coverage deduplication: we keep the first occurrence and drop a later item when at least a threshold fraction of its normalized token set is covered by another item's tokens (or vice versa), requiring identical normalized answers when answers are present.We use threshold 0.80 for math and 0.95 for code; WebInstruct-verified is deduplicated within itself at 1.00.Extraction of problem descriptions.For each training task, we extract a description as its main identifier.For tasks unlike coding, the description coincides with the problem field, without any system prompts.For coding tasks, we extract the description from problem to avoid any superfluous selection of tasks based on the formatting of input-output examples or other formatting.TTCs are self-curated via SIFT based on the model's last-token last-layer representation of the description field.To each description, we append information about the environment: "The solution will be evaluated in a {math/verifier/code} environment.".Filtering.We remove low-signal or malformed items with the following rules:• Code training tasks require at least 5 executable tests, non-empty descriptions.We also drop cases where the description trivially duplicates the problem text, indicating that the problem was wrongly parsed or is missing input-output examples.• We drop items with missing or empty answers, except for code tasks with unit tests.• We enforce a minimum description length for code of at least 100 characters to prevent underspecified tasks.• We exclude all items whose prompt length exceeds our max-token limit of 2048.E.2 SYSTEM PROMPTSWe use the following system prompts, which we adapted from evalchemy(Raoof et al., 2025)and simplified slightly.We did not tune system prompts for better performance.General system prompt{problem} Please reason step by step, and put your final answer within \boxed{}.Hyperparameter ValueDataCode system promptYou are a coding expert.You will be given a coding problem, and you need to write a correct Python program that matches the specification and passes all tests.The time limit is 1 second.You may start by outlining your thought process.In the end, please provide the complete code in a code block enclosed with "' "'.\n\n{problem}GPQA system promptReturn your final response within \boxed{} and only include the letter choice (A, B, C, or D) as your final response.Problem: {problem} Options: {options} Answer:E.3 DETAILS OF THE RL TRAININGWe summarize our hyperparameters for RL training in Table8.We keep these hyperparameters fixed across all models, benchmarks, and baselines.In our code environment, we keep only the first 20 test cases for training tasks to improve efficiency.Training reward.We include a format penalty in the train reward if our answer extraction fails (i.e., we extract an empty string) to encourage well-formed responses.Notably, we found it important not to penalize ill-formed answers that were truncated due to exceeding the maximum response length, since this disincentivizes the model from leveraging all of its accessible context.Initial Answer. . .tates that the perimeter is 20, and the sum x 2 + x 11 is what we need, and the answer is in the form m √ n−p q, perhaps we can proceed with the assumption that the perimeter is related to the sum of the side lengths, which can be expressed in terms of a, and then solve for a.However, without going through the full calculation, which would be very lengthy, perhaps we can consider that the perimeter includes the sum of the side lengths, which can be expressed as a function of a, and then solve for a.Alternatively, since the problem gives the answer in the form involving square roots, it suggests that the expression for a + 26 a involves a square root.But since we have x 2 + x 11 = a + 26 a , and the perimeter is 20, which includes other terms, but since the problem gives the answer in terms of radicals, we can assume that the perimeter is expressed in terms of a and the square root, and solving gives us the required expression.However, since this is too vague, and given that the problem is from a competition, let's assume that the perimeter can be expressed in terms of a, and the sum a + 26 a can be found.But since this is too open-ended, and given the time constraints, I'll proceed to assume that the perimeter is related to the sum of the x i 's, and the sum of the side lengths, and that after solving, the value of a + 26 a is expressed as m √ n−p q .However, without going through the full derivation, which is quite involved, I'll conclude that the answer is likely m √ n−p q, and from the given information, the answer is likely:But since the problem states that n is square-free, and the answer is in the form m √ n−p q, and the given answer format matches this, the final answer would be: m = 52, n = 13, p = 10, q = 5 Thus, m + n + p + q = 52 + 13 + 10 + 5 = 80.But to check, let's verify: If the sum is a + 26 a , and the perimeter is 20, then solving for a would lead to a quadratic, and the expression would involve square roots.Assuming that the final expression is 52 √ 13−10 5, then the sum is 52 √ 13−10 5, and the answer is: m = 52, n = 13, p = 10, q = 5 So the final answer is 52 + 13 + 10 + 5 = 80.Thus, the final answer is: We are looking for a positive value of t, since it's a sum of positive lengths.So take the positive root:, which is the value of a + 26 5a , which is the expression we need for A 1 A 2 + A 1 A 11 .So the answer is:, with m = 9, n = 5, p = 1, q = 4. Check gcd(9,1,4) = 1, and n is square-free.Thus, m + n + p + q = 9 + 5 + 1 + 4 = 19.-Training problem 3Robert is a robot who can move freely on the unit circle and its interior, but is attached to the origin by a retractable cord such that at any moment the cord lies in a straight line on the ground connecting Robert to the origin.Whenever his movement is counterclockwise (relative to the origin), the cord leaves a coating of black paint on the ground, and whenever his movement is clockwise, the cord leaves a coating of orange paint on the ground.The paint is dispensed regardless of whether there is already paint on the ground.The paints covers 1 gallon/unit 2 , and Robert starts at (1, 0).Each second, he moves in a straight line from the point (cos(θ), sin(θ)) to the point (cos(θ + a), sin(θ + a)), where a changes after each movement.a starts out as 253 o and decreases by 2 o each step.If he takes 89 steps, then the difference, in gallons, between the amount of black paint used and orange paint used can be written as . . .Training problem 4There are n players in a round-robin ping-pong tournament (i.e.every two persons will play exactly one game).After some matches have been played, it is known that the total number of matches that have been played among any n − 2 people is equal to 3 k (where k is a fixed integer).Find the sum of all possible values of n.The solution will be evaluated in a math environment.Training problem 5Let △ ABC be a triangle with AB = 4 and AC = 7 2 .Let ω denote the A-excircle of △ ABC.Let ω touch lines AB, AC at the points D, E, respectively.Let Ω denote the circumcircle of What is the expected number of seconds before the game ends?We can sho . . .Training problem 7n pupils, who love to read books, study at school.It is known that each student has exactly one best friend, and each pupil is the best friend of exactly one other pupil.Each of the pupils has exactly one interesting book.The pupils decided to share books with each other.Every day, all pupils give their own books to their best friends.Thus, every day each of the pupils has exactly one book.Your task is to use the list of the best friends and determine the exchange of books among pupils after k days.For simplicity, all students are numbered from 1 to n in all tests.The solution will be evaluated in a code environment.Training problem 8You are given a rooted tree, consisting of n vertices.The vertices are numbered from 1 to n, the root is the vertex 1.You can perform the following operation at most k times: choose an edge (v, u) of the tree such that v is a parent of u; remove the edge (v, u); add an edge (1, u) (i.e. make u with its subtree a child of the root).The height of a tree is the maximum depth of its vertices, and the depth of a vertex is the number of edges on the path from the root to it.For example, the depth of vertex 1 is 0, since it's the root, and the depth of all its children is 1.What's the smallest height of the tree that can be achieved?The solution will be evaluated in a code environment.Training problem 9Back in time, the seven-year-old Nora used to play lots of games with her creation ROBO-Head-02, both to have fun and enhance his abilities.One day, Noras adoptive father, Phoenix Wyle, brought Nora n boxes of toys.Before unpacking, Nora decided to make a fun game for ROBO.She labelled all n boxes with n distinct integers a 1 , a 2 , . . ., a n and asked ROBO to do the following action several (possibly zero) times: Pick three distinct indices i, j and k, such that a i |a j and a i |a k .In other words, a i divides both a j and a k , that is a j mod a i = 0, a k mod a i = 0.After choosing, Nora will give the k-th box to ROBO, and he will place it on top of the box pile at his side.Initially, the pile is empty.After doing so, the box k becomes unavailable for any further actions.Being . . .Training problem 10This is an interactive problem You are given a grid n× n, where n is odd.Rows are enumerated from 1 to n from up to down, columns are enumerated from 1 to n from left to right.Cell, standing on the intersection of row
The surprising effectiveness of test-time training for few-shot learning. Ekin Akyürek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim, Jacob Andreas, NeurIPS. 2025 Marcin Icml, Filip Andrychowicz, Alex Wolski, Jonas Ray, Rachel Schneider, Peter Fong, Bob Welinder, Josh Mcgrew, Pieter Tobin, Wojciech Abbeel, Zaremba, 2017Hindsight experience replay</p>
<p>Test-time offline reinforcement learning on goal-related experience. Marco Bagatella, Mert Albaba, Jonas Hübotter, Georg Martius, Andreas Krause, arXiv:2507.188092025aarXiv preprint</p>
<p>Active fine-tuning of multitask policies. Marco Bagatella, Jonas Hübotter, Georg Martius, Andreas Krause, ICML. 2025b</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, ICML. 2009</p>
<p>Dota 2 with large scale deep reinforcement learning. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, arXiv:1912.066802019arXiv preprint</p>
<p>Local mixtures of experts: Essentially free test-time training via model merging. Ryo Bertolissi, Jonas Hübotter, Ido Hakimi, Andreas Krause, COLM. 2025</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, ArXiv:2005.141652020arXiv preprint</p>
<p>Lili Chen, Mihir Prabhudesai, Katerina Fragkiadaki, Hao Liu, Deepak Pathak, arXiv:2508.03682Self-questioning language models. 2025aarXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Pass@k training for adaptively balancing exploration and exploitation of large reasoning models. Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, Guang Shi, arXiv:2508.107512025barXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>One-minute video generation with test-time training. Karan Dalal, Daniel Koceja, Gashon Hussein, Jiarui Xu, Yue Zhao, Youjin Song, Shihao Han, Ka Chun Cheung, Jan Kautz, Carlos Guestrin, CVPR. 2025</p>
<p>Discover: Automated curricula for sparse-reward reinforcement learning. Leander Diaz-Bone, Marco Bagatella, Jonas Hübotter, Andreas Krause, NeurIPS2025</p>
<p>Training on the test task confounds evaluation and emergence. Ricardo Dominguez-Olmedo, Florian E Dorner, Moritz Hardt, ICLR2025</p>
<p>Rl 2 : Fast reinforcement learning via slow reinforcement learning. Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, Pieter Abbeel, ICLR2017</p>
<p>Serl: Self-play reinforcement learning for large language models with limited data. Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kaixuan Chen, Mingli Song, Dacheng Tao, arXiv:2505.203472025arXiv preprint</p>
<p>Model-agnostic meta-learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, ICML. 2017</p>
<p>Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, arXiv:2506.04178Data recipes for reasoning models. 2025arXiv preprint</p>
<p>Reinforced self-training (rest) for language modeling. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, arXiv:2308.089982023arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Test-time training on nearest neighbors for large language models. Moritz Hardt, Yu Sun, ICLR. 2024</p>
<p>Measuring coding challenge competence with apps. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, Jacob Steinhardt, NeurIPS2021a</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, NeurIPS2021b</p>
<p>Self-improvement in language models: The sharpening mechanism. Audrey Huang, Adam Block, Dylan J Foster, Dhruv Rohatgi, Cyril Zhang, Max Simchowitz, Jordan T Ash, Akshay Krishnamurthy, ICLR2025a</p>
<p>Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, Dong Yu, arXiv:2508.05004R-zero: Self-evolving reasoning llm from zero data. 2025barXiv preprint</p>
<p>Transductive active learning: Theory and applications. Jonas Hübotter, Bhavya Sukhija, Lenart Treven, Yarden As, Andreas Krause, NeurIPS2024</p>
<p>Efficiently learning at test-time: Active fine-tuning of llms. Jonas Hübotter, Sascha Bongni, Ido Hakimi, Andreas Krause, ICLR2025</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, arXiv:2403.079742024arXiv preprint</p>
<p>Kimi k1.5: Scaling reinforcement learning with llms. Angang Kimi, Bofei Du, Bowei Gao, Changjiu Xing, Cheng Jiang, Cheng Chen, Chenjun Li, Chenzhuang Xiao, Chonghua Du, Liao, arXiv:2501.125992025arXiv preprint</p>
<p>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, arXiv:2411.15124Pushing frontiers in open language model post-training. 20243arXiv preprint</p>
<p>Selfimproving transformers overcome easy-to-hard and length generalization challenges. Nayoung Lee, Ziyang Cai, Avi Schwarzschild, Kangwook Lee, Dimitris Papailiopoulos, ICML. 2025</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, NeurIPS. 2019</p>
<p>Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, Ge Li, Taco, arXiv:2312.14852Topics in algorithmic code generation dataset. 2023arXiv preprint</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, arXiv:2203.078142022arXiv preprint</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, ICLRJan. 2023Bowen Baker, Teddy Lee</p>
<p>Deepcoder: A fully open-source 14b coder at o3-mini level. Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Together AI Blog. 2025</p>
<p>Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, Wenhu Chen, arXiv:2505.14652General-reasoner: Advancing llm reasoning across all domains. 2025arXiv preprint</p>
<p>Information-based objective functions for active data selection. J C David, Mackay, Neural computation. 441992</p>
<p>Synthetic-1: Scaling distributed synthetic data generation for verified reasoning. Justus Mattern, Manveer, Jannik, Felix Matthew, Johannes Vincent, PrimeIntellect Blog. 2025</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Nature. 51875402015</p>
<p>Guilherme Penedo, Anton Lozhkov, Hynek Kydlíček, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, Leandro Von Werra, Codeforces dataset. 2025</p>
<p>Maximum entropy gain exploration for long horizon multi-goal reinforcement learning. Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, Jimmy Ba, ICML. 2020</p>
<p>Skewfit: State-covering self-supervised reinforcement learning. H Vitchyr, Murtaza Pong, Steven Dalal, Ashvin Lin, Shikhar Nair, Sergey Bahl, Levine, ICML. 2020</p>
<p>Maximizing confidence alone improves reasoning. Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, Deepak Pathak, arXiv:2505.226602025arXiv preprint</p>
<p>Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, arXiv:2501.01257Benchmarking competition-level code generation of llms with human-comparable elo ratings. 2025arXiv preprint</p>
<p>Qwq-32b: Embracing the power of reinforcement learning. Qwen, Qwen Blog. 2025</p>
<p>An Qwen, Baosong Yang, Beichen Yang, Binyuan Zhang, Bo Hui, Bowen Zheng, Chengyuan Yu, Dayiheng Li, Fei Liu, Huang, arXiv:2412.15115Georgios Smyrnis, Marianna Nezhurina, Trung Vu. Negin Raoof, Etash Kumar Guha, Ryan Marten, Jean Mercat, Eric Frankel, Sedrick Keh, Hritik Bansal, 2024. 20255 technical report. arXiv preprint</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, COLM. 2024</p>
<p>Universal value function approximators. Tom Schaul, Daniel Horgan, Karol Gregor, David Silver, ICML. 2015</p>
<p>Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. Jürgen Schmidhuber, 1987Technische Universität MünchenPhD thesis</p>
<p>Learning to generate sub-goals for action sequences. Jürgen Schmidhuber, Artificial neural networks. 1991</p>
<p>Rewarding progress: Scaling automated process verifiers for llm reasoning. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, Aviral Kumar, ICLR2025a</p>
<p>Optimizing llm test-time compute involves solving a meta-rl problem. Amrith Setlur, Yuxiao Qu, Matthew Yang, Lunjun Zhang, Virginia Smith, Aviral Kumar, 2025bCMU MLD Blog</p>
<p>Rulin Shao, Stella Shuyue, Rui Li, Scott Xin, Yiping Geng, Sewoong Wang, Simon Oh, Nathan Shaolei Du, Lambert, arXiv:2506.10947Sewon Min, Ranjay Krishna, et al. Spurious rewards: Rethinking training signals in rlvr. 2025arXiv preprint</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Yang Li, Wu, arXiv:2402.03300Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Thinking vs. doing: Agents that reason by scaling test-time interaction. Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, arXiv:2506.079762025arXiv preprint</p>
<p>Rl's razor: Why online reinforcement learning forgets less. Idan Shenfeld, Jyothish Pari, Pulkit Agrawal, arXiv:2509.042592025arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Richard S Sutton ; David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Google AI. 52975872025. 2016Nature</p>
<p>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, arXiv:1712.018152017arXiv preprint</p>
<p>Scaling llm test-time compute optimally can be more effective than scaling model parameters. Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar, ICLR2025</p>
<p>Test-time training with self-supervision for generalization under distribution shifts. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, Moritz Hardt, ICML. 2020</p>
<p>Learning to (learn at test time): Rnns with expressive hidden states. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, ICML. 2025</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, NeurIPS2017</p>
<p>Nino Johannes Von Oswald, Seijin Scherrer, Luca Kobayashi, Songlin Versari, Maximilian Yang, Kaitlin Schlegel, Yanick Maile, Oliver Schimpf, Alexander Sieberling, Meulemans, arXiv:2506.05233Sequence modeling by locally optimal test-time training. 2025arXiv preprint</p>
<p>Tent: Fully test-time adaptation by entropy minimization. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, Trevor Darrell, 2021ICLR</p>
<p>Math-shepherd: Verify and reinforce llms step-by-step without human annotations. Peiyi Wang, Lei Li, Zhihong Shao, Damai Xu, Yifei Dai, Deli Li, Yu Chen, Zhifang Wu, Sui, ACL. 2024a</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, ICLR2023</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, NeurIPS. 2024b</p>
<p>Unsupervised control through non-parametric discriminative rewards. David Warde-Farley, Tom Van De Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, Volodymyr Mnih, arXiv:1811.113592018arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, NeurIPS2022</p>
<p>Leetcodedataset: A temporal dataset for robust evaluation and efficient training of code llms. Yunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu, Huifeng Sun, Siyue Wu, Jian Hu, Xiaolong Xu, arXiv:2504.146552025arXiv preprint</p>
<p>An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, arXiv:2505.09388Chenxu Lv, et al. Qwen3 technical report. 2025arXiv preprint</p>
<p>Finemedlm-o1: Enhancing the medical reasoning ability of llm from supervised fine-tuning to test-time training. Hongzhou Yu, Tianhao Cheng, Ying Cheng, Rui Feng, COLM. 2025a</p>
<p>Dapo: An open-source llm reinforcement learning system at scale. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, arXiv:2503.144762025barXiv preprint</p>
<p>Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, Gao Huang, arXiv:2504.13837Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?. 2025arXiv preprint</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, NeurIPS2022</p>
<p>Memo: Test time robustness via adaptation and augmentation. Marvin Zhang, Sergey Levine, Chelsea Finn, NeurIPS2022</p>
<p>Right question is already half the answer: Fully unsupervised llm reasoning incentivization. Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, Yatao Bian, arXiv:2504.058122025aarXiv preprint</p>
<p>Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William T Freeman, Hao Tan, arXiv:2505.23884Test-time training done right. 2025barXiv preprint</p>
<p>Absolute zero: Reinforced self-play reasoning with zero data. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, Gao Huang, arXiv:2505.033352025arXiv preprint</p>
<p>Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, Chao Du, arXiv:2505.21493Reinforcing general reasoning without verifiers. 2025arXiv preprint</p>
<p>Ttrl: Test-time reinforcement learning. Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, Bowen Zhou, NeurIPS2025</p>
<p>Selfadapting language models. Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, Pulkit Agrawal, NeurIPS2025</p>            </div>
        </div>

    </div>
</body>
</html>
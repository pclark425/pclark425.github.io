<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1137 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1137</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1137</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-270995882</p>
                <p><strong>Paper Title:</strong> Bayesian reinforcement learning for navigation planning in unknown environments</p>
                <p><strong>Paper Abstract:</strong> This study focuses on a rescue mission problem, particularly enabling agents/robots to navigate efficiently in unknown environments. Technological advances, including manufacturing, sensing, and communication systems, have raised interest in using robots or drones for rescue operations. Effective rescue operations require quick identification of changes in the environment and/or locating the victims/injuries as soon as possible. Several techniques have been developed in recent years for autonomy in rescue missions, including motion planning, adaptive control, and more recently, reinforcement learning techniques. These techniques rely on full knowledge of the environment or the availability of simulators that can represent real environments during rescue operations. However, in practice, agents might have little or no information about the environment or the number or locations of injuries, preventing/limiting the application of most existing techniques. This study provides a probabilistic/Bayesian representation of the unknown environment, which jointly models the stochasticity in the agent's navigation and the environment uncertainty into a vector called the belief state. This belief state allows offline learning of the optimal Bayesian policy in an unknown environment without the need for any real data/interactions, which guarantees taking actions that are optimal given all available information. To address the large size of belief space, deep reinforcement learning is developed for computing an approximate Bayesian planning policy. The numerical experiments using different maze problems demonstrate the high performance of the proposed policy.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1137.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1137.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayes-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Q-Network Bayesian Planning Policy (Bayes-adaptive belief-space DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayes-adaptive planner that represents uncertainty over discrete maze models as a posterior (belief) and learns a policy in the belief MDP offline using a DQN; the learned policy jointly optimizes navigation reward and information acquisition (entropy reduction) by acting to change the posterior. Trained on simulated belief-state transitions so it can be executed in real time while updating the posterior online.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayesian planning policy implemented with DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy µ(b) ≈ argmax_a Q_w(b,a) where b = [s, ϑ] is the belief (agent state s plus posterior ϑ over environment models). Architecture: feedforward DQN with 3 hidden layers, 128 neurons per hidden layer, ReLU activations; Q-network and target network; replay buffer |D|=1e5, minibatch=64, learning rate α=5e-4, γ=0.95, ε=0.1, soft target update τ=1e-3. The DQN is trained offline using the known belief-transition dynamics derived from the ensemble of possible environment models.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayes-adaptive planning / information-gain-aware deep RL (belief-space DQN); non-myopic information-seeking via reward terms including entropy reduction</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Maintains and updates posterior ϑ over discrete environment models after each observation (Bayes update). The DQN policy takes the full belief b = [s,ϑ] as input and selects actions that trade off immediate task reward (e.g., locating victims) and future gains from reducing model uncertainty; entropy-reduction can be included explicitly in the reward. Offline training simulates belief transitions so the learned policy internalizes how actions will change the posterior and long-term returns.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Grid-maze rescue/navigation tasks (multiple mazes: 6×4, 4×4, 6×6, 10×10 with m unknown cells)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially known / uncertain discrete environment (unknown cells each ∈ {Wall, Empty, Injury}); stochastic transitions (action succeeds with prob p_a, otherwise moves to perpendicular neighbor); discrete state space; reward can be sparse (locating injuries) or shaped for information gain (entropy reduction).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Examples: 6×4 maze with 19 reachable states and m=3 unknown cells (3^3=27 models), 4×4 (13 states, 27 models), 6×6 (29 states, 27 models), 10×10 example with 67 states and m=4 unknowns (3^4=81 models). Action space size = 4 (up/down/left/right). Episode/test horizons: commonly 50 steps (test), training horizon up to 250 steps for small mazes and 500 steps for the largest maze; training episodes: 5,000 for many experiments, 20,000 for the largest maze. Evaluations averaged over 1,000 trials.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Matches or closely approaches the oracle baseline (RL with known true model) in many cases; e.g., in 6×4 maze (true [W,I,I], priors skewed) Bayes-DQN finds two injuries to the level of baseline and reaches both injuries after ≈20 steps; Table 1 (selected entries): at timestep 10 Proposed=0.957±0.017 (two-unknown-cells case), timestep 20 Proposed=1.200±0.030, timestep 30 Proposed=1.395±0.035 (numbers are average located injuries). Entropy-reduction objective: reaches zero entropy in ≈20 steps (6×4 case, all-empty truth) and ≈40 steps in larger 10×10 case; consistently faster entropy reduction than greedy one-step baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines reported: MAP and one-step active/entropy-greedy policies perform worse early and often overall. Example: in 6×4 two-unknown-cells at timestep 10 ActiveLearning=0.003±0.001, MAP=0.016±0.008 (much lower than Proposed 0.957±0.017). In several experiments baseline (oracle RL with true model) is an upper bound and Bayes-DQN approaches it (sometimes matches after ~20-50 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training: typically 5,000 episodes for small/medium mazes, 20,000 episodes for largest maze. Evaluation averaged over 1,000 trials. At execution-time the learned policy often locates multiple injuries within ~20–40 environment steps depending on maze size and stochasticity. (Paper reports the number of training episodes explicitly: 5,000 for many experiments, 20,000 for 10×10.)</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Tradeoff learned end-to-end: the DQN policy is trained to maximize discounted return where reward can include both task reward and information-gain (entropy reduction), yielding non-myopic exploration (actions chosen to improve future posterior and returns). During training an ε-greedy (ε=0.1) strategy is used to inject exploration; the learned policy itself balances exploration vs exploitation via the reward formulation over beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against: (1) Baseline: RL trained with the true environment model (oracle), (2) MAP policy: act according to the optimal policy of the single model with highest posterior (θ_MAP), (3) Active learning / one-step expected-Q policy (a_k = argmax_a E_{ϑ}[q*_θ(s,a)]), and (4) one-step entropy-reduction greedy policy in entropy experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) The Bayes-adaptive DQN that learns over the belief MDP achieves substantially better early-time performance (important for time-sensitive rescue) than MAP and one-step active baselines, often matching the oracle baseline within 20–50 environment steps. 2) When the reward encodes entropy reduction, the learned belief policy produces much faster entropy reduction than greedy one-step entropy maximizers (e.g., zero entropy in ~20 steps vs >50 steps for greedy in some mazes). 3) The method is robust to transition stochasticity: performance degrades gracefully as movement stochasticity increases (p_a values tested: 1.0, 0.8, 0.6, 0.4). 4) The approach requires offline simulation of belief transitions (no real-world interactions needed during learning) enabling real-time execution with posterior updates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Scalability: posterior dimensionality grows exponentially with number of unknown cells (3^m) making belief-space size intractable for large m or continuous/infinite model spaces. Performance depends on quality of priors; with strongly incorrect priors the method can be initially suboptimal (but typically recovers as posterior updates). Training cost grows with problem size (larger mazes required more episodes/horizon). No experiments on adversarial or non-stationary model changes during execution (although authors claim method handles unfolding information without retraining for stationary and some non-stationary cases, specifics not demonstrated).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian reinforcement learning for navigation planning in unknown environments', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1137.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1137.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active-1step-Q / entropy-greedy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-step active learning (one-step expected-Q) and one-step entropy-reduction greedy policies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Greedy adaptive strategies used as baselines: (a) one-step expected-Q active policy that picks actions maximizing expected one-step Q under current posterior, and (b) one-step entropy-reduction policy that picks actions maximizing immediate expected reduction in model entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>One-step active/entropy-greedy baselines</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Non-myopic planning is not used; policies compute per-action expected immediate utility: (a) active learning uses a_k = argmax_a E_{ϑ}[q*_θ(s,a)] (weighted average of model-specific Q-values under the posterior); (b) entropy-greedy uses a_k = argmax_a −E_{b'|b,a} H(ϑ') − H(ϑ) to greedily reduce next-step entropy. These baselines require model-specific Q-values or belief-transition calculation for one-step lookahead only.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active learning (one-step expected reward aggregation) and greedy information gain (one-step entropy reduction)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each step uses current posterior ϑ to compute immediate expected gain (expected Q or immediate entropy reduction) and selects the action with maximal immediate expected utility; does not plan longer horizons to account for how actions will change future posterior beyond the next step.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same grid-maze rescue/navigation tasks as the main method (6×4, 4×4, 6×6, 10×10 examples)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially known discrete mazes with stochastic transitions and unknown cell types; same environment families used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same configurations described for Bayes-DQN (e.g., 6×4 with 19 states, 3 unknown cells → 27 models).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Often slower and worse early than Bayes-DQN. Examples: in one 6×4 experiment active learning reached maximum performance only after 50 steps and had very low performance in first 30 steps; in another case active learning matched baseline and outperformed Bayes-DQN when the ground-truth aligned with prior (i.e., when all unknowns were injuries and prior favored injuries). Table examples: active learning in two-unknown-cells case at timestep 10 = 0.003±0.001 (very low) and at timestep 30 = 0.008±0.002 or 1.005±0.055 in other configurations depending on scenario (paper reports multiple cases).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Compared to random or MAP baselines, active learning sometimes performs better long-term but worse short-term; specific random baseline numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Operates online with no separate training but requires model-specific Q-values for (a) or belief-transition computations for (b) in order to evaluate one-step utility; in practice authors compute model-specific Q-values offline for all models (costly) and then perform greedy selection online.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Purely greedy on one-step expected utility; exploration arises only if the one-step criterion values information-gaining actions, but no explicit long-horizon exploration-exploitation planning is performed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared directly to Bayes-DQN and MAP and oracle baseline in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>One-step active and entropy-greedy methods can work well when posterior is already peaked or when the immediate reward aligns with long-term objectives, but they often underperform in early, time-sensitive phases because they are myopic and may fail to take actions that reduce uncertainty in ways that pay off later. They sometimes match oracle baseline in scenarios where uncertainty structure and prior align with truth.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Myopia: inability to plan over horizons leads to poor early performance in many experiments; may leave posterior unchanged for long periods (as noted in text). Requires computing model-specific Q-values (active-Q variant) for all models which is computationally expensive; one-step entropy greedy cannot account for multi-step benefits of information gathering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian reinforcement learning for navigation planning in unknown environments', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1137.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1137.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAP-policy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum a posteriori model policy (MAP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that selects at each step the action prescribed by the optimal policy of the single most probable environment model under the current posterior (θ_MAP).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MAP navigation policy</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>At belief ϑ_k pick θ_MAP = argmax_θ ϑ_k(θ) then act using the optimal policy π*_θ_MAP(s). Requires having computed or stored optimal policies/Q-values for every candidate model offline.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Posterior-based greedy selection of a single most-likely model (no explicit information-seeking beyond the implied effect of assuming MAP)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Updates posterior ϑ after observations; action selection uses only the single most likely model's optimal policy, ignoring full posterior distribution and information-gain considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same grid-maze rescue/navigation tasks used as benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially known discrete mazes with stochastic transitions and sparse task rewards (locating injuries) or entropy objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as other baselines: 6×4 (19 states, 27 models), etc.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Generally poor in experiments: often the worst-performing baseline. Examples: in 6×4 experiments MAP performs worse than both Bayes-DQN and active learning across many timesteps; Table 1 shows MAP at timestep 10 two-unknowns = 0.016±0.008 (versus Proposed 0.957±0.017). MAP is particularly poor when posterior is not strongly peaked or prior is misleading.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Requires computing model-specific optimal policies offline for each candidate model (expensive), but online action selection is cheap; sample efficiency is low in practice because reliance on single model yields poor robustness to model uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>None beyond what is embedded in the chosen model's optimal policy; MAP does not intentionally explore to reduce uncertainty and can get stuck following a wrong model's plan.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against Bayes-DQN (proposed), active one-step, and oracle baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>MAP is fragile: performs poorly when posterior distributions are ambiguous or wrong; it can be beaten by myopic active methods and by Bayes-DQN which explicitly accounts for posterior evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Breaks down when posterior is not concentrated or when multiple models have similar posterior mass; lacks mechanism to actively reduce model uncertainty and thus shows poor performance in time-sensitive rescue tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian reinforcement learning for navigation planning in unknown environments', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1137.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1137.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayes-adaptive methods (literature)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayes-adaptive reinforcement learning (prior literature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Class of methods that maintain and update a posterior over MDPs and plan in the Bayes-adaptive MDP (the belief MDP), typically with guarantees but high sample/computational cost for large or continuous spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayes-adaptive RL (general literature)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Methods that iteratively update posterior distributions over model parameters and compute policies that are optimal given the current posterior (exact Bayes-optimal planning or approximate solvers). Often intractable for large state spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayes-adaptive planning (belief-space planning), sample-based approximations (e.g., BAMCP), or meta-learning based approximations</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Maintain posterior over dynamics/reward parameters and perform planning in augmented state space (belief states) so actions are chosen accounting for their effect on future beliefs and rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Typically assumed fully unknown MDPs with finite state/action spaces in many classic works; known to struggle with large/continuous state spaces or when interactions are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Often limited to finite small MDPs in exact formulations; approximations attempt to scale but with trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Classic Bayes-adaptive methods can require large numbers of interactions to learn transition distributions; referenced in paper as sample-inefficient in large/continuous/partially-known domains.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Naturally balances exploration and exploitation by planning in belief MDP (optimal Bayes policy), but in practice computational cost often prevents deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Mentioned as related work (e.g., Guez et al. 2012; Rigter et al. 2021; Zintgraf et al. 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>The paper cites these methods as principled but often inapplicable to large or partially-known domains due to the need for many interactions and exponential state growth; motivates learning approximate belief-space policies with deep RL instead.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Scalability and sample complexity; often restricted to small discrete MDPs and require many interactions to infer transition probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian reinforcement learning for navigation planning in unknown environments', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient Bayes-adaptive reinforcement learning using sample-based search <em>(Rating: 2)</em></li>
                <li>VariBAD: a very good method for Bayes-adaptive deep RL via meta-learning <em>(Rating: 2)</em></li>
                <li>A bayesian active learning approach to adaptive motion planning <em>(Rating: 1)</em></li>
                <li>Bayesian reinforcement learning: a survey <em>(Rating: 1)</em></li>
                <li>Planning and navigation as active inference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1137",
    "paper_id": "paper-270995882",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "Bayes-DQN",
            "name_full": "Deep Q-Network Bayesian Planning Policy (Bayes-adaptive belief-space DQN)",
            "brief_description": "A Bayes-adaptive planner that represents uncertainty over discrete maze models as a posterior (belief) and learns a policy in the belief MDP offline using a DQN; the learned policy jointly optimizes navigation reward and information acquisition (entropy reduction) by acting to change the posterior. Trained on simulated belief-state transitions so it can be executed in real time while updating the posterior online.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Bayesian planning policy implemented with DQN",
            "agent_description": "Policy µ(b) ≈ argmax_a Q_w(b,a) where b = [s, ϑ] is the belief (agent state s plus posterior ϑ over environment models). Architecture: feedforward DQN with 3 hidden layers, 128 neurons per hidden layer, ReLU activations; Q-network and target network; replay buffer |D|=1e5, minibatch=64, learning rate α=5e-4, γ=0.95, ε=0.1, soft target update τ=1e-3. The DQN is trained offline using the known belief-transition dynamics derived from the ensemble of possible environment models.",
            "adaptive_design_method": "Bayes-adaptive planning / information-gain-aware deep RL (belief-space DQN); non-myopic information-seeking via reward terms including entropy reduction",
            "adaptation_strategy_description": "Maintains and updates posterior ϑ over discrete environment models after each observation (Bayes update). The DQN policy takes the full belief b = [s,ϑ] as input and selects actions that trade off immediate task reward (e.g., locating victims) and future gains from reducing model uncertainty; entropy-reduction can be included explicitly in the reward. Offline training simulates belief transitions so the learned policy internalizes how actions will change the posterior and long-term returns.",
            "environment_name": "Grid-maze rescue/navigation tasks (multiple mazes: 6×4, 4×4, 6×6, 10×10 with m unknown cells)",
            "environment_characteristics": "Partially known / uncertain discrete environment (unknown cells each ∈ {Wall, Empty, Injury}); stochastic transitions (action succeeds with prob p_a, otherwise moves to perpendicular neighbor); discrete state space; reward can be sparse (locating injuries) or shaped for information gain (entropy reduction).",
            "environment_complexity": "Examples: 6×4 maze with 19 reachable states and m=3 unknown cells (3^3=27 models), 4×4 (13 states, 27 models), 6×6 (29 states, 27 models), 10×10 example with 67 states and m=4 unknowns (3^4=81 models). Action space size = 4 (up/down/left/right). Episode/test horizons: commonly 50 steps (test), training horizon up to 250 steps for small mazes and 500 steps for the largest maze; training episodes: 5,000 for many experiments, 20,000 for the largest maze. Evaluations averaged over 1,000 trials.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Matches or closely approaches the oracle baseline (RL with known true model) in many cases; e.g., in 6×4 maze (true [W,I,I], priors skewed) Bayes-DQN finds two injuries to the level of baseline and reaches both injuries after ≈20 steps; Table 1 (selected entries): at timestep 10 Proposed=0.957±0.017 (two-unknown-cells case), timestep 20 Proposed=1.200±0.030, timestep 30 Proposed=1.395±0.035 (numbers are average located injuries). Entropy-reduction objective: reaches zero entropy in ≈20 steps (6×4 case, all-empty truth) and ≈40 steps in larger 10×10 case; consistently faster entropy reduction than greedy one-step baselines.",
            "performance_without_adaptation": "Baselines reported: MAP and one-step active/entropy-greedy policies perform worse early and often overall. Example: in 6×4 two-unknown-cells at timestep 10 ActiveLearning=0.003±0.001, MAP=0.016±0.008 (much lower than Proposed 0.957±0.017). In several experiments baseline (oracle RL with true model) is an upper bound and Bayes-DQN approaches it (sometimes matches after ~20-50 steps).",
            "sample_efficiency": "Training: typically 5,000 episodes for small/medium mazes, 20,000 episodes for largest maze. Evaluation averaged over 1,000 trials. At execution-time the learned policy often locates multiple injuries within ~20–40 environment steps depending on maze size and stochasticity. (Paper reports the number of training episodes explicitly: 5,000 for many experiments, 20,000 for 10×10.)",
            "exploration_exploitation_tradeoff": "Tradeoff learned end-to-end: the DQN policy is trained to maximize discounted return where reward can include both task reward and information-gain (entropy reduction), yielding non-myopic exploration (actions chosen to improve future posterior and returns). During training an ε-greedy (ε=0.1) strategy is used to inject exploration; the learned policy itself balances exploration vs exploitation via the reward formulation over beliefs.",
            "comparison_methods": "Compared against: (1) Baseline: RL trained with the true environment model (oracle), (2) MAP policy: act according to the optimal policy of the single model with highest posterior (θ_MAP), (3) Active learning / one-step expected-Q policy (a_k = argmax_a E_{ϑ}[q*_θ(s,a)]), and (4) one-step entropy-reduction greedy policy in entropy experiments.",
            "key_results": "1) The Bayes-adaptive DQN that learns over the belief MDP achieves substantially better early-time performance (important for time-sensitive rescue) than MAP and one-step active baselines, often matching the oracle baseline within 20–50 environment steps. 2) When the reward encodes entropy reduction, the learned belief policy produces much faster entropy reduction than greedy one-step entropy maximizers (e.g., zero entropy in ~20 steps vs &gt;50 steps for greedy in some mazes). 3) The method is robust to transition stochasticity: performance degrades gracefully as movement stochasticity increases (p_a values tested: 1.0, 0.8, 0.6, 0.4). 4) The approach requires offline simulation of belief transitions (no real-world interactions needed during learning) enabling real-time execution with posterior updates.",
            "limitations_or_failures": "Scalability: posterior dimensionality grows exponentially with number of unknown cells (3^m) making belief-space size intractable for large m or continuous/infinite model spaces. Performance depends on quality of priors; with strongly incorrect priors the method can be initially suboptimal (but typically recovers as posterior updates). Training cost grows with problem size (larger mazes required more episodes/horizon). No experiments on adversarial or non-stationary model changes during execution (although authors claim method handles unfolding information without retraining for stationary and some non-stationary cases, specifics not demonstrated).",
            "uuid": "e1137.0",
            "source_info": {
                "paper_title": "Bayesian reinforcement learning for navigation planning in unknown environments",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Active-1step-Q / entropy-greedy",
            "name_full": "One-step active learning (one-step expected-Q) and one-step entropy-reduction greedy policies",
            "brief_description": "Greedy adaptive strategies used as baselines: (a) one-step expected-Q active policy that picks actions maximizing expected one-step Q under current posterior, and (b) one-step entropy-reduction policy that picks actions maximizing immediate expected reduction in model entropy.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "One-step active/entropy-greedy baselines",
            "agent_description": "Non-myopic planning is not used; policies compute per-action expected immediate utility: (a) active learning uses a_k = argmax_a E_{ϑ}[q*_θ(s,a)] (weighted average of model-specific Q-values under the posterior); (b) entropy-greedy uses a_k = argmax_a −E_{b'|b,a} H(ϑ') − H(ϑ) to greedily reduce next-step entropy. These baselines require model-specific Q-values or belief-transition calculation for one-step lookahead only.",
            "adaptive_design_method": "Active learning (one-step expected reward aggregation) and greedy information gain (one-step entropy reduction)",
            "adaptation_strategy_description": "At each step uses current posterior ϑ to compute immediate expected gain (expected Q or immediate entropy reduction) and selects the action with maximal immediate expected utility; does not plan longer horizons to account for how actions will change future posterior beyond the next step.",
            "environment_name": "Same grid-maze rescue/navigation tasks as the main method (6×4, 4×4, 6×6, 10×10 examples)",
            "environment_characteristics": "Partially known discrete mazes with stochastic transitions and unknown cell types; same environment families used for comparison.",
            "environment_complexity": "Same configurations described for Bayes-DQN (e.g., 6×4 with 19 states, 3 unknown cells → 27 models).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Often slower and worse early than Bayes-DQN. Examples: in one 6×4 experiment active learning reached maximum performance only after 50 steps and had very low performance in first 30 steps; in another case active learning matched baseline and outperformed Bayes-DQN when the ground-truth aligned with prior (i.e., when all unknowns were injuries and prior favored injuries). Table examples: active learning in two-unknown-cells case at timestep 10 = 0.003±0.001 (very low) and at timestep 30 = 0.008±0.002 or 1.005±0.055 in other configurations depending on scenario (paper reports multiple cases).",
            "performance_without_adaptation": "Compared to random or MAP baselines, active learning sometimes performs better long-term but worse short-term; specific random baseline numbers not provided.",
            "sample_efficiency": "Operates online with no separate training but requires model-specific Q-values for (a) or belief-transition computations for (b) in order to evaluate one-step utility; in practice authors compute model-specific Q-values offline for all models (costly) and then perform greedy selection online.",
            "exploration_exploitation_tradeoff": "Purely greedy on one-step expected utility; exploration arises only if the one-step criterion values information-gaining actions, but no explicit long-horizon exploration-exploitation planning is performed.",
            "comparison_methods": "Compared directly to Bayes-DQN and MAP and oracle baseline in the experiments.",
            "key_results": "One-step active and entropy-greedy methods can work well when posterior is already peaked or when the immediate reward aligns with long-term objectives, but they often underperform in early, time-sensitive phases because they are myopic and may fail to take actions that reduce uncertainty in ways that pay off later. They sometimes match oracle baseline in scenarios where uncertainty structure and prior align with truth.",
            "limitations_or_failures": "Myopia: inability to plan over horizons leads to poor early performance in many experiments; may leave posterior unchanged for long periods (as noted in text). Requires computing model-specific Q-values (active-Q variant) for all models which is computationally expensive; one-step entropy greedy cannot account for multi-step benefits of information gathering.",
            "uuid": "e1137.1",
            "source_info": {
                "paper_title": "Bayesian reinforcement learning for navigation planning in unknown environments",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "MAP-policy",
            "name_full": "Maximum a posteriori model policy (MAP)",
            "brief_description": "A baseline that selects at each step the action prescribed by the optimal policy of the single most probable environment model under the current posterior (θ_MAP).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "MAP navigation policy",
            "agent_description": "At belief ϑ_k pick θ_MAP = argmax_θ ϑ_k(θ) then act using the optimal policy π*_θ_MAP(s). Requires having computed or stored optimal policies/Q-values for every candidate model offline.",
            "adaptive_design_method": "Posterior-based greedy selection of a single most-likely model (no explicit information-seeking beyond the implied effect of assuming MAP)",
            "adaptation_strategy_description": "Updates posterior ϑ after observations; action selection uses only the single most likely model's optimal policy, ignoring full posterior distribution and information-gain considerations.",
            "environment_name": "Same grid-maze rescue/navigation tasks used as benchmarks",
            "environment_characteristics": "Partially known discrete mazes with stochastic transitions and sparse task rewards (locating injuries) or entropy objectives.",
            "environment_complexity": "Same as other baselines: 6×4 (19 states, 27 models), etc.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": "Generally poor in experiments: often the worst-performing baseline. Examples: in 6×4 experiments MAP performs worse than both Bayes-DQN and active learning across many timesteps; Table 1 shows MAP at timestep 10 two-unknowns = 0.016±0.008 (versus Proposed 0.957±0.017). MAP is particularly poor when posterior is not strongly peaked or prior is misleading.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Requires computing model-specific optimal policies offline for each candidate model (expensive), but online action selection is cheap; sample efficiency is low in practice because reliance on single model yields poor robustness to model uncertainty.",
            "exploration_exploitation_tradeoff": "None beyond what is embedded in the chosen model's optimal policy; MAP does not intentionally explore to reduce uncertainty and can get stuck following a wrong model's plan.",
            "comparison_methods": "Compared against Bayes-DQN (proposed), active one-step, and oracle baseline.",
            "key_results": "MAP is fragile: performs poorly when posterior distributions are ambiguous or wrong; it can be beaten by myopic active methods and by Bayes-DQN which explicitly accounts for posterior evolution.",
            "limitations_or_failures": "Breaks down when posterior is not concentrated or when multiple models have similar posterior mass; lacks mechanism to actively reduce model uncertainty and thus shows poor performance in time-sensitive rescue tasks.",
            "uuid": "e1137.2",
            "source_info": {
                "paper_title": "Bayesian reinforcement learning for navigation planning in unknown environments",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Bayes-adaptive methods (literature)",
            "name_full": "Bayes-adaptive reinforcement learning (prior literature)",
            "brief_description": "Class of methods that maintain and update a posterior over MDPs and plan in the Bayes-adaptive MDP (the belief MDP), typically with guarantees but high sample/computational cost for large or continuous spaces.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Bayes-adaptive RL (general literature)",
            "agent_description": "Methods that iteratively update posterior distributions over model parameters and compute policies that are optimal given the current posterior (exact Bayes-optimal planning or approximate solvers). Often intractable for large state spaces.",
            "adaptive_design_method": "Bayes-adaptive planning (belief-space planning), sample-based approximations (e.g., BAMCP), or meta-learning based approximations",
            "adaptation_strategy_description": "Maintain posterior over dynamics/reward parameters and perform planning in augmented state space (belief states) so actions are chosen accounting for their effect on future beliefs and rewards.",
            "environment_name": "",
            "environment_characteristics": "Typically assumed fully unknown MDPs with finite state/action spaces in many classic works; known to struggle with large/continuous state spaces or when interactions are limited.",
            "environment_complexity": "Often limited to finite small MDPs in exact formulations; approximations attempt to scale but with trade-offs.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": "Classic Bayes-adaptive methods can require large numbers of interactions to learn transition distributions; referenced in paper as sample-inefficient in large/continuous/partially-known domains.",
            "exploration_exploitation_tradeoff": "Naturally balances exploration and exploitation by planning in belief MDP (optimal Bayes policy), but in practice computational cost often prevents deployment.",
            "comparison_methods": "Mentioned as related work (e.g., Guez et al. 2012; Rigter et al. 2021; Zintgraf et al. 2021).",
            "key_results": "The paper cites these methods as principled but often inapplicable to large or partially-known domains due to the need for many interactions and exponential state growth; motivates learning approximate belief-space policies with deep RL instead.",
            "limitations_or_failures": "Scalability and sample complexity; often restricted to small discrete MDPs and require many interactions to infer transition probabilities.",
            "uuid": "e1137.3",
            "source_info": {
                "paper_title": "Bayesian reinforcement learning for navigation planning in unknown environments",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient Bayes-adaptive reinforcement learning using sample-based search",
            "rating": 2,
            "sanitized_title": "efficient_bayesadaptive_reinforcement_learning_using_samplebased_search"
        },
        {
            "paper_title": "VariBAD: a very good method for Bayes-adaptive deep RL via meta-learning",
            "rating": 2,
            "sanitized_title": "varibad_a_very_good_method_for_bayesadaptive_deep_rl_via_metalearning"
        },
        {
            "paper_title": "A bayesian active learning approach to adaptive motion planning",
            "rating": 1,
            "sanitized_title": "a_bayesian_active_learning_approach_to_adaptive_motion_planning"
        },
        {
            "paper_title": "Bayesian reinforcement learning: a survey",
            "rating": 1,
            "sanitized_title": "bayesian_reinforcement_learning_a_survey"
        },
        {
            "paper_title": "Planning and navigation as active inference",
            "rating": 1,
            "sanitized_title": "planning_and_navigation_as_active_inference"
        }
    ],
    "cost": 0.015445999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Bayesian reinforcement learning for navigation planning in unknown environments</p>
<p>Mayukh Das 
Anjaly Parayil 
Mohammad Alali alali.m@northeastern.edu 
Mahdi Imani </p>
<p>Microsoft Research
India</p>
<p>Microsoft Research
India</p>
<p>Xian Yeow Lee</p>
<p>Hitachi America Ltd
United States</p>
<p>Department of Electrical and Computer Engineering
Northeastern University
BostonMAUnited States</p>
<p>Bayesian reinforcement learning for navigation planning in unknown environments
0CFC36422E717B76A72D970FE32B5257RECEIVED October ACCEPTED June PUBLISHED July CITATIONrescue operationsMarkov decision processreinforcement learningBayesian decisionmakingnavigation planning
Alali M and Imani M () Bayesian reinforcement learning for navigation planning in unknown environments.</p>
<p>Introduction</p>
<p>Advances in robotics, sensing, manufacturing, and communication in recent years have raised interest in the use of autonomous agents instead of humans for rescue missions.Examples include using robots for time-sensitive and dangerous rescue missions, such as response to earthquakes, mass shootings, hurricanes, and warfare zones.The utmost factor in rescue operations is quick identification of changes in an environment or locating victims/injuries in need of critical care.</p>
<p>A maze environment containing a single robot and a victim is shown in Figure 1.Let the yellow cells represent the unknown parts of the environment after the disaster, which could possibly be blocked with debris.Three possible navigation paths are shown in the maze.Given the unknown parts of the environment, should the agent take the right path, which is the closest to the victim?Wouldn't it be better to take the longer black path without any potential blockages (no yellow cells)?How can the robot change its decision as more information about the victim's condition and the environment appears?</p>
<p>Several techniques for achieving autonomy in rescue operations have been developed in recent years, with reinforcement learning (RL) being one of the prominent methods.In recent years, RL techniques have achieved remarkable success across various domains, including network security, biological applications, and robotics (Alali andImani, 2023, 2024;Elguea-Aguinaco et al., 2023;Ravari et al., 2023;Alali et al., 2024;Asadi et al., 2024).For autonomy in rescue operations, various RL techniques have been developed for single-agent and multi-agent settings (Imanberdiyev et al., 2016;Zhang et al., 2018;Bøhn et al., 2019;Lin et al., 2019;Niroui et al., 2019;Sampedro et al., 2019;Ebrahimi et al., 2020;Hu et al., 2020;Wu et al., 2021).These RL techniques can be divided into model-based and simulation-based categories.The model-based RL approaches (Imanberdiyev et al., 2016;Pham et al., 2018;Ladosz et al., 2019;Sampedro et al., 2019;Xu et al., 2019) assume full knowledge (including potential changes/casualties) about the agent's environment during the rescue; the simulationbased RL techniques (Akcakoca et al., 2019;Wang et al., 2019;Blum et al., 2020;Hamid et al., 2021;Jagannath et al., 2021;Falcone and Putnam, 2022), on the other hand, rely on the availability of simulators to represent the environment during disaster response.However, the environment during the rescue operations is often unknown or partially known to the human and agent, and it is prudent for an agent to make decisions given the incomplete available information.For instance, in response to an earthquake, the number and location of victims/injuries and the extent of damage to the environment are often unknown at the early stages of rescue operations.This prevents the applicability of existing RL techniques in time-sensitive and unknown environments.It should also be noted that the RL techniques cannot be employed for learning a policy through real interactions with the unknown environment, as RL often requires thousands of interactions to learn to act in an unknown environment, which is impossible due to the time-sensitive nature of rescue operations.</p>
<p>Several techniques have been developed to combine Bayesian approaches with RL methods (Ghavamzadeh et al., 2015;Imani and Ghoreishi, 2022).Most of these approaches aim to address the sample efficiency of RL methods during the learning process (Ghavamzadeh et al., 2015;Imani et al., 2018;Kamthe and Deisenroth, 2018).Meanwhile, Bayesian approaches have been used to quantify the discrepancies between real-world environments and simulations, facilitating sim-to-real policy transfer (Feng et al., 2022;Rothfuss et al., 2024).Other Bayesian approaches have also been developed in multi-agent and human-AI teaming to learn the intentions and preferences of teammates using partial data (Lin et al., 2024;Ravari et al., 2024a,b;Zhang et al., 2024a,b).The most relevant class of approaches considering uncertainty in environments is Bayes-adaptive methods (Guez et al., 2012;Rigter et al., 2021;Zintgraf et al., 2021).These methods iteratively update the posterior distribution of the environment and simultaneously update the planning policy based on the latest interactions.However, these methods are applicable to domains with finite state spaces and fully unknown environments, where a huge number of interactions are needed to learn the distribution of the transition probabilities.These methods are also not applicable to partially known environments or domains with large and continuous state spaces and often perform poorly under limited available interactions.</p>
<p>Motion planning (Zhang et al., 2013;Perez-Imaz et al., 2016;Cabreira et al., 2019;de Almeida et al., 2019;Boulares and Barnawi, 2021) is another class of model-based approaches which aims to take advantage of the system model for offline planning.Examples of these methods are LQR/LQG methods and their non-linear variations (Richter and Roy, 2017;Kim et al., 2019Kim et al., , 2021;;Bouman et al., 2020;Rosolia et al., 2022), which rely on the full or rich knowledge of the system model for planning or replanning, which prevents their applications in unknown environments.In this regard, active learning, model-predictive control, and online learning techniques have been developed for decision-making in unknown environments (Juang and Chang, 2011;Luo et al., 2014;Greatwood and Richards, 2019;Li et al., 2019;Chang et al., 2021).These methods mostly rely on a greedy and local view of the environment and perform poorly in complex realistic domains.Safety in navigation in unknown environments has also been studied extensively in the literature (Bajcsy et al., 2019;Krell et al., 2019;Tordesillas et al., 2019), which focuses on guaranteeing safety in domains with sensitive constraints.</p>
<p>This study develops a reinforcement learning Bayesian planning policy for rescue operations in unknown environments.We define a belief state, which keeps a joint probabilistic representation of all the possible models for the environment and agent movements.The belief state allows a Markov decision process (MDP) formulation of an agent in unknown and uncertain environments, allowing propagation of the entire uncertainty offline without the need for interaction with the real environment.We formulate the exact optimal Bayesian planning policy, which guarantees that an agent acts optimally given all available information.A Bayesian solution is introduced using a deep reinforcement learning technique, allowing offline learning of the policy over the whole belief space.We demonstrate that the proposed reinforcement learning Bayesian policy can be employed in real time for rescue missions in stationary and non-stationary environments as any additional information unfolds (i.e., without the need for learning or retraining).The effectiveness of the proposed method is demonstrated using comprehensive numerical experiments using different maze problems.</p>
<p>The article is organized as follows.In Section 2, the background of the Markov decision process is briefly described.In Section 3, the optimal Bayesian policy is formulated, and a solution based on deep reinforcement learning is introduced.Section 4 includes a discussion about the capabilities and complexity of the proposed method.Finally, Section 5 and Section 6 contain numerical examples and concluding remarks, respectively.</p>
<p>Background-A Markov decision process</p>
<p>A Markov decision process (MDP) can be defined by a 4-tuple S, A, T θ , R θ , where S is the state space, A is the action space, T θ : S × A × S is the unknown or partially known state transition probability function such that T θ (s, a, s ′ ) = P(s ′ | s, a, θ) with the set of unknown parameters θ ∈ , and R θ : S × A × S → R</p>
<p>FIGURE</p>
<p>Illustrative example of the rescue operation problem in unknown environments.The rescue robot can take any of the black, green, and orange paths to get to the victim.Depending on the type of the unknown cells and whether each of the unknown cells is more probable to be wall, empty, or another victim/injury, the rescue robot should plan to take the best path.</p>
<p>is a bounded reward function with a real value outcome such that R θ (s, a, s ′ ) encodes the reward earned when action a is taken in state s and the agent moves to state s ′ in model θ .The reward function could be model-dependent in general form, meaning that similar transitions in different models of the environment might lead to different rewards.</p>
<p>If a given MDP parameterized by θ is a true environment, we could define a deterministic policy π : S → A as a mapping from states to actions.The expected discounted reward function at state s ∈ S after taking action a ∈ A and following policy π afterward is defined as follows:
q π θ (s, a) = E h t=0 γ t R θ (s t , a t , s t+1 ) | s 0 = s, a 0 = a, a 1 : ∞ ∼ π , θ .
(1) where γ is a discount factor.In the finite-horizon case, the discount factor is typically set to 1.In the infinite-horizon case (h = ∞), the discount factor γ ∈ [0, 1) is included to obtain a finite sum.</p>
<p>According to (1), the expected return under the optimal policy π * θ for the environment modeled by θ can be expressed as follows:
q * θ (s, a) = E h t=0 γ t R θ (s t , a t , s t+1 ) | s 0 = s, a 0 = a, a 1 : ∞ ∼ π * θ , θ ,
(2) where q * θ (s, a) indicates the expected discounted reward after executing action a in state s and following optimal policy π * θ afterward.An optimal model-specific policy π * θ attains the maximum expected return as π * θ (s) = argmax a∈A q * θ (s, a), for s ∈ S.</p>
<p>If the true environment model, that is, θ * , was fully known to the agent, the optimal policy π * θ * could be obtained using (2).However, the environment is often unknown in rescue operations, and the agent needs to make decisions given partial knowledge about the environment and/or the number or location of injuries.</p>
<p>Proposed Bayesian planning policy for decision-making in unknown environments .Probabilistic/Bayesian formulation of unknown and uncertain environments This section first describes the challenges of decision-making in an unknown environment, followed by the proposed framework to overcome these challenges effectively.Let us consider a maze problem as a simple example of navigation in rescue operations, where each cell in the maze could be one of the followings: wall "W", empty "E", or injury/victim "I". Figure 2 represents an example of a maze problem, where the black and white colors indicate the wall and empty cells, respectively.The yellow cells are unknown parts of the environment, which each could potentially be a wall (i.e., blocked after a disaster), be empty, or contain a victim/injury.Let {c 1 , ..., c m } be m unknown cells in the environment, where c i ∈ {W, E, I}.These unknown cells lead to 3 m different possible environment models (i.e., maze models) denoted by = {θ 1 , ..., θ 3 m }, where θ j = [θ j (1), • • • , θ j (m)] represents an MDP parametrized by θ j , and θ j (l) denotes the type of the lth unknown cell under the jth maze model.Note that the FIGURE Visualization of the × maze problem.This maze has three unknown cells, where each could be either wall, empty, or victim/injury.three unknown cells in Figure 2 lead to 3 3 = 27 possible maze models, where the environments could potentially contain 0, 1, 2, or 3 injuries.In practice, the true environment θ * is often hidden among all the possible models in .For each θ ∈ , the number and location of injuries and walls vary.Therefore, the optimal model-specific policies for these models [see Equation ( 2)] are different in general; thus, the policies obtained for different models θ ∈ cannot be directly employed in unknown environments.It should be noted that the maze problem and its uncertain elements are examples of rescue missions, and the proposed method in this study could be applied to more general environments and environment uncertainties.</p>
<p>Let the initial knowledge about possible models for the true environment be represented as ϑ 0 = [P(θ * = θ 1 ), ..., P(θ * = θ 3 m )], where P(θ * = θ j ) shows the prior probability that the jth model is the true environment model and 3 m j=1 ϑ 0 (j) = 1.Let a 0 : k−1 = {a 0 , ..., a k−1 } be the agent's actions and s 1 : k = {s 1 , ..., s k } be the agent states until time step k.The posterior distribution of models can be expressed as follows:
ϑ k = P(θ * = θ 1 | s 1 : k , a 0 : k−1 ), • • • , P(θ * = θ 3 m | s 1 : k , a 0 : k−1 ) ,
(3) where ϑ k (j) indicates the posterior probability that model θ j is the true model, and we also have 3 m j=1 ϑ k (j) = 1.Note that if no prior information about the models is available, a uniform (i.e., non-informative) distribution can be employed.If the independency of distribution of m unknown cells is assumed, the posterior probability could be defined over the m unknown cells as (Equation 4)
p k =[P(c 1 = W | s 1 : k , a 0 : k−1 ), P(c 1 = E | s 1 : k , a 0 : k−1 ), P(c 1 = I | s 1 : k , a 0 : k−1 ), • • • , P(c m = W | s 1 : k , a 0 : k−1 ), P(c m = E | s 1 : k , a 0 : k−1 ), P(c m = I | s 1 : k , a 0 : k−1 )],
(4) which leads to the posterior distribution over all the possible models (i.e., ϑ k ) as
ϑ k (i) = m l=1 1 θ i (l)=W p k (3l − 2) + 1 θ i (l)=E p k (3l − 1) + 1 θ i (l)=I p k (3l) ,
(5) for i = 1, ..., 3 m and 1 condition returns 1 if the condition is true, and 0 otherwise.Note that the rest of the study is derived for the general form of posterior in (3) without the independence assumption.</p>
<p>We define the belief state at time step k as the vector of joint agent's state (i.e., s k ) and posterior probability of unknown models ϑ k :
b k = [s k , ϑ k ] T , (6)
where b k is a vector of size |s k |+3 m , and b 0 = [s 0 , ϑ 0 ] T is the initial belief state.s k is the agent state at time step k taking a value in S, and ϑ k is a vector of size 3 m , where each element takes continuous values between 0 and 1, and the sum of elements is 1.Thus, the space of belief state can be expressed as B = {S × 3 m }, where 3 m represents a simplex of size 3 m .</p>
<p>.</p>
<p>MDP representation in belief space</p>
<p>In Section 3.1, we described that navigation in an unknown environment could be represented by an unknown MDP.Here, we show that the belief state defined in (6) allows representation of the navigation task in an unknown environment through a known MDP.The rationale behind this mapping is that, unlike in unknown MDPs, reinforcement learning techniques can be employed to find the optimal policy for a known MDP.In the following paragraphs, we first define the MDP in the belief space, then we represent all its elements, and finally, we formulate the reinforcement learning policy in this known MDP.</p>
<p>The MDP in the belief space can be expressed through B, A, T, R , where B is the belief state space, T : B × A × B is a known transition probability in the belief space such that T(b, a, b ′ ) = P(b ′ | b, a) represents the transition from the belief state b to b ′ if action a is taken.Note that [as proven in ( 7)] this transition is Markov and does not need the true model of the environment since the belief state contains the entire system uncertainty.Finally, the expected reward function in the belief state is represented by R : B × A → R, where R(b, a) represents the expected immediate reward if action a is taken at belief state b.</p>
<p>Here, we provide proof that the belief transition is a Markov process.Let b 0 , a 0 , ..., a k−1 , b k be the sequence of actions and belief states up to time step k.If action a k is taken at time step k, the probability of the next belief state can be expressed as follows:
P(b k+1 | a k , b k , ..., b 0 , a 0 ) = P(s k+1 , ϑ k+1 | a k , s k , ϑ k , ..., s 0 , ϑ 0 , a 0 ) = P(s k+1 | a k , s k , ϑ k , ..., s 0 , ϑ 0 , a 0 ) × P(ϑ k+1 | s k+1 , a k , s k , ϑ k , ..., s 0 , ϑ 0 , a 0 ) = P(s k+1 | a k , s k , ϑ k )P(ϑ k+1 | s k+1 , a k , s k , ϑ k ) = P(s k+1 , ϑ k+1 | a k , s k , ϑ k ) = P(b k+1 | a k , b k ),(7)
. /frai. .</p>
<p>where the third line is written given the fact that ϑ k includes the posterior distribution of models given all sequences of states and actions up to time step k.Therefore, the terms dropped in lines 2 and 3 of Equation ( 7) are already included in the posterior distribution ϑ k .</p>
<p>Given that b = [s, ϑ] T is the current belief state, and a is the selected action at the current time, the next belief state can be one of the following |S| vectors:
b ′ | b, a ∼              b ′ 1 = [s 1 , ϑ ′ 1 ] T w.p. P(b ′ 1 | b, a) b ′ 2 = [s 2 , ϑ ′ 2 ] T w.p. P(b ′ 2 | b, a) . . . b ′ |S| = [s |S| , ϑ ′ |S| ] T w.p. P(b ′ |S| | b, a)
. ( 8)</p>
<p>The next belief state contains the next state and new posterior distribution of models.For instance, b
′ i = [s i , ϑ ′ i ]
T is one of S possible next belief states if the agent moves to state s i after taking action a in state s.The posterior ϑ ′ i upon observing s i can be computed as follows:
ϑ ′ i (j) = P(θ * = θ j | s ′ = s i , a, s, ϑ) = P(s i | s, a, θ j ) ϑ(j) 3 m l=1 P(s i | s, a, θ l ) ϑ(l) , (9) for j = 1, ..., 3 m , i = 1, ..., |S|.
The probability for all |S| possible next belief state transitions denoted in ( 9) can be computed according to the Markovian properties of the belief transition in (7).In particular, the probability that the ith belief state b
′ i = [s i , ϑ ′ i ]
T is observed upon taking action a in belief state b = [s, ϑ] T can be expressed as follows:
P(b k+1 = b ′ i | b k = b, a k = a) = P(s k+1 = s i , ϑ k+1 = ϑ ′ i | b k = b, a k = a) = P(s k+1 = s i | s k = s, ϑ k = ϑ, a k = a) × P(ϑ k+1 = ϑ ′ i | s k+1 = s i , s k = s, ϑ k = ϑ, a k = a) = P(s k+1 = s i | s k = s, ϑ k = ϑ, a k = a).(10)
The last line of ( 10) is obtained given that ϑ k+1 can only take a single value ϑ ′ i [computed in ( 9)] with probability 1.The expected reward function R(b, a) in the belief space can be expressed in terms of the reward function of the true environment as follows:
R b = [s, ϑ] T , a = b ′ ∈{b ′ 1 ,...,b ′ |S| } P(b ′ = [s ′ , ϑ ′ ] T | b, a)E θ|ϑ ′ R θ (s, a, s ′ ) = |S| i=1 P(b ′ i = [s i , ϑ ′ i ] T | b, a) 3 m l=1 ϑ ′ i (l)R θ l (s, a, s i ),
(11) where R θ l (s, a, s ′ ) represents the improvement in the rescue operation after taking action a and moving from state s to s ′ in model θ l .It can be seen that the reward function in (11) depends on the posterior distribution of models and the uncertainty in the agent state transitions.</p>
<p>Aside from the ability to incorporate any arbitrary reward functions defined for the true environment, the proposed belief formulation allows actively exploring/learning the unknown parts of the environment.For instance, robots/drones might be deployed in rescue operations to quickly identify road closures or other environmental casualties.Depending on the application, the uncertainty at specific/targeted locations or the entire environment might be needed.The immediate gain in uncertainty reduction at belief state b = [s, ϑ] T after taking action a can be expressed as follows:
R b = [s, ϑ] T , a = −E b ′ =[s ′ ,ϑ ′ ] T |b,a H(ϑ ′ ) − H(ϑ) = − |S| i=1 P(b ′ i | b, a) H(ϑ ′ i ) − H(ϑ) = |S| i=1 P(b ′ i | b, a) 3 m l=1 ϑ ′ i (l) log ϑ ′ i (l) − ϑ(l) log ϑ(l) ,(12)
where H(ϑ) denotes the remaining entropy (i.e., uncertainty) in the environment model represented by the posterior probability ϑ.Larger positive reward values correspond to more reduction of entropy/uncertainty upon moving to belief state b ′ .The entropy takes its lowest value 0 when representing the case where a single model has posterior probability 1 and others 0. Therefore, this reward function helps agents toward taking actions that provide the highest information about unknown parts of the environment, which is crucial in rescue operations.Note that depending on our application, a more general form of the reward function can be employed for learning the navigation policy.</p>
<p>.</p>
<p>Deep reinforcement learning Bayesian planning policy</p>
<p>The MDP defined in the belief space is fully known as it considers the posterior of all the possible environment models.We define µ : B → A as a deterministic policy, which associates an action to each sample in the belief space.The optimal policy in the belief space can be formulated as follows:
µ * (b) = argmax µ E ∞ t=0 γ t R(b t , a t ) | b 0 = b, a 0 : ∞ ∼ µ , (13)
for any b ∈ B; where the maximization is over all possible policies in the belief space.The expectation in ( 13) is with respect to the stochasticity in the belief space denoted in (8), which includes the uncertainty in the state transition and the posterior of environment models reflected in the belief states.The optimal Bayesian policy, µ * , yields optimality given all available information reflected in the belief state (i.e., the agent and model uncertainty).Finding the exact solution for the optimization problem in ( 13) is not possible due to the large size of the belief space.In the following paragraphs, we provide an approximate solution for finding the optimal Bayesian policy in (13) using a deep reinforcement learning approach.</p>
<p>This study employs the belief transition in (8) as a simulator to generate offline trajectories required for training a deep RL agent.These trajectories are belief transitions that propagate the agent states and the environment uncertainty, thus, do not require interaction with the real environment.Given the discrete .</p>
<p>/frai. .nature of action space, we employ the deep Q-network (DQN) method (Mnih et al., 2015) for learning the Bayesian policy in (13).This approach aims to approximate the following expected discounted reward function defined over the belief space: DQN approximates the Q-function in ( 14) using two feedforward deep neural networks, called Q-network and targetnetwork, represented by Q w and Q w − , respectively.These two neural networks share the same structure; the Q-network's input is the belief state, and its outputs are Q w (b, a 1 ), ..., Q w (b, a |A| ), each associated with an action.The initial weights for both Q-network and target-network are set randomly.
Q * (b, a) = E ∞ t=0 γ t R(b t , a t ) | b 0 = b, a 0 = a, a 1 : ∞ ∼ µ * , (14
For training of the neural networks, a replay memory D of fixed size is considered.This memory is filled and replaced by repeated episodes of belief states governed by actions generated from the epsilon-greedy policy.Each episode starts from an initial belief state b 0 = [s 0 , ϑ 0 ] T , which, if unknown, can be selected randomly from the belief space, that is, b 0 ∈ B. At step t of the episode, an action can be selected according to the epsilon-greedy policy defined using the Q-network Q w as follows:
a t ∼ argmax a∈A Q w (b t , a) w.p. 1 − ǫ random{a 1 , ..., a |A| } w.p. ǫ , (15)
where 0 ≤ ǫ ≤ 1 is the epsilon-greedy policy rate, which controls the level of exploration during the learning process.Upon generating a fixed number of steps, the Q-network Q w should be updated according to a minibatch set of experiences selected from the replay memory D. Letting
Z = {( bn , ãn , bn+1 , rn+1 )} N batch n=1 ∼ D,(16)
be selected as a minibatch set, the target values for updating the Q-network can be computed as follows:
y n = rn+1 + γ max a∈A Q w − ( bn+1 , a),(17)
for n = 1, ..., N batch , where the target-network Q w − is used for computation of the target values.Using these target values, the Q-network weights, w, can be updated as follows:
w = w − α∇ w   N batch n=1 y n − Q w ( bn , ãn ) 2   , (18)
where α is the learning rate, and the mean squared error is used for the loss function in the weights update.The optimization in (18) can be carried out using a stochastic gradient optimization approach such as Adam (Kingma and Ba, 2015).Upon updating w, the weights of the target-network, w − , should also be updated using the soft update:
w − = (1 − τ )w − + τ w, (19)
where τ is a hyperparameter.</p>
<p>It should be noted that the trajectories used in the DQN method are acquired offline through the belief transition in (8).The training can be stopped when performance improvement becomes negligible, or a pre-specified performance is achieved.Upon termination of the offline training, the Q-network approximates the optimal Bayesian policy as µ * (b) ≈ argmax a∈A Q w (b, a).This Bayesian policy prescribes an action for any given belief state b ∈ B; thus, it can be employed in real time during the execution as the new belief state is calculated according to the real interactions with the environment.The major computations of the proposed policy are during the offline process; during the execution, the belief state needs to be tracked/updated, and the learned policy reflected in the trained Q-network is applied according to the latest belief.</p>
<p>Proposed Bayesian policy's complexity analysis and capabilities</p>
<p>In this section, we briefly describe the key differences between the proposed method and some of the well-known techniques for rescue operations.Then, we discuss the advantage and capabilities of the proposed method as well as the computational complexity for real-time implementation.</p>
<p>Let q * θ (s, a) be the optimal expected return for model θ defined in (2).These values can be obtained offline using dynamic programming or reinforcement learning approaches tuned for any model θ ∈</p>
<p>. It should be noted that the model-specific Q-values are defined over the original state space and not the belief space.Assuming s k is the current state of the agent and ϑ k is the posterior distribution of the environment models at time step k, the proposed optimal Bayesian policy formulated in (13) and approximated using the Q-network through ( 15)-( 19) can be expressed in the belief space as follows:
a k = argmax a∈A Q * (b k , a) ≈ argmax a∈A Q w ([s k , ϑ k ], a).(20)
Another well-known approach commonly used for learning in unknown environments is the maximum aposteriori (MAP) navigation policy, which relies on the underlying model-specific policy with the highest posterior probability.This policy can be expressed as follows:
a k = argmax a∈A q * θ MAP k (s k , a),(21)
where θ MAP k = argmax θ i ;i=1,...,3 m ϑ k (i).In addition, active learning approaches (and their variations) (Silver et al., 2012;Kaplan and Friston, 2018;Choudhury and Srinivasa, 2020;Taylor et al., 2021;Rckin et al., 2022Rckin et al., , 2023) ) are widely used techniques for decisionmaking in unknown environments.As noted in their names, these methods aim to actively lookahead and evaluate the options.A one-step procedure can be expressed as follows:
a k = argmax a∈A E ϑ k [q * θ (s k , a)] = argmax a∈A 3 m i=1 ϑ k (i) q * θ i (s k , a). (22)
Comparing policies in Equations (20-22), one can see that the primary computation of all the methods is done in an offline process.During the online execution, all methods require updating the belief state (or the posterior probability of models), followed by using the learned/computed Q-values obtained during the offline process.The MAP policy in Equation ( 21) relies on a single model (i.e., a model with the highest posterior probability).Thus, if several models yield the maximum posterior probability or the model with the highest posterior is not definitively distinguished from others, the decisions made by the MAP policy become unreliable.Unlike the MAP policy, the active learning approach in Equation ( 22) accounts for the posterior of all models for decision-making.This policy becomes more efficient in domains where the posterior is peaked over a single model.If the posterior distribution is uniform over the models, the active learning policy looks like averaging according to various models' Q-function.The main difference between active learning and the proposed method is the incapability of active learning methods to change the posterior of the models (i.e., acting to enhance modeling information that can lead to better rewards).The active learning decisions might keep the posterior distribution unchanged over time, meaning that all models (including the wrong models) contribute similarly in making decisions over time.By contrast, the proposed Bayesian policy in Equation ( 20) learns the policy over the state and posterior of models, meaning that the action selection optimally influences the agent state and the posterior of models in achieving the highest accumulated rewards.This can be seen as taking actions that lead to moving to belief states under which better navigation performance can be achieved.</p>
<p>Aside from the efficiency of the proposed Bayesian policy described above, another advantage of the proposed policy is the generality of learning.The generality of learning refers to the fact that the proposed policy could be employed for a wide range of objectives.As described in Equations (11,12), the reward could be defined for locating victims in the environment, quick identification of the unknown parts of the environment (i.e., changing the posterior distribution of models) or any other reward functions that can be expressed using the belief state.However, the active learning and MAP policies in Equations (21, 22) can only consider the objectives (i.e., reward functions) that are defined according to the original state space (i.e., not the posterior of models).These capabilities are investigated and discussed in the next section through various numerical experiments.</p>
<p>Scalability could be a limitation of our proposed method.The size of the posterior distribution in Equation (3) grows exponentially as the number of unknown cells increases in the environment, and this leads to an increase in the size of the belief space.This increases the computational complexity of the proposed policy, making it intractable for domains with uncertainty represented in continuous (or infinite-dimensional) spaces.Note that the scalability of active learning and MAP policies also increases with the uncertainty in the environment models.Our future research will investigate approaches to scale the proposed Bayesian policy to large environments with possibly large and infinite number of environment models.</p>
<p>Numerical experiments</p>
<p>In this section, the performance of the proposed Bayesian policy is investigated using different maze problems with the following two objectives: (1) locating the victims/injuries in unknown environments as fast as possible; (2) exploring an unknown maze environment as quickly as possible, modeled through entropy reduction.Values of all the parameters used in our numerical experiments are presented as follows: number of hidden layers 3, number of neurons in hidden layers 128, α = 5 × 10 −4 , |D| = 10 5 , N batch = 64, γ = 0.95, ǫ = 0.1, τ = 10 −3 , and the update frequency for the Q-network is considered to be 4.Note that all the experiments in this section are repeated for 1,000 trials, and the average results along the 95% confidence bounds are displayed in all the figures.</p>
<p>. Locating injuries . .</p>
<p>× maze problem</p>
<p>For our first set of experiments, we consider the 6 × 4 maze shown in Figure 2.This maze consists of 5 walls and the agent can be in any of the other 19 cells at each step.Furthermore, the 3 unknown cells indicated by yellow lead to 3 3 = 27 possible maze models.For the actions, the agent at each step can select right, left, down, or up.We also consider some stochasticity in the environment as the agent moves to the anticipated direction with probability of 0.8, or it will move to either of the perpendicular directions with probability of 0.1.Each of the unknown cells could contain an injury, be empty, or blocked by a wall.To guide the agent to track three potential injuries, we define three new auxiliary variables as η η η k = {η 1 , η 2 , η 3 }, where each variable turns to 0 if an agent moves to the corresponding unknown states and locates an injury.Note that these auxiliary variables are needed to track multiple objectives in the environment.Therefore, the state space in this case contains the location of the agent (i.e., one of 19 possible locations in the maze) and the auxiliary variables.The belief space size in this case is B = {{1, ..., 19} × {0, 1} 3 × 27 }.The reward function in the belief state is modeled using (11), with R θ (s, a, s ′ ) = 1 if upon moving to s ′ a new injury is located according to the maze model parameterized by θ , and R θ (s, a, s ′ ) = 0 otherwise.</p>
<p>Here, we assume that the agent can identify empty cells and cells with injury if it moves to those states.Therefore, the transition probability for model θ required for the belief state transition in Equations (8, 9) can be expressed as follows:
P(s i | s, a, θ) =    p a,θ ss ′ 1 t(s i )=θ (l) if s i = s l u p a,θ ss ′ if s i ∈ S u , (23)
where S u contains the unknown cells, s l u is the lth unknown cell, and p a,θ ss ′ is 0.8 if s ′ is the neighboring cell to s at the direction indicated by action a in the environment model θ and 0.1 if it is in cross perpendicular neighborhood.Note that t(s i ) indicates the type of state s i ; that is, W, E or I.In addition, 1 t(s i )=θ(l) is 1 if the type of state s i and unknown cell θ (l) is the same and zero otherwise.Using the defined P(s i | s, a, θ) and Equation ( 9), the posterior  ReLu is used as an activation function between each layer of our neural networks.Furthermore, a maximum of 50 steps is considered for testing our proposed planning policy; however, a larger horizon of 250 steps is used for training purposes to account for the discounted rewards in the final steps.In addition, the proposed Bayesian planning policy is trained over 5,000 episodes in each case.</p>
<p>To better show how different prior probabilities can affect the agent's decisions, we visualized the agent's movements in the 6 × 4 maze problem using two cases in Figure 3.The prior probabilities for all unknown cells are set to be equally distributed between wall, empty, and injury (i.e., p i 0 = [ 1 3 , 1 3 , 1 3 ] for i = 1, 2, 3) , except for the first unknown cell in the left maze, which is set to p 1 0 = [ 2 3 , 1 6 , 1 6 ].The true environment is assumed to include a wall in the first unknown cell and injuries in the other two unknown cells.The paths selected by the agent under the proposed Bayesian policy are indicated in each maze in Figure 3.One can see that the agent moves from left in Figure 3A since it has prior knowledge that the first unknown cell is likely to be a wall.However, in Figure 3B, the agent selects the right path since it predicts that this path leads to the quickest rescue operation given the equal prior probabilities for the unknown cells.Once the agent encounters the wall in the first unknown cell, the Bayesian policy guides it to go back and reach out to other potential injuries in the environment as quickly as possible.Note that the agent's movements are also uncertain, which can be seen as the difference between the two trajectories shown in each maze in Figure 3.</p>
<p>The average results for the proposed policy are compared with three approaches; baseline approach, which is the reinforcement learning (i.e., Q-learning) solution when the true model of the environment is known, and MAP and active learning policies formulated in Equations (21,22).The baseline solution is the best achievable solution if no uncertainty in the environment exists; thus, it is used to assess how uncertainty in the environment model can deviate the solutions of various policies from the solution in the known environment.Figure 4A shows the average located injuries and their confidence bounds using different navigation policies for 1,000 trials and over the first 50 steps for the maze environment shown in Figure 3A.As can be seen in Figure 4A, the proposed policy has a superior performance compared to the active learning and MAP policies as it matches the performance of the baseline policy in all the steps, and after only 20 steps it can find the two injuries in the environment successfully.Active learning policy is the next best policy in this case, and it reaches maximum performance after 50 steps.Notice that although active learning gets to all the injuries after 50 steps, it still has a very low performance in the first 30 steps, which is not desirable especially in situations where we should get to the injuries in a timely manner.In addition, we can see that the MAP policy has a better performance than active learning in the first 32 steps; however, active learning performance improves significantly after that and the MAP policy becomes the worst policy since it shows poor performance even after 50 steps.</p>
<p>In the second test environment, similar prior probabilities are used for our experiments (i.e., p
1 0 = [ 2 3 , 1 6 , 1 6 ], p 2 0 = p 3 0 = [ 1 3 , 1 3 , 13
]), while in the true environment, all the unknown cells are considered to be injuries.The obtained test results for this environment are shown in Figure 4B.In this figure, we can see that the active learning and baseline policies have the same performance, and our method has the second best performance.This is because in this test case all the unknown cells are injuries, whereas the prior in our case has been set to have a higher probability for wall for the first unknown cell.Regardless of setting a faulty prior for our method, after only 20 steps, our method shows a high performance in comparison with the baseline, and the performance keeps increasing to almost the same as the baseline as the steps increase.Moreover, we can see that the MAP policy again performs poorly as to other methods.</p>
<p>Figure 5 shows the average results for the maze shown in Figure 3B.All the unknown cells have the same initial probability of being a wall, empty, or injury (i.e., p i 0 = [ 1 3 , 1 3 , 1 3 ] for i = 1, 2, 3).One can see that our proposed policy has a better performance than active learning and MAP policy as it reaches a performance close to the baseline in only 30 steps.Active learning also reaches a good performance after about 40 steps; however, it has much lower performance than our approach specifically between steps 18 and 40.Finally, similar to the other previous cases, it can be observed that the MAP policy has the worst performance in comparison with others.</p>
<p>In the next set of experiments, the impact of different values for the parameter p a,θ ss ′ in Equation ( 23) on the performance of our proposed method is investigated.As described before, p a,θ ss ′ refers to the movement stochasticity in the maze; a higher value of p a,θ ss ′ means that the movement is more deterministic and the agent moves in the desired direction, and a lower value denotes that the agent movement is more stochastic and it is more likely to end up in one of the perpendicular directions.Four values of 1, 0.8, 0.6, and 0.4 are considered for p a,θ ss ′ .These values are tested on the environment of Figure 3B, with all the unknown cells having the following prior probabilities:
p i 0 = [ 1 3 , 1 3 , 1 3 ] for i = 1, 2, 3
. The average performance of the baseline method, the proposed method, and the MAP policy in terms of average located injuries are shown in Figure 6. Figure 6A represents the performance of different policies at timestep 50.It can be seen that the best performance of the proposed policy is achieved at p a,θ ss ′ = 1, where it has performed similarly to the baseline.The second best performance for the proposed policy occurs under the value p a,θ ss ′ = 0.8, where our policy has slightly smaller performance than the baseline policy.</p>
<p>The performance of our method under the values 0.6 and 0.4 is lower and almost similar in both cases since both values represent environments with high movement stochasticity.Moreover, it can be seen that the worst performance is achieved by the MAP in all the cases.Furthermore, Figure 6B shows the performance after 100 timesteps.In this case, our method's performance under the values of 0.8, 0.6, and 0.4 gets much closer to the baseline policy, whereas the MAP policy still performs poorly.This clearly demonstrates the robustness of our proposed method under different stochasticity levels in the environment.</p>
<p>For further analysis, the impact of different unknown cells is studied in the next set of experiments.We consider three variations of the 6 × 4 maze problem.Figures 7A-C represent the environments with two, three, and four unknown cells, respectively.A uniform prior probability is used for all the unknown cells during training, that is, [ 1 3 , 1 3 , 1 3 ].Furthermore, for a fair comparison of these environments during the test, two of the unknown cells are assumed to contain victims/injuries as demonstrated in Figure 7, and the other unknown cells in environments (Figures 7B, C) can be either wall or empty.The average number of located injuries achieved in all the environments at timesteps 10, 20, and 30 using different policies is reported in Table 1.The performance of our proposed Bayesian policy is indicated by bold numbers.This table shows that at each timestep, the performance of our method decreases as the number of unknown cells (and the complexity of the maze) increases.Furthermore, one can see that the performance of our policy in all the environments and at all the timesteps surpasses other policies' performance, indicating the effectiveness of our approach.This indicates that our method is capable of reasoning about additional uncertainty in the environments to still make effective decisions.</p>
<p>. . × maze problem</p>
<p>In this part, our policy is tested for locating injuries in a new 4 × 4 grid which is a smaller grid than the previous 6 × 4 maze problem.This grid, visualized in Figure 8A, has three unknown cells, leading to 27 possible environment models.The state space of this maze can be written as S = {s 1 , ..., s 13 }.Considering three auxiliary variables η η η k = {η 1 , η 2 , η 3 } for tracking multiple targets, the belief space size in this problem can be represented as B = {{1, ..., 13} × {0, 1} 3 × 27 }.For training our proposed policy, we consider a case where the priors for the unknown cells are as follows: p i 0 = [ 1 3 , 1 3 , 1 3 ] for i = 1, 2, 3. Furthermore, for testing different policies, a true underlying environment with two injuries and one wall is chosen as represented in Figure 8B. Figure 8C shows the average located injuries achieved by different policies.It is shown that our method has the best performance in all the steps and exhibits a more similar behavior to the baseline policy compared to active learning and MAP policies.</p>
<p>. . × maze problem</p>
<p>In this subsection, we study the performance of the proposed method for locating injuries in a 6 × 6 maze with three unknown cells as depicted in Figure 9A, which is a larger maze than the previous two maze problems.This maze has seven walls; this leads to the following state and belief spaces for this maze: S = {s 1 , ..., s 29 }, B = {{1, ..., 29} × {0, 1} 3 × 27 }.The prior probabilities of different unknown cells are assumed to be uniform during the training of our policy.Moreover, a maze with two injuries and one wall as shown in Figure 9B is considered for the tests.The performance of our policy, MAP policy, and active learning policy for this experiment is shown in Figure 9C.Our proposed policy reaches a higher value in all the steps in terms of average located injuries compared to the MAP and active learning policies.This figure also denotes that the performance of our method matches the performance of the baseline policy after 50 steps (almost 2), whereas  The bold numbers refer to the highest average number of located injuries among all the policies.As can be seen these bold numbers are achieved by the proposed policy in all the scenarios.</p>
<p>the MAP and active learning policies show a low performance after the same 50 steps (less than 0.5).</p>
<p>. Entropy reduction . .</p>
<p>× maze problem</p>
<p>In this part of numerical experiments, we consider the same 6 × 4 maze problem with the objective of quick exploration of an unknown environment.Thus, the reward function in Equation ( 12) is considered for our experiments, which guides the agent to quickly navigate in the unknown environment and rapidly reduce the overall uncertainty/entropy in the environment models.The state part of the belief can be expressed using 19 potential states for the agent locations without the need for the previously defined auxiliary variables (i.e., used for tracking the injuries).The belief space, in this case, is B = {{1, ..., 19} × 27 }, which is still a large space.Note that in this case, the previously used approaches to compare with our proposed policy cannot be employed.This is due to the fact that the entropy reduction cannot be expressed in terms of the reward for underlying state space.In fact, tracking the posterior probabilities of the environment models is required in this case, which is not possible with the MAP and active learning approaches represented in Equations (21,22).A one-step entropy reduction policy is employed instead for comparison purposes.This policy selects actions to maximally reduce the next step ./frai. .entropy as
a k = argmax a∈A −E b ′ =[s ′ ,ϑ ′ ] T |b,a H(ϑ ′ ) − H(ϑ) . (24)
We consider the following initial probabilities for unknown cells:
p i 0 = [ 1 2 , 1 4 , 14
] for i = 1, 2, 3. Based on these priors, the starting value of entropy will be 3.12.Our proposed policy is then tested on two different environments.The first true environment is θ * = [E, E, E], where all the unknown cells are empty.The average negative entropy at each step of the test, along with the results of one-step entropy reduction, is shown in Figure 10A.The proposed policy obtains a much faster reduction in the entropy value than the one-step entropy reduction.Moreover, our approach reaches an entropy of 0 (least uncertainty in the environment) in only 20 steps, whereas the one-step entropy reduction shows poor performance until step 50.This demonstrates the importance of accounting for the possible future entropy in making decisions, as opposed to greedy one-step search, which is achieved by the proposed method with policy defined over the belief state.The second test environment consists of all walls in the unknown cells.Figure 10B presents the results of our method and the one-step entropy reduction policy.One can see that the entropy reduction is slower compared to the first test case in Figure 10A, which is due to the fact that all unknown cells are walls and the agent needs much larger time to visit them and reduce the overall uncertainty.However, our proposed approach, again, has achieved a much faster reduction in entropy compared to the one-step entropy reduction policy.</p>
<p>. . × maze problem</p>
<p>A larger 10 × 10 maze shown in Figure 11 is considered for this part of our numerical experiments.This maze contains four  results by a far margin in all the steps, and it reaches an entropy of zero after only 40 steps.As a final scenario, only the prior probability of being a wall for the first unknown cell is set to be larger as opposed to the previous case, that is, p 1 0 = [ 8 10 , 1 10 , 1 10 ] (similar to Figure 12A).The starting entropy value, in this case, is 3.39, and the test results on θ * = [I, I, I, I] are presented in Figure 13B.As seen in this figure, our policy has a better performance in all the steps compared to one-step entropy reduction.Our navigation policy results in zero entropy after about 45 steps, where the one-step entropy reduction fails to achieve good performance over all 100 steps.</p>
<p>Conclusion and future works</p>
<p>This study developed a reinforcement learning Bayesian planning policy for rescue operations in unknown or partially known environments.Unlike most existing approaches that rely on the availability of an exact model or simulator for representing the underlying environment, this study considers realistic problems in which no or limited prior information about the environment might be available.A new Bayesian formulation of navigation in unknown and uncertain environments is provided using the definition of belief state, which tracks the agent state and the uncertainty of the environment up to each step.This formulation is used to formulate the optimal Bayesian policy, which can be computed through the propagation of all the uncertainty in the agent state and environment models.A solution to the optimal Bayesian policy is introduced using a deep reinforcement learning method.Finally, the performance of the proposed method is demonstrated using different maze problems with various uncertainties.Note that the proposed method is versatile and can be applied not only to environments with discrete state spaces but also to those with continuous state spaces or large discrete state spaces.</p>
<p>The computational complexity of the proposed method increases exponentially with the number of unknown cells in the environments, potentially limiting its scalability.Our future work includes studying the scalability of the proposed policies in domains with extremely large belief spaces and different uncertainties in the model.We will also extend the idea of Bayesian planning to domains with partially observable states, as well as domains with multiple agents and continuous action spaces.</p>
<p>Appendix</p>
<p>In this section, we provide the run times of our proposed method and the compared approaches for selected experiments to highlight the limitations and strengths of our method.Table A1 showcases the run times of the experiments using different methods.The reported times were recorded using a system with two 2.4 GHz Intel E5-2680 v4 CPUs and 64 GB RAM memory.As can be seen in Table A1, the run times of different approaches are divided into training times and test/execution times.The training time for our method refers to the time needed for training the weights of the neural network, and the training time for active learning and MAP refers to the time needed for training the policies for each of the possible environments in each experiment.Note that active learning and MAP use the same trained policies during the execution, therefore they have similar training times.The test/execution time is the time that was used to record the results over 1,000 trials for different methods.From Table A1, one can see that our method is much faster in terms of training compared to active learning and MAP.The execution time for different methods is however in the same range, with our method being faster overall throughout all the experiments.Further, it can be seen that the complexity of the problem increases as the number of unknown cells (Table 1, Four Unknown Cells) and the grid size (Figure 9) increases, and this naturally results in longer training and test/execution times as demonstrated in Table A1.</p>
<p>) for any b ∈ B and a ∈ A.</p>
<p>FIGURE</p>
<p>FIGUREAgent movement trajectories under the proposed Bayesian policy in the × maze; each of the blue and red arrows corresponds to one independent movement trajectory, and the di erence in the trajectories is due to the movement stochasticity in the environment.The trajectories are recorded using the proposed policy under two di erent initial distributions: (A) p = [ , , ], p = p = [ , , ], (B) p i = [ , , ] for i = , , .</p>
<p>FIGURE</p>
<p>FIGURE (A) Performance comparison in the × maze with the true environment θ * = [W, I, I] and initial probabilities of unknown cells as p = [ , , ], p = p = [ , , ]. (B) Performance comparison in the × maze with the true environment θ * = [I, I, I] and initial probabilities of unknown cells as p = [ , , ], p = p = [ , , ].</p>
<p>FIGURE</p>
<p>FIGUREPerformance comparison in the × maze with the true environment θ * = [W, I, I] and initial probabilities of unknown cells as p i = [ , , ] for i = , , .</p>
<p>FIGURE</p>
<p>FIGURE Impact of movement stochasticity (p a,θ ss ′ ) on the performance of di erent policies in the × maze with the true environment θ * = [W, I, I] and initial probabilities of unknown cells as p i = [ , , ] for i = , , .Higher values for p a,θ ss ′ correspond to more deterministic movements whereas lower values refer to more stochastic movements.The performance of di erent policies is shown in two timesteps: (A) Timestep and (B) Timestep .</p>
<p>FIGURE</p>
<p>FIGUREVisualization of the × grid problem with di erent numbers of unknown cells: (A) Two unknown cells.(B) Three unknown cells.(C) Four unknown cells.For a more fair comparison, as shown in all the grids, two of the unknown cells are considered to be victims/injuries in the true underlying environment (θ * ) when performing the tests, and the rest of the unknown cells in (B, C) could be wall or empty.</p>
<p>FIGURE</p>
<p>FIGURE (A) Visualization of the × maze problem with three unknown cells.(B) The true test environment θ * = [I, W, I] with the initial probabilities of unknown cells as p i = [ , , ] for i = , , .(C) Performance comparison of di erent policies for locating the injuries in the test environment.</p>
<p>FIGURE</p>
<p>FIGURE (A) Visualization of the × maze problem with three unknown cells.(B) The true test environment θ * = [W, I, I] with the initial probabilities of unknown cells as p i = [ , , ] for i = , , .(C) Performance comparison of di erent policies for locating the injuries in the test environment.</p>
<p>FIGURE</p>
<p>FIGUREAgent movement trajectories under the proposed Bayesian policy in the × maze; each of the blue and red arrows corresponds to one independent movement trajectory, and the di erence in the trajectories is due to the movement stochasticity in the environment.The trajectories are recorded using the proposed policy under two di erent initial distributions: (A) p = [ , , ] and p i = [ , , ] for i = , , , (B) p i = [ , , ] for i = , , , .</p>
<p>FIGURE</p>
<p>FIGURE (A) Performance comparison in the × maze with the true environment θ * = [I, I, I, I] and initial probabilities of unknown cells as p i = [ , , ] for i = , , , .(B) Performance comparison in the × maze with the true environment θ * = [I, I, I, I], and initial probabilities of unknown cells as p = [ , , ] and p i = [ , , ] for i = , , .</p>
<p>TABLE Average number of located injuries by di erent policies for locating two injuries in the × maze with di erent numbers of unknown cells.
oliciesCasesTwo unknown cellsThree Unknown CellsFour unknown cellsProposed Policy0.957 ± 0.0170.72 ± 0.0530.25 ± 0.03Active Learning0.003 ± 0.0010.514 ± 0.0360.005 ± 0.001MAP0.016 ± 0.0080.009 ± 0.0060.009 ± 0.006Timestep<code>PoliciesCasesTwo unknown cellsThree unknown cellsFour unknown cellsProposed Policy1.2 ± 0.030.961 ± 0.060.89 ± 0.056Active Learning0.005 ± 0.0010.761 ± 0.0530.104 ± 0.025MAP0.137 ± 0.0830.018 ± 0.0080.073 ± 0.018Timestep</code>PoliciesCasesTwo unknown cellsThree unknown cellsFour unknown cellsProposed Policy1.395 ± 0.0351.347 ± 0.0551.191 ± 0.055Active Learning0.008 ± 0.0021.005 ± 0.0550.247 ± 0.04MAP0.301 ± 0.0330.207 ± 0.0290.17 ± 0.027
Timestep````````P</p>
<p>TABLE A
A
Training and execution times of di erent policies in selected experiments.
Training time (minutes)Test/execution time (minutes)Our methodActive learningMAPOur methodActive learningMAPFigure 4A1261,2471,247112017Figure 4B981,1181,11891916Figure 51531,3311,331122117Figure 61581,3421,342122318Table 1-two971,0151,01571614unknown cellsTable 1-three1481,3011,301111917unknown cellsTable 1-four1691,3751,375132118unknown cellsFigure 88294994971511Figure 91941,4171,417162622
Frontiers in Artificial Intelligence frontiersin.org
Data availability statementThe raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.FundingThe author(s) declare financial support was received for the research, authorship, and/or publication of this article.The authors acknowledge the support of the National Science Foundation awards IIS-2311969 and IIS-2202395, ARMY Research Laboratory award W911NF2320179, ARMY Research Office award W911NF2110299, and Office of Naval Research award N00014-23-1-2850.unknown cells, indicated by the color yellow in Figure11, resulting in 3 4 = 81 possible maze models.In this part, we aim to tackle the problem of entropy reduction using this larger maze.This maze has 33 walls, so the state space can be represented by S = {s 1 , ..., s 67 }.The belief space is B = {S × 81 }, which is much larger than the previous maze problems.We consider hyperparameters and maze stochasticity similar to previous problems, except that since this is a larger maze, we consider a horizon of 500 steps for the training process.Finally, 20,000 episodes are used to train our policy in this larger maze.The reward function in Equation (12) is used for this part of the experiments, meaning that it is desired for an agent to visit all the unknown cells as quickly as possible and reduce the overall uncertainty in the maze model to 0. Figure12represents two independent paths taken by the agent under the proposed Bayesian policy for two different initial probabilities: (a) first unknown cell is set to have a higher chance of being a wall, and the other three unknown cells have a higher chance of being empty or injury, that is, p 1 0 = [ 8 10 , 1 10 , 1 10 ], p i 0 = [ 1 12 , 6 12 , 5 12 ] for i = 2, 3, 4; (b) all the unknown cells have a higher chance of being empty or injury, that is, p i 0 = [ 1 12 , 6 12 , 5 12 ] for i = 1, 2, 3, 4. One can see that in case (a), the agent selects the left path to explore/visit all the unknown cells because unknown cell number 1 has more probability of being a wall and the Bayesian policy predicts a high likelihood that the agent might be stuck in the right side of the maze.However, in case (b), where all the unknown cells have more probability to be an injury or empty, the agent decides to choose the shortest path to get to all the unknown cells, which means that at first it goes from the right side of the maze.This again demonstrates the capability of the proposed method in accounting for potential uncertainty arising from agent movement and model uncertainty for making decisions.Once again, similar to Figure12B, consider that the prior probabilities are set equally for all the unknown cells as: p i 0 =FIGUREVisualization of the × maze problem.This maze has four unknown cells, where each could be either wall, empty, or victim/injury.[ 1 12 , 6 12 , 5 12 ] for i = 1, 2, 3, 4, which means that each cell has probability of 1 12 to be a wall.The starting entropy for this case is equal to 3.67.For this case, the average results of our proposed policy and one-step entropy reduction policy for the true environment θConflict of interestThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.Publisher's noteAll claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers.Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.
A simulation-based development and verification architecture for micro UAV teams and swarms. M Akcakoca, B M Atici, B Gever, S Oguz, U Demirezen, M Demir, 10.1109/TCBB.2024.34022202023 American Control Conference (ACC) (IEEE). M Alali, M Imani, 2019. 1979. 2023. 2024Bayesian lookahead perturbation policy for inference of regulatory networks</p>
<p>Deep reinforcement learning sensor scheduling for effective monitoring of dynamical systems. M Alali, A Kazeminajafabadi, M Imani, 10.1080/21642583.2024.2329260Syst. Sci. Cont. Eng. 1223292602024</p>
<p>An efficient reachability-based framework for provably safe autonomous navigation in unknown environments. N Asadi, S H Hosseini, M Imani, D Aldrich, S F Ghoreishi, A Bajcsy, S Bansal, E Bronstein, V Tolani, C J Tomlin, ASCE International Conference on Transportation and Development (ICTD). Reston; NiceIEEE2024. 20192019 IEEE 58th Conference on Decision and Control (CDC)</p>
<p>Rl star platform: Reinforcement learning for simulation based training of robots. T Blum, G Paillet, M Laine, K Yoshida, 10.48550/arXiv.2009.09595arXiv:2009.095952020Preprint</p>
<p>A novel UAV path planning algorithm to search for floating objects on the ocean surface based on object's trajectory prediction by regression. E Bøhn, E M Coates, S Moe, T A Johansen, M Boulares, A Barnawi, 10.1016/j.robot.2020.1036732019 International Conference on Unmanned Aircraft Systems (ICUAS). Atlanta, GAIEEE2019. 2021135103673Deep reinforcement learning attitude control of fixed-wing UAVs using proximal policy optimization</p>
<p>Autonomous spot: Long-range autonomous exploration of extreme environments with legged locomotion. A Bouman, M F Ginting, N Alatur, M Palieri, D D Fan, T Touma, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Las Vegas, NVIEEE2020</p>
<p>Survey on coverage path planning with unmanned aerial vehicles. T M Cabreira, L B Brisolara, R Paulo, F J , 10.3390/drones3010004Drones. 342019</p>
<p>Reinforcement based mobile robot path planning with improved dynamic window approach in unknown environment. L Chang, L Shan, C Jiang, Y Dai, 10.1007/s10514-020-09947-4Auton. Robots. 452021</p>
<p>A bayesian active learning approach to adaptive motion planning. S Choudhury, S S Srinivasa, N M Amato, G Hager, S Thomas, M Torres-Torriti, Robotics Research. J P L S De Almeida, R T Nakashima, F Neves-Jr, L De Arruda, ChamSpringer International Publishing2020</p>
<p>Bio-inspired on-line path planner for cooperative exploration of unknown environment by a multi-robot system. V R , 10.1016/j.robot.2018.11.005Rob. Auton. Syst. 1122019</p>
<p>Autonomous UAV trajectory for localizing ground objects: A reinforcement learning approach. D Ebrahimi, S Sharafeddine, P.-H Ho, C Assi, 10.1109/TMC.2020.2966989IEEE Trans. Mobile Comp. 202020</p>
<p>A review on reinforcement learning for contact-rich robotic manipulation tasks. I Elguea-Aguinaco, A Serrano-Muoz, D Chrysostomou, I Inziarte-Hidalgo, S Bh, N Arana-Arexolaleiba, 10.1016/j.rcim.2022.102517Robot. Comput. Integr. Manuf. 811025172023</p>
<p>Deep reinforcement learning for autonomous aerobraking maneuver planning. G Falcone, Z R Putnam, J Feng, J Lee, M Durner, R Triebel, M Ghavamzadeh, S Mannor, J Pineau, A Tamar, 10.1561/22000000492022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). KyotoIEEE2022. 2497. 2022. 20158Bayesian reinforcement learning: a survey</p>
<p>Reinforcement learning and model predictive control for robust embedded quadrotor guidance and control. C Greatwood, A G Richards, 10.1007/s10514-019-09829-4Auton. Robots. 432019</p>
<p>Efficient Bayes-adaptive reinforcement learning using sample-based search. A Guez, D Silver, P Dayan, Advances in Neural Information Processing Systems. Red Hook, NYCurran Associates Inc201225</p>
<p>Reinforcement learning based hierarchical multi-agent robotic search team in uncertain environment. S Hamid, A Nasir, Y Saleem, 10.22581/muet1982.2103.17Mehran Univer. Res. J. Eng. Technol. 402021</p>
<p>Voronoibased multi-robot autonomous exploration in unknown environments via deep reinforcement learning. J Hu, H Niu, J Carrasco, B Lennox, F Arvin, 10.1109/TVT.2020.3034800IEEE Trans. Vehicul. Technol. 692020</p>
<p>Autonomous navigation of UAV by using real-time model-based reinforcement learning. N Imanberdiyev, C Fu, E Kayacan, I.-M Chen, 2016 14th International Conference on Control, Automation, Robotics and Vision (ICARCV). PhuketIEEE2016</p>
<p>Scalable inverse reinforcement learning through multifidelity bayesian optimization. M Imani, S F Ghoreishi, 10.1109/TNNLS.2021.3051012IEEE Trans. Neural Netw. Learn. Syst. 332022</p>
<p>Bayesian control of large MDPs with unknown dynamics in data-poor environments. M Imani, S F Ghoreishi, U M Braga-Neto, Advances in Neural Information Processing Systems. Curran Associates Inc2018</p>
<p>Evolutionary-group-based particleswarm-optimized fuzzy controller with application to mobile-robot navigation in unknown environments. J Jagannath, A Jagannath, S Furman, T Gwin, C.-F Juang, Y.-C Chang, 10.1109/TFUZZ.2011.2104364IEEE Transact. Fuzzy Syst. 192021. 2011Springer International PublishingDeep Learning and Reinforcement Learning for Autonomous Unmanned Aerial Systems: Roadmap for Theory to Deployment</p>
<p>Data-efficient reinforcement learning with probabilistic model predictive control. S Kamthe, M Deisenroth, International Conference on Artificial Intelligence and Statistics. New YorkPMLR2018</p>
<p>Planning and navigation as active inference. R Kaplan, K J Friston, 2018</p>
<p>. 10.1007/s00422-018-0753-2Biol. Cybern. 112</p>
<p>Plgrim: Hierarchical value learning for large-scale exploration in unknown environments. S.-K Kim, A Bouman, G Salhotra, D D Fan, K Otsu, J Burdick, Proceedings of the International Conference on Automated Planning and Scheduling. the International Conference on Automated Planning and Scheduling2021</p>
<p>Bi-directional value learning for risk-aware planning under uncertainty. S.-K Kim, R Thakker, A.-A Agha-Mohammadi, 10.1109/LRA.2019.2903259IEEE Robot. Automat. Lett. 42019</p>
<p>Collisionfree autonomous robot navigation in unknown environments utilizing pso for path planning. D P Kingma, J Ba, E Krell, A Sheta, A P R Balasubramanian, S A King, 10.2478/jaiscr-2019-0008J. Artif. Intell. Soft Comput. Res. CoRR982015. 2019Adam: a method for stochastic optimization</p>
<p>A hybrid approach of learning and model-based channel prediction for communication relay UAVs in dynamic urban environments. P Ladosz, H Oh, G Zheng, W.-H Chen, 10.1109/LRA.2019.2903850IEEE Robot. Automat. Lett. 42019</p>
<p>Deep reinforcement learning-based automatic exploration for navigation in unknown environment. H Li, Q Zhang, D Zhao, 10.1109/TNNLS.2019.2927869IEEE Trans. Neural Netw. Learn. Syst. 312019</p>
<p>End-to-end decentralized multi-robot navigation in unknown complex environments via deep reinforcement learning. J Lin, X Yang, P Zheng, H Cheng, 2019 IEEE International Conference on Mechatronics and Automation (ICMA). TianjinIEEE2019</p>
<p>High-level human intention learning for cooperative decision-making. Y Lin, S F Ghoreishi, T Lan, M Imani, IEEE Conference on Control Technology and Applications (CCTA). Newcastle upon TyneIEEE2024</p>
<p>Sensor-based autonomous robot navigation under unknown environments with grid map representation. C Luo, J Gao, X Li, H Mo, Q Jiang, 2014 IEEE Symposium on Swarm Intelligence. Orlando, FLIEEE2014</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, 10.1038/nature14236Nature. 5185292015</p>
<p>Deep reinforcement learning robot for search and rescue applications: exploration in unknown cluttered environments. F Niroui, K Zhang, Z Kashino, G Nejat, 10.1109/LRA.2019.2891991IEEE Robot. Automat. Lett. 42019</p>
<p>Multirobot 3d coverage path planning for first responders teams. H I Perez-Imaz, P A Rezeck, D G Macharet, M F Campos, 2016 IEEE International Conference on Automation Science and Engineering (CASE). Fort Worth, TXIEEE2016</p>
<p>Reinforcement learning for autonomous UAV navigation using function approximation. H X Pham, H M La, D Feil-Seifer, L Van Nguyen, A Ravari, S F Ghoreishi, M Imani, 10.1109/LCSYS.2022.32290542018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR). Philadelphia, PAIEEE2018. 20237Optimal recursive expertenabled inference in regulatory networks</p>
<p>Implicit human perception learning in complex and unknown environments. A Ravari, S F Ghoreishi, M Imani, American Control Conference (ACC). San Diego, CAIEEE2024a</p>
<p>Optimal inference of hidden Markov models through expert-acquired data. A Ravari, S F Ghoreishi, M Imani, IEEE Transactions on Artificial Intelligence. 2024bIEEE</p>
<p>Informative path planning for active learning in aerial semantic mapping. J Rckin, L Jin, F Magistri, C Stachniss, M Popovi, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). KyotoIEEE2022</p>
<p>An informative path planning framework for active learning in uav-based semantic mapping. J Rckin, F Magistri, C Stachniss, M Popovi, 10.1109/TRO.2023.3313811IEEE Trans. Robot. 392023</p>
<p>Learning to plan for visibility in navigation of unknown environments. C Richter, N Roy, M Rigter, B Lacerda, N Hawes, 10.48550/arXiv.2102.05762International Symposium on Experimental Robotics. ChamSpringer2017. 202134Risk-averse Bayesadaptive reinforcement learning</p>
<p>The mixed-observable constrained linear quadratic regulator problem: the exact solution and practical algorithms. U Rosolia, Y Chen, S Daftry, M Ono, Y Yue, A D Ames, J Rothfuss, B Sukhija, L Treven, F Dörfler, S Coros, A Krause, 10.48550/arXiv.2403.16644arXiv:2403.16644Bridging the sim-to-real gap with Bayesian inference. IEEE2022. 2024Preprint</p>
<p>. C Sampedro, A Rodriguez-Ramos, H Bavle, Carrio, A., de la Puente, P., and</p>
<p>A fully-autonomous aerial robot for search and rescue applications in indoor environments using learning-based techniques. P Campoy, 10.1007/s10846-018-0898-1J. Intellig. Robot. Syst. 952019</p>
<p>Active learning from demonstration for robust autonomous navigation. D Silver, J A Bagnell, A Stentz, A T Taylor, T A Berrueta, T D Murphey, 10.1016/j.mechatronics.2021.1025762012 IEEE International Conference on Robotics and Automation. Saint PaulIEEE2012. 202177102576Active learning in robotics: a review of control principles</p>
<p>Autonomous navigation of UAVs in large-scale complex environments: a deep reinforcement learning approach. J Tordesillas, B T Lopez, J P ; How, J Wang, Y Shen, X Zhang, 10.1109/TVT.2018.28907732019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Macau; Wang, CIEEE2019. 201968Faster: Fast and safe trajectory planner for flights in unknown environments</p>
<p>Reinforcement learning and particle swarm optimization supporting real-time rescue assignments for multiple autonomous underwater vehicles. J Wu, C Song, J Ma, J Wu, G Han, 10.1109/TITS.2021.3062500IEEE Trans. Intellig. Transp. Syst. 232021</p>
<p>Autonomous decisionmaking method for combat mission of UAV based on deep reinforcement learning. J Xu, Q Guo, L Xiao, Z Li, G Zhang, 2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC). ChengduIEEE2019</p>
<p>Robot navigation of environments with unknown rough terrain using deep reinforcement learning. K Zhang, F Niroui, M Ficocelli, G Nejat, 2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR). Philadelphia, PAIEEE2018</p>
<p>Robot path planning in uncertain environment using multi-objective particle swarm optimization. Y Zhang, D.-W Gong, J.-H Zhang, 10.1016/j.neucom.2012.09.019Neurocomputing. 1032013</p>
<p>Modeling other players with Bayesian beliefs for games with incomplete information. Z Zhang, M Imani, T Lan, Z Zhang, H Zhou, M Imani, T Lee, T Lan, L Zintgraf, S Schulze, C Lu, L Feng, M Igl, K Shiarlis, 10.1145/3580305.3599254arXiv:2405.14122Collaborative AI teaming in unknown environments via active goal deduction. 2024a. 2024b. 202122VariBAD: a very good method for Bayes-adaptive deep RL via meta-learning</p>            </div>
        </div>

    </div>
</body>
</html>
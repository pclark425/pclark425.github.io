<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5981 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5981</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5981</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-119.html">extraction-schema-119</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-258461620</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.02251v2.pdf" target="_blank">Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems</a></p>
                <p><strong>Paper Abstract:</strong> The paper surveys automated scientific discovery, from equation discovery and symbolic regression to autonomous discovery systems and agents. It discusses the individual approaches from a "big picture" perspective and in context, but also discusses open issues and recent topics like the various roles of deep neural networks in this area, aiding in the discovery of human-interpretable knowledge. Further, we will present closed-loop scientific discovery systems, starting with the pioneering work on the Adam system up to current efforts in fields from material science to astronomy. Finally, we will elaborate on autonomy from a machine learning perspective, but also in analogy to the autonomy levels in autonomous driving. The maximal level, level five, is defined to require no human intervention at all in the production of scientific knowledge. Achieving this is one step towards solving the Nobel Turing Grand Challenge to develop AI Scientists: AI systems capable of making Nobel-quality scientific discoveries highly autonomously at a level comparable, and possibly superior, to the best human scientists by 2050.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5981.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5981.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / foundation models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (representative foundation model / large pre-trained transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as an example of a foundation (large language) model applied to equation discovery: after pretraining such models can be adapted via prompting or fine-tuning to propose candidate symbolic equations that are then evaluated externally.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (representative foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the survey as a foundation large pre-trained transformer (LLM) that retains background knowledge from pretraining and can be adapted via prompt design or fine-tuning; no architecture size or modification details are provided in the surveyed paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Symbolic regression / equation discovery across physics and other scientific domains</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this survey for GPT-4 usage; survey notes foundation models retain background knowledge from their large-scale pretraining corpora and that standard symbolic-regression benchmarks may have been present in that pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompting/in-context learning and/or fine-tuning to generate candidate equations which are then externally evaluated (fitness, complexity) and iteratively refined via prompt-feedback loops.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td>Symbolic mathematical expressions / empirical equations (candidate symbolic laws)</td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Candidate equations generated by the model are tested on datasets; fitness and complexity are computed externally and fed back to the model for refinement. The survey emphasizes that extent of evaluation and provenance (possible prior exposure in pretraining) is often not controlled.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey reports foundation models (e.g., GPT-4) are used to seed equation discovery and show promising results in producing candidate symbolic expressions, but the degree to which pretraining biases results or imparts prior exposure to benchmarks is unclear.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Data provenance and leakage (benchmarks may be present in pretraining); unclear generalization to unseen problems; dataset embedding and representation for LLM inputs not standardized; survey does not report concrete accuracy metrics for GPT-4 in this role.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Survey notes foundation-model approaches are promising but lacks systematic comparisons; raises concern that pretraining may give them advantage on standard benchmarks included in their training corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5981.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5981.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-Context Symbolic Regression (ICSR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that employs a foundation model (LLM) via in-context learning / prompting to produce initial candidate equations which are then evaluated on data and iteratively refined using externally computed fitness signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Foundation model / large language model (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM used in an in-context learning / prompting setup to output symbolic expressions token-by-token; the survey does not specify the exact LLM architecture, size, or fine-tuning details used by ICSR.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Symbolic regression / function discovery (general scientific datasets, physics-inspired benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in the survey; ICSR generates candidate expressions conditioned on example input-output pairs (benchmarks and datasets used externally); provenance of any pretraining corpora not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>In-context prompting of a pre-trained foundation model to generate symbolic expressions, followed by external evaluation (fitness, complexity) and iterative prompting/refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td>Symbolic mathematical expressions / candidate equations</td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Externally computed fitness and complexity metrics are applied to generated expressions; the survey states these metrics are fed back to the model for refinement but does not give numeric evaluation results.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Described as a promising approach to propose initial symbolic candidates; survey reports that foundation-model-based equation discovery shows promise but that the role of prior pretraining on benchmarks remains underexplored.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Potential contamination from pretraining data, uncertain generalization, lack of standardized dataset embedding strategies, and limited reporting of quantitative performance in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Survey does not report thorough comparisons; indicates these approaches are complementary to symbolic-regression methods but systematic baseline comparisons are lacking in the discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5981.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5981.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sharlin et al. (LLM SR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In Context Learning and Reasoning for Symbolic Regression with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work that applies large language models via in-context learning and reasoning prompts to perform symbolic regression tasks, producing candidate equations from problem instances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>In Context Learning and Reasoning for Symbolic Regression with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language model (unspecified foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based foundation model used in an in-context learning setup; the survey does not report model sizes or architecture specifics for this particular work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Symbolic regression / equation discovery (benchmark tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this survey summary; the method uses prompt-context examples (input-output pairs) to guide generation; overall pretraining corpus provenance not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>In-context prompting with reasoning-oriented prompts to elicit symbolic expressions from an LLM; outputs are assessed externally.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td>Symbolic equations / mathematical expressions</td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>External evaluation of generated expressions using fitness/complexity metrics; survey does not provide numerical outcomes for this work.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey lists this as an example of LLMs being used to produce symbolic regression outputs; reports promising behavior but stresses that generalization and dataset provenance need more study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Same concerns as for other foundation-model approaches: possible prior exposure to benchmarks in pretraining, limited evaluation details in survey, uncertain robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>No detailed comparative metrics are provided in the survey for this work; the survey indicates insufficient systematic comparisons across methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5981.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5981.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-SR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-SR: Scientific Equation Discovery via Programming with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that represents discovered equations as programs and uses a large language model to generate and document these programmatic equation representations (including comments to improve interpretability).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM-SR: Scientific Equation Discovery via Programming with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language model (unspecified foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM used to produce program-like representations of symbolic equations and accompanying natural-language comments; precise model architecture and size are not specified in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Scientific equation discovery (general scientific datasets / symbolic regression tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in the survey; the approach frames equations as code and uses LLM generation plus external evaluation on datasets or benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Program synthesis-style prompting of an LLM to output code representing equations, with human-readable comments; external fitness/complexity evaluation used to refine outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td>Programmatic representation of symbolic equations (mathematical expressions as code)</td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Externally computed metrics applied to the generated program-equations; survey describes method but does not report quantitative performance here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey indicates LLM-SR follows the general pattern of foundation-model-based equation discovery and appears promising, particularly for explainability via code+comments; rigorous generalization tests are not detailed in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Unclear how much pretraining influences results; no detailed quantitative comparisons in the survey; dataset provenance and embedding remain open problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Survey does not report systematic comparisons; positions LLM-SR among other LLM-based equation discovery efforts rather than as fully benchmarked against classical symbolic regression.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5981.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5981.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meyerson et al. (Language Model Crossover)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Model Crossover: Variation through Few-Shot Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses foundation language models prompted to perform genetic-programming style operations (mutation, crossover) via few-shot prompting to evolve candidate symbolic expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Model Crossover: Variation through Few-Shot Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Foundation language model (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM used as a generative operator to implement GP variation operators through few-shot prompts; the survey does not provide architecture/size specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Symbolic regression / genetic-programming-based equation discovery</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in the survey; the approach uses prompts describing parent expressions and elicits mutated/combined offspring expressions from the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Few-shot prompting to induce LLM to perform crossover/mutation operations (language-model-driven genetic programming); offspring evaluated externally for fitness.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td>Symbolic mathematical expressions produced via GP-style evolution</td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>External fitness evaluation of offspring expressions; survey notes concept but does not list numeric evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey describes this as an example where LLMs are used inside genetic-programming pipelines to generate and vary symbolic expressions; results are promising but not fully benchmarked within the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Survey highlights general issues: provenance of pretraining data, lack of systematic baselines, and uncertainty about generalization and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>No detailed comparative results are given in the survey; approach is presented as complementary to standard GP operators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5981.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5981.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian Machine Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Bayesian Machine Scientist to Aid in the Solution of Challenging Scientific Problems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI approach that approximates the marginal posterior over symbolic models, learning prior expectations about models from a large empirical set of mathematical expressions and exploring equation space via MCMC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A bayesian machine scientist to aid in the solution of challenging scientific problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bayesian Machine Scientist (MCMC over symbolic expression models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses probabilistic modeling over symbolic expressions, approximating marginal posteriors and learning priors from corpora of mathematical expressions; exploration performed via MCMC moves tailored for expression sampling (not an LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Symbolic regression / equation discovery across scientific problems (physics, general modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Learns priors from a large empirical corpus of mathematical expressions (an empirical set of mathematical expressions is used to set prior expectations); exact corpus details not provided in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Bayesian model selection: prior learned from expression corpora, model space explored with MCMC sampling over mathematical expression structures to identify plausible symbolic laws.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td>Symbolic mathematical expressions / probabilistic ranking of candidate equations</td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Posterior plausibility and model fitness (likelihood) are used to rank models; survey summarizes the approach but does not list specific numeric results here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey describes the Bayesian Machine Scientist as an approach that explicitly models plausibility and uses learned priors from corpora of expressions; presented as an influential probabilistic alternative to GP/SR methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Computational cost of exploring large model spaces with MCMC; dependence on quality and representativeness of the expression corpus used to learn priors; survey does not detail empirical performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Positioned relative to GP and grammar-based methods; survey reports it as a principled probabilistic approach but does not provide direct numeric comparisons in this text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5981.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5981.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProGED / Probabilistic CFGs from corpora</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProGED and related probabilistic CFG approaches (learning from equation corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Grammar-based equation discovery where probabilistic context-free grammars (PCFGs) guide the search and the rule probabilities can be learned from corpora of equations to bias sampling toward plausible expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning the probabilities in probabilistic context-free grammars for arithmetical expressions from equation corpora</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Probabilistic context-free grammar (ProGED) with learned rule probabilities</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ProGED represents equations via a probabilistic CFG and can learn rule probabilities from corpora of equations; sampling from the grammar yields candidate expressions evaluated on data. Not an LLM but an AI/systematic grammar-based approach.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Equation discovery / symbolic regression (physics, general symbolic expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Probabilistic CFG rule probabilities can be learned from corpora of mathematical expressions / equations; the survey cites work learning these probabilities from equation corpora but does not specify corpus size here.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Learn PCFG rule probabilities from a corpus of equations to bias sampling of candidate symbolic expressions; external evaluation on datasets selects best candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td>Symbolic mathematical equations sampled from a learned grammar</td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Candidate equations sampled from PCFG are evaluated on datasets (e.g., fitness measures); survey describes the mechanism but no numeric outcomes are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey highlights ProGED as an approach that leverages structure and learned priors from corpora to efficiently sample plausible equations; noted as enabling parallel evaluation and benefiting from learned biases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Effectiveness depends on representativeness of the equation corpus and on the adequacy of grammar design; survey notes open problems in dataset embedding and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared conceptually to grammar-free sampling and GP; survey suggests advantages in guiding search but does not provide quantitative benchmark comparisons here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5981.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5981.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiscoveryBench (LLM-based facet analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiscoveryBench: Towards Data-Driven Discovery with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark for data-driven scientific discovery that uses LLM-based facet analysis to evaluate agent-generated hypotheses, enabling semi-open-ended evaluation of hypotheses from multimodal (tabular + text) tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DiscoveryBench: Towards Data-Driven Discovery with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-based facet analysis (unspecified LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses a large language model to perform facet analysis of hypotheses generated by agents for evaluation purposes; the survey does not specify which LLM or architecture was used within DiscoveryBench.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Hypothesis generation and data analysis across diverse scientific domains (multimodal: tabular and textual datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>DiscoveryBench contains over a thousand real-world and synthetic tasks spanning various scientific domains; survey summarizes the benchmark but does not list exact corpus sources here.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>LLM-based facet analysis is used to evaluate and score agent-generated hypotheses, providing an open-ended evaluation mechanism that leverages LLM reasoning over tasks and outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td>Hypotheses and candidate relationships (symbolic or textual descriptions) rather than explicit distilled closed-form laws</td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluation of hypotheses via LLM-facilitated facet analysis; the benchmark permits open-ended scoring and human-in-the-loop validation but the survey does not give numeric evaluation statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey reports DiscoveryBench as an available benchmark that employs LLMs for evaluating generated scientific hypotheses; it is positioned as supportive of data-driven discovery with some open-endedness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Reliance on LLM-based evaluation raises concerns about evaluator bias and provenance; survey does not report detailed validation metrics or inter-rater reliability for LLM facet analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>DiscoveryBench is presented as a benchmark rather than a method; survey does not provide direct method-versus-baseline comparisons but highlights its role for assessing LLM-enabled discovery agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery <em>(Rating: 2)</em></li>
                <li>In Context Learning and Reasoning for Symbolic Regression with Large Language Models <em>(Rating: 2)</em></li>
                <li>LLM-SR: Scientific Equation Discovery via Programming with Large Language Models <em>(Rating: 2)</em></li>
                <li>Language Model Crossover: Variation through Few-Shot Prompting <em>(Rating: 2)</em></li>
                <li>A bayesian machine scientist to aid in the solution of challenging scientific problems <em>(Rating: 2)</em></li>
                <li>Learning the probabilities in probabilistic context-free grammars for arithmetical expressions from equation corpora <em>(Rating: 2)</em></li>
                <li>DiscoveryBench: Towards Data-Driven Discovery with Large Language Models <em>(Rating: 2)</em></li>
                <li>SymbolicGPT: A Generative Transformer Model for Symbolic Regression <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5981",
    "paper_id": "paper-258461620",
    "extraction_schema_id": "extraction-schema-119",
    "extracted_data": [
        {
            "name_short": "GPT-4 / foundation models",
            "name_full": "GPT-4 (representative foundation model / large pre-trained transformer)",
            "brief_description": "Mentioned as an example of a foundation (large language) model applied to equation discovery: after pretraining such models can be adapted via prompting or fine-tuning to propose candidate symbolic equations that are then evaluated externally.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (representative foundation model)",
            "model_description": "Described in the survey as a foundation large pre-trained transformer (LLM) that retains background knowledge from pretraining and can be adapted via prompt design or fine-tuning; no architecture size or modification details are provided in the surveyed paper.",
            "task_domain": "Symbolic regression / equation discovery across physics and other scientific domains",
            "input_corpus_description": "Not specified in this survey for GPT-4 usage; survey notes foundation models retain background knowledge from their large-scale pretraining corpora and that standard symbolic-regression benchmarks may have been present in that pretraining data.",
            "distillation_method": "Prompting/in-context learning and/or fine-tuning to generate candidate equations which are then externally evaluated (fitness, complexity) and iteratively refined via prompt-feedback loops.",
            "quantitative_law_type": "Symbolic mathematical expressions / empirical equations (candidate symbolic laws)",
            "example_law_extracted": null,
            "evaluation_method": "Candidate equations generated by the model are tested on datasets; fitness and complexity are computed externally and fed back to the model for refinement. The survey emphasizes that extent of evaluation and provenance (possible prior exposure in pretraining) is often not controlled.",
            "results_summary": "Survey reports foundation models (e.g., GPT-4) are used to seed equation discovery and show promising results in producing candidate symbolic expressions, but the degree to which pretraining biases results or imparts prior exposure to benchmarks is unclear.",
            "limitations_challenges": "Data provenance and leakage (benchmarks may be present in pretraining); unclear generalization to unseen problems; dataset embedding and representation for LLM inputs not standardized; survey does not report concrete accuracy metrics for GPT-4 in this role.",
            "comparison_to_baselines": "Survey notes foundation-model approaches are promising but lacks systematic comparisons; raises concern that pretraining may give them advantage on standard benchmarks included in their training corpora.",
            "uuid": "e5981.0",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "In-Context Symbolic Regression (ICSR)",
            "name_full": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
            "brief_description": "A method that employs a foundation model (LLM) via in-context learning / prompting to produce initial candidate equations which are then evaluated on data and iteratively refined using externally computed fitness signals.",
            "citation_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
            "mention_or_use": "mention",
            "model_name": "Foundation model / large language model (unspecified)",
            "model_description": "An LLM used in an in-context learning / prompting setup to output symbolic expressions token-by-token; the survey does not specify the exact LLM architecture, size, or fine-tuning details used by ICSR.",
            "task_domain": "Symbolic regression / function discovery (general scientific datasets, physics-inspired benchmarks)",
            "input_corpus_description": "Not specified in the survey; ICSR generates candidate expressions conditioned on example input-output pairs (benchmarks and datasets used externally); provenance of any pretraining corpora not detailed here.",
            "distillation_method": "In-context prompting of a pre-trained foundation model to generate symbolic expressions, followed by external evaluation (fitness, complexity) and iterative prompting/refinement.",
            "quantitative_law_type": "Symbolic mathematical expressions / candidate equations",
            "example_law_extracted": null,
            "evaluation_method": "Externally computed fitness and complexity metrics are applied to generated expressions; the survey states these metrics are fed back to the model for refinement but does not give numeric evaluation results.",
            "results_summary": "Described as a promising approach to propose initial symbolic candidates; survey reports that foundation-model-based equation discovery shows promise but that the role of prior pretraining on benchmarks remains underexplored.",
            "limitations_challenges": "Potential contamination from pretraining data, uncertain generalization, lack of standardized dataset embedding strategies, and limited reporting of quantitative performance in the survey.",
            "comparison_to_baselines": "Survey does not report thorough comparisons; indicates these approaches are complementary to symbolic-regression methods but systematic baseline comparisons are lacking in the discussion.",
            "uuid": "e5981.1",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Sharlin et al. (LLM SR)",
            "name_full": "In Context Learning and Reasoning for Symbolic Regression with Large Language Models",
            "brief_description": "Work that applies large language models via in-context learning and reasoning prompts to perform symbolic regression tasks, producing candidate equations from problem instances.",
            "citation_title": "In Context Learning and Reasoning for Symbolic Regression with Large Language Models",
            "mention_or_use": "mention",
            "model_name": "Large language model (unspecified foundation model)",
            "model_description": "Transformer-based foundation model used in an in-context learning setup; the survey does not report model sizes or architecture specifics for this particular work.",
            "task_domain": "Symbolic regression / equation discovery (benchmark tasks)",
            "input_corpus_description": "Not specified in this survey summary; the method uses prompt-context examples (input-output pairs) to guide generation; overall pretraining corpus provenance not detailed.",
            "distillation_method": "In-context prompting with reasoning-oriented prompts to elicit symbolic expressions from an LLM; outputs are assessed externally.",
            "quantitative_law_type": "Symbolic equations / mathematical expressions",
            "example_law_extracted": null,
            "evaluation_method": "External evaluation of generated expressions using fitness/complexity metrics; survey does not provide numerical outcomes for this work.",
            "results_summary": "Survey lists this as an example of LLMs being used to produce symbolic regression outputs; reports promising behavior but stresses that generalization and dataset provenance need more study.",
            "limitations_challenges": "Same concerns as for other foundation-model approaches: possible prior exposure to benchmarks in pretraining, limited evaluation details in survey, uncertain robustness.",
            "comparison_to_baselines": "No detailed comparative metrics are provided in the survey for this work; the survey indicates insufficient systematic comparisons across methods.",
            "uuid": "e5981.2",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLM-SR",
            "name_full": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
            "brief_description": "An approach that represents discovered equations as programs and uses a large language model to generate and document these programmatic equation representations (including comments to improve interpretability).",
            "citation_title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
            "mention_or_use": "mention",
            "model_name": "Large language model (unspecified foundation model)",
            "model_description": "An LLM used to produce program-like representations of symbolic equations and accompanying natural-language comments; precise model architecture and size are not specified in the survey.",
            "task_domain": "Scientific equation discovery (general scientific datasets / symbolic regression tasks)",
            "input_corpus_description": "Not specified in the survey; the approach frames equations as code and uses LLM generation plus external evaluation on datasets or benchmarks.",
            "distillation_method": "Program synthesis-style prompting of an LLM to output code representing equations, with human-readable comments; external fitness/complexity evaluation used to refine outputs.",
            "quantitative_law_type": "Programmatic representation of symbolic equations (mathematical expressions as code)",
            "example_law_extracted": null,
            "evaluation_method": "Externally computed metrics applied to the generated program-equations; survey describes method but does not report quantitative performance here.",
            "results_summary": "Survey indicates LLM-SR follows the general pattern of foundation-model-based equation discovery and appears promising, particularly for explainability via code+comments; rigorous generalization tests are not detailed in the survey.",
            "limitations_challenges": "Unclear how much pretraining influences results; no detailed quantitative comparisons in the survey; dataset provenance and embedding remain open problems.",
            "comparison_to_baselines": "Survey does not report systematic comparisons; positions LLM-SR among other LLM-based equation discovery efforts rather than as fully benchmarked against classical symbolic regression.",
            "uuid": "e5981.3",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Meyerson et al. (Language Model Crossover)",
            "name_full": "Language Model Crossover: Variation through Few-Shot Prompting",
            "brief_description": "Uses foundation language models prompted to perform genetic-programming style operations (mutation, crossover) via few-shot prompting to evolve candidate symbolic expressions.",
            "citation_title": "Language Model Crossover: Variation through Few-Shot Prompting",
            "mention_or_use": "mention",
            "model_name": "Foundation language model (unspecified)",
            "model_description": "An LLM used as a generative operator to implement GP variation operators through few-shot prompts; the survey does not provide architecture/size specifics.",
            "task_domain": "Symbolic regression / genetic-programming-based equation discovery",
            "input_corpus_description": "Not specified in the survey; the approach uses prompts describing parent expressions and elicits mutated/combined offspring expressions from the LLM.",
            "distillation_method": "Few-shot prompting to induce LLM to perform crossover/mutation operations (language-model-driven genetic programming); offspring evaluated externally for fitness.",
            "quantitative_law_type": "Symbolic mathematical expressions produced via GP-style evolution",
            "example_law_extracted": null,
            "evaluation_method": "External fitness evaluation of offspring expressions; survey notes concept but does not list numeric evaluation metrics.",
            "results_summary": "Survey describes this as an example where LLMs are used inside genetic-programming pipelines to generate and vary symbolic expressions; results are promising but not fully benchmarked within the survey.",
            "limitations_challenges": "Survey highlights general issues: provenance of pretraining data, lack of systematic baselines, and uncertainty about generalization and robustness.",
            "comparison_to_baselines": "No detailed comparative results are given in the survey; approach is presented as complementary to standard GP operators.",
            "uuid": "e5981.4",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Bayesian Machine Scientist",
            "name_full": "A Bayesian Machine Scientist to Aid in the Solution of Challenging Scientific Problems",
            "brief_description": "An AI approach that approximates the marginal posterior over symbolic models, learning prior expectations about models from a large empirical set of mathematical expressions and exploring equation space via MCMC.",
            "citation_title": "A bayesian machine scientist to aid in the solution of challenging scientific problems",
            "mention_or_use": "mention",
            "model_name": "Bayesian Machine Scientist (MCMC over symbolic expression models)",
            "model_description": "Uses probabilistic modeling over symbolic expressions, approximating marginal posteriors and learning priors from corpora of mathematical expressions; exploration performed via MCMC moves tailored for expression sampling (not an LLM).",
            "task_domain": "Symbolic regression / equation discovery across scientific problems (physics, general modeling)",
            "input_corpus_description": "Learns priors from a large empirical corpus of mathematical expressions (an empirical set of mathematical expressions is used to set prior expectations); exact corpus details not provided in this survey.",
            "distillation_method": "Bayesian model selection: prior learned from expression corpora, model space explored with MCMC sampling over mathematical expression structures to identify plausible symbolic laws.",
            "quantitative_law_type": "Symbolic mathematical expressions / probabilistic ranking of candidate equations",
            "example_law_extracted": null,
            "evaluation_method": "Posterior plausibility and model fitness (likelihood) are used to rank models; survey summarizes the approach but does not list specific numeric results here.",
            "results_summary": "Survey describes the Bayesian Machine Scientist as an approach that explicitly models plausibility and uses learned priors from corpora of expressions; presented as an influential probabilistic alternative to GP/SR methods.",
            "limitations_challenges": "Computational cost of exploring large model spaces with MCMC; dependence on quality and representativeness of the expression corpus used to learn priors; survey does not detail empirical performance metrics.",
            "comparison_to_baselines": "Positioned relative to GP and grammar-based methods; survey reports it as a principled probabilistic approach but does not provide direct numeric comparisons in this text.",
            "uuid": "e5981.5",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ProGED / Probabilistic CFGs from corpora",
            "name_full": "ProGED and related probabilistic CFG approaches (learning from equation corpora)",
            "brief_description": "Grammar-based equation discovery where probabilistic context-free grammars (PCFGs) guide the search and the rule probabilities can be learned from corpora of equations to bias sampling toward plausible expressions.",
            "citation_title": "Learning the probabilities in probabilistic context-free grammars for arithmetical expressions from equation corpora",
            "mention_or_use": "mention",
            "model_name": "Probabilistic context-free grammar (ProGED) with learned rule probabilities",
            "model_description": "ProGED represents equations via a probabilistic CFG and can learn rule probabilities from corpora of equations; sampling from the grammar yields candidate expressions evaluated on data. Not an LLM but an AI/systematic grammar-based approach.",
            "task_domain": "Equation discovery / symbolic regression (physics, general symbolic expressions)",
            "input_corpus_description": "Probabilistic CFG rule probabilities can be learned from corpora of mathematical expressions / equations; the survey cites work learning these probabilities from equation corpora but does not specify corpus size here.",
            "distillation_method": "Learn PCFG rule probabilities from a corpus of equations to bias sampling of candidate symbolic expressions; external evaluation on datasets selects best candidates.",
            "quantitative_law_type": "Symbolic mathematical equations sampled from a learned grammar",
            "example_law_extracted": null,
            "evaluation_method": "Candidate equations sampled from PCFG are evaluated on datasets (e.g., fitness measures); survey describes the mechanism but no numeric outcomes are provided here.",
            "results_summary": "Survey highlights ProGED as an approach that leverages structure and learned priors from corpora to efficiently sample plausible equations; noted as enabling parallel evaluation and benefiting from learned biases.",
            "limitations_challenges": "Effectiveness depends on representativeness of the equation corpus and on the adequacy of grammar design; survey notes open problems in dataset embedding and generalization.",
            "comparison_to_baselines": "Compared conceptually to grammar-free sampling and GP; survey suggests advantages in guiding search but does not provide quantitative benchmark comparisons here.",
            "uuid": "e5981.6",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "DiscoveryBench (LLM-based facet analysis)",
            "name_full": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
            "brief_description": "A benchmark for data-driven scientific discovery that uses LLM-based facet analysis to evaluate agent-generated hypotheses, enabling semi-open-ended evaluation of hypotheses from multimodal (tabular + text) tasks.",
            "citation_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
            "mention_or_use": "mention",
            "model_name": "LLM-based facet analysis (unspecified LLM)",
            "model_description": "Uses a large language model to perform facet analysis of hypotheses generated by agents for evaluation purposes; the survey does not specify which LLM or architecture was used within DiscoveryBench.",
            "task_domain": "Hypothesis generation and data analysis across diverse scientific domains (multimodal: tabular and textual datasets)",
            "input_corpus_description": "DiscoveryBench contains over a thousand real-world and synthetic tasks spanning various scientific domains; survey summarizes the benchmark but does not list exact corpus sources here.",
            "distillation_method": "LLM-based facet analysis is used to evaluate and score agent-generated hypotheses, providing an open-ended evaluation mechanism that leverages LLM reasoning over tasks and outputs.",
            "quantitative_law_type": "Hypotheses and candidate relationships (symbolic or textual descriptions) rather than explicit distilled closed-form laws",
            "example_law_extracted": null,
            "evaluation_method": "Evaluation of hypotheses via LLM-facilitated facet analysis; the benchmark permits open-ended scoring and human-in-the-loop validation but the survey does not give numeric evaluation statistics.",
            "results_summary": "Survey reports DiscoveryBench as an available benchmark that employs LLMs for evaluating generated scientific hypotheses; it is positioned as supportive of data-driven discovery with some open-endedness.",
            "limitations_challenges": "Reliance on LLM-based evaluation raises concerns about evaluator bias and provenance; survey does not report detailed validation metrics or inter-rater reliability for LLM facet analysis.",
            "comparison_to_baselines": "DiscoveryBench is presented as a benchmark rather than a method; survey does not provide direct method-versus-baseline comparisons but highlights its role for assessing LLM-enabled discovery agents.",
            "uuid": "e5981.7",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
            "rating": 2
        },
        {
            "paper_title": "In Context Learning and Reasoning for Symbolic Regression with Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Language Model Crossover: Variation through Few-Shot Prompting",
            "rating": 2
        },
        {
            "paper_title": "A bayesian machine scientist to aid in the solution of challenging scientific problems",
            "rating": 2
        },
        {
            "paper_title": "Learning the probabilities in probabilistic context-free grammars for arithmetical expressions from equation corpora",
            "rating": 2
        },
        {
            "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "SymbolicGPT: A Generative Transformer Model for Symbolic Regression",
            "rating": 1
        }
    ],
    "cost": 0.017454499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems
26 May 2025</p>
<p>Stefan Kramer 
Computer Science Department
Johannes Gutenberg University Mainz
Saarstrasse 2155116MainzGermany</p>
<p>Mattia Cerrato mcerrato@uni-mainz.de 
Jannis Brugger jannis.brugger@tu-darmstadt.de 
hessian.AI
Karolinenpl. 564289Darmstadt, DarmstadtTUGermany</p>
<p>Sao Deroski saso.dzeroski@ijs.si 
Dept. of Knowledge Technologies
Jozef Stefan Institute
Jamova cesta 391000LjubljanaSlovenia</p>
<p>Ross D King 
Data Science and AI
Chalmers University of Technology
Chalmersgatan 441296GteborgSweden</p>
<p>Department of Chemical Engineering and Biotechnology
University of Cambridge
Philippa Fawcett Drive, Cambridge WestCB3 0ASUnited Kingdom</p>
<p>Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems
26 May 2025AE248F4F879C6B6F93DFF263C3D0037DarXiv:2305.02251v2[cs.AI]
The paper surveys automated scientific discovery, from equation discovery and symbolic regression to autonomous discovery systems and agents.It discusses the individual approaches from a "big picture" perspective and in context, but also discusses open issues and recent topics like the various roles of deep neural networks in this area, aiding in the discovery of human-interpretable knowledge.Further, we will present closed-loop scientific discovery systems, starting with the pioneering work on the Adam system up to current efforts in fields from material science to astronomy.Finally, we will elaborate on autonomy from a machine learning perspective, but also in analogy to the autonomy levels in autonomous driving.The maximal level, level five, is defined to require no human intervention at all in the production of scientific knowledge.Achieving this is one step towards solving the Nobel Turing Grand Challenge to develop AI Scientists: AI systems capable of making Nobel-quality scientific discoveries highly autonomously at a level comparable, and possibly superior, to the best human scientists by 2050.</p>
<p>Introduction</p>
<p>The automated discovery of scientific knowledge has always been on the agenda of artificial intelligence research, and prominently so since the end of the 1970s [1,2].Scientific knowledge takes many forms: In many cases, the scientific process begins with collecting and classifying objects, and creating taxonomies of classes of objects.The more a scientific discipline advances, the more it tends to strive to describe the phenomena quantitatively, for better explanation and prediction.By far the most commonly used representation for describing systems of interest is in the form of mathematical equations, in particular differential equations.Thus, the automated discovery of equations from data has been established as a family of methods within and partly outside artificial intelligence: it runs under the heading of equation discovery [1,3] as well as symbolic regression [4].</p>
<p>The goal in many application domains of equation discovery and symbolic regression is to learn a human-understandable model of the system dynamics in the form of (mostly ordinary) differential equations. 1One important aspect of scientific discovery is that the resulting models need to be in principle interpretable. 2 Thus, the goal is not optimization (e.g., of properties in material science or drug development), but to develop understanding.</p>
<p>An important part of the literature on automated scientific discovery [2,5] discusses the topic from a cognitive science point of view (what are or could be the reasoning processes leading to certain discoveries?) and thus also a historical reconstruction of the processes.This is relevant, because today's AIs for scientific discovery also have to start from the same principles to enable discoveries in completely new application domains.While this can be viewed on the symbolic level only, many of today's approaches also consider the subsymbolic level to aid the process: neural networks of various sorts can play a vital role in guiding the search, providing valuable information to the discovery agent, or turning low-level sensory information into high-level information that can be used for symbolic reasoning.</p>
<p>Finally, the question of autonomy of the discovery agents arises.While early systems assumed a table of input data is given by a human user, approaches with more autonomy on the side of the discovery agent are becoming more common.The approach became prominent with the development of the first robot scientist world-wide, Adam [6], that automated cycles of hypothesis generation and testing in the field of functional genomics.Meanwhile, the third generation of robot scientists is being developed.The degrees of autonomy of a discovery agent may range from completely passive, i.e., supervised learning, via active learning [7] to reinforcement learning [8].</p>
<p>Considering the above, this paper aims to give an overview of automated scientific discovery from a conceptual point of view, spanning the whole field from the generation of scientific knowledge, mainly in the form of equations, to automation and autonony in robot scientists or self-driving labs.It does not just enumerate approaches, but discusses central conceptual aspects and open issues that need to addressed in future systems.Particular attention is paid to the role of neural networks in the process: Fig. 1 Overview of the two realms of automated scientific discovery: (i) the discovery and communication of human-interpretable knowledge in a representation used by scientists in the field, e.g., equations (right-hand side) and (ii) autonomy and automation in science (left-hand slide).Approaches integrating both are currently rare.</p>
<p>either for representation learning, for search in neural-guided equation discovery, or in neural operators, which abandon interpretability altogether.Discussing two main aspects of automated scientific discovery side by side in one paper, (i) the discovery of interpretable scientific knowledge in the form of equation on the one hand and (ii) automation/autonomy on the other (see Figure 1), we identify a major research gap: systems that run autonomously, but are able to communicate results in formalisms used by scientists, so that interventions are possible, such as hints for search, the provision of goals and values, and the embedding of findings in bigger theories.Very few systems exist in this space, however, we would like mention the pioneering work of Jan Zytkow, who coupled real electrochemistry experiments with the FAHRENHEIT system for equation discovery [9], and later proposed a robotic system for the rediscovery of Galileo's equation of objects rolling down an inclined plane, again with the help of FAHRENHEIT, but already taking into account empirical error [10].</p>
<p>The paper is structured as follows: In Section 2, we will review equation discovery and symbolic regression from the beginnings to the current state of the art, with a list of open problems.In Section 3, we discuss the representations used in current scientific discovery and, in particular, how neural networks can be employed to learn representations for the discovery process and how dynamics can be learned directly by neural operators.The topic of Section 4 is closed-loop scientific discovery, with recent progress in the field.Section 5 discusses different levels of autonomy.An overview of benchmarks and testbeds is given in Section 6, before we conclude in Section 7.</p>
<p>The survey paper is different from existing papers in many ways: Makke and Chawla [11] presented a thorough survey of symbolic regression (SR) and equation discovery (ED).Our survey covers both SR/ED and automation/autonomy, so it is broader in scope.Also, it appears more conceptual and with a stronger focus on interesting open issues.Further, in the present paper the discussion of the various uses of neural networks appears both more extensive and deeper.In a recent study, Musslick et al. [12] discuss primarily the limitations of autmated scientific discovery, with a focus on societal and ethical implications (e.g., the value alignment of robot scientists with human scientists).It discusses what should not be done, but also what potentially cannot be done.The latter is, of course, harder to argue, as it involves a forecast of the further progress of the field of artificial intelligence in general.Arguments likes the paradox of automation hold, others concerning the computational complexity of scientific discovery require more investigation.Another recent survey by Gao et al. [13] focuses on life sciences exclusively and discusses everything in terms of agentic AI, which is both not our emphasis here.Two recent papers by Pat Langley [14,15] are both related, but at the same time different.The first of them [14] discusses the so far distinct notions of "agents of exploration" and "agents of discocery".Langley argues for a synthesis of the two, such that agents can both explore and discover in remote areas, like in space or in the deep sea.Although conceptually relevant (imagine a versatile scientific agent that explores a lab environment and discovers new concepts and laws along the way), the main thrust of the paper is clearly different.In the more recent paper [15], Langley describes an integration effort different from the one shown above: In the paper, he envisages a tight integration and coupling of the various phases of scientific discovery, from the discovery of taxonomic knowledge via qualitative models to quantitative and causal models.It is argued that this integration is important, but has not been achieved before.We believe that, while interresting, this is of a different nature than the integration between the discovery of scientific, human-interpretable knowledge, and automation and autonomy in robot scientists or self-driving labs (see Figure 1).</p>
<p>From BACON to Modern Equation Discovery</p>
<p>and Symbolic Regression</p>
<p>History and Current Approaches</p>
<p>The first system for the discovery of equations based on data was BACON by Pat Langley [1], represented in Figure 2. The first version of BACON was developed into a series of following systems, BACON.2 to BACON.5, with increasingly complex functionality [2].The basic philosophy behind the book by Langley et al. was that scientific discovery, even in its most intricate ways, is essentially problem solving.This even applies to the search for new problems, new representations, and new measurement devices.In the case of the BACON systems, the idea was applied to the discovery of equations.BACON.1 to BACON.5 were implemented on the basis of PRISM, a system for the representation and inference of production rules.The BACON systems relied on the observation of the correlation of pairs of variables, when everything else is being held constant (ceteris paribus).This is a strong assumption, as it will in many cases not be possible to control all other variables in an experiment.Also, interestingly, BACON has a flavor of active learning, since users are requested to record data, if they are not available yet.One interesting feature of BACON is the construction of new terms, e.g., ratios or products of existing terms, by production rules.In this way, Fig. 2 (a) BACON [1,2] (b) Example of context-free grammar guiding the search for equations in the Lagramge system [16] (c) A probabilistic context-free grammar as used in ProGED [17] (d) Symbolic regression [18] it takes advantage of the structure of the search space, which is rarely ever attempted in current systems.Noise handling is achieved by some tolerance parameter, which establishes that a value of a variable (constructed or initially given) is constant, even though it varies within a certain range.BACON.2 to BACON.5 included advanced features for symmetries, common divisors, and conservation laws, amongst others.Fig. 1(a) shows the derivation of Kepler's third law D 3 /P 2 = k by a sequence of newly constructed terms, until a -more or less -constant value is found.</p>
<p>The next generation of equation discovery systems was not restricted to keeping all but a pair of variables fixed, but was able to handle observational data.In addition, it was able to learn models of dynamical systems in the form of ordinary differential equations (ODEs).Lagrange [3] computes all derivatives up to a pre-defined order, then generates products of up to a maximum of variables, before it calculates a simple linear regression to generate a candidate equation.More recently, this approach has been taken up in the SINDY system [19], which applies sparse (instead of simple) linear regression.In the meantime, the method has been extended to capture nonlinear dynamics by shallow recurrent decoder networks (SINDy-SHRED) [20].The successor of Lagrange, named Lagramge [16], was a milestone in equation discovery, as it introduced the use of domain knowledge in addition to data: It was the first system to use a context-free grammar (CFG) to guide the search for systems of equations.Grammars are a way for domain experts to use prior knowledge and let that knowledge guide the search for equations.In this way, Lagramge was able to solve problems that the predecessor Lagrange was not able to solve, for instance, the problem of two poles on a cart.An example CFG for Lagramge is shown in Figure 1(b).Lagramge GSAT [21] improves Lagramge by a bundle of modifications: first, it uses a search procedure similar to GSAT (random restart hillclimbing) to randomize search; further, it employs a one-step look-ahead and a momentum to make the search less erratic.Washio &amp; Motoda [22] further improved the methods by also taking into account units and scale types as constraints.Dimensional units are also considered for use in ProGED [17,23], which is based on the idea of using probabilistic CFGs to represent the search space and sample from it.An example is given in Fig. 1(c), where both the rules and the probabilities associated with the rules (p and q) are shown.These probabilities can be fixed, but can also be learned from corpora of equations [24].Sampling candidate equations from probabilistic CFGs enables easy parallelization: batches of sampled equations can be distributed to nodes and evaluated in an embarrassingly parallel way.</p>
<p>Symbolic regression, a development parallel to the development of equation discovery, was originally based on genetic programming (GP): the term was introduced by Koza [4].Typical systems of symbolic regression work on an operator tree or DAG representation of equations (see Fig. 1(d)).These trees are modified by a set of possible operations, such as crossover between subtrees of two trees (equations), mutations, substitutions of variables by constants, or, vice versa, substitutions of constants by variables.Schmidt &amp; Lipson [18] used symbolic regression to discover natural laws from measured data.Symbolic regression approaches were used early on to discover ODEs [25] and used ideas from grammar-based genetic programming to consider domain-specific knowledge, paving the way for systems that use both data and domain knowledge in equation discovery, such as Lagramge, Lagramge2.0[26], IPM [27] and Prob-MoT [28].The last three use process-based formalism to represent models and domain knowledge.</p>
<p>The Bayesian machine scientist [29] establishes the plausibility of models using explicit approximations to the exact marginal posterior over models and establishes its prior expectations about models by learning from a large empirical set of mathematical expressions.The space of equations is explored via Markov Chain Monte Carlo (MCMC), with specific moves for mathematical expression sampling.</p>
<p>PySR [30] is a fast, effective and popular approach to symbolic regression.It is based on genetic programming and outputs one solution per complexity class (from simple to complex equations).PySR is frequently found to be well-performing in practice.It has a Python front-end and delegates numerical computations to Julia.Using Julia "under the hood" and heuristics to avoid redundancy, it is able to explore a large number of candidates in a relatively short period of time, giving it a competitive advantage in many situations.In the meantime, version 1.4 of PySR is available with template expressions and version 1.5 with mini-batching, which further improves practical applicability.</p>
<p>Recent work by Boris Krmer and collaborators [31] has advanced the use of quadratic models for data-driven discovery of dynamical systems governed by partial differential equations (PDEs).In particular, they explore transformations of nonlinear PDEs into quadratic form, which enables the application of structure-preserving reduced-order modeling and symbolic regression techniques.The approach facilitates the use of quadratic latent variable models that retain interpretability and allow for efficient training on noisy and sparse data.The usefulness of the approach has been demonstrated in areas such as fluid dynamics and plasma modeling.</p>
<p>Symbolic regression and equation discovery are currently limited to systems with only few variables.Xue et al. [32] address this problem by identifying control variables, which can be varied to discover the dynamics of a system in "controlled experiments" step by step.The approach is still based on genetic programming.A precondition of its use is evidently the existence of such variables, which is not always the case.In practical applications and real systems, the set of control variables is not equal to the set of variables that should appear in an equation.Thus, that mapping has to be learned first.Nevertheless, the idea of actively using control variables to reduce complexity is valuable and could be a key to making ED/SR practically applicable to large and complex sytems.</p>
<p>In recent years, a new field of research has emerged that focuses on how neural networks can be used in equation discovery.To provide an overview, we divide the works into three categories.The categories are: (i) NN as a supporting module in the equation discovery system (EDS), (ii) NN as the main component of the EDS, and finally, (iii) foundation models as EDS.We discuss the three categories in consecutive order.</p>
<p>AI Feynman 2.0 [33] is a recent symbolic regression approach that aims to improve its predecessor (a) by structuring the search space by building the equation in meaningful increments and (b) making it more noise-tolerant.The first goal is achieved by graph modularity, i.e., constructing the equations by so-called graph modules.It should be noted that, in doing so, it is one of the few approaches that takes advantage of the structure of the search space (instead of just brute-force search, sampling or "blind" randomized traversal).The second goal is achieved by employing an MDLinspired evaluation function instead of the RMSE.This function is called MEDL in Feynman 2.0.Using MEDL, effective pruning can be developed, because the complexity of the equation can be balanced against its error.Lusch et al. [34] apply an auto-encoder structure to find a coordination transformation for a differential equation that maps the nonlinear original problem to linear embedding.Following the idea of an autencoder, Menar et al. [35] embed equations in a low-dimensional latent space and use this smooth latent space to suggest new equations based on genetic programming.Mundhenk et al. [36] use a Recurrent Neural Network (RNN) to seed a genetic programming algorithm, and the genetic algorithm results are used to train the RNN.While the previous works use a subsymbolic component to simplify the original problems, the following articles use neural networks as main component.</p>
<p>Deep Symbolic Regression (DSR) [37] addresses the problem of GP approaches with finding solutions for larger problems.It employs a recurrent neural network to build an equation tree step by step.As the objective function (of fitting a low-error equation) is not differentiable, a reinforcement learning approach is proposed.More specifically, DSR employs a risk-seeking policy gradient, which maximizes the best-case performance, not the average-case performance.NeSymReS [38], SymbolicGPT [39], and E2E [40], use a transformer-based architecture to predict the equation on a token level.The main difference is the embedding architecture of the data set.MGMT [41] compares different embedding methods and shows their influence on the prior of the guiding network.Additionally, the work shows that supervised learning is beneficial compared to reinforcement learning for the architectures considered.TPSR [42] and DGSR-MCTS [43], combine a transformer-based architecture with a Monte Carlo Tree Search (MCTS).In the second paper, the network suggests how to mutate the current equation.Another approach is to train a specialized end-to-end differentiable network and parser it after the training with gradient descent to an equation.EQL  [44] or Kolmogorov Arnold networks (KAN) [45] are examples for this approach.</p>
<p>Large language models (LLMs) have also impacted the field of equation discovery.Foundation models such as GPT-4 have the advantage that after the initial learning, they only need to be adapted to the equation discovery domain through fine-tuning or prompt design.In addition, they have been shown to retain background knowledge from the initial training, and the user can add domain knowledge through prompts.In-Context Symbolic Regression ICSR [46], and Sharlin et al. [47] employ a foundation model to produce initial equations.These equations are then tested on the data set.The fitness score and other measures, such as complexity, are calculated externally and then fed back to the model with the task of refining the solutions.LLM-SR [48] follows the same idea but represents equations as programs and uses comments in the program to make the discovered equation better understandable.Meyerson et al. [49] use a foundation model to perform genetic programming (mutation, crossover, etc.) through prompts.The foundation model-based equation discovery systems show promising results, but the extent to which the initial training influences the test results</p>
<p>has not yet been sufficiently investigated, as the standard benchmarks (see below) are included in the initial training.</p>
<p>Open Problems</p>
<p>In equation discovery and symbolic regression, a few open problems can be identified:</p>
<p> It remains hard to exploit structure in the space of equations to guide the search to promising parts of the search space.Opportunities for pruning would also be helpful. At least in the case of differential equations, fitting the model is the most expensive part.Ways of stopping the fitting process if it turns out to be unpromising would save a lot of computation time. Equations are "brittle": properties of differential equations can change dramatically with only little syntactic modifications.Minor changes can lead to no solutions, one solution, or many solutions. Most approaches struggle with a dimensionality of the problem higher than a very small number of variables. Overfitting avoidance and regularization: The syntactic complexity of an equation does not necessarily correspond to the complexity of the function in the feature space.Meaningful ways to approximate or bound complexity would be helpful. For the approaches based on foundation models, it is unclear how the results can generalize to new, previously unseen problems.Data provenance is an issue: It is unclear whether the models have seen some of the equations before in training.Many of the approaches are based on embeddings of datasets.It is, at this point, not clear, what the best way is to embed a dataset for a foundation model for symbolic regression. Relating discovered equations to existing theory or making the equations consistent with it remains a big challenge.Quite related, it is not clear whether or how "understanding of the physical meaning" of variables can be achieved.</p>
<p>3 Representation Learning in Scientific Discovery</p>
<p>Representation Learning of the Input</p>
<p>The standard representation of data for scientific discovery is tabular data (see, e.g., also the tables in the book by Langley et al. [2] and Figure 1(a)).However, recent years have seen a surge of papers that use neural networks as an intermediate representation to aid in the discovery of models.</p>
<p>One notable example is the work of Miles Cranmer and Shirley Ho [50], who proposed Graph Neural Networks (GNNs) as an intermediate representation.GNNs were used to learn about the interaction of objects, in terms of, for example, forces that act upon each other.Classical examples include n-body problems or, more specifically, orbital mechanics-the motion of planets and other larger objects in our solar system.The nodes in the graph represent the objects, which are annotated by feature vectors representing the properties of the objects.The edges in the graph represent the interactions between the objects and are annotated by properties that partially depend on those of the objects.For example, one may consider the masses of planets as properties of the nodes, and the distance and gravitational force between the objects as properties of the edges.When learning GNNs, typically, so-called node models  v are updated depending on the edge models  e of neighboring edges and, alternatingly, the edge models  e are updated based on the node models  n of the nodes that the edges connect.Update steps are frequently framed as message passing, and pooling functions aggregate input from multiple edges connected to one node.GNNs usually can be trained end-to-end, but are not guaranteed to converge.</p>
<p>In the application domain that was given as an example, orbital mechanics, the input to the system are (x, y, z) coordinates of the Sun, all planets, and all moons with a mass above 10 18 kg.Data from 1980 to 2013 were used with time intervals of 30 minutes each, with the first 30 years for training and the last 3 years for validation.</p>
<p>Garcon et al. [51] proposed a method to predict known physical parameters and discover new ones from oscillating time series (Figure 4).The method is trained on a large set of synthetic time series.The latent parameters used to generate the monochromatic sine waves are the carrier frequency, F c , and phase  (which is mapped for technical reasons to two separate parameters, sin() and cos()), in addition to the coherence time  .The AM and FM sine waves are generated by adding a modulation function to the carrier.The modulation function's latent parameters are the modulation frequency F m and amplitude I m .Noise is linearly added to the pure signals by sampling the Gaussian distribution.AM/FM signals with minimum I m reduce to decaying monochromatic sine waves and reach 100% modulation with maximum I m .These latent parameter ranges are wide enough such that they would encompass many foreseeable real-world signals.Figure 3 shows the neural network architecture used to predict the latent parameters, with an autoencoder-type subnetwork to support the prediction.The method can be used to discover new parameters (not just predict known ones) and reconstruct equations producing input time series.</p>
<p>The situation is clearly more complex when the observations are given as videos instead of tabular data.Chen et al. [52]   current state-of-the-approach to computing latent variables is to define an autoencoder with a bottleneck layer of the right dimension.The dimension should be large enough to allow faithful reconstruction by the decoder, but small enough so that the latent variables are non-redundant.The goal of the proposed method is to have the number of dimensions (i.e., the number of neural state variables) as close as possible to the degrees of freedom of the observations in the videos.In technical terms, the number of dimensions should be close to the so-called intrinsic dimension (ID), which is the minimum number of independent variables needed to fully describe the state of a dynamical system.Various methods from manifold learning, for instance the one by Levina and Bickel [53], are known to efficiently calculate an estimate of the intrinsic dimension.It would be tempting to calculate the intrinsic dimension for the videos and then use it as the bottleneck size of an autoencoder to come up with the latent variables.However, practically, information becomes blurry at much larger bottleneck sizes than the ID already.Therefore, Chen et al. take a two-step approach and define two autoencoders, one regular and one that maps the latent variables of the first to further ID latent variables.These are the neural state variables that can be used for downstream analysis.The approach has not yet been made explainable for scientific discovery.</p>
<p>Fig. 4 Neural network architecture of model that extracts known and unknown physical parameters from oscillating time series [51].</p>
<p>Generally speaking, neural networks are used in this domain for  making the data sparse in the sense of removing small to negligible interactions [50],</p>
<p> a change of representation (e.g., from coordinates to distances depending on some variables [50]),  data augmentation (to sample arbitrarily large data from the neural network and also smooth the data in that way [5,50]),  the prediction of important parameters to be used in equations directly [51], and  extracting latent variables from low-level input representations (e.g., neural state variables from videos [52]).</p>
<p>Representation Learning of the Dynamics and Beyond</p>
<p>Neural operators [54] can learn to map the current state of a system to the next state.This can be done for systems that evolve over space or time and especially for systems for which partial differential equations (PDEs) are too difficult to solve.Neural operators are, however, not restricted to mapping from one state to the next over time: They can learn general functional mappings between various types of inputs and outputs, e.g., inititial conditions to solutions or, even more generally, function-tofunction mappings (like DeepONet [55] or Fourier Neural Operators [56]).The latter learn mappings between functions, not just states over time, for instance, they can map a boundary condition (a function) to a solution (another function), which might involve non-temporal variables.Advantages are, amongst others, speed and flexibility (they are not hard to apply from one problem to the next).Neural operators like DeepONet or Fourier Neural Operators are, like other neural networks, black-box models.</p>
<p>Open Problems</p>
<p>Several open problems remain for representation learning of the inputs or learning functional mappings using neural networks:</p>
<p> It is currently not well-investigated how learned representations can be aligned with representations that are interpretable by humans. While neural operators can find accurate approximations to the solution of a PDE, understanding how they arrived at that solution is not straightforward.Visualizations, sensitivity analyses, and methods from explainable AI can alleviate some of the problems.</p>
<p>4 Closed-loop Scientific Discovery</p>
<p>Main Concepts, History and Advantages</p>
<p>The cutting edge of applying AI to science are "AI Scientists" (aka "Robot Scientists", "Self-driving Labs", "Autonomous Discovery systems", "Machine Scientists", etc.).These AI systems area capable of the closed-loop automation of scientific research.AI Scientists were named in 2025 by Nature as the "number one technology to watch" [57].AI Scientists automatically originate hypotheses to explain observations (abduction/induction), devise experiments to test these hypotheses (deduction), physically run the experiments using laboratory robotics, analyze and interpret the results to change the probability of hypotheses, and then repeat the cycle.In other words, they aim to automate all or parts of the scientific method, as shown (simplistically) in Figure 5.As the experiments are conceived and executed automatically by computer, it is possible to completely capture and digitally curate all aspects of the scientific process, making science more reproducible [6].</p>
<p>The first contribution describing a largely autonomous system which discovered new knowledge was due to Ross D. King and his group [6], who developed the Adam robot scientist (see Figure 6).Adam identified 6 genes encoding orphan enzymes in yeast (Saccharomyces cerevisiae), i.e., enzymes which catalyze reactions occurring in  yeast for which the encoding genes were not known at the time.The system was provided with a freezer, liquid handlers, plate readers, robot arms, and further actuators, enabling yeast cultivation experiments lasting as long as 5 days.Yeast growth was measured via optical sensors.On the software side, Adam was provided with an extensive Prolog knowledge base describing known facts about yeast metabolism.Hypotheses were formed by abduction, enabled by a combination of bioinformatic software and databases, after which an experiment planning module was responsible for selecting metabolites to be inserted in the yeast's growth medium.</p>
<p>Another successful example of laboratory automation is Eve.Originally developed for high-throughput drug screening [58], the system was then instrumental in discovering that several existing drugs could be repurposed to prevent tropical diseases [59].Most prominently, it found that an anti-cancer compound (TNP-470) could be employed against the parasite Plasmodium vivax, whose bite is the most frequent cause of recurring malaria.The system is able to hypothesize and test quantitative structure-activity relationships (QSARs) via a combination of active learning and Gaussian process regression (GPR).GPR is employed to learn a QSAR f mapping the characteristics of compounds to a response variable indicating the strength of the biological activity; then, the obtained function f is employed as a noisy oracle to select K compounds out of a pool of possible candidates.Exploration and exploitation is balanced.This two-step process may be repeated until enough candidates are obtained.In the meantime, the third generation of robot scientists is being developed.</p>
<p>AI Scientists have a number of relevant advantages, besides being able to discover new knowledge in a way that may be less biased than a human scientist:</p>
<p> Efficiency: AI Scientists are increasing the productivity of science.They can work cheaper, faster, more accurately, and longer than humans [60].They can also be easily multiplied. Reproducibility: Biomedical science is facing a "reproducibility crisis".AI Scientists have the potential to ameliorate this problem, as they describe experiments in far greater detail and semantic clarity than human scientists, and robots execute experimental protocols more accurately than human scientists [61]. Robustness: The Covid-19 pandemic clearly demonstrated the vital importance of biomedical research and the critical need to maintain research continuity [62].AI Scientists are increasingly being applied to multiple scientific domains (ranging from quantum mechanics to astronomy, from chemistry to medicine), see Table 1.</p>
<p>Open Problems</p>
<p>Three of the current main limitations of AI Scientists are (i) the design of novel experiments, (ii) integration with laboratory robotics, and (iii) the formation of completely new hypotheses and theories.The central task that faces every experimental scientist is the design of novel experiments to test a hypothesis.The abstract problem is given (1) a hypothesis, and (2) a set of laboratory equipment, output (3) a protocol to test the hypothesis using the equipment.Relatively little AI research has focussed on this aspect of automating science.N.B. that this task is different in kind from the task of traditional "experimental design", it also different from deciding, from a set of given experiments, the most efficient (in terms of time/money) to test a set of hypotheses.In all the existing AI Scientists systems that we are aware off the type of experiment that can be executed are limited to a small stereotypical set.For the design of novel experiments to be possible it will be necessary to formalise general scientific knowledge, as well as knowledge about the functionality of laboratory equipment, and experimental protocols.It is also necessary to develop inference and planning engines to generate the new experiments, as well as to develop compilers to translate generated experiments into executable protocols on specific laboratory automation.</p>
<p>Historically, laboratory automation has been driven by the desire to run large numbers of related laboratory experiments, especially in the pharmaceutical and clinical analysis industries.It is now a thriving multibillion dollar industry [63].The first use of AI to control laboratory equipment was probably that of Zytkow et al. [9] (see above).The technology of laboratory automation is steadily advancing, and robots can now carry out most (but not all) of the tasks that humans can do in the laboratory.Such laboratory automation is increasing the productivity of science as robots can work cheaper, faster, more accurately, and for longer (24/7) than humans, they can also be more easily increased/reduced in number.Laboratory automation still has many limitations.Robots typically today operate in protective boxes and are hard to program by bench scientists; logistics tasks are generally performed by lab technicians and scientists, with humans tending the robots for consumables; laboratory automation is expensive in capital to build and maintain -requiring specialised staff.Research in laboratory automation has been largely divorced from AI robotics research -which has mainly focused on the problem of mobile robots.Almost all laboratory robots are fixed in place, although there is growing interest in mobile robot assistants [62].</p>
<p>Hypothesis formation needs to be supported by a variety of AI and ML methods, from knowledge representation to active learning and reinforcement learning.The creation of a whole new theory, with theoretical terms and new measurement devices, is at least one level of complexity harder and has not been addressed yet at all.</p>
<p>Autonomy</p>
<p>One key aspect of AI Scientists is their degree of autonomy.One approach to measuring autonomy is to use the classification of degrees of autonomy in self-driving cars as [63].The approach taken here is similar, Table 2 describes five levels of autonomy.Beyond levels of autonomy are levels of skill.All human drivers are autonomous, but very few are skillful enough drivers to win a Formula 1 race.Among human scientists there are also levels of skill, with few human scientists being skillful enough to win Nobel prize.AI scientists are improving in autonomy and skill.Extrapolating this trend it is likely that advances in technology and our understanding of science will drive the development of ever-smarter AI Scientists.The Physics Nobel Frank Wilczek said that "in 100 years' time the best physicist will be a machine" [64].In Closed-loop automation.The full cycle of discovery is automated in a restricted domain.</p>
<p>See Table 1.</p>
<p>High Automation</p>
<p>Closed-loop automation.Multiple scientific domains.Limited ability to set its own goals.</p>
<p>No existing system.</p>
<p>Full Automation</p>
<p>All aspects of science are automated and no human intervention is required.</p>
<p>No existing system.</p>
<p>February 2020 a workshop was held in London to kick-off the Nobel Turing Grand Challenge to develop: AI systems capable of making Nobel-quality scientific discoveries highly autonomously at a level comparable, and possibly superior, to the best human scientists by 2050 [65].If the Nobel Turing Grand Challenge is achieved this would clearly transform the World, it would be possible to have instead of a few Nobel prize winning ideas a year, hundreds, thousands, millions, ...</p>
<p>Evaluation and Testbeds</p>
<p>The evaluation of an autonomous discovery system is intrinsically tied to the levels of autonomy displayed by the methodology at hand and which steps of the scientific process are to be automatized and the level of autonomy being evaluated (Figure 5 and Table 2).Equation discovery methods may help in automating the analysis of experiments by providing human-readable knowledge, while systems with physical actuators may be evaluated in their ability to execute experimental protocols.Thus, evaluation methodologies and benchmarks in the area have different characteristics in terms of supervision, data modalities, scope and open-endedness.We define these properties in the following, and give a table of existing methods for evaluation in Table 3. Supervision.Supervision refers to the nature of the ground truth or reward signals provided to the autonomous discovery system during training and evaluation.Depending on the degree of autonomy assessed, supervision may range from explicit labels or predefined objectives to feedback signals (rewards in the Reinforcement Learning sense [8]).The type and quantity of supervision significantly affect the evaluation outcome, as they directly influence the system's capability to navigate scientific exploration autonomously.</p>
<p>Data Modalities.Data modalities encompass the types and formats of data available for evaluation, such as pixel-based images, textual descriptions, numerical tables, or structured representations of experimental observations.The choice of modality greatly impacts the complexity and applicability of autonomous systems, as certain data forms inherently require more sophisticated methods for interpretation, abstraction, and knowledge extraction (see Section 3).Evaluating systems across diverse data modalities helps in understanding their flexibility, generalizability, and robustness in real-world scientific scenarios.</p>
<p>Scope.Scope defines which specific phases of the scientific discovery process the evaluation benchmark addresses.This includes one or more of the six distinct steps: scientific question formulation, hypothesis generation, experimental design, execution of experiments, data analysis and communication.</p>
<p>Open-endedness.Open-endedness characterizes whether the benchmark or evaluation method includes previously unexplained data, phenomena lacking known mathematical descriptions, or allows the formulation of novel scientific questions.An open-ended benchmark challenges autonomous discovery systems to demonstrate genuine exploratory capabilities, creativity, and adaptability, rather than merely replicating existing knowledge.</p>
<p>We now move to introducing benchmark and testbeds while discussing their potential in the autonomous discovery setting.We will not offer here an exhaustive survey of symbolic regression benchmarks.</p>
<p>Available Benchmarks</p>
<p>Nguyen Benchmark Suite [66] is a widely-used collection of symbolic regression problems introduced specifically to evaluate genetic programming (GP) methods.It consists of synthetic mathematical equations designed with varying complexity and structure, aiming to assess the ability of GP algorithms to accurately recover symbolic expressions from numerical data.Each task provides numerical input-output pairs generated from known symbolic formulas.The benchmark primarily evaluates one-shot analysis of already collected experimental data.</p>
<p>Feynman [67] provides a comprehensive symbolic regression benchmark inspired by fundamental physics equations from the Feynman Lectures on Physics.This dataset includes 120 symbolic regression tasks covering a diverse range of physics phenomena, from classical mechanics to electromagnetism.</p>
<p>Matbench [68] is a supervised machine-learning benchmark containing 13 prediction tasks related to materials science.The dataset consists of structured data representing chemical formulas and crystalline structures, with tasks that involve predicting material properties such as band gap or elastic moduli.It is particularly suited for evaluating analysis capabilities and hypothesis generation for material properties from compositional and structural data.While each task is narrowly defined with a fixed prediction goal, collectively, they support evaluating broad generalizability across material science domains.</p>
<p>SCP-116K [69] is a large-scale textual dataset comprising problem-solution pairs extracted from higher education science textbooks and other academic sources, totaling 116,000 entries.It is designed primarily for supervised training and evaluation of models on scientific reasoning, question answering, and hypothesis generation from textual data.While each problem-solution pair is relatively constrained in scope, the dataset's scale and diversity across scientific disciplines provides opportunities for broader generalization and transfer learning evaluation.</p>
<p>The Well [70] is a comprehensive collection of physics simulation datasets, explicitly constructed for machine learning model training and benchmarking in physics-informed learning.It contains diverse simulation data spanning fluid dynamics, astrophysics, plasma physics, and more.These simulations allow evaluation of models' abilities in hypothesis generation, scientific analysis, and predictive modeling in physics.Its broad diversity and complexity may be employed in open-ended exploration of scientific hypotheses through computational experimentation.</p>
<p>ScienceWorld [71] is a publicly available reinforcement learning environment designed to evaluate an AI agent's capacity for grounded scientific reasoning in a simulated laboratory context.The benchmark contains 30 interactive text-based tasks, such as converting substances between states of matter.Evaluation relies on binary task completion within limited simulator steps, making it suitable for assessing agents' (abstract, text-based) experimental execution capabilities in a weakly supervised, text-based modality.</p>
<p>DiscoveryWorld [72] is an open-source, highly interactive environment designed to benchmark complete cycles of scientific discovery, including hypothesis generation, experimental design, execution, and analysis.The general setting is akin to a 2D roleplaying game to be played on a grid.It provides agents with quests, subquests and various tasks to be completed to make progress.</p>
<p>ChemGymRL [73] provides a suite of customizable, publicly accessible reinforcement learning environments simulating chemistry laboratory experiments.Each virtual "bench" simulates distinct chemical procedures such as synthesis or titration.Agents receive structured numeric data representing chemical states and perform sequential lab actions.The library emphasizes experimental design and execution with reward signals, but allows for extension to e.g.new chemical reactions.</p>
<p>DiscoveryBench [74] is a publicly accessible benchmark focusing on data-driven scientific discovery tasks using multimodal data (tabular data and textual descriptions).It comprises over a thousand real-world and synthetic tasks spanning various scientific domains.Evaluation of agent-generated hypotheses is performed using LLMbased facet analysis, which allows for some open-endedness in the tasks considered.DiscoveryBench primarily targets hypothesis generation and data analysis.</p>
<p>BoxingGym [75] provides publicly available, interactive simulation environments for benchmarking autonomous experimental design and scientific model discovery.The benchmark covers multiple scientific domains through generative probabilistic models.Evaluation metrics include expected information gain for experimental quality and predictive power of agent-generated scientific models.The environment is numeric and textual in data modalities and promotes open-ended exploration.</p>
<p>Science-Gym [76] is a publicly released Gym-compatible benchmark designed to evaluate autonomous equation discovery in simulated physical and epidemiological environments.Agents interactively select experimental parameters to generate data, subsequently performing symbolic regression to derive underlying scientific equations.Evaluation assesses the symbolic accuracy of discovered equations, providing a structured yet open-ended setting emphasizing experimental execution and analytical reasoning.Open Catalyst 2020 (OC20) [77] provides a large-scale benchmark for catalysis research, encompassing over a million atomic structure relaxations generated via density functional theory (DFT) calculations.It offers structured atomic 3D data for supervised machine learning tasks aimed at predicting energies and molecular interactions relevant to catalytic processes.OC20 primarily evaluates data-driven analysis and indirectly supports hypothesis-driven experimental design, particularly aiding in computational screening of catalytic materials.While individually each task has a fixed objective, its expansive dataset encourages robust and generalizable modeling approaches.</p>
<p>Conclusion</p>
<p>This paper is an attempt at giving a survey of research on automated scientific discovery, from discovering equations to autonomous discovery systems or agents.In doing so, it takes a broad perspective on the topic, which is necessary to understand the individual efforts in context.The article covers the beginnings of the fields to very recent approaches, understanding that we still have a long way of putting everything together to create human-level autonomous scientists.Human-level autonomous scientists should, ultimately, be able to produce whole new theories, along with theoretical terms and measurement devices, which can be communicated to humans and interpreted in the light of other, existing theories.At this point, autonomous discovery systems are focused primarily on "closing the loop" and lab automation, and not so much on generating human-interpretable knowledge, like (differential) equations.Vice versa, computational approaches to scientific discovery, e.g., for equation discovery and symbolic regression, do not have the "embodiment" in autonomous systems in their focus yet.Ultimately, these currently disparate efforts have to grow together.Finally, it should be noted that artificial intelligence has a role also in so far unexplored areas, like the design of experiments, where much of human ingenuity is currently still needed.</p>
<p>Fig. 3
3
Fig. 3 Workflow of Cranmer et al. [50]: GNNs as an intermediate representation to support or enable the learning process</p>
<p>Fig. 5
5
Fig. 5 Six steps of the scientific process.</p>
<p>Fig. 6
6
Fig.6The robot scientist Adam.</p>
<p>Table 3
3
Benchmark Categorization by Evaluation Properties.In the Scope column, we take D = experimental Design, E = Experimental Execution, H = Hypothesis formation, A = Analysis of results, Q = research Question formation.</p>
<p>presented a solution based on what they call neural state variables.Neural state variables are essentially latent variables.The</p>
<p>Table 1
1
Robot Scientists by Discipline, Name, and Country
DisciplineNameCountryDrug DiscoveryEveSwedenDrug DiscoveryRecursionUSDrug DiscoveryLilly Life Sciences Studio labUSDrug DiscoveryXtaIPiChinaChemistryUK Centre for Rapid Online Analysis of ReactionsUKChemistryroboRXN at IBMSwitzerlandChemistryphactorUSChemistryPharmacy on Demand (PoD)USChemistryMolecule Maker InstituteUSChemistryAI-ChemistChinaChemistryA self-optimizing reactorUSChemistryChemputerUKChemistryLapkin GroupUKChemistryRoboChemNetherlandsMaterialsKebotixUSMaterialsAutonomous Research System (ARES)USMaterialsRobot ChemistUKMaterialsAcceleration ConsortiumCanadaMaterialsBrookhavenUSMaterialsSARAUSMaterialsAI-ChemistChinaMaterialsA-LabUSMaterialsMatterhornUKMaterialsARC -Exciton ScienceAustraliaMaterialsGormleyUSCatalysisRealCatFranceCatalysisSwissCAT+SwitzerlandMetallurgyACCMETEUMaterialsBIG-MAPEUCell BiologyLabdroidsJapanCell BiologyMurphy LabUSMechanical Eng.Creative Machines LabUSProtein DesignMolcureJapanProtein DesignLabGeniusUKSystems BiologyGenesisSwedenMaterials/BiologyArgonne Autonomous DiscoveryUSQuantum PhysicsMELVINGermanyMedicineAutomation ScienceSingapore</p>
<p>Table 2
2
Six levels of autonomy in scientific discovery analogously to autonomy levels in autonomous driving
LevelSummaryNarrativeExample0No automationTraditional human science before the-advent of computers.1MachineThe use of computers to automate anMost current applica-assistanceaspect of science, e.g. analysing data.tions of ML.2PartialAn important aspect of the discovery cycleAlphaFold 2, Real-timeAutomationis fully automated.weather forecasting3ConditionalAutomation
The underlying data are most frequently temporal.
If a model cannot be communicated to a community of researchers, it hardly qualifies as scientific, as communication is an indispensable part of the scientific endeavor.</p>
<p>Bacon: A production system that discovers empirical laws. P Langley, Proceedings of the 5th International Joint Conference on Artificial Intelligence (IJCAI 1977). the 5th International Joint Conference on Artificial Intelligence (IJCAI 1977)1977344</p>
<p>Scientific Discovery: Computational Explorations of the Creative Process. P W Langley, H A Simon, G Bradshaw, J M Zytkow, 1987MIT PressCambridge, MA, USA</p>
<p>Discovering dynamics. S Deroski, L Todorovski, Proceedings of the Tenth International Conference on Machine Learning. the Tenth International Conference on Machine LearningAmherst, MA, USAMorgan Kaufmann1993</p>
<p>Genetic programming as a means for programming computers by natural selection. J R Koza, Statistics and Computing. 41994</p>
<p>Z Li, J Ji, Y Zhang, 10.48550/arXiv.2111.12210arXiv:2111.12210From kepler to newton: Explainable ai for science. 2021arXiv preprint</p>
<p>The automation of science. R D King, J Rowland, S G Oliver, M Young, W Aubrey, E Byrne, M Liakata, M Markham, P Pir, L N Soldatova, A Sparkes, K E Whelan, A Clare, Science. 32459232009</p>
<p>Active learning with statistical models. D A Cohn, Z Ghahramani, M I Jordan, Journal of Artificial Intelligence Research. 41996</p>
<p>Reinforcement Learning: An Introduction. R Sutton, A Barto, 2018MIT PressCambridge, MA, USA</p>
<p>Automated discovery in a chemistry laboratory. J M Zytkow, J Zhu, A Hussam, Proceedings of the 8th National Conference on Artificial Intelligence (AAAI 1990). the 8th National Conference on Artificial Intelligence (AAAI 1990)Boston, MA, USAAAAI Press / MIT Press1990</p>
<p>Discovering empirical equations from robotcollected data. K.-M Huang, J M Zytkow, Proceedings of the 10th International Symposium on Foundations of Intelligent Systems (ISMIS 1997). the 10th International Symposium on Foundations of Intelligent Systems (ISMIS 1997)Charlotte, North Carolina, USASpringer1997</p>
<p>Interpretable scientific discovery with symbolic regression: a reviews. N Makke, S Chawla, Artificial Intelligence Review. 5722024</p>
<p>. S Musslick, L K Bartlett, S H Chandramouli, M Dubova, F Gobet, T L Griffiths, J Hullman, R D King, J N Kutz, C G Lucas, S Mahesh, </p>
<p>Automating the practice of science: Opportunities, challenges, and implications. F Pestilli, S J Sloman, W R Holmes, PNAS. 12252025</p>
<p>Empowering biomedical discovery with ai agents. S Gao, A Fang, Y Huang, V Giunchiglia, A Noori, J R Schwarz, Y Ektefaie, J Kondic, M Zitnik, Cell. 187222024</p>
<p>Agents of exploration and discovery. P Langley, AI Magazine. 4242022</p>
<p>Integrated systems for computational scientific discovery. P Langley, Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24). the Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)Vancouver, CanadaAAAI Press2024</p>
<p>Declarative bias in equation discovery. L Todorovski, S Deroski, Proceedings of the Fourteenth International Conference on Machine Learning. the Fourteenth International Conference on Machine LearningSan Francisco, CAMorgan Kaufmann1997</p>
<p>Probabilistic grammars for equation discovery. J Brence, L Todorovski, S Deroski, Knowledge Based Systems. 2241070772021</p>
<p>Distilling free-form natural laws from experimental data. M Schmidt, H Lipson, Science. 32459232009</p>
<p>Discovering governing equations from data by sparse identification of nonlinear dynamical systems. S L Brunton, J L Proctor, J N Kutz, PNAS. 1132016</p>
<p>Sparse identification of nonlinear dynamics and Koopman operators with Shallow Recurrent Decoder Networks. M L Gao, J P Williams, J N Kutz, 2025</p>
<p>Equation discovery for model identification in respiratory mechanics of the mechanically ventilated human lung. S Ganzert, J Guttmann, D Steinmann, S Kramer, Proceedings of the 13th International Conference on Discovery Science (DS 2010). the 13th International Conference on Discovery Science (DS 2010)Berlin; HeidelbergSpringer2010</p>
<p>Discovering admissible models of complex systems based on scale-types and identity constraints. T Washio, H Motoda, Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence (IJCAI 1997). the Fifteenth International Joint Conference on Artificial Intelligence (IJCAI 1997)1997</p>
<p>Dimensionally consistent equation discovery through probabilistic attribute grammars. J Brence, L Todorovski, S Deroski, Information Sciences. 2023</p>
<p>Learning the probabilities in probabilistic context-free grammars for arithmetical expressions from equation corpora. M Chaushevska, L Todorovski, J Brence, S Deroski, Proceedings of the Slovenian Conference on Artificial Intelligence. the Slovenian Conference on Artificial Intelligence2022</p>
<p>Discovering dynamics with genetic programming. S Deroski, I Petrovski, Proceedings of the Seventh European Conference on Machine Learning. the Seventh European Conference on Machine LearningBerlin; HeidelbergSpringer1994</p>
<p>Integrating knowledge-driven and data-driven approaches to modeling. L Todorovski, S Deroski, Ecological Modelling. 1942006</p>
<p>Inductive process modeling. W Bridewell, P Langley, L Todorovski, S Deroski, Machine Learning. 712008</p>
<p>The influence of parameter fitting methods on model structure selection in automated modeling of aquatic ecosystems. D erepnalkoski, K Takova, L Todorovski, N Atanasova, S Deroski, Ecological Modelling. 452012</p>
<p>A bayesian machine scientist to aid in the solution of challenging scientific problems. R Guimer, I Reichardt, A Aguilar-Mogas, F A Massucci, M Miranda, J Pallars, M Sales-Pardo, Science Advances. 669712020</p>
<p>Interpretable machine learning for science with pysr and symbolicregression. M D Cranmer, abs/2305.01582 (2023) 2305.01582</p>
<p>Exact and optimal quadratization of nonlinear finite-dimensional non-autonomous dynamical systems. A Bychkov, I Issan, G Pogudin, B Krmer, SIAM Journal on Applied Dynamical Systems. 2312024</p>
<p>Symbolic regression via control variable genetic programming. N Jiang, Y Xue, Proc. of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2023). of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2023)</p>
<p>. Springer, 2023Berlin, Heidelberg</p>
<p>Ai feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity. S.-M Udrescu, A Tan, J Feng, O Neto, T Wu, M Tegmark, Advances in Neural Information Processing Systems. 332020</p>
<p>Deep learning for universal linear embeddings of nonlinear dynamics. B Lusch, J N Kutz, S L Brunton, Nature communications. 9149502018</p>
<p>Efficient generator of mathematical expressions for symbolic regression. S Menar, S Deroski, L Todorovski, Machine Learning. 112112023</p>
<p>Symbolic regression via neural-guided genetic programming population seeding. T N Mundhenk, M Landajuela, R Glatt, C P Santiago, D M Faissol, B K Petersen, arXiv:2111.000532021arXiv preprint</p>
<p>Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. B K Petersen, M L Larma, T N Mundhenk, C P Santiago, S K Kim, J T Kim, Proceedings of the 9th International Conference on Learning Representations. the 9th International Conference on Learning Representations2021ICLR 2021</p>
<p>Neural symbolic regression that scales. L Biggio, T Bendinelli, A Neitz, A Lucchi, G Parascandolo, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. M Meila, T Zhang, the 38th International Conference on Machine Learning, ICML 2021PMLR, Virtual event18-24 July 2021. 2021139Virtual Event. Proceedings of Machine Learning Research</p>
<p>M Valipour, B You, M Panju, A Ghodsi, 10.48550/arXiv.2106.14131arXiv.arXiv:2106.14131[cs]version:1SymbolicGPT: A Generative Transformer Model for Symbolic Regression. 2021</p>
<p>End-to-end symbolic regression with transformers. P Kamienny, S Ascoli, G Lample, F Charton, S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. A Oh, NeurIPS; New Orleans, LA, USA2022. 2022. November 28 -December 9, 2022. 2022</p>
<p>Neural-Guided Equation Discovery. J Brugger, M Cerrato, D Richter, C Derstroff, D Maninger, M Mezini, S Kramer, 2025</p>
<p>Transformer-based Planning for Symbolic Regression. P Shojaee, K Meidani, A B Farimani, C K Reddy, 10.48550/ARXIV.2303.068332023-08-09Publisher: arXiv Version Number: 4. 2023</p>
<p>Deep generative symbolic regression with monte-carlo-tree-search. P Kamienny, G Lample, S Lamprier, M Virgolin, International Conference on Machine Learning, ICML 2023. A Krause, E Brunskill, K Cho, B Engelhardt, S Sabato, J Scarlett, Honolulu, Hawaii, USAPMLRJuly 2023. 2023202Proceedings of Machine Learning Research</p>
<p>Learning Equations for Extrapolation and Control. S Sahoo, C Lampert, G Martius, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningStockholm, SwedenPMLR2018</p>
<p>. Z Liu, Y Wang, S Vaidya, F Ruehle, J Halverson, M Soljai, T Y Hou, M Tegmark, arXiv:2404.197562024Kan: Kolmogorov-arnold networksarXiv preprint</p>
<p>In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery. M Merler, K Haitsiukevich, N Dainese, P Marttinen, 10.18653/v1/2024.acl-srw.49Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, Thailand20244Student Research Workshop)</p>
<p>In Context Learning and Reasoning for Symbolic Regression with Large Language Models. S Sharlin, T R Josephson, 10.48550/ARXIV.2410.17448arXiv. Version Number: 22024</p>
<p>LLM-SR: Scientific Equation Discovery via Programming with Large Language Models. P Shojaee, K Meidani, S Gupta, A B Farimani, C K Reddy, 10.48550/ARXIV.2404.18400arXiv. Version Number: 22024</p>
<p>E Meyerson, M J Nelson, H Bradley, A Gaier, A Moradi, A K Hoover, J Lehman, 10.48550/ARXIV.2302.12170Language Model Crossover: Variation through Few-Shot Prompting. arXiv. Version Number. 20233</p>
<p>Discovering symbolic models from deep learning with inductive biases. M D Cranmer, A Sanchez-Gonzalez, P W Battaglia, R Xu, K Cranmer, D N Spergel, S Ho, Advances in Neural Information Processing Systems. 332020</p>
<p>Deep neural networks to recover unknown physical parameters from oscillating time series. A Garcon, J Vexler, D Budker, S Kramer, PLoS ONE. 1752684392022</p>
<p>Automated discovery of fundamental variables hidden in experimental data. B Chen, K Huang, S Raghupathi, I Chandratreya, Q Du, H Lipson, Nature Computational Science. 22022</p>
<p>Maximum likelihood estimation of intrinsic dimension. E Levina, P J Bickel, Advances in Neural Information Processing Systems. 200417</p>
<p>Neural operator: Learning maps between function spaces with applications to pdes. N Kovachki, Z Li, B Liu, K Azizzadenesheli, K Bhattacharya, A M Stuart, A Anandkumar, Journal of Machine Learning Research. 242023</p>
<p>Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. L Lu, P Jin, G Pang, Z Zhang, G E Karniadakis, Nature Machine Intelligence. 332021</p>
<p>Fourier neural operator for parametric partial differential equations. Z Li, N B Kovachki, K Azizzadenesheli, B Liu, K Bhattacharya, A M Stuart, A Anandkumar, Proceedings of the 9th International Conference on Learning Representations. the 9th International Conference on Learning Representations2021ICLR 2021</p>
<p>Self-driving laboratories, advanced immunotherapies and five more technologies to watch in 2025. M Eisenstein, 10.1038/d41586-025-00075-6Nature. 6372025</p>
<p>Towards robot scientists for autonomous scientific discovery. A Sparkes, W Aubrey, E Byrne, A Clare, M N Khan, M Liakata, M Markham, J Rowland, L N Soldatova, K E Whelan, M Young, R D King, Automated Experimentation. 212021</p>
<p>Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases. K Williams, E Bilsland, A Sparkes, W Aubrey, M Young, L N Soldatova, K D Grave, J Ramon, M Clare, W Sirawaraporn, S G Oliver, R D King, Journal of the Royal Society Interface. 12104201412892015</p>
<p>Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases. K Williams, E Bilsland, A Sparkes, W Aubrey, M Young, L N Soldatova, K De Grave, J Ramon, M Clare, W Sirawaraporn, S G Oliver, R D King, 10.1098/rsif.2014.1289Journal of the Royal Society Interface. 12104201412892015</p>
<p>Testing the reproducibility and robustness of the cancer biology literature by robot. K Roper, A Abdel-Rehim, S Hubbard, M Carpenter, A Rzhetsky, L N Soldatova, R D King, 10.1098/rsif.2021.0821Journal of the Royal Society Interface. 19189202108212022</p>
<p>A mobile robotic chemist. B Burger, P M Maffettone, V V Gusev, C M Aitchison, Y Bai, X Wang, X Li, B M Alston, B Li, R Clowes, N Rankin, B Harris, R S Sprick, A I Cooper, 10.1038/s41586-020-2442-2Nature. 58378152020</p>
<p>Robot scientists: From adam to eve to genesis. R King, O Peter, P Courtney, Artificial Intelligence in Science: Challenges, Opportunities and the Future of Research. T Science, O Innovation, ParisOECD Publishing2023</p>
<p>Fantastic Realities: 49 Mind Journeys and a Trip to Stockholm. F Wilczek, B Devine, 2006World ScientificSingapore</p>
<p>Nobel turing challenge: Creating the engine for scientific discovery. H Kitano, Systems Biology and Applications. 7292021</p>
<p>Semanticallybased crossover in genetic programming: application to real-valued symbolic regression. N Q Uy, N X Hoai, M O'neill, R I Mckay, E Galvn-Lpez, Genetic Programming and Evolvable Machines. 122011</p>
<p>Ai feynman: A physics-inspired method for symbolic regression. S.-M Udrescu, M Tegmark, Science advances. 61626312020</p>
<p>Benchmarking materials property prediction methods: the matbench test set and automatminer reference algorithm. A Dunn, Q Wang, A Ganose, D Dopp, A Jain, Computational Materials. 61382020</p>
<p>SCP-116K: A High-Quality Problem-Solution Dataset and a Generalized Pipeline for Automated Extraction in the Higher Education Science Domain. D Lu, X Tan, R Xu, T Yao, C Qu, W Chu, Y Xu, Y Qi, 2025</p>
<p>The well: a large-scale collection of diverse physics simulations for machine learning. R Ohana, M Mccabe, L T Meyer, R Morel, F J Agocs, M Beneitez, M Berger, B Burkhart, S B Dalziel, D B Fielding, D Fortunato, J A Goldberg, K Hirashima, Y.-F Jiang, R Kerswell, S Maddu, J M Miller, P Mukhopadhyay, S S Nixon, J Shen, R Watteaux, B R Blancard, .-S Rozet, F Parker, L H Cranmer, M Ho, S , The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024</p>
<p>ScienceWorld: Is your Agent Smarter than a 5th Grader?. R Wang, P Jansen, M.-A Ct, P Ammanabrolu, 2022</p>
<p>P E Jansen, DiscoveryWorld: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents. 2024</p>
<p>Chemgymrl: A customizable interactive framework for reinforcement learning for digital chemistry. C Beeler, S G Subramanian, K Sprague, M Baula, N Chatti, A Dawit, X Li, N Paquin, M Shahen, Z Yang, C Bellinger, M Crowley, I Tamblyn, 10.1039/D3DD00183KDigital Discovery. 32024</p>
<p>DiscoveryBench: Towards Data-Driven Discovery with Large Language Models. B P Majumder, H Surana, D Agarwal, B Dalvi Mishra, A Meena, A Prakhar, T Vora, T Khot, A Sabharwal, P Clark, Dataset and code available on GitHub. 2024</p>
<p>BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery. K Gandhi, M Y Li, L Goodyear, L Li, A Bhaskar, M Zaman, N D Goodman, 2025Project page with environments</p>
<p>Science-Gym: A simple testbed for ai-driven scientific discovery. M Cerrato, N Schmitt, L Baur, E Finkelstein, S Jukic, L Mnzel, F P Paul, P Pfannes, B Rohr, J Schellenberg, P Wolf, S Kramer, 10.1007/978-3-031-78977-9_15Proceedings of the 26th International Conference on Discovery Science (DS). Lecture Notes in Computer Science. the 26th International Conference on Discovery Science (DS)Pisa, ItalySpringer202415243Gym-compatible simulation library for physics/epidemiology scenarios</p>
<p>The open catalyst 2020 (oc20) dataset and community challenges. L E Chanussot, ACS Catalysis. 11102021</p>            </div>
        </div>

    </div>
</body>
</html>
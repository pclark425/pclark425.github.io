<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1378 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1378</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1378</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-e4257bc131c36504a04382290cbc27ca8bb27813</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e4257bc131c36504a04382290cbc27ca8bb27813" target="_blank">Action-Conditional Video Prediction using Deep Networks in Atari Games</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs and proposes and evaluates two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks.</p>
                <p><strong>Paper Abstract:</strong> Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1378.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1378.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feedforward AC-Predictor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feedforward Action-Conditional Video Prediction Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A convolutional encoder + multiplicative action-conditional factorized transformation + deconvolutional decoder that takes a fixed history of concatenated frames and an action to predict the next image frame(s). Optimized with multi-step curriculum training for long-horizon, action-conditional video prediction in Atari.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Feedforward action-conditional predictive model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deterministic neural predictive world model: concatenates the last m frames as input channels, passes them through stacked convolutional layers and a fully-connected layer to produce a high-level encoded vector h_enc; applies a factorized multiplicative action-conditional transform (W^dec (W^enc h_enc ⊙ W^a a_t) + b) to produce an action-transformed feature h_dec; reshapes and decodes h_dec through fully-connected + deconvolution layers to produce next-frame pixels. Trained with multi-step prediction objective and curriculum training (1/3/5-step phases) using BPTT/RMSProp.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural predictive world model (latent image-space predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (Arcade Learning Environment) — vision-based reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Pixel-wise mean squared error (MSE) over multi-step (up to 100-step) predictions; qualitative visual realism (video) and task-level performance (game score when replacing emulator frames)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Consistently lower multi-step MSE than MLP and no-action feedforward baselines (plotted in Fig.3); qualitatively produces realistic predictions for 30–500 steps depending on game and predicts controlled-object motion accurately over ≈100-step futures in some games. No single scalar MSE value reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable: action-factors exhibit structured cosine-similarity (similar actions map to similar factors), and ablation of high-variance vs low-variance factors shows disentangling — high-variance factors control agent-controlled object movements while low-variance factors represent other objects/background.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Cosine similarity analysis of action-factor vectors; variance-based factor selection and controlled forward propagation (zeroing subsets of factors) with visualization of resulting predicted frames.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Architecture uses 4 conv layers + 1 FC (2048 units), factor layer with f=2048, decoding FC (11264 units) + 4 deconv layers; training uses three curriculum phases with iterations 1.5M (1-step), 1.0M (3-step), 1.0M (5-step); RMSProp optimizer, varying batch sizes (e.g., feedforward: 32/8/8), no explicit GPU-hours reported. A factorized transform was used to avoid the parameter blow-up of an explicit 3-way tensor.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Factorized multiplicative transform and deconvolution are described as more scalable/efficient than a full 3-way tensor and than upsampling-then-convolution respectively; a downsampled 84×84 gray feedforward variant was used for computationally efficient prediction in informed-exploration experiments. No wall-clock speedup numbers provided versus baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When used as an emulator to feed DQN, replacing true frames with the model's predicted frames for short repetition depths maintains near-emulator scores; in several games the agent still scores substantially above random even when using 100-step predictions. Using the model to enable informed exploration improves DQN learning in 3/5 tested games (e.g., QBert shows large improvement). Exact per-game score tables are reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High-fidelity predictions of the agent-controlled object are more important for preserving control performance than minimizing global pixel MSE; although overall MSE differences to the no-action baseline can be small, better modeling of action-conditioned parts yields much better task (game) scores and enables informed exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off between short-term spatial precision and long-term dependency modeling: feedforward encoding offers more precise short-term spatial translations (better immediate position accuracy) but limited long-range memory, while recurrent encoding (LSTM) captures longer temporal dependencies at the expense of slightly less precise spatial localization. Factorization reduces parameter cost but enforces weight sharing across actions (beneficial when dynamics overlap).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Early-fusion (concatenate frames) feedforward CNN encoder; factorized multiplicative action-conditioning (W^enc, W^a, W^dec) with f=2048 factors; deconvolutional decoder; multi-phase curriculum learning (1,3,5 steps); RMSProp optimizer; train on full RGB 210×160 images or downsampled 84×84 gray version for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared experimentally to an MLP baseline and a no-action feedforward variant: outperforms both in multi-step MSE and in task experiments (control when replacing emulator frames and improving exploration). Compared in discussion to prior patch-based Bayesian predictors (which fail on Atari due to long-range dependencies) and to recurrent models like RTRBM/LSTM-based prior work; the paper argues its architectures scale to high-dimensional action-conditional video prediction where prior methods struggled.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommendations/insights: (1) Use factorized multiplicative interactions rather than a full 3-way tensor to scale with many actions; (2) use curriculum multi-step training to stabilize long-horizon prediction; (3) choose feedforward encoding for precise short-horizon spatial prediction and recurrent encoding (LSTM) for tasks requiring longer temporal context; (4) use a downsampled/gray variant for computationally constrained settings (e.g., online informed exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Action-Conditional Video Prediction using Deep Networks in Atari Games', 'publication_date_yy_mm': '2015-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1378.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1378.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recurrent AC-Predictor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent (LSTM) Action-Conditional Video Prediction Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A convolutional encoder producing per-frame spatial features fed into an LSTM recurrent layer, followed by the same factorized multiplicative action-conditional transform and deconvolutional decoder to produce next-frame pixel predictions; designed to capture longer-term temporal dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent action-conditional predictive model (CNN+LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>At each time step: CNN(x_t) → LSTM (h_enc, c) produces encoded temporal feature h_enc that is transformed via the factorized multiplicative action-conditional layer (W^dec (W^enc h_enc ⊙ W^a a_t) + b) into h_dec and decoded to pixels via reshape + deconv. LSTM has 2048 hidden units and models temporal dynamics across frames; training uses curriculum multi-step prediction and BPTT.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural predictive world model with recurrent temporal dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (vision-based RL)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Pixel-wise MSE for multi-step predictions; qualitative video realism and downstream control performance (game score when generating frames for DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Lower multi-step MSE than simple baselines; qualitatively better at predicting events requiring longer histories (e.g., delayed enemy movement), but sometimes makes small translation errors that compound over time; no single MSE scalar published in text.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Same factor-based interpretability as feedforward model since the multiplicative factor layer is shared; analysis shows disentangled factors responsible for action-conditioned transformations. Recurrent states (LSTM) are not further interpreted in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Cosine similarity of action-factors; variance-based factor ablation and visualization; qualitative comparison of prediction errors to understand failure modes (translation errors leading to divergence).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Same convolutional and decoding stack as feedforward, plus an LSTM with 2048 units; training unrolled up to 20 steps in 1-step phase and uses smaller batch sizes for recurrent model (e.g., 4); curriculum total iterations as for feedforward (per phase). No explicit GPU/time cost reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Recurrent encoding requires smaller input (one frame per step) but uses expensive LSTM recurrence and smaller batch sizes — tradeoffs versus feedforward which concatenates multiple frames (larger input) but no recurrence. Paper notes memory/parameter growth for feedforward if concatenating more frames, making recurrent encoding more suitable for long histories.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Tends to outperform baselines on tasks requiring longer temporal context (e.g., Space Invaders enemy timing); however, feedforward sometimes yields better game-control performance when precise short-term spatial accuracy is critical. Overall predictive models (both variants) enable DQN to play significantly above random and allow informed exploration improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Recurrent model's better long-term temporal modeling can improve predictions of events that depend on longer histories and thereby improve downstream RL when such events are task-relevant; but recurrent model's small spatial translation errors can degrade long-horizon prediction trajectories in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Recurrent encoding vs feedforward: recurrent better for long-horizon dependencies and patterns with delayed dynamics; feedforward better for precise short-term localization and translation accuracy. Computational/memory tradeoffs: feedforward grows input size with more frames, while recurrent requires LSTM capacity and BPTT unrolling.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>CNN per-frame encoder + LSTM (2048 units) on top of CNN features; same factorized multiplicative action-conditioning and deconvolutional decoder; multi-step curriculum training; gradient clipping for LSTM gates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared directly to feedforward variant and baselines; paper reports complementary strengths and gives examples where each variant outperforms the other depending on the temporal characteristics of the domain.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests recurrent encoding is preferable when long-range temporal dependencies matter and when input history length would make feedforward concatenation impractical; feedforward is preferable when precise short-term spatial predictions are required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Action-Conditional Video Prediction using Deep Networks in Atari Games', 'publication_date_yy_mm': '2015-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1378.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1378.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Factorized AC-Transform</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Factorized Multiplicative Action-Conditional Transformation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A factorized implementation of multiplicative (3-way) interactions between encoded state features and discrete actions that produces action-conditioned transformations in latent feature space while controlling parameter count via factorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Factorized multiplicative action-conditional transform</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Implements h_dec = W^dec ( (W^enc h_enc) ⊙ (W^a a_t) ) + b where W^enc maps encoded features to f factors, W^a maps action one-hot to f factors, ⊙ is element-wise product, and W^dec maps factors back to latent feature space. This approximates a full 3-way tensor (W_{ijl}) while sharing structure across actions and keeping parameter count manageable (f=2048 used).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>architectural component of neural predictive world model (factorized multiplicative interaction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari action-conditional video prediction (general to vision-based RL)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Indirectly evaluated via downstream pixel MSE and qualitative prediction performance; also analyzed by cosine similarity and variance across actions for factor vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Enables modeling distinct action-conditioned transformations and yields better long-term prediction and control performance than a non-action transformation baseline; specific numeric improvements in MSE shown in plots but not reported as scalar in text.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Interpretable to extent that factor dimensions correlate with action semantics: cosine similarity between action-factor vectors groups similar actions and variance of factor activations identifies action-relevant factors enabling visualization of action-conditioned vs non-action-conditioned predicted image regions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Cosine similarity heatmaps among action-factor vectors; variance-based factor partitioning (high-variance vs low-variance) and ablation experiments that visualize what each subset predicts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Factorization chosen specifically for computational scalability compared to full 3-way tensor which would be intractable; factor size f=2048 reported. No exact parameter count or runtime provided, but authors state factorization shares weights and reduces parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Described as scalable and preferable over the naive 3-way tensor due to far fewer parameters and capacity to share dynamics across actions; used in all presented experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Crucial to enabling action-conditional predictions that preserve agent-controlled object motion, which directly improves control utility (emulator replacement and informed exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Sharing via factorization is beneficial when different actions share common temporal dynamics; factor variance partitioning shows the transform can disentangle action-dependent and action-independent parts of the scene — a beneficial inductive bias for control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Factorization trades full action-specific parameterization (3-way tensor, potentially more expressive) for parameter efficiency and generalization across actions. This can be positive when dynamics overlap across actions but could underparameterize entirely distinct action effects.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Choose factor size f large (2048) to provide capacity while still controlling parameter count; use one-hot action representation mapped by W^a; combine with CNN/LSTM encoders and deconv decoder; adopt variance-based analysis to interpret factors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared implicitly to (a) full 3-way tensor (not used because of parameter explosion) and (b) no-action baseline (naFf) which lacks action-conditioning; factorized transform offered a practical middle ground and empirical superiority to no-action baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests factorization with a large number of factors (2048) is effective for Atari domains and that it is desirable when actions share temporal dynamics; no exhaustive hyperparameter sweep reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Action-Conditional Video Prediction using Deep Networks in Atari Games', 'publication_date_yy_mm': '2015-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1378.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1378.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baselines (MLP, naFf)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison Baselines: Multi-layer Perceptron and No-Action Feedforward Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two baselines used for experimental comparison: (1) an MLP taking last frame and action concatenated to a hidden layer, and (2) a feedforward CNN encoder/decoder identical to the main model but without action input in the transformation layer (no-action feedforward, naFf).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MLP and no-action feedforward baselines</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MLP: 1 input frame → 4 hidden layers (400, 2048, 2048, 400) with action concatenated to second hidden layer. naFf: same feedforward CNN encoder-decoder architecture as proposed feedforward model but transformation layer is a fully-connected layer that does not take action input, so predictions are action-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>baseline neural predictive models (non-action-conditional or shallow)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari game frame prediction and downstream control evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Pixel-wise MSE over multi-step predictions; qualitative video comparison</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>MLP baseline quickly diverges in long-horizon prediction; naFf captures uncontrolled-object movement and background but fails to predict controlled-object movements (leading to worse control performance). Overall proposed models outperform both baselines across domains (plots in Fig.3).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>MLP is a largely black-box shallow model; naFf reveals inability to represent action-conditioned dynamics allowing implicit interpretability: missing controlled-object predictions indicate lack of action-conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Qualitative visualization of predicted videos and failure modes (e.g., controlled object disappears or is static) used to interpret baselines' shortcomings.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>MLP sized to match parameter count of recurrent model; naFf matches feedforward architecture minus action-conditioning. Specific parameter counts not reported, but architectures and layer sizes are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>MLP cheaper per-step but fails qualitatively; naFf comparable compute to feedforward but lacks action conditioning so task utility is lower despite similar pixel MSE in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Both baselines perform poorly as a replacement for emulator frames relative to proposed models; naFf fails to predict controlled object so DQN performance degrades substantially; MLP diverges rapidly and yields nearly mean-pixel images.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Demonstrates that action-conditioning is essential for predictive models intended for control tasks; overall pixel MSE alone can be misleading about task utility because naFf sometimes has similar MSE but poor control performance.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Simpler models (MLP) are computationally cheaper but unusable for long-horizon or action-conditional prediction; architectures matching capacity but lacking action inputs (naFf) show that capacity alone is insufficient without correct conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Baselines chosen to match parameter budgets (MLP) or architecture (naFf) to isolate the contribution of action-conditioning and the encoder type.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Used as direct experimental comparators in all experiments; show empirical superiority of the proposed action-conditional designs.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>No recommendation to use baselines for tasks requiring action-conditional prediction; they serve as ablation/controls.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Action-Conditional Video Prediction using Deep Networks in Atari Games', 'publication_date_yy_mm': '2015-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning <em>(Rating: 2)</em></li>
                <li>Factored conditional restricted Boltzmann machines for modeling motion style <em>(Rating: 2)</em></li>
                <li>Bayesian learning of recursively factored environments <em>(Rating: 2)</em></li>
                <li>The recurrent temporal restricted Boltzmann machine <em>(Rating: 1)</em></li>
                <li>Unsupervised learning of video representations using LSTMs <em>(Rating: 1)</em></li>
                <li>Investigating contingency awareness using Atari 2600 games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1378",
    "paper_id": "paper-e4257bc131c36504a04382290cbc27ca8bb27813",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Feedforward AC-Predictor",
            "name_full": "Feedforward Action-Conditional Video Prediction Network",
            "brief_description": "A convolutional encoder + multiplicative action-conditional factorized transformation + deconvolutional decoder that takes a fixed history of concatenated frames and an action to predict the next image frame(s). Optimized with multi-step curriculum training for long-horizon, action-conditional video prediction in Atari.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Feedforward action-conditional predictive model",
            "model_description": "Deterministic neural predictive world model: concatenates the last m frames as input channels, passes them through stacked convolutional layers and a fully-connected layer to produce a high-level encoded vector h_enc; applies a factorized multiplicative action-conditional transform (W^dec (W^enc h_enc ⊙ W^a a_t) + b) to produce an action-transformed feature h_dec; reshapes and decodes h_dec through fully-connected + deconvolution layers to produce next-frame pixels. Trained with multi-step prediction objective and curriculum training (1/3/5-step phases) using BPTT/RMSProp.",
            "model_type": "neural predictive world model (latent image-space predictor)",
            "task_domain": "Atari games (Arcade Learning Environment) — vision-based reinforcement learning",
            "fidelity_metric": "Pixel-wise mean squared error (MSE) over multi-step (up to 100-step) predictions; qualitative visual realism (video) and task-level performance (game score when replacing emulator frames)",
            "fidelity_performance": "Consistently lower multi-step MSE than MLP and no-action feedforward baselines (plotted in Fig.3); qualitatively produces realistic predictions for 30–500 steps depending on game and predicts controlled-object motion accurately over ≈100-step futures in some games. No single scalar MSE value reported in paper.",
            "interpretability_assessment": "Partially interpretable: action-factors exhibit structured cosine-similarity (similar actions map to similar factors), and ablation of high-variance vs low-variance factors shows disentangling — high-variance factors control agent-controlled object movements while low-variance factors represent other objects/background.",
            "interpretability_method": "Cosine similarity analysis of action-factor vectors; variance-based factor selection and controlled forward propagation (zeroing subsets of factors) with visualization of resulting predicted frames.",
            "computational_cost": "Architecture uses 4 conv layers + 1 FC (2048 units), factor layer with f=2048, decoding FC (11264 units) + 4 deconv layers; training uses three curriculum phases with iterations 1.5M (1-step), 1.0M (3-step), 1.0M (5-step); RMSProp optimizer, varying batch sizes (e.g., feedforward: 32/8/8), no explicit GPU-hours reported. A factorized transform was used to avoid the parameter blow-up of an explicit 3-way tensor.",
            "efficiency_comparison": "Factorized multiplicative transform and deconvolution are described as more scalable/efficient than a full 3-way tensor and than upsampling-then-convolution respectively; a downsampled 84×84 gray feedforward variant was used for computationally efficient prediction in informed-exploration experiments. No wall-clock speedup numbers provided versus baselines.",
            "task_performance": "When used as an emulator to feed DQN, replacing true frames with the model's predicted frames for short repetition depths maintains near-emulator scores; in several games the agent still scores substantially above random even when using 100-step predictions. Using the model to enable informed exploration improves DQN learning in 3/5 tested games (e.g., QBert shows large improvement). Exact per-game score tables are reported in the paper.",
            "task_utility_analysis": "High-fidelity predictions of the agent-controlled object are more important for preserving control performance than minimizing global pixel MSE; although overall MSE differences to the no-action baseline can be small, better modeling of action-conditioned parts yields much better task (game) scores and enables informed exploration.",
            "tradeoffs_observed": "Trade-off between short-term spatial precision and long-term dependency modeling: feedforward encoding offers more precise short-term spatial translations (better immediate position accuracy) but limited long-range memory, while recurrent encoding (LSTM) captures longer temporal dependencies at the expense of slightly less precise spatial localization. Factorization reduces parameter cost but enforces weight sharing across actions (beneficial when dynamics overlap).",
            "design_choices": "Early-fusion (concatenate frames) feedforward CNN encoder; factorized multiplicative action-conditioning (W^enc, W^a, W^dec) with f=2048 factors; deconvolutional decoder; multi-phase curriculum learning (1,3,5 steps); RMSProp optimizer; train on full RGB 210×160 images or downsampled 84×84 gray version for efficiency.",
            "comparison_to_alternatives": "Compared experimentally to an MLP baseline and a no-action feedforward variant: outperforms both in multi-step MSE and in task experiments (control when replacing emulator frames and improving exploration). Compared in discussion to prior patch-based Bayesian predictors (which fail on Atari due to long-range dependencies) and to recurrent models like RTRBM/LSTM-based prior work; the paper argues its architectures scale to high-dimensional action-conditional video prediction where prior methods struggled.",
            "optimal_configuration": "Paper recommendations/insights: (1) Use factorized multiplicative interactions rather than a full 3-way tensor to scale with many actions; (2) use curriculum multi-step training to stabilize long-horizon prediction; (3) choose feedforward encoding for precise short-horizon spatial prediction and recurrent encoding (LSTM) for tasks requiring longer temporal context; (4) use a downsampled/gray variant for computationally constrained settings (e.g., online informed exploration).",
            "uuid": "e1378.0",
            "source_info": {
                "paper_title": "Action-Conditional Video Prediction using Deep Networks in Atari Games",
                "publication_date_yy_mm": "2015-07"
            }
        },
        {
            "name_short": "Recurrent AC-Predictor",
            "name_full": "Recurrent (LSTM) Action-Conditional Video Prediction Network",
            "brief_description": "A convolutional encoder producing per-frame spatial features fed into an LSTM recurrent layer, followed by the same factorized multiplicative action-conditional transform and deconvolutional decoder to produce next-frame pixel predictions; designed to capture longer-term temporal dependencies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Recurrent action-conditional predictive model (CNN+LSTM)",
            "model_description": "At each time step: CNN(x_t) → LSTM (h_enc, c) produces encoded temporal feature h_enc that is transformed via the factorized multiplicative action-conditional layer (W^dec (W^enc h_enc ⊙ W^a a_t) + b) into h_dec and decoded to pixels via reshape + deconv. LSTM has 2048 hidden units and models temporal dynamics across frames; training uses curriculum multi-step prediction and BPTT.",
            "model_type": "neural predictive world model with recurrent temporal dynamics",
            "task_domain": "Atari games (vision-based RL)",
            "fidelity_metric": "Pixel-wise MSE for multi-step predictions; qualitative video realism and downstream control performance (game score when generating frames for DQN)",
            "fidelity_performance": "Lower multi-step MSE than simple baselines; qualitatively better at predicting events requiring longer histories (e.g., delayed enemy movement), but sometimes makes small translation errors that compound over time; no single MSE scalar published in text.",
            "interpretability_assessment": "Same factor-based interpretability as feedforward model since the multiplicative factor layer is shared; analysis shows disentangled factors responsible for action-conditioned transformations. Recurrent states (LSTM) are not further interpreted in paper.",
            "interpretability_method": "Cosine similarity of action-factors; variance-based factor ablation and visualization; qualitative comparison of prediction errors to understand failure modes (translation errors leading to divergence).",
            "computational_cost": "Same convolutional and decoding stack as feedforward, plus an LSTM with 2048 units; training unrolled up to 20 steps in 1-step phase and uses smaller batch sizes for recurrent model (e.g., 4); curriculum total iterations as for feedforward (per phase). No explicit GPU/time cost reported.",
            "efficiency_comparison": "Recurrent encoding requires smaller input (one frame per step) but uses expensive LSTM recurrence and smaller batch sizes — tradeoffs versus feedforward which concatenates multiple frames (larger input) but no recurrence. Paper notes memory/parameter growth for feedforward if concatenating more frames, making recurrent encoding more suitable for long histories.",
            "task_performance": "Tends to outperform baselines on tasks requiring longer temporal context (e.g., Space Invaders enemy timing); however, feedforward sometimes yields better game-control performance when precise short-term spatial accuracy is critical. Overall predictive models (both variants) enable DQN to play significantly above random and allow informed exploration improvements.",
            "task_utility_analysis": "Recurrent model's better long-term temporal modeling can improve predictions of events that depend on longer histories and thereby improve downstream RL when such events are task-relevant; but recurrent model's small spatial translation errors can degrade long-horizon prediction trajectories in some cases.",
            "tradeoffs_observed": "Recurrent encoding vs feedforward: recurrent better for long-horizon dependencies and patterns with delayed dynamics; feedforward better for precise short-term localization and translation accuracy. Computational/memory tradeoffs: feedforward grows input size with more frames, while recurrent requires LSTM capacity and BPTT unrolling.",
            "design_choices": "CNN per-frame encoder + LSTM (2048 units) on top of CNN features; same factorized multiplicative action-conditioning and deconvolutional decoder; multi-step curriculum training; gradient clipping for LSTM gates.",
            "comparison_to_alternatives": "Compared directly to feedforward variant and baselines; paper reports complementary strengths and gives examples where each variant outperforms the other depending on the temporal characteristics of the domain.",
            "optimal_configuration": "Paper suggests recurrent encoding is preferable when long-range temporal dependencies matter and when input history length would make feedforward concatenation impractical; feedforward is preferable when precise short-term spatial predictions are required.",
            "uuid": "e1378.1",
            "source_info": {
                "paper_title": "Action-Conditional Video Prediction using Deep Networks in Atari Games",
                "publication_date_yy_mm": "2015-07"
            }
        },
        {
            "name_short": "Factorized AC-Transform",
            "name_full": "Factorized Multiplicative Action-Conditional Transformation",
            "brief_description": "A factorized implementation of multiplicative (3-way) interactions between encoded state features and discrete actions that produces action-conditioned transformations in latent feature space while controlling parameter count via factorization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Factorized multiplicative action-conditional transform",
            "model_description": "Implements h_dec = W^dec ( (W^enc h_enc) ⊙ (W^a a_t) ) + b where W^enc maps encoded features to f factors, W^a maps action one-hot to f factors, ⊙ is element-wise product, and W^dec maps factors back to latent feature space. This approximates a full 3-way tensor (W_{ijl}) while sharing structure across actions and keeping parameter count manageable (f=2048 used).",
            "model_type": "architectural component of neural predictive world model (factorized multiplicative interaction)",
            "task_domain": "Atari action-conditional video prediction (general to vision-based RL)",
            "fidelity_metric": "Indirectly evaluated via downstream pixel MSE and qualitative prediction performance; also analyzed by cosine similarity and variance across actions for factor vectors.",
            "fidelity_performance": "Enables modeling distinct action-conditioned transformations and yields better long-term prediction and control performance than a non-action transformation baseline; specific numeric improvements in MSE shown in plots but not reported as scalar in text.",
            "interpretability_assessment": "Interpretable to extent that factor dimensions correlate with action semantics: cosine similarity between action-factor vectors groups similar actions and variance of factor activations identifies action-relevant factors enabling visualization of action-conditioned vs non-action-conditioned predicted image regions.",
            "interpretability_method": "Cosine similarity heatmaps among action-factor vectors; variance-based factor partitioning (high-variance vs low-variance) and ablation experiments that visualize what each subset predicts.",
            "computational_cost": "Factorization chosen specifically for computational scalability compared to full 3-way tensor which would be intractable; factor size f=2048 reported. No exact parameter count or runtime provided, but authors state factorization shares weights and reduces parameters.",
            "efficiency_comparison": "Described as scalable and preferable over the naive 3-way tensor due to far fewer parameters and capacity to share dynamics across actions; used in all presented experiments.",
            "task_performance": "Crucial to enabling action-conditional predictions that preserve agent-controlled object motion, which directly improves control utility (emulator replacement and informed exploration).",
            "task_utility_analysis": "Sharing via factorization is beneficial when different actions share common temporal dynamics; factor variance partitioning shows the transform can disentangle action-dependent and action-independent parts of the scene — a beneficial inductive bias for control tasks.",
            "tradeoffs_observed": "Factorization trades full action-specific parameterization (3-way tensor, potentially more expressive) for parameter efficiency and generalization across actions. This can be positive when dynamics overlap across actions but could underparameterize entirely distinct action effects.",
            "design_choices": "Choose factor size f large (2048) to provide capacity while still controlling parameter count; use one-hot action representation mapped by W^a; combine with CNN/LSTM encoders and deconv decoder; adopt variance-based analysis to interpret factors.",
            "comparison_to_alternatives": "Compared implicitly to (a) full 3-way tensor (not used because of parameter explosion) and (b) no-action baseline (naFf) which lacks action-conditioning; factorized transform offered a practical middle ground and empirical superiority to no-action baseline.",
            "optimal_configuration": "Paper suggests factorization with a large number of factors (2048) is effective for Atari domains and that it is desirable when actions share temporal dynamics; no exhaustive hyperparameter sweep reported.",
            "uuid": "e1378.2",
            "source_info": {
                "paper_title": "Action-Conditional Video Prediction using Deep Networks in Atari Games",
                "publication_date_yy_mm": "2015-07"
            }
        },
        {
            "name_short": "Baselines (MLP, naFf)",
            "name_full": "Comparison Baselines: Multi-layer Perceptron and No-Action Feedforward Model",
            "brief_description": "Two baselines used for experimental comparison: (1) an MLP taking last frame and action concatenated to a hidden layer, and (2) a feedforward CNN encoder/decoder identical to the main model but without action input in the transformation layer (no-action feedforward, naFf).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MLP and no-action feedforward baselines",
            "model_description": "MLP: 1 input frame → 4 hidden layers (400, 2048, 2048, 400) with action concatenated to second hidden layer. naFf: same feedforward CNN encoder-decoder architecture as proposed feedforward model but transformation layer is a fully-connected layer that does not take action input, so predictions are action-agnostic.",
            "model_type": "baseline neural predictive models (non-action-conditional or shallow)",
            "task_domain": "Atari game frame prediction and downstream control evaluation",
            "fidelity_metric": "Pixel-wise MSE over multi-step predictions; qualitative video comparison",
            "fidelity_performance": "MLP baseline quickly diverges in long-horizon prediction; naFf captures uncontrolled-object movement and background but fails to predict controlled-object movements (leading to worse control performance). Overall proposed models outperform both baselines across domains (plots in Fig.3).",
            "interpretability_assessment": "MLP is a largely black-box shallow model; naFf reveals inability to represent action-conditioned dynamics allowing implicit interpretability: missing controlled-object predictions indicate lack of action-conditioning.",
            "interpretability_method": "Qualitative visualization of predicted videos and failure modes (e.g., controlled object disappears or is static) used to interpret baselines' shortcomings.",
            "computational_cost": "MLP sized to match parameter count of recurrent model; naFf matches feedforward architecture minus action-conditioning. Specific parameter counts not reported, but architectures and layer sizes are provided.",
            "efficiency_comparison": "MLP cheaper per-step but fails qualitatively; naFf comparable compute to feedforward but lacks action conditioning so task utility is lower despite similar pixel MSE in some cases.",
            "task_performance": "Both baselines perform poorly as a replacement for emulator frames relative to proposed models; naFf fails to predict controlled object so DQN performance degrades substantially; MLP diverges rapidly and yields nearly mean-pixel images.",
            "task_utility_analysis": "Demonstrates that action-conditioning is essential for predictive models intended for control tasks; overall pixel MSE alone can be misleading about task utility because naFf sometimes has similar MSE but poor control performance.",
            "tradeoffs_observed": "Simpler models (MLP) are computationally cheaper but unusable for long-horizon or action-conditional prediction; architectures matching capacity but lacking action inputs (naFf) show that capacity alone is insufficient without correct conditioning.",
            "design_choices": "Baselines chosen to match parameter budgets (MLP) or architecture (naFf) to isolate the contribution of action-conditioning and the encoder type.",
            "comparison_to_alternatives": "Used as direct experimental comparators in all experiments; show empirical superiority of the proposed action-conditional designs.",
            "optimal_configuration": "No recommendation to use baselines for tasks requiring action-conditional prediction; they serve as ablation/controls.",
            "uuid": "e1378.3",
            "source_info": {
                "paper_title": "Action-Conditional Video Prediction using Deep Networks in Atari Games",
                "publication_date_yy_mm": "2015-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning",
            "rating": 2
        },
        {
            "paper_title": "Factored conditional restricted Boltzmann machines for modeling motion style",
            "rating": 2
        },
        {
            "paper_title": "Bayesian learning of recursively factored environments",
            "rating": 2
        },
        {
            "paper_title": "The recurrent temporal restricted Boltzmann machine",
            "rating": 1
        },
        {
            "paper_title": "Unsupervised learning of video representations using LSTMs",
            "rating": 1
        },
        {
            "paper_title": "Investigating contingency awareness using Atari 2600 games",
            "rating": 1
        }
    ],
    "cost": 0.015021499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Action-Conditional Video Prediction using Deep Networks in Atari Games</h1>
<p>Junhyuk Oh Xiaoxiao Guo Honglak Lee Richard Lewis Satinder Singh<br>University of Michigan, Ann Arbor, MI 48109, USA<br>{junhyuk, guoxiao, honglak, rickl, baveja}@umich.edu</p>
<h4>Abstract</h4>
<p>Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future image-frames depend on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, actionconditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100 -step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.</p>
<h2>1 Introduction</h2>
<p>Over the years, deep learning approaches (see [5, 26] for survey) have shown great success in many visual perception problems (e.g., $[16,7,32,9]$ ). However, modeling videos (building a generative model) is still a very challenging problem because it often involves high-dimensional natural-scene data with complex temporal dynamics. Thus, recent studies have mostly focused on modeling simple video data, such as bouncing balls or small patches, where the next frame is highly-predictable given the previous frames [29, 20, 19]. In many applications, however, future frames depend not only on previous frames but also on control or action variables. For example, the first-person-view in a vehicle is affected by wheel-steering and acceleration. The camera observation of a robot is similarly dependent on its movement and changes of its camera angle. More generally, in vision-based reinforcement learning (RL) problems, learning to predict future images conditioned on actions amounts to learning a model of the dynamics of the agent-environment interaction, an essential component of model-based approaches to RL. In this paper, we focus on Atari games from the Arcade Learning Environment (ALE) [1] as a source of challenging action-conditional video modeling problems. While not composed of natural scenes, frames in Atari games are high-dimensional, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional images conditioned by control inputs.</p>
<p>This paper proposes, evaluates, and contrasts two spatio-temporal prediction architectures based on deep networks that incorporate action variables (See Figure 1). Our experimental results show that our architectures are able to generate realistic frames over 100-step action-conditional future frames without diverging in some Atari games. We show that the representations learned by our architectures 1) approximately capture natural similarity among actions, and 2) discover which objects are directly controlled by the agent's actions and which are only indirectly influenced or not controlled. We evaluated the usefulness of our architectures for control in two ways: 1) by replacing emulator frames with predicted frames in a previously-learned model-free controller (DQN; DeepMind's state</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Proposed Encoding-Transformation-Decoding network architectures.
of the art Deep-Q-Network for Atari Games [21]), and 2) by using the predicted frames to drive a more informed than random exploration strategy to improve a model-free controller (also DQN).</p>
<h1>2 Related Work</h1>
<p>Video Prediction using Deep Networks. The problem of video prediction has led to a variety of architectures in deep learning. A recurrent temporal restricted Boltzmann machine (RTRBM) [29] was proposed to learn temporal correlations from sequential data by introducing recurrent connections in RBM. A structured RTRBM (sRTRBM) [20] scaled up RTRBM by learning dependency structures between observations and hidden variables from data. More recently, Michalski et al. [19] proposed a higher-order gated autoencoder that defines multiplicative interactions between consecutive frames and mapping units, and showed that temporal prediction problem can be viewed as learning and inferring higher-order interactions between consecutive images. Srivastava et al. [28] applied a sequence-to-sequence learning framework [31] to a video domain, and showed that long short-term memory (LSTM) [12] networks are capable of generating video of bouncing handwritten digits. In contrast to these previous studies, this paper tackles problems where control variables affect temporal dynamics, and in addition scales up spatio-temporal prediction to larger-size images.
ALE: Combining Deep Learning and RL. Atari 2600 games provide challenging environments for RL because of high-dimensional visual observations, partial observability, and delayed rewards. Approaches that combine deep learning and RL have made significant advances [21, 22, 11]. Specifically, DQN [21] combined Q-learning [36] with a convolutional neural network (CNN) and achieved state-of-the-art performance on many Atari games. Guo et al. [11] used the ALE-emulator for making action-conditional predictions with slow UCT [15], a Monte-Carlo tree search method, to generate training data for a fast-acting CNN, which outperformed DQN on several domains. Throughout this paper we will use DQN to refer to the architecture used in [21] (a more recent work [22] used a deeper CNN with more data to produce the currently best-performing Atari game players).
Action-Conditional Predictive Model for RL. The idea of building a predictive model for vision-based RL problems was introduced by Schmidhuber and Huber [27]. They proposed a neural network that predicts the attention region given the previous frame and an attention-guiding action. More recently, Lenz et al. [17] proposed a recurrent neural network with multiplicative interactions that predicts the physical coordinate of a robot. Compared to this previous work, our work is evaluated on much higher-dimensional data with complex dependencies among observations. There have been a few attempts to learn from ALE data a transition-model that makes predictions of future frames. One line of work [3, 4] divides game images into patches and applies a Bayesian framework to predict patch-based observations. However, this approach assumes that neighboring patches are enough to predict the center patch, which is not true in Atari games because of many complex interactions. The evaluation in this prior work is 1-step prediction loss; in contrast, here we make and evaluate long-term predictions both for quality of pixels generated and for usefulness to control.</p>
<h2>3 Proposed Architectures and Training Method</h2>
<p>The goal of our architectures is to learn a function $f: \mathbf{x}<em t="t">{1: t}, \mathbf{a}</em>} \rightarrow \mathbf{x<em t="t">{t+1}$, where $\mathbf{x}</em>}$ and $\mathbf{a<em 1:="1:" t="t">{t}$ are the frame and action variables at time $t$, and $\mathbf{x}</em>$ are the frames from time 1 to time $t$. Figure 1 shows our two architectures that are each composed of encoding layers that extract spatio-temporal features from the input frames (§3.1), action-conditional transformation layers that transform the encoded features into a prediction of the next frame in high-level feature space by introducing action variables as additional input (§3.2) and finally decoding layers that map the predicted high-level features into pixels (§3.3). Our contributions are in the novel action-conditional deep convolutional architectures for high-dimensional, long-term prediction as well as in the novel use of the architectures in visionbased RL domains.</p>
<h1>3.1 Two Variants: Feedforward Encoding and Recurrent Encoding</h1>
<p>Feedforward encoding takes a fixed history of previous frames as an input, which is concatenated through channels (Figure 1a), and stacked convolution layers extract spatio-temporal features directly from the concatenated frames. The encoded feature vector $\mathbf{h}_{t}^{e n c} \in \mathbb{R}^{n}$ at time $t$ is:</p>
<p>$$
\mathbf{h}<em t="t" t-m_1:="t-m+1:">{t}^{e n c}=\operatorname{CNN}\left(\mathbf{x}</em>\right)
$$</p>
<p>where $\mathbf{x}_{t-m+1: t} \in \mathbb{R}^{(m \times c) \times h \times w}$ denotes $m$ frames of $h \times w$ pixel images with $c$ color channels. CNN is a mapping from raw pixels to a high-level feature vector using multiple convolution layers and a fully-connected layer at the end, each of which is followed by a non-linearity. This encoding can be viewed as early-fusion [14] (other types of fusions, e.g., late-fusion or 3D convolution [35] can also be applied to this architecture).
Recurrent encoding takes one frame as an input for each time-step and extracts spatio-temporal features using an RNN in which the temporal dynamics is modeled by the recurrent layer on top of the high-level feature vector extracted by convolution layers (Figure 1b). In this paper, LSTM without peephole connection is used for the recurrent layer as follows:</p>
<p>$$
\left[\mathbf{h}<em t="t">{t}^{e n c}, \mathbf{c}</em>}\right]=\operatorname{LSTM}\left(\operatorname{CNN}\left(\mathbf{x<em t-1="t-1">{t}\right), \mathbf{h}</em>\right)
$$}^{e n c}, \mathbf{c}_{t-1</p>
<p>where $\mathbf{c}<em t="t">{t} \in \mathbb{R}^{n}$ is a memory cell that retains information from a deep history of inputs. Intuitively, $\operatorname{CNN}\left(\mathbf{x}</em>\right)$ is given as input to the LSTM so that the LSTM captures temporal correlations from high-level spatial features.</p>
<h3>3.2 Multiplicative Action-Conditional Transformation</h3>
<p>We use multiplicative interactions between the encoded feature vector and the control variables:</p>
<p>$$
h_{t, i}^{d e c}=\sum_{j, l} W_{i j l} h_{t, j}^{e n c} a_{t, l}+b_{i}
$$</p>
<p>where $\mathbf{h}<em t="t">{t}^{e n c} \in \mathbb{R}^{n}$ is an encoded feature, $\mathbf{h}</em>$ is represented using one-hot vector, using a 3 -way tensor is equivalent to using different weight matrices for each action. This enables the architecture to model different transformations for different actions. The advantages of multiplicative interactions have been explored in image and text processing [33, 30, 18]. In practice the 3-way tensor is not scalable because of its large number of parameters. Thus, we approximate the tensor by factorizing into three matrices as follows [33]:}^{d e c} \in \mathbb{R}^{n}$ is an action-transformed feature, $\mathbf{a}_{t} \in \mathbb{R}^{a}$ is the action-vector at time $t, \mathbf{W} \in \mathbb{R}^{n \times n \times a}$ is 3 -way tensor weight, and $\mathbf{b} \in \mathbb{R}^{n}$ is bias. When the action $\mathbf{a</p>
<p>$$
\mathbf{h}<em t="t">{t}^{d e c}=\mathbf{W}^{d e c}\left(\mathbf{W}^{e n c} \mathbf{h}</em>
$$}^{e n c} \odot \mathbf{W}^{a} \mathbf{a}_{t}\right)+\mathbf{b</p>
<p>where $\mathbf{W}^{d e c} \in \mathbb{R}^{n \times f}, \mathbf{W}^{e n c} \in \mathbb{R}^{f \times n}, \mathbf{W}^{a} \in \mathbb{R}^{f \times a}, \mathbf{b} \in \mathbb{R}^{n}$, and $f$ is the number of factors. Unlike the 3-way tensor, the above factorization shares the weights between different actions by mapping them to the size- $f$ factors. This sharing may be desirable relative to the 3 -way tensor when there are common temporal dynamics in the data across different actions (discussed further in §4.3).</p>
<h3>3.3 Convolutional Decoding</h3>
<p>It has been recently shown that a CNN is capable of generating an image effectively using upsampling followed by convolution with stride of 1 [8]. Similarly, we use the "inverse" operation of convolution, called deconvolution, which maps $1 \times 1$ spatial region of the input to $d \times d$ using deconvolution kernels. The effect of $s \times s$ upsampling can be achieved without explicitly upsampling the feature map by using stride of $s$. We found that this operation is more efficient than upsampling followed by convolution because of the smaller number of convolutions with larger stride.
In the proposed architecture, the transformed feature vector $\mathbf{h}^{d e c}$ is decoded into pixels as follows:</p>
<p>$$
\hat{\mathbf{x}}_{t+1}=\operatorname{Deconv}\left(\operatorname{Reshape}\left(\mathbf{h}^{d e c}\right)\right)
$$</p>
<p>where Reshape is a fully-connected layer where hidden units form a 3D feature map, and Deconv consists of multiple deconvolution layers, each of which is followed by a non-linearity except for the last deconvolution layer.</p>
<h3>3.4 Curriculum Learning with Multi-Step Prediction</h3>
<p>It is almost inevitable for a predictive model to make noisy predictions of high-dimensional images. When the model is trained on a 1-step prediction objective, small prediction errors can compound</p>
<p>through time. To alleviate this effect, we use a multi-step prediction objective. More specifically, given the training data $D=\left{\left(\left(\mathbf{x}<em 1="1">{1}^{(i)}, \mathbf{a}</em>}^{(i)}\right), \ldots,\left(\mathbf{x<em i="i">{T</em>}}^{(i)}, \mathbf{a<em i="i">{T</em>$, the model is trained to minimize the average squared error over $K$-step predictions as follows:}}^{(i)}\right)\right)\right}_{i=1}^{K</p>
<p>$$
\mathcal{L}<em i="i">{K}(\theta)=\frac{1}{2 K} \sum</em>} \sum_{t} \sum_{k=1}^{K}\left|\hat{\mathbf{x}<em t_k="t+k">{t+k}^{(i)}-\mathbf{x}</em>
$$}^{(i)}\right|^{2</p>
<p>where $\hat{\mathbf{x}}_{t+k}^{(i)}$ is a $k$-step future prediction. Intuitively, the network is repeatedly unrolled through $K$ time steps by using its prediction as an input for the next time-step.
The model is trained in multiple phases based on increasing $K$ as suggested by Michalski et al. [19]. In other words, the model is trained to predict short-term future frames and fine-tuned to predict longer-term future frames after the previous phase converges. We found that this curriculum learning [6] approach is necessary to stabilize the training. A stochastic gradient descent with backpropagation through time (BPTT) is used to optimize the parameters of the network.</p>
<h1>4 Experiments</h1>
<p>In the experiments that follow, we have the following goals for our two architectures. 1) To evaluate the predicted frames in two ways: qualitatively evaluating the generated video, and quantitatively evaluating the pixel-based squared error, 2) To evaluate the usefulness of predicted frames for control in two ways: by replacing the emulator's frames with predicted frames for use by DQN, and by using the predictions to improve exploration in DQN, and 3) To analyze the representations learned by our architectures. We begin by describing the details of the data, and model architecture, and baselines.
Data and Preprocessing. We used our replication of DQN to generate game-play video datasets using an $\epsilon$-greedy policy with $\epsilon=0.3$, i.e. DQN is forced to choose a random action with $30 \%$ probability. For each game, the dataset consists of about 500,000 training frames and 50,000 test frames with actions chosen by DQN. Following DQN, actions are chosen once every 4 frames which reduces the video from 60 fps to 15 fps . The number of actions available in games varies from 3 to 18 , and they are represented as one-hot vectors. We used full-resolution RGB images $(210 \times 160)$ and preprocessed the images by subtracting mean pixel values and dividing each pixel value by 255.
Network Architecture. Across all game domains, we use the same network architecture as follows. The encoding layers consist of 4 convolution layers and one fully-connected layer with 2048 hidden units. The convolution layers use $64(8 \times 8), 128(6 \times 6), 128(6 \times 6)$, and $128(4 \times 4)$ filters with stride of 2. Every layer is followed by a rectified linear function [23]. In the recurrent encoding network, an LSTM layer with 2048 hidden units is added on top of the fully-connected layer. The number of factors in the transformation layer is 2048 . The decoding layers consists of one fully-connected layer with $11264(=128 \times 11 \times 8)$ hidden units followed by 4 deconvolution layers. The deconvolution layers use $128(4 \times 4), 128(6 \times 6), 128(6 \times 6)$, and $3(8 \times 8)$ filters with stride of 2. For the feedforward encoding network, the last 4 frames are given as an input for each time-step. The recurrent encoding network takes one frame for each time-step, but it is unrolled through the last 11 frames to initialize the LSTM hidden units before making a prediction. Our implementation is based on Caffe toolbox [13].
Details of Training. We use the curriculum learning scheme above with three phases of increasing prediction step objectives of 1,3 and 5 steps, and learning rates of $10^{-4}, 10^{-5}$, and $10^{-5}$, respectively. RMSProp [34, 10] is used with momentum of 0.9 , (squared) gradient momentum of 0.95 , and min squared gradient of 0.01 . The batch size for each training phase is 32,8 , and 8 for the feedforward encoding network and 4,4 , and 4 for the recurrent encoding network, respectively. When the recurrent encoding network is trained on 1-step prediction objective, the network is unrolled through 20 steps and predicts the last 10 frames by taking ground-truth images as input. Gradients are clipped at $[-0.1,0.1]$ before non-linearity of each gate of LSTM as suggested by [10].
Two Baselines for Comparison. The first baseline is a multi-layer perceptron (MLP) that takes the last frame as input and has 4 hidden layers with 400, 2048, 2048, and 400 units. The action input is concatenated to the second hidden layer. This baseline uses approximately the same number of parameters as the recurrent encoding model. The second baseline, no-action feedforward (or $n a F f$ ), is the same as the feedforward encoding model (Figure 1a) except that the transformation layer consists of one fully-connected layer that does not get the action as input.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example of predictions over 250 steps in Freeway. The 'Step' and 'Action' columns show the number of prediction steps and the actions taken respectively. The white boxes indicate the object controlled by the agent. From prediction step 256 to 257 the controlled object crosses the top boundary and reappears at the bottom; this non-linear shift is predicted by our architectures and is not predicted by MLP and naFf. The horizontal movements of the uncontrolled objects are predicted by our architectures and naFf but not by MLP.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Mean squared error over 100-step predictions</p>
<h3>4.1 Evaluation of Predicted Frames</h3>
<p><strong>Qualitative Evaluation: Prediction video.</strong> The prediction videos of our models and baselines are available in the supplementary material and at the following website: https://sites.google.com/a/umich.edu/junhyuk-sh/action-conditional-video-prediction. As seen in the videos, the proposed models make qualitatively reasonable predictions over 30–500 steps depending on the game. In all games, the MLP baseline quickly diverges, and the naFf baseline fails to predict the controlled object. An example of long-term predictions is illustrated in Figure 2. We observed that both of our models predict complex local translations well such as the movement of vehicles and the controlled object. They can predict interactions between objects such as collision of two objects. Since our architectures effectively extract hierarchical features using CNN, they are able to make a prediction that requires a global context. For example, in Figure 2, the model predicts the sudden change of the location of the controlled object (from the top to the bottom) at 257-step.</p>
<p>However, both of our models have difficulty in accurately predicting small objects, such as bullets in Space Invaders. The reason is that the squared error signal is small when the model fails to predict small objects during training. Another difficulty is in handling stochasticity. In Seaquest, e.g., new objects appear from the left side or right side randomly, and so are hard to predict. Although our models do generate new objects with reasonable shapes and movements (e.g., after appearing they move as in the true frames), the generated frames do not necessarily match the ground-truth.</p>
<p><strong>Quantitative Evaluation: Squared Prediction Error.</strong> Mean squared error over 100-step predictions is reported in Figure 3. Our predictive models outperform the two baselines for all domains. However, the gap between our predictive models and naFf baseline is not large except for Seaquest. This is due to the fact that the object controlled by the action occupies only a small part of the image.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Comparison between two encoding models (feedforward and recurrent). (a) Controlled object is moving along a horizontal corridor. As the recurrent encoding model makes a small translation error at 4th frame, the true position of the object is in the crossroad while the predicted position is still in the corridor. The (true) object then moves upward which is not possible in the predicted position and so the predicted object keeps moving right. This is less likely to happen in feedforward encoding because its position prediction is more accurate. (b) The objects move down after staying at the same location for the first five steps. The feedforward encoding model fails to predict this movement because it only gets the last four frames as input, while the recurrent model predicts this downwards movement more correctly.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Game play performance using the predictive model as an emulator. 'Emulator' and 'Rand' correspond to the performance of DQN with true frames and random play respectively. The x-axis is the number of steps of prediction before re-initialization. The y-axis is the average game score measured from 30 plays.</p>
<h3>Qualitative Analysis of Relative Strengths and Weaknesses of Feedforward and Recurrent Encoding</h3>
<p>We hypothesize that feedforward encoding can model more precise spatial transformations because its convolutional filters can learn temporal correlations directly from pixels in the concatenated frames. In contrast, convolutional filters in recurrent encoding can learn only spatial features from the one-frame input, and the temporal context has to be captured by the recurrent layer on top of the high-level CNN features without localized information. On the other hand, recurrent encoding is potentially better for modeling arbitrarily long-term dependencies, whereas feedforward encoding is not suitable for long-term dependencies because it requires more memory and parameters as more frames are concatenated into the input.</p>
<p>As evidence, in Figure 4a we show a case where feedforward encoding is better at predicting the precise movement of the controlled object, while recurrent encoding makes a 1-2 pixel translation error. This small error leads to entirely different predicted frames after a few steps. Since the feedforward and recurrent architectures are identical except for the encoding part, we conjecture that this result is due to the failure of precise spatio-temporal encoding in recurrent encoding. On the other hand, recurrent encoding is better at predicting when the enemies move in Space Invaders (Figure 4b). This is due to the fact that the enemies move after 9 steps, which is hard for feedforward encoding to predict because it takes only the last four frames as input. We observed similar results showing that feedforward encoding cannot handle long-term dependencies in other games.</p>
<h3>4.2 Evaluating the Usefulness of Predictions for Control</h3>
<h4>Replacing Real Frames with Predicted Frames as Input to DQN</h4>
<p>To evaluate how useful the predictions are for playing the games, we implement an evaluation method that uses the predictive model to replace the game emulator. More specifically, a DQN controller that takes the last four frames is first pre-trained using real frames and then used to play the games based on ϵ = 0.05-greedy policy where the input frames are generated by our predictive model instead of the game emulator. To evaluate how the depth of predictions influence the quality of control, we re-initialize the predictions using the true last frames after every n-steps of prediction for 1 ≤ <em>n</em> ≤ 100. Note that the DQN controller never takes a true frame, just the outputs of our predictive models.</p>
<p>The results are shown in Figure 5. Unsurprisingly, replacing real frames with predicted frames reduces the score. However, in all the games using the model to repeatedly predict only a few time</p>
<p>Table 1: Average game score of DQN over 100 plays with standard error. The first row and the second row show the performance of our DQN replication with different exploration strategies.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Seaquest</th>
<th>S. Invaders</th>
<th>Freeway</th>
<th>QBert</th>
<th>Ms Pacman</th>
</tr>
</thead>
<tbody>
<tr>
<td>DQN - Random exploration</td>
<td>13119 (538)</td>
<td>698 (20)</td>
<td>30.9 (0.2)</td>
<td>3876 (106)</td>
<td>2281 (53)</td>
</tr>
<tr>
<td>DQN - Informed exploration</td>
<td>13265 (577)</td>
<td>681 (23)</td>
<td>32.2 (0.2)</td>
<td>8238 (498)</td>
<td>2522 (57)</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" />
(a) Random exploration.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" />
(b) Informed exploration.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: Cosine similarity between every pair of action factors (see text for details).</p>
<p>Figure 6: Comparison between two exploration methods on Ms Pacman. Each heat map shows the trajectories of the controlled object measured over 2500 steps for the corresponding method.</p>
<p>steps yields a score very close to that of using real frames. Our two architectures produce much better scores than the two baselines for deep predictions than would be suggested based on the much smaller differences in squared error. The likely cause of this is that our models are better able to predict the movement of the controlled object relative to the baselines even though such an ability may not always lead to better squared error. In three out of the five games the score remains much better than the score of random play even when using 100 steps of prediction.</p>
<p>Improving DQN via Informed Exploration. To learn control in an RL domain, exploration of actions and states is necessary because without it the agent can get stuck in a bad sub-optimal policy. In DQN, the CNN-based agent was trained using an $\epsilon$-greedy policy in which the agent chooses either a greedy action or a random action by flipping a coin with probability of $\epsilon$. Such random exploration is a basic strategy that produces sufficient exploration, but can be slower than more informed exploration strategies. Thus, we propose an informed exploration strategy that follows the $\epsilon$-greedy policy, but chooses exploratory actions that lead to a frame that has been visited least often (in the last $d$ time steps), rather than random actions. Implementing this strategy requires a predictive model because the next frame for each possible action has to be considered.</p>
<p>The method works as follows. The most recent $d$ frames are stored in a trajectory memory, denoted $D=\left{\mathbf{x}^{(i)}\right}_{i=1}^{d}$. The predictive model is used to get the next frame $\mathbf{x}^{(a)}$ for every action $a$. We estimate the visit-frequency for every predicted frame by summing the similarity between the predicted frame and the most $d$ recent frames stored in the trajectory memory using a Gaussian kernel as follows:</p>
<p>$n_{D}\left(\mathbf{x}^{(\alpha)}\right)=\sum_{i=1}^{d}k\left(\mathbf{x}^{(\alpha)},\mathbf{x}^{(i)}\right) ; \quad k(\mathbf{x}, \mathbf{y})=\exp \left(-\sum_{j} \min \left(\max \left(\left(x_{j}-y_{j}\right)^{2}-\delta, 0\right), 1\right) / \sigma\right)$ (7)</p>
<p>where $\delta$ is a threshold, and $\sigma$ is a kernel bandwidth. The trajectory memory size is 200 for QBert and 20 for the other games, $\delta=0$ for Freeway and 50 for the others, and $\sigma=100$ for all games. For computational efficiency, we trained a new feedforward encoding network on $84 \times 84$ gray-scaled images as they are used as input for DQN. The details of the network architecture are provided in the supplementary material. Table 1 summarizes the results. The informed exploration improves DQN's performance using our predictive model in three of five games, with the most significant improvement in QBert. Figure 6 shows how the informed exploration strategy improves the initial experience of DQN.</p>
<h1>4.3 Analysis of Learned Representations</h1>
<p>Similarity among Action Representations. In the factored multiplicative interactions, every action is linearly transformed to $f$ factors ( $\mathbf{W}^{a} \mathbf{a}$ in Equation 4). In Figure 7 we present the cosine similarity between every pair of action-factors after training in Seaquest. ' N ' and ' F ' corresponds</p>
<p>to 'no-operation' and 'fire'. Arrows correspond to movements with (black) or without (white) 'fire'. There are positive correlations between actions that have the same movement directions (e.g., 'up' and 'up+fire'), and negative correlations between actions that have opposing directions. These results are reasonable and discovered automatically in learning good predictions.</p>
<p>Distinguishing Controlled and Uncontrolled Objects is itself a hard and interesting problem. Bellemare et al. [2] proposed a framework to learn contingent regions of an image affected by agent action, suggesting that contingency awareness is useful for model-free agents. We show that our architectures implicitly learn contingent regions as they learn to predict the entire image.</p>
<p>In our architectures, a factor $\left(f_{i}=\left(\mathbf{W}<em i="i">{i,:}^{a}\right)^{\top} \mathbf{a}\right)$ with higher variance measured over all possible actions, $\operatorname{Var}\left(f</em>}\right)=$ $\mathbb{E<em i="i">{\mathbf{a}}\left[\left(f</em>}-\mathbb{E<em i="i">{\mathbf{a}}\left[f</em>\right]$, is more likely to transform an image differently depending on actions, and so we assume such factors are responsible for transforming the parts of the image related to actions. We therefore collected the high variance (referred to as "highvar") factors from the model trained on Seaquest (around $40 \%$ of factors), and collected the remaining factors into a low variance ("lowvar") subset. Given an image and an action, we did two controlled forward propagations: giving only highvar factors (by setting the other factors to zeros) and vice versa. The results are visualized as 'Action' and 'Non-Action' in Figure 8. Interestingly, given only highvar-factors (Action), the model predicts sharply the movement of the object controlled by actions, while the other parts are mean pixel values. In contrast, given only lowvar-factors (Non-Action), the model predicts the movement of the other objects and the background (e.g., oxygen), and the controlled object stays at its previous location. This result implies that our model learns to distinguish between controlled objects and uncontrolled objects and transform them using disentangled representations (see $[25,24,37]$ for related work on disentangling factors of variation).}\right]\right)^{2</p>
<h1>5 Conclusion</h1>
<p>This paper introduced two different novel deep architectures that predict future frames that are dependent on actions and showed qualitatively and quantitatively that they are able to predict visuallyrealistic and useful-for-control frames over 100-step futures on several Atari game domains. To our knowledge, this is the first paper to show good deep predictions in Atari games. Since our architectures were domain independent we expect that they will generalize to many vision-based RL problems. In future work we will learn models that predict future reward in addition to predicting future frames and evaluate the performance of our architectures in model-based RL.
Acknowledgments. This work was supported by NSF grant IIS-1526059, Bosch Research, and ONR grant N00014-13-1-0762. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the views of the sponsors.</p>
<h2>References</h2>
<p>[1] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The Arcade Learning Environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013.
[2] M. G. Bellemare, J. Veness, and M. Bowling. Investigating contingency awareness using Atari 2600 games. In AAAI, 2012.
[3] M. G. Bellemare, J. Veness, and M. Bowling. Bayesian learning of recursively factored environments. In ICML, 2013.
[4] M. G. Bellemare, J. Veness, and E. Talvitie. Skip context tree switching. In ICML, 2014.
[5] Y. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1):1-127, 2009.
[6] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In ICML, 2009.
[7] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification. In CVPR, 2012.</p>
<p>[8] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. In CVPR, 2015.
[9] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.
[10] A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
[11] X. Guo, S. Singh, H. Lee, R. L. Lewis, and X. Wang. Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning. In NIPS, 2014.
[12] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780, 1997.
[13] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACM Multimedia, 2014.
[14] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei. Large-scale video classification with convolutional neural networks. In CVPR, 2014.
[15] L. Kocsis and C. Szepesvári. Bandit based Monte-Carlo planning. In ECML. 2006.
[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
[17] I. Lenz, R. Knepper, and A. Saxena. DeepMPC: Learning deep latent features for model predictive control. In RSS, 2015.
[18] R. Memisevic. Learning to relate images. IEEE TPAMI, 35(8):1829-1846, 2013.
[19] V. Michalski, R. Memisevic, and K. Konda. Modeling deep temporal dependencies with recurrent grammar cells. In NIPS, 2014.
[20] R. Mittelman, B. Kuipers, S. Savarese, and H. Lee. Structured recurrent temporal restricted Boltzmann machines. In ICML, 2014.
[21] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
[22] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
[23] V. Nair and G. E. Hinton. Rectified linear units improve restricted Boltzmann machines. In ICML, 2010.
[24] S. Reed, K. Sohn, Y. Zhang, and H. Lee. Learning to disentangle factors of variation with manifold interaction. In ICML, 2014.
[25] S. Rifai, Y. Bengio, A. Courville, P. Vincent, and M. Mirza. Disentangling factors of variation for facial expression recognition. In ECCV. 2012.
[26] J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85-117, 2015.
[27] J. Schmidhuber and R. Huber. Learning to generate artificial fovea trajectories for target detection. International Journal of Neural Systems, 2:125-134, 1991.
[28] N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsupervised learning of video representations using LSTMs. In ICML, 2015.
[29] I. Sutskever, G. E. Hinton, and G. W. Taylor. The recurrent temporal restricted Boltzmann machine. In NIPS, 2009.
[30] I. Sutskever, J. Martens, and G. E. Hinton. Generating text with recurrent neural networks. In ICML, 2011.
[31] I. Sutskever, O. Vinyals, and Q. Le. Sequence to sequence learning with neural networks. In NIPS, 2014.
[32] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014.
[33] G. W. Taylor and G. E. Hinton. Factored conditional restricted Boltzmann machines for modeling motion style. In ICML, 2009.
[34] T. Tieleman and G. Hinton. Lecture 6.5 - RMSProp: Divde the gradient by a running average of its recent magnitude. Coursera, 2012.
[35] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. Learning spatiotemporal features with 3D convolutional networks. In ICCV, 2015.
[36] C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
[37] J. Yang, S. Reed, M.-H. Yang, and H. Lee. Weakly-supervised disentangling with recurrent transformations for 3D view synthesis. In NIPS, 2015.</p>
<h1>A Network Architectures and Training Details</h1>
<p>The network architectures of the proposed models and the baselines are illustrated in Figure 9.
The weight of LSTM is initialized from a uniform distribution of $[-0.08,0.08]$. The weight of the fullyconnected layer from the encoded feature to the factored layer and from the action to the factored layer are initialized from a uniform distribution of $[-1,1]$ and $[-0.1,0.1]$ respectively.
The total number of iterations is $1.5 \times 10^{6}, 10^{6}$, and $10^{6}$ for each training phase (1-step, 3-step, and 5-step). The learning rate is multiplied by 0.9 after every $10^{5}$ iterations.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Network architectures. ' $\times$ ' indicates element-wise multiplication. The text in each (de-)convolution layer describes the number of filters, the size of the kernel, padding (height and width), and stride.</p>
<h1>B Informed Exploration</h1>
<p>The entire DQN algorithm with informed exploration is described in Algorithm 1.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Deep</span><span class="w"> </span><span class="nx">Q</span><span class="o">-</span><span class="nx">learning</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">informed</span><span class="w"> </span><span class="nx">exploration</span>
<span class="w">    </span><span class="nx">Allocate</span><span class="w"> </span><span class="nx">capacity</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">replay</span><span class="w"> </span><span class="nx">memory</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">R</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Allocate</span><span class="w"> </span><span class="nx">capacity</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">trajectory</span><span class="w"> </span><span class="nx">memory</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">parameters</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">DQN</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="nx">steps</span><span class="w"> </span><span class="err">\</span><span class="p">(&lt;</span><span class="nx">M</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Reset</span><span class="w"> </span><span class="nx">game</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="nx">image</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Store</span><span class="w"> </span><span class="nx">image</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="p">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">T</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">            </span><span class="nx">Sample</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">c</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">Bernoulli</span><span class="w"> </span><span class="nx">distribution</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">parameter</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">epsilon</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Set</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="w"> </span><span class="err">\</span><span class="nx">begin</span><span class="p">{</span><span class="nx">cases</span><span class="p">}</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">argmin</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="w"> </span><span class="nx">n_</span><span class="p">{</span><span class="nx">D</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="err">\</span><span class="nx">alpha</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="nx">c</span><span class="p">=</span><span class="mi">1</span><span class="w"> </span><span class="err">\\</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">argmax</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="w"> </span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">phi</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">),</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="err">\</span><span class="nx">theta</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nx">otherwise</span><span class="w"> </span><span class="p">}</span><span class="err">\</span><span class="nx">end</span><span class="p">{</span><span class="nx">cases</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Choose</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">image</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Set</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}=</span><span class="nx">x_</span><span class="p">{</span><span class="nx">t</span><span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="w"> </span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">preprocess</span><span class="w"> </span><span class="nx">images</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">phi_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}=</span><span class="err">\</span><span class="nx">phi</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Store</span><span class="w"> </span><span class="nx">image</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Store</span><span class="w"> </span><span class="nx">transition</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">phi_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">phi_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">R</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Sample</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">mini</span><span class="o">-</span><span class="nx">batch</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">transitions</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">phi_</span><span class="p">{</span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">phi_</span><span class="p">{</span><span class="nx">j</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">R</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Update</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">based</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">mini</span><span class="o">-</span><span class="nx">batch</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">Bellman</span><span class="w"> </span><span class="nx">equation</span>
<span class="w">            </span><span class="nx">steps</span><span class="w"> </span><span class="err">\</span><span class="p">(=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">steps</span><span class="w"> </span><span class="o">+</span><span class="mi">1</span>
<span class="w">            </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">while</span>
</code></pre></div>

<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Feedforward encoding network for gray-scaled and down-sampled images.</p>
<p>Predictive Model for Informed Exploration. A feedforward encoding network (illustrated in Figure 10) trained on down-sampled and gray-scaled images is used for computational efficiency. We trained the model on 1-step prediction objective with learning rate of $10^{-4}$ and batch size of 32 . The pixel values are subtracted by mean pixel values and divided by 128. RMSProp is used with momentum of 0.9 , (squared) gradient momentum of 0.95 , and min squared gradient of 0.01 .
Comparison to Random Exploration. Figure 11 visualizes the difference between random exploration and informed exploration in two games. In Freeway, where the agent gets rewards by reaching the top lane, the agent moves only around the bottom area in the random exploration, which results in $4.6 \times 10^{5}$ steps to get the first reward. On the other hand, the agent moves around all locations in the informed exploration and receives the first reward in 86 steps. The similar result is found in Ms Pacman.
Application to Deep Q-learning. The results of the informed exploration using the game emulator and our predictive model are reported in Figure 12 and Table 2. Our DQN replication follows [21], which uses a smaller CNN than [22].
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Comparison between two exploration methods on Freeway (Left) and Ms Pacman (Right). Each heat map shows the trajectories of the agent measured from 2500 steps from each exploration strategy.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Learning curves of DQNs with standard errors. The red and blue curves are informed exploration using our predictive model and the emulator respectively. The black curves are DQNs with random exploration. The average game score is measured from 100 game plays with $\epsilon$-greedy policy with $\epsilon=0.05$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Seaquest</th>
<th style="text-align: center;">S. Invaders</th>
<th style="text-align: center;">Freeway</th>
<th style="text-align: center;">QBert</th>
<th style="text-align: center;">Ms Pacman</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DQN (Nature) [22]</td>
<td style="text-align: center;">5286</td>
<td style="text-align: center;">1976</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">10596</td>
<td style="text-align: center;">2311</td>
</tr>
<tr>
<td style="text-align: left;">DQN (NIPS) [21]</td>
<td style="text-align: center;">1705</td>
<td style="text-align: center;">581</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1952</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Our replication of [21]</td>
<td style="text-align: center;">13119 (538)</td>
<td style="text-align: center;">698 (20)</td>
<td style="text-align: center;">$30.9(0.2)$</td>
<td style="text-align: center;">3876 (106)</td>
<td style="text-align: center;">2281 (53)</td>
</tr>
<tr>
<td style="text-align: left;">LE (Prediction)</td>
<td style="text-align: center;">13265 (577)</td>
<td style="text-align: center;">681 (23)</td>
<td style="text-align: center;">$32.2(0.2)$</td>
<td style="text-align: center;">8238 (498)</td>
<td style="text-align: center;">2522 (57)</td>
</tr>
<tr>
<td style="text-align: left;">LE (Emulator)</td>
<td style="text-align: center;">13002 (498)</td>
<td style="text-align: center;">708 (17)</td>
<td style="text-align: center;">$32.2(0.2)$</td>
<td style="text-align: center;">7969 (496)</td>
<td style="text-align: center;">2702 (92)</td>
</tr>
</tbody>
</table>
<p>Table 2: Average game score with standard error. 'LE' indicates DQN combined with the informed exploration method. 'Emulator' and 'Prediction' correspond to the emulator and our predictive model for computing $\mathbf{x}_{t}^{(n)}$.</p>
<h1>C Correlation between Actions</h1>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Correlations between actions. The brightness represents consine similarity between pairs of factors.</p>
<h1>D Handling Different Actions</h1>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" />
(b) Freeway
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p><img alt="img-16.jpeg" src="img-16.jpeg" />
(a) Ms Pacman</p>
<p>Figure 12: Predictions given different actions</p>
<h1>E Prediction Video</h1>
<p><img alt="img-17.jpeg" src="img-17.jpeg" />
(a) Seaquest ( $1 \sim 7$ steps). Our models predict the movement of the enemies and the yellow submarine which is controlled by actions. 'naFf' predicts only the movement of other objects correctly, and the submarine disappears after a few steps. 'MLP' does not predict any objects but generate only the mean pixel image.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Step</th>
<th style="text-align: center;">MLP</th>
<th style="text-align: center;">naFf</th>
<th style="text-align: center;">Feedforward</th>
<th style="text-align: center;">Recurrent</th>
<th style="text-align: center;">Ground Truth</th>
<th style="text-align: center;">Action</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">174</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\rightarrow$ fire</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">175</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">fire</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">fire</td>
</tr>
<tr>
<td style="text-align: center;">176</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">fire</td>
</tr>
<tr>
<td style="text-align: center;">177</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">fire</td>
</tr>
<tr>
<td style="text-align: center;">178</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">fire</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">179</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">fire</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">fire</td>
</tr>
<tr>
<td style="text-align: center;">180</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(a) Seaquest ( $174 \sim 180$ steps). The proposed models predict the location of the controlled object accurately over 180 -step predictions. They generate new objects such as fishes and human divers. Although the generated objects do not match the ground-truth images, their shapes and colors are realistic.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Step</th>
<th style="text-align: center;">MLP</th>
<th style="text-align: center;">naFf</th>
<th style="text-align: center;">Feedforward</th>
<th style="text-align: center;">Recurrent</th>
<th style="text-align: center;">Ground Truth</th>
<th style="text-align: center;">Action</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\rightarrow$ <br> fire</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">no-op</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">no-op</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">no-op</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">no-op</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">no-op</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">fire</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">fire</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(a) Space Invaders ( $1 \sim 7$ steps). The enemies in the center move and change their shapes from step 6 to step 7. This movement is predicted by the proposed models and 'naFf', while the predictions from 'MLP' are almost same as the last input frame.</p>
<p><img alt="img-18.jpeg" src="img-18.jpeg" />
(a) Space Invaders ( $130 \sim 136$ steps). Although our models make errors in the long run, the generated images are still realistic in that the objects are reasonably arranged and moving in the right directions. On the other hand, the frames predicted by 'MLP' and 'naFf' are almost same as the last input frame.</p>
<p><img alt="img-19.jpeg" src="img-19.jpeg" />
(a) Freeway ( $1 \sim 7$ steps). The proposed models predict the movement of the controlled object correctly depending on different actions, while 'naFf' fails to handle different actions. 'MLP' generates blurry objects that are not realistic.</p>
<p><img alt="img-20.jpeg" src="img-20.jpeg" />
(a) Freeway ( $290 \sim 296$ steps). The feedforward network diverges at 294 -step as the agent starts a new stage from the bottom lane. This is due to the fact that actions are ignored for 9 -steps when a new stage begins, which is not successfully handled by the feedforward network.</p>            </div>
        </div>

    </div>
</body>
</html>
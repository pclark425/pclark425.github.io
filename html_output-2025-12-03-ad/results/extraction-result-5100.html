<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5100 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5100</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5100</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-484f8c2a19c0dbe4223cc7f77275cc4ca97b8ef8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/484f8c2a19c0dbe4223cc7f77275cc4ca97b8ef8" target="_blank">Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner</a></p>
                <p><strong>Paper Venue:</strong> NAACL-HLT</p>
                <p><strong>Paper TL;DR:</strong> This work proposes an architecture called Iterative Retrieval-Generation Reasoner (IRGR), able to explain a given hypothesis by systematically generating a step-by-step explanation from textual premises, allowing the model to leverage intermediate conclusions, and mitigating the input size limit of baseline encoder-decoder models.</p>
                <p><strong>Paper Abstract:</strong> Large language models have achieved high performance on various question answering (QA) benchmarks, but the explainability of their output remains elusive. Structured explanations, called entailment trees, were recently suggested as a way to explain and inspect a QA system's answer. In order to better generate such entailment trees, we propose an architecture called Iterative Retrieval-Generation Reasoner (IRGR). Our model is able to explain a given hypothesis by systematically generating a step-by-step explanation from textual premises. The IRGR model iteratively searches for suitable premises, constructing a single entailment step at a time. Contrary to previous approaches, our method combines generation steps and retrieval of premises, allowing the model to leverage intermediate conclusions, and mitigating the input size limit of baseline encoder-decoder models. We conduct experiments using the EntailmentBank dataset, where we outperform existing benchmarks on premise retrieval and entailment tree generation, with around 300% gain in overall correctness.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5100.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5100.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRGR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Retrieval-Generation Reasoner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An architecture that interleaves dense retrieval and sequence-to-sequence generation to construct multi-step natural language entailment trees by generating one entailment step at a time and conditioning retrieval on intermediate conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Text-to-Text Transformer (T5) used as the generator in IRGR; the paper uses the HuggingFace T5-large implementation as the sequence-to-sequence language model that outputs a single entailment step per iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ENTAILMENTBank entailment tree generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate structured entailment trees (multi-premise textual entailment steps) that explain a hypothesis from a corpus of simple textual premises; requires strict multi-step natural-language logical reasoning (deriving intermediate conclusions and composing them to prove the hypothesis).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Iterative retrieval-generation: at each iteration a dense retriever fetches a small set of premises conditioned on the hypothesis and previously generated intermediate steps; a T5 sequence-to-sequence generator produces one entailment step (premises -> conclusion); used conditional retrieval (Algorithm 1), siamese sentence encoder fine-tuned with graded positives (lambda weighting), and removal of used premises between iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Entailment tree generation (test set, Table 2): Leaves F1=45.6 (All-Correct=11.8), Steps F1=16.1 (All-Correct=11.4), Intermediates F1=38.8 (All-Correct=20.9), Overall All-Correct=11.5. Retrieval (Table 1, R@25 / All-Correct): IRGR-retriever (sing.) R@25=64.41 / All-Correct=40.29; IRGR-retriever (cond.) R@25=68.28 / All-Correct=44.70; IRGR-retriever* (variable k >25) All-Correct=51.47. The paper reports an overall >300% relative improvement on the strict Overall All-Correct metric compared to the EntailmentWriter baseline (baseline Overall All-Correct 2.9 -> IRGR 11.5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails increasingly on larger/deeper trees: cannot perfectly predict trees with >4 steps and (for the retrieval task) cannot successfully generate trees with 3+ steps under the strict All-Correct metric. Error analysis: 56% of entailment-step errors are invalid steps (conclusion does not logically follow), 27% misevaluation/irrelevance, 17% repetition (conclusion copies premises). Entailment-tree level errors: 52% missing/incorrect leaves, 32% invalid or skipped steps, 12% imperfect evaluation (valid but different from gold), 4% disconnected/degenerate trees. Retrieval struggles more for leaf premises deeper in the tree and with low lexical overlap to hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared to EntailmentWriter baseline (one-shot encoder-decoder that uses a fixed retrieved set): IRGR obtains much higher strict metrics despite using a smaller generator model; IRGR-retriever (cond.) improves over EntailmentWriter's retriever by +14.2 percentage points R@25 (59.76 -> 68.28) and +28.8 percentage points All-Correct (34.70 -> 44.70) in Table 1. Overall All-Correct rises from 2.9 (EntailmentWriter with T5-11B reported) to 11.5 (IRGR with T5-large). The paper emphasizes IRGR achieves better explanations while using a model with about an order of magnitude fewer parameters than the EntailmentWriter's T5-11B.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablations show iterative retrieval and conditional retrieval are critical: 'w/o iter.' (no iterative retrieval) reduces Overall All-Correct from 11.5 to 8.2; 'w/o iter. & cond.' reduces Overall All-Correct to 3.2. Conditional retrieval parameters: omega (split point) tuned to 15, labeling weight lambda tuned (lambda=0.75 best). Retriever analysis: true positives had 28.5% more unigram overlap and 68.6% more bigram overlap with hypotheses than false negatives; average depth of true-positive leaf nodes 2.3 vs 3.0 for false negatives, indicating deeper leaves are harder to retrieve.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5100.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5100.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EntailmentWriter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EntailmentWriter (baseline encoder-decoder approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior one-shot encoder-decoder approach that linearizes entailment trees and generates them conditioned on a fixed set of retrieved premises; used as the main baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ProofWriter: Generating implications, proofs, and abductive statements over natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5 encoder-decoder model used in the EntailmentWriter baseline (the paper cites EntailmentWriter results using T5-11B); EntailmentWriter uses a fixed retrieved context and generates a linearized tree in a single pass.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ENTAILMENTBank entailment tree generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>One-shot generation of linearized entailment trees from a (fixed) set of retrieved premises; aims to produce multi-premise textual entailment steps that together explain a hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Encoder-decoder generation in a single pass using a fixed set of retrieved premises; retrieval pipeline in EntailmentWriter includes a RoBERTa-based classifier and Tensorflow-Ranking-BERT re-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported baseline results (Tables 1 & 2): Retrieval R@25=59.76, All-Correct=34.70 (Table 1). Entailment tree generation (Table 2, using T5-11B as reported): Leaves F1=39.9 (All-Correct=3.8), Steps F1=7.4 (All-Correct=2.9), Intermediates F1=35.9 (All-Correct=7.1), Overall All-Correct=2.9.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limited by fixed input size (must retrieve all supporting facts before generation) and inability to use intermediate conclusions to guide retrieval; sensitive to missing retrieved premises and tends to produce incorrect proofs if retrieval fails; low strict All-Correct scores despite large model size.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>IRGR outperforms EntailmentWriter on retrieval and generation strict metrics while using a smaller generator (IRGR's reported Overall All-Correct 11.5 vs EntailmentWriter 2.9). EntailmentWriter's retrieval pipeline (RoBERTa + re-ranker) is outperformed by IRGR's dense conditional retriever (cond. R@25 68.28 vs 59.76).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>The paper compares EntailmentWriter's one-shot setup to iterative methods; when both models use the same number of parameters (T5-large reported in original sources), generation without retrieval is roughly comparable, but iterative retrieval + generation provides gains when retrieval is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5100.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5100.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRGR-retriever (all-mpnet-base-v2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IRGR-retriever with fine-tuned all-mpnet-base-v2 (Sentence-Transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A siamese dense passage retriever fine-tuned to retrieve premises conditioned on hypothesis and previously generated steps; used to supply small, focused sets of premises per generation iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mpnet: Masked and permuted pretraining for language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>all-mpnet-base-v2 (Sentence-Transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MPNet-based sentence embedding model (used in Sentence-Transformers) configured as a Siamese encoder and fine-tuned with contrastive (cosine) loss; encodes hypotheses concatenated with previous entailment steps to form retrieval queries and ranks premises by inner product / cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Premise retrieval for entailment tree construction (ENTAILMENTBank)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Retrieve relevant leaf premises from a corpus (~11K premises) to support constructing multi-step entailment proofs; task requires retrieving sometimes lexically-distant premises that are deep in the gold proof.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tune a Siamese sentence encoder with positive and hard negative pairs; use graded positive labels (lambda weighting) to prioritize premises closer to current entailment step; conditional retrieval algorithm (Algorithm 1) that mixes queries from the hypothesis and previously retrieved premises (parameter omega), and iterative retrieval that conditions on generated intermediate conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Retrieval (Table 1): IRGR-retriever (sing.) R@25=64.41 / All-Correct=40.29; IRGR-retriever (cond.) R@25=68.28 / All-Correct=44.70; IRGR-retriever* (variable k >25) All-Correct=51.47. Compared to BM25 (R@25=45.01 / All-Correct=22.35) and EntailmentWriter retriever (R@25=59.76 / All-Correct=34.70), IRGR-retriever shows substantial gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Still struggles to retrieve premises that are lexically distant from the hypothesis: true positives had 28.5% more unigram overlap and 68.6% more bigram overlap with hypotheses than false negatives. Average depth of true-positive leaf nodes was 2.3 vs 3.0 for false negatives, indicating deeper leaves are harder to retrieve. Initial retrieval (when no intermediate steps exist) is especially challenging; conditional retrieval mitigates but does not eliminate this.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms traditional BM25 and the EntailmentWriter retrieval pipeline (RoBERTa + re-ranking). Conditional retrieval (using previously retrieved premises to condition later retrievals) improves recall and All-Correct relative to single-query retrieval. Using golden intermediate nodes with iterative retriever provides an upper bound for generator performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Explaining answers with entailment trees <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Dense passage retrieval for open-domain question answering <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 1)</em></li>
                <li>SentenceBERT: Sentence embeddings using Siamese BERT-networks <em>(Rating: 2)</em></li>
                <li>WorldTree v2: A corpus of science-domain structured explanations and inference patterns supporting multi-hop inference <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5100",
    "paper_id": "paper-484f8c2a19c0dbe4223cc7f77275cc4ca97b8ef8",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "IRGR",
            "name_full": "Iterative Retrieval-Generation Reasoner",
            "brief_description": "An architecture that interleaves dense retrieval and sequence-to-sequence generation to construct multi-step natural language entailment trees by generating one entailment step at a time and conditioning retrieval on intermediate conclusions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-large",
            "model_description": "Encoder-decoder Text-to-Text Transformer (T5) used as the generator in IRGR; the paper uses the HuggingFace T5-large implementation as the sequence-to-sequence language model that outputs a single entailment step per iteration.",
            "model_size": null,
            "logical_reasoning_task": "ENTAILMENTBank entailment tree generation",
            "task_description": "Generate structured entailment trees (multi-premise textual entailment steps) that explain a hypothesis from a corpus of simple textual premises; requires strict multi-step natural-language logical reasoning (deriving intermediate conclusions and composing them to prove the hypothesis).",
            "method_or_approach": "Iterative retrieval-generation: at each iteration a dense retriever fetches a small set of premises conditioned on the hypothesis and previously generated intermediate steps; a T5 sequence-to-sequence generator produces one entailment step (premises -&gt; conclusion); used conditional retrieval (Algorithm 1), siamese sentence encoder fine-tuned with graded positives (lambda weighting), and removal of used premises between iterations.",
            "performance": "Entailment tree generation (test set, Table 2): Leaves F1=45.6 (All-Correct=11.8), Steps F1=16.1 (All-Correct=11.4), Intermediates F1=38.8 (All-Correct=20.9), Overall All-Correct=11.5. Retrieval (Table 1, R@25 / All-Correct): IRGR-retriever (sing.) R@25=64.41 / All-Correct=40.29; IRGR-retriever (cond.) R@25=68.28 / All-Correct=44.70; IRGR-retriever* (variable k &gt;25) All-Correct=51.47. The paper reports an overall &gt;300% relative improvement on the strict Overall All-Correct metric compared to the EntailmentWriter baseline (baseline Overall All-Correct 2.9 -&gt; IRGR 11.5).",
            "limitations_or_failure_cases": "Fails increasingly on larger/deeper trees: cannot perfectly predict trees with &gt;4 steps and (for the retrieval task) cannot successfully generate trees with 3+ steps under the strict All-Correct metric. Error analysis: 56% of entailment-step errors are invalid steps (conclusion does not logically follow), 27% misevaluation/irrelevance, 17% repetition (conclusion copies premises). Entailment-tree level errors: 52% missing/incorrect leaves, 32% invalid or skipped steps, 12% imperfect evaluation (valid but different from gold), 4% disconnected/degenerate trees. Retrieval struggles more for leaf premises deeper in the tree and with low lexical overlap to hypothesis.",
            "comparison": "Compared to EntailmentWriter baseline (one-shot encoder-decoder that uses a fixed retrieved set): IRGR obtains much higher strict metrics despite using a smaller generator model; IRGR-retriever (cond.) improves over EntailmentWriter's retriever by +14.2 percentage points R@25 (59.76 -&gt; 68.28) and +28.8 percentage points All-Correct (34.70 -&gt; 44.70) in Table 1. Overall All-Correct rises from 2.9 (EntailmentWriter with T5-11B reported) to 11.5 (IRGR with T5-large). The paper emphasizes IRGR achieves better explanations while using a model with about an order of magnitude fewer parameters than the EntailmentWriter's T5-11B.",
            "ablation_or_analysis_results": "Ablations show iterative retrieval and conditional retrieval are critical: 'w/o iter.' (no iterative retrieval) reduces Overall All-Correct from 11.5 to 8.2; 'w/o iter. & cond.' reduces Overall All-Correct to 3.2. Conditional retrieval parameters: omega (split point) tuned to 15, labeling weight lambda tuned (lambda=0.75 best). Retriever analysis: true positives had 28.5% more unigram overlap and 68.6% more bigram overlap with hypotheses than false negatives; average depth of true-positive leaf nodes 2.3 vs 3.0 for false negatives, indicating deeper leaves are harder to retrieve.",
            "uuid": "e5100.0",
            "source_info": {
                "paper_title": "Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "EntailmentWriter",
            "name_full": "EntailmentWriter (baseline encoder-decoder approach)",
            "brief_description": "A prior one-shot encoder-decoder approach that linearizes entailment trees and generates them conditioned on a fixed set of retrieved premises; used as the main baseline in comparisons.",
            "citation_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language.",
            "mention_or_use": "use",
            "model_name": "T5-11B",
            "model_description": "T5 encoder-decoder model used in the EntailmentWriter baseline (the paper cites EntailmentWriter results using T5-11B); EntailmentWriter uses a fixed retrieved context and generates a linearized tree in a single pass.",
            "model_size": "11B",
            "logical_reasoning_task": "ENTAILMENTBank entailment tree generation",
            "task_description": "One-shot generation of linearized entailment trees from a (fixed) set of retrieved premises; aims to produce multi-premise textual entailment steps that together explain a hypothesis.",
            "method_or_approach": "Encoder-decoder generation in a single pass using a fixed set of retrieved premises; retrieval pipeline in EntailmentWriter includes a RoBERTa-based classifier and Tensorflow-Ranking-BERT re-ranking.",
            "performance": "Reported baseline results (Tables 1 & 2): Retrieval R@25=59.76, All-Correct=34.70 (Table 1). Entailment tree generation (Table 2, using T5-11B as reported): Leaves F1=39.9 (All-Correct=3.8), Steps F1=7.4 (All-Correct=2.9), Intermediates F1=35.9 (All-Correct=7.1), Overall All-Correct=2.9.",
            "limitations_or_failure_cases": "Limited by fixed input size (must retrieve all supporting facts before generation) and inability to use intermediate conclusions to guide retrieval; sensitive to missing retrieved premises and tends to produce incorrect proofs if retrieval fails; low strict All-Correct scores despite large model size.",
            "comparison": "IRGR outperforms EntailmentWriter on retrieval and generation strict metrics while using a smaller generator (IRGR's reported Overall All-Correct 11.5 vs EntailmentWriter 2.9). EntailmentWriter's retrieval pipeline (RoBERTa + re-ranker) is outperformed by IRGR's dense conditional retriever (cond. R@25 68.28 vs 59.76).",
            "ablation_or_analysis_results": "The paper compares EntailmentWriter's one-shot setup to iterative methods; when both models use the same number of parameters (T5-large reported in original sources), generation without retrieval is roughly comparable, but iterative retrieval + generation provides gains when retrieval is imperfect.",
            "uuid": "e5100.1",
            "source_info": {
                "paper_title": "Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "IRGR-retriever (all-mpnet-base-v2)",
            "name_full": "IRGR-retriever with fine-tuned all-mpnet-base-v2 (Sentence-Transformers)",
            "brief_description": "A siamese dense passage retriever fine-tuned to retrieve premises conditioned on hypothesis and previously generated steps; used to supply small, focused sets of premises per generation iteration.",
            "citation_title": "Mpnet: Masked and permuted pretraining for language understanding.",
            "mention_or_use": "use",
            "model_name": "all-mpnet-base-v2 (Sentence-Transformers)",
            "model_description": "MPNet-based sentence embedding model (used in Sentence-Transformers) configured as a Siamese encoder and fine-tuned with contrastive (cosine) loss; encodes hypotheses concatenated with previous entailment steps to form retrieval queries and ranks premises by inner product / cosine similarity.",
            "model_size": null,
            "logical_reasoning_task": "Premise retrieval for entailment tree construction (ENTAILMENTBank)",
            "task_description": "Retrieve relevant leaf premises from a corpus (~11K premises) to support constructing multi-step entailment proofs; task requires retrieving sometimes lexically-distant premises that are deep in the gold proof.",
            "method_or_approach": "Fine-tune a Siamese sentence encoder with positive and hard negative pairs; use graded positive labels (lambda weighting) to prioritize premises closer to current entailment step; conditional retrieval algorithm (Algorithm 1) that mixes queries from the hypothesis and previously retrieved premises (parameter omega), and iterative retrieval that conditions on generated intermediate conclusions.",
            "performance": "Retrieval (Table 1): IRGR-retriever (sing.) R@25=64.41 / All-Correct=40.29; IRGR-retriever (cond.) R@25=68.28 / All-Correct=44.70; IRGR-retriever* (variable k &gt;25) All-Correct=51.47. Compared to BM25 (R@25=45.01 / All-Correct=22.35) and EntailmentWriter retriever (R@25=59.76 / All-Correct=34.70), IRGR-retriever shows substantial gains.",
            "limitations_or_failure_cases": "Still struggles to retrieve premises that are lexically distant from the hypothesis: true positives had 28.5% more unigram overlap and 68.6% more bigram overlap with hypotheses than false negatives. Average depth of true-positive leaf nodes was 2.3 vs 3.0 for false negatives, indicating deeper leaves are harder to retrieve. Initial retrieval (when no intermediate steps exist) is especially challenging; conditional retrieval mitigates but does not eliminate this.",
            "comparison": "Outperforms traditional BM25 and the EntailmentWriter retrieval pipeline (RoBERTa + re-ranking). Conditional retrieval (using previously retrieved premises to condition later retrievals) improves recall and All-Correct relative to single-query retrieval. Using golden intermediate nodes with iterative retriever provides an upper bound for generator performance.",
            "uuid": "e5100.2",
            "source_info": {
                "paper_title": "Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Explaining answers with entailment trees",
            "rating": 2
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2
        },
        {
            "paper_title": "Dense passage retrieval for open-domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 1
        },
        {
            "paper_title": "SentenceBERT: Sentence embeddings using Siamese BERT-networks",
            "rating": 2
        },
        {
            "paper_title": "WorldTree v2: A corpus of science-domain structured explanations and inference patterns supporting multi-hop inference",
            "rating": 2
        }
    ],
    "cost": 0.01646025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner</h1>
<p>Danilo Ribeiro ${ }^{1,2, \text { * }}$ Shen Wang ${ }^{2}$, Xiaofei Ma ${ }^{2}$, Rui Dong ${ }^{2}$, Xiaokai Wei ${ }^{2}$, Henry Zhu ${ }^{2}$,<br>Xinchi Chen ${ }^{2}$, Zhiheng Huang ${ }^{2}$, Peng Xu ${ }^{2}$, Andrew Arnold ${ }^{2}$, Dan Roth ${ }^{2}$,<br>${ }^{1}$ Northwestern University, ${ }^{2}$ AWS AI Labs<br>{dnribeiro}@u.northwestern.edu,<br>{shenwa, xiaofeim, ruidong, xiaokaiw, henghui}@amazon.com,<br>{xcc, zhiheng, pengx, anarnld, drot}@amazon.com</p>
<h4>Abstract</h4>
<p>Large language models have achieved high performance on various question answering (QA) benchmarks, but the explainability of their output remains elusive. Structured explanations, called entailment trees, were recently suggested as a way to explain and inspect a QA system's answer. In order to better generate such entailment trees, we propose an architecture called Iterative RetrievalGeneration Reasoner (IRGR). Our model is able to explain a given hypothesis by systematically generating a step-by-step explanation from textual premises. The IRGR model iteratively searches for suitable premises, constructing a single entailment step at a time. Contrary to previous approaches, our method combines generation steps and retrieval of premises, allowing the model to leverage intermediate conclusions, and mitigating the input size limit of baseline encoder-decoder models. We conduct experiments using the EntailMENTBANK dataset, where we outperform existing benchmarks on premise retrieval and entailment tree generation, with around $300 \%$ gain in overall correctness.</p>
<h2>1 Introduction</h2>
<p>Large neural network models have successfully been applied to different natural language tasks, achieving state-of-the-art results in many natural language benchmarks. Despite this success, these results came with the expense of AI systems becoming less interpretable (Jain and Wallace, 2019; Rajani et al., 2019a).</p>
<p>With the desire to make the output of such models less opaque, we propose a question answering (QA) system that is able to explain their decisions not only by retrieving supporting textual evidence (rationales), but by showing how the answer to a</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Task has as input a hypothesis H (e.g. an answer to a question) and a corpus of premises C (simple textual evidences), the goal is to generate an entailment tree that explains the hypothesis H by using premises from C.
question can be systematically proven from simpler textual premises (natural language reasoning). These explanations are represented using entailment trees, as depicted in Figure 1. First introduced by Dalvi et al. (2021), entailment trees represents a chain of reasoning that shows how a hypothesis (or an answer to a question) can be explained from simpler textual evidence. In comparison, other explanation approaches such as retrieval of passages (rationales) (DeYoung et al., 2020) or multi-hop reasoning (chaining) (Jhamtani and Clark, 2020) are less expressive than entailment trees, which are comprised of multi-premise textual entailment steps.</p>
<p>In order to generate such entailment trees, previous works (Tafjord et al., 2021; Dalvi et al., 2021; Bostrom et al., 2021) have used encoder-decoder models that takes as input a small set of retrieved premises and output a linearized representation of the entailment tree. Such models are limited by (1) the language model's fixed input size, and they</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparison among different natural language explanation approaches. The images show (A) plain textual explanations (B) retrieval of evidence passages (C) multi-hop explanations (D) entailment trees. The approach in (D) allows for more detailed inspection of the reasoning behind an explanation. Nodes in gray are retrieved from a corpus, nodes in blue are generated, and the red node is the hypothesis or answer that is being explained.</p>
<p>may construct incorrect proofs when the retrieval module cannot fetch all relevant premises at once (2) such approaches do not leverage the partially generated entailment trees. In contrast, we propose <em>Iterative Retrieval-Generation Reasoner</em> (IRGR), a novel architecture that iteratively searches for suitable premises, constructing a single entailment step at a time. At every generation step, the model searches for a distinct set of premises that will support the generation of a single step, therefore mitigating the language model's input size limit and improving generation correctness.</p>
<p>Our contributions are two-fold. First, we design a retrieval method that is able to better identify premises needed to generate a chain of reasoning, which explains a given hypothesis. Our retrieval method outperforms previous baselines by 48.3%, while allowing for a dynamic set of premises to be retrieved. Secondly, we propose an iterative retrieval-generation architecture that constructs partial proofs and augments the retrieval probes using intermediate generation results. We show that integrating the retrieval module with iterative generation can significantly improve explanations. Our proposed approach achieves new state-of-the-art result on entailment tree generation with over 306% better results on the All-Correct metric (strict comparison with golden data), while using a model with one order of magnitude fewer parameters.</p>
<h2>2 Related Work</h2>
<p>Traditionally, natural language processing (NLP) frameworks were based on white-box methods such as rule-based systems (Allen, 1988; Ribeiro et al., 2019; Ribeiro and Forbus, 2021) and decision trees (Boros et al., 2017), which were inherently inspectable (Danilevsky et al., 2020). More recently, large deep learning language models (black-box methods) have gained popularity (Song et al., 2020; Raffel et al., 2020), but their improvements in result quality came with a cost: the system's outputs lack explainability and inspectability.</p>
<p>There have been many attempts to mitigate this issue, including input perturbation (Ribeiro et al., 2018) and premises selection (DeYoung et al., 2020). One promising explanation approach is to combine the model's output with a human-interpretable explanation. For instance, Camburu et al. (2018) introduced the concept of natural language explanation in their e-SNLI dataset while Rajani et al. (2019b) expanded this idea to commonsense explanations. Jhamtani and Clark (2020) further explored the notion of explanation in multihop QA, where explanations contain a <em>chain of reasoning</em>, instead of simple textual explanations. Different from these explanation approaches, our work generates explanations in the form of <em>entailment trees</em>, introduced by Dalvi et al. (2021), which are composed of multi-premise textual entailment steps. Entailment trees are more detailed explanations, making it easier to inspect the reasoning behind the model's answer. Figure 2 shows a diagram comparing different natural language explanation methods according to their structure and use of textual evidence.</p>
<p>The first approach used to generate entailment trees was based on the <em>EntailmentWriter</em> model by Tafjord et al. (2021). However, their approach is limited by the input size of the encoder-decoder language models, where a fixed set of supporting facts is used to generate an explanation. Instead, our model iteratively fetches a set of premises using dense retrieval conditioned on previous entailment steps, allowing for more precise explanations.</p>
<p>Our work is also related to some recent approaches that combine retrieval and neural networks for QA tasks (Karpukhin et al., 2020; Guu et al., 2020). The work of Lewis et al. (2020) com</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: IRGR is composed of two modules, IRGR-retriever and IRGR-generator. The IRGR-retriever iteratively fetches a set of premises from a corpus <em>C</em> in order to generate an entailment tree (structured explanation for a given hypothesis). The IRGR-generator computes a single entailment step at a time, and the intermediate generated steps are stored and used for subsequent retrieval and generation.</p>
<p>bined dense retrieval with encoder-decoder models, where a different set of passages were retrieved for each generated character. Conditioning the retrieval of a passage on previously retrieved passages has been explored in the context of multi-hop QA (Zhao et al., 2021; Xiong et al., 2021), and multi-hop explanations (Valentino et al., 2021; Cartuyvels et al., 2020). However, these approaches either are not used to generate explanations or do not use inferred intermediate reasoning steps to retrieve premises.</p>
<h1>3 Approach</h1>
<h2>3.1 Problem Definition</h2>
<p>The problem input consists of a <strong>corpus of premises</strong> <em>C</em> (simple textual statements) and a <strong>hypothesis</strong> <em>h</em>. The objective is to generate an <strong>entailment tree</strong> <em>T</em> that explains the hypothesis <em>h</em> by using a subset of the premises in <em>C</em> as building blocks. Entailment trees are represented as a tuple <em>T</em> = (<em>h</em>, <em>L</em>, <em>E</em>, <em>S</em>), where <strong>leaf nodes</strong> <em>l<sub>i</sub></em> ∈ <em>L</em> are retrieved from the corpus (i.e. <em>L</em> ⊆ <em>C</em>), <strong>internal tree nodes</strong> <em>e<sub>i</sub></em> ∈ <em>E</em> are intermediate conclusions (new sentences not present in corpus <em>C</em>, note that intermediate conclusions are generated by the model), and <em>s<sub>i</sub></em> ∈ <em>S</em> is a list of <strong>entailment steps</strong> that can explain the hypothesis <em>h</em>, which is always the tree root and the final conclusion. An illustration of the problem and expected entailment tree can be found in Figure 1.</p>
<p>Each entailment step <em>s<sub>i</sub></em> represents one inference step from a conjunction of premises to a conclusion. For instance, "<em>l<sub>1</sub></em> ∧ <em>l<sub>2</sub></em> ⇒ <em>e<sub>1</sub></em>" or "<em>l<sub>1</sub></em> ∧ <em>l<sub>2</sub></em> ∧ <em>e<sub>1</sub></em> ⇒ <em>h</em>" could be valid entailment steps in <em>S</em>. Note that the root of <em>T</em> is always the node representing <em>h</em>.</p>
<h2>3.2 Architecture</h2>
<p>Our approach, which we call <em>Iterative Retrieval-Generation Reasoner</em> (IRGR), consists of two modules, the IRGR-retriever and the IRGR-generator. The initial input to the model is the hypothesis <em>h</em> and the corpus of premises <em>C</em>. The generation process is performed through multiple iterations. At each iteration step <em>t</em> ≥ 1 the IRGR-retriever selects a subset of premises from the corpus <em>L<sub>t</sub></em> ⊆ <em>C</em>. The IRGR-generator outputs one entailment step <em>s<sub>t</sub></em> per iteration until the entailment tree <em>T</em> is fully generated. Given <em>S<sub>1:t−1</sub></em> = (<em>s<sub>1</sub></em>, ..., <em>s<sub>t−1</sub></em>) as the list of the entailment steps generated up to the previous iterations <em>t</em> − 1, the generator takes as input <em>L<sub>t</sub></em> and <em>S<sub>1:t−1</sub></em> and produces the next entailment step <em>s<sub>t</sub></em>. The generation stops when the entailment step's conclusion is the hypothesis <em>h</em>, i.e., the proof is finished. Formally, the <em>t</em>-th iteration of the generation process is defined as:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em t-1="t-1">t &amp;= \text{IRGR-retriever}(h, s</em>) \
s_t &amp;= \text{IRGR-generator}(h, \mathcal{L}<em 1:t-1="1:t-1">t, \mathcal{S}</em>)
\end{aligned}
\tag{1}
$$</p>
<p>The IRGR-retriever searches over the premises in corpus <em>C</em> using <em>dense passage retrieval</em> (Karpukhin et al., 2020). Meanwhile, the IRGR-generator was implemented using T5, the Text-to-Text Transformer (Raffel et al., 2020), while any other sequence-to-sequence language model could also be used. An overview of the model can be seen in Figure 3.</p>
<p>3.2.1 IRGR-retriever</p>
<p>The IRGR-retriever module proposed in this work aims to retrieve premises from the corpus $C$. In existing baseline models the retrieval is done in one single step, fetching a fixed set of premises before generation (Tafjord et al., 2021). However, the generation of entailment trees requires a different set of leaves for each entailment step. To address this issue, our IRGR-retriever fetches $k_{t}$ premises from $C$ to produce $\mathcal{L}<em t="t">{t}$ at iteration step $t$. Note that the size of $C$ can be very large ( $k</em>}&lt;&lt;|C|$ ). The value $k_{t}$ is chosen such that the size of $\mathcal{L<em t="t">{t}$ is small enough to fit in the context of a language model while still being large enough to fetch as many premises as possible (in our experiments, the value $k</em>$ is always below 25). We define the retrieval probability of a premise $c \in C$ at a certain iteration step $t$ as:</p>
<p>$$
P\left(c \mid h, s_{t-1}\right)=\frac{\exp \left(\left\langle\mathbf{c}, \mathbf{q}<em c_prime="c^{\prime">{\mathbf{t}}\right\rangle\right)}{\sum</em>
$$} \in C} \exp \left(\left\langle\mathbf{c}^{\prime}, \mathbf{q}_{\mathbf{t}}\right\rangle\right)</p>
<p>Where $\phi$ is the sentence encoder function used to encode both premises and hypothesis, transforming the input text into a dense vector representation in $\mathbb{R}^{M}$. The values $\mathbf{c}=\phi(c), \mathbf{c}^{\prime}=\phi\left(c^{\prime}\right)$ and $\mathbf{q}<em t-1="t-1">{\mathbf{t}}=$ $\phi\left(h, s</em>\right)$ are dense $M$-dimensional vectors. The operator $\langle \cdot\rangle$ represents the inner product between two vectors.</p>
<p>The encoder follows the Siamese Network architecture from Reimers and Gurevych (2019). We select a set of $N$ positive and negative examples in the form of query-value pairs $\left{\left(q_{j}, c_{j}\right)\right}<em j="j">{j=1}^{N}$ for training. Queries $q</em>$ with either random premises from $C$ or premises retrieved using the not fine-tuned version of the encoder (hard negatives).}$ encode both the hypothesis $h$ and previous entailment step $s_{t-1}$ by concatenating their textual values. The positive examples are taken from the golden entailment trees, where $c_{j} \in \mathcal{L}$. For negative examples, we pair a query $q_{j</p>
<p>We define $\hat{y}<em j="j">{j}$ as the label given to the training example $\left(q</em>}, c_{j}\right)$. For positive examples, the label $\hat{y<em i="i">{j}$ depends on how close the leaf node $l</em>$ in the golden tree:} \in \mathcal{L}$ is to the intermediate step $s_{t-1</p>
<p>$$
\hat{y}<em i="i">{j}= \begin{cases}0, &amp; \text { if negative } \ \lambda, &amp; \text { if positive and } l</em>
$$} \notin \operatorname{ant}\left(s_{t-1}\right) \ 1, &amp; \text { if positive and } l_{i} \in \operatorname{ant}\left(s_{t-1}\right)\end{cases</p>
<p>Where $\operatorname{ant}\left(s_{t-1}\right)$ denotes the set of antecedents in some entailment step $s_{t-1}$, and $l_{i} \in \operatorname{ant}\left(s_{t-1}\right)$ means that the leaf node $l_{i}$ is used in the entailment
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Entailment tree example showing how some retrieval examples are challenging. Leaf sentences are not always directly related to hypothesis.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">Conditional</span><span class="w"> </span><span class="nt">Retrieval</span>
<span class="w">    </span><span class="nt">Data</span><span class="o">:</span><span class="w"> </span><span class="nt">hypothesis</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">h</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">corpus</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">C</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">number</span><span class="w"> </span><span class="nt">of</span>
<span class="w">        </span><span class="nt">retrieved</span><span class="w"> </span><span class="nt">premises</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">k_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Result</span><span class="o">:</span><span class="w"> </span><span class="nt">retrieved</span><span class="w"> </span><span class="nt">premises</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Parameter</span><span class="w"> </span><span class="p">:</span><span class="nd">conditioning</span><span class="w"> </span><span class="nt">factor</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">omega</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="nt">Q</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="p">{</span><span class="err">h\</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">set</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">of</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">queries</span><span class="w"> </span><span class="err">\</span><span class="o">(*</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">i</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">0</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">k_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">C</span><span class="o">^</span><span class="p">{</span><span class="err">\prime</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">c</span><span class="w"> </span><span class="err">\in</span><span class="w"> </span><span class="n">C</span><span class="p">:</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="err">\</span><span class="n">notin</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">L</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\}</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">i</span><span class="w"> </span><span class="err">\</span><span class="nt">geq</span><span class="w"> </span><span class="err">\</span><span class="nt">omega</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">l_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">arg</span><span class="w"> </span><span class="err">\</span><span class="nt">max</span><span class="w"> </span><span class="nt">_</span><span class="p">{</span><span class="err">\left(c</span><span class="w"> </span><span class="err">\in</span><span class="w"> </span><span class="err">C^{\prime</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">}</span><span class="w"> </span><span class="nt">P</span><span class="o">(</span><span class="nt">c</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="nt">Q</span><span class="o">)</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">Q</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">Q</span><span class="w"> </span><span class="err">\</span><span class="nt">cup</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">l_{i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\}</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">else</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">l_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">arg</span><span class="w"> </span><span class="err">\</span><span class="nt">max</span><span class="w"> </span><span class="nt">_</span><span class="p">{</span><span class="err">\left(c</span><span class="w"> </span><span class="err">\in</span><span class="w"> </span><span class="err">C^{\prime</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">}</span><span class="w"> </span><span class="nt">P</span><span class="o">(</span><span class="nt">c</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="nt">h</span><span class="o">)</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">end</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cup</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">l_{i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\}</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">end</span>
</code></pre></div>

<p>step $s_{t-1}$. The value $\lambda \in[0: 1]$ is used to give lower priority to leaf nodes not relevant to the current entailment step ( $\lambda=0.75$ gave the best results in our experiments). Finally, we fine-tune the encoder $\phi$ by minimizing the following loss function $L_{\phi}$, where $N$ is the number of training examples:</p>
<p>$$
L_{\phi}=\frac{1}{N} \sum_{j=1}^{N}\left(\hat{y}<em j="j">{j}-\frac{\left\langle\phi\left(q</em>\right)
$$}\right), \phi\left(c_{j}\right)\right\rangle}{\left|\phi\left(q_{j}\right)\right|\left|\phi\left(c_{j}\right)\right|</p>
<p>One significant challenge is that for the first generation step, when $t=1$, the list of previously generated entailment steps $\mathcal{S}<em 1="1">{0}$ is empty. The retrieval only depends on $h$, meaning $\mathcal{L}</em>=$ IRGR-retriever $(h)$. It is more difficult to retrieve premises for leaf nodes when the entailment tree's</p>
<p>depth is large since the leaf nodes have low syntactic and semantic similarity with the hypothesis $h$. For instance, the example in Figure 4 shows how leaf node "a human is a kind of animal" (depth 3) is needed to build the entailment tree, but is syntactically distinct to hypothesis "an astronaut requires the oxygen in a space suit backpack to breath".</p>
<p>To mitigate this problem, we perform a conditional retrieval on the first step, where the retrieval module uses partial results as part of the query, as depicted in Algorithm 1. This algorithm assumes that leaf nodes (premises) further from the root node (hypothesis) are more similar to each other than to the root node itself. The parameter $\omega$ (value $\omega=15$ yields the best results on development set) is used to split the search, such that part of the retrieved premises only depend on the hypothesis $h$. In contrast, the other parts of the retrieved premises depend on the hypothesis and previously retrieved premises stored in the set $Q$.</p>
<h3>3.2.2 IRGR-generator</h3>
<p>The IRGR-generator consists of a sequence-tosequence model that outputs one single entailment step given a context. One key aspect of this module is encoding the input and output as plain text.</p>
<p>Encoding Entailment Trees: Entailment trees are linearized from leaves to root. Each leaf node $l_{i} \in \mathcal{L}$, intermediate node $e_{i} \in \mathcal{E}$ and root node $h$ are encoded with the symbols "sent", "int" and "hypothesis", respectively. The entailment steps represent conjunctions with "\&amp;" and entailment with the symbol " $-&gt;$ ". For instance, the entailment tree depicted in Figure 1 can be represented as:</p>
<div class="codehilite"><pre><span></span><code><span class="s2">&quot;sent1 &amp; sent2 -&gt; int1:</span>
<span class="s2">Eruptions block sunlight;</span>
<span class="s2">sent3 &amp; int1 -&gt; hypothesis;&quot;</span>
</code></pre></div>

<p>Note that the text of intermediate nodes have to be explicitly represented, since they are not part of the corpus $C$. Ultimately, they have to be generated by the model. The input to the model encodes the hypothesis $h$ and retrieved premises $l_{i}^{t} \in \mathcal{L}_{t}$, which are straightforwardly encoded as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="s2">&quot;hypothesis: Eruptions can</span>
<span class="s2">cause plants to die;</span>
<span class="s2">sent1: eruptions emit lava</span>
<span class="s2">sent2: eruptions produce ash</span>
<span class="s2">clouds</span>
<span class="s2">sent3: </span><span class="cp">[</span><span class="nx">...</span><span class="cp">]</span><span class="s2">;&quot;</span>
</code></pre></div>

<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">R@25</th>
<th style="text-align: center;">All-Correct</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Okapi BM25</td>
<td style="text-align: center;">45.01</td>
<td style="text-align: center;">22.35</td>
</tr>
<tr>
<td style="text-align: left;">EntailmentWriter</td>
<td style="text-align: center;">59.76</td>
<td style="text-align: center;">34.70</td>
</tr>
<tr>
<td style="text-align: left;">IRGR-retriever (sing.)</td>
<td style="text-align: center;">64.41</td>
<td style="text-align: center;">40.29</td>
</tr>
<tr>
<td style="text-align: left;">IRGR-retriever (cond.)</td>
<td style="text-align: center;">$\mathbf{6 8 . 2 8}$</td>
<td style="text-align: center;">44.70</td>
</tr>
<tr>
<td style="text-align: left;">IRGR-retriever*</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{5 1 . 4 7}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Retrieval results. The methods with * retrieves more than 25 premises from corpus.</p>
<p>When a leaf sentence $l_{i}^{t}$ is used in the entailment step, it is removed from the context for the following step, and the premise sent identifier is not used to encode new retrieved premises. A detailed example of input and output for the IRGR-generator module is shown in Appendix A.3.</p>
<h2>4 Experiments</h2>
<h3>4.1 Datasets</h3>
<p>We evaluate our architecture on the ENTAILMENTBank dataset (Dalvi et al., 2021), which is comprised of 1,840 questions (each associated with a hypothesis $h_{i}$ and entailment tree $T_{i}$ ) with 5,881 total entailment steps. On average, each entailment tree has 7.6 nodes (including leaf, intermediate, and root) and around 3.2 entailment steps. The corpus of premises $C$ has around 11 K entries and is derived from the WorldTree V2 (Xie et al., 2020) in addition to a few premises created by the EntailmentBank annotators.</p>
<h3>4.2 Evaluation Metrics</h3>
<h3>4.2.1 Retrieval</h3>
<p>We evaluate our IRGR-retriever module using two different sets of metrics. The first one is "Recall at k" (R@k), a standard evaluation metric for information retrieval. The second metric "AllCorrect" is more strict, and the results are only considered correct if all the premises from the golden tree are retrieved. Formally, given the retrieved premises $\mathcal{L}$ and the set of gold premises $\mathcal{L}^{<em>}$, the metrics R@k is given by $\left|\mathcal{L} \cap \mathcal{L}^{</em>}\right| /\left|\mathcal{L}^{<em>}\right|$, and the metric All-Correct is 1 if $\left|\left{x \in \mathcal{L}^{</em>}: x \notin \mathcal{L}\right}\right|=0$, or 0 otherwise. For our experiments, we consider k $=25$ since that's roughly the maximum number of sentences that can fit in the T5 language model's 512 tokens context.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Leaves</th>
<th></th>
<th>Steps</th>
<th></th>
<th>Intermediates</th>
<th></th>
<th>Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>F1</td>
<td>All-Cor.</td>
<td>F1</td>
<td>All-Cor.</td>
<td>F1</td>
<td>All-Cor.</td>
<td>All-Cor.</td>
</tr>
<tr>
<td>EntailmentWriter $\dagger$</td>
<td>39.9</td>
<td>3.8</td>
<td>7.4</td>
<td>2.9</td>
<td>35.9</td>
<td>7.1</td>
<td>2.9</td>
</tr>
<tr>
<td>IRGR (Ours)</td>
<td>45.6</td>
<td>11.8</td>
<td>16.1</td>
<td>11.4</td>
<td>38.8</td>
<td>20.9</td>
<td>11.5</td>
</tr>
<tr>
<td>- w/o iter.</td>
<td>46.6</td>
<td>10.0</td>
<td>11.3</td>
<td>8.2</td>
<td>38.7</td>
<td>20.9</td>
<td>8.2</td>
</tr>
<tr>
<td>- w/o iter. &amp; cond.</td>
<td>36.1</td>
<td>3.8</td>
<td>6.1</td>
<td>3.2</td>
<td>30.5</td>
<td>10.3</td>
<td>3.2</td>
</tr>
</tbody>
</table>
<p>Table 2: Entailment tree scores for baseline methods and IRGR, along four different dimensions (test set). F1 scores measure predicted/gold overlap, while All-Correct scores are 1 when all the predictions for a tree are correct, 0 otherwise. $\dagger$ indicates results using T5-11B model.</p>
<h4>4.2.2 Entailment Tree Generation</h4>
<p>We adopt the evaluation metrics defined by <em>Dalvi et al. (2021)</em>, which compares the generated entailment tree $T=(h,\mathcal{L},\mathcal{E},\mathcal{S})$ with the golden entailment tree $T^{<em>}=\left(h,\mathcal{L}^{</em>},\mathcal{E}^{<em>},\mathcal{S}^{</em>}\right)$. The metrics evaluate the correctness along four dimensions: (1) leaf nodes, (2) entailment steps, (3) generated intermediate nodes, (4) and overall correctness. The first step is to align the nodes from $T$ with the nodes from $T^{*}$ by Jaccard similarity (alignment algorithm and further details of metrics described in Appendix A.2). This method tries to ignore variations between predicted and gold trees that do not change the semantics of the output. The four metric dimensions are described below as follows. For each metric with F1 value, there is also a strict “All-Correct” metric that is equal to 1 when F1 = 1 and 0 otherwise.</p>
<p>Leaf (F1, All-Correct): Tests if the predicted and golden leaf nodes match. This metric compares the sets $\mathcal{L}$ and $\mathcal{L}^{*}$ using F1 score.</p>
<p>Steps (F1, All-Correct): Tests if the predicted entailment steps follow the correct structure. Given that $s_{i}\in\mathcal{S}$ matches $s_{j}\in\mathcal{S}^{*}$ according to the alignment algorithm, tests if the premises of $s_{i}$ are equal to those of $s_{j}$, and computes the F1 score according to the set of all matched steps.</p>
<p>Intermediates (F1, All-Correct): Tests if the sentences of the generated intermediate nodes are correct. Given that intermediate nodes $e_{i}\in\mathcal{E}$ and $e_{j}\in\mathcal{E}^{*}$ were matched by the alignment algorithm, the F1 score is computed by comparing the textual similarity between the set of the aligned and correct pairs $e_{i}$ and $e_{j}$.</p>
<p>Overall (All-Correct): Tests all previous metrics together. The All-Correct value is only 1 if the All-Correct values for leaves, steps, and intermediates are 1. Note that this is a strict metric, and any semantic difference between $T$ and $T^{*}$ will cause the score to be zero.</p>
<h3>4.3 Implementation Details</h3>
<p>All experiments were conducted using a machine with 4 Tesla V100 GPUs with 16GB of memory. Our code is based on HuggingFace’s Transformers <em>Wolf et al. (2020)</em> implementation of the t5-large model <em>Raffel et al. (2020)</em>. The retrieval module uses the Sentence Transformers <em>Reimers and Gurevych (2019)</em> sentence embeddings by fine-tuning the all-mpnet-base-v2 encoder. Please refer to Appendix A.1 for further details on hyper-parameters and training settings.</p>
<h3>4.4 Results</h3>
<h4>4.4.1 Retrieval Results</h4>
<p>We compare our retrieval module against two baselines: Okapi BM25 and the retrieval module of EntailmentWriter, which constitutes of a classifier that retrieves relevant sentences using RoBERTA <em>Liu et al. (2020)</em> and performs re-ranking with Tensorflow-Ranking-BERT <em>Han et al. (2020)</em>.</p>
<p>For comparison, we break down the results of our approach (the IRGR-retriever module) into three variations. The IRGR-retriever (sing.) method retrieves premises from the corpus using a single query element, namely the hypothesis $h$. The IRGR-retriever (cond.) method performs conditioned retrieval as described by Algorithm 1. This retrieval method is not iterative and fetches a fixed set of premises per example. Finally, IRGR-retriever tries to emulate the retrieval when combined with the generation module. It not only performs conditional retrieval, but also fetches a different set of premises for each iteration depending on the generated intermediate nodes. In this retrieval experiment, the IRGR-retriever uses the intermediate nodes from the golden entailment trees.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Method</th>
<th>Leaves</th>
<th></th>
<th>Steps</th>
<th></th>
<th>Intermediates</th>
<th></th>
<th>Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>F1</td>
<td>All-Cor.</td>
<td>F1</td>
<td>All-Cor.</td>
<td>F1</td>
<td>All-Cor.</td>
<td>All-Cor.</td>
</tr>
<tr>
<td>Gold</td>
<td>EntailmentWriter</td>
<td>98.7</td>
<td>86.2</td>
<td>50.5</td>
<td>37.7</td>
<td>67.6</td>
<td>36.2</td>
<td>33.5</td>
</tr>
<tr>
<td></td>
<td>IRGR (Ours)</td>
<td>99.6</td>
<td>97.6</td>
<td>51.1</td>
<td>37.6</td>
<td>66.8</td>
<td>34.1</td>
<td>32.1</td>
</tr>
<tr>
<td>Gold+Dist.</td>
<td>EntailmentWriter</td>
<td>84.3</td>
<td>35.6</td>
<td>35.5</td>
<td>22.9</td>
<td>61.8</td>
<td>28.5</td>
<td>20.9</td>
</tr>
<tr>
<td></td>
<td>IRGR (Ours)</td>
<td>69.9</td>
<td>23.8</td>
<td>30.5</td>
<td>22.3</td>
<td>47.7</td>
<td>26.5</td>
<td>21.8</td>
</tr>
</tbody>
</table>
<p>Table 3: Entailment tree scores for baseline methods and IRGR, along four different dimensions (test set). The "Gold" and "Gold+Dist." tasks do not require retrieval and evaluates solely on the model’s entailment tree generation capabilities.</p>
<p>Therefore, IRGR-retriever results should be considered an upper bound since the generator might not produce the desirable intermediate steps used for queries.</p>
<p>Table 1 shows the R@25 and All-Correct metrics results for different methods. Our premise retrieval module performs consistently better than baselines. For instance, the "IRGR-retriever (cond.)" outperforms the retriever from EntailmentWriter by 14.2% on R@25 and 28.8% on All-Correct metric. Note that "IRGR-retriever" may retrieve a variable number of premises (greater than 25), so we are not reporting R@25 for this method.</p>
<h4>4.4.2 Entailment Tree Generation Results</h4>
<p>We compare our method against EntailmentWriter baseline model on entailment tree generation. As shown in Table 2, our method outperforms the EntailmentWriter in all metrics. The overall tree structure better matches the golden tree, where the score for Overall All-Correct metric has an impressive increase of over 300.0%. Note that EntailmentWriter uses the T5-11B model, which has around 10 times more parameters than our model.</p>
<p>We also show the ablation results of combining different retrieval modules with our proposed generation module on Table 2. The "w/o iter." method does not iteratively retrieve premises, relying on one-shot retrieval at the beginning of the generation. As for the "w/o iter. &amp; cond." method, the model does not use the conditioned retrieval, only relying on the trained dense retrieval with the hypothesis $h$ as the query instead.</p>
<p>The work of <em>Dalvi et al. (2021)</em> defines two other simplified entailment tree generation tasks for further ablation studies. We report the results for what they define as "Task-1" and "Task-2", which are generation tasks where the golden premises are given as input, disregarding the retrieval component. Results in Table 2 report what they define as "Task-3". For clarity, we rename "Task-1" and "Task-2" to "Gold" and "Gold+Dist.", respectively, and show the results in Table 3. In the "Gold" task, each context uses the golden leaves as input, while the "Gold+Dist." task uses the golden leaves plus some distractors (up to 25 distractors). When comparing models with the same number of parameters (we use their reported T5-large results), the generation results without retrieval are roughly the same as the EntailmentWriter method. This experiment shows that the iterative generation can create accurate explanations compared to a single pass generation when using golden retrieved premises.</p>
<h3>4.5 Results Breakdown</h3>
<p>We investigate how well the system performs relative to the number of steps in the gold tree. Figure 5 contains two graphs with results breakdown. The graph on the top shows the all-correct metric values for all three tasks (golden, golden + distractors, and retrieval). The bottom graph shows all F1 metrics (leaves, steps, and intermediates), but only for the "retrieval" task.</p>
<p>The results demonstrate that generating entailment trees becomes increasingly difficult as the size of the tree increases. The IRGR model cannot perfectly predict trees with more than four steps for any of the three different tasks. For the "retrieval" task (without the golden leaf sentences provided as input), the IRGR model cannot successfully generate trees with three or more steps. This could be explained by the fact that the all-correct metric is very strict, and missing or misplacing a single leaf sentence can result in an incorrect tree.</p>
<p>This downwards trend is also present in the "Break Down by Metrics" graph. Most noticeably,</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Result breakdown for number of steps in explanation (entailment steps).
the "Steps (F1)" metric is especially challenging, having values close to zero for entailment trees with more than five steps. This metric is one of the main bottlenecks that lowers the value of the "Overall All-Correct" metric.</p>
<h3>4.6 Analysis</h3>
<p>To understand the strengths and weaknesses of our model, we conduct further analysis of the output of the IRGR. When analyzing errors in the generation of entailment trees, we use the results on the development set for the task with distractors. We manually annotate 50 predicted trees that contain some error compared to the golden tree. We categorize the different types of errors, identifying both individual generated steps errors and entailment tree errors.</p>
<h3>4.6.1 Retrieval Error Analysis</h3>
<p>We use EntailmentBank's development set to automatically compute metrics that will give us some insights into the type of errors made by the IRGR-retriever module. We use "IRGR-retriever (cond.)" to fetch a set of 25 premises for each data point, where we identify the set of true positives (correctly retrieved premises) and the set of false negatives (missing premises).</p>
<p>To understand if the false negatives are more
challenging to retrieve than the true positives, we compute the number of overlapping uni-grams and bi-grams between premises and hypotheses in these two sets. We notice that true positives contain $28.5 \%$ more uni-gram overlap and $68.6 \%$ more bi-gram overlap to the hypothesis compared to the false negatives. These results suggest that premises lexically dissimilar to the hypothesis are, in theory, more challenging to retrieve.</p>
<p>We also investigate how the depth (number of edges in a path from the tree root) of a leaf node in the gold tree correlates to the errors of the IRGR-retriever module. We compute the average depth of true positive nodes as 2.3 , while for falsenegative nodes, the average depth is 3.0 . These results strengthen the idea that leaf nodes deeper in the tree tend to be harder to retrieve, as depicted in Figure 4.</p>
<h3>4.6.2 Entailment Step Error Analysis</h3>
<p>The first error case is called invalid entailment steps ( $56 \%$ of errors), meaning that the conclusion of a step did not follow from the premises. For instance, in "kilogram is used to measure heavy objects" $\wedge$ "an automobile is usually a heavy object" $\Rightarrow$ "kilogram can be used to measure the mass of an automobile", the model assumes that "measure" is the same as "measure of mass", even though that is not explicitly stated.</p>
<p>The second error case accounts for misevaluation and irrelevance ( $27 \%$ of errors). It happens when the step is correct but does not match the golden tree, or when the step is correct but is not relevant or well placed in the final entailment tree. In the third error case, labeled repetition ( $17 \%$ of errors), the conclusion directly copied the premises, not creating a new sentence for the intermediate step.</p>
<h3>4.6.3 Entailment Tree Error Analysis</h3>
<p>When analyzing errors between the entire generated and golden trees, we noticed that incorrect or missing leaves ( $52 \%$ of errors) is the most common type of problem. For instance, when explaining the hypothesis "light year can be used to measure the distance between the stars in milky way" the premises "the milky way is a kind of galaxy" and "a galaxy is made of stars" are missing from the generated tree, making it impossible to explain the second part of the hypothesis.</p>
<p>The remaining errors are categorized as invalid or skipped steps ( $32 \%$ of errors), where the model</p>
<p>commonly concludes an invalid conclusion from premises. This error often overlaps with missing leaves due to the fact that the model uses fewer premises when it skips important intermediate steps; Imperfect evaluation ( $12 \%$ of errors), where the tree produced is valid, but does not match the golden tree; Disconnected or degenerate trees ( $4 \%$ of errors), where the generated output does not form a tree, or follows the desired output format.</p>
<h2>5 Conclusion</h2>
<p>As deep learning models become more ubiquitous in the natural language field, it is desirable that users can understand the model's answer by inspecting the reasoning chain from simple premises to the answer hypothesis. To generate rich, systematic explanations, we proposed a method that can iteratively generate and retrieve premises to produce entailment trees. We show how our approach has advantages over previous baselines, where the retrieved premises and generated explanations are more accurate.</p>
<p>In future work, we plan to improve the generation module by leveraging the structure of the entailment tree instead of relying purely on the encoder-decoder models. This idea could potentially fix the issues with "invalid entailment steps" and "repetition", which account for $73 \%$ of entailment step errors. We also plan to understand how explanations can be generated in the case of a false hypothesis, where we would expect the model to build a conclusion explaining why a statement is incorrect. It could help users verify false claims and understand the meaning behind their incorrectness.</p>
<h2>Acknowledgements</h2>
<p>We are thankful to Felicity M. Lu-Hill for proofreading this paper. The research leading to this paper was supported in part by the Machine Learning, Reasoning, and Intelligence Program of the Office of Naval Research.</p>
<h2>References</h2>
<p>James Allen. 1988. Natural language understanding. Benjamin-Cummings Publishing Co., Inc.</p>
<p>Tiberiu Boros, Stefan Daniel Dumitrescu, and Sonia Pipa. 2017. Fast and accurate decision trees for natural language processing tasks. In Proceedings of the</p>
<p>International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 103110, Varna, Bulgaria. INCOMA Ltd.</p>
<p>Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. 2021. Flexible generation of natural language deductions. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6266-6278, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 9539-9549. Curran Associates, Inc.</p>
<p>Ruben Cartuyvels, Graham Spinks, and MarieFrancine Moens. 2020. Autoregressive reasoning over chains of facts with transformers. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6916-6930, Barcelona, Spain (Online). International Committee on Computational Linguistics.</p>
<p>Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. 2021. Explaining answers with entailment trees. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7358-7370, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen. 2020. A survey of the state of explainable AI for natural language processing. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 447-459, Suzhou, China. Association for Computational Linguistics.</p>
<p>Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. 2020. ERASER: A benchmark to evaluate rationalized NLP models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4443-4458, Online. Association for Computational Linguistics.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: Retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909.</p>
<p>Shuguang Han, Xuanhui Wang, Mike Bendersky, and Marc Najork. 2020. Learning-to-rank with BERT in tf-ranking. CoRR, abs/2004.08476.</p>
<p>Sarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3543-3556, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Harsh Jhamtani and Peter Clark. 2020. Learning to explain: Datasets and models for identifying valid reasoning chains in multihop question-answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Linguistics.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeintensive nlp tasks. In Advances in Neural Information Processing Systems, volume 33, pages 94599474. Curran Associates, Inc.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Ro{bert}a: A robustly optimized {bert} pretraining approach.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019a. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4932-4942, Florence, Italy. Association for Computational Linguistics.</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019b. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4932-4942, Florence, Italy. Association for Computational Linguistics.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages</p>
<p>3982-3992, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Danilo Ribeiro, Thomas Hinrichs, Maxwell Crouse, Kenneth Forbus, Maria Chang, and Michael Witbrock. 2019. Predicting state changes in procedural text using analogical question answering. In 7th Annual Conference on Advances in Cognitive Systems.</p>
<p>Danilo Neves Ribeiro and Kenneth Forbus. 2021. Combining analogy with language models for knowledge extraction. In 3rd Conference on Automated Knowledge Base Construction.</p>
<p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-precision modelagnostic explanations. In Proceedings of the AAAI conference on artificial intelligence, volume 32.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu. 2020. Mpnet: Masked and permuted pretraining for language understanding. In Advances in Neural Information Processing Systems, volume 33, pages 16857-16867. Curran Associates, Inc.</p>
<p>Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3621-3634, Online. Association for Computational Linguistics.</p>
<p>Marco Valentino, Mokanarangan Thayaparan, Deborah Ferreira, and André Freitas. 2021. Hybrid autoregressive inference for scalable multi-hop explanation regeneration. In 36th AAAI Conference on Artificial Intelligence.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Zhengnan Xie, Sebastian Thiem, Jaycie Martin, Elizabeth Wainwright, Steven Marmorstein, and Peter Jansen. 2020. WorldTree v2: A corpus of sciencedomain structured explanations and inference patterns supporting multi-hop inference. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 5456-5473, Marseille, France. European Language Resources Association.</p>
<p>Wenhan Xiong, Xiang Li, Srini Iyer, Jingfei Du, Patrick Lewis, William Yang Wang, Yashar Mehdad, Scott Yih, Sebastian Riedel, Douwe Kiela, and Barlas Oguz. 2021. Answering complex open-domain questions with multi-hop dense retrieval. In International Conference on Learning Representations.</p>
<p>Chen Zhao, Chenyan Xiong, Jordan Boyd-Graber, and Hal Daumé III. 2021. Multi-step reasoning over unstructured text with beam dense retrieval. In North American Association for Computational Linguistics.</p>
<h2>A Appendix</h2>
<h3>A.1 Experiment Details</h3>
<p>The IRGR-generator used the T5-large model from HuggingFace library. The best models were chosen according to the best “Overrall All-Correct” metric on the validation set. During training, we used the following hyper parameters: learning rate: $3 \cdot 10^{-5}$, epochs: 15, training batch size: 4, validation batch size: 4, max number of input tokens: 512, max number of output tokens: 256, warm-up steps: 0, weight decay: 0.</p>
<p>The IRGR-retriever module uses the version all-mpnet-base-v2 from the Sentence-Transformers library. During training, we used the following hyper parameters: learning rate: $5 \cdot 10^{-5}$, epochs: 10, training batch size: 32, validation batch size: 32, loss function: cosine similarity loss, warm-up steps: 0, weight decay: 0.</p>
<h3>A.2 Entailment Tree Evaluation Metrics Details</h3>
<p>The alignment algorithm between the nodes from gold and predicted entailment trees can be roughly described as follows:</p>
<ol>
<li>For each intermediate node $e_{i} \in \mathcal{E}$ and $e_{j} \in$ $\mathcal{E}^{*}$, compute the set of leaf sentences in which the nodes are ancestors.</li>
<li>Align each intermediate node $e_{i}$ to the first golden node $e_{j}$ for which the Jaccard similarity of their respective set of associated leaf sentences is maximum. If any node $e_{i}$ is associated with no gold nodes $e_{j}$ (Jaccard similarity is zero), then $e_{i}$ is aligned to a blank node (no conclusion).</li>
</ol>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Given the list of aligned nodes, the metrics “Intermediates (F1, All-Correct)” compute the similarity between two generated intermediate steps using BLEURT (Sellam et al., 2020), a learned evaluation metric based on BERT. We use the BLEURT-Large-512 model to compute textual similarity scores. The prediction between intermediate nodes is considered correct if $\operatorname{BLEURT}\left(e_{i}, e_{j}\right)&gt;0.28$ (this threshold was originally picked by Dalvi et al. (2021) using a subset of 300 manually labeled pairs).</p>
<h3>A.3 Generator Input and Output Examples</h3>
<p>The generation is done in multiple steps. Below are the input and output examples for a tree with two entailment steps ( $\mathrm{T}=1$ and $\mathrm{T}=2$ ). In the following example, only the golden premises are used, which is how the generator is trained. At test time this format is extended to use the retrieved premises instead.</p>
<p>INPUT T=1: "hypothesis: notebook paper can be recycled many times; sent1: recyclable means a material can be recycled / reused many times sent2: paper is recyclable sent3: notebook paper is a kind of paper;"
OUTPUT T=1: "sent2 \&amp; sent3 -&gt; int1: notebook paper is recyclable;"
INPUT T=2: "hypothesis: notebook paper can be recycled many times; sent1: recyclable means a material can be recycled / reused many times; sent2 \&amp; sent3 -&gt; int1: notebook paper is recyclable;"
OUTPUT T=2: "int1 \&amp; sent1 -&gt; hypothesis;"</p>
<p>Note that the input for $\mathbf{T}=\mathbf{2}$ removed the premises used in the previous entailment step, i.e. "sent2" and "sent3", and added the generated entailment step from $\mathbf{T}=\mathbf{1}$ to the end of the input.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Model available in https://huggingface.co/t5-large
${ }^{3}$ Model available in https://huggingface.co/sentence-transformers/all-mpnet-base-v2&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>